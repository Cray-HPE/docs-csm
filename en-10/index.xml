<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cray System Management Documentation on Cray System Management (CSM)</title>
    <link>/docs-csm/en-10/</link>
    <description>Recent content in Cray System Management Documentation on Cray System Management (CSM)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-10</language>
    <lastBuildDate>Thu, 24 Oct 2024 03:38:47 +0000</lastBuildDate>
    <atom:link href="/docs-csm/en-10/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Update Management Network From 1.4 To 1.5</title>
      <link>/docs-csm/en-10/upgrade/update_management_network/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:47 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/update_management_network/</guid>
      <description>Update Management Network From 1.4 To 1.5 IMPORTANT: These procedures only need to be followed if upgrading from CSM 0.9 (Shasta 1.4). If upgrading from CSM 1.0.1 (Shasta 1.5), these procedures should already have been done.&#xA;New Features and Functions for v1.5 Accessing Network Switches Finding Switch Hostnames and IP Addresses Reminders CMM Static Lag Configuration BGP Aruba Mellanox Apollo Server Configuration 1. New Features and Functions for v1.5 Static Lags from the CDU switches to the CMMs (Aruba and Dell).</description>
    </item>
    <item>
      <title>Relevant Troubleshooting Links for Upgrade-Related Issues</title>
      <link>/docs-csm/en-10/upgrade/1.0.11/upgrade_troubleshooting/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:46 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.11/upgrade_troubleshooting/</guid>
      <description>Relevant Troubleshooting Links for Upgrade-Related Issues General Kubernetes Commands for Troubleshooting Please see Kubernetes_Troubleshooting_Information.&#xA;Troubleshooting PXE Boot Issues If execution of the upgrade procedures results in NCNs that have errors booting, please refer to these troubleshooting procedures: PXE Booting Runbook&#xA;Troubleshooting NTP During execution of the upgrade procedure, if it is noted that there is clock skew on one or more NCNs, the following procedure can be used to troubleshoot NTP config or to sync time: Configure NTP on NCNs</description>
    </item>
    <item>
      <title>CEPHADM</title>
      <link>/docs-csm/en-10/upgrade/1.0.1/resource_material/storage/cephadm-reference/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:45 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.1/resource_material/storage/cephadm-reference/</guid>
      <description>CEPHADM cephadm is a new function introduced in Ceph Octopus 15. It allows for an easier method to install and manage Ceph nodes.&#xA;Traditional Ceph commands On ncn-s001, ncn-s002, or ncn-s003:&#xA;ncn-s# cephadm shell The previous command creates a container and opens an interactive shell with access to run Ceph commands the traditional way.&#xA;Or optionally, you can execute your command as follos:&#xA;ncn-s# cephadm shell -- ceph -s Ceph Device Operations There are multiple ways to do Ceph device operations now.</description>
    </item>
    <item>
      <title>Worker-Specific Manual Steps</title>
      <link>/docs-csm/en-10/upgrade/1.0.1/resource_material/k8s/worker-reference/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:45 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.1/resource_material/k8s/worker-reference/</guid>
      <description>Worker-Specific Manual Steps Determine if the worker being rebuilt is running a cray-cps-cm-pm pod, by running the cray cps deployment list command below. If so, there is a final step to redeploy this pod once the worker is rebuilt. In the example below, nodes ncn-w001, ncn-w002, and ncn-w003 have the pod.&#xA;NOTE: If the command below does not return any pod names, proceed to step 2.&#xA;NOTE: A 404 error is expected if the COS product is not installed on the system.</description>
    </item>
    <item>
      <title>Kubernetes Log File Locations</title>
      <link>/docs-csm/en-10/troubleshooting/kubernetes/kubernetes_log_file_locations/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:44 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/kubernetes/kubernetes_log_file_locations/</guid>
      <description>Kubernetes Log File Locations Locations of various K8s log types on the system.&#xA;Log Type Component Purpose Location Kubernetes Master API server Responsible for serving the API kubectl -n kube-system logs -l component=kube-apiserver Scheduler Responsible for making scheduling decisions kubectl -n kube-system logs -l component=kube-scheduler Controller Manages replication controllers kubectl -n kube-system logs -l component=kube-controller-manager Kubernetes Worker Kubelet Responsible for running containers on the node journalctl -xeu kubelet Kube proxy Responsible for service load balancing kubectl -n kube-system logs -l k8s-app=kube-proxy </description>
    </item>
    <item>
      <title>Cray Advanced Platform Monitoring and Control (CAPMC) Reinit and Configuration Notice</title>
      <link>/docs-csm/en-10/troubleshooting/capmc/capmc_reinit_and_config/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:42 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/capmc/capmc_reinit_and_config/</guid>
      <description>Cray Advanced Platform Monitoring and Control (CAPMC) Reinit and Configuration Notice CAPMC is now capable of doing a reinit of hardware that does not support GracefulRestart or ForceRestart. This is done by powering off the target component and then powering it back on. This behavior is controlled by a setting in the CAPMC ConfigMap. Along with reinit is the ability to control other aspects of CAPMCs behavior. To do this, the CAPMC ConfigMap must be modified to include a CapmcConfiguration section.</description>
    </item>
    <item>
      <title>Gigabyte BMC Missing Redfish Data</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/gigabyte_bmc_missing_redfish_data/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:42 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/gigabyte_bmc_missing_redfish_data/</guid>
      <description>Gigabyte BMC Missing Redfish Data Follow this procedure if you notice data from Gigabyte nodes is missing from Hardware State Manager (HSM) or other CSM tools.&#xA;If data from Gigabyte nodes is missing from HSM or other CSM tools, check the Redfish endpoint on the BMC to see if the data is present.&#xA;If the data is not present in the Redfish, then a cold reset of the BMC is needed to refresh the Redfish values.</description>
    </item>
    <item>
      <title>Adding a Ceph Node to the Ceph Cluster</title>
      <link>/docs-csm/en-10/operations/utility_storage/add_ceph_node/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/add_ceph_node/</guid>
      <description>Adding a Ceph Node to the Ceph Cluster NOTE: This operation can be done to add more than one node at the same time.&#xA;Add Join Script Copy join script from ncn-m001 to the storage node that was rebuilt or added.&#xA;Run this command on the storage node that was rebuilt or added.&#xA;ncn-s# mkdir -pv /usr/share/doc/csm/scripts &amp;amp;&amp;amp; scp -p ncn-m001:/usr/share/doc/csm/scripts/join_ceph_cluster.sh /usr/share/doc/csm/scripts Start monitoring the Ceph health alongside the main procedure.</description>
    </item>
    <item>
      <title>Access System Management Health Services</title>
      <link>/docs-csm/en-10/operations/system_management_health/access_system_management_health_services/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:39 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_management_health/access_system_management_health_services/</guid>
      <description>Access System Management Health Services All System Management Health services are exposed outside the cluster through the Keycloak gatekeeper and Istio&amp;rsquo;s ingress gateway to enforce the authentication and authorization policies. The URLs to access these services are available on any system with CAN, BGP, MetalLB, and external DNS properly configured. This page provides administrators with the URLs on the system needed to set up the System Management Health services and access their components, via the Grafana and Kiali applications.</description>
    </item>
    <item>
      <title>Add Liquid-Cooled Cabinets to SLS</title>
      <link>/docs-csm/en-10/operations/system_layout_service/add_liquid-cooled_cabinets_to_sls/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:39 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_layout_service/add_liquid-cooled_cabinets_to_sls/</guid>
      <description>Add Liquid-Cooled Cabinets to SLS This procedure adds one or more liquid-cooled cabinets and associated CDU management switches to SLS.&#xA;NOTE: This procedure is intended to be used in conjunction with the top level Add additional Liquid-Cooled Cabinets to a System procedure.&#xA;Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI.&#xA;The latest CSM documentation is installed on the system.</description>
    </item>
    <item>
      <title>Configure BMC and Controller Parameters with SCSD</title>
      <link>/docs-csm/en-10/operations/system_configuration_service/configure_bmc_and_controller_parameters_with_scsd/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_configuration_service/configure_bmc_and_controller_parameters_with_scsd/</guid>
      <description>Configure BMC and Controller Parameters with SCSD The System Configuration Service (SCSD) allows administrators to set various BMC and controller parameters for components in liquid-cooled cabinets. These parameters are typically set during discovery, but this tool enables parameters to be set before or after discovery. The operations to change these parameters are available in the cray CLI under the scsd command.&#xA;The parameters which can be set are:&#xA;SSH key NTP server Syslog server BMC/Controller passwords SSH console key IMPORTANT: If the scsd tool is used to update the SSHConsoleKey value outside of ConMan, it will disrupt the ConMan connection to the console and collection of console logs.</description>
    </item>
    <item>
      <title>Create a Backup of the Spire Postgres Database</title>
      <link>/docs-csm/en-10/operations/spire/create_a_backup_of_the_spire_postgres_database/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/spire/create_a_backup_of_the_spire_postgres_database/</guid>
      <description>Create a Backup of the Spire Postgres Database Perform a manual backup of the contents of the Spire Postgres database. This backup can be used to restore the contents of the Spire Postgres database at a later point in time using the Restore Spire Postgres from Backup procedure.&#xA;Prerequisites Healthy Spire Postgres Cluster.&#xA;Use patronictl list on the Spire Postgres cluster to determine the current state of the cluster and note which member is the Leader.</description>
    </item>
    <item>
      <title>API Authorization</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/api_authorization/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/api_authorization/</guid>
      <description>API Authorization Authorization for REST API calls is only done at the API gateway. This is facilitated through policy checks to the Open Policy Agent (OPA). Every REST API call into the system is sent to the OPA to make an authorization decision. The decision is based on the authenticated JSON Web Token (JWT) passed into the request.&#xA;This page lists the available personas and the supported REST API endpoints for each.</description>
    </item>
    <item>
      <title>NTP Resiliency</title>
      <link>/docs-csm/en-10/operations/resiliency/ntp_resiliency/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/resiliency/ntp_resiliency/</guid>
      <description>NTP Resiliency Synchronize the time on all non-compute nodes (NCNs) via Network Time Protocol (NTP). Avoid a single point of failure for NTP when testing system resiliency.&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Procedure Set the date manually if the time on NCNs is off by more than an a few hours.&#xA;For example:&#xA;ncn-m001# timedatectl set-time &amp;#34;2021-02-19 15:04:00&amp;#34; Configure NTP on the Pre-install Toolkit (PIT).&#xA;ncn-m001# /root/bin/configure-ntp.sh Sync NTP on all other nodes.</description>
    </item>
    <item>
      <title>Cray Advanced Platform Monitoring and Control (CAPMC)</title>
      <link>/docs-csm/en-10/operations/power_management/cray_advanced_platform_monitoring_and_control_capmc/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/cray_advanced_platform_monitoring_and_control_capmc/</guid>
      <description>Cray Advanced Platform Monitoring and Control (CAPMC) The Cray Advanced Platform Monitoring and Control (CAPMC) service enables direct hardware control of nodes, compute blades, router modules, and liquid cooled chassis. CAPMC talks to BMCs via Redfish to control power, query status, and manage power capping on target components. These controls enable an administrator and 3rd party software to more intelligently manage state and system-wide power consumption.&#xA;Administrators can use the cray CLI for power operations from any system that has HTTPS access to the System Management Services.</description>
    </item>
    <item>
      <title>Manage Repositories with Nexus</title>
      <link>/docs-csm/en-10/operations/package_repository_management/manage_repositories_with_nexus/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/package_repository_management/manage_repositories_with_nexus/</guid>
      <description>Manage Repositories with Nexus This section describes how to connect to Nexus with the Web UI, as well as how to access the REST API from non-compute nodes (NCNs) or compute nodes to manage repositories.&#xA;System domain name Access Nexus with the web UI Access Nexus with the REST API Pagination Check the status of Nexus List repositories List assets Create a repository Update a repository Delete a repository Create a blob store Delete a blob store System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system&amp;rsquo;s fully qualified domain name (FQDN).</description>
    </item>
    <item>
      <title>Final Validation Steps</title>
      <link>/docs-csm/en-10/operations/node_management/rebuild_ncns/final_validation_steps/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/rebuild_ncns/final_validation_steps/</guid>
      <description>Final Validation Steps Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig after rebooting the node.&#xA;NOTE: The following command will indicate if a CFS job is currently in progress for this node.&#xA;IMPORTANT: This command assumes that you have set the variables from the prerequisites section.&#xA;ncn-mw# cray cfs components describe $XNAME --format json Example output:&#xA;{ &amp;#34;configurationStatus&amp;#34;: &amp;#34;configured&amp;#34;, &amp;#34;desiredConfig&amp;#34;: &amp;#34;ncn-personalization-full&amp;#34;, &amp;#34;enabled&amp;#34;: true, &amp;#34;errorCount&amp;#34;: 0, &amp;#34;id&amp;#34;: &amp;#34;x3000c0s7b0n0&amp;#34;, &amp;#34;retryPolicy&amp;#34;: 3, If the configurationStatus is pending, wait for the job to finish before continuing.</description>
    </item>
    <item>
      <title>Access and Update Settings for Replacement NCNs</title>
      <link>/docs-csm/en-10/operations/node_management/access_and_update_the_settings_for_replacement_ncns/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/access_and_update_the_settings_for_replacement_ncns/</guid>
      <description>Access and Update Settings for Replacement NCNs When a new NCN is added to the system as a hardware replacement, it might use the default credentials. Contact HPE Cray service to learn what these are.&#xA;Use this procedure to verify that the default BMC credentials are set correctly after a replacement NCN is installed, cabled, and powered on.&#xA;All NCN BMCs must have credentials set up for ipmitool access.&#xA;Prerequisites A new non-compute node (NCN) has been added to the system as a hardware replacement.</description>
    </item>
    <item>
      <title>Add NCN Data</title>
      <link>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/add_ncn_data/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/add_ncn_data/</guid>
      <description>Add NCN Data Description Add NCN data to the System Layout Service (SLS), Boot Script Service (BSS), and Hardware State Manager (HSM) as needed, in order to add an NCN to the system.&#xA;Scenarios where this procedure is applicable:&#xA;Adding a management NCN that has not previously been in the system: Add an additional NCN to an existing cabinet Add an NCN that is replacing another NCN of the same type and in the same slot Add a new NCN that replaces an NCN removed from the system in a different location Adding a management NCN that has been present in the system previously: Add an NCN that was previously removed from the system to move it to a new location Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system.</description>
    </item>
    <item>
      <title>Check BGP Status and Reset Sessions</title>
      <link>/docs-csm/en-10/operations/network/metallb_bgp/check_bgp_status_and_reset_sessions/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/metallb_bgp/check_bgp_status_and_reset_sessions/</guid>
      <description>Check BGP Status and Reset Sessions Check the Border Gateway Protocol (BGP) status on the Aruba and Mellanox switches and verify that all sessions are in an Established state. If the state of any session in the table is Idle, then the BGP sessions must be reset.&#xA;Prerequisites Procedure Mellanox Aruba Further steps Prerequisites This procedure requires administrative privileges.&#xA;Procedure The following procedures may not resolve the problem after just one attempt.</description>
    </item>
    <item>
      <title>Management Network ACL Configuration</title>
      <link>/docs-csm/en-10/operations/network/management_network/management_network_acl_configuration/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/management_network/management_network_acl_configuration/</guid>
      <description>Management Network ACL Configuration This page describes the purpose of the ACLs and how they are configured.&#xA;Requirements Access to the switches Aruba Configuration These ACLs are designed to block traffic from the Node Management Network (NMN) to and from the Hardware Management Network (HMN).&#xA;These need to be set where the Layer3 interface is located, this will most likely be a VSX pair of switches. These ACLs are required on both switches in the pair.</description>
    </item>
    <item>
      <title>Add NCNs and UANs to External DNS</title>
      <link>/docs-csm/en-10/operations/network/external_dns/add_ncns_and_uans_to_external_dns/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:26 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/external_dns/add_ncns_and_uans_to_external_dns/</guid>
      <description>Add NCNs and UANs to External DNS Edit the cray-externaldns-coredns ConfigMap to associate names with the Customer Access Network (CAN) IP addresses for non-compute nodes (NCNs) and User Access Nodes (UANs) in external DNS.&#xA;The cray-externaldns-coredns file contains the configuration files for external DNS&amp;rsquo; CoreDNS instance.&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Procedure View the existing cray-externaldns-coredns ConfigMap.&#xA;ncn-w001# kubectl -n services get configmap cray-externaldns-coredns -o jsonpath=&amp;#39;{.data}&amp;#39; map[Corefile:.:53 { errors health log ready kubernetes cluster.</description>
    </item>
    <item>
      <title>DHCP</title>
      <link>/docs-csm/en-10/operations/network/dhcp/dhcp/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:26 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/dhcp/dhcp/</guid>
      <description>DHCP The Dynamic Host Configuration Protocol (DHCP) service on the HPE Cray EX system uses the Internet Systems Consortium (ISC) Kea tool. Kea provides more robust management capabilities for DHCP servers.&#xA;For more information: https://www.isc.org/kea/.&#xA;The following improvements to the DHCP service are included:&#xA;API access to manage DHCP Scalable pod that uses MetalLB instead of host networking Options for updates to HPE Cray EX management system IP addresses DHCP Helper Workflow The DHCP-Helper uses the following workflow:</description>
    </item>
    <item>
      <title>Domain Name Service (DNS)</title>
      <link>/docs-csm/en-10/operations/network/dns/dns/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:26 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/dns/dns/</guid>
      <description>Domain Name Service (DNS) The central DNS infrastructure provides the structural networking hierarchy and datastore for the system. All DNS requests are managed by resolvers, not by the central DNS infrastructure. Resolvers provide the following within DNS:&#xA;Security by scoping requests from clients.&#xA;For example, disallowing cross-network DNS lookups.&#xA;Load reduction on the central DNS infrastructure:&#xA;Caching requests and handling scoping requests. Providing request recursion where necessary. The Data Helper Tools are used to update records, and takes in changes from the following sources:</description>
    </item>
    <item>
      <title>Access to System Management Services</title>
      <link>/docs-csm/en-10/operations/network/access_to_system_management_services/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/access_to_system_management_services/</guid>
      <description>Access to System Management Services The standard configuration for System Management Services (SMS) is the containerized REST micro-service with a public API. All of the micro-services provide an HTTP interface and are collectively exposed through a single gateway URL. The API gateway for the system is available at a well known URL based on the domain name of the system. It acts as a single HTTPS endpoint for terminating Transport Layer Security (TLS) using the configured certificate authority.</description>
    </item>
    <item>
      <title>CAN with Dual-Spine Configuration</title>
      <link>/docs-csm/en-10/operations/network/customer_access_network/can_with_dual-spine_configuration/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/customer_access_network/can_with_dual-spine_configuration/</guid>
      <description>CAN with Dual-Spine Configuration The Customer Access Network (CAN) needs to be connected to both spines in a dual-spine configuration so that each spine can access the outside network. However, the NCNs should only have one default gateway. Therefore, the multi-active gateway protocol (MAGP) on the Mellanox spines can be used to create a virtual router gateway IP address that can direct to either of the spines, depending on the state of the spines.</description>
    </item>
    <item>
      <title>About Kubernetes Taints and Labels</title>
      <link>/docs-csm/en-10/operations/kubernetes/about_kubernetes_taints_and_labels/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:23 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/about_kubernetes_taints_and_labels/</guid>
      <description>About Kubernetes Taints and Labels Kubernetes labels control node affinity, which is the property of pods that attracts them to a set of nodes. On the other hand, Kubernetes taints enable a node to repel a set of pods. In addition, pods can have tolerances for taints to allow them to run on nodes with certain taints.&#xA;Taints are controlled with the kubectl taint nodes command, while node labels for various nodes can be customized with a configmap that contains the desired values.</description>
    </item>
    <item>
      <title>Adjust HM Collector resource limits and requests</title>
      <link>/docs-csm/en-10/operations/hmcollector/adjust_hmcollector_resource_limits_requests/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:22 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hmcollector/adjust_hmcollector_resource_limits_requests/</guid>
      <description>Adjust HM Collector resource limits and requests Inspect current resource usage Inspect pods for OOMKilled events How to adjust resource limits Inspect current resource usage View resource usage of the containers in the cray-hms-hmcollector-ingress pods:&#xA;ncn-mw# kubectl -n services top pod -l app.kubernetes.io/name=cray-hms-hmcollector --containers Example output:&#xA;POD NAME CPU(cores) MEMORY(bytes) cray-hms-hmcollector-7c5b797c5c-zxt67 istio-proxy 187m 275Mi cray-hms-hmcollector-7c5b797c5c-zxt67 cray-hms-hmcollector 4398m 296Mi The default resource limits for the cray-hms-hmcollector container are:&#xA;CPU: 4 or 4000m Memory: 5Gi The default resource limits for the istio-proxy container are:</description>
    </item>
    <item>
      <title>Build a New UAN Image Using the Default Recipe</title>
      <link>/docs-csm/en-10/operations/image_management/build_a_new_uan_image_using_the_default_recipe/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:22 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/image_management/build_a_new_uan_image_using_the_default_recipe/</guid>
      <description>Build a New UAN Image Using the Default Recipe Build or rebuild the User Access Node (UAN) image using either the default UAN image or image recipe. Both of these are supplied by the UAN product stream installer.&#xA;Prerequisites Overview Remove Slingshot Diagnostics RPM From Default UAN Recipe Build the UAN Image Automatically Using IMS Build the UAN Image By Customizing It Manually Prerequisites Both the Cray Operation System (COS) and UAN product streams must be installed.</description>
    </item>
    <item>
      <title>Add a Switch to the HSM Database</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/add_a_switch_to_the_hsm_database/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/add_a_switch_to_the_hsm_database/</guid>
      <description>Add a Switch to the HSM Database Manually add a switch to the Hardware State Manager (HSM) database. Switches need to be in the HSM database in order to update their firmware with the Firmware Action Service (FAS).&#xA;Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. The component name (xname), IP address, user name, and password are known for the switch being added.</description>
    </item>
    <item>
      <title>Access Compute Node Logs</title>
      <link>/docs-csm/en-10/operations/conman/access_compute_node_logs/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/conman/access_compute_node_logs/</guid>
      <description>Access Compute Node Logs This procedure shows how the ConMan utility can be used to retrieve compute node logs.&#xA;Prerequisites The user performing this procedure needs to have access permission to the cray-console-operator pod.&#xA;Limitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.&#xA;Procedure Note: this procedure has changed since the CSM 0.9 release.&#xA;Log on to a Kubernetes master or worker node.</description>
    </item>
    <item>
      <title>FAS Admin Procedures</title>
      <link>/docs-csm/en-10/operations/firmware/fas_admin_procedures/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/firmware/fas_admin_procedures/</guid>
      <description>FAS Admin Procedures Procedures for leveraging the Firmware Action Service (FAS) CLI to manage firmware.&#xA;Topics Warning for non-compute nodes (NCNs) Ignore management nodes within FAS Override an image for an update Check for new firmware versions with a dry-run Load firmware from Nexus Load firmware from RPM or ZIP file Warning for non-compute nodes (NCNs) NCNs and their BMCs should be locked with the HSM locking API to ensure they are not unintentionally updated by FAS.</description>
    </item>
    <item>
      <title>Ansible Execution Environments</title>
      <link>/docs-csm/en-10/operations/configuration_management/ansible_execution_environments/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:17 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/ansible_execution_environments/</guid>
      <description>Ansible Execution Environments Configuration Framework Service (CFS) sessions are comprised of a single Kubernetes pod with several containers. Inventory and Git clone setup containers run first, and a teardown container runs last (if the session is running an image customization).&#xA;The containers that run the Ansible code cloned from the Git repositories in the configuration layers are Ansible Execution Environments (AEE). The AEE is provided as a SLES-based docker image, which includes Ansible version 2.</description>
    </item>
    <item>
      <title>CRUS Workflow</title>
      <link>/docs-csm/en-10/operations/compute_rolling_upgrades/crus_workflow/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:17 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/compute_rolling_upgrades/crus_workflow/</guid>
      <description>CRUS Workflow Note: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality.&#xA;The following workflow is intended to be a high-level overview of how to upgrade compute nodes. This workflow depicts how services interact with each other during the compute node upgrade process, and helps to provide a quicker and deeper understanding of how the system functions.</description>
    </item>
    <item>
      <title>Artifact Management</title>
      <link>/docs-csm/en-10/operations/artifact_management/artifact_management/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:14 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/artifact_management/artifact_management/</guid>
      <description>Artifact Management The Ceph Object Gateway Simple Storage Service (S3) API is used for artifact management. The RESTful API that Ceph provides via the gateway is compatible with the basic data access model of the Amazon S3 API. See the https://docs.ceph.com/docs/mimic/radosgw/s3/ for more information about compatibility. The object gateway is also referred to as the RADOS gateway or simply RGW.&#xA;S3 is an object storage service that provides high-level performance, scalability, security, and data availability.</description>
    </item>
    <item>
      <title>BOS Workflows</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/bos_workflows/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:14 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/bos_workflows/</guid>
      <description>BOS Workflows The following workflows present a high-level overview of common Boot Orchestration Service (BOS) operations. These workflows depict how services interact with each other when booting, configuring, or shutting down nodes. They also help provide a quicker and deeper understanding of how the system functions.&#xA;Terminology Workflows Boot and configure nodes Reconfigure nodes Power off nodes Terminology The following are mentioned in the workflows:&#xA;Boot Orchestration Service (BOS) is responsible for booting, configuring, and shutting down collections of nodes.</description>
    </item>
    <item>
      <title>Add a Volume to UAS</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/add_a_volume_to_uas/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:08 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/add_a_volume_to_uas/</guid>
      <description>Add a Volume to UAS This procedure registers and configures a volume in UAS so that the volume can be mounted in UAIs.&#xA;See List Volumes Registered in UAS for examples of valid volume configurations. Refer to Elements of a UAI for descriptions of the volume configuration fields and values.&#xA;Note the following caveats about adding volumes to UAS:&#xA;A volume description may specify an underlying directory that is NFS-mounted on the UAI host nodes.</description>
    </item>
    <item>
      <title>Security Hardening</title>
      <link>/docs-csm/en-10/operations/csm_product_management/apply_security_hardening/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:08 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/csm_product_management/apply_security_hardening/</guid>
      <description>Security Hardening This is an overarching guide to further harden the security posture of a Cray System Management (CSM) system.&#xA;If a subset of the steps in this procedure were completed as a consequence of an install, upgrade, or other guidance, then it is safe to skip that subset following a review.&#xA;Prerequisites None.&#xA;Procedure Change passwords and credentials.&#xA;Perform procedure(s) in Change Passwords and Credentials.&#xA;Restrict access to ncn-images S3 Bucket.</description>
    </item>
    <item>
      <title>Accessing LiveCD USB Device After Reboot</title>
      <link>/docs-csm/en-10/operations/access_livecd_usb_device_after_reboot/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:07 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/access_livecd_usb_device_after_reboot/</guid>
      <description>Accessing LiveCD USB Device After Reboot This is a procedure that only applies to the LiveCD USB device after the PIT node has been rebooted.&#xA;USB ONLY If the installation above was done from a Remote ISO.&#xA;After deploying the LiveCD&amp;rsquo;s NCN, the LiveCD USB itself is unharmed and available to an administrator.&#xA;Procedure Mount and view the USB device.&#xA;ncn-m001# mkdir -pv /mnt/{cow,pitdata} ncn-m001# mount -vL cow /mnt/cow ncn-m001# mount -vL PITDATA /mnt/pitdata ncn-m001# ls -ld /mnt/cow/rw/* Example output:</description>
    </item>
    <item>
      <title>CAPMC Deprecation Notice many CAPMC v1 features are being partially deprecated</title>
      <link>/docs-csm/en-10/introduction/capmc_deprecation/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:07 +0000</pubDate>
      <guid>/docs-csm/en-10/introduction/capmc_deprecation/</guid>
      <description>CAPMC Deprecation Notice: many CAPMC v1 features are being partially deprecated Deprecated Features in CSM 1.0.1 Many CAPMC v1 REST API and CLI features are being deprecated as part of CSM version 1.0.1; Full removal of the following deprecated CAPMC features will happen in CSM version 1.3. Further development of CAPMC service or CLI has stopped. CAPMC has entered end-of-life but will still be generally available. CAPMC is going to be replaced with the Power Control Service (PCS) in a future release.</description>
    </item>
    <item>
      <title>Set Gigabyte Node BMC to Factory Defaults</title>
      <link>/docs-csm/en-10/install/set_gigabyte_node_bmc_to_factory_defaults/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:06 +0000</pubDate>
      <guid>/docs-csm/en-10/install/set_gigabyte_node_bmc_to_factory_defaults/</guid>
      <description>Set Gigabyte Node BMC to Factory Defaults There are cases when a Gigabyte node BMC must be reset to its factory default settings. This page describes when this reset is appropriate, and how to use management scripts and text files to do the reset.&#xA;Set the BMC to the factory default settings in the following cases:&#xA;There are problems using the ipmitool command and Redfish does not respond. There are problems using the ipmitool command and Redfish is running.</description>
    </item>
    <item>
      <title>Hotfix to workaround known mac-learning issue with 8325</title>
      <link>/docs-csm/en-10/install/8325_mac_learning_hotfix/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:03 +0000</pubDate>
      <guid>/docs-csm/en-10/install/8325_mac_learning_hotfix/</guid>
      <description>Hotfix to workaround known mac-learning issue with 8325 Issue description Aruba CR: 90598&#xA;Affected platform: 8325&#xA;Symptom: MAC learning stops.&#xA;Scenario: Under extremely rare DMA stress conditions, anL2 learning thread may timeout and exit preventing future MAC learning.&#xA;Workaround: Reboot the switch or monitor the L2 thread and restart it with an NAE script.&#xA;Fixed in: 10.06.0130, 10.7.0010 and above.&#xA;Aruba release notes&#xA;To fix the issue without upgrading software You can run a NAE script on the 8325 platform switches to resolve mac learning issue.</description>
    </item>
    <item>
      <title>Certificate Authority</title>
      <link>/docs-csm/en-10/background/certificate_authority/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:00 +0000</pubDate>
      <guid>/docs-csm/en-10/background/certificate_authority/</guid>
      <description>Certificate Authority While a system is being installed for the first time, a certificate authority (CA) is needed. This can be generated for a system, or one can be supplied from a customer intermediate CA. Outside of a new installation, there is no supported method to rotate or change the platform CA in this release.&#xA;Topics Overview Use default platform-generated CA Customize platform-generated CA Use external CA Overview At install time, a PKI certificate authority can either be generated for a system, or a customer can opt to supply their own intermediate CA.</description>
    </item>
    <item>
      <title>Prepare For Upgrade</title>
      <link>/docs-csm/en-10/upgrade/prepare_for_upgrade/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:47 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/prepare_for_upgrade/</guid>
      <description>Prepare For Upgrade Before beginning an upgrade to a new version of CSM, there are a few things to do on the system first.&#xA;Gracefully shutdown workloads affected by CSM upgrade.&#xA;Warning: Although it is expected that compute nodes and application nodes will continue to provide their services without interruption when the management nodes and services are being upgraded by CSM, it is important to be aware of the possibility of interruption of running jobs.</description>
    </item>
    <item>
      <title>Stage 0 - Prerequisites and Preflight Checks</title>
      <link>/docs-csm/en-10/upgrade/1.0.11/stage_0_prerequisites/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:46 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.11/stage_0_prerequisites/</guid>
      <description>Stage 0 - Prerequisites and Preflight Checks NOTE: CSM-1.0.1 or later is required in order to upgrade to CSM-1.0.11.&#xA;The following command can be used to check the CSM version on the system:&#xA;ncn# kubectl get cm -n services cray-product-catalog -o json | jq -r &amp;#39;.data.csm&amp;#39; This check will also be conducted in the &amp;lsquo;prerequisites.sh&amp;rsquo; script listed below and will fail if the system is not running CSM-1.0.1 or CSM-1.0.10.</description>
    </item>
    <item>
      <title>Kubernetes Troubleshooting Information</title>
      <link>/docs-csm/en-10/troubleshooting/kubernetes/kubernetes_troubleshooting_information/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:44 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/kubernetes/kubernetes_troubleshooting_information/</guid>
      <description>Kubernetes Troubleshooting Information Commands for performing basic Kubernetes cluster troubleshooting.&#xA;Access pod logs Describe a node Describe a pod Open a shell on a pod Run a single command on a pod Connect to a running container Scale a deployment Remove a deployment with the manifest and reapply the deployment Delete a pod Access pod logs Use one of the following commands to retrieve pod-related logs:&#xA;ncn-mw# kubectl logs POD_NAME ncn-mw# kubectl logs POD_NAME -c CONTAINER_NAME If the pods keeps crashing, open a log for the previous instance using the following command:</description>
    </item>
    <item>
      <title>Stage 0 - Prerequisites and Preflight Checks</title>
      <link>/docs-csm/en-10/upgrade/1.0.1/stage_0_prerequisites/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:44 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.1/stage_0_prerequisites/</guid>
      <description>Stage 0 - Prerequisites and Preflight Checks NOTE: CSM-0.9.4 or later CSM 0.9.x is required in order to upgrade to CSM-1.0.1 (available with Shasta v1.5).&#xA;NOTE: Installed CSM versions may be listed from the product catalog using the following command. This will sort a semantic version without a hyphenated suffix after the same semantic version with a hyphenated suffix, e.g. 1.0.0 &amp;gt; 1.0.0-beta.19.&#xA;Use the following command can be used to check the CSM version on the system:</description>
    </item>
    <item>
      <title>Hang Listing BOS Sessions</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/hang_listing_bos_sessions/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:42 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/hang_listing_bos_sessions/</guid>
      <description>Hang Listing BOS Sessions Overview Symptoms Remedy Prevention Overview BOS v1 loses the ability to list its sessions after too many of them exist in its database. This has only been observed happening when the total number of sessions in the database is well over 1000.&#xA;Symptoms When this situation occurs, attempts to list BOS sessions using the API or CLI will hang. This may also be noticed when performing the Validate CSM Health procedure &amp;ndash; the cmsdev test tool will exhibit the same hang when it tries to query BOS for a session list.</description>
    </item>
    <item>
      <title>Add Ceph OSDs</title>
      <link>/docs-csm/en-10/operations/utility_storage/add_ceph_osds/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/add_ceph_osds/</guid>
      <description>Add Ceph OSDs IMPORTANT: This document is addressing how to add an OSD when the OSD auto-discovery fails to add in new drives.&#xA;Check to ensure you have OSD auto-discovery enabled.&#xA;ncn-s00(1/2/3)# ceph orch ls osd NAME RUNNING REFRESHED AGE PLACEMENT IMAGE NAME IMAGE ID osd.all-available-devices 9/9 4m ago 3d * registry.local/ceph/ceph:v15.2.8 5553b0cb212c NOTE: Ceph version 15.2.x and newer will utilize the ceph orchestrator to add any available drives on the storage nodes to the OSD pool.</description>
    </item>
    <item>
      <title>Add UAN CAN IP Addresses to SLS</title>
      <link>/docs-csm/en-10/operations/system_layout_service/add_uan_can_ip_addresses_to_sls/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:39 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_layout_service/add_uan_can_ip_addresses_to_sls/</guid>
      <description>Add UAN CAN IP Addresses to SLS Add the Customer Access Network (CAN) IP addresses for User Access Nodes (UANs) to the IP address reservations in the System Layout Service (SLS). Adding these IP addresses will propagate the data needed for the Domain Name Service (DNS).&#xA;For more information on CAN IP addresses, refer to the Customer Access Network (CAN).&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Procedure Retrieve the SLS data for the CAN.</description>
    </item>
    <item>
      <title>Configure Prometheus Email Alert Notifications</title>
      <link>/docs-csm/en-10/operations/system_management_health/configure_prometheus_email_alert_notifications/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:39 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_management_health/configure_prometheus_email_alert_notifications/</guid>
      <description>Configure Prometheus Email Alert Notifications Configure an email alert notification for all Prometheus Postgres replication alerts: PostgresReplicationLagSMA, PostgresReplicationServices, PostgresqlFollowerReplicationLagSMA, and PostgresqlFollowerReplicationLagServices.&#xA;System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system&amp;rsquo;s fully qualified domain name (FQDN).&#xA;The FQDN can be found by running the following command on any Kubernetes NCN.&#xA;ncn-mw# kubectl get secret site-init -n loftsman -o jsonpath=&amp;#39;{.data.customizations\.yaml}&amp;#39; | base64 -d | yq r - spec.</description>
    </item>
    <item>
      <title>Manage Parameters with the scsd Service</title>
      <link>/docs-csm/en-10/operations/system_configuration_service/manage_parameters_with_the_scsd_service/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_configuration_service/manage_parameters_with_the_scsd_service/</guid>
      <description>Manage Parameters with the scsd Service The System Configuration Service commands below enable administrators to set various BMC and controller parameters. These parameters are controlled with the scsd command in the Cray CLI.&#xA;Retrieve Current Information from Targets Get the network protocol parameters (NTP/syslog server, SSH keys) and boot order for the targets in the payload. All fields are only applicable to Liquid Cooled controllers. Attempts to set them for Air Cooled BMCs will be ignored, and retrieving them for Air Cooled BMCs will return empty strings.</description>
    </item>
    <item>
      <title>Restore missing Spire metadata</title>
      <link>/docs-csm/en-10/operations/spire/restore_missing_spire_metadata/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/spire/restore_missing_spire_metadata/</guid>
      <description>Restore missing Spire metadata If the Boot Script Service (BSS) metadata server does contain the proper Spire metadata, then the computes will fail to boot. This is due to dracut pulling server data from the metadata during startup. To fix this issue, the spire-update-bss job needs to be rerun.&#xA;Error [ 557.513984] Apr 22 18:02:02 nid000004 dracut-initqueue[4177]: time=&amp;#34;2022-04-22T18:02:02Z&amp;#34; level=info msg=&amp;#34;SVID is not found. Starting node attestation&amp;#34; subsystem_name=attestor trust_domain_id=&amp;#34;spiffe://null&amp;#34; [ 557.514000] Apr 22 18:02:07 nid000004 dracut-initqueue[4194]: Agent is unavailable.</description>
    </item>
    <item>
      <title>Access the Keycloak User Management UI</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/access_the_keycloak_user_management_ui/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/access_the_keycloak_user_management_ui/</guid>
      <description>Access the Keycloak User Management UI This procedure can be used to access the interface to manage Keycloak users. Users can be added with this interface. See Create Internal User Accounts in the Keycloak Shasta Realm.&#xA;Prerequisites This procedure uses SYSTEM_DOMAIN_NAME as an example for the DNS name of the non-compute node (NCN). Replace this name with the actual NCN&amp;rsquo;s DNS name while executing this procedure. This procedure assumes that the password for the Keycloak admin account is known.</description>
    </item>
    <item>
      <title>Recreate StatefulSet Pods on Another Node</title>
      <link>/docs-csm/en-10/operations/resiliency/recreate_statefulset_pods_on_another_node/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/resiliency/recreate_statefulset_pods_on_another_node/</guid>
      <description>Recreate StatefulSet Pods on Another Node Some pods are members of StatefulSets, meaning that there is a very specific number of them, each likely running on a different node. Similar to DaemonSets, these pods will never be recreated on another node as long as they are sitting in a Terminating state.&#xA;Warning: This procedure should only be done for pods that are known to no longer be running. Corruption may occur if the worker node is still running when deleting the deleting the StatefulSet pod in a Terminating state.</description>
    </item>
    <item>
      <title>Ignore Nodes with CAPMC</title>
      <link>/docs-csm/en-10/operations/power_management/ignore_nodes_with_capmc/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/ignore_nodes_with_capmc/</guid>
      <description>Ignore Nodes with CAPMC Update the Cray Advanced Platform Monitoring and Control (CAPMC) ConfigMap to ignore non-compute nodes (NCNs) and ensure that they cannot be powered off or reset.&#xA;Modifying the CAPMC ConfigMap to ignore nodes can prevent them from accidentally being power cycled.&#xA;Nodes can also be locked with the Hardware State Manager (HSM) API. Refer to Lock and Unlock Management Nodes for more information.&#xA;Prerequisites This procedure requires administrative privileges.</description>
    </item>
    <item>
      <title>Nexus Configuration</title>
      <link>/docs-csm/en-10/operations/package_repository_management/nexus_configuration/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/package_repository_management/nexus_configuration/</guid>
      <description>Nexus Configuration Expect each product to create and use its own File type blob store. For example, the Cray System Management (CSM) product uses csm.&#xA;The default blob store is also available, but Cray products are discouraged from using it.&#xA;Repositories CSM creates the registry (format docker) and charts (format helm) repositories for managing container images and Helm charts across all Cray products. However, each product&amp;rsquo;s release may contain a number of RPM repositories that are added to Nexus.</description>
    </item>
    <item>
      <title>Identify Nodes and Update Metadata</title>
      <link>/docs-csm/en-10/operations/node_management/rebuild_ncns/identify_nodes_and_update_metadata/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/rebuild_ncns/identify_nodes_and_update_metadata/</guid>
      <description>Identify Nodes and Update Metadata Inspect and modify the JSON file This section applies to all node types. The commands in this section assume the variables from the prerequisites section have been set.&#xA;Step 1 - Generate the Boot Script Service (BSS) boot parameters JSON file Run the following commands from a node that has the Cray CLI initialized:&#xA;ncn-mw# cray bss bootparameters list --name $XNAME --format=json | jq .[] &amp;gt; ${XNAME}.</description>
    </item>
    <item>
      <title>Alpha Framework to Add, Remove, Replace, or Move NCNs</title>
      <link>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/add_remove_replace_ncns/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/add_remove_replace_ncns/</guid>
      <description>Alpha Framework to Add, Remove, Replace, or Move NCNs Add, remove, replace, or move non-compute nodes (NCNs). This applies to worker, storage, or master nodes. Use this procedure in the event that:&#xA;Worker, storage, or master nodes are being replaced and the MAC address is changing. Worker or storage nodes are being added. Worker, storage, or master nodes are being moved to a different cabinet. IMPORTANT: Always maintain at least two of the first three worker, storage, and master nodes when adding, removing, replacing, or moving NCNs.</description>
    </item>
    <item>
      <title>Management Network Access Port Configurations</title>
      <link>/docs-csm/en-10/operations/network/management_network/management_network_access_port_configurations/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/management_network/management_network_access_port_configurations/</guid>
      <description>Management Network Access Port Configurations Requirements Access to switches SHCD Configuration This configuration describes the edge port configuration. This configuration is in the NMN/HMN/Mountain-TDS Management Tab of the SHCD.&#xA;Typically, these are ports that are connected to iLOs (BMCs), gateway nodes, or compute nodes/CMM switches.&#xA;sw-leaf-001(config)# interface 1/1/35 no shutdown no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge This configuration describes the ports that go to the Node Management Network (NMN/VLAN2).</description>
    </item>
    <item>
      <title>MetalLB in BGP-Mode</title>
      <link>/docs-csm/en-10/operations/network/metallb_bgp/metallb_in_bgp-mode/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/metallb_bgp/metallb_in_bgp-mode/</guid>
      <description>MetalLB in BGP-Mode MetalLB is a component in Kubernetes that manages access to LoadBalancer services from outside the Kubernetes cluster. There are LoadBalancer services on the Node Management Network (NMN), Hardware Management Network (HMN), and Customer Access Network (CAN).&#xA;MetalLB can run in either Layer2-mode or BGP-mode for each address pool it manages. BGP-mode is used for the NMN, HMN, and CAN. This enables true load balancing (Layer2-mode does failover, not load balancing) and allows for a more robust layer 3 configuration for these networks.</description>
    </item>
    <item>
      <title>Connect to the CAN</title>
      <link>/docs-csm/en-10/operations/network/customer_access_network/connect_to_the_can/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:26 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/customer_access_network/connect_to_the_can/</guid>
      <description>Connect to the CAN How to connect to the CAN physically and via layer 3.&#xA;There are multiple ways to connect to the Customer Access Network (CAN), both physically and via a layer 3 connection.&#xA;Physical Connection to the CAN The physical connection to the CAN is made via the load balancer or the spine switches. The uplink connection from the system to the customer network is achieved by using the highest numbered port(s).</description>
    </item>
    <item>
      <title>Enable ncsd on UANs</title>
      <link>/docs-csm/en-10/operations/network/dns/enable_ncsd_on_uans/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:26 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/dns/enable_ncsd_on_uans/</guid>
      <description>Enable ncsd on UANs Configure User Access Nodes (UANs) to start the ncsd service at boot time.&#xA;The nscd service is not currently enabled by default and systemd does not start it at boot time. There are two ways to start nscd on UAN nodes: manually starting the service or enabling the service in the UAN image. While restarting nscd manually has to be performed each time the UAN is rebooted, enabling nscd in the image only has to be done once.</description>
    </item>
    <item>
      <title>External DNS</title>
      <link>/docs-csm/en-10/operations/network/external_dns/external_dns/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:26 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/external_dns/external_dns/</guid>
      <description>External DNS External DNS, along with the Customer Access Network (CAN), Border Gateway Protocol (BGP), and MetalLB, makes it simpler to access the HPE Cray EX API and system management services. Services are accessible directly from a laptop without needing to tunnel into a non-compute node (NCN) or override /etc/hosts settings. Some services may require a JSON Web Token (JWT) to access them, while others may require Keycloak to login using a DC LDAP password.</description>
    </item>
    <item>
      <title>Troubleshoot DHCP Issues</title>
      <link>/docs-csm/en-10/operations/network/dhcp/troubleshoot_dhcp_issues/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:26 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/dhcp/troubleshoot_dhcp_issues/</guid>
      <description>Troubleshoot DHCP Issues There are several things to check for when troubleshooting issues with Dynamic Host Configuration Protocol (DHCP) servers.&#xA;Incorrect DHCP IP addresses One of the most common issues is when the DHCP IP addresses are not matching in the Domain Name Service (DNS).&#xA;Check to make sure cray-dhcp is not running in Kubernetes:&#xA;ncn-mw# kubectl get pods -A | grep cray-dhcp Example output:&#xA;services cray-dhcp-5f8c8767db-hg6ch 1/1 Running 0 35d If the cray-dhcp pod is running, use the following command to shut down the pod:</description>
    </item>
    <item>
      <title>Connect to the HPE Cray EX Environment</title>
      <link>/docs-csm/en-10/operations/network/connect_to_the_hpe_cray_ex_environment/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/connect_to_the_hpe_cray_ex_environment/</guid>
      <description>Connect to the HPE Cray EX Environment The HPE Cray EX Management Network (SMNet) has multiple separate physical and logical links that are used to segregate traffic.&#xA;The diagram below shows the available connections from within the SMNet, as well as the connections to the customer network:&#xA;There are multiple ways to connect to the HPE Cray EX environment. The various methods are described in the following table:&#xA;Role Description Administrative External customer network connection to the worker node&amp;rsquo;s hardware management and administrative port Application node access External customer network connection to an Application Node Customer Access Network (CAN) Customer connection to the CAN gateway to access the HPE Cray EX CAN There are also several ways to physically connect to the nodes on the system.</description>
    </item>
    <item>
      <title>About Postgres</title>
      <link>/docs-csm/en-10/operations/kubernetes/about_postgres/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:23 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/about_postgres/</guid>
      <description>About Postgres The system uses PostgreSQL (known as Postgres) as a database solution. Postgres databases use SQL language to store and manage databases on the system.&#xA;To learn more about Postgres, see https://www.postgresql.org/docs/.&#xA;The Patroni tool can be used to manage and maintain information in a Postgres database. It handles tasks such as listing cluster members and the replication status, configuring and restarting databases, and more. For more information about this tool, refer to Troubleshoot Postgres Database.</description>
    </item>
    <item>
      <title>Build an Image Using IMS REST Service</title>
      <link>/docs-csm/en-10/operations/image_management/build_an_image_using_ims_rest_service/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:22 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/image_management/build_an_image_using_ims_rest_service/</guid>
      <description>Build an Image Using IMS REST Service Create an image root from an IMS recipe.&#xA;Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system.&#xA;System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCN) and include the following deployments:&#xA;cray-ims, the Image Management Service (IMS) cray-nexus, the Nexus repository manager service The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system.</description>
    </item>
    <item>
      <title>Add an NCN to the HSM Database</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/add_an_ncn_to_the_hsm_database/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/add_an_ncn_to_the_hsm_database/</guid>
      <description>Add an NCN to the HSM Database This procedure details how to customize the bare-metal non-compute node (NCN) on a system and add the NCN to the Hardware State Manager (HSM) database.&#xA;The examples in this procedure use ncn-w0003-nmn as the Customer Access Node (CAN). Use the correct CAN for the system.&#xA;Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI.</description>
    </item>
    <item>
      <title>Access Console Log Data Via the System Monitoring Framework (SMF)</title>
      <link>/docs-csm/en-10/operations/conman/access_console_log_data_via_the_system_monitoring_framework_smf/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/conman/access_console_log_data_via_the_system_monitoring_framework_smf/</guid>
      <description>Access Console Log Data Via the System Monitoring Framework (SMF) Console log data is collected by SMF and can be queried through the Kibana UI or Elasticsearch. Each line of the console logs are an individual record in the SMF database.&#xA;Prerequisites System domain name Procedure Prerequisites This procedure requires the Kibana service to be up and running on a non-compute node (NCN).&#xA;System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system&amp;rsquo;s fully qualified domain name (FQDN).</description>
    </item>
    <item>
      <title>FAS CLI</title>
      <link>/docs-csm/en-10/operations/firmware/fas_cli/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/firmware/fas_cli/</guid>
      <description>FAS CLI This section describes the basic capabilities of the Firmware Action Service (FAS) CLI commands. These commands can be used to manage firmware for system hardware supported by FAS. Refer to the prerequisites section before proceeding to any of the sections for the supported operations.&#xA;The following CLI operations are described:&#xA;Prerequisites Actions Execute an action Procedure Abort an action Procedure Describe an action Interpreting output Procedure Get high level summary Get details of action Get details of operation Snapshots Create a snapshot Procedure List snapshots Procedure View snapshots Procedure Update a firmware image Procedure FAS loader commands Loader status Load firmware from Nexus Load individual RPM or ZIP into FAS Display results of loader run Delete loader run data Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system.</description>
    </item>
    <item>
      <title>Ansible Inventory</title>
      <link>/docs-csm/en-10/operations/configuration_management/ansible_inventory/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:17 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/ansible_inventory/</guid>
      <description>Ansible Inventory The Configuration Framework Service (CFS) provides several options for targeting nodes or boot images for configuration by Ansible. The contents of the Ansible inventory determine which nodes are available for configuration in each CFS session and how default configuration values can be customized. For more information on what it means to define an inventory, see Specifying Hosts and Groups.&#xA;The following are the inventory options provided by CFS:</description>
    </item>
    <item>
      <title>Compute Rolling Upgrades</title>
      <link>/docs-csm/en-10/operations/compute_rolling_upgrades/compute_rolling_upgrades/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:17 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/compute_rolling_upgrades/compute_rolling_upgrades/</guid>
      <description>Compute Rolling Upgrades Note: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality.&#xA;The Compute Rolling Upgrade Service (CRUS) upgrades sets of compute nodes without requiring an entire set of nodes to be out of service at once. CRUS manages the workload management status of nodes, handling each of the following steps required to upgrade compute nodes:</description>
    </item>
    <item>
      <title>Compute Node Boot Issue Symptom Node Console or Logs Indicate that the Server Response has Timed Out</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/boot_issue_symptom_node_console_or_logs_indicate_that_the_server_response_has_timed_out/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:14 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/boot_issue_symptom_node_console_or_logs_indicate_that_the_server_response_has_timed_out/</guid>
      <description>Compute Node Boot Issue Symptom: Node Console or Logs Indicate that the Server Response has Timed Out If the TFTP request is able to access the TFTP service pod but is unable to find its way back to the node, it may be because the kernel is not tracking established TFTP connections.&#xA;Symptoms Problem detection Resolution Symptoms The following image is tcpdump data from within the TFTP pod. It shows what happens when the TFTP request cannot find a route back to the node that sent the request.</description>
    </item>
    <item>
      <title>Generate Temporary S3 Credentials</title>
      <link>/docs-csm/en-10/operations/artifact_management/generate_temporary_s3_credentials/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:14 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/artifact_management/generate_temporary_s3_credentials/</guid>
      <description>Generate Temporary S3 Credentials Cray provides a simple token service (STS) via the API gateway for administrators to generate temporary Simple Storage Service (S3) credentials for use with S3 buckets. Temporary S3 credentials are generated using either cURL or Python.&#xA;The generated S3 credentials will expire after one hour.&#xA;Retrieve temporary S3 credentials with cURL Retrieve temporary S3 credentials with Python Retrieve temporary S3 credentials with cURL Obtain a JWT token.</description>
    </item>
    <item>
      <title>Broker Mode UAI Management</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/broker_mode_uai_management/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:08 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/broker_mode_uai_management/</guid>
      <description>Broker Mode UAI Management A UAI broker is a special kind of UAI whose job is not to host users directly but to field attempts to reach a UAI, locate or create a UAI for the user making the attempt, and then pass the connection on to the correct UAI. Multiple UAI brokers can be created, each serving a UAI of a different class, making it possible to set up UAIs for varying workflows and environments as needed.</description>
    </item>
    <item>
      <title>Change Passwords and Credentials</title>
      <link>/docs-csm/en-10/operations/csm_product_management/change_passwords_and_credentials/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:08 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/csm_product_management/change_passwords_and_credentials/</guid>
      <description>Change Passwords and Credentials This is an overarching procedure to change all credentials managed by Cray System Management (CSM) in HPE Cray EX system to new values.&#xA;There are many passwords and credentials used in different contexts to manage the system. These can be changed as needed. Their initial settings are documented, so it is recommended to change them during or soon after a CSM software installation.&#xA;Prerequisites Review procedures in Manage System Passwords.</description>
    </item>
    <item>
      <title>CSM Overview</title>
      <link>/docs-csm/en-10/introduction/csm_overview/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:07 +0000</pubDate>
      <guid>/docs-csm/en-10/introduction/csm_overview/</guid>
      <description>CSM Overview This CSM Overview describes the Cray System Management ecosystem with the hardware, software, network, and access to these services and components.&#xA;The CSM installation prepares and deploys a distributed system across a group of management nodes organized into a Kubernetes cluster which uses Ceph for utility storage. These nodes perform their function as Kubernetes master nodes, Kubernetes worker nodes, or utility storage nodes with the Ceph storage.&#xA;System services on these nodes are provided as containerized micro-services packaged for deployment as helm charts.</description>
    </item>
    <item>
      <title>SHCD HMN Tab/HMN Connections Rules</title>
      <link>/docs-csm/en-10/install/shcd_hmn_connections_rules/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:06 +0000</pubDate>
      <guid>/docs-csm/en-10/install/shcd_hmn_connections_rules/</guid>
      <description>SHCD HMN Tab/HMN Connections Rules Introduction Compute node Dense four node chassis - Gigabyte or Intel chassis Single node chassis - Apollo 6500 XL675D Dual node chassis - Apollo 6500 XL645D Chassis Management Controller (CMC) Management node Master Worker Storage Application node Single node chassis Building component names (xnames) for nodes in a single application node chassis Dual node chassis Building component names (xnames) for nodes in a dual application node chassis Columbia Slingshot switch PDU cabinet controller Cooling door Management switches Introduction The HMN tab of the SHCD describes the air-cooled hardware present in the system and how these devices are connected to the Hardware Management Network (HMN).</description>
    </item>
    <item>
      <title>Aruba SNMP Known Issue</title>
      <link>/docs-csm/en-10/install/aruba_snmp_known_issue_10_06_0010/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:03 +0000</pubDate>
      <guid>/docs-csm/en-10/install/aruba_snmp_known_issue_10_06_0010/</guid>
      <description>Aruba SNMP Known Issue Affected Devices 8320/8325/8360 Aruba CX&#xA;Aruba Defect CR 153440&#xA;Aruba Public Documentation/Images Aruba support portal:&#xA;https://asp.arubanetworks.com/&#xA;Aruba 8325 release notes 10.06.0120:&#xA;https://www.arubanetworks.com/techdocs/AOS-CX/10.06/RN/5200-8179.pdf&#xA;Aruba 8360 release notes 10.06.0120:&#xA;https://www.arubanetworks.com/techdocs/AOS-CX/10.06/RN/5200-8180.pdf&#xA;Where the Issue May Occur During Install During initial network discovery of Nodes, SNMP may not accurately report MAC-address from Aruba Leaf/Spine switches. This will lead to a situation where not all connected devices are discovered as expected. Further troubleshooting would show that the SNMP walk output would not match &amp;lsquo;show mac-address-table&amp;rsquo; command output from the switch.</description>
    </item>
    <item>
      <title>cloud-init Basecamp Configuration</title>
      <link>/docs-csm/en-10/background/cloud-init_basecamp_configuration/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:00 +0000</pubDate>
      <guid>/docs-csm/en-10/background/cloud-init_basecamp_configuration/</guid>
      <description>cloud-init Basecamp Configuration Metal Basecamp is a cloud-init DataSource available on the LiveCD. Basecamp&amp;rsquo;s configuration file offers many inputs for various cloud-init scripts baked into the NCN images.&#xA;This page details what those settings are.&#xA;Basecamp Config Files Purging Basecamp CAN CEPH Certificate Authority RADOS Gateway Wiping DNS Resolution Configuration Static Fallback Kubernetes NTP Node Auditing Generally these settings are determined by the cray-site-init tool. See csi config --help for more information.</description>
    </item>
    <item>
      <title>Cray System Management (CSM) - Release Notes</title>
      <link>/docs-csm/en-10/release_notes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:00 +0000</pubDate>
      <guid>/docs-csm/en-10/release_notes/</guid>
      <description>Cray System Management (CSM) - Release Notes CSM 1.0.11 The following lists enumerate the improvements and enhancements since CSM 1.0.10&#xA;New Functionality Backport current cabinet expansion procedure from CSM 1.2 into the CSM 1.0 docs Bug Fixes SECURITY: CVE-2022-0185: Linux kernel buffer overflow/container escape SECURITY: CVE-2021-4034: pwnkit: Local Privilege Escalation in polkit&amp;rsquo;s pkexec SECURITY: Address log4j vulnerabilities with regards to kafka in the CSM-1.0.11 patch SECURITY: Update strimzi operator 0.15.0 to use patched kafka images Bug Fix: csm upgrade incorrectly records CPS nodes Bug Fix: update_bss_metadata.</description>
    </item>
    <item>
      <title>Stage 1 - Ceph Image Upgrade</title>
      <link>/docs-csm/en-10/upgrade/1.0.11/stage_1/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:46 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.11/stage_1/</guid>
      <description>Stage 1 - Ceph Image Upgrade Procedure IMPORTANT:&#xA;Reminder: Before running any upgrade scripts, be sure the Cray CLI output format is reset to default by running the following command:&#xA;ncn# unset CRAY_FORMAT In order for nodes to properly PXE boot, Border Gateway Protocol (BGP) must be healthy. Before proceeding, check the status of BGP as described in the Check BGP Status and Reset Sessions procedure.&#xA;Run ncn-upgrade-ceph-nodes.sh for ncn-s001. Follow output of the script carefully.</description>
    </item>
    <item>
      <title>Stage 1 - Ceph upgrade from Nautilus (14.2.x) to Octopus (15.2.x)</title>
      <link>/docs-csm/en-10/upgrade/1.0.1/stage_1/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:44 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.1/stage_1/</guid>
      <description>Stage 1 - Ceph upgrade from Nautilus (14.2.x) to Octopus (15.2.x) IMPORTANT:&#xA;Reminder: Before running any upgrade scripts, be sure the Cray CLI output format is reset to default by running the following command:&#xA;ncn# unset CRAY_FORMAT Stage 1.1 Run ncn-upgrade-ceph-initial.sh for ncn-s001. Follow output of the script carefully. The script will pause for manual interaction.&#xA;ncn-m001# /usr/share/doc/csm/upgrade/1.0.1/scripts/upgrade/ncn-upgrade-ceph-initial.sh ncn-s001 Repeat the previous step for each other storage node, one at a time.</description>
    </item>
    <item>
      <title>Troubleshoot Kubernetes Master or Worker node in NotReady state</title>
      <link>/docs-csm/en-10/troubleshooting/kubernetes/troubleshoot_kubernetes_node_notready/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:44 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/kubernetes/troubleshoot_kubernetes_node_notready/</guid>
      <description>Troubleshoot Kubernetes Master or Worker node in NotReady state Use this procedure to check if a Kubernetes master or worker node is in a NotReady state.&#xA;Identify the node in question Identify the node in NotReady state.&#xA;ncn-mw# kubectl get nodes Example output:&#xA;NAME STATUS ROLES AGE VERSION ncn-m001 Ready master 27h v1.19.9 ncn-m002 Ready master 19h v1.19.9 ncn-m003 Ready master 18h v1.19.9 ncn-w001 NotReady &amp;lt;none&amp;gt; 36h v1.19.9 ncn-w002 Ready &amp;lt;none&amp;gt; 36h v1.</description>
    </item>
    <item>
      <title>Interpreting HMS Health Check Results</title>
      <link>/docs-csm/en-10/troubleshooting/interpreting_hms_health_check_results/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:42 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/interpreting_hms_health_check_results/</guid>
      <description>Interpreting HMS Health Check Results Table of contents Introduction HMS smoke tests HMS functional tests Additional troubleshooting smd_discovery_status_test_ncn-smoke.sh HTTPsGetFailed ChildVerificationFailed DiscoveryStarted Install blocking vs. Non-blocking failures Known issues Warning flags incorrectly set in HSM for Mountain BMCs BMCs set to On state in HSM ComponentEndpoints of Redfish subtype AuxiliaryController in HSM Custom Roles and SubRoles for Components in HSM Introduction This document describes how to interpret the results of the HMS health check scripts and techniques for troubleshooting when failures occur.</description>
    </item>
    <item>
      <title>Multiple Console Node Pods on the Same Worker</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/multiple_console_node_pods_on_the_same_worker/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:42 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/multiple_console_node_pods_on_the_same_worker/</guid>
      <description>Multiple Console Node Pods on the Same Worker In versions before CSM v1.3.0, there is no anti-affinity specified for the cray-console-node pods. This leads to the possibility of several pods running on the same worker node. This can be inconvenient during worker reboot operations and can reduce service reliability.&#xA;Manually edit deployment Pod scheduling behavior Manually edit deployment This procedure implements anti-affinity Kubernetes scheduling in versions prior to CSM v1.3.0 by manually editing the cray-console-node deployment.</description>
    </item>
    <item>
      <title>Adjust Ceph Pool Quotas</title>
      <link>/docs-csm/en-10/operations/utility_storage/adjust_ceph_pool_quotas/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/adjust_ceph_pool_quotas/</guid>
      <description>Adjust Ceph Pool Quotas Ceph pools are used for storing data. Use this procedure to set the Ceph pool quotas to determine the wanted number of bytes per pool. The smf Ceph pool now has replication factor of two.&#xA;Resolve Ceph health issues caused by a pool reaching its quota.&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Limitations Currently, only smf includes a quota.&#xA;Procedure Log in as root on ncn-m001.</description>
    </item>
    <item>
      <title>Create a Backup of the SLS Postgres Database</title>
      <link>/docs-csm/en-10/operations/system_layout_service/create_a_backup_of_the_sls_postgres_database/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:39 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_layout_service/create_a_backup_of_the_sls_postgres_database/</guid>
      <description>Create a Backup of the SLS Postgres Database Perform a manual backup of the contents of the SLS Postgres database. This backup can be used to restore the contents of the SLS Postgres database at a later point in time using the Restoring SLS Postgres cluster from backup procedure.&#xA;Prerequisites Healthy SLS Postgres Cluster. Use patronictl list on the SLS Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:</description>
    </item>
    <item>
      <title>Grafana Dashboards by Component</title>
      <link>/docs-csm/en-10/operations/system_management_health/grafana_dashboards_by_component/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:39 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_management_health/grafana_dashboards_by_component/</guid>
      <description>Grafana Dashboards by Component This document show the dashboards that are available for each component in Grafana.&#xA;A Grafana dashboard is a powerful open source analytical and visualization tool that consists of multiple individual panels arranged in a grid. The panels interact with configured data sources, including the following:&#xA;AWS CloudWatch Prometheus Ceph Cluster Host Overview Details MDS Performance OSD Overview Device Details Pool Overview Details RBD Overview RGW Overview Instance Details CoreDNS CoreDNS ETCD Main Clusters Istio Mesh Performance Pilot Service Workload Kea DHCP Kubernetes API server Compute Resources Compute Resources / Cluster Compute Resources / Namespace (Pods) Compute Resources / Namespace (Workloads) Compute Resources / Node (Pods) Compute Resources / Pod Compute Resources / Workload Controller Manager kubelet Networking Networking / Cluster Networking / Namespace (Pods) Networking / Namespace (Workload) Networking / Pod Networking / Workload Persistent Volumes Proxy Scheduler StatefulSets Nodes Exporter Full Main Postgres PostgreSQL Statistics Prometheus Prometheus Use Method Cluster Node </description>
    </item>
    <item>
      <title>Restore Spire Postgres without an Existing Backup</title>
      <link>/docs-csm/en-10/operations/spire/restore_spire_postgres_without_a_backup/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/spire/restore_spire_postgres_without_a_backup/</guid>
      <description>Restore Spire Postgres without an Existing Backup Reinstall the Spire Helm chart in the event that spire-postgres databases cannot be restored from a backup.&#xA;Uninstall Spire Uninstall the Spire Helm chart.&#xA;ncn-mw# helm uninstall -n spire spire Wait for the pods in the spire namespace to terminate. Once that is done, remove the spire-data-server PVCs.&#xA;ncn-mw# kubectl get pvc -n spire | grep spire-data-spire-server | awk &amp;#39;{print $1}&amp;#39; | xargs kubectl delete -n spire pvc Disable spire-agent on all of the Kubernetes NCNs (all worker nodes and master nodes) and delete the join data.</description>
    </item>
    <item>
      <title>Set BMC Credentials</title>
      <link>/docs-csm/en-10/operations/system_configuration_service/set_bmc_credentials/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_configuration_service/set_bmc_credentials/</guid>
      <description>Set BMC Credentials Use the System Configuration Service (SCSD) to set the BMCs credentials to unique values, or set them all to the same value. Redfish BMCs get installed into the system with default credentials. Once the machine is shipped, the Redfish credentials must be changed on all BMCs. This is done using System Configuration Service (SCSD) through the Cray CLI.&#xA;Important: If the credentials for other devices need to be changed, refer to the following device-specific credential changing procedures:</description>
    </item>
    <item>
      <title>Add LDAP User Federation</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/add_ldap_user_federation/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:35 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/add_ldap_user_federation/</guid>
      <description>Add LDAP User Federation Add LDAP user federation using the Keycloak localization tool.&#xA;Prerequisites System domain name Procedure Prerequisites LDAP user federation is not currently configured in Keycloak. For example, if it was not configured in Keycloak when the system was initially installed or the LDAP user federation was removed.&#xA;System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system&amp;rsquo;s fully qualified domain name (FQDN).</description>
    </item>
    <item>
      <title>Resilience of System Management Services</title>
      <link>/docs-csm/en-10/operations/resiliency/resilience_of_system_management_services/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/resiliency/resilience_of_system_management_services/</guid>
      <description>Resilience of System Management Services HPE Cray EX systems are designed so that system management services (SMS) are fully resilient and that there is no single point of failure. The design of the system allows for resiliency in the following ways:&#xA;Three non-compute nodes (NCNs) are configured as Kubernetes master nodes. When one master goes down, operations (such as jobs running across compute nodes) are expected to continue. At least three utility storage nodes provide persistent storage for the services running on the Kubernetes management nodes.</description>
    </item>
    <item>
      <title>Liquid Cooled Node Power Management</title>
      <link>/docs-csm/en-10/operations/power_management/liquid_cooled_node_card_power_management/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/liquid_cooled_node_card_power_management/</guid>
      <description>Liquid Cooled Node Power Management Liquid Cooled AMD EPYC compute blade node card power capabilities and limits.&#xA;Liquid Cooled cabinet node card power features are supported by the node controller (nC) firmware and CPU vendor. The nC exposes the power control API for each node via the node&amp;rsquo;s Redfish Control schema. Out-of-band power management data is produced and collected by the nC hardware and firmware. This data can be published to a collector using the Redfish EventService, or retrieved on-demand from the Redfish ChassisSensors resource.</description>
    </item>
    <item>
      <title>Nexus Deployment</title>
      <link>/docs-csm/en-10/operations/package_repository_management/nexus_deployment/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/package_repository_management/nexus_deployment/</guid>
      <description>Nexus Deployment Nexus is deployed with the cray-nexus chart to the nexus namespace as part of the Cray System Management (CSM) release. Nexus is deployed after critical platform services are up and running. Product installers configure and populate Nexus blob stores and repositories using the cray-nexus-setup container image. As a result, there is no singular product that provides all Nexus repositories or assets; instead, individual products must be installed. However, CSM configures the charts Helm repository and the registry Docker repository, which all products may use.</description>
    </item>
    <item>
      <title>6.2. Validate Master Node</title>
      <link>/docs-csm/en-10/operations/node_management/rebuild_ncns/post_rebuild_master_node_validation/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/rebuild_ncns/post_rebuild_master_node_validation/</guid>
      <description>6.2. Validate Master Node Validate the master node rebuilt successfully.&#xA;Skip this section if a worker or storage node was rebuilt.&#xA;Verify the new node is in the cluster.&#xA;Run the following command from any master or worker node that is already in the cluster. It is helpful to run this command several times to watch for the newly rebuilt node to join the cluster. This should occur within 10 to 20 minutes.</description>
    </item>
    <item>
      <title>Add Switch Configuration for NCN</title>
      <link>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/add_switch_config/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/add_switch_config/</guid>
      <description>Add Switch Configuration for NCN Description Update the network switches for the NCN that is being added.&#xA;Procedure Update Networking to Add NCN Example ncn-w004 IP data:&#xA;10.102.4.15 ncn-w004.can 10.254.1.22 ncn-w004.hmn 10.1.1.11 ncn-w004.mtl 10.252.1.13 ncn-w004.nmn ncn-w004 10.254.1.21 ncn-w004-mgmt Mellanox and Dell Mellanox and Dell Spine/Agg Switch Updates Spine/Agg edge port configuration&#xA;Mellanox and Dell Spine BGP Updates The configuration will be the same across both switches.&#xA;This is for workers only.</description>
    </item>
    <item>
      <title>Management Network CAN Setup</title>
      <link>/docs-csm/en-10/operations/network/management_network/management_network_can_setup/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/management_network/management_network_can_setup/</guid>
      <description>Management Network CAN Setup Access from the customer site to the system over shared networks is known as the Customer Access Network (CAN).&#xA;Requirements Access to switches SHCD Configuration The CAN configuration is highly dependent on customer requirements and may not meet the specifications below.&#xA;To access the HPE Cray EX nodes and services from the customer network, there is minimal configuration needed on the spine switch and the customer switch connected upstream from the spine switch to allow the customer_access_network subnet to be routed to the HPE Cray EX system.</description>
    </item>
    <item>
      <title>MetalLB in BGP-Mode Configuration</title>
      <link>/docs-csm/en-10/operations/network/metallb_bgp/metallb_in_bgp-mode_configuration/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/metallb_bgp/metallb_in_bgp-mode_configuration/</guid>
      <description>MetalLB in BGP-Mode Configuration MetalLB in BGP-mode provides a more robust configuration for the Node Management Network (NMN), Hardware Management Network (HMN), and Customer Access Network (CAN). This configuration is generated from the csi config init input values. BGP-mode is enabled by updating the protocols in these configuration files.&#xA;MetalLB Peer Configuration The content for metallb_bgp_peers is generated by the csi config init command. In addition to the MetalLB configuration, there is configuration needed on the spine switches to set up the BGP router on these switches.</description>
    </item>
    <item>
      <title>Customer Access Network</title>
      <link>/docs-csm/en-10/operations/network/customer_access_network/customer_access_network_can/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:26 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/customer_access_network/customer_access_network_can/</guid>
      <description>Customer Access Network The Customer Access Network (CAN) provides access from outside the customer network to services, non-compute nodes (NCNs), and User Access Nodes (UANs) in the system. This allows for the following:&#xA;Clients outside of the system: Log in to each of the NCNs and UANs. Access web UIs within the system (e.g. Prometheus, Grafana, and more). Access the Rest APIs within the system. Access a DNS server within the system for resolution of names for the webUI and REST API services.</description>
    </item>
    <item>
      <title>External DNS Failing to Discover Services Workaround</title>
      <link>/docs-csm/en-10/operations/network/external_dns/external_dns_failing_to_discover_services_workaround/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:26 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/external_dns/external_dns_failing_to_discover_services_workaround/</guid>
      <description>External DNS Failing to Discover Services Workaround Many external DNS issues can be worked around by directly connecting to the desired backend service. This can circumvent authentication and authorization protections, but it may be necessary to access specific services when mitigating critical issues.&#xA;Istio&amp;rsquo;s ingress gateway uses Gateway and VirtualService objects to configure how traffic is routed to backend services. Currently, there is only one gateway supporting the Customer Access Network (CAN), which is services/services-gateway.</description>
    </item>
    <item>
      <title>Manage the DNS Unbound Resolver</title>
      <link>/docs-csm/en-10/operations/network/dns/manage_the_dns_unbound_resolver/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:26 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/dns/manage_the_dns_unbound_resolver/</guid>
      <description>Manage the DNS Unbound Resolver The unbound DNS instance is used to resolve names for the physical equipment on the management networks within the system, such as NCNs, UANs, switches, and compute nodes. This instance is accessible only within the HPE Cray EX system.&#xA;Check the status of the cray-dns-unbound pods Unbound logs View manager (DNS Helper) logs Restart Unbound Clear bad data in the Unbound ConfigMap Check the status of the cray-dns-unbound pods Check the status of the pods:</description>
    </item>
    <item>
      <title>Default IP Address Ranges</title>
      <link>/docs-csm/en-10/operations/network/default_ip_address_ranges/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/default_ip_address_ranges/</guid>
      <description>Default IP Address Ranges The initial installation of the system creates default networks with default settings and with no external exposure. These IP address default ranges ensure that no nodes in the system attempt to use the same IP address as a Kubernetes service or pod, which would result in undefined behavior that is extremely difficult to reproduce or debug.&#xA;The following table shows the default IP address ranges:&#xA;Network IP Address Range Kubernetes service network 10.</description>
    </item>
    <item>
      <title>About etcd</title>
      <link>/docs-csm/en-10/operations/kubernetes/about_etcd/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:23 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/about_etcd/</guid>
      <description>About etcd The system uses etcd for storing all of its cluster data. It is an open source database that is excellent for maintaining the state of Kubernetes. Failures in the etcd cluster at the heart of Kubernetes will cause a failure of Kubernetes. To mitigate this risk, the system is deployed with etcd on dedicated disks and with a specific configuration to optimize Kubernetes workloads. The system also provides additional etcd cluster(s) as necessary to help maintain an operational state of services.</description>
    </item>
    <item>
      <title>Convert TGZ Archives to SquashFS Images</title>
      <link>/docs-csm/en-10/operations/image_management/convert_tgz_archives_to_squashfs_images/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:22 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/image_management/convert_tgz_archives_to_squashfs_images/</guid>
      <description>Convert TGZ Archives to SquashFS Images If customizing a pre-built image root archive compressed as a .txz or other non-SquashFS format, convert the image root to SquashFS and upload the SquashFS archive to S3.&#xA;The steps in this section only apply if the image root is not in SquashFS format.&#xA;Prerequisites There is a pre-built image that is not currently in SquashFS format.&#xA;Procedure Locate the image root to be converted to SquashFS.</description>
    </item>
    <item>
      <title>Component Group Members</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/component_group_members/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/component_group_members/</guid>
      <description>Component Group Members The members object in the group definition has additional actions available for managing the members after the group has been created.&#xA;The following is an example of group members:&#xA;{ &amp;#34;ids&amp;#34; : [ &amp;#34;x0c0s0b0n0&amp;#34;,&amp;#34;x0c0s0b0n1&amp;#34;,&amp;#34;x0c0s0b1n0&amp;#34; ] } Retrieve Group Members Retrieve just the members array for a group:&#xA;ncn-m# cray hsm groups members list GROUP_LABEL Retrieve only the members of a group that are also in a specific partition:</description>
    </item>
    <item>
      <title>ConMan</title>
      <link>/docs-csm/en-10/operations/conman/conman/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/conman/conman/</guid>
      <description>ConMan ConMan is a tool used for connecting to remote consoles and collecting console logs. These node logs can then be used for various administrative purposes, such as troubleshooting node boot issues.&#xA;ConMan runs on the system as a containerized service. It runs in a set of Docker containers within Kubernetes pods named cray-console-operator and cray-console-node. ConMan can be configured to encrypt usernames and passwords. Node console logs are stored locally within the cray-console-operator pod in the /var/log/conman/ directory, as well as being collected by the System Monitoring Framework (SMF).</description>
    </item>
    <item>
      <title>FAS Filters</title>
      <link>/docs-csm/en-10/operations/firmware/fas_filters/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/firmware/fas_filters/</guid>
      <description>FAS Filters FAS uses five primary filters for actions and snapshots to determine what operations to create. The filters are listed below:&#xA;Selection Filters - Determine what operations will be created. The following selection filters are available: stateComponentFilter targetFilter inventoryHardwareFilter imageFilter Command Filters - Determine how the operations will be executed. The following command filters are available: command All filters are logically connected with AND logic. Only the stateComponentFilter, targetFilter, and inventoryHardwareFilter are used for snapshots.</description>
    </item>
    <item>
      <title>Automatic Session Deletion with sessionTTL</title>
      <link>/docs-csm/en-10/operations/configuration_management/automatic_session_deletion_with_sessionttl/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:17 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/automatic_session_deletion_with_sessionttl/</guid>
      <description>Automatic Session Deletion with sessionTTL By default, the Configuration Framework Service (CFS) will delete completed CFS sessions whose start date was more than seven days prior. Kubernetes jobs associated with these sessions will also be deleted as part of this process. This is done to ensure that CFS sessions do not accumulate and eventually adversely affect the performance of the Kubernetes cluster.&#xA;For larger systems or systems that do frequent reboots of nodes that are configured with CFS sessions, this setting may need to be reduced.</description>
    </item>
    <item>
      <title>Troubleshoot Nodes Failing to Upgrade in a CRUS Session</title>
      <link>/docs-csm/en-10/operations/compute_rolling_upgrades/troubleshoot_nodes_failing_to_upgrade_in_a_crus_session/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:17 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/compute_rolling_upgrades/troubleshoot_nodes_failing_to_upgrade_in_a_crus_session/</guid>
      <description>Troubleshoot Nodes Failing to Upgrade in a CRUS Session Note: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality.&#xA;Troubleshoot compute nodes failing to upgrade during a Compute Rolling Upgrade Service (CRUS) session and rerun the session on the failed nodes.&#xA;When nodes are marked as failed they are added to the failed node group associated with the upgrade session, and the nodes are marked as down in the workload manager (WLM).</description>
    </item>
    <item>
      <title>Boot Issue Symptom Node HSN Interface Does Not Appear or Show Detected Links Detected</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/boot_issue_symptom_node_hsn_interface_does_not_appear_or_shows_no_link_detected/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:14 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/boot_issue_symptom_node_hsn_interface_does_not_appear_or_shows_no_link_detected/</guid>
      <description>Boot Issue Symptom: Node HSN Interface Does Not Appear or Show Detected Links Detected A node may fail to boot if the HSN interface is experiencing issues, or if it is not able to detect any links.&#xA;Symptom The node&amp;rsquo;s HSN interface does not appear in the output of the ip addr command or the output of the ethtool interface command shows no link detected.&#xA;Resolution Reseat the node&amp;rsquo;s PCIe card.</description>
    </item>
    <item>
      <title>Manage Artifacts with the Cray CLI</title>
      <link>/docs-csm/en-10/operations/artifact_management/manage_artifacts_with_the_cray_cli/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:14 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/artifact_management/manage_artifacts_with_the_cray_cli/</guid>
      <description>Manage Artifacts with the Cray CLI The artifacts (objects) available for use on the system are created and managed with the Cray CLI. The cray artifacts command provides the ability to manage any given artifact. The Cray CLI automatically authenticates users and provides Simple Storage Service (S3) credentials.&#xA;All operations with the cray artifacts command assume that the user has already been authenticated. If the user has not been authenticated with the Cray CLI, run the following command and enter the appropriate credentials:</description>
    </item>
    <item>
      <title>Configure End-User UAI Classes for Broker Mode</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/configure_end-user_uai_classes_for_broker_mode/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:09 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/configure_end-user_uai_classes_for_broker_mode/</guid>
      <description>Configure End-User UAI Classes for Broker Mode Each UAI broker will create and manage a single class of end-user UAIs. Setting up UAI classes for this is similar to Configure a Default UAI Class for Legacy Mode with the following exceptions:&#xA;The public_ip flag for brokered UAI classes should be set to false The default flag for brokered UAI classes may be set to true or false but should, most likely, be set to false.</description>
    </item>
    <item>
      <title>Configure Keycloak Account</title>
      <link>/docs-csm/en-10/operations/csm_product_management/configure_keycloak_account/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:08 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/csm_product_management/configure_keycloak_account/</guid>
      <description>Configure Keycloak Account Installation of CSM software includes a default account for administrative access to keycloak.&#xA;Depending on choices made during the installation, there may be a federated connection to an external Identity Provider (IdP), such as an LDAP or AD server, which enables the use of external accounts in keycloak.&#xA;However, if the external accounts are not available, then an &amp;ldquo;internal user account&amp;rdquo; could be created in keycloak. Having a usable account in keycloak with administrative authorization enables the use of the cray CLI for many administrative commands, such as those used to Validate CSM Health and general operation of the management services via the API gateway.</description>
    </item>
    <item>
      <title>Differences from Previous Release</title>
      <link>/docs-csm/en-10/introduction/differences/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:07 +0000</pubDate>
      <guid>/docs-csm/en-10/introduction/differences/</guid>
      <description>Differences from Previous Release The most noteworthy changes since the previous release are described here.&#xA;New features Deprecated features Removed features Other changes New features The following features are new in this release:&#xA;Scaling improvements for larger systems to the following services: BOS CAPMC FAS New hardware supported in this release: Compute nodes Milan-Based Grizzly Peak with A100 40 GB GPU Milan-Based Windom Liquid Cooled System Rome-Based HPE Apollo 6500 XL675d Gen10+ with A100 40 GB GPU Rome-Based HPE Apollo 6500 XL645d Gen10+ with A100 40 GB GPU User Access Nodes (UANs) Milan-Based HPE DL 385(v2) Gen10+ Rome-Based HPE DL 385(v1) Gen10 Node consoles are now managed by cray-console-node which is based on ConMan HSM now has a v2 REST API Deprecated features The following features are no longer supported and are planned to be removed in a future release:</description>
    </item>
    <item>
      <title>Switch PXE Boot from Onboard NIC to PCIe</title>
      <link>/docs-csm/en-10/install/switch_pxe_boot_from_onboard_nic_to_pcie/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:07 +0000</pubDate>
      <guid>/docs-csm/en-10/install/switch_pxe_boot_from_onboard_nic_to_pcie/</guid>
      <description>Switch PXE Boot from Onboard NIC to PCIe This section details how to migrate NCNs from using their onboard NICs for PXE booting to booting over the PCIe cards.&#xA;Switch PXE Boot from Onboard NIC to PCIe Enabling UEFI PXE Mode Mellanox Print Current UEFI and SR-IOV State Setting Expected Values High-Speed Network Obtaining Mellanox Tools QLogic FastLinq Kernel Modules Disabling or Removing On-Board Connections This applies to Newer systems (Spring 2020 or newer) where onboard NICs are still used.</description>
    </item>
    <item>
      <title>Boot LiveCD Virtual ISO</title>
      <link>/docs-csm/en-10/install/boot_livecd_virtual_iso/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:03 +0000</pubDate>
      <guid>/docs-csm/en-10/install/boot_livecd_virtual_iso/</guid>
      <description>Boot LiveCD Virtual ISO This page will walk-through booting the LiveCD .iso file directly onto a BMC.&#xA;Topics: Boot LiveCD Virtual ISO Topics: Details Prerequisites BMCs&amp;rsquo; Virtual Mounts HPE iLO BMCs Gigabyte BMCs Configure Backing up the Overlay COW FS Restoring from an Overlay COW FS Backup Details Prerequisites A Cray Pre-Install Toolkit ISO is required for this process. This ISO can be obtained from:&#xA;The Cray Pre-Install Toolkit ISO included in a CSM release tar file.</description>
    </item>
    <item>
      <title>Cray Site Init Files</title>
      <link>/docs-csm/en-10/background/cray_site_init_files/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:00 +0000</pubDate>
      <guid>/docs-csm/en-10/background/cray_site_init_files/</guid>
      <description>Cray Site Init Files This page describes administrative knowledge around the pre-config files to csi or the output files from csi.&#xA;Information for collecting certain files starts in Configuration Payload&#xA;application_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv Topics: Save-File / Avoiding Parameters Details Save-File / Avoiding Parameters A system_config.yaml file may be provided by the administrator that will omit the need for specifying parameters on the command line.&#xA;This file is dumped in the generated configuration after every csi config init call.</description>
    </item>
    <item>
      <title>Stage 2 - Kubernetes Node Image Upgrade</title>
      <link>/docs-csm/en-10/upgrade/1.0.11/stage_2/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:46 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.11/stage_2/</guid>
      <description>Stage 2 - Kubernetes Node Image Upgrade NOTE: During the CSM-1.0.1 install the LiveCD containing the initial install files for this system should have been unmounted from the master node when rebooting into the Kubernetes cluster. The scripts run in this section will also attempt to unmount/eject it if found to ensure the USB stick does not get erased.&#xA;IMPORTANT:&#xA;Reminder: Before running any upgrade scripts, be sure the Cray CLI output format is reset to default by running the following command:</description>
    </item>
    <item>
      <title>Stage 2 - Ceph image upgrade</title>
      <link>/docs-csm/en-10/upgrade/1.0.1/stage_2/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:44 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.1/stage_2/</guid>
      <description>Stage 2 - Ceph image upgrade IMPORTANT:&#xA;Reminder: Before running any upgrade scripts, be sure the Cray CLI output format is reset to default by running the following command:&#xA;ncn# unset CRAY_FORMAT In order for nodes to properly PXE boot, Border Gateway Protocol (BGP) must be healthy. Before proceeding, check the status of BGP as described in the Check BGP Status and Reset Sessions procedure.&#xA;Run ncn-upgrade-ceph-nodes.sh for ncn-s001. Follow output of the script carefully.</description>
    </item>
    <item>
      <title>Troubleshoot Liveliness or Readiness Probe Failures</title>
      <link>/docs-csm/en-10/troubleshooting/kubernetes/troubleshoot_liveliness_readiness_probe_failures/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:44 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/kubernetes/troubleshoot_liveliness_readiness_probe_failures/</guid>
      <description>Troubleshoot Liveliness or Readiness Probe Failures Identify and troubleshoot Readiness or Liveliness probes that report services as unhealthy intermittently.&#xA;This is a known issue and can be classified into two categories, connection refused and client timeout. The commands in this procedure assume the user is logged into either a master or worker non-compute node (NCN).&#xA;Troubleshoot a refused connection Refused connection - symptom Refused connection - procedure Troubleshoot a client timeout Client timeout - symptom Client timeout - procedure Next steps Troubleshoot a refused connection Refused connection - symptom The symptom of this problem is a connection refused message in the event log.</description>
    </item>
    <item>
      <title>SLS Not Working During Node Rebuild</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/sls_not_working_during_node_rebuild/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:42 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/sls_not_working_during_node_rebuild/</guid>
      <description>SLS Not Working During Node Rebuild During some node rebuilds, the SLS Postgres database gets into a bad state, causing SLS to become unhealthy. This page outlines how to detect if this has happened and provides a remediation procedure.&#xA;Detection This procedure can be run on any master or worker NCN (unless it is the node being rebuilt).&#xA;Get a token to use for API requests to SLS.&#xA;ncn-mw# TOKEN=$(\ set -o pipefail secret=`kubectl get secrets admin-client-auth -o jsonpath=&amp;#39;{.</description>
    </item>
    <item>
      <title>Ceph Daemon Memory Profiling</title>
      <link>/docs-csm/en-10/operations/utility_storage/ceph_daemon_memory_profiling/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/ceph_daemon_memory_profiling/</guid>
      <description>Ceph Daemon Memory Profiling Use Case: This page is meant as an instructional guide to provide information back to HPECray to assist in tuning and troubleshooting exercises.&#xA;Procedure:&#xA;NOTE: For this example we are going to use a ceph-mon process on ncn-s001&#xA;Identify the process and location of the daemon to profile.&#xA;ncn-s00(1/2/3)# ceph orch ps --daemon_type mon NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID mon.</description>
    </item>
    <item>
      <title>Dump SLS Information</title>
      <link>/docs-csm/en-10/operations/system_layout_service/dump_sls_information/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:39 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_layout_service/dump_sls_information/</guid>
      <description>Dump SLS Information Perform a dump of the System Layout Service (SLS) database and an encrypted dump of the credentials stored in Vault.&#xA;This procedure will create three files in the current directory (private_key.pem, public_key.pem, sls_dump.json). These files should be kept in a safe and secure place as the private key can decrypt the encrypted passwords stored in the SLS dump file.&#xA;This procedure preserves the information stored in SLS when backing up or reinstalling the system.</description>
    </item>
    <item>
      <title>System Management Health</title>
      <link>/docs-csm/en-10/operations/system_management_health/system_management_health/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:39 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_management_health/system_management_health/</guid>
      <description>System Management Health The primary goal of the System Management Health service is to enable system administrators to assess the health of their system. Operators need to quickly and efficiently troubleshoot system issues as they occur and be confident that a lack of issues indicates the system is operating normally. This service currently runs as a Helm chart on the system&amp;rsquo;s management Kubernetes cluster and monitors the health status of core system components, triggering alerts as potential issues are observed.</description>
    </item>
    <item>
      <title>System Configuration Service</title>
      <link>/docs-csm/en-10/operations/system_configuration_service/system_configuration_service/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_configuration_service/system_configuration_service/</guid>
      <description>System Configuration Service The System Configuration Service (SCSD) allows administrators to set various BMC and controller parameters. These parameters are typically set during discovery, but this tool enables parameters to be set before or after discovery. The operations to change these parameters are available in the Cray CLI under the scsd command.&#xA;The following are the parameters that most commonly must be set:&#xA;SSH keys IMPORTANT: If the scsd tool is used to update the SSHConsoleKey value outside of ConMan, it will disrupt the ConMan connection to the console and collection of console logs.</description>
    </item>
    <item>
      <title>Troubleshoot Spire Failing to Start on NCNs</title>
      <link>/docs-csm/en-10/operations/spire/troubleshoot_spire_failing_to_start_on_ncns/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/spire/troubleshoot_spire_failing_to_start_on_ncns/</guid>
      <description>Troubleshoot Spire Failing to Start on NCNs The spire-agent service may fail to start on Kubernetes non-compute nodes (NCNs). A key indication of this failure is when logging errors occur with the journalctl command. The following are logging errors that will indicate if the spire-agent is failing to start:&#xA;The join token does not exist or has already been used message is returned The last lines of the logs contain multiple lines of systemd[1]: spire-agent.</description>
    </item>
    <item>
      <title>Audit Logs</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/audit_logs/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:35 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/audit_logs/</guid>
      <description>Audit Logs Overview Audit logs are used to monitor the system and search for suspicious behavior. Host and Kubernetes API audit logging can be enabled to produce extra audit logs for analysis. Enabling audit logging is optional. If enabled it generates some load and data on the non-compute nodes (NCNs).&#xA;By default, host and Kubernetes API audit logging are not enabled. It is not required for both to be enabled or disabled at the same time.</description>
    </item>
    <item>
      <title>Resiliency</title>
      <link>/docs-csm/en-10/operations/resiliency/resiliency/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/resiliency/resiliency/</guid>
      <description>Resiliency HPE Cray EX systems are designed so that system management services (SMS) are fully resilient and that there is no single point of failure.</description>
    </item>
    <item>
      <title>Power Off Compute and IO Cabinets</title>
      <link>/docs-csm/en-10/operations/power_management/power_off_compute_and_io_cabinets/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/power_off_compute_and_io_cabinets/</guid>
      <description>Power Off Compute and IO Cabinets Power off HPE Cray EX liquid-cooled and standard racks.&#xA;Cabinet/rack types Liquid-cooled cabinets HPE Cray EX liquid-cooled cabinet CDU and PDU circuit breakers are controlled manually.&#xA;When the PDU breakers are switched to OFF, the Chassis Management Modules (CMMs) and Cabinet Environmental Controllers (CECs) are also powered off.&#xA;Warning: The cabinet 480VAC power bus bars remain energized. Facility power must be disconnected to completely remove power from the cabinet.</description>
    </item>
    <item>
      <title>Nexus Export and Restore</title>
      <link>/docs-csm/en-10/operations/package_repository_management/nexus_export_and_restore/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/package_repository_management/nexus_export_and_restore/</guid>
      <description>Nexus Export and Restore The current process for ensuring the safety of the nexus-data PVC is a one time, space intensive, manual process, and is only recommended to be done while Nexus is in a known good state. An export is recommended to be done before an upgrade, in order to enable the ability to roll back. Taking an export can also be used to improve Nexus resiliency by allowing easy fixes for data corruption.</description>
    </item>
    <item>
      <title>6.3. Validate Storage Node</title>
      <link>/docs-csm/en-10/operations/node_management/rebuild_ncns/post_rebuild_storage_node_validation/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:31 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/rebuild_ncns/post_rebuild_storage_node_validation/</guid>
      <description>6.3. Validate Storage Node Validate the storage node rebuilt successfully.&#xA;Skip this section if a master or worker node was rebuilt.&#xA;Verify there are 3 mons, 3 mds, 3 mgr processes, and rgws&#xA;ncn-m# ceph -s cluster: id: 22d01fcd-a75b-4bfc-b286-2ed8645be2b5 health: HEALTH_OK services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 4m) mgr: ncn-s001(active, since 19h), standbys: ncn-s002, ncn-s003 mds: cephfs:1 {0=ncn-s001=up:active} 2 up:standby osd: 12 osds: 12 up (since 2m), 12 in (since 2m) rgw: 3 daemons active (ncn-s001.</description>
    </item>
    <item>
      <title>Allocate NCN IP Addresses</title>
      <link>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/allocate_ncn_ip_addresses/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/allocate_ncn_ip_addresses/</guid>
      <description>Allocate NCN IP Addresses Description This procedure allocates IP addresses for an NCN being added to a system. The addresses are allocated on the applicable networks (HMN, NMN, MTL, CAN, etc.), and added to both the System Layout Service (SLS) and the Boot Script Service (BSS).&#xA;This procedure will perform and verify the following:&#xA;If the NCN being added is one of the first three master, storage, or worker NCNs, then its IP address is expected to already be present and consistent between SLS and BSS.</description>
    </item>
    <item>
      <title>External DNS csi config init Input Values</title>
      <link>/docs-csm/en-10/operations/network/external_dns/external_dns_csi_config_init_input_values/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/external_dns/external_dns_csi_config_init_input_values/</guid>
      <description>External DNS csi config init Input Values External DNS requires the system-name, site-domain, and can-external-dns values that are defined with the csi config init command. These values are used to customize the External DNS configuration during installation.&#xA;The system-name and site-domain Values The system-name and site-domain values specified as part of the csi config init are used together in the system-name.site-domain format, creating the external domain for external hostnames for services accessible from the Customer Access Network (CAN).</description>
    </item>
    <item>
      <title>Management Network Flow Control Settings</title>
      <link>/docs-csm/en-10/operations/network/management_network/management_network_flow_control_settings/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/management_network/management_network_flow_control_settings/</guid>
      <description>Management Network Flow Control Settings This page is designed to go over all the flow control settings for Dell/Mellanox systems.&#xA;Leaf Switch Node Connections For the node connections to a leaf switch, disable the transmit flowcontrol and enable receive flowcontrol with the following commands:&#xA;NOTE: If using a TDS system involving a Hill cabinet, make sure to confirm that no CMM nor CEC components are connected to any leaf switches in your system.</description>
    </item>
    <item>
      <title>Troubleshoot BGP not Accepting Routes from MetalLB</title>
      <link>/docs-csm/en-10/operations/network/metallb_bgp/troubleshoot_bgp_not_accepting_routes_from_metallb/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/metallb_bgp/troubleshoot_bgp_not_accepting_routes_from_metallb/</guid>
      <description>Troubleshoot BGP not Accepting Routes from MetalLB Check the number of routes that the Border Gateway Protocol (BGP) Router is accepting in the peering session. This procedure is useful if Kubernetes LoadBalancer services in the NMN, HMN, or CAN address pools are not accessible from outside the cluster.&#xA;Regain access to Kubernetes LoadBalancer services from outside the cluster.&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Procedure Log into the spine or aggregate switch.</description>
    </item>
    <item>
      <title>Externally Exposed Services</title>
      <link>/docs-csm/en-10/operations/network/customer_access_network/externally_exposed_services/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:26 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/customer_access_network/externally_exposed_services/</guid>
      <description>Externally Exposed Services The following services are exposed on the Customer Access Network (CAN). Each of these services requires an IP address on the CAN subnet so they are reachable on the CAN. This IP address is allocated by the MetalLB component.&#xA;Services under Istio Ingress Gateway and Keycloak Gatekeeper Ingress share an ingress, so they all use the IP allocated to the Ingress.&#xA;Each service is given a DNS name that is served by the External DNS service to make them resolvable from the site network.</description>
    </item>
    <item>
      <title>Troubleshoot Common DNS Issues</title>
      <link>/docs-csm/en-10/operations/network/dns/troubleshoot_common_dns_issues/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:26 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/dns/troubleshoot_common_dns_issues/</guid>
      <description>Troubleshoot Common DNS Issues The Domain Name Service (DNS) is part of an integrated infrastructure set designed to provide dynamic host discovery, addressing, and naming. There are several different place to look for troubleshooting as DNS interacts with Dynamic Host Configuration Protocol (DHCP), the Hardware Management Service (HMS), the System Layout Service (SLS), and the State Manager Daemon (SMD).&#xA;The information below describes what to check when experiencing issues with DNS.</description>
    </item>
    <item>
      <title>Network</title>
      <link>/docs-csm/en-10/operations/network/network/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/network/</guid>
      <description>Network There are several different networks supported by the HPE Cray EX system. This page outlines the available internal and external networks, as well as the devices that connect to each network.&#xA;External networks Customer network (data center) System networks Hardware Management Network (HMN) Node Management Network (NMN) ClusterStor Management Network High Speed Network (HSN) IP address ranges Access Control Lists (ACLs) External networks Customer network (data center) The following devices are connected to this network:</description>
    </item>
    <item>
      <title>About kubectl</title>
      <link>/docs-csm/en-10/operations/kubernetes/about_kubectl/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:23 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/about_kubectl/</guid>
      <description>About kubectl kubectl is a CLI that can be used to run commands against a Kubernetes cluster. The format of the kubectl command is shown below:&#xA;ncn# kubectl COMMAND RESOURCE_TYPE RESOURCE_NAME FLAGS An example of using kubectl to retrieve information about a pod is shown below:&#xA;ncn# kubectl get pod POD_NAME1 POD_NAME2 kubectl is installed by default on the non-compute node (NCN) image. To learn more about kubectl, refer to https://kubernetes.</description>
    </item>
    <item>
      <title>Create UAN Boot Images</title>
      <link>/docs-csm/en-10/operations/image_management/create_uan_boot_images/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:22 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/image_management/create_uan_boot_images/</guid>
      <description>Create UAN Boot Images Update configuration management Git repository to match the installed version of the UAN product. Then use that updated configuration to create UAN boot images and a BOS session template.&#xA;This is the overall workflow for preparing UAN images for booting UANs:&#xA;Clone the UAN configuration Git repository and create a branch based on the branch imported by the UAN installation. Update the configuration content and push the changes to the newly created branch.</description>
    </item>
    <item>
      <title>Component Groups and Partitions</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/component_groups_and_partitions/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/component_groups_and_partitions/</guid>
      <description>Component Groups and Partitions The Hardware State Manager (HSM) provides the group and partition services. Both are means of grouping (also known as labeling) system components that are tracked by HSM. Components include the nodes, blades, controllers, and more on a system.&#xA;There is no limit to the number of members a group or partition contains. The only limitation is that all members must be actual members of the system. The HSM needs to know that the components exist.</description>
    </item>
    <item>
      <title>Disable ConMan After the System Software Installation</title>
      <link>/docs-csm/en-10/operations/conman/disable_conman_after_system_software_installation/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/conman/disable_conman_after_system_software_installation/</guid>
      <description>Disable ConMan After the System Software Installation The ConMan utility is enabled by default. The first procedure provides instructions for disabling it after the system software has been installed, and the second procedure provides instructions on how to later re-enable it.&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Disable Procedure Note: this procedure has changed since the CSM 0.9 release.&#xA;Log on to a Kubernetes master or worker node.&#xA;Scale the cray-console-operator pods to 0 replicas.</description>
    </item>
    <item>
      <title>FAS Recipes</title>
      <link>/docs-csm/en-10/operations/firmware/fas_recipes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/firmware/fas_recipes/</guid>
      <description>FAS Recipes NOTE: This is a collection of various FAS recipes for performing updates. For step by step directions and commands, see FAS Use Cases.&#xA;The following example JSON files are useful to reference when updating specific hardware components. In all of these examples, the overrideDryrun field will be set to false; set them to true to perform a live update.&#xA;When updating an entire system, walk down the device hierarchy component type by component type, starting first with routers (switches), proceeding to chassis, and then finally to nodes.</description>
    </item>
    <item>
      <title>CFS Flow</title>
      <link>/docs-csm/en-10/operations/configuration_management/cfs_flow_diagrams/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:17 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/cfs_flow_diagrams/</guid>
      <description>CFS Flow Single session flow Automated session flow Single session flow This section covers the components and actions taken when a user or service creates a session using the CFS sessions endpoint.&#xA;A user creates a CFS configuration. A user creates a CFS session, causing a session record to be created. When a session record is created, the CFS-API also posts an event to a Kafka queue. The CFS-Operator is always monitoring the Kafka queue, and handles events as they come in.</description>
    </item>
    <item>
      <title>Troubleshoot a Failed CRUS Session Because of Bad Parameters</title>
      <link>/docs-csm/en-10/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_bad_parameters/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:17 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_bad_parameters/</guid>
      <description>Troubleshoot a Failed CRUS Session Because of Bad Parameters Note: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality.&#xA;A CRUS session must be deleted and recreated if it does not start or complete because of parameters having incorrect values.&#xA;The following are examples of incorrect parameters:&#xA;Choosing the wrong Boot Orchestration Service (BOS) session template.</description>
    </item>
    <item>
      <title>Boot Orchestration</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/boot_orchestration/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:14 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/boot_orchestration/</guid>
      <description>Boot Orchestration The Boot Orchestration Service (BOS) is responsible for booting, configuring, and shutting down collections of nodes. This is accomplished using BOS components, such as boot orchestration session templates and sessions, as well as launching a Boot Orchestration Agent (BOA) that fulfills boot requests.&#xA;BOS users create a BOS session template via the REST API. A session template is a collection of metadata for a group of nodes and their desired boot artifacts and configuration.</description>
    </item>
    <item>
      <title>Use S3 Libraries and Clients</title>
      <link>/docs-csm/en-10/operations/artifact_management/use_s3_libraries_and_clients/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:14 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/artifact_management/use_s3_libraries_and_clients/</guid>
      <description>Use S3 Libraries and Clients Several command line clients and language-specific libraries are available in addition to the Simple Storage Service (S3) RESTful API. Developers and system administrators can interact with artifacts in the S3 object store with these tools.&#xA;To learn more, refer to the following links:&#xA;S3 Python client S3 Go client Amazon Web Services (AWS) S3 CLI documentation </description>
    </item>
    <item>
      <title>Configure UAIs in UAS</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/configure_uais_in_uas/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:09 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/configure_uais_in_uas/</guid>
      <description>Configure UAIs in UAS The four main items of UAI configuration in UAS. Links to procedures for listing, adding, examining, updating, and deleting each item.&#xA;Options for the elements of a UAI are maintained in the UAS configuration. The following can be configured in UAS:&#xA;UAI images Volumes Resource specifications UAI Classes Only users who are defined as administrators in an HPE Cray EX system and are logged in using the administrative CLI (cray command) can configure UAS.</description>
    </item>
    <item>
      <title>Configure Non-Compute Nodes with CFS</title>
      <link>/docs-csm/en-10/operations/csm_product_management/configure_non-compute_nodes_with_cfs/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:08 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/csm_product_management/configure_non-compute_nodes_with_cfs/</guid>
      <description>Configure Non-Compute Nodes with CFS Non-compute node (NCN) personalization applies post-boot configuration to the HPE Cray EX management nodes. Several HPE Cray EX product environments outside of CSM require NCN personalization to function. Consult the manual for each product to configure them on NCNs by referring to the 1.5 HPE Cray EX System Software Getting Started Guide S-8000 on the HPE Customer Support Center.&#xA;This procedure defines the NCN personalization process for the CSM product using the Configuration Framework Service (CFS).</description>
    </item>
    <item>
      <title>Documentation Conventions</title>
      <link>/docs-csm/en-10/introduction/documentation_conventions/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:07 +0000</pubDate>
      <guid>/docs-csm/en-10/introduction/documentation_conventions/</guid>
      <description>Documentation Conventions Several conventions have been used in the preparation of this documentation.&#xA;Markdown Format File Formats Typographic Conventions Command Prompt Conventions which describe the context for user, host, directory, chroot environment, or container environment Markdown Format This documentation is in Markdown format. Although much of it can be viewed with any text editor, a richer experience will come from using a tool which can render the Markdown to show different font sizes, the use of bold and italics formatting, inclusion of diagrams and screen shots as image files, and to follow navigational links within a topic file and to other files.</description>
    </item>
    <item>
      <title>Troubleshooting Installation Problems</title>
      <link>/docs-csm/en-10/install/troubleshooting_installation/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:07 +0000</pubDate>
      <guid>/docs-csm/en-10/install/troubleshooting_installation/</guid>
      <description>Troubleshooting Installation Problems The installation of the Cray System Management (CSM) product requires knowledge of the various nodes and switches for the HPE Cray EX system. The procedures in this section should be referenced during the CSM install for additional information on system hardware, troubleshooting, and administrative tasks related to CSM.&#xA;Topics: Reset root Password on LiveCD Reinstall LiveCD PXE Boot Troubleshooting Wipe NCN Disks for Reinstallation Restart Network Services and Interfaces on NCNs Utility Storage Node Installation Troubleshooting Ceph CSI Troubleshooting Safeguards for CSM NCN Upgrades Details Reset root Password on LiveCD</description>
    </item>
    <item>
      <title>Bootstrap PIT Node from LiveCD Remote ISO</title>
      <link>/docs-csm/en-10/install/bootstrap_livecd_remote_iso/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:03 +0000</pubDate>
      <guid>/docs-csm/en-10/install/bootstrap_livecd_remote_iso/</guid>
      <description>Bootstrap PIT Node from LiveCD Remote ISO The Pre-Install Toolkit (PIT) node needs to be bootstrapped from the LiveCD. There are two media available to bootstrap the PIT node&amp;ndash;the RemoteISO or a bootable USB device. This procedure describes using the RemoteISO. If not using the RemoteISO, see Bootstrap PIT Node from LiveCD USB&#xA;The installation process is similar to the USB based installation with adjustments to account for the lack of removable storage.</description>
    </item>
    <item>
      <title>Stage 3 - CSM Service Upgrades</title>
      <link>/docs-csm/en-10/upgrade/1.0.11/stage_3/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:46 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.11/stage_3/</guid>
      <description>Stage 3 - CSM Service Upgrades IMPORTANT:&#xA;Reminder: Before running any upgrade scripts, be sure the Cray CLI output format is reset to default by running the following command:&#xA;ncn# unset CRAY_FORMAT Update Loftsman to version 1.2 if not present:&#xA;ncn-m# [[ $(loftsman --version | grep 1.2) ]] || zypper --no-gpg-checks --plus-repo=https://packages.local/repository/csm-sle-15sp2 in -y loftsman Run csm-service-upgrade.sh to deploy upgraded CSM applications and services:&#xA;ncn-m# /usr/share/doc/csm/upgrade/1.0.11/scripts/upgrade/csm-service-upgrade.sh Once Stage 3 is completed, proceed to Stage 4</description>
    </item>
    <item>
      <title>Stage 3 - Kubernetes Upgrade from 1.18.6 to 1.19.9</title>
      <link>/docs-csm/en-10/upgrade/1.0.1/stage_3/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:44 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.1/stage_3/</guid>
      <description>Stage 3 - Kubernetes Upgrade from 1.18.6 to 1.19.9 NOTE: During the CSM-0.9 install the LiveCD containing the initial install files for this system should have been unmounted from the master node when rebooting into the Kubernetes cluster. The scripts run in this section will also attempt to unmount/eject it if found to ensure the USB stick does not get erased.&#xA;IMPORTANT:&#xA;Reminder: Before running any upgrade scripts, be sure the Cray CLI output format is reset to default by running the following command:</description>
    </item>
    <item>
      <title>Troubleshoot Unresponsive kubectl Commands</title>
      <link>/docs-csm/en-10/troubleshooting/kubernetes/troubleshoot_unresponsive_kubectl_commands/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:44 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/kubernetes/troubleshoot_unresponsive_kubectl_commands/</guid>
      <description>Troubleshoot Unresponsive kubectl Commands Use this procedure to check if any kworkers are in an error state because of a high load. Once the error has been identified, workaround the issue by returning the high load to a normal level.&#xA;Symptoms One or more of the following issues are possible symptoms of this issue.&#xA;The kubectl command can become unresponsive because of a high load. ps aux cannot return or complete because of aspects of the /proc file system being locked.</description>
    </item>
    <item>
      <title>CFS Sessions are Stuck in Pending State</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/cfs_sessions_stuck_in_pending/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/cfs_sessions_stuck_in_pending/</guid>
      <description>CFS Sessions are Stuck in Pending State In rare cases it is possible that a CFS session can be stuck in a pending state. Sessions should only enter the pending state briefly, for no more than a few seconds while the corresponding Kubernetes job is being scheduled. If any sessions are in this state for more than a minute, they can safely be deleted. If the sessions were created automatically and retires are enabled, the sessions should be recreated automatically.</description>
    </item>
    <item>
      <title>Ceph Health States</title>
      <link>/docs-csm/en-10/operations/utility_storage/ceph_health_states/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/ceph_health_states/</guid>
      <description>Ceph Health States Ceph reports several different health states depending on the condition of a cluster. These health states can provide a lot of information about the current functionality of the Ceph cluster, what troubleshooting steps needs to be taken, and if a support ticket needs to be filed.&#xA;The health of a Ceph cluster can be viewed with the following command:&#xA;ncn-m001# ceph -s cluster: id: 5f3b4031-d6c0-4118-94c0-bffd90b534eb health: HEALTH_OK &amp;lt;&amp;lt;-- Health state .</description>
    </item>
    <item>
      <title>Load SLS Database with Dump File</title>
      <link>/docs-csm/en-10/operations/system_layout_service/load_sls_database_with_dump_file/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:39 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_layout_service/load_sls_database_with_dump_file/</guid>
      <description>Load SLS Database with Dump File Load the contents of the SLS dump file to restore SLS to the state of the system at the time of the dump. This will upload and overwrite the current SLS database with the contents of the SLS dump file, and update Vault with the encrypted credentials.&#xA;Use this procedure to restore SLS data after a system re-install.&#xA;Prerequisites The System Layout Service (SLS) database has been dumped.</description>
    </item>
    <item>
      <title>System Management Health Checks and Alerts</title>
      <link>/docs-csm/en-10/operations/system_management_health/system_management_health_checks_and_alerts/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:39 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_management_health/system_management_health_checks_and_alerts/</guid>
      <description>System Management Health Checks and Alerts A health check corresponds to a Prometheus query against metrics aggregated to the Prometheus instance. Core platform components like Kubernetes and Istio collect service-related metrics by default, which enables the System Management Health service to implement generic service health checks without custom instrumentation. Health checks are intended to be coarse-grained and comprehensive, as opposed to fine-grained and exhaustive. Health checks related to infrastructure adhere to the Utilization Saturation Errors (USE) method whereas services follow the Rate Errors Duration (RED) method.</description>
    </item>
    <item>
      <title>Update Spire Intermediate CA Certificate</title>
      <link>/docs-csm/en-10/operations/spire/update_spire_intermediate_ca_certificate/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/spire/update_spire_intermediate_ca_certificate/</guid>
      <description>Update Spire Intermediate CA Certificate Prior to CSM 1.2.5 there is no mechanism to automatically update the spire intermediate CA certificate before it expires. This certificate expires after one year. Administrators will want to keep track of when this certificate expires and manually update the certificate before it expires.&#xA;Obtain the Expiration Date for the Spire Intermediate CA To obtain the expiration date of the Spire intermediate CA certificate, run the following command on a node that has access to kubectl (such as ncn-m001):</description>
    </item>
    <item>
      <title>Authenticate an Account with the Command Line</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/authenticate_an_account_with_the_command_line/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:35 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/authenticate_an_account_with_the_command_line/</guid>
      <description>Authenticate an Account with the Command Line Retrieve a token to authenticate to the Cray CLI using the command line. If the Cray CLI is needed before localization occurs and Keycloak is setup, an administrator can use this procedure to authenticate to the Cray CLI.&#xA;Procedure Retrieve the Kubernetes secret to be used for authentication.&#xA;ncn-mw# ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=&amp;#39;{.data.client-secret}&amp;#39; | base64 -d) Create the setup-token.json file and modify it to be readable only by root.</description>
    </item>
    <item>
      <title>Resiliency Testing Procedure</title>
      <link>/docs-csm/en-10/operations/resiliency/resiliency_testing_procedure/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/resiliency/resiliency_testing_procedure/</guid>
      <description>Resiliency Testing Procedure This document and the procedures contained within it are for the purposes of communicating the kind of testing done by the internal Cray System Management (CSM) team to ensure a basic level of system resiliency in the event of the loss of a single non-compute node (NCN).&#xA;It is assumed that some procedures are already known by admins and thus does not go into great detail or attempt to encompass every command necessary for execution.</description>
    </item>
    <item>
      <title>Power Off the External Lustre File System</title>
      <link>/docs-csm/en-10/operations/power_management/power_off_the_external_lustre_file_system/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/power_off_the_external_lustre_file_system/</guid>
      <description>Power Off the External Lustre File System General procedure for powering off an external ClusterStor system.&#xA;Use this procedure as a general guide to power off an external ClusterStor system. Refer to the detailed procedures in the appropriate ClusterStor administration guide:&#xA;Title Model ClusterStor E1000 Administration Guide 4.2 - S-2758 ClusterStor E1000 ClusterStor Administration Guide 3.4 - S-2756 ClusterStor L300/L300N ClusterStor Administration Guide - S-2755 Legacy ClusterStor Procedure SSH to the primary MGMT node as admin.</description>
    </item>
    <item>
      <title>Package Repository Management</title>
      <link>/docs-csm/en-10/operations/package_repository_management/package_repository_management/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/package_repository_management/package_repository_management/</guid>
      <description>Package Repository Management Repositories are added to systems to extend the system functionality beyond what is initially delivered. The Sonatype Nexus Repository Manager is the primary method for repository management. Nexus hosts the Yum, Docker, raw, and Helm repositories for software and firmware content.&#xA;Refer to the following for more information about Nexus:&#xA;The official Sonatype documentation Manage Repositories with Nexus </description>
    </item>
    <item>
      <title>7.1. Validate Worker Node</title>
      <link>/docs-csm/en-10/operations/node_management/rebuild_ncns/post_rebuild_worker_node_validation/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:31 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/rebuild_ncns/post_rebuild_worker_node_validation/</guid>
      <description>7.1. Validate Worker Node Validate the worker node rebuilt successfully.&#xA;Skip this section if a master or storage node was rebuilt.&#xA;Verify the new node is in the cluster.&#xA;Run the following command from any master or worker node that is already in the cluster. It is helpful to run this command several times to watch for the newly rebuilt node to join the cluster. This should occur within 10 to 20 minutes.</description>
    </item>
    <item>
      <title>Boot NCN</title>
      <link>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/boot_ncn/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/boot_ncn/</guid>
      <description>Boot NCN Description Boot a master, worker, or storage non-compute node (NCN) that is to be added to the cluster.&#xA;Procedure Open and watch the console for the node being rebuilt Log in to a second session in order to watch the console.&#xA;Please open this link in a new tab or page Log in to a Node Using ConMan&#xA;The first session will be needed to run the commands in the following Rebuild Node steps.</description>
    </item>
    <item>
      <title>Troubleshoot Services without an Allocated IP Address</title>
      <link>/docs-csm/en-10/operations/network/metallb_bgp/troubleshoot_services_without_an_allocated_ip_address/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/metallb_bgp/troubleshoot_services_without_an_allocated_ip_address/</guid>
      <description>Troubleshoot Services without an Allocated IP Address Check if a given service has an IP address allocated for it if the Kubernetes LoadBalancer services in the NMN, HMN, or CAN address pools are not accessible from outside the cluster.&#xA;Regain access to Kubernetes LoadBalancer services from outside the cluster.&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Procedure Check the status of the services with the kubectl command to see the External-IP of the service.</description>
    </item>
    <item>
      <title>Ingress Routing</title>
      <link>/docs-csm/en-10/operations/network/external_dns/ingress_routing/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/external_dns/ingress_routing/</guid>
      <description>Ingress Routing Ingress routing to services via Istio&amp;rsquo;s ingress gateway is configured by VirtualService custom resource definitions (CRD). When using external hostnames, there needs to be a VirtualService CRD that matches the external hostname to the desired destination.&#xA;For example, the configuration below controls the ingress routing for prometheus.SYSTEM_DOMAIN_NAME:&#xA;ncn-w001# kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus NAME GATEWAYS HOSTS AGE cray-sysmgmt-health-prometheus [services/services-gateway] [prometheus.SYSTEM_DOMAIN_NAME] 22h ncn-w001# kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus -o yaml apiVersion: networking.</description>
    </item>
    <item>
      <title>Management Network Switch Rename</title>
      <link>/docs-csm/en-10/operations/network/management_network/management_network_switch_rename/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/management_network/management_network_switch_rename/</guid>
      <description>Management Network Switch Rename Any system moving from Shasta v1.3 to Shasta v1.4 software needs to adjust the hostnames and IP addresses for all switches to match the new standard. There is now a virtual IP address ending in .1 which is used by spine switches. In Shasta v1.3, the first spine switch used the .1 address. In Shasta v1.4, the ordering of the switches has changed with spine switches being grouped first.</description>
    </item>
    <item>
      <title>Required Labels if CAN is Not Configured</title>
      <link>/docs-csm/en-10/operations/network/customer_access_network/required_labels_if_can_is_not_configured/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:26 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/customer_access_network/required_labels_if_can_is_not_configured/</guid>
      <description>Required Labels if CAN is Not Configured Some services on the system are required to access services outside of the HPE Cray EX system. If the Customer Access Network (CAN) is not configured on the system, these services will need to be pinned to ncn-m001 because that is the only node that has external access. See Customer Access Network (CAN) for more implications if CAN is not configured.&#xA;The label used for scheduling these services is no_external_access:</description>
    </item>
    <item>
      <title>Backups for etcd-operator Clusters</title>
      <link>/docs-csm/en-10/operations/kubernetes/backups_for_etcd-operator_clusters/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:23 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/backups_for_etcd-operator_clusters/</guid>
      <description>Backups for etcd-operator Clusters Backups are periodically created for etcd clusters. These backups are stored in the Ceph Rados Gateway (S3). Not all services are backed up automatically. Services that are not backed up automatically will need to be manually rediscovered if the cluster is unhealthy.&#xA;Clusters with automated backups Clusters without automated backups Clusters with automated backups The following services are backed up daily (one week of backups retained) as part of the automated solution:</description>
    </item>
    <item>
      <title>Customize an Image Root Using IMS</title>
      <link>/docs-csm/en-10/operations/image_management/customize_an_image_root_using_ims/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:22 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/image_management/customize_an_image_root_using_ims/</guid>
      <description>Customize an Image Root Using IMS The Image Management Service (IMS) customization workflow sets up a temporary image customization environment within a Kubernetes pod and mounts the image to be customized in that environment. A system administrator then makes the desired changes to the image root within the customization environment.&#xA;Afterwards, the IMS customization workflow automatically copies the NCN CA public key to /etc/cray/ca/certificate_authority.crt within the image root being customized, in order to enable secure communications between NCNs and client nodes.</description>
    </item>
    <item>
      <title>Component Memberships</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/component_memberships/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/component_memberships/</guid>
      <description>Component Memberships Memberships are a read-only resource that is generated automatically by changes to groups and partitions. Each component in /hsm/v2/State/Components is represented. Filter options are available to prune the list, or a specific component name (xname) can be given. All groups and the partition (if any) of each component are listed.&#xA;At this point in time, only information about node components is needed. The --type node filter option is used in the commands below to retrieve information about node memberships only.</description>
    </item>
    <item>
      <title>FAS Use Cases</title>
      <link>/docs-csm/en-10/operations/firmware/fas_use_cases/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/firmware/fas_use_cases/</guid>
      <description>FAS Use Cases Use the Firmware Action Service (FAS) to update the firmware on supported hardware devices. Each procedure includes the prerequisites and example recipes required to update the firmware.&#xA;When updating an entire system, walk down the device hierarchy component type by component type, starting first with Routers (switches), proceeding to Chassis, and then finally to Nodes. While this is not strictly necessary, it does help eliminate confusion.&#xA;Refer to FAS Filters for more information on the content used in the example JSON files.</description>
    </item>
    <item>
      <title>Establish a Serial Connection to NCNs</title>
      <link>/docs-csm/en-10/operations/conman/establish_a_serial_connection_to_ncns/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/conman/establish_a_serial_connection_to_ncns/</guid>
      <description>Establish a Serial Connection to NCNs The ConMan pod can be used to establish a serial console connection with each non-compute node (NCN) in the system.&#xA;In the scenario of a power down or reboot of an NCN worker, one must first determine if any cray-console pods are running on that NCN. It is important to move cray-console pods to other worker nodes before rebooting or powering off a worker node to minimize disruption in console logging.</description>
    </item>
    <item>
      <title>CFS Global Options</title>
      <link>/docs-csm/en-10/operations/configuration_management/cfs_global_options/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:17 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/cfs_global_options/</guid>
      <description>CFS Global Options The Configuration Framework Service (CFS) provides a global service options endpoint for modifying the base configuration of the service itself.&#xA;View the options with the following command:&#xA;ncn# cray cfs options list --format json { &amp;#34;additionalInventoryUrl&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;batchSize&amp;#34;: 25, &amp;#34;batchWindow&amp;#34;: 60, &amp;#34;batcherCheckInterval&amp;#34;: 10, &amp;#34;defaultAnsibleConfig&amp;#34;: &amp;#34;cfs-default-ansible-cfg&amp;#34;, &amp;#34;defaultBatcherRetryPolicy&amp;#34;: 1, &amp;#34;defaultPlaybook&amp;#34;: &amp;#34;site.yml&amp;#34;, &amp;#34;hardwareSyncInterval&amp;#34;: 10, &amp;#34;sessionTTL&amp;#34;: &amp;#34;7d&amp;#34; } The following are the CFS global options:&#xA;additionalInventoryUrl&#xA;A Git clone URL to supply additional inventory content to all CFS sessions.</description>
    </item>
    <item>
      <title>Troubleshoot a Failed CRUS Session Because of Unmet Conditions</title>
      <link>/docs-csm/en-10/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_unmet_conditions/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:17 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_unmet_conditions/</guid>
      <description>Troubleshoot a Failed CRUS Session Because of Unmet Conditions Note: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality.&#xA;If a CRUS session has any unmet conditions, adding or fixing them will cause the session to continue from wherever it got stuck. Updating other parts of the system to meet the required conditions of a CRUS session will unblock the upgrade session.</description>
    </item>
    <item>
      <title>Boot UANs</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/boot_uans/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:14 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/boot_uans/</guid>
      <description>Boot UANs Boot UANs with an image so that they are ready for user logins.&#xA;Prerequisites UAN boot images and a BOS session template have been created. See Create UAN Boot Images.&#xA;Procedure Create a BOS session to boot the UAN nodes.&#xA;ncn-mw# cray bos session create --template-uuid uan-sessiontemplate-PRODUCT_VERSION \ --operation reboot --format json | tee session.json Example output:&#xA;{ &amp;#34;links&amp;#34;: [ { &amp;#34;href&amp;#34;: &amp;#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d&amp;#34;, &amp;#34;jobId&amp;#34;: &amp;#34;boa-89680d0a-3a6b-4569-a1a1-e275b71fce7d&amp;#34;, &amp;#34;rel&amp;#34;: &amp;#34;session&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;GET&amp;#34; }, { &amp;#34;href&amp;#34;: &amp;#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d/status&amp;#34;, &amp;#34;rel&amp;#34;: &amp;#34;status&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;GET&amp;#34; } ], &amp;#34;operation&amp;#34;: &amp;#34;reboot&amp;#34;, &amp;#34;templateUuid&amp;#34;: &amp;#34;uan-sessiontemplate-PRODUCT_VERSION&amp;#34; } The first attempt to reboot the UANs will most likely fail.</description>
    </item>
    <item>
      <title>Configure a Broker UAI Class</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/configure_a_broker_uai_class/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:09 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/configure_a_broker_uai_class/</guid>
      <description>Configure a Broker UAI Class Configuring a broker UAI class consists of the following:&#xA;Create volumes to hold any site-specific authentication, SSH, or other configuration required Choose the end-user UAI class for which the broker UAI will serve instances Create a UAI Class with (at a minimum): namespace set to uas default set to false volume_mounts set to the list of customization volume-ids created above public_ip set to true uai_compute_network set to false uai_creation_class set to the class-id of the end-user UAI class Example of Volumes to Connect Broker UAIs to LDAP Broker UAIs authenticate users in SSH, and pass the SSH connection on to the selected or created end-user UAI.</description>
    </item>
    <item>
      <title>Perform NCN Personalization</title>
      <link>/docs-csm/en-10/operations/csm_product_management/perform_ncn_personalization/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:08 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/csm_product_management/perform_ncn_personalization/</guid>
      <description>Perform NCN Personalization NCN personalization is the process of applying product-specific configuration to NCNs post-boot.&#xA;Prerequisites Prior to running this procedure, gather the following information required by CFS to create a configuration layer:&#xA;HTTP clone URL for the configuration repository in VCS Path to the Ansible play to run in the repository Commit ID in the repository for CFS to pull and run on the nodes Products may supply multiple plays to run, in which case multiple configuration layers must be created.</description>
    </item>
    <item>
      <title>Utility Storage Installation Troubleshooting</title>
      <link>/docs-csm/en-10/install/utility_storage_node_installation_troubleshooting/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:07 +0000</pubDate>
      <guid>/docs-csm/en-10/install/utility_storage_node_installation_troubleshooting/</guid>
      <description>Utility Storage Installation Troubleshooting If there is a failure in the creation of Ceph storage on the utility storage nodes for one of these scenarios, the Ceph storage might need to be reinitialized.&#xA;Topics Scenario 1 (Shasta v1.4 only) Scenario 2 (Shasta v1.5 only) Details Scenario 1 (Shasta 1.4 only) IMPORTANT (FOR NODE INSTALLS/REINSTALLS ONLY): If the Ceph install failed, check the following:&#xA;ncn-s# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 83.</description>
    </item>
    <item>
      <title>Bootstrap PIT Node from LiveCD USB</title>
      <link>/docs-csm/en-10/install/bootstrap_livecd_usb/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:04 +0000</pubDate>
      <guid>/docs-csm/en-10/install/bootstrap_livecd_usb/</guid>
      <description>Bootstrap PIT Node from LiveCD USB The Pre-Install Toolkit (PIT) node needs to be bootstrapped from the LiveCD. There are two media available to bootstrap the PIT node: the RemoteISO or a bootable USB device. This procedure describes using the USB device. If not using the USB device, see Bootstrap Pit Node from LiveCD Remote ISO.&#xA;There are 5 overall steps that provide a bootable USB with SSH enabled, capable of installing Shasta v1.</description>
    </item>
    <item>
      <title>NCN BIOS</title>
      <link>/docs-csm/en-10/background/ncn_bios/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:01 +0000</pubDate>
      <guid>/docs-csm/en-10/background/ncn_bios/</guid>
      <description>NCN BIOS This page specifies the BIOS settings that are desirable for non-compute nodes (NCNs).&#xA;NOTES&#xA;Any tunables on this page are in the interest of performance and stability. If either of those facets seem to be infringed by any of the content on this page, then contact HPE Cray for support. The table below declares desired settings; unlisted settings should remain at vendor default. This table may be expanded as new settings are adjusted.</description>
    </item>
    <item>
      <title>Stage 4 - Rollout DNS Unbound Deployment Restart</title>
      <link>/docs-csm/en-10/upgrade/1.0.11/stage_4/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:46 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.11/stage_4/</guid>
      <description>Stage 4 - Rollout DNS Unbound Deployment Restart NOTE:&#xA;This stage is only necessary if CSM v1.0.10 has not yet been installed (i.e. upgrading from v1.0.1 directly to v1.0.11).&#xA;Instruct Kubernetes to gracefully restart the Unbound pods:&#xA;ncn-m001:~ # kubectl -n services rollout restart deployment cray-dns-unbound deployment.apps/cray-dns-unbound restarted ncn-m001:~ # kubectl -n services rollout status deployment cray-dns-unbound Waiting for deployment &amp;#34;cray-dns-unbound&amp;#34; rollout to finish: 0 out of 3 new replicas have been updated.</description>
    </item>
    <item>
      <title>PXE Booting Runbook</title>
      <link>/docs-csm/en-10/troubleshooting/pxe_runbook/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:44 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/pxe_runbook/</guid>
      <description>PXE Booting Runbook PXE booting is a key component of a working Shasta system. There are a lot of different components involved, which increases the complexity. This guide runs through the most common issues and shows what is needed in order to have a successful PXE boot.&#xA;NCNs on install ncn-m001 on reboot or NCN boot 2.1. Verify DHCP packets can be forwarded from the workers to the MTL network (VLAN1) 2.</description>
    </item>
    <item>
      <title>Stage 4 - CSM Service Upgrades</title>
      <link>/docs-csm/en-10/upgrade/1.0.1/stage_4/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:44 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.1/stage_4/</guid>
      <description>Stage 4 - CSM Service Upgrades IMPORTANT:&#xA;Reminder: Before running any upgrade scripts, be sure the Cray CLI output format is reset to default by running the following command:&#xA;ncn# unset CRAY_FORMAT Run csm-service-upgrade.sh to deploy upgraded CSM applications and services:&#xA;ncn-m002# /usr/share/doc/csm/upgrade/1.0.1/scripts/upgrade/csm-service-upgrade.sh Once Stage 4 service upgrade is complete, proceed to Stage 5</description>
    </item>
    <item>
      <title>SAT/HSM/CAPMC Component Power State Mismatch</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/component_power_state_mismatch/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/component_power_state_mismatch/</guid>
      <description>SAT/HSM/CAPMC Component Power State Mismatch Because of various hardware or communication issues, the node state reported by SAT and HSM (Hardware State Manager) may become out of sync with the actual hardware state reported by CAPMC or Redfish. In most cases this will be noticed when trying to power on or off nodes with BOS/BOA, and will present as SAT or HSM reporting nodes are On while CAPMC reports them as Off (or vice versa).</description>
    </item>
    <item>
      <title>Ceph Orchestrator General Usage and Tips</title>
      <link>/docs-csm/en-10/operations/utility_storage/ceph_orchestrator_usage/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/ceph_orchestrator_usage/</guid>
      <description>Ceph Orchestrator General Usage and Tips Description &amp;ldquo;Is a module that provides a command line interface (CLI) to orchestrator modules (ceph-mgr modules which interface with external orchestration services).&amp;rdquo; - source (https://docs.ceph.com/en/latest/mgr/orchestrator/)&#xA;This provides a nice centralized interface for the management of the ceph cluster. This includes:&#xA;Single command upgrades, assuming all images are in place. Reduces the need to be on the physical server to address a large number of ceph service restarts or configuration changes Better integration with the Ceph Dashboard (Coming soon) Ability to write custom orchestration modules Troubleshooting Ceph Orchestrator Watching cephadm Log Messages This is useful when making changes via the orchestrator like add/remove/scale services or upgrades.</description>
    </item>
    <item>
      <title>Troubleshoot Prometheus Alerts</title>
      <link>/docs-csm/en-10/operations/system_management_health/troubleshoot_prometheus_alerts/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_management_health/troubleshoot_prometheus_alerts/</guid>
      <description>Troubleshoot Prometheus Alerts CephMgrIsAbsent and CephMgrIsMissingReplicas CephNetworkPacketsDropped) CPUThrottlingHigh KubePodNotReady PostgresqlFollowerReplicationLagSMA PostgresqlHighRollbackRate PostgresqlInactiveReplicationSlot PostgresqlNotEnoughConnections CephMgrIsAbsent and CephMgrIsMissingReplicas If the CephMgrIsAbsent and/or CephMgrIsMissingReplicas alerts fire, use the following steps to ensure the prometheus module has been enabled for Ceph. The following steps should be executed on ncn-s001:&#xA;ncn-s001# ceph mgr module ls | jq &amp;#39;.enabled_modules&amp;#39; Example output:&#xA;[ &amp;#34;cephadm&amp;#34;, &amp;#34;iostat&amp;#34;, &amp;#34;restful&amp;#34; ] If prometheus is missing from the output, enable with the following command:</description>
    </item>
    <item>
      <title>Restore SLS Postgres Database from Backup</title>
      <link>/docs-csm/en-10/operations/system_layout_service/restore_sls_postgres_database_from_backup/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:39 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_layout_service/restore_sls_postgres_database_from_backup/</guid>
      <description>Restore SLS Postgres Database from Backup This procedure can be used to restore the SLS Postgres database from a previously taken manual backup created by the Create a Backup of the SLS Postgres Database procedure.&#xA;Prerequisites Healthy Postgres Cluster.&#xA;Use patronictl list on the SLS Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:&#xA;ncn# kubectl exec cray-sls-postgres-0 -n services -c postgres -it -- patronictl list + Cluster: cray-sls-postgres (6975238790569058381) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +---------------------+------------+--------+---------+----+-----------+ | cray-sls-postgres-0 | 10.</description>
    </item>
    <item>
      <title>Backup and Restore Vault Clusters</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/backup_and_restore_vault_clusters/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:35 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/backup_and_restore_vault_clusters/</guid>
      <description>Backup and Restore Vault Clusters View the existing Vault backups on the system and use a completed backup to perform a restore operation.&#xA;Velero is used to perform a nightly backup of Vault. The backup includes Kubernetes object state, in addition to pod volume data for the Vault statefulset. For more information on Velero, refer to the external Velero documentation.&#xA;CAUTION: A restore operation should only be performed in extreme situations.</description>
    </item>
    <item>
      <title>Restore System Functionality if a Kubernetes Worker Node is Down</title>
      <link>/docs-csm/en-10/operations/resiliency/restore_system_functionality_if_a_kubernetes_worker_node_is_down/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/resiliency/restore_system_functionality_if_a_kubernetes_worker_node_is_down/</guid>
      <description>Restore System Functionality if a Kubernetes Worker Node is Down Services running on Kubernetes worker nodes can be properly restored if downtime occurs. Use this procedure to ensure that if a Kubernetes worker node is lost or restored after being down, then certain features the node was providing can also be restored or recovered on another node.&#xA;Capture the metadata for the unhealthy node before bringing down the node. The pods will successfully terminate when the node goes down, which should resolve most pods in an error state.</description>
    </item>
    <item>
      <title>Power On Compute and IO Cabinets</title>
      <link>/docs-csm/en-10/operations/power_management/power_on_compute_and_io_cabinets/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/power_on_compute_and_io_cabinets/</guid>
      <description>Power On Compute and IO Cabinets Power on liquid-cooled and standard rack cabinet PDUs.&#xA;Liquid-cooled Cabinets - HPE Cray EX liquid-cooled cabinet CDU and PDU circuit breakers are controlled manually.&#xA;After the CDU is switched on and healthy, the liquid-cooled PDU circuit breakers can be switched ON. With PDU breakers ON, the Chassis Management Modules (CMM) and Cabinet Environmental Controllers (CEC) power on and boot. These devices can then communicate with the management cluster and larger system management network.</description>
    </item>
    <item>
      <title>Package Repository Management with Nexus</title>
      <link>/docs-csm/en-10/operations/package_repository_management/package_repository_management_with_nexus/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/package_repository_management/package_repository_management_with_nexus/</guid>
      <description>Package Repository Management with Nexus Overview of RPM repositories and container registry in Nexus.&#xA;RPM repositories Container registry Adding images Registry mirror configuration Pull example using CRI Pull example using containerd Pull example using Podman RPM repositories Repositories are available at https://packages.local/repository/REPO_NAME. For example, to configure the csm-sle-15sp2 repository on a non-compute node (NCN):&#xA;ncn# zypper addrepo -fG https://packages.local/repository/csm-sle-15sp2 csm-sle-15sp2 Example output:&#xA;Adding repository &amp;#39;csm-sle-15sp2&amp;#39; .................................................................................................[done] Warning: GPG checking is disabled in configuration of repository &amp;#39;csm-sle-15sp2&amp;#39;.</description>
    </item>
    <item>
      <title>Power Cycle and Rebuild Node</title>
      <link>/docs-csm/en-10/operations/node_management/rebuild_ncns/power_cycle_and_rebuild_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:31 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/rebuild_ncns/power_cycle_and_rebuild_nodes/</guid>
      <description>Power Cycle and Rebuild Node This section applies to all node types. The commands in this section assume the variables from the prerequisites section have been set.&#xA;Procedure Open and watch the console for the node being rebuilt.&#xA;Log in to a second session to use it to watch the console using the instructions at the link below:&#xA;Open this link in a new tab or page Log in to a Node Using ConMan</description>
    </item>
    <item>
      <title>Collect NCN MAC Addresses</title>
      <link>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/collect_ncn_mac_addresses/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/collect_ncn_mac_addresses/</guid>
      <description>Collect NCN MAC Addresses This procedure can be used to to collect MAC addresses from the NCNs along with their assigned interface names for use with the Add NCN Data procedure. A temporary MAC address collection iPXE bootscript is put into place on the system to discover the MAC addresses of the NCNs, along with their associated interface names (such as mgmt0).&#xA;WARNING This procedure will temporarily break the system&amp;rsquo;s ability to properly boot nodes in the system.</description>
    </item>
    <item>
      <title>Update BGP Neighbors</title>
      <link>/docs-csm/en-10/operations/network/metallb_bgp/update_bgp_neighbors/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/metallb_bgp/update_bgp_neighbors/</guid>
      <description>Update BGP Neighbors This page will detail the manual procedure to configure and verify BGP neighbors on the management switches.&#xA;You will not have BGP peers until CSM install.sh has run. This is where MetalLB is deployed.&#xA;How do I check the status of the BGP neighbors? Log into the spine switches and run show bgp ipv4 unicast summary for Aruba/HPE switches and show ip bgp summary for Mellanox. Are my Neighbors stuck in IDLE?</description>
    </item>
    <item>
      <title>Troubleshoot DNS Configuration Issues</title>
      <link>/docs-csm/en-10/operations/network/external_dns/troubleshoot_dns_configuration_issues/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/external_dns/troubleshoot_dns_configuration_issues/</guid>
      <description>Troubleshoot DNS Configuration Issues Troubleshoot issues when DNS is not properly configured to delegate name resolution to the core DNS instance on a specific cluster. Although the CAN IP address may still be routable using the IP address directly, it may not work because Istio&amp;rsquo;s ingress gateway depends on the hostname (or SNI) to route traffic. For command line tools like cURL, using the &amp;ndash;resolve option to force correct resolution can be used to work around this issue.</description>
    </item>
    <item>
      <title>Update Management Network Firmware</title>
      <link>/docs-csm/en-10/operations/network/management_network/update_management_network_firmware/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/management_network/update_management_network_firmware/</guid>
      <description>Update Management Network Firmware This page describes how to update firmware on the management network switches.&#xA;Requirements Access to the switches from the LiveCD/ncn-m001.&#xA;Configuration All firmware can be found in the HFP package provided with the Shasta release.&#xA;Switch Firmware Vendor Model Version Aruba 6300 ArubaOS-CX_6400-6300_10.06.0010 Aruba 8320 ArubaOS-CX_8320_10.06.0010 or ArubaOS-CX_8320_10.06.0110 Aruba 8325 ArubaOS-CX_8325_10.06.0010 Aruba 8360 ArubaOS-CX_8360_10.06.0010 or ArubaOS-CX_8360_10.06.0110 Dell S3048-ON 10.5.1.4 Dell S4148F-ON 10.5.1.4 Dell S4148T-ON 10.5.1.4 Mellanox MSN2100 3.</description>
    </item>
    <item>
      <title>Troubleshoot CAN Issues</title>
      <link>/docs-csm/en-10/operations/network/customer_access_network/troubleshoot_can_issues/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:26 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/customer_access_network/troubleshoot_can_issues/</guid>
      <description>Troubleshoot CAN Issues Various connection points to check when using the CAN and how to fix any issues that arise.&#xA;The most frequent issue with the Customer Access Network (CAN) is trouble accessing IP addresses outside of the HPE Cray EX system from a node or pod inside the system.&#xA;The best way to resolve this issue is to try to ping an outside IP address from one of the NCNs other than ncn-m001, which has a direct connection that it can use instead of the Customer Access Network (CAN).</description>
    </item>
    <item>
      <title>Kubernetes and Bare Metal EtcD Certificate Renewal</title>
      <link>/docs-csm/en-10/operations/kubernetes/cert_renewal_for_kubernetes_and_bare_metal_etcd/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:23 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/cert_renewal_for_kubernetes_and_bare_metal_etcd/</guid>
      <description>Kubernetes and Bare Metal EtcD Certificate Renewal As part of the installation, Kubernetes generates certificates for the required subcomponents. This document will help walk through the process of renewing the certificates.&#xA;IMPORTANT:&#xA;Depending on the version of Kubernetes, the command may or may not reside under the alpha category. Use kubectl certs --help and kubectl alpha certs --help to determine this. The overall command syntax is the same; the only difference is whether or not the command structure includes alpha.</description>
    </item>
    <item>
      <title>Delete or Recover Deleted IMS Content</title>
      <link>/docs-csm/en-10/operations/image_management/delete_or_recover_deleted_ims_content/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:22 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/image_management/delete_or_recover_deleted_ims_content/</guid>
      <description>Delete or Recover Deleted IMS Content The Image Management System (IMS) manages user-supplied SSH public keys, customizable image recipes, images, and IMS jobs that are used to build or customize images. In previous versions of IMS, deleting an IMS public key, recipe, or image resulted in that item being permanently deleted. Additionally, IMS recipes and images store linked artifacts in the Simple Storage Service (S3) datastore. These artifacts are referenced by the IMS recipe and image records.</description>
    </item>
    <item>
      <title>Component Partition Members</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/component_partition_members/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/component_partition_members/</guid>
      <description>Component Partition Members The members object in the partition definition has additional actions available for managing the members after the partition has been created.&#xA;The following is an example of partition members:&#xA;{ &amp;#34;ids&amp;#34; : [ &amp;#34;x0c0s0b0n0&amp;#34;,&amp;#34;x0c0s0b0n1&amp;#34;,&amp;#34;x0c0s0b1n0&amp;#34;,&amp;#34;x0c0s0b1n1&amp;#34; ] } Retrieve Partition Members Retrieving members of a partition is very similar to how group members are retrieved and modified. No filtering options are available in partitions. However, there are partition and group filtering parameters for the /hsm/v2/State/Components and /hsm/v2/memberships collections, with both essentially working the same way.</description>
    </item>
    <item>
      <title>Update Firmware with FAS</title>
      <link>/docs-csm/en-10/operations/firmware/update_firmware_with_fas/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/firmware/update_firmware_with_fas/</guid>
      <description>Update Firmware with FAS The Firmware Action Service (FAS) provides an interface for managing firmware versions of Redfish-enabled hardware in the system. FAS interacts with the Hardware State Managers (HSM), device data, and image data in order to update firmware.&#xA;Reset Gigabyte node BMC to factory defaults if having problems with ipmitool, using Redfish, or when flashing procedures fail. See Set Gigabyte Node BMC to Factory Defaults.&#xA;FAS images contain the following information that is needed for a hardware device to update firmware versions:</description>
    </item>
    <item>
      <title>Log in to a Node Using ConMan</title>
      <link>/docs-csm/en-10/operations/conman/log_in_to_a_node_using_conman/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/conman/log_in_to_a_node_using_conman/</guid>
      <description>Log in to a Node Using ConMan This procedure shows how to connect to the node&amp;rsquo;s Serial Over LAN (SOL) via ConMan.&#xA;Prerequisites The user performing this procedure needs to have access permission to the cray-console-operator and cray-console-node pods.&#xA;Procedure Note: this procedure has changed since the CSM 0.9 release.&#xA;Log on to a Kubernetes master or worker node.&#xA;Find the cray-console-operator pod.&#xA;ncn-mw# OP_POD=$(kubectl get pods -n services -o wide|grep cray-console-operator|awk &amp;#39;{print $1}&amp;#39;); echo $OP_POD Example output:</description>
    </item>
    <item>
      <title>CFS Key Management and Permission Denied Errors</title>
      <link>/docs-csm/en-10/operations/configuration_management/cfs_key_management/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/cfs_key_management/</guid>
      <description>CFS Key Management and Permission Denied Errors Configuration Framework Service (CFS) manages its own keys separate from keys for communication between CFS and the components or images that it is configuring. These are separate from the keys used by users and should not need to be managed.&#xA;Permission Denied Errors If Ansible is unable to connect with its target and fails with an Unreachable - Permission denied error, the first place to check is the cfs-state-reporter on the target node.</description>
    </item>
    <item>
      <title>Upgrade Compute Nodes with CRUS</title>
      <link>/docs-csm/en-10/operations/compute_rolling_upgrades/upgrade_compute_nodes_with_crus/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:17 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/compute_rolling_upgrades/upgrade_compute_nodes_with_crus/</guid>
      <description>Upgrade Compute Nodes with CRUS Note: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality.&#xA;Upgrade a set of compute nodes with the Compute Rolling Upgrade Service (CRUS). Manage the workload management status of nodes and quiesce each node before taking the node out of service and upgrading it. Then reboot it into the upgraded state and return it to service within the workload manager (WLM).</description>
    </item>
    <item>
      <title>Check the Progress of BOS Session Operations</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/check_the_progress_of_bos_session_operations/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/check_the_progress_of_bos_session_operations/</guid>
      <description>Check the Progress of BOS Session Operations Describes how to view the logs of BOS operations with Kubernetes.&#xA;When a Boot Orchestration Service (BOS) session is created, it will return a job ID. This ID can be used to locate the Boot Orchestration Agent (BOA) Kubernetes job that executes the session. For example:&#xA;ncn-mw# cray bos session create --template-uuid SESSIONTEMPLATE_NAME --operation boot --format toml Example output:&#xA;operation = &amp;#34;boot&amp;#34; templateName = &amp;#34;SESSIONTEMPLATE_NAME&amp;#34; [[links]] href = &amp;#34;foo-c7faa704-3f98-4c91-bdfb-e377a184ab4f&amp;#34; jobId = &amp;#34;boa-a939bd32-9d27-433f-afc2-735e77ec8e58&amp;#34; rel = &amp;#34;session&amp;#34; type = &amp;#34;GET&amp;#34; All BOS Kubernetes pods operate in the services namespace.</description>
    </item>
    <item>
      <title>Configure a Default UAI Class for Legacy Mode</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/configure_a_default_uai_class_for_legacy_mode/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:09 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/configure_a_default_uai_class_for_legacy_mode/</guid>
      <description>Configure a Default UAI Class for Legacy Mode Using a default UAI class is optional but recommended for any site using the legacy UAI management mode that wants to have some control over UAIs created by users. UAI classes used for this purpose need to have certain minimum configuration in them:&#xA;The image_id field set to identify the image used to construct UAIs The volume_list field set to the list of volumes to mount in UAIs The public_ip field set to true The uai_compute_network flag set to true (if workload management will be used) The default flag set to true to make this the default UAI class To make UAIs useful, there is a minimum set of volumes that should be defined in the UAS configuration:</description>
    </item>
    <item>
      <title>Post-Install Customizations</title>
      <link>/docs-csm/en-10/operations/csm_product_management/post_install_customizations/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:08 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/csm_product_management/post_install_customizations/</guid>
      <description>Post-Install Customizations Post-install customizations may be needed as systems scale. These customizations also need to persist across future installs or upgrades. Not all resources can be customized post-install; common scenarios are documented in the following sections.&#xA;The following is a guide for determining where issues may exist, how to adjust the resources, and how to ensure the changes will persist. Different values may be be needed for systems as they scale.</description>
    </item>
    <item>
      <title>Scenarios for Shasta v1.5</title>
      <link>/docs-csm/en-10/introduction/scenarios/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:07 +0000</pubDate>
      <guid>/docs-csm/en-10/introduction/scenarios/</guid>
      <description>Scenarios for Shasta v1.5 There are multiple scenarios for installing CSM software which are described in this documentation with many supporting procedures.&#xA;Scenarios for Shasta v1.5 Installation Upgrade Migration Installation There are two ways to install the CSM software. There are some differences between a first time install which must create the initial configuration payload and configure the management network switches, whereas a reinstall can reuse a previous configuration payload and skip the configuration of management network switches.</description>
    </item>
    <item>
      <title>Validate Management Network Cabling</title>
      <link>/docs-csm/en-10/install/validate_management_network_cabling/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:07 +0000</pubDate>
      <guid>/docs-csm/en-10/install/validate_management_network_cabling/</guid>
      <description>Validate Management Network Cabling This page is designed to be a guide on how all nodes in a Shasta system are wired to the management network.&#xA;The Shasta Cabling Diagram (SHCD) for this system describes how the cables connect the nodes to the management network switches and the connections between the different types of management network switches. Having SHCD data which matches how the physical system is cabled will be needed later when preparing the hmn_connections.</description>
    </item>
    <item>
      <title>Cable Management Network Servers</title>
      <link>/docs-csm/en-10/install/cable_management_network_servers/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:04 +0000</pubDate>
      <guid>/docs-csm/en-10/install/cable_management_network_servers/</guid>
      <description>Cable Management Network Servers This topic describes nodes in the air-cooled cabinet with diagrams and pictures showing where to find the ports on the nodes and how to cable the nodes to the management network switches.&#xA;HPE Hardware HPE DL385 HPE DL325 HPE Worker Node Cabling HPE Master Node Cabling HPE Storage Node Cabling HPE UAN Cabling HPE Apollo 6500 XL645D HPE Apollo 6500 XL675D Gigabyte/Intel Hardware Worker Node Cabling Master Node Cabling Storage Node Cabling UAN Cabling HPE Hardware HPE DL385 The OCP Slot is noted (number 7) in the image above.</description>
    </item>
    <item>
      <title>NCN Boot Workflow</title>
      <link>/docs-csm/en-10/background/ncn_boot_workflow/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:01 +0000</pubDate>
      <guid>/docs-csm/en-10/background/ncn_boot_workflow/</guid>
      <description>NCN Boot Workflow This document provides information on non-compute node (NCN) boot devices and boot ordering.&#xA;Boot sources Determine the current boot order Reasons to change the boot order after CSM install Determine if NCNs booted via disk or PXE Set BMCs to DHCP Boot order overview Setting boot order Trimming boot order Example boot orders Reverting changes Locating USB device Boot sources Non-compute nodes (NCNs) can boot from two sources:</description>
    </item>
    <item>
      <title>Stage 5 - Verification</title>
      <link>/docs-csm/en-10/upgrade/1.0.11/stage_5/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:46 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.11/stage_5/</guid>
      <description>Stage 5 - Verification Verify that the following command includes the new CSM version (1.0.11):&#xA;ncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=&amp;#39;{.data.csm}&amp;#39; | yq r -j - | jq -r &amp;#39;to_entries[] | .key&amp;#39; | sort -V 1.0.1 1.0.10 1.0.11 Verify that fix for CVE-2022-0185 (Linux kernel buffer overflow/container escape) is in place:&#xA;ncn-m001:~ # pdsh -w $(grep -oP &amp;#39;ncn-\w\d+&amp;#39; /etc/hosts | sort -u | tr -t &amp;#39;\n&amp;#39; &amp;#39;,&amp;#39;) &amp;#34;uname -a&amp;#34; ncn-m001: Linux ncn-m002 5.</description>
    </item>
    <item>
      <title>Stage 5 - Workaround for MAC-learning issue with Aruba 8325 switches</title>
      <link>/docs-csm/en-10/upgrade/1.0.1/stage_5/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:45 +0000</pubDate>
      <guid>/docs-csm/en-10/upgrade/1.0.1/stage_5/</guid>
      <description>Stage 5 - Workaround for MAC-learning issue with Aruba 8325 switches Issue description:&#xA;Aruba CR: 90598&#xA;Affected platform: Aruba 8325 switches&#xA;Symptom: MAC learning stops&#xA;Scenario: Under extremely rare DMA stress conditions, an L2 learning thread may timeout and exit, preventing future MAC learning&#xA;Workaround: Reboot the switch or monitor the L2 thread and restart it with an NAE script&#xA;Fixed in: 10.06.0130, 10.7.0010, and above&#xA;Aruba release notes&#xA;Overview NOTE: If you do not have Aruba 8325 switches in your system, skip this stage and return to main upgrade page.</description>
    </item>
    <item>
      <title>Console Logs Fill All Available Storage Space</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/console_log_storage_filling/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/console_log_storage_filling/</guid>
      <description>Console Logs Fill All Available Storage Space The log rotation functionality of the console logging services has a bug where the files are not rotated correctly. This will lead to the logs continuing to expand until eventually all space is consumed on the storage. This can lead to issues where the log files cease to capture log output, and can sometimes lead to the cray-console-node pods failing to connect to nodes.</description>
    </item>
    <item>
      <title>Ceph Service Check Script Usage</title>
      <link>/docs-csm/en-10/operations/utility_storage/ceph_service_check_script_usage/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/ceph_service_check_script_usage/</guid>
      <description>Ceph Service Check Script Usage Description: This is a new Ceph service script that will check the status of Ceph and then verify that status against the individual Ceph storage nodes.&#xA;Location: /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh&#xA;Usage: usage: ceph-service-status.sh # runs a simple Ceph health check ceph-service-status.sh -n &amp;lt;node&amp;gt; -s &amp;lt;service&amp;gt; # checks a single service on a single node ceph-service-status.sh -n &amp;lt;node&amp;gt; -a true # checks all Ceph services on a node ceph-service-status.</description>
    </item>
    <item>
      <title>Restore SLS Postgres without an Existing Backup</title>
      <link>/docs-csm/en-10/operations/system_layout_service/restore_sls_postgres_without_an_existing_backup/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:39 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_layout_service/restore_sls_postgres_without_an_existing_backup/</guid>
      <description>Restore SLS Postgres without an Existing Backup This procedure is intended to repopulate SLS in the event when no Postgres backup exists.&#xA;Prerequisite Healthy SLS Service. Verify all 3 SLS replicas are up and running:&#xA;ncn# kubectl -n services get pods -l cluster-name=cray-sls-postgres NAME READY STATUS RESTARTS AGE cray-sls-postgres-0 3/3 Running 0 18d cray-sls-postgres-1 3/3 Running 0 18d cray-sls-postgres-2 3/3 Running 0 18d Procedure Retrieve the initial sls_input_file.json that was used to initially install the system with from sls S3 bucket.</description>
    </item>
    <item>
      <title>Certificate Types</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/certificate_types/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:35 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/certificate_types/</guid>
      <description>Certificate Types The system software installation process creates an X.509 Certificate Authority (CA) on the primary non-compute node (NCN) and uses the CA to create an NCN host X.509 certificate. This host certificate is used during the installation process to configure the API gateway for TLS so that communications to the gateway can use HTTPS.&#xA;Clients should use HTTPS to talk to services behind the API gateway and need to ensure that the NCN CA certificate is known by the client software when making requests.</description>
    </item>
    <item>
      <title>Power On and Boot Compute and User Access Nodes</title>
      <link>/docs-csm/en-10/operations/power_management/power_on_and_boot_compute_nodes_and_user_access_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/power_on_and_boot_compute_nodes_and_user_access_nodes/</guid>
      <description>Power On and Boot Compute and User Access Nodes Use Boot Orchestration Service (BOS) and choose the appropriate session template to power on and boot compute and UANs.&#xA;This procedure boots all compute nodes and user access nodes (UANs) in the context of a full system power-up.&#xA;Prerequisites All compute cabinet PDUs, servers, and switches must be powered on. The Slingshot Fabric is up and configured. Refer to the following documentation for more information on how to bring up the Slingshot Fabric: The HPE Slingshot Operations Guide PDF for HPE Cray EX systems.</description>
    </item>
    <item>
      <title>Repair Yum Repository Metadata</title>
      <link>/docs-csm/en-10/operations/package_repository_management/repair_yum_repository_metadata/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/package_repository_management/repair_yum_repository_metadata/</guid>
      <description>Repair Yum Repository Metadata Nexus may have trouble generating or regenerating repository metadata (for example, repodata/repomd.xml), especially for larger repositories. Configure the Repair - Rebuild Yum repository metadata (repodata) task in Nexus to create the metadata if the standard generation fails. This is not typically needed, so it is considered to be a repair task.&#xA;The example in this procedure is for creating a repair task to rebuild Yum metadata for the mirror-1.</description>
    </item>
    <item>
      <title>Prepare Master Node</title>
      <link>/docs-csm/en-10/operations/node_management/rebuild_ncns/prepare_master_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:31 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/rebuild_ncns/prepare_master_nodes/</guid>
      <description>Prepare Master Node Description Prepare a master node before rebuilding it.&#xA;Procedure Step 1 - Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig before shutting down the node The following command will indicate if a CFS job is currently in progress for this node. This command assumes you have set the variables from the prerequisites section.&#xA;ncn-m# cray cfs components describe $XNAME --format json { &amp;#34;configurationStatus&amp;#34;: &amp;#34;configured&amp;#34;, &amp;#34;desiredConfig&amp;#34;: &amp;#34;ncn-personalization-full&amp;#34;, &amp;#34;enabled&amp;#34;: true, &amp;#34;errorCount&amp;#34;: 0, &amp;#34;id&amp;#34;: &amp;#34;x3000c0s7b0n0&amp;#34;, &amp;#34;retryPolicy&amp;#34;: 3, If the configurationStatus is pending, wait for the job finish before rebooting this node.</description>
    </item>
    <item>
      <title>Redeploy Services Impacted by Adding or Permanently Removing Storage Nodes</title>
      <link>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/redeploy_services/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/redeploy_services/</guid>
      <description>Redeploy Services Impacted by Adding or Permanently Removing Storage Nodes This procedure redeploys S3 and sysmgmt-health services to add or remove storage node endpoints.&#xA;This procedure can be skipped if a worker or master node has been added. In that case, proceed to the next step to Validate NCN or return to the main Add, Remove, Replace, or Move NCNs page.&#xA;This procedure can be skipped if a worker or master node have been removed.</description>
    </item>
    <item>
      <title>Troubleshoot Connectivity to Services with External IP addresses</title>
      <link>/docs-csm/en-10/operations/network/external_dns/troubleshoot_systems_not_provisioned_with_external_ip_addresses/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/external_dns/troubleshoot_systems_not_provisioned_with_external_ip_addresses/</guid>
      <description>Troubleshoot Connectivity to Services with External IP addresses Systems that do not support CAN will not have services provisioned with external IP addresses on CAN. Kubernetes will report a &amp;lt;pending&amp;gt; status for the external IP address of the service experiencing connectivity issues.&#xA;If SSH access to a non-compute node (NCN) is available, it is possible to override resolution of external hostnames and forward local ports into the cluster for the cluster IP address of the corresponding service.</description>
    </item>
    <item>
      <title>Check for and Clear etcd Cluster Alarms</title>
      <link>/docs-csm/en-10/operations/kubernetes/check_for_and_clear_etcd_cluster_alarms/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:23 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/check_for_and_clear_etcd_cluster_alarms/</guid>
      <description>Check for and Clear etcd Cluster Alarms Check for any etcd cluster alarms and clear them as needed. An etcd cluster alarm must be manually cleared.&#xA;For example, a cluster&amp;rsquo;s database NOSPACE alarm is set when database storage space is no longer available. A subsequent defrag may free up database storage space, but writes to the database will continue to fail while the NOSPACE alarm is set.&#xA;Prerequisites This procedure requires root privileges.</description>
    </item>
    <item>
      <title>Image Management</title>
      <link>/docs-csm/en-10/operations/image_management/image_management/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:23 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/image_management/image_management/</guid>
      <description>Image Management The Image Management Service (IMS) uses the open source Kiwi-NG tool to build image roots from compressed Kiwi image descriptions. These compressed Kiwi image descriptions are referred to as &amp;ldquo;recipes.&amp;rdquo; Kiwi-NG builds images based on a variety of different Linux distributions, specifically SUSE, RHEL, and their derivatives. Kiwi image descriptions must follow the Kiwi development schema. More information about the development schema and the Kiwi-NG tool can be found in the documentation: https://doc.</description>
    </item>
    <item>
      <title>Create a Backup of the HSM Postgres Database</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/create_a_backup_of_the_hsm_postgres_database/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/create_a_backup_of_the_hsm_postgres_database/</guid>
      <description>Create a Backup of the HSM Postgres Database Perform a manual backup of the contents of the Hardware State Manager (HSM) Postgres database. This backup can be used to restore the contents of the HSM Postgres database at a later point in time using the Restore HSM Postgres from Backup procedure.&#xA;Prerequisites Healthy HSM Postgres Cluster.&#xA;Use patronictl list on the HSM Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:</description>
    </item>
    <item>
      <title>Updating BMC Firmware and BIOS for ncn-m001</title>
      <link>/docs-csm/en-10/operations/firmware/updating_firmware_m001/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/firmware/updating_firmware_m001/</guid>
      <description>Updating BMC Firmware and BIOS for ncn-m001 Retrieve the model name and firmware image required to update an HPE or Gigabyte ncn-m001 node.&#xA;NOTE&#xA;On HPE nodes, the BMC firmware is iLO 5 and BIOS is System ROM. The commands in the procedure must be run on ncn-m001. This procedure should only be used if FAS is not available, such as during initial CSM install. Prerequisites Find the model name Get the firmware images Flash the firmware Flash Gigabyte ncn-m001 Flash HPE ncn-m001 Prerequisites WARNING: This procedure should not be performed during a CSM install while ncn-m001 is booted as the PIT node using a remote ISO image.</description>
    </item>
    <item>
      <title>Manage Node Consoles</title>
      <link>/docs-csm/en-10/operations/conman/manage_node_consoles/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/conman/manage_node_consoles/</guid>
      <description>Manage Node Consoles ConMan is used for connecting to remote consoles and collecting console logs. These node logs can then be used for various administrative purposes, such as troubleshooting node boot issues.&#xA;ConMan runs on the system in a set of containers within Kubernetes pods named cray-console-operator and cray-console-node.&#xA;The cray-console-operator and cray-console-node pods determine which nodes they should monitor by checking with the Hardware State Manager (HSM) service. They do this once when they starts.</description>
    </item>
    <item>
      <title>Change the Ansible Verbosity Logs</title>
      <link>/docs-csm/en-10/operations/configuration_management/change_the_ansible_verbosity_logs/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/change_the_ansible_verbosity_logs/</guid>
      <description>Change the Ansible Verbosity Logs It is useful to view the Ansible logs in a Configuration Framework Session (CFS) session with greater verbosity than the default. CFS sessions are able to set the Ansible verbosity from the command line when the session is created. The verbosity will apply to all configuration layers in the session.&#xA;Specify an integer using the &amp;ndash;ansible-verbosity option, where 1 = -v, 2 = -vv, and so on.</description>
    </item>
    <item>
      <title>Clean Up After a BOS/BOA Job is Completed or Cancelled</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/clean_up_after_a_bos-boa_job_is_completed_or_cancelled/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/clean_up_after_a_bos-boa_job_is_completed_or_cancelled/</guid>
      <description>Clean Up After a BOS/BOA Job is Completed or Cancelled When a Boot Orchestration Service (BOS) session is created, there are a number of items created on the system. When a session is cancelled or completed, these items need to be cleaned up to ensure there is not lingering content from the session on the system.&#xA;When a session is launched, the following items are created:&#xA;Boot Orchestration Agent (BOA) job: The Kubernetes job that runs and handles the BOS session.</description>
    </item>
    <item>
      <title>Create UAIs From Specific UAI Images in Legacy Mode</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_uais_from_specific_uai_images_in_legacy_mode/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:09 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_uais_from_specific_uai_images_in_legacy_mode/</guid>
      <description>Create UAIs From Specific UAI Images in Legacy Mode A user can create a UAI from a specific UAI image (assuming no default UAI class exists) using a command of the form:&#xA;user&amp;gt; cray uas create --publickey &amp;lt;path&amp;gt; --imagename &amp;lt;image-name&amp;gt; &amp;lt;image-name&amp;gt; is the name shown above in the list of UAI images.&#xA;For example:&#xA;vers&amp;gt; cray uas images list default_image = &amp;#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest&amp;#34; image_list = [ &amp;#34;dtr.dev.cray.com/cray/cray-uai-broker:latest&amp;#34;, &amp;#34;dtr.dev.cray.com/cray/cray-uas-sles15:latest&amp;#34;, &amp;#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest&amp;#34;,] vers&amp;gt; cray uas create --publickey ~/.</description>
    </item>
    <item>
      <title>Redeploying a Chart</title>
      <link>/docs-csm/en-10/operations/csm_product_management/redeploying_a_chart/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:08 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/csm_product_management/redeploying_a_chart/</guid>
      <description>Redeploying a Chart Administrators are able to customize many aspects of the system in order to address problems or tailor it to better suit their requirements. Often this requires redeploying one or more Helm charts. This page outlines the procedure for doing this in CSM. Other parts of the CSM documentation will reference this page if you are instructed to redeploy a chart. In those cases, the source page that links to this one should specify which charts should be redeployed and what customizations (if any) should be made to them.</description>
    </item>
    <item>
      <title>Site Survey Worksheet</title>
      <link>/docs-csm/en-10/introduction/site_survey/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:07 +0000</pubDate>
      <guid>/docs-csm/en-10/introduction/site_survey/</guid>
      <description>Site Survey Worksheet This site survey worksheet identifies information which should be collected in advance of a CSM installation.&#xA;Topics First master node Site time information Site DNS information CAN network ranges Default internal network ranges SHCD csi command line configuration payload csi configuration payload files site-init customizations Application nodes Filesystems 1. First master node (ncn-m001) The first master node (ncn-m001) is also called the PIT node early in the installation process, but later becomes ncn-m001.</description>
    </item>
    <item>
      <title>Wipe NCN Disks for Reinstallation</title>
      <link>/docs-csm/en-10/install/wipe_ncn_disks_for_reinstallation/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:07 +0000</pubDate>
      <guid>/docs-csm/en-10/install/wipe_ncn_disks_for_reinstallation/</guid>
      <description>Wipe NCN Disks for Reinstallation This page details how to wipe NCN disks.&#xA;Everything in this section should be considered DESTRUCTIVE.&#xA;After following these procedures, an NCN can be rebooted and redeployed.&#xA;All types of disk wipe can be run from Linux or from an initramFS/initrd emergency shell.&#xA;The following are potential use cases for wiping disks:&#xA;Adding a node that is not bare. Adopting new disks that are not bare.</description>
    </item>
    <item>
      <title>Ceph CSI Troubleshooting</title>
      <link>/docs-csm/en-10/install/ceph_csi_troubleshooting/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:04 +0000</pubDate>
      <guid>/docs-csm/en-10/install/ceph_csi_troubleshooting/</guid>
      <description>Ceph CSI Troubleshooting If there has been a failure to initialize all Ceph CSI components on ncn-s001, then the storage node cloud-init may need to be rerun.&#xA;Topics: Verify Ceph CSI Rerun Storage Node cloud-init Details 1. Verify Ceph CSI Verify that the ceph-csi requirements are in place&#xA;Log in to ncn-s00 and run this command&#xA;ncn-s001# ceph -s If it returns a connection error then assume Ceph is not installed.</description>
    </item>
    <item>
      <title>NCN Images</title>
      <link>/docs-csm/en-10/background/ncn_images/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:01 +0000</pubDate>
      <guid>/docs-csm/en-10/background/ncn_images/</guid>
      <description>NCN Images Overview of NCN images LiveCD server Overview of NCN images The management non-compute nodes (NCNs) boot from images which are created from layers on top of a common base image. The common image is customized with a kubernetes layer for the master nodes and worker nodes. The common image is customized with a storage-ceph layer for the utility storage nodes.&#xA;When booting NCNs, an administrator will need to choose between stable (Release) and unstable (pre-release/development) images.</description>
    </item>
    <item>
      <title>Cray CLI 403 Forbidden Errors</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/craycli_403_forbidden_errors/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/craycli_403_forbidden_errors/</guid>
      <description>Cray CLI 403 Forbidden Errors There is a known issue where the Keycloak configuration obtained from LDAP is incomplete causing the keycloak-users-localize job to fail to complete. This causes 403 Forbidden errors when trying to use the Cray CLI. This can also cause a Keycloak test to fail during CSM health validation.&#xA;Fix To recover from this situation, the following can be done.&#xA;Log into the Keycloak admin console. See Access the Keycloak User Management UI</description>
    </item>
    <item>
      <title>Ceph Storage Types</title>
      <link>/docs-csm/en-10/operations/utility_storage/ceph_storage_types/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/ceph_storage_types/</guid>
      <description>Ceph Storage Types As a reference, the following ceph and rbd commands are run from a master node or ncn-s001/2/3. Certain commands will work on different systems. For example, the rbd command can be used on the worker nodes if specifying the proper key.&#xA;Ceph Block (rbd) List block devices in a specific pool:&#xA;ncn-m001# rbd -p POOL_NAME ls -l NAME SIZE PARENT FMT PROT LOCK kube_vol 4 GiB 2 Create a block device:</description>
    </item>
    <item>
      <title>System Layout Service (SLS)</title>
      <link>/docs-csm/en-10/operations/system_layout_service/system_layout_service_sls/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:39 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_layout_service/system_layout_service_sls/</guid>
      <description>System Layout Service (SLS) The System Layout Service (SLS) holds information about the system design, such as the physical locations of network hardware, compute nodes, and cabinets. It also stores information about the network, such as which port on which switch should be connected to each compute node.&#xA;SLS stores a generalized abstraction of the system that other services can access. The Hardware State Manager (HSM) keeps track of information for hardware state or identifiers.</description>
    </item>
    <item>
      <title>Change Air-Cooled Node BMC Credentials</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/change_air-cooled_node_bmc_credentials/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:35 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/change_air-cooled_node_bmc_credentials/</guid>
      <description>Change Air-Cooled Node BMC Credentials This procedure will use the System Configuration Service (SCSD) to change all air-cooled Node BMCs in the system to the same global credential.&#xA;Limitations All air-cooled and liquid-cooled BMCs share the same global credentials. The air-cooled Slingshot switch controllers (Router BMCs) must have the same credentials as the liquid-cooled Slingshot switch controllers.&#xA;Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system.</description>
    </item>
    <item>
      <title>Power On and Start the Management Kubernetes Cluster</title>
      <link>/docs-csm/en-10/operations/power_management/power_on_and_start_the_management_kubernetes_cluster/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/power_on_and_start_the_management_kubernetes_cluster/</guid>
      <description>Power On and Start the Management Kubernetes Cluster Power on and start management services on the HPE Cray EX management Kubernetes cluster.&#xA;Prerequisites All management rack PDUs are connected to facility power and facility power is on. An authentication token is required to access the API gateway and to use the sat command. See the &amp;ldquo;SAT Authentication&amp;rdquo; section of the HPE Cray EX System Admin Toolkit (SAT) product stream documentation (S-8031) for instructions on how to acquire a SAT authentication token.</description>
    </item>
    <item>
      <title>Restrict Admin Privileges in Nexus</title>
      <link>/docs-csm/en-10/operations/package_repository_management/restrict_admin_privileges_in_nexus/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/package_repository_management/restrict_admin_privileges_in_nexus/</guid>
      <description>Restrict Admin Privileges in Nexus Prior to making the system available to users, change the ingress settings to disable connections to packages.local and registry.local from automatically gaining administrative privileges.&#xA;Connections to packages.local and registry.local automatically login clients as the admin user. Administrative privileges enable any user to make anonymous writes to Nexus, which means unauthenticated users can perform arbitrary actions on Nexus itself through the REST API, as well as in repositories by uploading or deleting assets.</description>
    </item>
    <item>
      <title>Prepare Storage Node</title>
      <link>/docs-csm/en-10/operations/node_management/rebuild_ncns/prepare_storage_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:31 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/rebuild_ncns/prepare_storage_nodes/</guid>
      <description>Prepare Storage Node Description Prepare a storage node before rebuilding it.&#xA;IMPORTANT: All of the output examples may not reflect the cluster status where this operation is being performed. For example, if this is a rebuild in place, then Ceph components will not be reporting down, in contrast to a failed node rebuild.&#xA;Prerequisites Procedure Next step Prerequisites Ensure that the latest CSM documentation RPM is installed on ncn-m001.&#xA;See Check for Latest Documentation.</description>
    </item>
    <item>
      <title>Remove NCN Data</title>
      <link>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/remove_ncn_data/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/remove_ncn_data/</guid>
      <description>Remove NCN Data Description Remove NCN data to System Layout Service (SLS), Boot Script Service (BSS) and Hardware State Manager (HSM) as needed to remove an NCN.&#xA;Procedure IMPORTANT: The following procedures assume you have set the variables from the prerequisites section&#xA;Prepare for the procedure.&#xA;Obtain an API token.&#xA;ncn-mw# cd /usr/share/docs/csm/scripts/operations/node_management/Add_Remove_Replace_NCNs ncn-mw# export TOKEN=$(curl -s -S -d grant_type=client_credentials -d client_id=admin-client \ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=&amp;#39;{.data.client-secret}&amp;#39; | base64 -d` \ https://api-gw-service-nmn.</description>
    </item>
    <item>
      <title>Update the can-external-dns Value Post-Installation</title>
      <link>/docs-csm/en-10/operations/network/external_dns/update_the_can-external-dns_value_post-installation/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:27 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/network/external_dns/update_the_can-external-dns_value_post-installation/</guid>
      <description>Update the can-external-dns Value Post-Installation By default, the services/cray-externaldns-coredns-tcp and services/cray-externaldns-coredns-udp services both share the same Customer Access Network (CAN) external IP as defined by the can-external-dns value. This value is specified during the csi config init input.&#xA;It is expected to be in the static range reserved in MetalLB&amp;rsquo;s can-dynamic-pool subnet. Theoretically, this is the only CAN IP address that must be known external to the system so IT DNS can delegate the system-name.</description>
    </item>
    <item>
      <title>Check the Health and Balance of etcd Clusters</title>
      <link>/docs-csm/en-10/operations/kubernetes/check_the_health_and_balance_of_etcd_clusters/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:23 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/check_the_health_and_balance_of_etcd_clusters/</guid>
      <description>Check the Health and Balance of etcd Clusters Check to see if all of the etcd clusters have healthy pods, are balanced, and have a healthy cluster database. There needs to be the same number of pods running on each worker node for the etcd clusters to be balanced. If the number of pods is not the same for each worker node, the cluster is not balanced.&#xA;Any clusters that do not have healthy pods will need to be rebuilt.</description>
    </item>
    <item>
      <title>Image Management Workflows</title>
      <link>/docs-csm/en-10/operations/image_management/image_management_workflows/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:23 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/image_management/image_management_workflows/</guid>
      <description>Image Management Workflows Overview of how to create an image and how to customize and image.&#xA;The following workflows are intended to be high-level overviews of image management tasks. These workflows depict how services interact with each other during image management and help to provide a quicker and deeper understanding of how the system functions.&#xA;The workflows in this section include:&#xA;Create a new image Customize an image Create a new image Use Case: The system administrator creates an image root from a customized recipe.</description>
    </item>
    <item>
      <title>HSM Roles and Subroles</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/hsm_roles_and_subroles/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/hsm_roles_and_subroles/</guid>
      <description>HSM Roles and Subroles The Hardware State Manager (HSM) contains several pre-defined roles and subroles that can be assigned to components and used to target specific hardware devices.&#xA;Roles and subroles assignments come from the System Layout Service (SLS) and are applied by HSM when a node is discovered.&#xA;HSM roles HSM subroles Add custom roles and subroles HSM roles The following is a list of all pre-defined roles:&#xA;Management Compute Application Service System Storage The Management role refers to NCNs and will generally have the Master, Worker, or Storage subrole assigned.</description>
    </item>
    <item>
      <title>Upload BMC Recovery Firmware into TFTP Server</title>
      <link>/docs-csm/en-10/operations/firmware/upload_olympus_bmc_recovery_firmware_into_tftp_server/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/firmware/upload_olympus_bmc_recovery_firmware_into_tftp_server/</guid>
      <description>Upload BMC Recovery Firmware into TFTP Server cray-upload-recovery-images is a utility for uploading the BMC recovery files for ChassisBMCs, NodeBMCs, and RouterBMCs to be served by the cray-tftp service. The tool uses the cray CLI (fas, artifacts) and cray-tftp to download the S3 recovery images (as remembered by FAS), then upload them into the PVC that is used by cray-tftp. cray-upload-recovery-images should be run on every system.&#xA;Prerequisites Cray System Management (CSM) software is installed.</description>
    </item>
    <item>
      <title>Troubleshoot ConMan Asking for Password on SSH Connection</title>
      <link>/docs-csm/en-10/operations/conman/troubleshoot_conman_asking_for_password_on_ssh_connection/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/conman/troubleshoot_conman_asking_for_password_on_ssh_connection/</guid>
      <description>Troubleshoot ConMan Asking for Password on SSH Connection If ConMan starts to ask for a password when there is an SSH connection to the node on liquid-cooled hardware, that usually indicates there is a problem with the SSH key that was established on the node BMC. The key may have been replaced or overwritten on the hardware.&#xA;Use this procedure to renew or reinstall the SSH key on the BMCs.</description>
    </item>
    <item>
      <title>Configuration Layers</title>
      <link>/docs-csm/en-10/operations/configuration_management/configuration_layers/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/configuration_layers/</guid>
      <description>Configuration Layers The Configuration Framework Service (CFS) uses configuration layers to specify the location of configuration content that will be applied. Configurations may include one or more layers. Each layer is defined by a Git repository clone URL, a Git commit, a name (optional), and the path in the repository to an Ansible playbook to execute.&#xA;Configurations with a single layer are useful when testing out a new configuration on targets, or when configuring system components with one product at a time.</description>
    </item>
    <item>
      <title>Clean Up Logs After a BOA Kubernetes Job</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/clean_up_logs_after_a_boa_kubernetes_job/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/clean_up_logs_after_a_boa_kubernetes_job/</guid>
      <description>Clean Up Logs After a BOA Kubernetes Job Delete log entries from previous boot orchestration jobs. The Boot Orchestration Service (BOS) launches a Boot Orchestration Agent (BOA) Kubernetes job. BOA then launches a Configuration Framework Service (CFS) session, resulting in a CFS-BOA Kubernetes job. Thus, there are two separate sets of jobs that can be removed.&#xA;Deleting log entries creates more space and helps improve the usability of viewing logs.</description>
    </item>
    <item>
      <title>Create a UAI</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_a_uai/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:09 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_a_uai/</guid>
      <description>Create a UAI It is rare that an an administrator would hand-craft a UAI in this way, but it is possible. This is the mechanism used to create broker UAIs for the broker mode of UAI management.&#xA;Refer to Broker Mode UAI Management for more information.&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Procedure Create a UAI manually.&#xA;Use a command of the following form:&#xA;cray uas admin uais create [options] The following options are available for use:</description>
    </item>
    <item>
      <title>Remove Artifacts from Product Installations</title>
      <link>/docs-csm/en-10/operations/csm_product_management/remove_artifacts_from_product_installations/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:08 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/csm_product_management/remove_artifacts_from_product_installations/</guid>
      <description>Remove Artifacts from Product Installations Remove product artifacts that were imported from various Cray products. These instructions provide guidance for removing Image Management Service (IMS) images, IMS recipes, and Git repositories present in the Cray Product Catalog from the system.&#xA;The examples in this procedure show how to remove the product artifacts for the Cray System Management (CSM) product.&#xA;WARNING: If individual Cray products have removal procedures, those instructions supersede this procedure.</description>
    </item>
    <item>
      <title>Clear Gigabyte CMOS</title>
      <link>/docs-csm/en-10/install/clear_gigabyte_cmos/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:04 +0000</pubDate>
      <guid>/docs-csm/en-10/install/clear_gigabyte_cmos/</guid>
      <description>Clear Gigabyte CMOS Because of a bug in the Gigabyte firmware, the Shasta 1.5 install may negatively impact Gigabyte motherboards when attempting to boot using bonded Mellanox network cards. The result is a board that is unusable until a CMOS clear is physically done via a jumper on the board itself.&#xA;A patched firmware release (newer than C20 BIOS) is expected to be available for a future release of Shasta. It is recommended that Gigabyte users wait for this new firmware before attempting an installation of Shasta 1.</description>
    </item>
    <item>
      <title>NCN Mounts and File Systems</title>
      <link>/docs-csm/en-10/background/ncn_mounts_and_file_systems/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:01 +0000</pubDate>
      <guid>/docs-csm/en-10/background/ncn_mounts_and_file_systems/</guid>
      <description>NCN Mounts and File Systems The management non-compute nodes (NCNs) use drive storage for persistence and block storage. This page outlines reference information for these disks, their partition tables, and their management.&#xA;What controls partitioning Plan of record / baseline Problems when above or below baseline Disk layout quick-reference tables OverlayFS and persistence SQFSRAID and ROOTRAID overlays Helpful commands OverlayFS example mount command losetup command lsblk command Persistent directories Layering: Upper and lower directory Layering: Real world example OverlayFS control Reset toggles Reset on next boot Reset on every boot Re-sizing the persistent overlay Thin overlay feature metalfs Old/retired FS labels What controls partitioning Partitioning is controlled by two aspects:</description>
    </item>
    <item>
      <title>Air-cooled hardware is not getting properly discovered with Aruba leaf switches.</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/discovery_aruba_snmp_issue/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/discovery_aruba_snmp_issue/</guid>
      <description>Air-cooled hardware is not getting properly discovered with Aruba leaf switches. Symptoms: The System has Aruba leaf switches. Air-cooled hardware is reported to not be present under State Components and Inventory Redfish Endpoints in Hardware State Manager by the hsm_discovery_verify.sh script. BMCs have IP addresses given out by DHCP, but in DNS their xname hostname does not resolve. Procedure to determine if you affected by this known issue: Determine the name of the last HSM discovery job that ran.</description>
    </item>
    <item>
      <title>Cephadm Reference Material</title>
      <link>/docs-csm/en-10/operations/utility_storage/cephadm_reference_material/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/cephadm_reference_material/</guid>
      <description>Cephadm Reference Material cephadm is a new function introduced in Ceph Octopus 15. It allows for an easier method to install and manage Ceph nodes.&#xA;The following sections include common examples:&#xA;Invoke Shells to Run Traditional Ceph Commands On ncn-s001/2/3:&#xA;ncn-s00[123]# cephadm shell # creates a container with access to run ceph commands the traditional way Optionally, execute the following command:&#xA;ncn-s00[123]# cephadm shell -- ceph -s Ceph-Volume There are multiple ways to do Ceph device operations now.</description>
    </item>
    <item>
      <title>Update SLS with UAN Aliases</title>
      <link>/docs-csm/en-10/operations/system_layout_service/update_sls_with_uan_aliases/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:39 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/system_layout_service/update_sls_with_uan_aliases/</guid>
      <description>Update SLS with UAN Aliases This guide shows the process for manually adding an alias to a UAN in SLS and ensuring that the node is being monitored by conman for console logs.&#xA;Prerequisites SLS is up and running and has been populated with data. Access to the API gateway api-gw-service (legacy: api-gw-service-nmn.local) Procedure Authenticate with Keycloak to obtain an API token:&#xA;export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \ -d client_id=admin-client \ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=&amp;#39;{.</description>
    </item>
    <item>
      <title>Change Credentials on ServerTech PDUs</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/change_credentials_on_servertech_pdus/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:35 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/change_credentials_on_servertech_pdus/</guid>
      <description>Change Credentials on ServerTech PDUs This procedure changes password used by the admn user on ServerTech PDUs. Either a single PDU can be updated to a new credential, or all ServerTech PDUs in the system can be updated to the same global credentials.&#xA;NOTES:&#xA;This procedure does not update the default credentials that RTS uses for new ServerTech PDUs added to a system. To change the default credentials, see Update default ServerTech PDU Credentials used by the Redfish Translation Service.</description>
    </item>
    <item>
      <title>Power On the External Lustre File System</title>
      <link>/docs-csm/en-10/operations/power_management/power_on_the_external_lustre_file_system/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/power_on_the_external_lustre_file_system/</guid>
      <description>Power On the External Lustre File System Use this procedure as a general guide to power on an external ClusterStor system. Refer to the detailed procedures that support each ClusterStor hardware and software release:&#xA;ClusterStor E1000 Administration Guide 4.2 - S-2758 for ClusterStor E1000 systems ClusterStor Administration Guide 3.4 - S-2756 for ClusterStor L300, L300N systems ClusterStor Administration Guide - S-2755 for Legacy ClusterStor systems Power up storage nodes in the following sequence:</description>
    </item>
    <item>
      <title>Troubleshoot Nexus</title>
      <link>/docs-csm/en-10/operations/package_repository_management/troubleshoot_nexus/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/package_repository_management/troubleshoot_nexus/</guid>
      <description>Troubleshoot Nexus This page contains general Nexus troubleshooting topics.&#xA;lookup registry.local: no such host Error initiating layer upload &amp;hellip; in registry.local error: not ready: https://packages.local lookup registry.local: no such host The following error may occur when running ./lib/setup-nexus.sh:&#xA;time=&amp;#34;2021-02-23T19:55:54Z&amp;#34; level=fatal msg=&amp;#34;Error copying tag \&amp;#34;dir:/image/grafana/grafana:7.0.3\&amp;#34;: Error writing blob: Head \&amp;#34;https://registry.local/v2/grafana/grafana/blobs/sha256:cf254eb90de2dc62aa7cce9737ad7e143c679f5486c46b742a1b55b168a736d3\&amp;#34;: dial tcp: lookup registry.local: no such host&amp;#34; + return Or a similar error:&#xA;time=&amp;#34;2021-03-04T22:45:07Z&amp;#34; level=fatal msg=&amp;#34;Error copying ref \&amp;#34;dir:/image/cray/cray-ims-load-artifacts:1.0.4\&amp;#34;: Error trying to reuse blob sha256:1ec886c351fa4c330217411b0095ccc933090aa2cd7ae7dcd33bb14b9f1fd217 at destination: Head \&amp;#34;https://registry.</description>
    </item>
    <item>
      <title>Prepare Worker Node</title>
      <link>/docs-csm/en-10/operations/node_management/rebuild_ncns/prepare_worker_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:31 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/rebuild_ncns/prepare_worker_nodes/</guid>
      <description>Prepare Worker Node Description Prepare a worker node before rebuilding it.&#xA;Procedure Step 1 - Determine if the worker being rebuilt is running the cray-cps-cm-pm pod If the cray-cps-cm-pm pod is running, there will be an extra step to redeploy this pod after the node is rebuilt.&#xA;Run the following on any node where the cray cli has been initialized:&#xA;cray cps deployment list --format json | grep -C1 podname Example output:</description>
    </item>
    <item>
      <title>Remove Roles</title>
      <link>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/remove_ncn_from_role/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/remove_ncn_from_role/</guid>
      <description>Remove Roles Description Remove master, worker, or storage NCN from current roles. Select the procedure below based on the node type, complete the remaining steps to wipe the drives, and then power off the node.&#xA;Procedure IMPORTANT: The following procedures assume that the variables from the prerequisites section have been set.&#xA;Remove roles Master node Worker node Storage node Wipe the drives Master node Worker node Storage node Power off the node Next step 1.</description>
    </item>
    <item>
      <title>Clear Space in an etcd Cluster Database</title>
      <link>/docs-csm/en-10/operations/kubernetes/clear_space_in_an_etcd_cluster_database/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:23 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/clear_space_in_an_etcd_cluster_database/</guid>
      <description>Clear Space in an etcd Cluster Database Use this procedure to clear the etcd cluster NOSPACE alarm. Once it is set it will remain set. If needed, defrag the database cluster before clearing the NOSPACE alarm.&#xA;Defragging the database cluster and clearing the etcd cluster NOSPACE alarm will free up database space.&#xA;Prerequisites This procedure requires root privileges. The etcd clusters are in a healthy state. Procedure Clear up space when the etcd database space has exceeded and has been defragged, but the NOSPACE alarm remains set.</description>
    </item>
    <item>
      <title>Import an External Image to IMS</title>
      <link>/docs-csm/en-10/operations/image_management/import_external_image_to_ims/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:23 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/image_management/import_external_image_to_ims/</guid>
      <description>Import an External Image to IMS The Image Management Service (IMS) is typically used to build images from IMS recipes and customize Images that are already known to IMS. However, it is sometimes the case that an image is built using a mechanism other than by IMS and needs to be added to IMS. In these cases, the following procedure can be used to add this external image to IMS and upload the image&amp;rsquo;s artifact(s) to the Simple Storage Service (S3).</description>
    </item>
    <item>
      <title>Hardware Management Services (HMS) Locking API</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/hardware_management_services_hms_locking_api/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/hardware_management_services_hms_locking_api/</guid>
      <description>Hardware Management Services (HMS) Locking API The locking feature is a part of the Hardware State Manager (HSM) API. The locking API enables administrators to lock components on the system. Locking components ensures other system actors, such as administrators or running services, cannot perform a firmware update with the Firmware Action Service (FAS) or a power state change with the Cray Advanced Platform Monitoring and Control (CAPMC). Locks only constrain FAS and CAPMC from each other and help ensure that a firmware update action will not be interfered with by a request to power off the device through CAPMC.</description>
    </item>
    <item>
      <title>Troubleshoot ConMan Blocking Access to a Node BMC</title>
      <link>/docs-csm/en-10/operations/conman/troubleshoot_conman_blocking_access_to_a_node_bmc/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/conman/troubleshoot_conman_blocking_access_to_a_node_bmc/</guid>
      <description>Troubleshoot ConMan Blocking Access to a Node BMC Disable ConMan if it is blocking access to a node by other means. ConMan runs on the system as a containerized service, and it is enabled by default. However, the use of ConMan to connect to a node blocks access to that node by other Serial over LAN (SOL) utilities or by a virtual KVM.&#xA;For information about how ConMan works, see ConMan.</description>
    </item>
    <item>
      <title>Configuration Management</title>
      <link>/docs-csm/en-10/operations/configuration_management/configuration_management/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/configuration_management/</guid>
      <description>Configuration Management The Configuration Framework Service (CFS) is available on systems for remote execution and configuration management of nodes and boot images. This includes nodes available in the Hardware State Manager (HSM) inventory (compute, management, and application nodes), and boot images hosted by the Image Management Service (IMS).&#xA;CFS configures nodes and images via a gitops methodology. All configuration content is stored in a version control service (VCS), and is managed by authorized system administrators.</description>
    </item>
    <item>
      <title>Compute Node Boot Issue Symptom Duplicate Address Warnings and Declined DHCP Offers in Logs</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/compute_node_boot_issue_symptom_duplicate_address_warnings_and_declined_dhcp_offers_in_logs/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/compute_node_boot_issue_symptom_duplicate_address_warnings_and_declined_dhcp_offers_in_logs/</guid>
      <description>Compute Node Boot Issue Symptom: Duplicate Address Warnings and Declined DHCP Offers in Logs If the DHCP and node logs show duplicate address warnings and indicate declined DHCP offers, it may be because another component owns the IP address that DHCP is trying to assign to a node. If this happens, the node will not accept the IP address and will repeatedly submit a DHCP discover request. As a result, the node and DHCP become entangled in a loop of requesting and rejecting.</description>
    </item>
    <item>
      <title>Create a UAI Class</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_a_uai_class/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:09 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_a_uai_class/</guid>
      <description>Create a UAI Class Add a new User Access Instance (UAI) class to the User Access Service (UAS) so that the class can be used to configure UAIs.&#xA;Prerequisites Install and initialize the cray administrative CLI.&#xA;Procedure Add a UAI class by using the command in the following example.&#xA;ncn-m001-pit# cray uas admin config classes create --image-id &amp;lt;image-id&amp;gt; [options] --image-id &amp;lt;image-id&amp;gt; specifies the UAI image identifier of the UAI image to be used in creating UAIs of the new class.</description>
    </item>
    <item>
      <title>Validate Signed RPMs</title>
      <link>/docs-csm/en-10/operations/csm_product_management/validate_signed_rpms/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:08 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/csm_product_management/validate_signed_rpms/</guid>
      <description>Validate Signed RPMs The HPE Cray EX system signs RPMs to provide an extra level of security. Use the following procedure to import a key from either My HPE Software Center or a Kubernetes Secret, and then use that key to validate the RPM package signatures on each node type.&#xA;The RPMs will vary on compute, application, worker, master, and storage nodes. Check each node type to ensure the RPMs are correctly signed.</description>
    </item>
    <item>
      <title>Collect MAC Addresses for NCNs</title>
      <link>/docs-csm/en-10/install/collect_mac_addresses_for_ncns/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:04 +0000</pubDate>
      <guid>/docs-csm/en-10/install/collect_mac_addresses_for_ncns/</guid>
      <description>Collect MAC Addresses for NCNs Now that the PIT node has been booted with the LiveCD and the management network switches have been configured, the actual MAC address for the management nodes can be collected. This process will include repetition of some of the steps done up to this point because csi config init will need to be run with the proper MAC addresses and some services will need to be restarted.</description>
    </item>
    <item>
      <title>NCN Networking</title>
      <link>/docs-csm/en-10/background/ncn_networking/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:01 +0000</pubDate>
      <guid>/docs-csm/en-10/background/ncn_networking/</guid>
      <description>NCN Networking Non-compute nodes (NCNs) and compute nodes (CNs) have different network interfaces used for booting; this topic focuses on the network interfaces for management NCNs.&#xA;NCN network interfaces Device naming Vendor and bus ID identification NCN network interfaces The following table includes information about the different NCN network interfaces:&#xA;Name Type MTU mgmt0 Slot 1 on the SMNET card 9000 mgmt1 Slot 2 on the SMNET card or slot 1 on the second SMNET card 9000 bond0 LACP link aggregate of mgmt0 and mgmt1, or mgmt0 and mgmt2 on dual-bonds (when bond1 is present) 9000 bond1 LACP link aggregate of mgmt1 and mgmt3 9000 lan0 Externally facing interface 1500 lan1 Another externally facing interface, or anything (unused) 1500 hsn0 High-Speed Network interface 9000 hsnN+1 Another high-speed network interface 9000 vlan002 Virtual LAN for managing nodes 1500 vlan004 Virtual LAN for managing hardware 1500 vlan007 Virtual LAN for the Customer Access Network 1500 These interfaces can be observed on an NCN with the following command.</description>
    </item>
    <item>
      <title>HMS Discovery Job Not Creating RedfishEndpoints In Hardware State Manager</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/discovery_job_not_creating_redfish_endpoints/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/discovery_job_not_creating_redfish_endpoints/</guid>
      <description>HMS Discovery Job Not Creating RedfishEndpoints In Hardware State Manager It is a known issue with the HMS Discovery cronjob that when a BMC does not respond by its IP address, the discovery job will not create a RedfishEndpoint for the BMC in Hardware State Manager (HSM). However, it does update the BMC MAC address in HSM with its component name (xname). The discovery job only creates a new RedfishEndpoints when it encounters an unknown MAC address without a component name (xname) associated with it.</description>
    </item>
    <item>
      <title>Collect Information about the Ceph Cluster</title>
      <link>/docs-csm/en-10/operations/utility_storage/collect_information_about_the_ceph_cluster/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/collect_information_about_the_ceph_cluster/</guid>
      <description>Collect Information about the Ceph Cluster These general commands for Ceph are helpful for obtaining information pertinent to troubleshooting issues.&#xA;As a reference, the Ceph commands below are run from a ceph-mon node. Certain commands will work on different systems. For example, the rbd command can be used on the worker nodes if specifying the proper key.&#xA;Ceph Log and File Locations Ceph configurations are located under /etc/ceph/ceph.conf Ceph data structure and bootstrap is located under /var/lib/ceph// Ceph logs are now accessible by a couple of different methods Utilizing cephadm ls to retrieve the systemd_unit on the node for the process, then utilize journalctl to dump the logs ceph log last [&amp;lt;num:int&amp;gt;] [debug|info|sec|warn|error] [*|cluster|audit|cephadm] Note that that this will dump general cluster logs cephadm logs [-h] [--fsid FSID] --name &amp;lt;systemd_unit&amp;gt; Check the Status of Ceph Print the status of the Ceph cluster with the following command:</description>
    </item>
    <item>
      <title>Change Cray EX Liquid-Cooled Cabinet Global Default Password</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/change_ex_liquid-cooled_cabinet_global_default_password/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:35 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/change_ex_liquid-cooled_cabinet_global_default_password/</guid>
      <description>Change Cray EX Liquid-Cooled Cabinet Global Default Password This procedure changes the global default root credential on HPE Cray EX liquid-cooled cabinet embedded controllers (BMCs). The chassis management module (CMM) controller (cC), node controller (nC), and Slingshot switch controller (sC) are generically referred to as &amp;ldquo;BMCs&amp;rdquo; in these procedures.&#xA;Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray Command Line Interface (cray CLI) for more information.</description>
    </item>
    <item>
      <title>Prepare the System for Power Off</title>
      <link>/docs-csm/en-10/operations/power_management/prepare_the_system_for_power_off/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/prepare_the_system_for_power_off/</guid>
      <description>Prepare the System for Power Off This procedure prepares the system to remove power from all system cabinets. Be sure the system is healthy and ready to be shut down and powered off.&#xA;The sat bootsys shutdown and sat bootsys boot commands are used to shut down the system.&#xA;Prerequisites An authentication token is required to access the API gateway and to use the sat command. See the &amp;ldquo;SAT Authentication&amp;rdquo; section of the HPE Cray EX System Admin Toolkit (SAT) product stream documentation (S-8031) for instructions on how to acquire a SAT authentication token.</description>
    </item>
    <item>
      <title>Adding a Ceph Node to the Ceph Cluster</title>
      <link>/docs-csm/en-10/operations/node_management/rebuild_ncns/re-add_storage_node_to_ceph/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:31 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/rebuild_ncns/re-add_storage_node_to_ceph/</guid>
      <description>Adding a Ceph Node to the Ceph Cluster NOTE: This operation can be done to add more than one node at the same time.&#xA;Add Join Script Copy and paste the below script into /srv/cray/scripts/common/join_ceph_cluster.sh&#xA;NOTE: This script may also available in the /usr/share/doc/csm/scripts directory where the latest docs-csm RPM is installed. If so, it can be copied from that node to the new storage node being rebuilt and skip to step 2.</description>
    </item>
    <item>
      <title>Remove Switch Configuration for NCN</title>
      <link>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/remove_switch_config/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/remove_switch_config/</guid>
      <description>Remove Switch Configuration for NCN Update the network switches for the NCN that was removed.&#xA;Update Networking to Remove NCN ncn-w004 IP data 10.102.4.15 ncn-w004.can 10.254.1.22 ncn-w004.hmn 10.1.1.11 ncn-w004.mtl 10.252.1.13 ncn-w004.nmn ncn-w004 10.254.1.21 ncn-w004-mgmt spine-01 switch updates no route-map ncn-w004 permit 10 match ip address pl-can no route-map ncn-w004 permit 10 set ip next-hop 10.102.4.15 no route-map ncn-w004 permit 20 match ip address pl-hmn no route-map ncn-w004 permit 20 set ip next-hop 10.</description>
    </item>
    <item>
      <title>Configure kubectl Credentials to Access the Kubernetes APIs</title>
      <link>/docs-csm/en-10/operations/kubernetes/configure_kubectl_credentials_to_access_the_kubernetes_apis/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:23 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/configure_kubectl_credentials_to_access_the_kubernetes_apis/</guid>
      <description>Configure kubectl Credentials to Access the Kubernetes APIs The credentials for kubectl are located in the admin configuration file on all non-compute node (NCN) master and worker nodes. They can be found at /etc/kubernetes/admin.conf for the root user. Use kubectl to access the Kubernetes cluster from a device outside the cluster.&#xA;For more information, refer to the Kubernetes home page.&#xA;Prerequisites This procedure requires administrative privileges and assumes that the device being used has:</description>
    </item>
    <item>
      <title>Upload and Register an Image Recipe</title>
      <link>/docs-csm/en-10/operations/image_management/upload_and_register_an_image_recipe/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:23 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/image_management/upload_and_register_an_image_recipe/</guid>
      <description>Upload and Register an Image Recipe Download and expand recipe archives from S3 and IMS. Modify and upload a recipe archive, and then register that recipe archive with IMS.&#xA;Prerequisites Limitations Register recipe with IMS Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCNs) and include the following deployment: cray-ims, the Image Management Service (IMS) The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system.</description>
    </item>
    <item>
      <title>Hardware State Manager (HSM)</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/hardware_state_manager/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/hardware_state_manager/</guid>
      <description>Hardware State Manager (HSM) The Hardware State Manager (HSM) monitors and interrogates hardware components in the HPE Cray EX system, tracking hardware state and inventory information, and making it available via REST queries and message bus events when changes occur.&#xA;In the CSM 0.9.3 release, v1 of the HSM API has begun its deprecation process in favor of the new HSM v2 API. Refer to the HSM API documentation for more information on the changes.</description>
    </item>
    <item>
      <title>Troubleshoot ConMan Failing to Connect to a Console</title>
      <link>/docs-csm/en-10/operations/conman/troubleshoot_conman_failing_to_connect_to_a_console/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/conman/troubleshoot_conman_failing_to_connect_to_a_console/</guid>
      <description>Troubleshoot ConMan Failing to Connect to a Console There are many reasons that ConMan may not be able to connect to a specific console. This procedure outlines several things to check that may impact the connectivity with a console.&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Procedure Find the cray-console-operator pod.&#xA;ncn-mw# OP_POD=$(kubectl get pods -n services -o wide|grep cray-console-operator|awk &amp;#39;{print $1}&amp;#39;) ncn-mw# echo ${OP_POD} Example output:&#xA;cray-console-operator-6cf89ff566-kfnjr Set the XNAME variable to the component name (xname) of the node whose console is of interest.</description>
    </item>
    <item>
      <title>Configuration Management of System Components</title>
      <link>/docs-csm/en-10/operations/configuration_management/configuration_management_of_system_components/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/configuration_management_of_system_components/</guid>
      <description>Configuration Management of System Components The configuration of individual system components is managed with the cray cfs components command. The Configuration Framework Service (CFS) contains a database of the configuration state of available hardware known to the Hardware State Manager (HSM). When new nodes are added to the HSM database, a CFS Hardware Sync Agent enters the component into the CFS database with a null state of configuration.&#xA;Administrators are able to set a desired CFS configuration for each component, and the CFS Batcher ensures the desired configuration state and the current configuration state match.</description>
    </item>
    <item>
      <title>Compute Node Boot Issue Symptom Message About Invalid EEPROM Checksum in Node Console or Log</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/compute_node_boot_issue_symptom_message_about_invalid_eeprom_checksum_in_node_console_or_log/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/compute_node_boot_issue_symptom_message_about_invalid_eeprom_checksum_in_node_console_or_log/</guid>
      <description>Compute Node Boot Issue Symptom: Message About Invalid EEPROM Checksum in Node Console or Log On rare occasions, the processor hardware may lose the Serial Over Lan (SOL) connections and may need to be reseated to allow the node to successfully boot.&#xA;Symptoms This issue can be identified if the following is displayed in the node&amp;rsquo;s console or log:&#xA;console.38:2018-09-08 04:54:51 [ 16.721165] ixgbe 0000:18:00.0: The EEPROM Checksum Is Not Valid console.</description>
    </item>
    <item>
      <title>Create a UAI Resource Specification</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_a_uai_resource_specification/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:09 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_a_uai_resource_specification/</guid>
      <description>Create a UAI Resource Specification Add a resource specification to UAS. Once added, a resource specification can be used to limit UAI resource consumption on host nodes and enable UAIs to access external data.&#xA;Prerequisites Install and initialize the cray administrative CLI.&#xA;Procedure Add a resource specification.&#xA;Use a command of the following form:&#xA;ncn-m001-pit # cray uas admin config resources create [--limit &amp;lt;k8s-resource-limit&amp;gt;] [--request &amp;lt;k8s-resource-request&amp;gt;] [--comment &amp;#39;&amp;lt;string&amp;gt;&amp;#39;] For example:</description>
    </item>
    <item>
      <title>Component Names (xnames)</title>
      <link>/docs-csm/en-10/operations/component_names_xnames/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:08 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/component_names_xnames/</guid>
      <description>Component Names (xnames) Component names (xnames) identify the geolocation for hardware components in the HPE Cray EX system. Every component is uniquely identified by these component names. Some, like the system cabinet number or the CDU number, can be changed by site needs. There is no geolocation encoded within the cabinet number, such as an X-Y coordinate system to relate to the floor layout of the cabinets. Other component names refer to the location within a cabinet and go down to the port on a card or switch or the socket holding a processor or a memory DIMM location.</description>
    </item>
    <item>
      <title>Collecting the BMC MAC Addresses</title>
      <link>/docs-csm/en-10/install/collecting_bmc_mac_addresses/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:04 +0000</pubDate>
      <guid>/docs-csm/en-10/install/collecting_bmc_mac_addresses/</guid>
      <description>Collecting the BMC MAC Addresses This guide will detail how to collect BMC MAC addresses from an HPE Cray EX system with configured switches. The BMC MAC address is the exclusive, dedicated LAN for the onboard BMC.&#xA;Results may vary if an unconfigured switch is being used.&#xA;Prerequisites There is a configured switch with SSH access or unconfigured with COM access (Serial Over LAN/DB-9). A file is available to record the collected BMC information.</description>
    </item>
    <item>
      <title>NCN Operating System Releases</title>
      <link>/docs-csm/en-10/background/ncn_operating_system_releases/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:01 +0000</pubDate>
      <guid>/docs-csm/en-10/background/ncn_operating_system_releases/</guid>
      <description>NCN Operating System Releases The management non-compute nodes (NCNs) define their products per image layer:&#xA;Kubernetes NCN images are always SLE_HPC (SuSE High Performance Computing) Ceph Storage NCN images are always SLE_HPC (SuSE High Performance Computing) with SES (SuSE Enterprise Storage) The sles-release RPM is uninstalled for NCNs, and instead, the sle_HPC-release RPM is installed. These both provide the same files, but differ for os-release and /etc/product.d/baseproduct.&#xA;The ses-release RPM is installed on top of the sle_HPC-release RPM in the Ceph images.</description>
    </item>
    <item>
      <title>Gitea/VCS 401 Errors</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/gitea_vcs_401_errors/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/gitea_vcs_401_errors/</guid>
      <description>Gitea/VCS 401 Errors Summary During fresh installs of csm-1.0.x, creation of the main admin user for gitea/VCS (Version Control Service) may fail. In this case, calls to the gitea/VCS API that require authentication will return 401 status codes with a message of token is required. The workaround is to manually create the admin user.&#xA;Symptoms During a fresh install, if this problem occurs, it will typically be noticed during the first run of the Validate CSM Health procedure when the SMS Health Checks are performed.</description>
    </item>
    <item>
      <title>Dump Ceph Crash Data</title>
      <link>/docs-csm/en-10/operations/utility_storage/dump_ceph_crash_data/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/dump_ceph_crash_data/</guid>
      <description>Dump Ceph Crash Data Ceph includes an option to dump crash data. Retrieve this data to get more information on a Ceph cluster that has crashed.&#xA;Prerequisites Ceph is reporting the cluster [WRN] overall HEALTH_WARN 1 daemons have recently crashed error in the output of the ceph -s or ceph health detail commands.&#xA;Procedure Get the Ceph crash listing and the corresponding IDs.&#xA;ncn-m001# ceph crash ls ID ENTITY NEW 2021-02-02_13:45:18.</description>
    </item>
    <item>
      <title>Change NCN Image Root Password and SSH Keys</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/change_ncn_image_root_password_and_ssh_keys/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:35 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/change_ncn_image_root_password_and_ssh_keys/</guid>
      <description>Change NCN Image Root Password and SSH Keys The default SSH keys in the NCN image must be removed. The default password for the root user must be changed. Customize the NCN images by changing the root password or adding different SSH keys for the root account. This procedure shows this process being done any time after the first time installation of the CSM software has been completed and the PIT node is booted as a regular master node.</description>
    </item>
    <item>
      <title>Recover from a Liquid Cooled Cabinet EPO Event</title>
      <link>/docs-csm/en-10/operations/power_management/recover_from_a_liquid_cooled_cabinet_epo_event/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/recover_from_a_liquid_cooled_cabinet_epo_event/</guid>
      <description>Recover from a Liquid Cooled Cabinet EPO Event Identify an emergency power off (EPO) has occurred and restore cabinets to a healthy state.&#xA;CAUTION: Verify the reason why the EPO occurred and resolve that problem before clearing the EPO state.&#xA;If a Cray EX liquid-cooled cabinet or cooling group experiences an EPO event, the compute nodes may not boot. Use CAPMC to force off all the chassis affected by the EPO event.</description>
    </item>
    <item>
      <title>6. Validate BOOTRAID artifacts</title>
      <link>/docs-csm/en-10/operations/node_management/rebuild_ncns/validate_boot_raid/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:31 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/rebuild_ncns/validate_boot_raid/</guid>
      <description>6. Validate BOOTRAID artifacts Perform the following steps on ncn-m001.&#xA;Initialize the cray command and follow the prompts (required for the next step).&#xA;ncn-m001# cray init Run the script to ensure the local BOOTRAID has a valid kernel and initrd&#xA;ncn-m001# /opt/cray/tests/install/ncn/scripts/validate-bootraid-artifacts.sh WAR CASMINST-2015 As a result of rebuilding any NCN(s), remove any dynamically assigned interface IP addresses that did not get released automatically by running the CASMINST-2015 script:&#xA;ncn-m001# /usr/share/doc/csm/scripts/CASMINST-2015.</description>
    </item>
    <item>
      <title>Update Firmware</title>
      <link>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/update_firmware/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/update_firmware/</guid>
      <description>Update Firmware Description Use FAS to update the firmware and set the BMC password.&#xA;Procedure Open a new tab and follow Update Firmware.&#xA;Proceed to the next step to Boot NCN and Configure or return to the main Add, Remove, Replace, or Move NCNs page.</description>
    </item>
    <item>
      <title>containerd</title>
      <link>/docs-csm/en-10/operations/kubernetes/containerd/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:24 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/containerd/</guid>
      <description>containerd containerd is a container runtime (systemd service) that runs on the host. It is used to run containers on the Kubernetes platform.&#xA;/var/lib/containerd filling up containerd slow startup after reboot Restarting containerd on a worker NCN /var/lib/containerd filling up In older versions of containerd, there are cases where the /var/lib/containerd directory fills up. In the event that this occurs, the following steps can be used to remediate the issue.</description>
    </item>
    <item>
      <title>Hardware State Manager (HSM) State and Flag Fields</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/hardware_state_manager_hsm_state_and_flag_fields/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:21 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/hardware_state_manager_hsm_state_and_flag_fields/</guid>
      <description>Hardware State Manager (HSM) State and Flag Fields HSM manages important information for hardware components in the system. Administrators can use the data returned by HSM to learn about the state of the system. To do so, it is critical that the State and Flag fields are understood, and the next steps to take are known when viewing output returned by HSM commands. It is also beneficial to understand what services can cause State or Flag changes in HSM.</description>
    </item>
    <item>
      <title>Configuration Management with the CFS Batcher</title>
      <link>/docs-csm/en-10/operations/configuration_management/configuration_management_with_the_cfs_batcher/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/configuration_management_with_the_cfs_batcher/</guid>
      <description>Configuration Management with the CFS Batcher Creating configuration sessions with the Configuration Framework Service (CFS) enables remote execution for configuring live nodes and boot images prior to booting. CFS also provides its Batcher component for configuration management of registered system components. The CFS Batcher periodically examines the aggregated configuration state of registered components and schedules CFS sessions against those that have not been configured to their desired state. The frequency of scheduling, the maximum number of components to schedule in the same CFS session, and the expiration time for scheduling less than full sessions are configurable.</description>
    </item>
    <item>
      <title>Compute Node Boot Issue Symptom Node is Not Able to Download the Required Artifacts</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/compute_node_boot_issue_symptom_node_is_not_able_to_download_the_required_artifacts/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/compute_node_boot_issue_symptom_node_is_not_able_to_download_the_required_artifacts/</guid>
      <description>Compute Node Boot Issue Symptom: Node is Not Able to Download the Required Artifacts If either or both of the kernel or the initrd boot artifacts are missing from the artifact repository, Boot Script Service (BSS), or both, the node will not be able to download the required boot artifacts and will fail to boot.&#xA;Symptoms The node&amp;rsquo;s console or log will display lines beginning with, &amp;lsquo;&amp;rsquo;Could not start download&amp;rsquo;&amp;rsquo;. Refer to the image below for an example of this error message.</description>
    </item>
    <item>
      <title>Create a UAI Using a Direct Administrative Command</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_a_uai_using_a_direct_administrative_command/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:09 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_a_uai_using_a_direct_administrative_command/</guid>
      <description>Create a UAI Using a Direct Administrative Command Administrators can use this method to manually create UAIs. This method is intended more for creating broker UAIs than for creating end-user UAIs.&#xA;Prerequisites Install and initialize the cray administrative CLI.&#xA;Procedure This method is intended more for creating broker UAIs than for creating end-user UAIs. Administrators can, however, create end-user UAIs using this method.&#xA;Create a UAI manually with a command of the form:</description>
    </item>
    <item>
      <title>Collecting NCN MAC Addresses</title>
      <link>/docs-csm/en-10/install/collecting_ncn_mac_addresses/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:04 +0000</pubDate>
      <guid>/docs-csm/en-10/install/collecting_ncn_mac_addresses/</guid>
      <description>Collecting NCN MAC Addresses This procedure details how to collect the NCN MAC addresses from an HPE Cray EX system. The MAC addresses needed for the Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1 columns in ncn_metadata.csv will be collected. This data will feed into the cloud-init metadata.&#xA;The Bootstrap MAC address will be used for identification of this node during the early part of the PXE boot process, before the bonded interface can be established.</description>
    </item>
    <item>
      <title>NCN Packages</title>
      <link>/docs-csm/en-10/background/ncn_packages/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:01 +0000</pubDate>
      <guid>/docs-csm/en-10/background/ncn_packages/</guid>
      <description>NCN Packages The management nodes boot from images which have many (RPM) packages installed. Lists of the packages for these images are generated on running nodes. A list of images can be collected by running a zypper command on one of the storage, worker, or master nodes.&#xA;Kubernetes Images The Kubernetes image is used to boot the master nodes and worker nodes.&#xA;Collection ncn-w002# zypper --disable-repositories se --installed-only | grep i+ | tr -d &amp;#39;|&amp;#39; | awk &amp;#39;{print $2}&amp;#39; The List SLE_HPC SLE_HPC-release acpid apparmor-profiles arptables at autoyast2 autoyast2-installation base bc bind-utils biosdevname ceph-common cfs-state-reporter cloud-init conntrack-tools cpupower crash cray-cos-release cray-cps-utils cray-dvs-kmp-default cray-dvs-service cray-heartbeat cray-lustre-client cray-lustre-client-devel cray-lustre-client-kmp-default cray-network-config cray-node-identity cray-orca cray-power-button cray-sat-podman craycli-wrapper createrepo_c dnsmasq dosfstools dracut-kiwi-live dracut-metal-mdsquash dump e2fsprogs ebtables emacs ethtool expect fping gdb genders git-core glibc gnuplot gperftools gptfdisk grub2 haproxy hpe-csm-scripts hplip ipmitool iproute2-bash-completion ipset ipvsadm irqbalance java-1_8_0-ibm java-1_8_0-ibm-plugin kdump kdumpid keepalived kernel-mft-mlnx-kmp-default kernel-source kernel-syms kexec-tools kmod-bash-completion kubeadm kubectl kubelet less libcanberra-gtk0 libecpg6 libpq5 libunwind-devel ltrace lvm2 man mariadb mariadb-tools mdadm minicom mlocate nmap numactl open-lldp patterns-base-base pdsh perf perl-MailTools perl-doc pixz podman podman-cni-config postgresql postgresql-contrib postgresql-docs postgresql-server python python-urlgrabber python3-Jinja2 python3-MarkupSafe python3-click python3-colorama python3-curses python3-ipaddr python3-lxml python3-netaddr python3-pexpect python3-pip python3-rpm python3-sip rarpd rasdaemon rsync rsyslog rzsz screen slingshot-network-config smartmontools socat spire-agent squashfs squid strace sudo sysstat systemtap tar tcpdump unixODBC usbutils vim wget wireshark xfsdump yast2-add-on yast2-bootloader yast2-country yast2-network yast2-services-manager yast2-users yum-metadata-parser zip CEPH The Ceph image is used to boot the utility storage nodes.</description>
    </item>
    <item>
      <title>BOS/BOA Incorrect command is output to rerun a failed operation.</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/incorrect_output_for_bos_command_rerun/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/incorrect_output_for_bos_command_rerun/</guid>
      <description>BOS/BOA Incorrect command is output to rerun a failed operation. When the Boot Orchestration Agent (BOA), an agent of the Boot Orchestration Service (BOS), encounters a failure, it issues a command to rerun the operation for any nodes that experienced the failure. However, the syntax of this command is faulty.&#xA;The faulty command includes squiggly braces around a comma separated list of quoted nodes. These squiggly braces, single quotes, and the spaces separating the individual nodes all need to be removed.</description>
    </item>
    <item>
      <title>Identify Ceph Latency Issues</title>
      <link>/docs-csm/en-10/operations/utility_storage/identify_ceph_latency_issues/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/identify_ceph_latency_issues/</guid>
      <description>Identify Ceph Latency Issues Examine the output of the ceph -s command to get context for potential issues causing latency.&#xA;Troubleshoot the underlying causes for the ceph -s command reporting slow PGs.&#xA;Prerequisites This procedure requires admin privileges.&#xA;Procedure View the status of Ceph.&#xA;ncn-m001# ceph -s cluster: id: 73084634-9534-434f-a28b-1d6f39cf1d3d health: HEALTH_WARN 1 filesystem is degraded 1 MDSs report slow metadata IOs Reduced data availability: 15 pgs inactive, 15 pgs peering 46 slow ops, oldest one blocked for 1395 sec, daemons [osd,2,osd,5,mon,ceph-1,mon,ceph-2,mon,ceph-3] have slow ops.</description>
    </item>
    <item>
      <title>Change NCN Image Root Password and SSH Keys on PIT Node</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/change_ncn_image_root_password_and_ssh_keys_on_pit_node/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:35 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/change_ncn_image_root_password_and_ssh_keys_on_pit_node/</guid>
      <description>Change NCN Image Root Password and SSH Keys on PIT Node The default SSH keys in the NCN image must be removed. The default password for the root user must be changed. Customize the NCN images by changing the root password and adding different SSH keys for the root account. This procedure shows this process being done on the PIT node during a first time installation of the CSM software.</description>
    </item>
    <item>
      <title>Save Management Network Switch Configuration Settings</title>
      <link>/docs-csm/en-10/operations/power_management/save_management_network_switch_configurations/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/save_management_network_switch_configurations/</guid>
      <description>Save Management Network Switch Configuration Settings Switches must be powered on and operating. This procedure is optional if switch configurations have not changed.&#xA;Optional Task: Save management network switch configurations before removing power from cabinets or the CDU. Management switch names are listed in the /etc/hosts file.&#xA;Obtain the list of switches From the command line on any NCN run:&#xA;ncn# grep &amp;#39;sw-&amp;#39; /etc/hosts Example output:&#xA;10.252.0.2 sw-spine-001 10.252.0.3 sw-spine-002 10.</description>
    </item>
    <item>
      <title>Wipe Disks</title>
      <link>/docs-csm/en-10/operations/node_management/rebuild_ncns/wipe_drives/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:31 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/rebuild_ncns/wipe_drives/</guid>
      <description>Wipe Disks Warning: This is the point of no return. Once the disks are wiped, the node must be rebuilt.&#xA;All commands in this section must be run on the node being rebuilt (unless otherwise indicated). These commands can be done from the ConMan console window.&#xA;Only follow the steps in the section for the node type that is being rebuilt:&#xA;Wipe Disks Wipe Disks: Master Wipe Disks: Worker Node Wipe Disks: Utility Storage Node Wipe Disks: Master Unmount the etcd volume and remove the volume group.</description>
    </item>
    <item>
      <title>Validate Health</title>
      <link>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/validate_health/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:28 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/validate_health/</guid>
      <description>Validate Health Description Validate that the system is healthy.&#xA;Procedure The following procedures can be run from any master or worker node.&#xA;Collect data about the system management platform health.&#xA;ncn-mw# /opt/cray/platform-utils/ncnHealthChecks.sh ncn-mw# /opt/cray/platform-utils/ncnPostgresHealthChecks.sh NOTE: If workers have been removed and the worker count is currently at two, the following failures can be ignored. A re-check will be needed once workers are added and the count returns to three or above.</description>
    </item>
    <item>
      <title>Create a Manual Backup of a Healthy etcd Cluster</title>
      <link>/docs-csm/en-10/operations/kubernetes/create_a_manual_backup_of_a_healthy_etcd_cluster/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:24 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/create_a_manual_backup_of_a_healthy_etcd_cluster/</guid>
      <description>Create a Manual Backup of a Healthy etcd Cluster Manually create a backup of a healthy etcd cluster and check to see if the backup was created successfully.&#xA;Backups of healthy etcd clusters can be used to restore the cluster if it becomes unhealthy at any point.&#xA;The commands in this procedure can be run on any master node (ncn-mXXX) or worker node (ncn-wXXX) on the system.&#xA;Prerequisites A healthy etcd cluster is available on the system.</description>
    </item>
    <item>
      <title>Lock and Unlock Management Nodes</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/lock_and_unlock_management_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:22 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/lock_and_unlock_management_nodes/</guid>
      <description>Lock and Unlock Management Nodes The ability to ignore non-compute nodes (NCNs) is turned off by default. Management nodes and NCNs are also not locked by default. The administrator must lock the NCNs to prevent unwanted actions from affecting these nodes.&#xA;This section only covers using locks with the Hardware State Manager (HSM). For more information on ignoring nodes, refer to the following sections:&#xA;Firmware Action Service (FAS): See Ignore Node within FAS.</description>
    </item>
    <item>
      <title>Configuration Sessions</title>
      <link>/docs-csm/en-10/operations/configuration_management/configuration_sessions/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/configuration_sessions/</guid>
      <description>Configuration Sessions Once configurations have been created with the required layers and values set in the configuration repositories (or the additional inventory repository), create a Configuration Framework Session (CFS) session to apply the configuration to the targets.&#xA;Sessions are created via the Cray CLI or through the CFS REST API. A session stages Ansible inventory (whether dynamic, static, or image customization), launches Ansible Execution Environments (AEE) in order for each configuration layer in the service mesh, tears down the environments as required, and reports the session status to the CFS API.</description>
    </item>
    <item>
      <title>Compute Node Boot Sequence</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/compute_node_boot_sequence/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/compute_node_boot_sequence/</guid>
      <description>Compute Node Boot Sequence Provides an overview of the compute node boot process and touches upon the fact that issues can be encountered during this process.&#xA;The following is a high-level overview of the boot sequence for compute nodes:&#xA;The compute node is powered on.&#xA;The BIOS issues a DHCP discover request.&#xA;DHCP responds with the following:&#xA;next-server, which is the IP address of the TFTP server. The name of the file to download from the TFTP server.</description>
    </item>
    <item>
      <title>Create a UAI with Additional Ports</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_a_uai_with_additional_ports/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:09 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_a_uai_with_additional_ports/</guid>
      <description>Create a UAI with Additional Ports An option is available to expose UAI ports to the customer user network in addition to the the port used for SSH access. These ports are restricted to ports 80, 443, and 8888. This procedure allows a user or administrator to create a new UAI with these additional ports.&#xA;Prerequisites A public SSH key&#xA;Limitations Only ports 80, 443, and 8888 can be exposed. Attempting to open any other ports will result in an error.</description>
    </item>
    <item>
      <title>Configure Administrative Access</title>
      <link>/docs-csm/en-10/install/configure_administrative_access/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:04 +0000</pubDate>
      <guid>/docs-csm/en-10/install/configure_administrative_access/</guid>
      <description>Configure Administrative Access There are several operations which configure administrative access to different parts of the system. Ensuring that the cray CLI can be used with administrative credentials enables use of many management services via commands. The management nodes can be locked from accidental manipulation by the cray capmc and cray fas commands when the intent is to work on the entire system except the management nodes. The cray scsd command can change the SSH keys, NTP server, syslog server, and BMC/controller passwords.</description>
    </item>
    <item>
      <title>Glossary</title>
      <link>/docs-csm/en-10/glossary/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:01 +0000</pubDate>
      <guid>/docs-csm/en-10/glossary/</guid>
      <description>Glossary Glossary of terms used in CSM documentation.&#xA;Ansible Execution Environment (AEE) Application Node (AN) Baseboard Management Controller (BMC) Blade Switch Controller (sC) Boot Orchestration Service (BOS) Boot Script Service (BSS) Cabinet Cooling Group Cabinet Environmental Controller (CEC) CEC microcontroller (eC) Chassis Management Module (CMM) Compute Node (CN) Compute Rolling Upgrade Service (CRUS) Configuration Framework Service (CFS) Content Projection Service (CPS) Cray Advanced Platform Monitoring and Control (CAPMC) Cray CLI (cray) Cray Operating System (COS) Cray Programming Environment (CPE) Cray Security Token Service (STS) Cray Site Init (CSI) Cray System Management (CSM) Customer Access Network (CAN) Data Virtualization Service (DVS) EX Compute Cabinet EX TDS Cabinet Fabric Firmware Action Service (FAS) Floor Standing CDU Hardware Management Network (HMN) Hardware Management Notification Fanout Daemon (HMNFD) Hardware State Manager (HSM) Hardware State Manager (SMD) Heartbeat Tracker Daemon (HBTD) High Speed Network (HSN) Image Management Service (IMS) JSON Web Token (JWT) Kubernetes NCNs Management Cabinet Management Nodes Mountain Cabinet Mountain Endpoint Discovery Service (MEDS) NIC Mezzanine Card (NMC) Node Controller (nC) Node Management Network (NMN) Node Memory Dump (NMD) Non-Compute Node (NCN) Online Certificate Status Protocol (OCSP) Olympus Cabinet Parallel Application Launch Service (PALS) Power Distribution Unit (PDU) Pre-Install Toolkit (PIT) LiveCD Public Key Infrastructure (PKI) Rack-Mounted CDU Rack System Compute Cabinet Redfish Translation Service (RTS) River Cabinet River Endpoint Discovery Service (REDS) Rosetta ASIC Service/IO Cabinet Simple Storage Service (S3) Slingshot Slingshot Blade Switch Slingshot Top of Rack (ToR) Switch Shasta Cabling Diagram (SHCD) Supply/Return Cutoff Valves System Admin Toolkit (SAT) System Configuration Service (SCSD) System Layout Service (SLS) System Management Network (SMNet) System Management Services (SMS) System Management Services (SMS) nodes System Monitoring Application (SMA) System Monitoring Framework (SMF) Top of Rack Switch Controller (sC-ToR) User Access Instance (UAI) User Access Node (UAN) User Access Service (UAS) Version Control Service (VCS) Virtual Network Identifier Daemon (VNID) xname Ansible Execution Environment (AEE) A component used by the Configuration Framework Service (CFS) to execute Ansible code from its configuration layers.</description>
    </item>
    <item>
      <title>Incorrectly Tagged zeromq Image</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/incorrectly_tagged_zeromq_image/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/incorrectly_tagged_zeromq_image/</guid>
      <description>Incorrectly Tagged zeromq Image CSM 1.0.11 shipped a version of shasta-cfg which expects the zeromq image to be tagged differently to the one shipped in the product tarball. This may result in the following error when performing the &amp;ldquo;Generate Sealed Secrets&amp;rdquo; step in prepare_site_init.md.&#xA;pit# /mnt/pitdata/prep/site-init/utils/secrets-seed-customizations.sh \ &amp;gt; /mnt/pitdata/prep/site-init/customizations.yaml Creating Sealed Secret keycloak-certs Generating type static_b64... Creating Sealed Secret keycloak-master-admin-auth Generating type static... Generating type static... Generating type randstr... Generating type static.</description>
    </item>
    <item>
      <title>Manage Ceph Services</title>
      <link>/docs-csm/en-10/operations/utility_storage/manage_ceph_services/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/manage_ceph_services/</guid>
      <description>Manage Ceph Services The following commands are required to start, stop, or restart Ceph services. Restarting Ceph services is helpful for troubleshoot issues with the utility storage platform.&#xA;List Ceph services ncn-s00(1/2/3)# ceph orch ps NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID mds.cephfs.ncn-s001.zwptsg ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c bb08bcb2f034 mds.cephfs.ncn-s002.qyvoyv ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 32c3ff10be42 mds.cephfs.ncn-s003.vvsuvy ncn-s003 running (3d) 7m ago 3d 15.</description>
    </item>
    <item>
      <title>Change Root Passwords for Compute Nodes</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/change_root_passwords_for_compute_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:35 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/change_root_passwords_for_compute_nodes/</guid>
      <description>Change Root Passwords for Compute Nodes Update the root password on the system for compute nodes.&#xA;Changing the root password at least once is a recommended best practice for system security.&#xA;Prerequisites The initial root password for compute nodes is not set. Use this procedure to initially set or later change the password.&#xA;Procedure Get an encrypted value for the new password.&#xA;Use the passwd command to update the password and get the encrypted hash of the new password.</description>
    </item>
    <item>
      <title>Set the Turbo Boost Limit</title>
      <link>/docs-csm/en-10/operations/power_management/set_the_turbo_boost_limit/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:33 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/set_the_turbo_boost_limit/</guid>
      <description>Set the Turbo Boost Limit Turbo boost limiting is supported on the Intel® and AMD® processors. Because processors have a high degree of variability in the amount of turbo boost each processor can supply, limiting the amount of turbo boost can reduce performance variability and reduce power consumption.&#xA;Turbo boost can be limited by setting the turbo_boost_limit kernel parameter to one of these values:&#xA;0 - Disable turbo boost 999 - (default) No limit is applied.</description>
    </item>
    <item>
      <title>Validate Added NCN</title>
      <link>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/validate_ncn/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_remove_replace_ncns/validate_ncn/</guid>
      <description>Validate Added NCN Only follow the steps in the section for the node type that was added:&#xA;Master Node Worker Node Storage Node Validate: Master Node Validate the master node added successfully.&#xA;Verify the new node is in the cluster.&#xA;Run the following command from any master or worker node that is already in the cluster. It is helpful to run this command several times to watch for the newly rebuilt node to join the cluster.</description>
    </item>
    <item>
      <title>Kubernetes CronJobs</title>
      <link>/docs-csm/en-10/operations/kubernetes/cronjobs/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:24 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/cronjobs/</guid>
      <description>Kubernetes CronJobs Kubernetes CronJobs create Kubernetes jobs on a repeating schedule, specified in traditional cron syntax.&#xA;CronJobs Not Scheduled In some cases, CronJobs can fail to get scheduled (such as the cray-dns-unbound-manager job) if a previously run job did not finish properly. In order to alleviate this condition, CSM deploys a traditional CronJob on one of the master NCNs to perform cleanup such that jobs continue to get rescheduled.&#xA;In the event jobs aren&amp;rsquo;t running, ensure the CronJob is specified properly using the following steps.</description>
    </item>
    <item>
      <title>Manage Component Groups</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/manage_component_groups/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:22 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/manage_component_groups/</guid>
      <description>Manage Component Groups The creation, deletion, and modification of groups is enabled by the Hardware State Manager (HSM) APIs.&#xA;Example group Prerequisites Create and modify a group Create a group Modify a group Retrieve a group Delete a group Example group The following is an example group that contains the optional fields tags and exclusiveGroup:&#xA;{ &amp;#34;label&amp;#34; : &amp;#34;blue&amp;#34;, &amp;#34;description&amp;#34; : &amp;#34;blue node group&amp;#34;, &amp;#34;tags&amp;#34; : [ &amp;#34;tag1&amp;#34;, &amp;#34;tag2&amp;#34; ], &amp;#34;members&amp;#34; : { &amp;#34;ids&amp;#34; : [ &amp;#34;x0c0s0b0n0&amp;#34;, &amp;#34;x0c0s0b0n1&amp;#34;, &amp;#34;x0c0s0b1n0&amp;#34;, &amp;#34;x0c0s0b1n1&amp;#34; ] }, &amp;#34;exclusiveGroup&amp;#34; : &amp;#34;colors&amp;#34; } Prerequisites The commands on this page will not work unless the Cray CLI has been initialized on the node where the commands are being run.</description>
    </item>
    <item>
      <title>Create a CFS Configuration</title>
      <link>/docs-csm/en-10/operations/configuration_management/create_a_cfs_configuration/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/create_a_cfs_configuration/</guid>
      <description>Create a CFS Configuration Create a Configuration Framework Service (CFS) configuration, which contains an ordered list of layers. Each layer is defined by a Git repository clone URL, a Git commit, a name, and the path in the repository to an Ansible playbook to execute.&#xA;Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. Procedure Create a JSON file to hold data about the CFS configuration.</description>
    </item>
    <item>
      <title>Configure the BOS Timeout When Booting Compute Nodes</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/configure_the_bos_timeout_when_booting_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/configure_the_bos_timeout_when_booting_nodes/</guid>
      <description>Configure the BOS Timeout When Booting Compute Nodes Manually update the boa-job-template ConfigMap to tune the timeout and sleep intervals for the Boot Orchestration Agent (BOA). Correcting the timeout value is a good troubleshooting option for when Boot Orchestration Service (BOS) sessions hang waiting for nodes to be in a Ready state.&#xA;If the BOS timeout occurs when booting compute nodes, the system will be unable to boot via BOS.</description>
    </item>
    <item>
      <title>Create and Register a Custom UAI Image</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_and_register_a_custom_uai_image/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:09 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_and_register_a_custom_uai_image/</guid>
      <description>Create and Register a Custom UAI Image Create a custom UAI image based on the current compute node image. This UAI image can then be used to build compute node software with the Cray Programming Environment (PE).&#xA;Prerequisites This procedure requires administrator privileges. Log into either a master or worker NCN node (not a LiveCD node). Procedure The default end-user UAI is not suitable for use with the Cray PE. The generic image cannot be guaranteed to be compatible with the software running on HPE Cray EX compute nodes at every customer site.</description>
    </item>
    <item>
      <title>Configure Aruba Aggregation Switch</title>
      <link>/docs-csm/en-10/install/configure_aruba_aggregation_switch/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:04 +0000</pubDate>
      <guid>/docs-csm/en-10/install/configure_aruba_aggregation_switch/</guid>
      <description>Configure Aruba Aggregation Switch This page describes how Aruba aggregation switches are configured.&#xA;Management nodes and Application nodes will be plugged into aggregation switches.&#xA;Switch models used: JL635A Aruba 8325-48Y8C&#xA;They run in a high availability pair and use VSX to provide redundancy.&#xA;Prerequisites Three connections between the switches, two of these are used for the Inter switch link (ISL), and one used for the keepalive.&#xA;The ISL uses 100GB ports and the keepalive will be a 25 GB port.</description>
    </item>
    <item>
      <title>Known Issue initrd.img.xz Not Found</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/initrd.img.zx_not_found/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/initrd.img.zx_not_found/</guid>
      <description>Known Issue: initrd.img.xz Not Found This is a problem that is fixed in CSM 1.0 and later, but if your system was upgraded from CSM 0.9 you may run into this. Below is the full error seen when attempting to boot:&#xA;Loading Linux ... Loading initial ramdisk ... error: file `/boot/grub2/../initrd.img.xz&amp;#39; not found. Press any key to continue... [ 2.528752] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) [ 2.</description>
    </item>
    <item>
      <title>Shrink the Ceph Cluster</title>
      <link>/docs-csm/en-10/operations/utility_storage/remove_ceph_node/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:40 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/remove_ceph_node/</guid>
      <description>Shrink the Ceph Cluster This procedure describes how to remove a Ceph node from the Ceph cluster. Once the node is removed, the cluster is also rebalanced to account for the changes. Use this procedure to reduce the size of a cluster.&#xA;Prerequisites This procedure requires administrative privileges. 3 SSH sessions. 1 to monitor the cluster. 1 to perform cluster wide actions from a ceph-mon node. 1 to perform node only actions on the node being removed.</description>
    </item>
    <item>
      <title>Change SNMP Credentials on Leaf Switches</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/change_snmp_credentials_on_leaf_switches/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:35 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/change_snmp_credentials_on_leaf_switches/</guid>
      <description>Change SNMP Credentials on Leaf Switches This procedure changes the SNMP credentials on management leaf switches in the system. Either a single leaf switch can be updated to use new SNMP credentials, or update all leaf switches in the system to use the same global SNMP credentials.&#xA;NOTE: This procedure will not update the default SNMP credentials used when new leaf switches are added to the system. To update the default SNMP credentials for new hardware, follow the Update Default Air-Cooled BMC and Leaf Switch SNMP Credentials procedure.</description>
    </item>
    <item>
      <title>Shut Down and Power Off Compute and User Access Nodes</title>
      <link>/docs-csm/en-10/operations/power_management/shut_down_and_power_off_compute_and_user_access_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/shut_down_and_power_off_compute_and_user_access_nodes/</guid>
      <description>Shut Down and Power Off Compute and User Access Nodes Shut down and power off compute and user access nodes (UANs). This procedure powers off all compute nodes in the context of an entire system shutdown.&#xA;Prerequisites The cray and sat commands must be initialized and authenticated with valid credentials for Keycloak. If these have not been prepared, then see Configure the Cray Command Line Interface (cray CLI) and refer to the &amp;ldquo;SAT Authentication&amp;rdquo; section of the HPE Cray EX System Admin Toolkit (SAT) product stream documentation (S-8031) for instructions on how to acquire a SAT authentication token.</description>
    </item>
    <item>
      <title>Replace a Compute Blade</title>
      <link>/docs-csm/en-10/operations/node_management/replace_a_compute_blade/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:31 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/replace_a_compute_blade/</guid>
      <description>Replace a Compute Blade Replace an HPE Cray EX liquid-cooled compute blade.&#xA;Shutdown software and power off the blade Temporarily disable endpoint discovery service (MEDS) for the compute nodes(s) being replaced. This example disables MEDS for the compute node in cabinet 1000, chassis 3, slot 0 (x1000c3s0b0). If there is more than 1 node card, in the blade specify each node card (x1000c3s0b0,x1000c3s0b1).&#xA;ncn-mw# cray hsm inventory redfishEndpoints update --enabled false x1000c3s0b0 Verify that the workload manager (WLM) is not using the affected nodes.</description>
    </item>
    <item>
      <title>Add TLS Certificates to BMCs</title>
      <link>/docs-csm/en-10/operations/node_management/add_tls_certificates_to_bmcs/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_tls_certificates_to_bmcs/</guid>
      <description>Add TLS Certificates to BMCs Use the System Configuration Service (SCSD) tool to create TLS certificates and store them in Vault secure storage. Once certificates are created, they are placed on to the target BMCs.&#xA;Prerequisites Limitations Generate TLS certificates Regenerate TLS certificates Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Limitations TLS certificates can only be set for liquid-cooled BMCs.</description>
    </item>
    <item>
      <title>Determine if Pods are Hitting Resource Limits</title>
      <link>/docs-csm/en-10/operations/kubernetes/determine_if_pods_are_hitting_resource_limits/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:24 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/determine_if_pods_are_hitting_resource_limits/</guid>
      <description>Determine if Pods are Hitting Resource Limits Determine if a pod is being CPU throttled or hitting its memory limits (OOMKilled). Use a script to determine if any pods are being CPU throttled, and check the Kubernetes events to see if any pods are hitting a memory limit.&#xA;IMPORTANT: The presence of CPU throttling does not always indicate a problem, but if a service is being slow or experiencing latency issues, this procedure can be used to evaluate if it is not performing well as a result of CPU throttling.</description>
    </item>
    <item>
      <title>Manage Component Partitions</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/manage_component_partitions/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:22 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/manage_component_partitions/</guid>
      <description>Manage Component Partitions The creation, deletion, and modification of partitions is enabled by the Hardware State Manager (HSM) APIs.&#xA;The following is an example partition that contains the optional tags field:&#xA;{ &amp;#34;name&amp;#34; : &amp;#34;partition 1&amp;#34;, &amp;#34;description&amp;#34; : &amp;#34;partition 1&amp;#34;, &amp;#34;tags&amp;#34; : [ &amp;#34;tag2&amp;#34; ], &amp;#34;members&amp;#34; : { &amp;#34;ids&amp;#34; : [ &amp;#34;x0c0s0b0n0&amp;#34;, &amp;#34;x0c0s0b0n1&amp;#34;, &amp;#34;x0c0s0b1n0&amp;#34; ] }, } Troubleshooting: If the Cray CLI has not been initialized, the CLI commands will not work.</description>
    </item>
    <item>
      <title>Create a CFS Session with Dynamic Inventory</title>
      <link>/docs-csm/en-10/operations/configuration_management/create_a_cfs_session_with_dynamic_inventory/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/create_a_cfs_session_with_dynamic_inventory/</guid>
      <description>Create a CFS Session with Dynamic Inventory A Configuration Framework Service (CFS) session using dynamic inventory is used to configure live nodes. To create a CFS session using the default dynamic inventory, simply provide a session name and the name of the configuration to apply:&#xA;ncn# cray cfs sessions create --name example \ --configuration-name configurations-example { &amp;#34;ansible&amp;#34;: { &amp;#34;config&amp;#34;: &amp;#34;cfs-default-ansible-cfg&amp;#34;, &amp;#34;limit&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;verbosity&amp;#34;: 0 }, &amp;#34;configuration&amp;#34;: { &amp;#34;limit&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;name&amp;#34;: &amp;#34;configurations-example&amp;#34; }, &amp;#34;name&amp;#34;: &amp;#34;example&amp;#34;, &amp;#34;status&amp;#34;: { &amp;#34;artifacts&amp;#34;: [], &amp;#34;session&amp;#34;: { &amp;#34;status&amp;#34;: &amp;#34;pending&amp;#34;, &amp;#34;succeeded&amp;#34;: &amp;#34;none&amp;#34; } }, &amp;#34;tags&amp;#34;: {}, &amp;#34;target&amp;#34;: { &amp;#34;definition&amp;#34;: &amp;#34;dynamic&amp;#34;, &amp;#34;groups&amp;#34;: null } } Add the --target-definition dynamic parameter to the create command to explicitly define the inventory type to be dynamic.</description>
    </item>
    <item>
      <title>Create a Session Template to Boot Compute Nodes with CPS</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/create_a_session_template_to_boot_compute_nodes_with_cps/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/create_a_session_template_to_boot_compute_nodes_with_cps/</guid>
      <description>Create a Session Template to Boot Compute Nodes with CPS When compute nodes are booted, the Content Projection Service (CPS) and Data Virtualization Service (DVS) project the root file system (rootfs) over the network to the compute nodes by default.&#xA;Another option when compute nodes are booted is to download their rootfs into RAM.&#xA;This page covers the appropriate contents for a BOS session template in order to use CPS and DVS.</description>
    </item>
    <item>
      <title>Create and Use Default UAIs in Legacy Mode</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_and_use_default_uais_in_legacy_mode/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:09 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/create_and_use_default_uais_in_legacy_mode/</guid>
      <description>Create and Use Default UAIs in Legacy Mode Create a UAI using the default UAI image or the default UAI class in legacy mode.&#xA;Procedure Create a UAI with a command of the following form:&#xA;user&amp;gt; cray uas create --public-key &amp;#39;&amp;lt;path&amp;gt;&amp;#39; &amp;lt;path&amp;gt; is the path to a file containing an SSH public-key matched to the SSH private key belonging to the user.&#xA;Watch the UAI and see when it is ready for logins.</description>
    </item>
    <item>
      <title>Configure Aruba CDU Switch</title>
      <link>/docs-csm/en-10/install/configure_aruba_cdu_switch/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:04 +0000</pubDate>
      <guid>/docs-csm/en-10/install/configure_aruba_cdu_switch/</guid>
      <description>Configure Aruba CDU Switch This page describes how Aruba CDU switches are configured.&#xA;CDU switches are located in liquid-cooled cabinets and provide connectivity to MTN (Mountain) components. CDU switches act as leaf switches in the architecture. Aruba JL720A 8360-48XT4 is the model used. They run in a high availability pair and use VSX to provide redundancy.&#xA;Prerequisites There are two uplinks from each CDU switch to the upstream switch. This is normally a spine switch.</description>
    </item>
    <item>
      <title>kube-multus pod is in ImagePullBackOff</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/kube_multus_pod_in_imagepullbackoff/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/kube_multus_pod_in_imagepullbackoff/</guid>
      <description>kube-multus pod is in ImagePullBackOff Description There is a known problem where kube-multus pods may fail to restart due to an ImagePullBackOff error. The multus:v3.1 image will need to be re-tagged in Nexus and the kube-multus pod will need to be restarted.&#xA;Run the following command to determine if any kube-multus pods are failing due to this issue. If any pods are in ImagePullBackOff, proceed with the fix.&#xA;ncn# kubectl get pods -n kube-system -l app=multus | grep ImagePullBackOff kube-multus-ds-amd64-4wkb5 0/1 ImagePullBackOff 0 18h Fix Option 1 If you have access to the quay.</description>
    </item>
    <item>
      <title>Restore Nexus Data After Data Corruption</title>
      <link>/docs-csm/en-10/operations/utility_storage/restore_corrupt_nexus/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/restore_corrupt_nexus/</guid>
      <description>Restore Nexus Data After Data Corruption In rare cases, if a Ceph upgrade is not completed successfully and has issues, the eventual Ceph health can end up with a damaged mds (cephfs) daemon. Ceph reports this as follows running the ceph -s command:&#xA;ncn-s002:~ # ceph -s cluster: id: 7ed70f4c-852e-494a-b9e7-5f722af6d6e7 health: HEALTH_ERR 1 filesystem is degraded 1 filesystem is offline 1 mds daemon damaged When Ceph is in this state, Nexus will likely not operate properly and can be recovered using the following procedure.</description>
    </item>
    <item>
      <title>Change the Keycloak Admin Password</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/change_the_keycloak_admin_password/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:35 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/change_the_keycloak_admin_password/</guid>
      <description>Change the Keycloak Admin Password Update the default password for the admin Keycloak account using the Keycloak user interface (UI). After updating the password in Keycloak, encrypt it on the system and verify that the change was made successfully.&#xA;System domain name Procedure System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system&amp;rsquo;s fully qualified domain name (FQDN).&#xA;The FQDN can be found by running the following command on any Kubernetes NCN.</description>
    </item>
    <item>
      <title>Shut Down and Power Off the Management Kubernetes Cluster</title>
      <link>/docs-csm/en-10/operations/power_management/shut_down_and_power_off_the_management_kubernetes_cluster/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/shut_down_and_power_off_the_management_kubernetes_cluster/</guid>
      <description>Shut Down and Power Off the Management Kubernetes Cluster Shut down management services and power off the HPE Cray EX management Kubernetes cluster.&#xA;Overview Prerequisites Check health of the management cluster Shut down the Kubernetes management cluster [Next step] Overview Understand the following concepts before powering off the management non-compute nodes (NCNs) for the Kubernetes cluster and storage:&#xA;The etcd cluster provides storage for the state of the management Kubernetes cluster.</description>
    </item>
    <item>
      <title>Reset Credentials on Redfish Devices</title>
      <link>/docs-csm/en-10/operations/node_management/reset_credentials_on_redfish_devices_for_reinstallation/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:31 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/reset_credentials_on_redfish_devices_for_reinstallation/</guid>
      <description>Reset Credentials on Redfish Devices Before re-installing or upgrading the system the credentials need to be changed back to their defaults for any devices that had their credentials changed post-install. This is necessary for the installation process to properly discover and communicate with these devices.&#xA;Prerequisites Administrative privileges are required.&#xA;Procedure Create an SCSD payload file with the default credentials for the Redfish devices that have been changed from the defaults.</description>
    </item>
    <item>
      <title>Add a Standard Rack Node</title>
      <link>/docs-csm/en-10/operations/node_management/add_a_standard_rack_node/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_a_standard_rack_node/</guid>
      <description>Add a Standard Rack Node These procedures are intended for trained technicians and support personnel only. Always follow ESD precautions when handling this equipment.&#xA;The example is this procedure adds a User Access Node (UAN) or compute node to an HPE Cray standard rack system. This example adds a node to rack number 3000 at U27.&#xA;Procedures for updating the Hardware State Manager (HSM) or System Layout Service (SLS) are similar when adding additional compute nodes or User Application Nodes (UANs).</description>
    </item>
    <item>
      <title>Disaster Recovery for Postgres</title>
      <link>/docs-csm/en-10/operations/kubernetes/disaster_recovery_postgres/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:24 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/disaster_recovery_postgres/</guid>
      <description>Disaster Recovery for Postgres In the event that the Postgres cluster has failed to the point that it must be recovered and there is no dump available to restore the data, a full service specific disaster recovery is needed.&#xA;Below are the service specific steps required to cleanup any existing resources, redeploy the resources, and repopulate the data.&#xA;Disaster recovery procedures by service:&#xA;Restore HSM (Hardware State Manger) Postgres without a Backup Restore SLS (System Layout Service) Postgres without a Backup Restore Spire Postgres without a Backup Restore Keycloak Postgres without a backup Restore console Postgres Restore Keycloak Postgres without a backup The following procedures are required to rebuild the automatically populated contents of Keycloak&amp;rsquo;s PostgreSQL database if the database has been lost and recreated.</description>
    </item>
    <item>
      <title>Manage HMS Locks</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/manage_hms_locks/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:22 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/manage_hms_locks/</guid>
      <description>Manage HMS Locks This section describes how to check the status of a lock, disable reservations, and repair reservations. The disable and repair operations only affect the ability to make reservations on hardware devices.&#xA;Some of the common scenarios an admin might encounter when working with the Hardware State Manager (HSM) Locking API are also described.&#xA;Check Lock Status Use the following command to verify if a component name (xname) is locked or not.</description>
    </item>
    <item>
      <title>Create an Image Customization CFS Session</title>
      <link>/docs-csm/en-10/operations/configuration_management/create_an_image_customization_cfs_session/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/create_an_image_customization_cfs_session/</guid>
      <description>Create an Image Customization CFS Session A configuration session that is meant to customize image roots tracked by the Image Management Service (IMS) can be created using the --target-definition image option. This option will instruct the Configuration Framework Service (CFS) to prepare the image IDs specified and assign them to the groups specified in Ansible inventory. IMS will then provide SSH connection information to each image root that CFS will use to configure Ansible.</description>
    </item>
    <item>
      <title>Edit the iPXE Embedded Boot Script</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/edit_the_ipxe_embedded_boot_script/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/edit_the_ipxe_embedded_boot_script/</guid>
      <description>Edit the iPXE Embedded Boot Script Manually adjust the iPXE embedded boot script to change the order of network interfaces for DHCP request. Changing the order of network interfaces for DHCP requests helps improve boot time performance.&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Procedure Edit the ConfigMap using one of the following options.&#xA;NOTE: Save a backup of the ConfigMap before making any changes.&#xA;The following is an example of creating a backup:</description>
    </item>
    <item>
      <title>Customize End-User UAI Images</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/customize_end-user_uai_images/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:09 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/customize_end-user_uai_images/</guid>
      <description>Customize End-User UAI Images The provided end-user UAI image is a basic UAI image that includes an up-to-date version of the Sles Linux Distribution and client support for both the Slurm and PBS Professional workload managers. It provides an entrypoint to using UAIs and doing workload management from UAIs. This UAI image is not suitable for use with the Cray PE because it cannot be assured of being up-to-date with what is running on Shasta compute nodes at a given site.</description>
    </item>
    <item>
      <title>Configure Aruba Leaf Switch</title>
      <link>/docs-csm/en-10/install/configure_aruba_leaf_switch/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:04 +0000</pubDate>
      <guid>/docs-csm/en-10/install/configure_aruba_leaf_switch/</guid>
      <description>Configure Aruba Leaf Switch This page describes how Aruba leaf switches are configured.&#xA;Leaf switches are located in air-cooled cabinets and provide connectivity to components in those cabinets. Aruba JL762A 6300M 48G 4SFP56 is the model used.&#xA;Prerequisites Connectivity to the switch is established. The Configure Aruba Management Network Base procedure has been run. There are two uplinks from the switch to the upstream switch; this is can be an aggregation switch or a spine switch.</description>
    </item>
    <item>
      <title>Kubernetes Master or Worker node&#39;s root filesystem is out of space</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/kubernetes_node_rootfs_out_of_space/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/kubernetes_node_rootfs_out_of_space/</guid>
      <description>Kubernetes Master or Worker node&amp;rsquo;s root filesystem is out of space Description There is a known bug in Kubernetes 1.19.9 where movement of a pod with an attached volume may not complete in time and cause the kubelet service to stream error messages to the /var/log/messages log file. If this goes unchecked, it will fill up the root file system.&#xA;Fix Log into the node that has space issues.&#xA;Verify that you have a large messages file in /var/log/.</description>
    </item>
    <item>
      <title>Troubleshoot Ceph-Mon Processes Stopping and Exceeding Max Restarts</title>
      <link>/docs-csm/en-10/operations/utility_storage/troubleshoot_ceph-mon_processes_stopping_and_exceeding_max_restarts/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/troubleshoot_ceph-mon_processes_stopping_and_exceeding_max_restarts/</guid>
      <description>Troubleshoot Ceph-Mon Processes Stopping and Exceeding Max Restarts Troubleshoot an issue where all of the ceph-mon processes stop and exceed their maximum amount of attempts at restarting. This bug corrupts the health of the Ceph cluster.&#xA;Return the Ceph cluster to a healthy state by resolving issues with ceph-mon processes.&#xA;Prerequisites This procedure requires admin privileges.&#xA;Procedure See Collect Information about the Ceph Cluster for more information on how to interpret the output of the Ceph commands used in this procedure.</description>
    </item>
    <item>
      <title>Change the LDAP Server IP Address for Existing LDAP Server Content</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/change_the_ldap_server_ip_address_for_existing_ldap_server_content/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:35 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/change_the_ldap_server_ip_address_for_existing_ldap_server_content/</guid>
      <description>Change the LDAP Server IP Address for Existing LDAP Server Content The IP address that Keycloak is using for the LDAP server can be changed. In the case where the new LDAP server has the same contents as the previous LDAP server, edit the LDAP user federation to switch Keycloak to use the new LDAP server.&#xA;Refer to Change the LDAP Server IP Address for New LDAP Server Content if the LDAP server is being replaced by a different LDAP server that has different content.</description>
    </item>
    <item>
      <title>Standard Rack Node Power Management</title>
      <link>/docs-csm/en-10/operations/power_management/standard_rack_node_power_management/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/standard_rack_node_power_management/</guid>
      <description>Standard Rack Node Power Management HPE Cray EX standard EIA rack node power management is supported by the server vendor BMC firmware. The BMC exposes the power control API for a node through the node&amp;rsquo;s Redfish Power schema.&#xA;Out-of-band power management data is polled by a collector and published on a Kafka bus for entry into the Power Management Database (PMDB). Access to the data stored in the PMDB is available through the System Monitoring Application (SMA) Grafana instance.</description>
    </item>
    <item>
      <title>Swap a Compute Blade with a Different System</title>
      <link>/docs-csm/en-10/operations/node_management/swap_a_compute_blade_with_a_different_system/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:31 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/swap_a_compute_blade_with_a_different_system/</guid>
      <description>Swap a Compute Blade with a Different System Swap an HPE Cray EX liquid-cooled compute blade between two systems.&#xA;The two systems in this example are:&#xA;Source system - Cray EX TDS cabinet x9000 with a healthy EX425 blade (Windom dual-injection) in chassis 3, slot 0&#xA;Destination system - Cray EX cabinet x1005 with a defective EX425 blade (Windom dual-injection) in chassis 3, slot 0&#xA;Substitute the correct component names (xnames) or other parameters in the command examples that follow.</description>
    </item>
    <item>
      <title>Add Additional Liquid-Cooled Cabinets to a System</title>
      <link>/docs-csm/en-10/operations/node_management/add_additional_liquid-cooled_cabinets_to_a_system/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/add_additional_liquid-cooled_cabinets_to_a_system/</guid>
      <description>Add Additional Liquid-Cooled Cabinets to a System This top level procedure outlines the process for adding additional liquid-cooled cabinets to a currently installed system.&#xA;Prerequisites The System&amp;rsquo;s SHCD file has been updated with the new cabinets and cabling changes. The new cabinets have been cabled up to the system, and the system&amp;rsquo;s cabling has been validated to be correct. Follow the procedure Create a Backup of the SLS Postgres Database. Follow the procedure Create a Backup of the HSM Postgres Database.</description>
    </item>
    <item>
      <title>Increase Kafka Pod Resource Limits</title>
      <link>/docs-csm/en-10/operations/kubernetes/increase_kafka_pod_resource_limits/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:24 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/increase_kafka_pod_resource_limits/</guid>
      <description>Increase Kafka Pod Resource Limits For larger scale systems, the Kafka resource limits may need to be increased. See Increase Pod Resource Limits for details on how to increase limits.&#xA;Increase Kafka Resource Limits Example&#xA;For a 1500 compute node system, increasing the cpu count to 6 and memory limits to 128G should be adequate.</description>
    </item>
    <item>
      <title>Restore Hardware State Manager (HSM) Postgres Database from Backup</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/restore_hsm_postgres_from_backup/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:22 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/restore_hsm_postgres_from_backup/</guid>
      <description>Restore Hardware State Manager (HSM) Postgres Database from Backup This procedure can be used to restore the HSM Postgres database from a previously taken backup. This can be a manual backup created by the Create a Backup of the HSM Postgres Database procedure, or an automatic backup created by the cray-smd-postgresql-db-backup Kubernetes cronjob.&#xA;Prerequisites Healthy System Layout Service (SLS). Recovered first if also affected.&#xA;Healthy HSM Postgres Cluster.&#xA;Use patronictl list on the HSM Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:</description>
    </item>
    <item>
      <title>Create and Populate a VCS Configuration Repository</title>
      <link>/docs-csm/en-10/operations/configuration_management/create_and_populate_a_vcs_configuration_repository/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/create_and_populate_a_vcs_configuration_repository/</guid>
      <description>Create and Populate a VCS Configuration Repository Create a new repository in the VCS and populate it with content for site customizations in a custom Configuration Framework Service (CFS) configuration layer.&#xA;Prerequisites The Version Control Service (VCS) login credentials for the crayvcs user are set up. See VCS Administrative User in Version Control Service (VCS) for more information. Procedure Create the empty repository in VCS.&#xA;Replace the CRAYVCS_PASSWORD value in the following command before running it.</description>
    </item>
    <item>
      <title>Healthy Compute Node Boot Process</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/healthy_compute_node_boot_process/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/healthy_compute_node_boot_process/</guid>
      <description>Healthy Compute Node Boot Process In order to investigate node boot-related issues, it is important to understand the flow of a healthy boot process and the associated components. This section outlines the normal flow of components that play a role in booting compute nodes, including DHCP, BSS, and TPTP.&#xA;DHCP A healthy DHCP exchange between server and client looks like the following:&#xA;Traffic Description Sender DHCP Discover A broadcast request from the client requesting an IP address.</description>
    </item>
    <item>
      <title>Customize the Broker UAI Image</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/customize_the_broker_uai_image/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:10 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/customize_the_broker_uai_image/</guid>
      <description>Customize the Broker UAI Image The broker UAI image that comes with UAS is the image used to construct broker UAIs.&#xA;The key pieces of the broker UAI image are:&#xA;An entrypoint shell script that initializes the container and starts the SSH daemon running. An SSH configuration that forces logged in users into the switchboard command which creates / selects end-user UAIs and redirects connections. The primary way to customize the broker UAI image is by defining volumes and connecting them to the broker UAI class for a given broker.</description>
    </item>
    <item>
      <title>Configure Aruba Management Network Base</title>
      <link>/docs-csm/en-10/install/configure_aruba_management_network_base/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:04 +0000</pubDate>
      <guid>/docs-csm/en-10/install/configure_aruba_management_network_base/</guid>
      <description>Configure Aruba Management Network Base This page provides instructions on how to setup the base network configuration of the Shasta Management network.&#xA;After applying the base configuration, all management switches will be accessible to apply the remaining configuration.&#xA;Prerequisites Console access to all of the switches SHCD available Configuration The base configuration can be applied once console access to the switches has been established. The purpose of this configuration is to have an IPv6 underlay that provides access to the management switches.</description>
    </item>
    <item>
      <title>Orphaned CFS Pods After Booting or Rebooting</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/orphaned_cfs_pods/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/orphaned_cfs_pods/</guid>
      <description>Orphaned CFS Pods After Booting or Rebooting After a boot or reboot, a few CFS pods may continue running even after they have finished, and never go away. The state of these pods is that the only container still running in the pod is istio-proxy and the pod does not have a metadata.ownerReference.&#xA;If kubectl get pods -n services | grep cfs is run after a boot or reboot, the orphaned CFS pods look like this:</description>
    </item>
    <item>
      <title>Troubleshooting Ceph MDS slow ops</title>
      <link>/docs-csm/en-10/operations/utility_storage/troubleshoot_ceph_mds_reporting_slow_requests_and_failure_on_client/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/troubleshoot_ceph_mds_reporting_slow_requests_and_failure_on_client/</guid>
      <description>Troubleshooting Ceph MDS slow ops Before doing any steps on this page, please make sure you looked at Identify_Ceph_Latency_Issues&#xA;IMPORTANT: This will be a mix of commands that need to be run on the host(s) running the MDS daemon(s) and other commands that can be run from any of the ceph-mon nodes.&#xA;NOTICE: These steps are based off upstream documentation. This can be viewed here. https://docs.ceph.com/en/octopus/cephfs/troubleshooting/.&#xA;Please ensure you are on the correct version of documentation for the cluster you are running.</description>
    </item>
    <item>
      <title>Change the LDAP Server IP Address for New LDAP Server Content</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/change_the_ldap_server_ip_address_for_new_ldap_server_content/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/change_the_ldap_server_ip_address_for_new_ldap_server_content/</guid>
      <description>Change the LDAP Server IP Address for New LDAP Server Content Delete the old LDAP user federation and create a new one. This procedure should only be done if the LDAP server is being replaced by a different LDAP server that has different contents.&#xA;Refer to Change the LDAP Server IP Address for Existing LDAP Server Content if the new LDAP server content matches the previous LDAP server content.&#xA;Prerequisites The LDAP server is being replaced by a different LDAP server that has different contents.</description>
    </item>
    <item>
      <title>System Power Off Procedures</title>
      <link>/docs-csm/en-10/operations/power_management/system_power_off_procedures/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/system_power_off_procedures/</guid>
      <description>System Power Off Procedures The procedures in this section detail the high-level tasks required to power off an HPE Cray EX system.&#xA;Note about Services Used During System Power Off The Cray Advanced Platform Monitoring and Control (CAPMC) service controls power to major components. CAPMC sequences the power off tasks in the correct order, but does not gracefully shut down software services. The Boot Orchestration Service (BOS) manages proper shutdown and power off tasks for compute nodes and User Access Nodes (UANs).</description>
    </item>
    <item>
      <title>TLS Certificates for Redfish BMCs</title>
      <link>/docs-csm/en-10/operations/node_management/tls_certificates_for_redfish_bmcs/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:31 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/tls_certificates_for_redfish_bmcs/</guid>
      <description>TLS Certificates for Redfish BMCs Redfish HTTP communications are capable of using TLS certificates and Certificate Authority (CA) trust bundles to improve security. Several Hardware Management Services (HMS) have been modified to enable the HTTP transports used for Redfish communications to use a CA trust bundle.&#xA;The following services communicate with Redfish BMCs:&#xA;State Manager Daemon (SMD) Cray Advanced Platform Monitoring and Control (CAPMC) Firmware Action Service (FAS) HMS Collector River Endpoint Discovery Service (REDS) Mountain Endpoint Discovery Service (MEDS) Each Redfish BMC must have a TLS certificate in order to be useful.</description>
    </item>
    <item>
      <title>Adding a Liquid-cooled Blade to a System</title>
      <link>/docs-csm/en-10/operations/node_management/adding_a_liquid-cooled_blade_to_a_system/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/adding_a_liquid-cooled_blade_to_a_system/</guid>
      <description>Adding a Liquid-cooled Blade to a System This procedure will add a liquid-cooled blades from an HPE Cray EX system.&#xA;Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI.&#xA;Knowledge of whether DVS is operating over the Node Management Network (NMN) or the High Speed Network (HSN).&#xA;Blade is being added to an existing liquid-cooled cabinet in the system.</description>
    </item>
    <item>
      <title>Increase Pod Resource Limits</title>
      <link>/docs-csm/en-10/operations/kubernetes/increase_pod_resource_limits/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:24 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/increase_pod_resource_limits/</guid>
      <description>Increase Pod Resource Limits Increase the appropriate resource limits for pods after determining if a pod is being CPU throttled or OOMKilled.&#xA;Return Kubernetes pods to a healthy state with resources available.&#xA;Prerequisites The names of the pods hitting their resource limits are known. See Determine if Pods are Hitting Resource Limits. Procedure Determine the current limits of a pod.&#xA;ncn-w001# kubectl get po -n services POD_ID -o yaml Look for the following section returned in the output:</description>
    </item>
    <item>
      <title>Restore Hardware State Manager (HSM) Postgres without an Existing Backup</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/restore_hsm_postgres_without_a_backup/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:22 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/restore_hsm_postgres_without_a_backup/</guid>
      <description>Restore Hardware State Manager (HSM) Postgres without an Existing Backup This procedure is intended to repopulate HSM in the event when no Postgres backup exists.&#xA;Prerequisite Healthy System Layout Service (SLS). Recovered first if also affected.&#xA;Healthy HSM service.&#xA;Verify all 3 HSM postgres replicas are up and running:&#xA;ncn# kubectl -n services get pods -l cluster-name=cray-smd-postgres NAME READY STATUS RESTARTS AGE cray-smd-postgres-0 3/3 Running 0 18d cray-smd-postgres-1 3/3 Running 0 18d cray-smd-postgres-2 3/3 Running 0 18d Procedure Re-run the HSM loader job.</description>
    </item>
    <item>
      <title>Customize Configuration Values</title>
      <link>/docs-csm/en-10/operations/configuration_management/customize_configuration_values/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/customize_configuration_values/</guid>
      <description>Customize Configuration Values In general, most systems will require some customization from the default values provided by HPE Cray products. These changes cannot be made on the pristine product branches that are imported during product installation and upgrades. Changes can only be made in Git branches that are based on the pristine branches.&#xA;Changing or overriding default values should be done in accordance with Ansible best practices and variable precedence in mind.</description>
    </item>
    <item>
      <title>Kernel Boot Parameters</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/kernel_boot_parameters/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/kernel_boot_parameters/</guid>
      <description>Kernel Boot Parameters The Image Management Service (IMS) extracts kernel boot parameters from the /boot/kernel-parameters file in the image, if that file exists, and stores them in S3. IMS already stores the other boot artifacts (kernel, initrd, and rootfs) in S3. When told to boot an image, the Boot Orchestration Service (BOS) will extract these parameters and deliver them to the Boot Script Service (BSS) so they can be used during the next boot of a node.</description>
    </item>
    <item>
      <title>Delete a UAI</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/delete_a_uai/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:10 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/delete_a_uai/</guid>
      <description>Delete a UAI The cray uas command allows users to manage UAIs. This procedure deletes one of the user&amp;rsquo;s UAIs. To delete all UAIs on the system, see List and Delete All UAIs for more information.&#xA;Prerequisites A UAI is up and running.&#xA;Limitations Currently, the user must SSH to the system as root.&#xA;Procedure Log in to an NCN as root.&#xA;List existing UAIs.&#xA;ncn-w001# cray uas list username = &amp;#34;user&amp;#34; uai_host = &amp;#34;ncn-w001&amp;#34; uai_status = &amp;#34;Running: Ready&amp;#34; uai_connect_string = &amp;#34;ssh user@203.</description>
    </item>
    <item>
      <title>Configure Aruba Spine Switch</title>
      <link>/docs-csm/en-10/install/configure_aruba_spine_switch/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:04 +0000</pubDate>
      <guid>/docs-csm/en-10/install/configure_aruba_spine_switch/</guid>
      <description>Configure Aruba Spine Switch This page describes how Aruba spine switches are configured.&#xA;Depending on the size of the HPE Cray EX system, the spine switches will serve different purposes. On TDS systems, the NCNs will plug directly into the spine switches. On larger systems with aggregation switches, the spine switches will provide connection between the aggregation switches.&#xA;Switch models used: JL635A Aruba 8325-48Y8C and JL636A Aruba 8325-32C&#xA;They run in a high availability pair and use VSX to provide redundancy.</description>
    </item>
    <item>
      <title>Common Platform CA Issues</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/platform_ca_issues/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/platform_ca_issues/</guid>
      <description>Common Platform CA Issues 1 NCN platform CA certificate does not match certificate in BSS During install, if the beginning steps are re-run after the NCNs are booted, then platform-ca files on those NCNs will no longer match the server&amp;rsquo;s CA certificate. This can be detected with a Goss test.&#xA;1.1 Error messages (Caused by SSLError(SSLError(1, &amp;#39;[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)&amp;#39;),)) curl: (60) SSL certificate problem: self signed certificate in certificate chain More details here: https://curl.</description>
    </item>
    <item>
      <title>Troubleshoot Ceph OSDs Reporting Full</title>
      <link>/docs-csm/en-10/operations/utility_storage/troubleshoot_ceph_osds_reporting_full/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/troubleshoot_ceph_osds_reporting_full/</guid>
      <description>Troubleshoot Ceph OSDs Reporting Full Use this procedure to examine the Ceph cluster and troubleshoot issues where Ceph runs out of space and the Kubernetes cluster cannot write data. The OSDs need to be reweighed to move data from the drive and get it back under the warning threshold.&#xA;When a single OSD for a pool fills up, the pool will go into read-only mode to protect the data. This can occur if the data distribution is unbalanced or if more storage nodes are needed.</description>
    </item>
    <item>
      <title>Configure Keycloak for LDAP/AD authentication</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/configure_keycloak_for_ldapad_authentication/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/configure_keycloak_for_ldapad_authentication/</guid>
      <description>Configure Keycloak for LDAP/AD authentication Keycloak enables users to be in an LDAP or Active Directory (AD) server. This allows users to get their tokens using their regular username and password, and use those tokens to perform operations on the system&amp;rsquo;s REST API.&#xA;Configuring Keycloak can be done using the admin GUI or through Keycloak&amp;rsquo;s web API.&#xA;For more information on setting up LDAP federation, see the Keycloak administrative documentation in a section titled https://www.</description>
    </item>
    <item>
      <title>System Power On Procedures</title>
      <link>/docs-csm/en-10/operations/power_management/system_power_on_procedures/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/system_power_on_procedures/</guid>
      <description>System Power On Procedures The procedures in this section detail the high-level tasks required to power on an HPE Cray EX system.&#xA;Important: If an emergency power off (EPO) event occurred, then see Recover from a Liquid-Cooled Cabinet EPO Event for recovery procedures.&#xA;If user IDs or passwords are needed, then see step 1 of the Prepare the System for Power Off procedure.&#xA;Note about services used during system power on The Cray Advanced Platform Monitoring and Control (CAPMC) service controls power to major components.</description>
    </item>
    <item>
      <title>Troubleshoot Interfaces with IP Address Issues</title>
      <link>/docs-csm/en-10/operations/node_management/troubleshoot_interfaces_with_ip_address_issues/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/troubleshoot_interfaces_with_ip_address_issues/</guid>
      <description>Troubleshoot Interfaces with IP Address Issues Correct NCNs that are failing to assigning a static IP address or detect a duplicate IP address.&#xA;The Wicked network manager tool will fail to bring an interface up if its assigned IP address already exists in the respective LAN. This can be detected by checking for signs of duplicate IP address messages in the log.&#xA;Prerequisites An NCN has an interface that is failing to assign a static IP address or that has a duplicate IP address.</description>
    </item>
    <item>
      <title>Build NCN Images Locally</title>
      <link>/docs-csm/en-10/operations/node_management/build_ncn_images_locally/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/build_ncn_images_locally/</guid>
      <description>Build NCN Images Locally Build and test NCN images locally by using the following procedure. This procedure can be done on any x86 machine with the following prerequisites.&#xA;Necessary software The listed software below will equip a local machine or build server to build for any medium (.squashfs, .vbox, .qcow2, .iso).&#xA;media (.iso, .ovf, or .qcow2) (depending on the layer) packer qemu envsubst Media packer can intake any ISO, the sections below detail utilized base ISOs in CRAY HPCaaS.</description>
    </item>
    <item>
      <title>Kubernetes</title>
      <link>/docs-csm/en-10/operations/kubernetes/kubernetes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:24 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/kubernetes/</guid>
      <description>Kubernetes The system management components are broken down into a series of micro-services. Each service is independently deployable, fine-grained, and uses lightweight protocols. As a result, the system&amp;rsquo;s micro-services are modular, resilient, and can be updated independently. Services within this architecture communicate via REST APIs.&#xA;About Kubernetes Kubernetes is a portable and extensible platform for managing containerized workloads and services. Kubernetes serves as a micro-services platform on the system that facilitates application deployment, scaling, and management.</description>
    </item>
    <item>
      <title>Set BMC Management Roles</title>
      <link>/docs-csm/en-10/operations/hardware_state_manager/set_bmc_management_role/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:22 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/hardware_state_manager/set_bmc_management_role/</guid>
      <description>Set BMC Management Roles The ability to ignore non-compute nodes (NCNs) is turned off by default. Management nodes and NCNs are also not locked by default. The administrator must lock the NCNs and their BMCs to prevent unwanted actions from affecting these nodes. To more easily identify the BMCs that are associated with the management nodes, they need to be marked with the Management role in the Hardware State Manager (HSM), just like their associated nodes.</description>
    </item>
    <item>
      <title>Delete CFS Sessions</title>
      <link>/docs-csm/en-10/operations/configuration_management/delete_cfs_sessions/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/delete_cfs_sessions/</guid>
      <description>Delete CFS Sessions Delete an existing Configuration Framework Service (CFS) configuration session with the CFS delete command.&#xA;Prerequisites This requires that the Cray command line interface is configured. See Configure the Cray Command Line Interface.&#xA;Delete single CFS session Use the session name to delete the session:&#xA;ncn# cray cfs sessions delete &amp;lt;session_name&amp;gt; No output is expected.&#xA;Delete multiple CFS sessions To delete all completed CFS sessions, use the deleteall command.</description>
    </item>
    <item>
      <title>Limit the Scope of a BOS Session</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/limit_the_scope_of_a_bos_session/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/limit_the_scope_of_a_bos_session/</guid>
      <description>Limit the Scope of a BOS Session The Boot Orchestration Service (BOS) supports an optional limit parameter when creating a session. This parameter can be used to further limit the nodes that BOS runs against, and is applied to all boot sets.&#xA;The limit parameter takes a comma-separated list of nodes, groups, or roles in any combination. The BOS session will be limited to run against components that match both the boot set information and one or more of the nodes, groups, or roles listed in the limit.</description>
    </item>
    <item>
      <title>Delete a UAI Class</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/delete_a_uai_class/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:10 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/delete_a_uai_class/</guid>
      <description>Delete a UAI Class Delete a UAI class. After deletion, the class will no longer be available for new UAIs.&#xA;Prerequisites Install and initialize the cray administrative CLI. Obtain the ID of the UAI class that will be deleted. Procedure Delete a UAI by using a command of the following form:&#xA;cray uas admin config classes delete UAI_CLASS_ID UAI_CLASS_ID is the UAS ID of the UAI class.&#xA;Delete a UAI class.</description>
    </item>
    <item>
      <title>Configure Dell Aggregation Switch</title>
      <link>/docs-csm/en-10/install/configure_dell_aggregation_switch/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:04 +0000</pubDate>
      <guid>/docs-csm/en-10/install/configure_dell_aggregation_switch/</guid>
      <description>Configure Dell Aggregation Switch This page describes how Dell aggregation switches are configured.&#xA;Management nodes and Application nodes will be plugged into aggregation switches.&#xA;They run in a high availability pair and use VLT to provide redundancy.&#xA;Prerequisites Three connections between the switches, two of these are used for the Inter switch link (ISL), and one used for the keepalive.&#xA;Connectivity to the switch is established.&#xA;The ISL uses 100GB ports and the keepalive will be a 25 GB port.</description>
    </item>
    <item>
      <title>Unbound in CrashLoopBackOff After Deployment Restart</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/unbound_clbo/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:43 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/unbound_clbo/</guid>
      <description>Unbound in CrashLoopBackOff After Deployment Restart There is a known race condition that can cause cray-dns-unbound to go into CLBO (CrashLoopBackOff) after running the following command: ncn# kubectl rollout restart deployment -n services cray-dns-unbound This can impact csm-1.0.10 or older. Run the following command to get cray-dns-unbound out of CLBO: ncn# kubectl patch deployment -n services cray-dns-unbound --type=&amp;#39;json&amp;#39; \ -p=&amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;replace&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/template/spec/containers/0/command&amp;#34;, &amp;#34;value&amp;#34;: [&amp;#34;sh&amp;#34;, &amp;#34;-c&amp;#34;, &amp;#34;touch /etc/unbound/records.conf;/srv/unbound/entrypoint.sh&amp;#34;]}]&amp;#39; </description>
    </item>
    <item>
      <title>Troubleshoot Ceph services not starting after a server crash</title>
      <link>/docs-csm/en-10/operations/utility_storage/troubleshoot_ceph_services_not_starting/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/troubleshoot_ceph_services_not_starting/</guid>
      <description>Troubleshoot Ceph services not starting after a server crash Issue There is a known issue where the Ceph container images will not start after a power failure or server component failure that causes the server to crash and not boot back up&#xA;There will be a message like this in the journalctl logs for the ceph services on the machine that crashed&#xA;ceph daemons will not start due to: Error: readlink /var/lib/containers/storage/overlay/l/CXMD7IEI4LUKBJKX5BPVGZLY3Y: no such file or directory</description>
    </item>
    <item>
      <title>Configure the RSA Plugin in Keycloak</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/configure_the_rsa_plugin_in_keycloak/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/configure_the_rsa_plugin_in_keycloak/</guid>
      <description>Configure the RSA Plugin in Keycloak Use Keycloak to configure a plugin that enables RSA token authentication.&#xA;Prerequisites Procedure Verification Prerequisites Access to the Keycloak UI is needed.&#xA;Procedure Verify the Shasta domain is being used.&#xA;This is indicated in the dropdown in the upper left of the UI.&#xA;Click on Authentication under the Configure header of the navigation area on the left side of the page.&#xA;Click on the Flows tab.</description>
    </item>
    <item>
      <title>User Access to Compute Node Power Data</title>
      <link>/docs-csm/en-10/operations/power_management/user_access_to_compute_node_power_data/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/user_access_to_compute_node_power_data/</guid>
      <description>User Access to Compute Node Power Data HPE Cray EX liquid-cooled compute node power management data access for users.&#xA;HPE Cray EX liquid-cooled compute node power management counters (pm_counters) enable users access to energy usage over time for billing and job profiling.&#xA;The blade-level and node-level accumulated energy telemetry is point-in-time power data. Blade accumulated energy data is collected out-of-band and is made available via workload managers. Users have access to the data in-band at the node-level via a special sysfs files in /sys/cray/pm\_counters on the node.</description>
    </item>
    <item>
      <title>Troubleshoot Issues with Redfish Endpoint Discovery</title>
      <link>/docs-csm/en-10/operations/node_management/troubleshoot_issues_with_redfish_endpoint_discovery/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/troubleshoot_issues_with_redfish_endpoint_discovery/</guid>
      <description>Troubleshoot Issues with Redfish Endpoint Discovery If a Redfish endpoint is in the HTTPsGetFailed status, then the endpoint does not need to be fully rediscovered. The error indicates an issue in the inventory process done by the Hardware State Manager (HSM). Restart the inventory process to fix this issue.&#xA;Update the HSM inventory to resolve issues with discovering Redfish endpoints.&#xA;Error Symptom The following is an example of the HSM error:</description>
    </item>
    <item>
      <title>Change Java Security Settings</title>
      <link>/docs-csm/en-10/operations/node_management/change_java_security_settings/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/change_java_security_settings/</guid>
      <description>Change Java Security Settings If Java will not allow a connection to an Intel nodes via SOL or iKVM, change Java security settings to add an exception for the nodes&amp;rsquo;s BMC IP address.&#xA;The Intel nodes ship with an insecure certificate, which causes an exception for Java when trying to connect via SOL or iKVM to these nodes. The workaround is to add the node&amp;rsquo;s BMC IP address to the Exception Site List in the Java Control Panel of the machine attempting to connect to the Intel node.</description>
    </item>
    <item>
      <title>Kubernetes Networking</title>
      <link>/docs-csm/en-10/operations/kubernetes/kubernetes_networking/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:24 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/kubernetes_networking/</guid>
      <description>Kubernetes Networking Every Kubernetes pod has an IP address in the pod network that is reachable within the cluster. The system uses the weave-net plugin for inter-node communication.&#xA;Access services from outside the cluster All services with a REST API must be accessed from outside the cluster using the Istio Ingress Gateway. This gateway can be accessed using a URL in the following format:&#xA;https://api.SYSTEM-NAME_DOMAIN-NAME The API requests then get routed to the appropriate node running that service.</description>
    </item>
    <item>
      <title>Enable Ansible Profiling</title>
      <link>/docs-csm/en-10/operations/configuration_management/enable_ansible_profiling/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/enable_ansible_profiling/</guid>
      <description>Enable Ansible Profiling Ansible tasks and playbooks can be profiled in order to determine execution times and single out poor performance in runtime. The default Configuration Framework Service (CFS) ansible.cfg in the cfs-default-ansible-cfg ConfigMap does not enable these profiling tools. If profiling tools are desired, modify the default Ansible configuration file to enable them.&#xA;Procedure Edit the cfs-default-ansible-cfg ConfigMap.&#xA;ncn-mw# kubectl edit cm cfs-default-ansible-cfg -n services Uncomment the indicated line by removing the # character from the beginning of the line.</description>
    </item>
    <item>
      <title>BOS Limitations for Gigabyte BMC Hardware</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/limitations_for_gigabyte_bmc_hardware/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/limitations_for_gigabyte_bmc_hardware/</guid>
      <description>BOS Limitations for Gigabyte BMC Hardware Special steps need to be taken when using the Boot Orchestration Service to boot, reboot, or shutdown Gigabyte hardware. Gigabyte hardware treats power off and power on requests as successful, regardless of if actually successfully completed. The power on/off requests are ignored by Cray Advanced Platform Monitoring and Control (CAPMC) if they are received within a short period of time, which is typically around 60 seconds per operation.</description>
    </item>
    <item>
      <title>Delete a UAI Image Registration</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/delete_a_uai_image_registration/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:10 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/delete_a_uai_image_registration/</guid>
      <description>Delete a UAI Image Registration Unregister a UAI image from UAS.&#xA;Prerequisites Verify that the UAI image to be deleted is registered with UAS. See Retrieve UAI Image Registration Information for instructions.&#xA;Procedure Deleting a UAI image from UAS effectively unregisters the UAI image from UAS. This procedure does delete the actual UAI image artifact.&#xA;Delete a UAS image registration by using a command of the following form:&#xA;ncn-m001-pit# cray uas admin config images delete IMAGE_ID Replace IMAGE_ID with image ID of the UAI image to unregister from UAS.</description>
    </item>
    <item>
      <title>Configure Dell CDU switch</title>
      <link>/docs-csm/en-10/install/configure_dell_cdu_switch/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:05 +0000</pubDate>
      <guid>/docs-csm/en-10/install/configure_dell_cdu_switch/</guid>
      <description>Configure Dell CDU switch This page describes how Dell CDU switches are configured.&#xA;CDU switches are located in liquid-cooled cabinets and provide connectivity to MTN (Mountain) components. CDU switches act as leaf switches in the architecture. They run in a high availability pair and use VLT to provide redundancy.&#xA;Prerequisites Two uplinks from each CDU switch to the upstream switch, this is normally a spine switch. Connectivity to the switch is established.</description>
    </item>
    <item>
      <title>wait for unbound or cray-dns-unbound-manager hangs</title>
      <link>/docs-csm/en-10/troubleshooting/known_issues/wait_for_unbound_hang/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:44 +0000</pubDate>
      <guid>/docs-csm/en-10/troubleshooting/known_issues/wait_for_unbound_hang/</guid>
      <description>wait_for_unbound or cray-dns-unbound-manager hangs Run the following command:&#xA;ncn# kubectl get jobs -n services | grep cray-dns-unbound-manager The output should look similar to the following:&#xA;services cray-dns-unbound-manager-1635352560 0/1 26h 26h services cray-dns-unbound-manager-1635448680 1/1 35s 8m37s services cray-dns-unbound-manager-1635448860 1/1 51s 5m36s services cray-dns-unbound-manager-1635449040 1/1 61s 2m35s If one of the jobs shows 0/1 for more than 10 minutes and there are others with 1/1, then that means the 0/1 job is hung.</description>
    </item>
    <item>
      <title>Troubleshoot Failure to Get Ceph Health</title>
      <link>/docs-csm/en-10/operations/utility_storage/troubleshoot_failure_to_get_ceph_health/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/troubleshoot_failure_to_get_ceph_health/</guid>
      <description>Troubleshoot Failure to Get Ceph Health Inspect Ceph commands that are failing by looking into the Ceph monitor logs (ceph-mon). For example, the monitoring logs can help determine any issues causing the ceph -s command to hang.&#xA;Troubleshoot Ceph commands failing to run and determine how to make them operational again. These commands need to be operational to obtain critical information about the Ceph cluster on the system.&#xA;Prerequisites This procedure requires admin privileges.</description>
    </item>
    <item>
      <title>Create Internal Groups in the Keycloak Shasta Realm</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/create_internal_groups_in_the_keycloak_shasta_realm/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/create_internal_groups_in_the_keycloak_shasta_realm/</guid>
      <description>Create Internal Groups in the Keycloak Shasta Realm Manually create a group in the Keycloak Shasta realm. New groups can be created with the Keycloak UI. In CSM, Keycloak groups must have the cn and gidNumber attributes, otherwise the keycloak-users-localize tool will fail to export the groups.&#xA;New Keycloak groups can be used to group users for authentication.&#xA;Prerequisites This procedure assumes that the password for the Keycloak admin account is known.</description>
    </item>
    <item>
      <title>Power Management</title>
      <link>/docs-csm/en-10/operations/power_management/power_management/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:34 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/power_management/power_management/</guid>
      <description>Power Management HPE Cray System Management (CSM) software manages and controls power out-of-band through Redfish APIs. Note that power management features are &amp;ldquo;asynchronous,&amp;rdquo; in that the client must determine whether the component status has changed after a power management API call returns.&#xA;In-band power management features are not supported in v1.4.&#xA;HPE supports Slurm as a workload manager which reports job energy usage and records it in the ITDB for system accounting purposes.</description>
    </item>
    <item>
      <title>Troubleshoot Loss of Console Connections and Logs on Gigabyte Nodes</title>
      <link>/docs-csm/en-10/operations/node_management/troubleshoot_loss_of_console_connections_and_logs_on_gigabyte_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/troubleshoot_loss_of_console_connections_and_logs_on_gigabyte_nodes/</guid>
      <description>Troubleshoot Loss of Console Connections and Logs on Gigabyte Nodes Problem Gigabyte console log information will no longer be collected. If attempting to initiate a console session through Cray console services, there will be an error reported. This error will occur every time the node is rebooted unless this workaround is applied.&#xA;Prerequisites Console log information is no longer being collected for Gigabyte nodes or ConMan is reporting an error.</description>
    </item>
    <item>
      <title>Change Settings for HMS Collector Polling of Air-Cooled Nodes</title>
      <link>/docs-csm/en-10/operations/node_management/change_settings_for_hms_collector_polling_of_air_cooled_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/change_settings_for_hms_collector_polling_of_air_cooled_nodes/</guid>
      <description>Change Settings for HMS Collector Polling of Air-Cooled Nodes The cray-hms-hmcollector service polls all air-cooled hardware to gather the necessary telemetry information for use by other services, such as the Cray Advanced Platform Monitoring and Control (CAPMC) service. This polling occurs every 10 seconds on a continual basis. Instabilities with the AMI Redfish implementation in the Gigabyte BMCs require a less significant approach when gathering power and temperature telemetry data. If the BMCs are overloaded, they can become unresponsive, return incorrect data, or encounter other errors.</description>
    </item>
    <item>
      <title>Kubernetes Storage</title>
      <link>/docs-csm/en-10/operations/kubernetes/kubernetes_storage/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:24 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/kubernetes_storage/</guid>
      <description>Kubernetes Storage Data belonging to micro-services in the management cluster is managed through persistent storage, which provides reliable and resilient data protection for containers running in the Kubernetes cluster.&#xA;The backing storage for this service is currently provided by JBOD disks that are spread across several nodes of the management cluster. These node disks are managed by Ceph, and are exposed to containers in the form of persistent volumes.</description>
    </item>
    <item>
      <title>Git Operations</title>
      <link>/docs-csm/en-10/operations/configuration_management/git_operations/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:18 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/git_operations/</guid>
      <description>Git Operations Use the git command to manage repository content in the Version Control Service (VCS).&#xA;Once a repository is cloned, the git command line tool is available to interact with a repository from the Version Control Service (VCS). The git command is used for making commits, creating new branches, and pushing new branches, tags, and commits to the remote repository stored in VCS.&#xA;When pushing changes to the VCS server using the crayvcs user, input the password retrieved from the Kubernetes secret as the credentials.</description>
    </item>
    <item>
      <title>Log File Locations and Ports Used in Compute Node Boot Troubleshooting</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/log_file_locations_and_ports_used_in_compute_node_boot_troubleshooting/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:15 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/log_file_locations_and_ports_used_in_compute_node_boot_troubleshooting/</guid>
      <description>Log File Locations and Ports Used in Compute Node Boot Troubleshooting This section includes the port IDs and log file locations of components associated with the node boot process.&#xA;Log File Locations The log file locations for ConMan, DHCP, and TFTP.&#xA;ConMan logs are located within the conman pod at /var/log/conman.log.&#xA;DHCP:&#xA;ncn-m001# kubectl logs DHCP_POD_ID TFTP:&#xA;ncn-m001# kubectl logs -n services TFTP_POD_ID Port IDs The following table includes the port IDs for DHCP and TFTP.</description>
    </item>
    <item>
      <title>Delete a UAI Resource Specification</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/delete_a_uai_resource_specification/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:10 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/delete_a_uai_resource_specification/</guid>
      <description>Delete a UAI Resource Specification Delete a specific UAI resource specification using the resource_id of that specification. Once deleted, UAIs will no longer be able to use that specification.&#xA;Prerequisites Install and initialize the cray administrative CLI.&#xA;Prerequisites To delete a particular resource specification, use a command of the following form:&#xA;ncn-m001-pit# cray uas admin config resources delete RESOURCE_ID Remove a UAI resource specification from UAS.&#xA;ncn-m001-pit# cray uas admin config resources delete 7c78f5cf-ccf3-4d69-ae0b-a75648e5cddb </description>
    </item>
    <item>
      <title>Configure Dell Leaf Switch</title>
      <link>/docs-csm/en-10/install/configure_dell_leaf_switch/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:05 +0000</pubDate>
      <guid>/docs-csm/en-10/install/configure_dell_leaf_switch/</guid>
      <description>Configure Dell Leaf Switch This page describes how Dell leaf switches are configured.&#xA;Leaf switches are located in air-cooled cabinets and provide connectivity to components in those cabinets.&#xA;Prerequisites Connectivity to the switch is established. There are two uplinks from the switch to the upstream switch; this is can be an aggregation switch or a spine switch. Here are example snippets from a leaf switch in the SHCD.&#xA;Source Source Label Info Destination Label Info Destination Description sw-smn01 x3000u40-j49 x3105u38-j47 sw-25g01 25g-15m-LC-LC sw-smn01 x3000u40-j50 x3105u39-j47 sw-25g02 25g-15m-LC-LC The uplinks are port 49 and 50 on the leaf.</description>
    </item>
    <item>
      <title>Troubleshoot Insufficient Standby MDS Daemons Available</title>
      <link>/docs-csm/en-10/operations/utility_storage/troubleshoot_insufficient_standby_mds_daemons_available/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/troubleshoot_insufficient_standby_mds_daemons_available/</guid>
      <description>Troubleshoot Insufficient Standby MDS Daemons Available Procedure Log into a node running ceph-mon. Typically this will be ncn-s001/2/3.&#xA;Check the ceph health.&#xA;ceph health detail Example Output:&#xA;HEALTH_WARN insufficient standby MDS daemons available [WRN] MDS_INSUFFICIENT_STANDBY: insufficient standby MDS daemons available have 0; want 1 more This output explicitly states that you need at least 1 more to clear the alert.&#xA;Determine which MDS daemons are down.&#xA;ceph orch ps --daemon_type mds Example Output:</description>
    </item>
    <item>
      <title>Create Internal User Accounts in the Keycloak Shasta Realm</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/create_internal_user_accounts_in_the_keycloak_shasta_realm/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/create_internal_user_accounts_in_the_keycloak_shasta_realm/</guid>
      <description>Create Internal User Accounts in the Keycloak Shasta Realm The following manual procedure can be used to create a user in the Keycloak Shasta realm. New accounts can be created with the Keycloak UI.&#xA;New administrator and user accounts are authenticated with Keycloak. Authenticated accounts are needed to use the Cray CLI.&#xA;Prerequisites This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process.</description>
    </item>
    <item>
      <title>Update Compute Node Mellanox HSN NIC Firmware</title>
      <link>/docs-csm/en-10/operations/node_management/update_compute_node_mellanox_hsn_nic_firmware/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/update_compute_node_mellanox_hsn_nic_firmware/</guid>
      <description>Update Compute Node Mellanox HSN NIC Firmware This procedure updates liquid-cooled or standard rack compute node NIC mezzanine cards (NMC) firmware for Slingshot 10 Mellanox ConnectX-5 NICs. The deployed RPM on compute nodes contains the scripts and firmware images required to perform the firmware and configuration updates.&#xA;Attention: The NIC firmware update is performed while the node is running the compute image (in-band). Use the CX-5 NIC firmware that is deployed with the compute node RPMs and not from some other repository.</description>
    </item>
    <item>
      <title>Change Settings in the Bond</title>
      <link>/docs-csm/en-10/operations/node_management/change_settings_in_the_bond/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/change_settings_in_the_bond/</guid>
      <description>Change Settings in the Bond iPXE is used to setup udev rules for interface names and bond members. The configuration of these are dynamic on boot until node customization runs (cloud-init) and sets up the conventional /etc/sysconfig/network/ifcfg-bond0 and other neighboring files.&#xA;The initial settings of the bonds can be changed directly in the LiveCD or with the Boot Script Service (BSS). When cabling is different than normal, there is flexibility for customizing bond links.</description>
    </item>
    <item>
      <title>Pod Resource Limits</title>
      <link>/docs-csm/en-10/operations/kubernetes/pod_resource_limits/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:24 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/pod_resource_limits/</guid>
      <description>Pod Resource Limits Kubernetes uses resource requests and Quality of Service (QoS) for scheduling pods. Resource requests can be provided explicitly for pods and containers, whereas pod QoS is implicit, based on the resource requests and limits of the containers in the pod. There are three types of QoS:&#xA;Guaranteed: All containers in a pod have explicit memory and CPU resource requests and limits. For each resource, the limit equals the request.</description>
    </item>
    <item>
      <title>Manage Multiple Inventories in a Single Location</title>
      <link>/docs-csm/en-10/operations/configuration_management/manage_multiple_inventories_in_a_single_location/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/manage_multiple_inventories_in_a_single_location/</guid>
      <description>Manage Multiple Inventories in a Single Location Many configuration layers may be present in a single configuration for larger systems that configure multiple Cray products. When values for each of these layers need to be customized, it can be tedious to override values in each of the respective repositories. The CFS additionalInventoryUrl option allows for static inventory files to be automatically added to the hosts directory of each configuration layer before it is applied by Ansible.</description>
    </item>
    <item>
      <title>Manage a BOS Session</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/manage_a_bos_session/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/manage_a_bos_session/</guid>
      <description>Manage a BOS Session Once there is a Boot Orchestration Service (BOS) session template created, users can perform operations on nodes, such as boot, reboot, configure, and shutdown. Managing sessions through the Cray CLI can be accomplished using the cray bos session commands.&#xA;Create a new session List all sessions Show details for a session Delete a session Create a new session Creating a new BOS session requires the following command-line options:</description>
    </item>
    <item>
      <title>Delete a UAI Using an Administrative Command</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/delete_a_uai_using_an_administrative_command/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:10 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/delete_a_uai_using_an_administrative_command/</guid>
      <description>Delete a UAI Using an Administrative Command Manually delete one or more UAIs.&#xA;Prerequisites Install and initialize the cray administrative CLI.&#xA;Procedure Delete one or more UAIs using a command of the following form:&#xA;ncn-m001-pit# cray uas admin uais delete OPTIONS OPTIONS is one or more of the following:&#xA;--owner USERNAME: delete all UAIs owned by the named user --class-id CLASS_ID: delete all UAIs of the specified UAI class --uai-list LIST_OF_UAI_NAMES: delete all the listed UAIs The following example deletes two UAIs by name:</description>
    </item>
    <item>
      <title>Configure Management Network Switches</title>
      <link>/docs-csm/en-10/install/configure_management_network/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:05 +0000</pubDate>
      <guid>/docs-csm/en-10/install/configure_management_network/</guid>
      <description>Configure Management Network Switches HPE Cray EX systems can have network switches in many roles: spine switches, leaf switches, aggregation switches, and CDU switches. Newer systems have HPE Aruba switches, while older systems have Dell and Mellanox switches. Switch IP addresses are generated by Cray Site Init (CSI).&#xA;The configuration steps are different for these switch vendors. The switch configuration procedures for HPE Aruba will be grouped separately from the switch configuration procedures for other vendors.</description>
    </item>
    <item>
      <title>Troubleshoot Large Object Map Objects in Ceph Health</title>
      <link>/docs-csm/en-10/operations/utility_storage/troubleshoot_large_object_map_objects_in_ceph_health/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/troubleshoot_large_object_map_objects_in_ceph_health/</guid>
      <description>Troubleshoot Large Object Map Objects in Ceph Health Troubleshoot an issue where Ceph reports a HEALTH_WARN of 1 large omap objects. Adjust the omap object key threshold or number of placement groups (PG) to resolve this issue.&#xA;Prerequisites Ceph health is reporting a HEALTH_WARN for large Object Map (omap) objects.&#xA;ncn-m001# ceph -s cluster: id: 464f8ee0-667d-49ac-a82b-43ba8d377f81 health: HEALTH_WARN 1 large omap objects clock skew detected on mon.ncn-m002, mon.ncn-m003 services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby osd: 18 osds: 18 up (since 20h), 18 in (since 9d) rgw: 3 daemons active (ncn-s001.</description>
    </item>
    <item>
      <title>Create a Backup of the Keycloak Postgres Database</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/create_a_backup_of_the_keycloak_postgres_database/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/create_a_backup_of_the_keycloak_postgres_database/</guid>
      <description>Create a Backup of the Keycloak Postgres Database Perform a manual backup of the contents of the Keycloak Postgres database. This backup can be used to restore the contents of the Keycloak Postgres database at a later point in time using the Restore Keycloak Postgres from Backup procedure.&#xA;Prerequisites Healthy Keycloak Postgres Cluster.&#xA;Use patronictl list on the Keycloak Postgres cluster to determine the current state of the cluster and note which member is the Leader.</description>
    </item>
    <item>
      <title>Update the Gigabyte Node BIOS Time</title>
      <link>/docs-csm/en-10/operations/node_management/update_the_gigabyte_node_bios_time/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/update_the_gigabyte_node_bios_time/</guid>
      <description>Update the Gigabyte Node BIOS Time Check and set the time for Gigabyte compute nodes.&#xA;If the console log indicates the time between the rest of the system and the compute nodes is off by several hours, then it prevents the spire-agent from getting a valid certificate, which causes the node boot to drop into the dracut emergency shell.&#xA;Procedure Retrieve the cray-console-operator pod ID.&#xA;ncn-mw# CONPOD=$(kubectl get pods -n services -o wide|grep cray-console-operator|awk &amp;#39;{print $1}&amp;#39;); echo ${CONPOD} Example output:</description>
    </item>
    <item>
      <title>Check and Set the metal.no-wipe Setting on NCNs</title>
      <link>/docs-csm/en-10/operations/node_management/check_and_set_the_metalno-wipe_setting_on_ncns/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/check_and_set_the_metalno-wipe_setting_on_ncns/</guid>
      <description>Check and Set the metal.no-wipe Setting on NCNs Configure the metal.no-wipe setting on non-compute nodes (NCNs) to preserve data on the nodes before doing an NCN reboot.&#xA;Run the ncnGetXnames.shscript to view the metal.no-wipe settings for each NCN. The component name (xname) and metal.no-wipe settings are also dumped out when executing the /opt/cray/platform-utils/ncnHealthChecks.sh script.&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Procedure Run the ncnGetXnames.sh script.&#xA;The output will end with a listing of all of the NCNs, their component names (xnames), and what the metal.</description>
    </item>
    <item>
      <title>Rebalance Healthy etcd Clusters</title>
      <link>/docs-csm/en-10/operations/kubernetes/rebalance_healthy_etcd_clusters/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:24 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/rebalance_healthy_etcd_clusters/</guid>
      <description>Rebalance Healthy etcd Clusters Rebalance the etcd clusters. The clusters need to be in a healthy state, and there needs to be the same number of pods running on each worker node for the etcd clusters to be balanced.&#xA;Restoring the balance of etcd clusters will help with the storage of Kubernetes cluster data.&#xA;Prerequisites etcd clusters are in a healthy state. etcd clusters do not have the same number of pods on each worker node.</description>
    </item>
    <item>
      <title>Set Limits for a Configuration Session</title>
      <link>/docs-csm/en-10/operations/configuration_management/set_limits_for_a_configuration_session/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/set_limits_for_a_configuration_session/</guid>
      <description>Set Limits for a Configuration Session The configuration layers and session hosts can be limited when running a Configuration Framework Service (CFS) session.&#xA;Limit CFS session hosts Subsets of nodes can be targeted in the inventory when running CFS sessions, which is useful specifically when running a session with dynamic inventory. Use the CFS --ansible-limit option when creating a session to apply the limits. The option directly corresponds to the --limit option offered by ansible-playbook, and can be used to specify hosts, groups, or combinations of them with patterns.</description>
    </item>
    <item>
      <title>Manage a Session Template</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/manage_a_session_template/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/manage_a_session_template/</guid>
      <description>Manage a Session Template A session template must be created before starting a session with the Boot Orchestration Service (BOS).&#xA;This page shows Cray CLI commands for managing BOS session templates. To find the API versions of any commands listed, add -vvv to the end of the CLI command, and the CLI will print the underlying call to the API in the output.&#xA;Session template framework Create a session template [Create with the Cray CLI](#create with-the-cray-cli) Create with a Bash script Resulting template List all session templates View a session template Delete a session template Session template framework When creating a new BOS session template, it can be helpful to start with a framework and then edit it as needed.</description>
    </item>
    <item>
      <title>Delete a Volume Configuration</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/delete_a_volume_configuration/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:10 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/delete_a_volume_configuration/</guid>
      <description>Delete a Volume Configuration Delete an existing volume configuration. This procedure does not delete the underlying object referred to by the UAS volume configuration.&#xA;Prerequisites Install and initialize the cray administrative CLI. Obtain the volume_id of the UAS volume to delete. Perform List Volumes Registered in UAS if necessary. Procedure Delete the target volume configuration.&#xA;To delete a UAS Volume, use a command of the following form:&#xA;ncn-m001-pit# cray uas admin config volumes delete &amp;lt;volume-id&amp;gt; For example:</description>
    </item>
    <item>
      <title>Configure Mellanox Spine Switch</title>
      <link>/docs-csm/en-10/install/configure_mellanox_spine_switch/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:05 +0000</pubDate>
      <guid>/docs-csm/en-10/install/configure_mellanox_spine_switch/</guid>
      <description>Configure Mellanox Spine Switch This page describes how Mellanox spine switches are configured.&#xA;Depending on the size of the HPE Cray EX system, the spine switches will serve different purposes. On TDS systems, the NCNs will plug directly into the spine switches. On larger systems with aggregation switches, the spine switches will provide connection between the aggregation switches.&#xA;Prerequisites One connection between the switches is used for the Inter switch link (ISL).</description>
    </item>
    <item>
      <title>Troubleshoot Pods Failing to Restart on Other Worker Nodes</title>
      <link>/docs-csm/en-10/operations/utility_storage/troubleshoot_pods_multi-attach_error/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/troubleshoot_pods_multi-attach_error/</guid>
      <description>Troubleshoot Pods Failing to Restart on Other Worker Nodes Troubleshoot an issue where pods cannot restart on another worker node because of the &amp;ldquo;Volume is already exclusively attached to one node and can&amp;rsquo;t be attached to another&amp;rdquo; error. Kubernetes does not currently support &amp;ldquo;readwritemany&amp;rdquo; access mode for Rados Block Device (RBD) devices, which causes an issue where devices fail to unmap correctly.&#xA;The issue occurs when unmounting the mounts tied to the RBD devices, which causes the rbd-task (watcher) to not stop for the RBD device.</description>
    </item>
    <item>
      <title>Create a Service Account in Keycloak</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/create_a_service_account_in_keycloak/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/create_a_service_account_in_keycloak/</guid>
      <description>Create a Service Account in Keycloak Set up a Keycloak service account using the Keycloak administration console or the Keycloak REST API. A service account can be used to get a long-lived token that is used by automation tools.&#xA;In Keycloak, service accounts are associated with a client. See Service Accounts for more information from the Keycloak documentation.&#xA;Procedure Follow the steps in only one of the following sections, depending on if it is preferred to use the Keycloak REST API or the Keycloak administration console UI.</description>
    </item>
    <item>
      <title>Updating Cabinet Routes on Management NCNs</title>
      <link>/docs-csm/en-10/operations/node_management/updating_cabinet_routes_on_management_ncns/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/updating_cabinet_routes_on_management_ncns/</guid>
      <description>Updating Cabinet Routes on Management NCNs This procedure will use configuration from System Layout Service (SLS) to set up the proper routing for all air-cooled and liquid-cooled cabinets present in the system on each of the Management NCNs.&#xA;Prerequisites Passwordless SSH to all of the management NCNs is configured.&#xA;Ensure that Cray Site Init (CSI) is installed and available on ncn-m001.&#xA;ncn-m001# csi version If the csi command is not available, then install it:</description>
    </item>
    <item>
      <title>Check the BMC Failover Mode</title>
      <link>/docs-csm/en-10/operations/node_management/check_the_bmc_failover_mode/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/check_the_bmc_failover_mode/</guid>
      <description>Check the BMC Failover Mode Gigabyte BMCs must have their failover mode disabled to prevent incorrect network assignment.&#xA;If Gigabyte BMC failover mode is not disabled, then some BMCs may receive incorrect IP addresses. Specifically, a BMC may request an IP address on the wrong subnet and be unable to re-acquire a new IP address on the correct subnet. If this occurs, administrators should ensure that the impacted BMC has its failover feature disabled.</description>
    </item>
    <item>
      <title>Rebuild Unhealthy etcd Clusters</title>
      <link>/docs-csm/en-10/operations/kubernetes/rebuild_unhealthy_etcd_clusters/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:24 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/rebuild_unhealthy_etcd_clusters/</guid>
      <description>Rebuild Unhealthy etcd Clusters Rebuild any cluster that does not have healthy pods by deleting and redeploying unhealthy pods. This procedure includes examples for rebuilding etcd clusters in the services namespace. This procedure must be used for each unhealthy cluster, and not just those used in the following examples.&#xA;This process also applies when etcd is not visible when running the kubectl get pods command.&#xA;The commands in this procedure can be run on any Kubernetes master or worker node on the system.</description>
    </item>
    <item>
      <title>Set the ansible.cfg for a Session</title>
      <link>/docs-csm/en-10/operations/configuration_management/set_the_ansible-cfg_for_a_session/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/set_the_ansible-cfg_for_a_session/</guid>
      <description>Set the ansible.cfg for a Session View and update the Ansible configuration used by CFS.&#xA;Ansible configuration is available through the ansible.cfg file. See the Configuring Ansible external documentation for more information about what values can be set.&#xA;CFS provides a default ansible.cfg file in the cfs-default-ansible-cfg Kubernetes ConfigMap in the services namespace.&#xA;To view the ansible.cfg file:&#xA;ncn-mw# kubectl get cm -n services cfs-default-ansible-cfg -o json | jq -r &amp;#39;.</description>
    </item>
    <item>
      <title>Node Boot Root Cause Analysis</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/node_boot_root_cause_analysis/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/node_boot_root_cause_analysis/</guid>
      <description>Node Boot Root Cause Analysis The first step in debugging compute node boot-related issues is to determine the underlying cause, and the stage that the issue was encountered at.&#xA;The ConMan tool collects compute node logs. To learn more about ConMan, refer to ConMan.&#xA;A node&amp;rsquo;s console data can be accessed through its log file, as described in Access Compute Node Logs). This information can also be accessed by connecting to the node&amp;rsquo;s console with ipmitool.</description>
    </item>
    <item>
      <title>Elements of a UAI</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/elements_of_a_uai/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:10 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/elements_of_a_uai/</guid>
      <description>Elements of a UAI All UAIs can have the following attributes associated with them:&#xA;A required container image An optional set of volumes An optional resource specification An optional collection of other configuration items This topic explains each of these attributes.&#xA;UAI container image The container image for a UAI (UAI image) defines and provides the basic environment available to the user. This environment includes, among other things:&#xA;The operating system (including version) Preinstalled packages A site can customize UAI images and add those images to UAS, allowing them to be used for UAI creation.</description>
    </item>
    <item>
      <title>Connect to Switch over USB-Serial Cable</title>
      <link>/docs-csm/en-10/install/connect_to_switch_over_usb_serial_cable/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:05 +0000</pubDate>
      <guid>/docs-csm/en-10/install/connect_to_switch_over_usb_serial_cable/</guid>
      <description>Connect to Switch over USB-Serial Cable In the event that network plumbing is lacking, down, or unconfigured for procuring devices, then it is recommended to use the Serial/COM ports on the spine and leaf switches.&#xA;This guide will instruct the user on procuring MAC addresses for the NCNs metadata files with the serial console.&#xA;Mileage may vary, as some obstacles such as BAUDRATE and terminal usage vary per manufacturer.&#xA;Common Manufacturers Refer to the external support/documentation portals for more information:</description>
    </item>
    <item>
      <title>Troubleshoot if RGW Health Check Fails</title>
      <link>/docs-csm/en-10/operations/utility_storage/troubleshoot_rgw_health_check_fail/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/troubleshoot_rgw_health_check_fail/</guid>
      <description>Troubleshoot if RGW Health Check Fails Use this procedure to determine why the rgw health check failed and what needs to be fixed.&#xA;Procedure In the goss test output, look at the value of x in Expected \&amp;lt; int \&amp;gt;: x (possible values are 1, 2, 3, 4, 5). Based on the value, navigate to the corresponding numbered item below for troubleshooting this issue.&#xA;Optional: Manually run the rgw health check script to see descriptive output.</description>
    </item>
    <item>
      <title>Default Keycloak Realms, Accounts, and Clients</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/default_keycloak_realms_accounts_and_clients/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/default_keycloak_realms_accounts_and_clients/</guid>
      <description>Default Keycloak Realms, Accounts, and Clients This page details the default Keycloak realms, accounts, and clients that are created when the system software is installed.&#xA;Default realms Default accounts Default clients Private clients Public clients Default realms Master Shasta Default accounts Username: admin&#xA;The password can be obtained with the following command:&#xA;ncn-mw# kubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode The password for the admin account can be changed.</description>
    </item>
    <item>
      <title>Use the Physical KVM</title>
      <link>/docs-csm/en-10/operations/node_management/use_the_physical_kvm/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/use_the_physical_kvm/</guid>
      <description>Use the Physical KVM For those who prefer to stand in front of the system and use a physically connected keyboard, mouse, and monitor, Cray provides a rack-mount-extendable KVM unit installed in rack unit slot 23 (RU23) of the management cabinet. It is connected to the first non-compute node (NCN) by default.&#xA;To use it, pull it out and raise the lid.&#xA;To bring up the main menu (shown in following figure), press Prnt Scrn.</description>
    </item>
    <item>
      <title>Clear Space in Root File System on Worker Nodes</title>
      <link>/docs-csm/en-10/operations/node_management/clear_space_in_root_file_system_on_worker_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/clear_space_in_root_file_system_on_worker_nodes/</guid>
      <description>Clear Space in Root File System on Worker Nodes The disk space on an NCN worker node can fill up if any services are consuming a large portion of the root file system on the node. This procedure shows how to safely clear some space on worker nodes to return them to an appropriate storage threshold.&#xA;Prerequisites An NCN worker node has a full disk.&#xA;Procedure Check to see if Docker is running.</description>
    </item>
    <item>
      <title>Recover from Postgres WAL Event</title>
      <link>/docs-csm/en-10/operations/kubernetes/recover_from_postgres_wal_event/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:24 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/recover_from_postgres_wal_event/</guid>
      <description>Recover from Postgres WAL Event A WAL event can occur because of lag, network communication, or bandwidth issues. This can cause the PVC hosted by Ceph and mounted inside the container on /home/postgres/pgdata to fill and the database to stop running. If no database dump exists, then the disk space issue needs to be fixed so that a dump can be taken. Then the dump can be restored to a newly created postgresql cluster.</description>
    </item>
    <item>
      <title>Specifying Hosts and Groups</title>
      <link>/docs-csm/en-10/operations/configuration_management/specifying_hosts_and_groups/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/specifying_hosts_and_groups/</guid>
      <description>Specifying Hosts and Groups When using the Configuration Framework Service (CFS), there are many steps where users may need to specify the hosts that CFS should configure. This can be done by specifying individual hosts, or groups of hosts. There are several places where a user may need to provide this information, particularly groups, and depending on where this information is provided, the behavior can change greatly.&#xA;Inventories CFS has multiple options for generating inventories, but regardless of which option is used, the information is then converted into an Ansible inventory/hosts file.</description>
    </item>
    <item>
      <title>Redeploy the iPXE and TFTP Services</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/redeploy_the_ipxe_and_tftp_services/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/redeploy_the_ipxe_and_tftp_services/</guid>
      <description>Redeploy the iPXE and TFTP Services Redeploy the iPXE and TFTP services if a pod with a ceph-fs Process Virtualization Service (PVS) on a Kubernetes worker node is causing a HEALTH_WARN error.&#xA;Resolve issues with ceph-fs and ceph-mds by restarting the iPXE and TFTP services. The Ceph cluster will return to a healthy state after this procedure.&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Procedure Find the iPXE and TFTP deployments.</description>
    </item>
    <item>
      <title>End-User UAIs</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/end_user_uais/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:10 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/end_user_uais/</guid>
      <description>End-User UAIs UAIs used for interactive logins are called end-user UAIs. End-user UAIs can be seen as lightweight User Access Nodes (UANs), but there are important differences between UAIs and UANs. First, end-user UAIs are not dedicated hardware like UANs. They are implemented as containers orchestrated by Kubernetes, which makes them subject to Kubernetes scheduling and resource management rules. One key element of Kubernetes orchestration is impermanence. While end-user UAIs are often long running, Kubernetes can reschedule or recreate them as needed to meet resource and node availability constraints.</description>
    </item>
    <item>
      <title>Create Application Node Config YAML</title>
      <link>/docs-csm/en-10/install/create_application_node_config_yaml/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:05 +0000</pubDate>
      <guid>/docs-csm/en-10/install/create_application_node_config_yaml/</guid>
      <description>Create Application Node Config YAML This topic provides directions on constructing the application_node_config.yaml file. This file controls how the csi config init command finds and treats application nodes discovered in the hmn_connections.json file when generating configuration files for the system.&#xA;Prerequisites Background Directions Prerequisites The application_node_config.yaml file can be constructed from information from one of the following sources:&#xA;The SHCD Excel spreadsheet for the system The hmn_connections.json file generated from the system&amp;rsquo;s SHCD Background SHCD and hmn_connections.</description>
    </item>
    <item>
      <title>Troubleshoot System Clock Skew</title>
      <link>/docs-csm/en-10/operations/utility_storage/troubleshoot_system_clock_skew/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/troubleshoot_system_clock_skew/</guid>
      <description>Troubleshoot System Clock Skew Resynchronize system clocks after Ceph reports a clock skew.&#xA;Systems use chronyd to synchronize their system clocks. If systems are not able to communicate, then the clocks can drift, causing clock skew. Another reason for this issue would be an individual manually changing the clocks or a task that may change the clocks and require a series of steps (time adjustments) to resynchronize.&#xA;Major time jumps where the clock is set back in time will require a full restart of all Ceph services.</description>
    </item>
    <item>
      <title>Delete Internal User Accounts in the Keycloak Shasta Realm</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/delete_internal_user_accounts_from_the_keycloak_shasta_realm/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/delete_internal_user_accounts_from_the_keycloak_shasta_realm/</guid>
      <description>Delete Internal User Accounts in the Keycloak Shasta Realm Manually delete a user account in the Keycloak Shasta realm. User accounts are maintained via the Keycloak user management UI.&#xA;Removing an account from Keycloak is a good way to revoke admin or user privileges.&#xA;Prerequisites This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process. The password can be obtained with the following command:</description>
    </item>
    <item>
      <title>Verify Node Removal</title>
      <link>/docs-csm/en-10/operations/node_management/verify_node_removal/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/verify_node_removal/</guid>
      <description>Verify Node Removal Use this procedure to verify that a node has been successfully removed from the system.&#xA;Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. This procedure requires the component name (xname) of the removed node to be known. Procedure Ensure that the Redfish endpoint of the removed node&amp;rsquo;s BMC has been disabled.&#xA;ncn-m001# cray hsm inventory redfishEndpoints describe x3000c0s19b4 --format toml In the following example output, the Enabled field is false, indicating that the Redfish endpoint has been disabled.</description>
    </item>
    <item>
      <title>Configuration of NCN Bonding</title>
      <link>/docs-csm/en-10/operations/node_management/configuration_of_ncn_bonding/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/configuration_of_ncn_bonding/</guid>
      <description>Configuration of NCN Bonding Non-compute nodes (NCNs) have network interface controllers (NICs) connected to the management network that are configured in a redundant manner via Link Aggregation Control Protocol (LACP) link aggregation. The link aggregation configuration can be modified by editing and applying various configuration files either through Ansible or the interfaces directly.&#xA;The bond configuration exists across three files on an NCN. These files may vary depending on the NCN in use:</description>
    </item>
    <item>
      <title>Repopulate Data in etcd Clusters When Rebuilding Them</title>
      <link>/docs-csm/en-10/operations/kubernetes/repopulate_data_in_etcd_clusters_when_rebuilding_them/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/repopulate_data_in_etcd_clusters_when_rebuilding_them/</guid>
      <description>Repopulate Data in etcd Clusters When Rebuilding Them When an etcd cluster is not healthy, it needs to be rebuilt. During that process, the pods that rely on etcd clusters lose data. That data needs to be repopulated in order for the cluster to go back to a healthy state.&#xA;Applicable services Prerequisites Procedures BOS BSS CPS CRUS External DNS FAS HMNFD MEDS REDS Applicable services The following services need their data repopulated in the etcd cluster:</description>
    </item>
    <item>
      <title>Target Ansible Tasks for Image Customization</title>
      <link>/docs-csm/en-10/operations/configuration_management/target_ansible_tasks_for_image_customization/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/target_ansible_tasks_for_image_customization/</guid>
      <description>Target Ansible Tasks for Image Customization The Configuration Framework Service (CFS) enables Ansible playbooks to run against both running nodes and images. See the &amp;ldquo;Use Cases&amp;rdquo; header in the Configuration Management section for more information about image customization and when it should be used.&#xA;CFS uses the cray_cfs_image variable to distinguish between node personalization (running on live nodes) and image customization (configuring an image prior to boot). When this variable is set to true, it indicates that the CFS session is an image customization type and the playbook is targeting an image.</description>
    </item>
    <item>
      <title>BOS Session Templates</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/session_templates/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/session_templates/</guid>
      <description>BOS Session Templates Session templates in the Boot Orchestration Service (BOS) are a reusable collection of boot, configuration, and component information. After creation they can be combined with a boot operation to create a BOS session that will apply the desired changes to the specified components. Session templates can be created via the API by providing JSON data or via the CLI by writing the JSON data to a file, which can then be referenced using the --file parameter.</description>
    </item>
    <item>
      <title>Examine a UAI Using a Direct Administrative Command</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/examine_a_uai_using_a_direct_administrative_command/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:10 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/examine_a_uai_using_a_direct_administrative_command/</guid>
      <description>Examine a UAI Using a Direct Administrative Command Print out information about a UAI.&#xA;Prerequisites Install and initialize the cray administrative CLI.&#xA;Procedure Print out information about a UAI.&#xA;To examine an existing UAI use a command of the following form:&#xA;linux# cray uas admin uais describe &amp;lt;uai-name&amp;gt; For example:&#xA;ncn-m001-pit# cray uas admin uais describe uai-vers-715fa89d Example output:&#xA;uai_age = &amp;#34;2d23h&amp;#34; uai_connect_string = &amp;#34;ssh vers@10.28.212.166&amp;#34; uai_host = &amp;#34;ncn-w001&amp;#34; uai_img = &amp;#34;registry.</description>
    </item>
    <item>
      <title>Create Cabinets YAML</title>
      <link>/docs-csm/en-10/install/create_cabinets_yaml/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:05 +0000</pubDate>
      <guid>/docs-csm/en-10/install/create_cabinets_yaml/</guid>
      <description>Create Cabinets YAML This page provides directions on constructing the cabinets.yaml file. This file lists cabinet IDs for any systems with non-contiguous cabinet ID numbers, as well as VLAN overrides, and controls how the csi config init command treats cabinet IDs.&#xA;The following example file is manually created and follows this format. Each &amp;ldquo;type&amp;rdquo; of cabinet can have several fields: total_number of cabinets of this type, starting_id for this cabinet type, and a list of the IDs.</description>
    </item>
    <item>
      <title>Troubleshoot a Down OSD</title>
      <link>/docs-csm/en-10/operations/utility_storage/troubleshoot_a_down_osd/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/troubleshoot_a_down_osd/</guid>
      <description>Troubleshoot a Down OSD Identify down OSDs and manually bring them back up.&#xA;Troubleshoot the Ceph health detail reporting down OSDs. Ensuring that OSDs are operational and data is balanced across them will help remove the likelihood of hotspots being created.&#xA;Prerequisites This procedure requires admin privileges.&#xA;Procedure Identify the down OSDs.&#xA;ncn-m/s(001/2/3)# ceph osd tree down ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 62.87558 root default -7 20.</description>
    </item>
    <item>
      <title>Get a Long-Lived Token for a Service Account</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/get_a_long-lived_token_for_a_service_account/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/get_a_long-lived_token_for_a_service_account/</guid>
      <description>Get a Long-Lived Token for a Service Account Set up a long-lived offline token for a service account using the Keycloak REST API. Keycloak implements the OpenID Connect protocol, so this is a standard procedure for any OpenID Connect server.&#xA;Refer to Offline Access in the official Keycloak documentation for more information.&#xA;Prerequisites A client or service account has been created. See Create a Service Account in Keycloak. The CLIENT_SECRET variable has been set up.</description>
    </item>
    <item>
      <title>View BIOS Logs for Liquid-Cooled Nodes</title>
      <link>/docs-csm/en-10/operations/node_management/view_bios_logs_for_liquid_cooled_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:32 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/view_bios_logs_for_liquid_cooled_nodes/</guid>
      <description>View BIOS Logs for Liquid-Cooled Nodes SSH to a liquid-cooled node and view the BIOS logs. The BIOS logs for liquid-cooled node controllers (nC) are stored in the /var/log/n0/current and /var/log/n1/current directories.&#xA;The BIOS logs for liquid-cooled nodes are helpful for troubleshooting boot-related issues.&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Procedure Log in to the node.&#xA;SSH into the node controller for the host component name (xname). For example, if the host xname (as defined in /etc/hosts) is x5000c1s0b0n0, then the node controller would be x5000c1s0b0.</description>
    </item>
    <item>
      <title>Configure NTP on NCNs</title>
      <link>/docs-csm/en-10/operations/node_management/configure_ntp_on_ncns/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/configure_ntp_on_ncns/</guid>
      <description>Configure NTP on NCNs The management nodes serve Network Time Protocol (NTP) at stratum 10, except for ncn-m001, which serves at stratum 8 (or lower if an upstream NTP server is set). All management nodes peer with each other.&#xA;Until an upstream NTP server is configured, the time on the NCNs may not match the current time at the site, but they will stay in sync with each other.&#xA;Topics</description>
    </item>
    <item>
      <title>Report the Endpoint Status for etcd Clusters</title>
      <link>/docs-csm/en-10/operations/kubernetes/report_the_endpoint_status_for_etcd_clusters/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/report_the_endpoint_status_for_etcd_clusters/</guid>
      <description>Report the Endpoint Status for etcd Clusters Report etcd cluster end point status. The report includes a cluster&amp;rsquo;s endpoint, database size, and leader status.&#xA;This procedure provides the ability to view the etcd cluster endpoint status.&#xA;Prerequisites This procedure requires root privileges. The etcd clusters are in a healthy state. Procedure Report the endpoint status for all etcd clusters in a namespace.&#xA;The following example is for the services namespace.</description>
    </item>
    <item>
      <title>Track the Status of a Session</title>
      <link>/docs-csm/en-10/operations/configuration_management/track_the_status_of_a_session/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/track_the_status_of_a_session/</guid>
      <description>Track the Status of a Session A configuration session can be a long-running process, and depends on many system factors, as well as the number of configuration layers and Ansible tasks that are run in each layer. The Configuration Framework Service (CFS) provides the session status through the session metadata to allow for tracking progress and session state.&#xA;Prerequisites View session status Troubleshooting Prerequisites A configuration session exists in CFS. The Cray CLI must be configured on the node where the commands are being run.</description>
    </item>
    <item>
      <title>BOS Sessions</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/sessions/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/sessions/</guid>
      <description>BOS Sessions The Boot Orchestration Service (BOS) creates a session when it is asked to perform an operation on a session template. Sessions provide a way to track the status of many nodes at once as they perform the same operation with the same session template information. When creating a session, both the operation and session template are required parameters.&#xA;BOA functionality BOS v1 session limitations The v1 version of BOS supports these operations:</description>
    </item>
    <item>
      <title>Legacy Mode User-Driven UAI Management</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/legacy_mode_user-driven_uai_management/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:10 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/legacy_mode_user-driven_uai_management/</guid>
      <description>Legacy Mode User-Driven UAI Management In the legacy mode, users create and manage their own UAIs through the Cray CLI. A user may create, list and delete only UAIs owned by the user. The user may not create a UAI for another user, nor may the user see or delete UAIs owned by another user. Once created, the information describing the UAI gives the user the information needed to reach the UAI using SSH and log into it.</description>
    </item>
    <item>
      <title>Create HMN Connections JSON File</title>
      <link>/docs-csm/en-10/install/create_hmn_connections_json/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:05 +0000</pubDate>
      <guid>/docs-csm/en-10/install/create_hmn_connections_json/</guid>
      <description>Create HMN Connections JSON File Use this procedure to generate the hmn_connections.json from the system&amp;rsquo;s SHCD Excel document. This process is typically needed when generating the hmn_connections.json file for a new system, or regenerating it when a system&amp;rsquo;s SHCD file is changed (specifically the HMN tab). The hms-shcd-parser tool can be used to generate the hmn_connections.json file.&#xA;The SHCD/HMN Connections Rules document explains the expected naming conventions and rules for the HMN tab of the SHCD file, and for the hmn_connections.</description>
    </item>
    <item>
      <title>Troubleshoot an Unresponsive Rados-Gateway (radosgw) S3 Endpoint</title>
      <link>/docs-csm/en-10/operations/utility_storage/troubleshoot_an_unresponsive_s3_endpoint/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/troubleshoot_an_unresponsive_s3_endpoint/</guid>
      <description>Troubleshoot an Unresponsive Rados-Gateway (radosgw) S3 Endpoint Issue 1: Rados-Gateway/s3 endpoint is not accessible ncn# response=$(curl --write-out &amp;#39;%{http_code}&amp;#39; --silent --output /dev/null http://rgw-vip)|echo &amp;#34;Curl Response Code: $response&amp;#34; Curl Response Code: 200 Expected Responses: 2xx, 3xx&#xA;Procedure: Check the individual endpoints.&#xA;ncn# num_storage_nodes=$(craysys metadata get num_storage_nodes);for node_num in $(seq 1 &amp;#34;$num_storage_nodes&amp;#34;); do nodename=$(printf &amp;#34;ncn-s%03d&amp;#34; &amp;#34;$node_num&amp;#34;); response=$(curl --write-out &amp;#39;%{http_code}&amp;#39; --silent --output /dev/null http://$nodename:8080); echo &amp;#34;Curl Response Code for ncn-s00$endpoint: $response&amp;#34;; done Curl Response Code for ncn-s003: 200 Curl Response Code for ncn-s003: 200 Curl Response Code for ncn-s003: 200 Troubleshooting: If an error occurs with the above script, then echo $num_storage_nodes.</description>
    </item>
    <item>
      <title>HashiCorp Vault</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/hashicorp_vault/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/hashicorp_vault/</guid>
      <description>HashiCorp Vault Overview Storage model Unseal keys Administrative access Kubernetes service account access Check the status of Vault clusters Overview A deployment of HashiCorp Vault, managed via the Bitnami Bank-vaults operator, stores private and public Certificate Authority (CA) material, and serves APIs through a PKI engine instance. This instance also serves as a general secrets engine for the system.&#xA;Kubernetes service account authorization is utilized to authenticate access to Vault. The configuration of Vault, as deployed on the system, can be viewed with the following command:</description>
    </item>
    <item>
      <title>Disable Nodes</title>
      <link>/docs-csm/en-10/operations/node_management/disable_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:29 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/disable_nodes/</guid>
      <description>Disable Nodes Use the Hardware State Manager (HSM) Cray CLI commands to disable nodes on the system.&#xA;Disabling nodes that are not configured correctly allows the system to successfully boot.&#xA;Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Procedure Disable one or more nodes with HSM.&#xA;ncn-m001# cray hsm state components bulkEnabled update --enabled false --component-ids XNAME_LIST Verify the desired nodes are disabled.</description>
    </item>
    <item>
      <title>Restore Bare-Metal etcd Clusters from an S3 Snapshot</title>
      <link>/docs-csm/en-10/operations/kubernetes/restore_bare-metal_etcd_clusters_from_an_s3_snapshot/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/restore_bare-metal_etcd_clusters_from_an_s3_snapshot/</guid>
      <description>Restore Bare-Metal etcd Clusters from an S3 Snapshot The etcd cluster that serves Kubernetes on master nodes is backed up every 10 minutes. These backups are pushed to Ceph Rados Gateway (S3).&#xA;Restoring the etcd cluster from backup is only meant to be used in a catastrophic scenario, whereby the Kubernetes cluster and master nodes are being rebuilt. This procedure shows how to restore the bare-metal etcd cluster from an Simple Storage Service (S3) snapshot.</description>
    </item>
    <item>
      <title>Troubleshoot Ansible Play Failures in CFS Sessions</title>
      <link>/docs-csm/en-10/operations/configuration_management/troubleshoot_ansible_play_failures_in_cfs_sessions/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/troubleshoot_ansible_play_failures_in_cfs_sessions/</guid>
      <description>Troubleshoot Ansible Play Failures in CFS Sessions View the Kubernetes logs for a Configuration Framework Service (CFS) pod in an error state to determine whether the error resulted from the CFS infrastructure or from an Ansible play that was run by a specific configuration layer in a CFS session.&#xA;Use this procedure to obtain important triage information for Ansible plays being called by CFS.&#xA;Prerequisites A failed configuration session exists in CFS.</description>
    </item>
    <item>
      <title>Stage Changes Without BOS</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/stage_changes_without_bos/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/stage_changes_without_bos/</guid>
      <description>Stage Changes Without BOS Sometimes there is a need to stages changes to take place on a reboot, without immediately rebooting a node. When this is called for, users can bypass BOS, and set boot artifacts or configuration that will only take place when a node is later booted, whether that occurs manually, or triggered by a task manager.&#xA;Stage Boot Artifacts For information on staging boot artifacts, see the section Upload Node Boot Information to Boot Script Service (BSS).</description>
    </item>
    <item>
      <title>List Available UAI Classes</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_available_uai_classes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:10 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_available_uai_classes/</guid>
      <description>List Available UAI Classes View all the details of every available UAI class. Use this information to select a class to apply to one or more UAIs.&#xA;Prerequisites Install and initialize the cray administrative CLI.&#xA;Procedure List all available UAI classes.&#xA;To list available UAI classes, use the following command:&#xA;ncn-m001-pit# cray uas admin config classes list The cray uas admin config classes list command supports the same --format options as the cray uas admin config volumes list command.</description>
    </item>
    <item>
      <title>Create NCN Metadata CSV</title>
      <link>/docs-csm/en-10/install/create_ncn_metadata_csv/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:05 +0000</pubDate>
      <guid>/docs-csm/en-10/install/create_ncn_metadata_csv/</guid>
      <description>Create NCN Metadata CSV The information in the ncn_metadata.csv file identifies each of the management nodes, assigns the function as a master, worker, or storage node, and provides the MAC address information needed to identify the BMC and the NIC which will be used to boot the node.&#xA;Some of the data in the ncn_metadata.csv can be found in the SHCD in the HMN tab. However, the hardest data to collect is the MAC addresses for the node&amp;rsquo;s BMC, the node&amp;rsquo;s bootable network interface, and the pair of network interfaces which will become the bonded interface bond0.</description>
    </item>
    <item>
      <title>Utility Storage</title>
      <link>/docs-csm/en-10/operations/utility_storage/utility_storage/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/utility_storage/utility_storage/</guid>
      <description>Utility Storage Utility storage is designed to support Kubernetes and the System Management Services (SMS) it orchestrates. Utility storage is a cost-effective solution for storing the large amounts of telemetry and log data collected.&#xA;Ceph is the utility storage platform that is used to enable pods to store persistent data. It is deployed to provide block, object, and file storage to the management services running on Kubernetes, as well as for telemetry data coming from the compute nodes.</description>
    </item>
    <item>
      <title>Keycloak Operations</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/keycloak_operations/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/keycloak_operations/</guid>
      <description>Keycloak Operations A service may need to access Keycloak to perform various tasks. These typical uses for a service to access Keycloak include creating a new service account, creating a new user, etc. These operations require Keycloak administrative access. As part of the System Management Services (SMS) installation process, Keycloak is initialized with a Master realm. An administrative client and user are created within this realm. The system installation process adds the information needed for the Keycloak administrator&amp;rsquo;s authentication into a Kubernetes secret that can be accessed by any pod.</description>
    </item>
    <item>
      <title>Dump a Non-Compute Node</title>
      <link>/docs-csm/en-10/operations/node_management/dump_a_non-compute_node/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/dump_a_non-compute_node/</guid>
      <description>Dump a Non-Compute Node Trigger a non-compute node (NCN) memory dump and send the dump for analysis. This procedure is helpful for debugging NCN crashes.&#xA;Prerequisites An NCN has crashed or an administrator has triggered a node crash.&#xA;Procedure Force a dump on an NCN.&#xA;ncn-m001# echo c &amp;gt; /proc/sysrq-trigger Wait for the node to reboot.&#xA;The NCN dump is stored in /var/crash on the local disk after the node is rebooted.</description>
    </item>
    <item>
      <title>Restore Postgres</title>
      <link>/docs-csm/en-10/operations/kubernetes/restore_postgres/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/restore_postgres/</guid>
      <description>Restore Postgres Below are the service-specific steps required to restore data to a Postgres cluster.&#xA;Restore Postgres procedures by service:&#xA;Restore Postgres for Spire Restore from backup Restore without backup Restore Postgres for Keycloak Restore Postgres for VCS Restore Postgres for Capsules Capsules Warehouse Server Capsules Dispatch Server Restore Postgres for HSM Restore from backup Restore without backup Restore Postgres for SLS Restore from backup Restore without backup Restore Postgres for Spire In the event that the Spire Postgres cluster must be rebuilt and the data restored, then the following procedures are recommended.</description>
    </item>
    <item>
      <title>Troubleshoot CFS Session Failing to Complete</title>
      <link>/docs-csm/en-10/operations/configuration_management/troubleshoot_cfs_session_failing_to_complete/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/troubleshoot_cfs_session_failing_to_complete/</guid>
      <description>Troubleshoot CFS Session Failing to Complete Troubleshoot issues where Configuration Framework Service (CFS) sessions/pods fail and Ansible hangs. These issues can be resolved by modifying Ansible to produce less output.&#xA;Prerequisites A CFS session or pod is failing to complete, and the Ansible logs are not showing progress or completion.&#xA;The following is an example of the error causing Ansible to hang:&#xA;PLAY [Compute] ***************************************************************** META: ran handlers META: ran handlers META: ran handlers PLAY [Compute] ***************************************************************** Using module file /usr/lib/python3.</description>
    </item>
    <item>
      <title>Tools for Resolving Compute Node Boot Issues</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/tools_for_resolving_boot_issues/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/tools_for_resolving_boot_issues/</guid>
      <description>Tools for Resolving Compute Node Boot Issues A number of tools can be used to analyze and debug issues encountered during the compute node boot process. The underlying issue and symptoms dictate the type of tool required.&#xA;nmap Use nmap to send out DHCP discover requests to test DHCP. nmap can be installed using the following command:&#xA;ncn# zypper install nmap To reach the DHCP server, the request generally needs to be sent over the Node Management network (NMN) from a non-compute node (NCN).</description>
    </item>
    <item>
      <title>List Available UAI Images in Legacy Mode</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_available_uai_images_in_legacy_mode/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:10 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_available_uai_images_in_legacy_mode/</guid>
      <description>List Available UAI Images in Legacy Mode A user can list the UAI images available for creating a UAI with a command of the form:&#xA;user&amp;gt; cray uas images list For example:&#xA;vers&amp;gt; cray uas images list default_image = &amp;#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest&amp;#34; image_list = [ &amp;#34;dtr.dev.cray.com/cray/cray-uai-broker:latest&amp;#34;, &amp;#34;dtr.dev.cray.com/cray/cray-uas-sles15:latest&amp;#34;, &amp;#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest&amp;#34;,] </description>
    </item>
    <item>
      <title>Create Switch Metadata CSV</title>
      <link>/docs-csm/en-10/install/create_switch_metadata_csv/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:05 +0000</pubDate>
      <guid>/docs-csm/en-10/install/create_switch_metadata_csv/</guid>
      <description>Create Switch Metadata CSV This page provides directions on constructing the switch_metadata.csv file.&#xA;Prerequisites The SHCD file for the system.&#xA;Check the description for component names while mapping names between the SHCD and the switch_metadata.csv file. See Component Names (xnames).&#xA;Overview This file is manually created to include information about all spine, leaf, CDU, and aggregation switches in the system. None of the Slingshot switches for the HSN should be included in this file.</description>
    </item>
    <item>
      <title>Validate CSM Health</title>
      <link>/docs-csm/en-10/operations/validate_csm_health/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:41 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/validate_csm_health/</guid>
      <description>Validate CSM Health Anytime after the installation of the CSM services, the health of the management nodes and all CSM services can be validated.&#xA;The following are examples of when to run health checks:&#xA;After CSM install.sh completes Before and after NCN reboots After the system is brought back up Any time there is unexpected behavior observed In order to provide relevant information to create support tickets The areas should be tested in the order they are listed on this page.</description>
    </item>
    <item>
      <title>Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/make_https_requests_from_sources_outside_the_management_kubernetes_cluster/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/make_https_requests_from_sources_outside_the_management_kubernetes_cluster/</guid>
      <description>Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster Clients lying outside the system&amp;rsquo;s management cluster need to trust the Certificate Authority (CA) certificate or host certificate in order to make requests to a non-compute node (NCN). Getting the client system to trust the CA certificate depends on the operating system.&#xA;This procedure shows an example of how to have a client trust the system&amp;rsquo;s CA certificate on a Mac OS X system.</description>
    </item>
    <item>
      <title>Enable Nodes</title>
      <link>/docs-csm/en-10/operations/node_management/enable_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/enable_nodes/</guid>
      <description>Enable Nodes Use the Hardware State Manager (HSM) Cray CLI commands to enable nodes on the system.&#xA;Enabling nodes that are available provides an accurate system configuration and node map.&#xA;Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Procedure Enable one or more nodes with HSM.&#xA;ncn-m001# cray hsm state components bulkEnabled update --enabled true --component-ids XNAME_LIST Verify the desired nodes are enabled.</description>
    </item>
    <item>
      <title>Restore an etcd Cluster from a Backup</title>
      <link>/docs-csm/en-10/operations/kubernetes/restore_an_etcd_cluster_from_a_backup/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/restore_an_etcd_cluster_from_a_backup/</guid>
      <description>Restore an etcd Cluster from a Backup Use an existing backup of a healthy etcd cluster to restore an unhealthy cluster to a healthy state.&#xA;The commands in this procedure can be run on any master or worker node on the system.&#xA;Prerequisites A backup of a healthy etcd cluster has been created.&#xA;Procedure List the backups for the desired etcd cluster.&#xA;The example below uses the Boot Orchestration Service (BOS).</description>
    </item>
    <item>
      <title>Update a CFS Configuration</title>
      <link>/docs-csm/en-10/operations/configuration_management/update_a_cfs_configuration/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/update_a_cfs_configuration/</guid>
      <description>Update a CFS Configuration Modify a Configuration Framework Service (CFS) configuration by specifying the JSON of the configuration and its layers. Use the cray cfs configurations update command, similar to creating a configuration.&#xA;Prerequisites A CFS configuration has been created. The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Procedure Add and/or remove the configuration layers from an existing JSON configuration file.</description>
    </item>
    <item>
      <title>Troubleshoot Booting Nodes with Hardware Issues</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_booting_nodes_with_hardware_issues/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_booting_nodes_with_hardware_issues/</guid>
      <description>Troubleshoot Booting Nodes with Hardware Issues This document explains how to identify a node with hardware issues.&#xA;If a node included in a BOS session template is having hardware issues, then it can prevent the node from powering back up correctly. The entire BOS session will fail with a timeout error waiting for the node to become ready.&#xA;The following example shows log output from a node with hardware issues, resulting in a failed BOS session:</description>
    </item>
    <item>
      <title>List Registered UAI Images</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_registered_uai_images/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:11 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_registered_uai_images/</guid>
      <description>List Registered UAI Images Administrators can use the cray uas admin config images list command to see the list of registered images. This command also displays the UAS registration information about each image.&#xA;Registering a UAI image name is insufficient to make that image available for UAIs. UAI images must also be registered, but also created and stored in the container registry need to link to procedure on how to do that.</description>
    </item>
    <item>
      <title>Deploy Management Nodes</title>
      <link>/docs-csm/en-10/install/deploy_management_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:05 +0000</pubDate>
      <guid>/docs-csm/en-10/install/deploy_management_nodes/</guid>
      <description>Deploy Management Nodes The following procedure deploys Linux and Kubernetes software to the management NCNs. Deployment of the nodes starts with booting the storage nodes followed by the master nodes and worker nodes together.&#xA;After the operating system boots on each node, there are some configuration actions which take place. Watching the console or the console log for certain nodes can help to understand what happens and when. When the process completes for all nodes, the Ceph storage is initialized and the Kubernetes cluster is created and ready for a workload.</description>
    </item>
    <item>
      <title>Manage Sealed Secrets</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/manage_sealed_secrets/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/manage_sealed_secrets/</guid>
      <description>Manage Sealed Secrets Sealed secrets are essential for managing sensitive information on the system. The following procedures for managing sealed secrets are included in this section:&#xA;Generate Sealed Secrets Post-Install Prevent Regeneration of Tracked Sealed Secrets View Tracked Sealed Secrets Decrypt Sealed Secrets for Review Fix an Incorrect Value in a Sealed Secret In the following sections, the term &amp;ldquo;tracked sealed secrets&amp;rdquo; is used to describe any existing secrets stored in spec.</description>
    </item>
    <item>
      <title>Enable Passwordless Connections to Liquid Cooled Node BMCs</title>
      <link>/docs-csm/en-10/operations/node_management/enable_passwordless_connections_to_liquid_cooled_node_bmcs/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/enable_passwordless_connections_to_liquid_cooled_node_bmcs/</guid>
      <description>Enable Passwordless Connections to Liquid Cooled Node BMCs Set the passwordless SSH keys for the root account and/or console of all liquid-cooled Baseboard Management Controllers (BMCs) on the system. This procedure will not work on BMCs for air-cooled hardware.&#xA;Warning: If administrator uses SCSD to update the SSHConsoleKey value outside of ConMan, it will disrupt the ConMan connection to the console and collection of console logs. Refer to ConMan for more information.</description>
    </item>
    <item>
      <title>Retrieve Cluster Health Information Using Kubernetes</title>
      <link>/docs-csm/en-10/operations/kubernetes/retrieve_cluster_health_information_using_kubernetes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/retrieve_cluster_health_information_using_kubernetes/</guid>
      <description>Retrieve Cluster Health Information Using Kubernetes The kubectl CLI commands can be used to retrieve information about the Kubernetes cluster components.&#xA;Nodes Retrieve node status ncn# kubectl get nodes Example output:&#xA;ncn-m001 Ready master 19d v1.14.3 ncn-m002 Ready master 19d v1.14.3 ncn-m003 Ready master 19d v1.14.3 ncn-w001 Ready &amp;lt;none&amp;gt; 19d v1.14.3 ncn-w002 Ready &amp;lt;none&amp;gt; 19d v1.14.3 ncn-w003 Ready &amp;lt;none&amp;gt; 19d v1.14.3 Pods Retrieve information about individual pods ncn# kubectl describe pod POD_NAME -n NAMESPACE_NAME Retrieve a list of all pods ncn# kubectl get pods -A Retrieve a list of healthy pods ncn# kubectl get pods -A | grep -E &amp;#39;Completed|Running&amp;#39; Retrieve a list of unhealthy pods Option 1: List all pods that are not reported as Completed or Running.</description>
    </item>
    <item>
      <title>Update the Privacy Settings for Gitea Configuration Content Repositories</title>
      <link>/docs-csm/en-10/operations/configuration_management/update_the_privacy_settings_for_gitea_configuration_content_repositories/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/update_the_privacy_settings_for_gitea_configuration_content_repositories/</guid>
      <description>Update the Privacy Settings for Gitea Configuration Content Repositories Change the visibility of Gitea configuration content repositories from public to private. All Cray-provided repositories are created as private by default.&#xA;Procedure Log in to the Version Control Service (VCS) as the crayvcs user.&#xA;Use the following URL to access the VCS web interface: https://vcs.SYSTEM-NAME.DOMAIN-NAME&#xA;Navigate to the cray organization.&#xA;The following URL should access it directly: https://vcs.SYSTEM-NAME.DOMAIN-NAME/vcs/cray&#xA;Select the repository title for each repository listed on the page.</description>
    </item>
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related to Dynamic Host Configuration Protocol (DHCP)</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_dynamic_host_configuration_protocol_dhcp/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_dynamic_host_configuration_protocol_dhcp/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Dynamic Host Configuration Protocol (DHCP) DHCP issues can result in node boot failures. This procedure helps investigate and resolve such issues.&#xA;Prerequisites This procedure requires administrative privileges. Procedure Check that the DHCP service is running.&#xA;ncn-m001# kubectl get pods -A | grep kea Example output:&#xA;services cray-dhcp-kea-554698bb69-r9wwt 3/3 Running 0 13h services cray-dhcp-kea-postgres-0 2/2 Running 0 10d services cray-dhcp-kea-postgres-1 2/2 Running 0 3d18h services cray-dhcp-kea-postgres-2 2/2 Running 0 10d services cray-dhcp-kea-wait-for-postgres-3-7gqvg 0/3 Completed 0 10d Start a tcpdump session on the NCN.</description>
    </item>
    <item>
      <title>List UAI Resource Specifications</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_uai_resource_specifications/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:11 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_uai_resource_specifications/</guid>
      <description>List UAI Resource Specifications Obtain a list of all the UAI resource specifications registered with UAS.&#xA;Prerequisites The cray administrative CLI must be installed and initialized.&#xA;Procedure List all the resource specifications registered in UAS.&#xA;The resource specifications returned by the following command are available for UAIs to use:&#xA;ncn-m001-pit# cray uas admin config resources list [[results]] comment = &amp;#34;my first example resource specification&amp;#34; limit = &amp;#34;{\&amp;#34;cpu\&amp;#34;: \&amp;#34;300m\&amp;#34;, \&amp;#34;memory\&amp;#34;: \&amp;#34;250Mi\&amp;#34;}&amp;#34; request = &amp;#34;{\&amp;#34;cpu\&amp;#34;: \&amp;#34;300m\&amp;#34;, \&amp;#34;memory\&amp;#34;: \&amp;#34;250Mi\&amp;#34;}&amp;#34; resource_id = &amp;#34;85645ff3-1ce0-4f49-9c23-05b8a2d31849&amp;#34; [[results]] comment = &amp;#34;my second example resource specification&amp;#34; limit = &amp;#34;{\&amp;#34;cpu\&amp;#34;: \&amp;#34;4\&amp;#34;, \&amp;#34;memory\&amp;#34;: \&amp;#34;1Gi\&amp;#34;}&amp;#34; request = &amp;#34;{\&amp;#34;cpu\&amp;#34;: \&amp;#34;4\&amp;#34;, \&amp;#34;memory\&amp;#34;: \&amp;#34;1Gi\&amp;#34;}&amp;#34; resource_id = &amp;#34;eff9e1f2-3560-4ece-a9ce-8093e05e032d&amp;#34; The following are the configurable parts of a resource specification:</description>
    </item>
    <item>
      <title>Manage System Passwords</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/manage_system_passwords/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:36 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/manage_system_passwords/</guid>
      <description>Manage System Passwords Many system services require login credentials to gain access to them. The information below is a comprehensive list of system passwords and how to change them.&#xA;Contact HPE Cray service in order to obtain the default usernames and passwords for any of these components or services.&#xA;Keycloak Default Keycloak admin user login credentials:&#xA;Username: admin&#xA;The password can be obtained with the following command:&#xA;ncn-w001# kubectl get secret -n services keycloak-master-admin-auth \ --template={{.</description>
    </item>
    <item>
      <title>Find Node Type and Manufacturer</title>
      <link>/docs-csm/en-10/operations/node_management/find_node_type_and_manufacturer/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/find_node_type_and_manufacturer/</guid>
      <description>Find Node Type and Manufacturer There are three different vendors providing nodes for air-cooled cabinets, which are Gigabyte, Intel, and HPE. The Hardware State Manager (HSM) contains the information required to determine which type of air-cooled node is installed. The endpoint returned in the HSM command can be used to determine the manufacturer.&#xA;HPE nodes contain the /redfish/v1/Systems/1 endpoint:&#xA;ncn-m001# cray hsm inventory componentEndpoints describe XNAME --format json | jq &amp;#39;.</description>
    </item>
    <item>
      <title>TDS Lower CPU Requests</title>
      <link>/docs-csm/en-10/operations/kubernetes/tds_lower_cpu_requests/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/tds_lower_cpu_requests/</guid>
      <description>TDS Lower CPU Requests Systems with only three worker nodes (typically Test and Development Systems (TDS)) will encounter pod scheduling issues when worker nodes are taken out of the Kubernetes cluster to be upgraded.&#xA;For systems with only three worker nodes, execute the following script to reduce the CPU request for some services with high CPU requests, in order to allow critical upgrade-related services to be successfully scheduled on only two worker nodes:</description>
    </item>
    <item>
      <title>Use a Custom ansible.cfg File</title>
      <link>/docs-csm/en-10/operations/configuration_management/use_a_custom_ansible-cfg_file/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/use_a_custom_ansible-cfg_file/</guid>
      <description>Use a Custom ansible.cfg File The Configuration Framework Service (CFS) allows for flexibility with the Ansible Execution Environment (AEE) by allowing for changes to included ansible.cfg file. When installed, CFS imports a custom ansible.cfg file into the cfs-default-ansible-cfg Kubernetes ConfigMap in the services namespace.&#xA;Administrators who want to make changes to the ansible.cfg file on a per-session or system-wide basis can upload a new file to a new ConfigMap in the services namespace, and then direct CFS to use their file.</description>
    </item>
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related to Slow Boot Times</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_slow_boot_times/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_slow_boot_times/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Slow Boot Times Inspect Boot Orchestration Service (BOS), Boot Orchestration Agent (BOA), and Configuration Framework Service (CFS) job logs in order to obtain information that is critical for boot troubleshooting. Use this procedure to determine why compute nodes are booting slower than expected.&#xA;Prerequisites A boot session has been created with BOS. The Cray CLI is configured. See Configure the Cray CLI. Procedure View the BOA logs.</description>
    </item>
    <item>
      <title>List UAIs</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_uais/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:11 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_uais/</guid>
      <description>List UAIs View the details of every UAI that is running by using a direct UAS administrative command.&#xA;Prerequisites This procedure requires administrative privileges. Install and initialize the cray administrative CLI. Procedure List the existing UAIs.&#xA;Use a command of the following form:&#xA;ncn-m001-pit# cray uas admin uais list [options] The [options] parameter includes the following selection options:&#xA;--owner &#39;&amp;lt;user-name&amp;gt;&#39; show only UAIs owned by the named user --class-id &#39;&amp;lt;class-id&#39; show only UAIs of the specified UAI class For example:</description>
    </item>
    <item>
      <title>Install CSM Services</title>
      <link>/docs-csm/en-10/install/install_csm_services/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:05 +0000</pubDate>
      <guid>/docs-csm/en-10/install/install_csm_services/</guid>
      <description>Install CSM Services This procedure will install CSM applications and services into the CSM Kubernetes cluster.&#xA;NOTE: Check the information in Known issues before starting this procedure to be warned about possible problems.&#xA;Initialize bootstrap registry Create Site-Init secret Deploy sealed secret decryption key Deploy CSM applications and services Setup Nexus Set management NCNs to use Unbound Apply pod priorities Apply After Sysmgmt Manifest workarounds Wait for everything to settle Next topic Known issues install.</description>
    </item>
    <item>
      <title>PKI Certificate Authority (CA)</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/pki_certificate_authority_ca/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/pki_certificate_authority_ca/</guid>
      <description>PKI Certificate Authority (CA) An instance of HashiCorp Vault, deployed via the Bitnami Bank-vaults operator, stores private and public Certificate Authority (CA) material, and serves APIs through a Public Key Infrastructure (PKI) engine instance.&#xA;CA material is injected as a start-up secret into Vault through a SealedSecret that translates into a Kubernetes Secret.&#xA;CA Certificate Distribution Trusted CA certificates are distributed via two channels:&#xA;Cloud-init metadata Kubernetes ConfigMaps Kubernetes-native workloads generally leverage ConfigMap-based distribution.</description>
    </item>
    <item>
      <title>Launch a Virtual KVM on Gigabyte Nodes</title>
      <link>/docs-csm/en-10/operations/node_management/launch_a_virtual_kvm_on_gigabyte_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/launch_a_virtual_kvm_on_gigabyte_nodes/</guid>
      <description>Launch a Virtual KVM on Gigabyte Nodes This procedure shows how to launch a virtual KVM to connect to Gigabyte node. The virtual KVM can be launched on any host that is on the same network as the node&amp;rsquo;s BMC. This method of connecting to a node is frequently used during system installation.&#xA;Prerequisites A laptop or workstation with a browser and access to the Internet. The externally visible IP address of the node&amp;rsquo;s integrated BMC.</description>
    </item>
    <item>
      <title>Troubleshoot Intermittent HTTP 503 Code Failures</title>
      <link>/docs-csm/en-10/operations/kubernetes/troubleshoot_intermittent_503s/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/troubleshoot_intermittent_503s/</guid>
      <description>Troubleshoot Intermittent HTTP 503 Code Failures There are cases where API calls or cray command invocations will fail (sometimes intermittently) with an HTTP 503 error code. In the event that this occurs, attempt to remediate the issue by taking the following actions, according to specific error codes found in the pod or Envoy container log.&#xA;The Envoy container is typically named istio-proxy, and it runs as a sidecar for pods that are part of the Istio mesh.</description>
    </item>
    <item>
      <title>Use a Specific Inventory in a Configuration Session</title>
      <link>/docs-csm/en-10/operations/configuration_management/use_a_specific_inventory_in_a_configuration_session/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/use_a_specific_inventory_in_a_configuration_session/</guid>
      <description>Use a Specific Inventory in a Configuration Session A special repository can be added to a Configuration Framework Service (CFS) configuration to help with certain scenarios, specifically when developing Ansible plays for use on the system. A static inventory often changes along with the Ansible content, and CFS users may need to test different configuration values simultaneously and not be forced to use the global additionalInventoryUrl.&#xA;Therefore, an additional_inventory mapping can be added to the CFS configuration.</description>
    </item>
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related to Trivial File Transfer Protocol (TFTP)</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_trivial_file_transfer_protocol_tftp/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_trivial_file_transfer_protocol_tftp/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Trivial File Transfer Protocol (TFTP) TFTP issues can result in node boot failures. Use this procedure to investigate and resolve such issues.&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Limitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.&#xA;Procedure Check that the TFTP service is running.&#xA;ncn-m001# kubectl get pods -n services -o wide | grep cray-tftp Start a tcpdump session on the NCN.</description>
    </item>
    <item>
      <title>List UAS Information</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_uas_information/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:11 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_uas_information/</guid>
      <description>List UAS Information Use the cray uas command to gather information about the User Access Service&amp;rsquo;s version, images, and running User Access Instances (UAIs).&#xA;List UAS Version with cray uas mgr-info list ncn-w001# cray uas mgr-info list service_name = &amp;#34;cray-uas-mgr&amp;#34;, version = &amp;#34;0.11.3&amp;#34; List Available UAS Images with cray uas images list ncn-w001# cray uas images list default_image = &amp;#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest&amp;#34; image_list = [ &amp;#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest&amp;#34;, &amp;#34;registry.local/cray/cray-uas-sles15sp1:latest&amp;#34;,] List All Running UAIs with cray uas uais list ncn-w001# cray uas uais list [[results]] username = &amp;#34;user&amp;#34; uai_host = &amp;#34;ncn-w001&amp;#34; uai_status = &amp;#34;Running: Ready&amp;#34; uai_connect_string = &amp;#34;ssh user@203.</description>
    </item>
    <item>
      <title>Prepare Compute Nodes</title>
      <link>/docs-csm/en-10/install/prepare_compute_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:06 +0000</pubDate>
      <guid>/docs-csm/en-10/install/prepare_compute_nodes/</guid>
      <description>Prepare Compute Nodes Configure HPE Apollo 6500 XL645d Gen10 Plus Compute Nodes Gather information Configure the iLO to use VLAN 4 Configure the switch port for the iLO to use VLAN 4 Clear bad MAC and IP address out of KEA Clear bad ID out of HSM Update the BIOS Time on Gigabyte Compute Nodes Next topic Configure HPE Apollo 6500 XL645d Gen10 Plus Compute Nodes The HPE Apollo 6500 XL645d Gen10 Plus compute node uses a NIC/shared iLO network port.</description>
    </item>
    <item>
      <title>PKI Services</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/pki_services/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/pki_services/</guid>
      <description>PKI Services The services in this section are integral parts of the Public Key Infrastructure (PKI) implementation.&#xA;HashiCorp Vault Jetstack Cert-manager TrustedCerts Operator HashiCorp Vault A deployment of HashiCorp Vault, managed via the Bitnami Bank-vaults operator, stores private and public Certificate Authority (CA) material, and serves APIs through a PKI engine instance. This instance also serves as a general secrets engine for the system.&#xA;Kubernetes service account authorization is utilized to authenticate access to Vault.</description>
    </item>
    <item>
      <title>Launch a Virtual KVM on Intel Servers</title>
      <link>/docs-csm/en-10/operations/node_management/launch_a_virtual_kvm_on_intel_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/launch_a_virtual_kvm_on_intel_nodes/</guid>
      <description>Launch a Virtual KVM on Intel Servers This procedure shows how to launch a virtual KVM to connect to an Intel node. The virtual KVM can be launched on any host that is on the same network as the nodes&amp;rsquo;s BMC. This method of connecting to a nodes is frequently used during system installation.&#xA;Prerequisites A laptop or workstation with a browser and access to the Internet. The externally visible IP address of the node&amp;rsquo;s integrated BMC.</description>
    </item>
    <item>
      <title>Troubleshoot Postgres Database</title>
      <link>/docs-csm/en-10/operations/kubernetes/troubleshoot_postgres_database/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/troubleshoot_postgres_database/</guid>
      <description>Troubleshoot Postgres Database This page contains general Postgres troubleshooting topics.&#xA;The patronictl tool Database unavailable Database disk full Replication lagging Postgres status SyncFailed Cluster member missing Postgres leader missing The patronictl tool The patronictl tool is used to call a REST API that interacts with Postgres databases. It handles a variety of tasks, such as listing cluster members and the replication status, configuring and restarting databases, and more.&#xA;The tool is installed in the database containers:</description>
    </item>
    <item>
      <title>VCS Branching Strategy</title>
      <link>/docs-csm/en-10/operations/configuration_management/vcs_branching_strategy/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/vcs_branching_strategy/</guid>
      <description>VCS Branching Strategy Individual products import configuration content (Ansible plays, roles, and more) into a repository in the Version Control Service (VCS) through their installation process. Typically, this repository exists in the cray organization in VCS and its name has the format [product name]-config-management.&#xA;The import branch of the product is considered &amp;ldquo;pristine content&amp;rdquo; and is added to VCS in a read-only branch. This step is taken to ensure the future updates of the product&amp;rsquo;s configuration content can be based on a clean branch, and that upgrades can proceed without merging issues.</description>
    </item>
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related to Unified Extensible Firmware Interface (UEFI)</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_unified_extensible_firmware_interface_uefi/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_unified_extensible_firmware_interface_uefi/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Unified Extensible Firmware Interface (UEFI) If a node is stuck in the UEFI shell, ConMan will be able to connect to it, but nothing else will appear in its logs. The node&amp;rsquo;s logs will look similar to the following, indicating that ConMan is updating its log hourly:&#xA;&amp;lt;ConMan&amp;gt; Console [86] log at 2018-09-07 20:00:00 CDT. &amp;lt;ConMan&amp;gt; Console [86] log at 2018-09-07 21:00:00 CDT. &amp;lt;ConMan&amp;gt; Console [86] log at 2018-09-07 22:00:00 CDT.</description>
    </item>
    <item>
      <title>List Volumes Registered in UAS</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_volumes_registered_in_uas/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:11 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_volumes_registered_in_uas/</guid>
      <description>List Volumes Registered in UAS List the details of all volumes registered in UAS with the cray uas admin config volumes list command. Use this command to obtain the volume_id value of volume, which is required for other UAS administrative commands.&#xA;Prerequisites Install and initialize the cray administrative CLI.&#xA;Procedure List the details of all the volumes registered in UAS.&#xA;Print out the list in TOML.&#xA;ncn-m001-pit# cray uas admin config volumes list [[results]] mount_path = &amp;#34;/lus&amp;#34; volume_id = &amp;#34;2b23a260-e064-4f3e-bee5-3da8e3664f29&amp;#34; volumename = &amp;#34;lustre&amp;#34; [results.</description>
    </item>
    <item>
      <title>Prepare Configuration Payload</title>
      <link>/docs-csm/en-10/install/prepare_configuration_payload/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:06 +0000</pubDate>
      <guid>/docs-csm/en-10/install/prepare_configuration_payload/</guid>
      <description>Prepare Configuration Payload The configuration payload consists of the information which must be known about the HPE Cray EX system so it can be passed to the csi (Cray Site Init) program during the CSM installation process.&#xA;Information gathered from a site survey is needed to feed into the CSM installation process, such as system name, system size, site network information for the CAN, site DNS configuration, site NTP configuration, network information for the node used to bootstrap the installation.</description>
    </item>
    <item>
      <title>Preserve Username Capitalization for Users Exported from Keycloak</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/preserve_username_capitalization_for_users_exported_from_keycloak/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/preserve_username_capitalization_for_users_exported_from_keycloak/</guid>
      <description>Preserve Username Capitalization for Users Exported from Keycloak Keycloak converts all characters in a username to lowercase when users are exported. Use this procedure to update the keycloak-users-localize tool with a configuration option that enables administrators to preserve the username letter case when users are exported from Keycloak.&#xA;The LDAP server that provides password resolution and user account federation supports mixed case usernames. If the usernames are changed to lowercase when exported from Keycloak, it can cause issues.</description>
    </item>
    <item>
      <title>Move a Standard Rack Node</title>
      <link>/docs-csm/en-10/operations/node_management/move_a_standard_rack_node/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/move_a_standard_rack_node/</guid>
      <description>Move a Standard Rack Node Update the location-based component name (xname) for a standard rack node within the system.&#xA;Prerequisites An authentication token has been retrieved.&#xA;ncn# function get_token () { curl -s -S -d grant_type=client_credentials \ -d client_id=admin-client \ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=&amp;#39;{.data.client-secret}&amp;#39; | base64 -d` \ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r &amp;#39;.access_token&amp;#39; } The Cray command line interface (CLI) tool is initialized and configured on the system.</description>
    </item>
    <item>
      <title>View Postgres Information for System Databases</title>
      <link>/docs-csm/en-10/operations/kubernetes/view_postgres_information_for_system_databases/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:25 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/kubernetes/view_postgres_information_for_system_databases/</guid>
      <description>View Postgres Information for System Databases Postgres uses SQL language to store and manage databases on the system. This procedure describes how to view and obtain helpful information about system databases, as well as the types of data being stored.&#xA;Prerequisites This procedure requires administrative privileges.&#xA;Procedure Log in to the Postgres container.&#xA;ncn-mw# kubectl -n services exec -it cray-smd-postgres-0 -- bash Example output:&#xA;Defaulting container name to postgres. Use &amp;#39;kubectl describe pod/cray-smd-postgres-0 -n services&amp;#39; to see all of the containers in this pod.</description>
    </item>
    <item>
      <title>Version Control Service (VCS)</title>
      <link>/docs-csm/en-10/operations/configuration_management/version_control_service_vcs/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/version_control_service_vcs/</guid>
      <description>Version Control Service (VCS) VCS overview Cloning a VCS repository VCS administrative user Change VCS administrative user password Access the cray Gitea organization Backup and restore data Backup Postgres data Backup PVC data Restore Postgres data Restore PVC data Alternative backup/restore strategy Alternative export method Alternative import method VCS overview The Version Control Service (VCS) includes a web interface for repository management, pull requests, and a visual view of all repositories and organizations.</description>
    </item>
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related to the Boot Script Service (BSS)</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_the_boot_script_service_bss/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_the_boot_script_service_bss/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to the Boot Script Service (BSS) Boot Script Service (BSS) delivers a boot script to a node based on its MAC address. This boot script tells the node where to obtain its boot artifacts, which include:&#xA;kernel initrd In addition, the boot script also contains the kernel boot parameters. This procedure helps resolve issues related to missing boot artifacts.&#xA;Prerequisites This procedure requires administrative privileges.</description>
    </item>
    <item>
      <title>List and Delete All UAIs</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_and_delete_all_uais/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:11 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/list_and_delete_all_uais/</guid>
      <description>List and Delete All UAIs Delete all UAIs currently on the system.&#xA;Prerequisites At least one UAI is running.&#xA;Procedure Log in to an NCN as root.&#xA;List all the UAIs on the system.&#xA;ncn-m001# cray uas uais list [[results]] username = &amp;#34;uastest&amp;#34; uai_host = &amp;#34;ncn-w001&amp;#34; uai_status = &amp;#34;Running: Ready&amp;#34; uai_connect_string = &amp;#34;ssh uastest@203.0.113.0 -p 32486 -i ~/.ssh/id_rsa&amp;#34; uai_img = &amp;#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest&amp;#34; uai_age = &amp;#34;2m&amp;#34; uai_name = &amp;#34;uai-uastest-f488eef6&amp;#34; [[results]] username = &amp;#34;uastest&amp;#34; uai_host = &amp;#34;ncn-w001&amp;#34; uai_status = &amp;#34;Running: Ready&amp;#34; uai_connect_string = &amp;#34;ssh uastest@203.</description>
    </item>
    <item>
      <title>Prepare Management Nodes</title>
      <link>/docs-csm/en-10/install/prepare_management_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:06 +0000</pubDate>
      <guid>/docs-csm/en-10/install/prepare_management_nodes/</guid>
      <description>Prepare Management Nodes The procedures described on this page must be completed before any node is booted with the Cray Pre-Install Toolkit (PIT), which is performed in a later document. When the PIT node is referenced during these procedures, it means the node that will be booted as the PIT node.&#xA;Quiesce compute and application nodes Disable DHCP service Wipe disks on booted nodes Set IPMI credentials Power off booted nodes Set node BMCs to DHCP Wipe USB device on PIT node Power off PIT node Quiesce compute nodes and application nodes Skip this section if compute nodes and application nodes are not booted.</description>
    </item>
    <item>
      <title>Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/provisioning_a_liquid-cooled_ex_cabinet_cec_with_default_credentials/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/provisioning_a_liquid-cooled_ex_cabinet_cec_with_default_credentials/</guid>
      <description>Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials This procedure provisions a Glibc compatible SHA-512 administrative password hash to a cabinet environmental controller (CEC). This password becomes the Redfish default global credential to access the CMM controllers and node controllers (BMCs).&#xA;This procedure does not provision Slingshot switch BMCs. Slingshot switch BMC default credentials must be changed using the procedures in the Slingshot product documentation. Refer to &amp;ldquo;Change Rosetta Login and Redfish API Credentials&amp;rdquo; in the Slingshot Operations Guide (&amp;gt; 1.</description>
    </item>
    <item>
      <title>Move a Standard Rack Node (Same Rack/Same HSN Ports)</title>
      <link>/docs-csm/en-10/operations/node_management/move_a_standard_rack_node_samerack_samehsnports/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/move_a_standard_rack_node_samerack_samehsnports/</guid>
      <description>Move a Standard Rack Node (Same Rack/Same HSN Ports) This procedure move standard rack UAN or compute node to a different location and uses the same Slingshot switch ports and management network ports.&#xA;Update the location-based component name (xname) for a standard rack node within the system.&#xA;If a node has an incorrect component name (xname) based on its physical location, then this procedure can be used to correct the component name (xname) of the node without the need to physically move the node.</description>
    </item>
    <item>
      <title>View Configuration Session Logs</title>
      <link>/docs-csm/en-10/operations/configuration_management/view_configuration_session_logs/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/view_configuration_session_logs/</guid>
      <description>View Configuration Session Logs Logs for the individual steps of a session are available via the kubectl log command for each container of a Configuration Framework Service (CFS) session. Refer to Configuration Sessions for more info about these containers.&#xA;To find the name of the Kubernetes pod that is running the CFS session:&#xA;ncn-mw# kubectl get pods --no-headers -o custom-columns=&amp;#34;:metadata.name&amp;#34; -n services -l cfsession=example Example output:&#xA;cfs-f9d18751-e6d1-4326-bf76-434293a7b1c5-q8tsc Store the returned pod name as the CFS_POD_NAME variable for future use:</description>
    </item>
    <item>
      <title>Troubleshoot Compute Node Boot Issues Using Kubernetes</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_using_kubernetes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:16 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_using_kubernetes/</guid>
      <description>Troubleshoot Compute Node Boot Issues Using Kubernetes A number of Kubernetes commands can be used to debug issues related to the node boot process. All of the traffic bound for the DHCP server, TFTP server, and Boot Script Service (BSS) is sent on the Node Management Network (NMN).&#xA;In the current arrangement, all three services are located on a management non-compute node (NCN). Thus, traffic must first travel through the NCN to reach these services inside their pods.</description>
    </item>
    <item>
      <title>Log in to a Broker UAI</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/log_in_to_a_broker_uai/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:11 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/log_in_to_a_broker_uai/</guid>
      <description>Log in to a Broker UAI SSH to log into a broker UAI and reach the end-user UAIs on demand.&#xA;Prerequisites The broker UAI is running. See Start a Broker UAI.&#xA;Procedure Log in to the broker UAI.&#xA;The following example is the first login for the vers user:&#xA;vers&amp;gt; ssh vers@10.103.13.162 The authenticity of host &amp;#39;10.103.13.162 (10.103.13.162)&amp;#39; can&amp;#39;t be established. ECDSA key fingerprint is SHA256:k4ef6vTtJ1Dtb6H17cAFh5ljZYTl4IXtezR3fPVUKZI. Are you sure you want to continue connecting (yes/no/[fingerprint])?</description>
    </item>
    <item>
      <title>Prepare Site Init</title>
      <link>/docs-csm/en-10/install/prepare_site_init/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:06 +0000</pubDate>
      <guid>/docs-csm/en-10/install/prepare_site_init/</guid>
      <description>Prepare Site Init These procedures guide administrators through setting up the site-init directory which contains important customizations for various products.&#xA;Note: There are two media available to bootstrap the PIT node&amp;ndash;the RemoteISO or a bootable USB device. Both of those can use this procedure. The only difference in this procedure is that the RemoteISO method will execute these commands on the PIT node with command prompt pit# while the USB method could be done on any Linux system with the command prompt linux#.</description>
    </item>
    <item>
      <title>Public Key Infrastructure (PKI)</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/public_key_infrastructure_pki/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/public_key_infrastructure_pki/</guid>
      <description>Public Key Infrastructure (PKI) Public Key Infrastructure (PKI) represents the algorithms, infrastructure, policies, and processes required to leverage applied public key cryptography methods for operational security use cases. The Rivest-Shamir-Adleman (RSA) and Elliptic-curve (ECC) are some example algorithm systems.&#xA;The use of PKI for the system is in the Transport Layer Security (TLS) protocol, which is the successor of the now deprecated Secure Sockets Layer (SSL). This is where trusted chains of Certificate Authorities (CAs) are used to authenticate the identity of servers and sometimes clients (for example, mutual TLS) for relying parties.</description>
    </item>
    <item>
      <title>NCN Drive Identification</title>
      <link>/docs-csm/en-10/operations/node_management/ncn_identify_drives_using_ledctl/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/ncn_identify_drives_using_ledctl/</guid>
      <description>NCN Drive Identification Basic usage for the ledmon/ledctl software for drive indentification via the drive leds.&#xA;Usage Turn on led locator beacon&#xA;ledctl locate=/dev/&amp;lt;drive&amp;gt; Turn off led locator beacon&#xA;ledctl locate_off=/dev/&amp;lt;drive&amp;gt; </description>
    </item>
    <item>
      <title>Write Ansible Code for CFS</title>
      <link>/docs-csm/en-10/operations/configuration_management/write_ansible_code_for_cfs/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:19 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configuration_management/write_ansible_code_for_cfs/</guid>
      <description>Write Ansible Code for CFS Cray provides Ansible plays and roles for software products deemed necessary for the system to function. Customers are free to write their own Ansible plays and roles to augment what Cray provides or implement new features. Basic knowledge of Ansible is needed to write plays and roles. The information below includes recommendations and best practices for writing and running Ansible code on the system successfully with the Configuration Framework Service (CFS).</description>
    </item>
    <item>
      <title>Troubleshoot UAN Boot Issues</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_uan_boot_issues/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:17 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/troubleshoot_uan_boot_issues/</guid>
      <description>Troubleshoot UAN Boot Issues Use this topic to guide troubleshooting of UAN boot issues.&#xA;The UAN boot process BOS boots UANs. BOS uses session templates to define various parameters such as:&#xA;Which nodes to boot Which image to boot Kernel parameters Whether to perform post-boot configuration (Node Personalization) of the nodes by CFS. Which CFS configuration to use if Node Personalization is enabled. UAN boots are performed in three phases:</description>
    </item>
    <item>
      <title>Log in to a User&#39;s UAI to Troubleshoot Issues</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/login_to_a_users_uai_to_troubleshoot_issues/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:11 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/login_to_a_users_uai_to_troubleshoot_issues/</guid>
      <description>Log in to a User&amp;rsquo;s UAI to Troubleshoot Issues Log in to a user&amp;rsquo;s User Access Instance (UAI) to help the user troubleshoot issues.&#xA;Prerequisites This procedure requires root access.&#xA;Limitations This procedure does not work if the pod is in either Error or Terminating states.&#xA;Procedure Log in to the first NCN acting as a Kubernetes master node (ncn-m001) as root.&#xA;Find and record the name of the UAI.</description>
    </item>
    <item>
      <title>PXE Boot Troubleshooting</title>
      <link>/docs-csm/en-10/install/pxe_boot_troubleshooting/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:06 +0000</pubDate>
      <guid>/docs-csm/en-10/install/pxe_boot_troubleshooting/</guid>
      <description>PXE Boot Troubleshooting This page is designed to cover various issues that arise when trying to PXE boot nodes in an HPE Cray EX system.&#xA;Configuration required for PXE booting Switch configuration Aruba configuration Mellanox configuration Next steps Restart BSS Restart Kea Missing BSS data In order for PXE booting to work successfully, the management network switches need to be configured correctly.&#xA;Configuration required for PXE booting To successfully PXE boot nodes, the following is required:</description>
    </item>
    <item>
      <title>Recovering from Mismatched BMC Credentials</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/recovering_from_mismatched_bmc_credentials/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/recovering_from_mismatched_bmc_credentials/</guid>
      <description>Recovering from Mismatched BMC Credentials Use this procedure to recover from the situation when new or replacement hardware has root credentials that do not match the system&amp;rsquo;s current default root user credentials.&#xA;This type of problem can occur in the following scenarios:&#xA;The site has customized the default root credentials using either the Updating the Liquid-Cooled EX Cabinet CEC with Default Credentials after a CEC Password Change or Update Default Air-Cooled BMC and Leaf Switch SNMP Credentials procedures.</description>
    </item>
    <item>
      <title>Node Management</title>
      <link>/docs-csm/en-10/operations/node_management/node_management/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/node_management/</guid>
      <description>Node Management The HPE Cray EX systems include two node types:&#xA;Compute Nodes that run high-performance computing applications and are named nidXXXXXX. Every system must contain four or more compute nodes, starting at nid000001. Non-Compute Nodes (NCNs) that carry out system management functions as part of the management Kubernetes cluster. NCNs outside of the Kubernetes cluster function as application nodes (AN). Nine or more management NCNs host system services:&#xA;ncn-m001, ncn-m002, and ncn-m003 are Kubernetes master nodes.</description>
    </item>
    <item>
      <title>Configure the Cray Command Line Interface (cray CLI)</title>
      <link>/docs-csm/en-10/operations/configure_cray_cli/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:20 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/configure_cray_cli/</guid>
      <description>Configure the Cray Command Line Interface (cray CLI) The cray command line interface (CLI) is a framework created to integrate all of the system management REST APIs into easily usable commands.&#xA;Procedures in the CSM installation workflow use the cray CLI to interact with multiple services. The cray CLI configuration needs to be initialized for the Linux account, and the Keycloak user running the procedure needs to be authorized. This section describes how to initialize the cray CLI for use by a user and how to authorize that user.</description>
    </item>
    <item>
      <title>Upload Node Boot Information to Boot Script Service (BSS)</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/upload_node_boot_information_to_boot_script_service_bss/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:17 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/upload_node_boot_information_to_boot_script_service_bss/</guid>
      <description>Upload Node Boot Information to Boot Script Service (BSS) The following information must be uploaded to BSS as a prerequisite to booting a node via iPXE:&#xA;The location of an initrd image in the artifact repository The location of a kernel image in the artifact repository Kernel boot parameters The nodes associated with that information, using either host name or node ID (NID) BSS manages the iPXE boot scripts that coordinate the boot process for nodes, and it enables basic association of boot scripts with nodes.</description>
    </item>
    <item>
      <title>Modify a UAI Class</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/modify_a_uai_class/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:11 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/modify_a_uai_class/</guid>
      <description>Modify a UAI Class Update a UAI class with a modified configuration.&#xA;Prerequisites Install and initialize the cray administrative CLI. Obtain the ID of the UAI class that will be modified. Limitations The ID of the UAI class cannot be modified.&#xA;Procedure To update an existing UAI class, use a command of the following form:&#xA;cray uas admin config classes update OPTIONS UAI_CLASS_ID OPTIONS are the same options supported for UAI class creation (see Create a UAI Class) and UAI_CLASS_ID is the ID of the UAI class.</description>
    </item>
    <item>
      <title>Redeploy PIT Node</title>
      <link>/docs-csm/en-10/install/redeploy_pit_node/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:06 +0000</pubDate>
      <guid>/docs-csm/en-10/install/redeploy_pit_node/</guid>
      <description>Redeploy PIT Node The following procedure contains information for rebooting and deploying the management node that is currently hosting the LiveCD. At the end of this procedure, the LiveCD will no longer be active. The node it was using will join the Kubernetes cluster as the final of three master nodes, forming a quorum.&#xA;IMPORTANT: While the node is rebooting, it will only be available through Serial-Over-LAN (SOL) and local terminals.</description>
    </item>
    <item>
      <title>Remove Internal Groups from the Keycloak Shasta Realm</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/remove_internal_groups_from_the_keycloak_shasta_realm/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/remove_internal_groups_from_the_keycloak_shasta_realm/</guid>
      <description>Remove Internal Groups from the Keycloak Shasta Realm Remove a group in the Keycloak Shasta realm. Unused Keycloak groups can be removed.&#xA;Prerequisites This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process.&#xA;The password can be obtained using the following command:&#xA;ncn-mw# kubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode Procedure Open the Keycloak user management interface.</description>
    </item>
    <item>
      <title>Node Management Workflows</title>
      <link>/docs-csm/en-10/operations/node_management/node_management_workflows/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/node_management_workflows/</guid>
      <description>Node Management Workflows The following workflows are intended to be high-level overviews of node management tasks. These workflows depict how services interact with each other during node management and help to provide a quicker and deeper understanding of how the system functions.&#xA;The workflows and procedures in this section include:&#xA;Add Nodes Remove Nodes Replace Nodes Move Nodes Add Nodes Add a Standard Rack Node Use Cases: Administrator permanently adds select compute nodes to expand the system.</description>
    </item>
    <item>
      <title>View the Status of a BOS Session</title>
      <link>/docs-csm/en-10/operations/boot_orchestration/view_the_status_of_a_bos_session/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:17 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/boot_orchestration/view_the_status_of_a_bos_session/</guid>
      <description>View the Status of a BOS Session The Boot Orchestration Service (BOS) supports a status endpoint that reports the status for individual BOS sessions. The status can be retrieved for each boot set within the session, as well as the individual items within a boot set.&#xA;Metadata View the status of a session View the status of a boot set View the status for an individual phase View the status for an individual category The status can be retrieved for each boot set within the session, as well as the individual items within a boot set.</description>
    </item>
    <item>
      <title>Obtain the Configuration of a UAS Volume</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/obtain_configuration_of_a_uas_volume/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:11 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/obtain_configuration_of_a_uas_volume/</guid>
      <description>Obtain the Configuration of a UAS Volume View the configuration information of a specific UAS volume. This procedure requires the volume_ID of that volume.&#xA;Prerequisites Install and initialize the cray administrative CLI. Obtain the UAS volume ID of a volume. Perform List Volumes Registered in UAS if needed. Procedure View the configuration of a specific UAS volume.&#xA;This command returns output in TOML format by default. JSON or YAML formatted output can be obtained by using the --format json or --format yaml options respectively.</description>
    </item>
    <item>
      <title>Reinstall LiveCD</title>
      <link>/docs-csm/en-10/install/reinstall_livecd/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:06 +0000</pubDate>
      <guid>/docs-csm/en-10/install/reinstall_livecd/</guid>
      <description>Reinstall LiveCD Setup a re-install of LiveCD on a node using the previous configuration.&#xA;Backup to the data partition:&#xA;pit# mkdir -pv /var/www/ephemeral/backup pit# pushd /var/www/ephemeral/backup pit# tar -czvf &amp;#34;dnsmasq-data-$(date &amp;#39;+%Y-%m-%d_%H-%M-%S&amp;#39;).tar.gz&amp;#34; /etc/dnsmasq.* pit# tar -czvf &amp;#34;network-data-$(date &amp;#39;+%Y-%m-%d_%H-%M-%S&amp;#39;).tar.gz&amp;#34; /etc/sysconfig/network/* pit# cp -pv /etc/hosts ./ pit# popd pit# umount -v /var/www/ephemeral Unplug the USB device.&#xA;The USB device should now contain all the information already loaded, as well as the backups of the initialized files.</description>
    </item>
    <item>
      <title>Remove the Email Mapper from the LDAP User Federation</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/remove_the_email_mapper_from_the_ldap_user_federation/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/remove_the_email_mapper_from_the_ldap_user_federation/</guid>
      <description>Remove the Email Mapper from the LDAP User Federation The email mapper is automatically added to the LDAP user federation in Keycloak, but it can be removed. The system does not use the user&amp;rsquo;s email for anything, so this function can be removed.&#xA;If there are duplicate email addresses for LDAP users, it can cause Keycloak to have issues syncing with LDAP. Removing the email mapper will fix this problem.</description>
    </item>
    <item>
      <title>Reboot NCNs</title>
      <link>/docs-csm/en-10/operations/node_management/reboot_ncns/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:30 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/node_management/reboot_ncns/</guid>
      <description>Reboot NCNs The following is a high-level overview of the non-compute node (NCN) reboot workflow:&#xA;Run the NCN pre-reboot checks and procedures.&#xA;Ensure that ncn-m001 is not booted to the LiveCD / PIT node. Check the metal.no-wipe settings for all NCNs. Run all platform health checks, including checks on the Border Gateway Protocol (BGP) peering sessions. Validate the current boot order (or specify the boot order). Run the rolling NCN reboot procedure.</description>
    </item>
    <item>
      <title>Register a UAI Image</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/register_a_uai_image/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:11 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/register_a_uai_image/</guid>
      <description>Register a UAI Image Register a UAI image with UAS. Registration tells UAS where to locate the image and whether to use the image as the default for UAIs.&#xA;Prerequisites Initialize cray administrative CLI. Create a UAI image and upload it to the container registry. See Customize End-User UAI Images. Procedure Register a UAI image with UAS.&#xA;The following is the minimum required CLI command form:&#xA;ncn-m001-pit# cray uas admin config images create --imagename &amp;lt;image_name&amp;gt; To register the image registry.</description>
    </item>
    <item>
      <title>Reset root Password on LiveCD</title>
      <link>/docs-csm/en-10/install/reset_root_password_on_livecd/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:06 +0000</pubDate>
      <guid>/docs-csm/en-10/install/reset_root_password_on_livecd/</guid>
      <description>Reset root Password on LiveCD It may become desirable to clear the password on the LiveCD.&#xA;The root password is preserved within the COW partition at cow:rw/etc/shadow. This is the modified copy of the /etc/shadow file used by the operating system.&#xA;If a site/user needs to reset/clear the password for root, they can mount their USB on another machine and remove this file from the COW partition. When next booting from the USB, it will reinitialize to an empty password for root, and again at next login, it will require the password to be changed.</description>
    </item>
    <item>
      <title>Remove the LDAP User Federation from Keycloak</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/remove_the_ldap_user_federation_from_keycloak/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/remove_the_ldap_user_federation_from_keycloak/</guid>
      <description>Remove the LDAP User Federation from Keycloak Use the Keycloak UI or Keycloak REST API to remove the LDAP user federation from Keycloak.&#xA;Removing user federation is useful if the LDAP server was decommissioned or if the administrator would like to make changes to the Keycloak configuration using the Keycloak user localization tool.&#xA;Prerequisites LDAP user federation is currently configured in Keycloak.&#xA;Procedure Follow the steps in only one of the sections below:</description>
    </item>
    <item>
      <title>Reset the UAS Configuration to Original Installed Settings</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/reset_the_uas_configuration_to_original_installed_settings/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:11 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/reset_the_uas_configuration_to_original_installed_settings/</guid>
      <description>Reset the UAS Configuration to Original Installed Settings How to remove a customized UAS configuration and restore the base installed configuration.&#xA;The configuration set up using the Cray CLI to interact with UAS persists as long as UAS remains installed and survives upgrades. This is called the running configuration and it is both persistent and malleable. During installation and localization, however, the installer creates a base installed configuration. It may be necessary to return to this base configuration.</description>
    </item>
    <item>
      <title>Restart Network Services and Interfaces on NCNs</title>
      <link>/docs-csm/en-10/install/restart_network_services_and_interfaces_on_ncns/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:06 +0000</pubDate>
      <guid>/docs-csm/en-10/install/restart_network_services_and_interfaces_on_ncns/</guid>
      <description>Restart Network Services and Interfaces on NCNs Interfaces within the network stack can be reloaded or reset to fix wedged interfaces. The NCNs have network device names set during first boot. The names vary based on the available hardware. For more information, see NCN Networking. Any process covered on this page will be covered by the installer.&#xA;The use cases for resetting services:&#xA;Interfaces not showing up IP Addresses not applying Member/children interfaces not being included Topics Restart Network Services and Interfaces))) Command Reference Check interface status (up/down/broken) Show routing and status for all devices Print real devices (ignore no-device) Show the currently enabled network service (Wicked or Network Manager) Restart Network Services There are a few daemons that make up the SUSE network stack.</description>
    </item>
    <item>
      <title>Restrict Network Access to the ncn-images S3 Bucket</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/restrict_access_to_ncn_images_s3_bucket/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/restrict_access_to_ncn_images_s3_bucket/</guid>
      <description>Restrict Network Access to the ncn-images S3 Bucket The configuration documented in this procedure is intended to prevent user-facing dedicated nodes (UANs, Compute Nodes) from retrieving NCN image content from Ceph S3 services, as running on storage nodes.&#xA;Specifically, the controls enacted via this procedure should do the following:&#xA;Block HAProxy access to the ncn-images bucket if the client is not an NCN (NMN) or PXE booting from the MTL network.</description>
    </item>
    <item>
      <title>Resource Specifications</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/resource_specifications/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:11 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/resource_specifications/</guid>
      <description>Resource Specifications Kubernetes uses resource limits and resource requests, to manage the system resources available to pods. Because UAIs run as pods under Kubernetes, UAS takes advantage of Kubernetes to manage the system resources available to UAIs.&#xA;In the UAS configuration, resource specifications contain that configuration. A UAI that is assigned a resource specification will use that instead of the default resource limits or requests on the Kubernetes namespace containing the UAI.</description>
    </item>
    <item>
      <title>Safeguards for CSM</title>
      <link>/docs-csm/en-10/install/safeguards_for_csm_ncn_upgrades/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:06 +0000</pubDate>
      <guid>/docs-csm/en-10/install/safeguards_for_csm_ncn_upgrades/</guid>
      <description>Safeguards for CSM This page covers safe-guards for preventing destructive behaviors on management nodes.&#xA;If you are reinstalling or upgrading you should run through these safe-guards on a by-case basis:&#xA;Whether or not CEPH should be preserved. Whether or not the RAIDs should be protected. Safeguard CEPH OSDs Edit /var/www/ephemeral/configs/data.json and align the following options:&#xA;{ .. // Disables Ceph wipe: &amp;#34;wipe-ceph-osds&amp;#34;: &amp;#34;no&amp;#34; .. } { .. // Restores default behavior: &amp;#34;wipe-ceph-osds&amp;#34;: &amp;#34;yes&amp;#34; .</description>
    </item>
    <item>
      <title>Re-Sync Keycloak Users to Compute Nodes</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/resync_keycloak_users_to_compute_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/resync_keycloak_users_to_compute_nodes/</guid>
      <description>Re-Sync Keycloak Users to Compute Nodes Resubmit the keycloak-users-localize job and run the keycloak-users-compute.yml Ansible play to synchronize the users and groups from Keycloak to the compute nodes. This procedure alters the /etc/passwd and /etc/group files used on compute nodes.&#xA;Use this procedure to quickly synchronize changes made in Keycloak to the compute nodes.&#xA;Prerequisites The COS product must be installed.&#xA;Procedure Resubmit the keycloak-users-localize job.&#xA;The output might appear slightly different than in the example below.</description>
    </item>
    <item>
      <title>Retrieve Resource Specification Details</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/retrieve_resource_specification_details/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:12 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/retrieve_resource_specification_details/</guid>
      <description>Retrieve Resource Specification Details Display a specific resource specification using the resource_id of that specification.&#xA;Prerequisites Install and initialize the cray administrative CLI.&#xA;Procedure Print out a resource specification.&#xA;To examine a particular resource specification, use a command of the following form:&#xA;ncn-m001-pit# cray uas admin config resources describe RESOURCE_ID For example:&#xA;ncn-m001-pit# cray uas admin config resources describe 85645ff3-1ce0-4f49-9c23-05b8a2d31849 comment = &amp;#34;my first example resource specification&amp;#34; limit = &amp;#34;{&amp;#34;cpu&amp;#34;: &amp;#34;300m&amp;#34;, &amp;#34;memory&amp;#34;: &amp;#34;250Mi&amp;#34;}&amp;#34; request = &amp;#34;{&amp;#34;cpu&amp;#34;: &amp;#34;300m&amp;#34;, &amp;#34;memory&amp;#34;: &amp;#34;250Mi&amp;#34;}&amp;#34; resource_id = &amp;#34;85645ff3-1ce0-4f49-9c23-05b8a2d31849&amp;#34; </description>
    </item>
    <item>
      <title>Retrieve an Authentication Token</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/retrieve_an_authentication_token/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/retrieve_an_authentication_token/</guid>
      <description>Retrieve an Authentication Token Retrieve a token for authenticating to the API gateway.&#xA;The following are important properties of authentication tokens:&#xA;Keycloak access tokens remain valid for 365 days. Secrets do not expire; they are persistent in Keycloak. Tokens and secrets can be revoked at anytime by an administrator. The API gateway uses OAuth2 for authentication. A token is required to authenticate with this gateway.&#xA;Procedure Some of the example commands pipe their output to python -mjson.</description>
    </item>
    <item>
      <title>Retrieve UAI Image Registration Information</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/retrieve_uai_image_registration_information/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:12 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/retrieve_uai_image_registration_information/</guid>
      <description>Retrieve UAI Image Registration Information Use this procedure to obtain the default and imagename values for a UAI image that has been registered with UAS. This procedure can also be used to confirm that a specific image ID is still registered with UAS.&#xA;This procedure returns the same information as List Registered UAI Images, but only for one image.&#xA;Prerequisites Obtain a valid UAS image ID.&#xA;Procedure Obtain the image ID for a UAI that has been registered with UAS.</description>
    </item>
    <item>
      <title>Retrieve the Client Secret for Service Accounts</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/retrieve_the_client_secret_for_service_accounts/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/retrieve_the_client_secret_for_service_accounts/</guid>
      <description>Retrieve the Client Secret for Service Accounts Get the client secret that is generated by Keycloak when the client or service account was created. The secret can be regenerated any time with an administrative action.&#xA;A client secret is needed to make requests using a new client or service account.&#xA;Prerequisites Procedure Use the Keycloak administration console UI Use the Keycloak REST API Prerequisites A client or service account has been created.</description>
    </item>
    <item>
      <title>Select and Configure Host Nodes for UAIs</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/select_and_configure_host_nodes_for_uais/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:12 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/select_and_configure_host_nodes_for_uais/</guid>
      <description>Select and Configure Host Nodes for UAIs Site administrators can control the set of UAI host nodes by labeling Kubernetes worker nodes appropriately.&#xA;UAIs run on NCNs that function as Kubernetes worker nodes. Use Kubernetes labels to prevent UAIs from running on one or more specific worker nodes. Any Kubernetes node that is not labeled to prevent UAIs from running on it is considered to be a UAI host node. In other words, UAI host node selection is an exclusive activity, not an inclusive one.</description>
    </item>
    <item>
      <title>SSH Keys</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/ssh_keys/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/ssh_keys/</guid>
      <description>SSH Keys An SSH key is required by Ansible and several other system management services on the system. If an SSH key is not available, then the Configuration Framework Service (CFS) is unusable, and Ansible cannot be invoked from ncn-w001. The SSH key is created in two parts, with a public and private key. The id_rsa.pub and id_rsa key values are located in the /root/.ssh directory on ncn-w001.&#xA;Important: Changing the SSH key can have serious implications.</description>
    </item>
    <item>
      <title>Special Purpose UAIs</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/special_purpose_uais/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:12 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/special_purpose_uais/</guid>
      <description>Special Purpose UAIs Even though most UAIs are end-user UAIs, UAI classes make it possible to construct UAIs to serve special purposes that are not strictly end-user oriented.&#xA;One kind of special purpose UAI is the broker UAI, which provides on demand end-user UAI launch and management (see Broker Mode UAI Management). While no other specialty UAI types currently exist, other applications are expected to arise and users are encouraged to innovate as needed.</description>
    </item>
    <item>
      <title>System Security and Authentication</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/system_security_and_authentication/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:37 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/system_security_and_authentication/</guid>
      <description>System Security and Authentication The system uses a number of mechanisms to ensure the security and authentication of internal and external requests.&#xA;API gateway service Keycloak JSON Web Tokens (JWTs) Security infrastructure Istio ingress gateway API gateway service The Cray API gateway service provides a common access gateway for all of the systems management REST APIs. Authentication is provided by an Identity and Access Management (IAM) service that integrates with Istio.</description>
    </item>
    <item>
      <title>Start a Broker UAI</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/start_a_broker_uai/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:12 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/start_a_broker_uai/</guid>
      <description>Start a Broker UAI Create a broker UAI after a broker UAI class has been created.&#xA;Prerequisites A broker UAI class has been set up. See Configure a Broker UAI Class.&#xA;Procedure Use the following command to create a broker UAI:&#xA;ncn-m001-pit# cray uas admin uais create --class-id &amp;lt;class-id&amp;gt; [--owner &amp;lt;name&amp;gt;] To make the broker obvious in the list of UAIs, giving it an owner name of broker is handy. The owner name on a broker is used for naming and listing, but nothing else, so this is a convenient convention.</description>
    </item>
    <item>
      <title>Transport Layer Security (TLS) for Ingress Services</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/transport_layer_security_for_ingress_services/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/transport_layer_security_for_ingress_services/</guid>
      <description>Transport Layer Security (TLS) for Ingress Services The Istio Secure Gateway and Keycloak Gatekeeper services utilize Cert-manager for their Transport Layer Security (TLS) certificate and private key. Certificate custom resource definitions are deployed as part of Helm Charts for these services.&#xA;To view properties of the Istio Secure Gateway certificate:&#xA;# kubectl describe certificate -n istio-system ingress-gateway-cert To view the properties of the Keycloak Gatekeeper certificate:&#xA;# kubectl describe certificate -n services keycloak-gatekeeper An outstanding bug in the Keycloak Gatekeeper service prevents it from updating its TLS certificate and key material upon Cert-manager renewal.</description>
    </item>
    <item>
      <title>Troubleshoot Common Mistakes when Creating a Custom End-User UAI Image</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_common_mistakes_when_creating_a_custom_end-user_uai_image/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:12 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_common_mistakes_when_creating_a_custom_end-user_uai_image/</guid>
      <description>Troubleshoot Common Mistakes when Creating a Custom End-User UAI Image There a several problems that may occur while making or working with a custom end-user UAI images. The following are some basic troubleshooting questions to ask:&#xA;Does SESSION_NAME match an actual entry in cray bos sessiontemplate list? Is the SESSION_ID set to an appropriate UUID format? Did the awk command not parse the UUID correctly? Did the file /etc/security/limits.d/99-slingshot-network.conf get removed from the tarball correctly?</description>
    </item>
    <item>
      <title>Troubleshoot Common Vault Cluster Issues</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/troubleshoot_common_vault_cluster_issues/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/troubleshoot_common_vault_cluster_issues/</guid>
      <description>Troubleshoot Common Vault Cluster Issues Search for underlying issues causing unhealthy Vault clusters. Check the Vault statefulset and various pod logs to determine what is impacting the health of the Vault.&#xA;Procedure View the Vault statefulset.&#xA;ncn-mw# kubectl -n vault get statefulset --show-labels Example output:&#xA;NAME READY AGE LABELS cray-vault 3/3 8d app.kubernetes.io/name=vault,vault_cr=cray-vault Check the pod logs for the bank-vaults container for Vault statefulset pods.&#xA;ncn-mw# kubectl logs -n vault cray-vault-0 --tail=-1 --prefix -c bank-vaults ncn-mw# kubectl logs -n vault cray-vault-1 --tail=-1 --prefix -c bank-vaults ncn-mw# kubectl logs -n vault cray-vault-0 --tail=-1 --prefix -c bank-vaults Check the Vault container logs within the pod.</description>
    </item>
    <item>
      <title>Troubleshoot Duplicate Mount Paths in a UAI</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_duplicate_mount_paths_in_a_uai/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:12 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_duplicate_mount_paths_in_a_uai/</guid>
      <description>Troubleshoot Duplicate Mount Paths in a UAI If a user attempts to create a UAI in the legacy mode and cannot create the UAI at all, a good place to look is at volumes. Duplicate mount_path specifications in the list of volumes in a UAI will cause a failure that looks like this:&#xA;ncn-m001-pit# cray uas create --publickey ~/.ssh/id_rsa.pub Usage: cray uas create [OPTIONS] Try &amp;#39;cray uas create --help&amp;#39; for help.</description>
    </item>
    <item>
      <title>Update Default Air-Cooled BMC and Leaf Switch SNMP Credentials</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/update_default_air-cooled_bmc_and_leaf_switch_snmp_credentials/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/update_default_air-cooled_bmc_and_leaf_switch_snmp_credentials/</guid>
      <description>Update Default Air-Cooled BMC and Leaf Switch SNMP Credentials This procedure updates the default credentials used when new air-cooled hardware is discovered for the first time. This includes the default Redfish credentials used for new air-cooled NodeBMCs and Slingshot switch BMCs (RouterBMCs), and SNMP credentials for new management leaf switches.&#xA;IMPORTANT: After this procedure is completed, all future air-cooled hardware added to the system will be assumed to be configured with the new global default credential.</description>
    </item>
    <item>
      <title>Troubleshoot Missing or Incorrect UAI Images</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_missing_or_incorrect_uai_images/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:12 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_missing_or_incorrect_uai_images/</guid>
      <description>Troubleshoot Missing or Incorrect UAI Images If a UAI shows a uai_status of Waiting and a uai_msg of ImagePullBackOff, that indicates that the UAI or the UAI class is configured to use an image that is not in the image registry.&#xA;Either obtaining and pushing the image to the image registry, or correcting the name or version of the image in the UAS configuration will usually resolve this.</description>
    </item>
    <item>
      <title>Update Default ServerTech PDU Credentials used by the Redfish Translation Service (RTS)</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/update_default_servertech_pdu_credentials_used_by_the_redfish_translation_service/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/update_default_servertech_pdu_credentials_used_by_the_redfish_translation_service/</guid>
      <description>Update Default ServerTech PDU Credentials used by the Redfish Translation Service (RTS) This procedure updates the default credentials used by the Redfish Translation Service (RTS) for when new ServerTech PDUs are discovered in a system.&#xA;The Redfish Translation Service provides a Redfish interface that the Hardware State Manager (HSM) and Cray Advanced Platform Monitoring and Control (CAPMC) services can use interact with ServerTech PDUs which do not natively support Redfish.</description>
    </item>
    <item>
      <title>Troubleshoot Stale Brokered UAIs</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_stale_brokered_uais/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:12 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_stale_brokered_uais/</guid>
      <description>Troubleshoot Stale Brokered UAIs When a broker UAI terminates and restarts, the SSH key used to forward SSH sessions to end-user UAIs changes (this is a known problem) and subsequent broker UAIs are unable to forward sessions to end-user UAIs. The symptom of this is that a user logging into a broker UAI will receive a password prompt from the end-user UAI and be unable to log in even if providing the correct password.</description>
    </item>
    <item>
      <title>Update NCN Passwords</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/update_ncn_passwords/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/update_ncn_passwords/</guid>
      <description>Update NCN Passwords The management nodes deploy with a default password in the image, so it is a recommended best practice for system security to change the root password in the image so that it is not the documented default password. In addition to the root password in the image, NCN personalization should be used to change the password as part of post-boot CFS. The password in the image should be used when console access is desired during the network boot of a management node that is being rebuilt, but this password should be different than the one stored in Vault that is applied by CFS during post-boot NCN personalization to change the on-disk password.</description>
    </item>
    <item>
      <title>Troubleshoot UAI Authentication Issues</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_uai_authentication_issues/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:12 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_uai_authentication_issues/</guid>
      <description>Troubleshoot UAI Authentication Issues Several troubleshooting steps related to authentication in a UAI.&#xA;Internal Server Error An error was encountered while accessing Keycloak because of an invalid token.&#xA;# cray uas create --publickey ~/.ssh/id_rsa.pub Usage: cray uas create [OPTIONS] Try &amp;#34;cray uas create --help&amp;#34; for help. Error: Internal Server Error: An error was encountered while accessing Keycloak The uas-mgr logs show:&#xA;2020-03-06 18:52:07,642 - uas_auth - ERROR - &amp;lt;class &amp;#39;requests.exceptions.HTTPError&amp;#39;&amp;gt; HTTPError(&amp;#39;401 Client Error: Unauthorized for url: https://api-gw-service-nmn.</description>
    </item>
    <item>
      <title>Updating the Liquid-Cooled EX Cabinet CEC with Default Credentials after a CEC Password Change</title>
      <link>/docs-csm/en-10/operations/security_and_authentication/updating_the_liquid-cooled_ex_cabinet_default_credentials_after_a_cec_password_change/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:38 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/security_and_authentication/updating_the_liquid-cooled_ex_cabinet_default_credentials_after_a_cec_password_change/</guid>
      <description>Updating the Liquid-Cooled EX Cabinet CEC with Default Credentials after a CEC Password Change This procedure changes the credential for liquid-cooled EX cabinet chassis controllers and node controller (BMCs) used by CSM services after the CECs have been set to a new global default credential.&#xA;NOTE: This procedure does not provision Slingshot switch BMCs (RouterBMCs). Slingshot switch BMC default credentials must be changed using the procedures in the Slingshot product documentation.</description>
    </item>
    <item>
      <title>Troubleshoot UAI Stuck in &#34;ContainerCreating&#34;</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_uai_stuck_in_containercreating/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:12 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_uai_stuck_in_containercreating/</guid>
      <description>Troubleshoot UAI Stuck in &amp;ldquo;ContainerCreating&amp;rdquo; Resolve an issue causing UAIs to show a uai_status field of Waiting, and a uai_msg field of ContainerCreating. It is possible that this is just a matter of starting the UAI taking longer than normal, perhaps as it pulls in a new UAI image from a registry. If the issue persists for a long time, it is worth investigating.&#xA;Prerequisites The UAI has been in the ContainerCreating status for several minutes.</description>
    </item>
    <item>
      <title>Troubleshoot UAIs by Viewing Log Output</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_uais_by_viewing_log_output/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:12 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_uais_by_viewing_log_output/</guid>
      <description>Troubleshoot UAIs by Viewing Log Output Sometimes a UAI will come up and run but will not work correctly. It is possible to see errors reported by elements of the UAI entrypoint script using the kubectl logs command. First find the UAI of interest. This starts by identifying the UAI name using the CLI:&#xA;ncn-m001-pit# cray uas admin uais list [[results]] uai_age = &amp;#34;4h30m&amp;#34; uai_connect_string = &amp;#34;ssh broker@10.103.13.162&amp;#34; uai_host = &amp;#34;ncn-w001&amp;#34; uai_img = &amp;#34;dtr.</description>
    </item>
    <item>
      <title>Troubleshoot UAIs with Administrative Access</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_uais_with_administrative_access/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:12 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_uais_with_administrative_access/</guid>
      <description>Troubleshoot UAIs with Administrative Access Sometimes there is no better way to figure out a problem with a UAI than to get inside it and look around as an administrator. This is done using kubectl exec to start a shell inside the running container as root (in the container). With this an administrator can diagnose problems, make changes to the running UAI, and find solutions. It is important to remember that any change made inside a UAI is transitory.</description>
    </item>
    <item>
      <title>Troubleshoot UAS Issues</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_uas_issues/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:13 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_uas_issues/</guid>
      <description>Troubleshoot UAS Issues This section provides examples of some commands that can be used to troubleshoot UAS-related issues.&#xA;Troubleshoot Connection Issues packet_write_wait: Connection to 203.0.113.0 port 30841: Broken pipe If an error message related to broken pipes returns, enable keep-alives on the client side. The admin should update the /etc/ssh/sshd_config and /etc/ssh/ssh_config files to add the following:&#xA;TCPKeepAlive yes ServerAliveInterval 120 ServerAliveCountMax 720 Invalid Credentials ncn-w001 # cray auth login --username USER --password WRONGPASSWORD Usage: cray auth login [OPTIONS] Try &amp;#34;cray auth login --help&amp;#34; for help.</description>
    </item>
    <item>
      <title>Troubleshoot UAS by Viewing Log Output</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_uas_by_viewing_log_output/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:13 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/troubleshoot_uas_by_viewing_log_output/</guid>
      <description>Troubleshoot UAS by Viewing Log Output At times there will be problems with UAS. Usually this takes the form of errors showing up on CLI commands that are not immediately interpretable as some sort of input error. It is sometimes useful to examine the UAS service logs to find out what is wrong.&#xA;The first thing to do is to find out the names of the Kubernetes pods running UAS:</description>
    </item>
    <item>
      <title>UAI Classes</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/uai_classes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:13 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/uai_classes/</guid>
      <description>UAI Classes This topic explains all the fields in a User Access Instance (UAI) class and gives guidance on setting them when creating UAI classes.&#xA;Example Listing and Overview The following is JSON-formatted example output from the cray uas admin config classes list command (see List Available UAI Classes). This output contains examples of three UAI classes:&#xA;A UAI broker class A brokered end-user UAI class A non-brokered end-user UAI class This topic uses the end-user UAI class section to explain each of fields within a UAI class.</description>
    </item>
    <item>
      <title>UAI Host Node Selection</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/uai_host_node_selection/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:13 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/uai_host_node_selection/</guid>
      <description>UAI Host Node Selection When selecting UAI host nodes, it is a good idea to take into account the amount of combined load users and system services will bring to those nodes. UAIs run by default at a lower priority than system services on worker nodes which means that, if the combined load exceeds the capacity of the nodes, Kubernetes will eject UAIs and/or refuse to schedule them to protect system services.</description>
    </item>
    <item>
      <title>UAI Host Nodes</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/uai_host_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:13 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/uai_host_nodes/</guid>
      <description>UAI Host Nodes UAIs run on Kubernetes worker nodes. There is a mechanism using Kubernetes labels to prevent UAIs from running on a specific worker node, however. Any Kubernetes node that is not labeled to prevent UAIs from running on it is considered to be a UAI host node. The administrator of a given site may control the set of UAI host nodes by labeling Kubernetes worker nodes appropriately.</description>
    </item>
    <item>
      <title>UAI Images</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/uai_images/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:13 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/uai_images/</guid>
      <description>UAI Images There are three kinds of UAI images used by UAS:&#xA;A pre-packaged broker UAI image provided with the UAS A pre-packaged basic end-user UAI Image provided with the UAS Custom end-user UAI images created on site, usually based on compute node contents UAS provides two stock UAI images when installed. The first is a standard end-user UAI Image that has the necessary software installed in it to support a basic Linux distribution login experience.</description>
    </item>
    <item>
      <title>UAI Management</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/uai_management/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:13 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/uai_management/</guid>
      <description>UAI Management UAS supports two manual methods and one automated method of UAI management:&#xA;Direct administrative UAI management Legacy mode user driven UAI management UAI broker mode UAI management Direct administrative UAI management is available mostly to allow administrators to set up UAI brokers for the UAI broker mode of UAI management and to control UAIs that are created under one of the other two methods. It is unlikely that a site will choose to create end-user UAIs this way, but it is possible to do.</description>
    </item>
    <item>
      <title>UAI Network Attachments</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/uai_network_attachments/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:13 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/uai_network_attachments/</guid>
      <description>UAI Network Attachments The UAI network attachment configuration flows from the CRAY Site Initializer (CSI) localization data through customizations.yaml into the UAS Helm chart and, ultimately, into Kubernetes in the form of a &amp;ldquo;network-attachment-definition&amp;rdquo;.&#xA;This section describes the data at each of those stages to show how the final network attachment gets created.&#xA;CSI Localization Data The details of CSI localization are beyond the scope of this guide, but here are the important settings, and the values used in the following examples:</description>
    </item>
    <item>
      <title>UAI macvlans Network Attachments</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/uai_macvlans_network_attachments/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:13 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/uai_macvlans_network_attachments/</guid>
      <description>UAI macvlans Network Attachments UAIs need to be able to reach compute nodes across the node management network (NMN). When the compute node NMN is structured as multiple subnets, this requires routing form the UAIs to those subnets. The default route in a UAI goes to the public network through the Customer Access Network (CAN) so that will not work for reaching compute nodes. To solve this problem, UAS installs Kubernetes network attachments within the Kubernetes user namespace, one of which is used by UAIs.</description>
    </item>
    <item>
      <title>UAS Limitations</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/uas_limitations/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:13 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/uas_limitations/</guid>
      <description>UAS Limitations Functionality that is currently not supported while using UAS.&#xA;Functionality Not Currently Supported by the User Access Service Lustre (lfs) commands within the UAS service pod Executing Singularity containers within the UAS service Building Docker containers within the UAS environment Building containerd containers within the UAS environment dmesg cannot run inside a UAI because of container security limitations Users cannot ssh from ncn-w001 to a UAI. This is because UAIs use LoadBalancer IP addresses on the Customer Access Network (CAN) instead of NodePorts and the LoadBalancer IP addresses are not accessible from ncn-w001.</description>
    </item>
    <item>
      <title>UAS and UAI Health Checks</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/uas_and_uai_health_checks/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:13 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/uas_and_uai_health_checks/</guid>
      <description>UAS and UAI Health Checks Initialize and authorize the CLI so a user may run procedures on any given node.&#xA;Initialize and Authorize the CLI The procedures below use the CLI as an authorized user and run on two separate node types. The first part runs on the LiveCD node while the second part runs on a non-LiveCD Kubernetes master or worker node. When using the CLI on either node, the CLI configuration must be initialized and the user running the procedure must be authorized.</description>
    </item>
    <item>
      <title>Update a Resource Specification</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/update_a_resource_specification/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:13 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/update_a_resource_specification/</guid>
      <description>Update a Resource Specification Modify a specific UAI resource specification using the resource_id of that specification.&#xA;Prerequisites Install and initialize the cray administrative CLI. Verify that the resource specification to be updated exists within UAS. Perform eitherList UAI Resource Specifications or Retrieve Resource Specification Details. Procedure To modify a particular resource specification, use a command of the following form:&#xA;ncn-m001-pit# cray uas admin config resources update [OPTIONS] RESOURCE_ID The [OPTIONS] used by this command are the same options used to create resource specifications.</description>
    </item>
    <item>
      <title>Update a UAI Image Registration</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/update_a_uai_image_registration/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:13 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/update_a_uai_image_registration/</guid>
      <description>Update a UAI Image Registration Modify the UAS registration information of a UAI image.&#xA;Prerequisites Verify that the image to be updated is registered with UAS. Refer to Retrieve UAI Image Registration Information.&#xA;Procedure Once an allowable UAI image has been created, it may be necessary to change its attributes. For example, the default image may need to change.&#xA;Modify the registration information of a UAI image by using a command of the form:</description>
    </item>
    <item>
      <title>Update a UAS Volume</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/update_a_uas_volume/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:13 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/update_a_uas_volume/</guid>
      <description>Update a UAS Volume Modify the configuration of an already-registered UAS volume. Almost any part of the configuration of a UAS volume can be modified.&#xA;Prerequisites Install and initialize the cray administrative CLI. Obtain the UAS volume ID of a volume. Perform List Volumes Registered in UAS if needed. Read Add a Volume to UAS. The options and caveats for updating volumes are the same as for creating volumes. Procedure Modify the configuration of a UAS volume.</description>
    </item>
    <item>
      <title>User Access Service (UAS)</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/user_access_service_uas/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:14 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/user_access_service_uas/</guid>
      <description>User Access Service (UAS) The User Access Service (UAS) is a containerized service managed by Kubernetes that enables application developers to create and run user applications. UAS runs on a non-compute node (NCN) that is acting as a Kubernetes worker node.&#xA;Users launch a User Access Instance (UAI) using the cray command. Users can also transfer data between the Cray system and external systems using the UAI.&#xA;When a user requests a new UAI, the UAS service returns status and connection information to the newly created UAI.</description>
    </item>
    <item>
      <title>View a UAI Class</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/view_a_uai_class/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:14 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/view_a_uai_class/</guid>
      <description>View a UAI Class Display all the information for a specific UAI class by referencing its class ID.&#xA;Prerequisites Install and initialize the cray administrative CLI. Obtain the ID of a UAI class. Procedure View all the information about a specific UAI class.&#xA;To examine an existing UAI class, use a command of the following form:&#xA;ncn-m001-pit# cray uas admin config classes describe &amp;lt;class-id&amp;gt; The following example uses the --format yaml option to display the UAI class configuration in YAML format.</description>
    </item>
    <item>
      <title>Volumes</title>
      <link>/docs-csm/en-10/operations/uas_user_and_admin_topics/volumes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:14 +0000</pubDate>
      <guid>/docs-csm/en-10/operations/uas_user_and_admin_topics/volumes/</guid>
      <description>Volumes Volumes provide a way to connect UAIs to external data, whether they be Kubernetes managed objects, external file systems or files, host node files and directories, or remote networked data to be used within the UAI.&#xA;The following are examples of how volumes are commonly used by UAIs:&#xA;To connect UAIs to configuration files like /etc/localtime maintained by the host node To connect end-user UAIs to Slurm or PBS Professional Workload Manager configuration shared through Kubernetes To connect end-user UAIs to Programming Environment libraries and tools hosted on the UAI host nodes To connect end-user UAIs to Lustre or other external storage for user data To connect broker UAIs to a directory service (see Configure a Broker UAI Class) or SSH configuration (see Customize the Broker UAI Image) needed to authenticate and redirect user sessions Any kind of volume recognized by the Kubernetes installation can be installed as a volume within UAS and will be used when creating UAIs.</description>
    </item>
  </channel>
</rss>
