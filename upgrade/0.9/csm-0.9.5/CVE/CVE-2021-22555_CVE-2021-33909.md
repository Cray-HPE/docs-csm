# CVE-2021-22555 CVE-2021-33909

This procedure covers patching `CVE-2021-22555` and `CVE-2021-33909` on Shasta V1.4.X (and upgrades CSM to v0.9.5).

A high-level overview of the procedure is as follows:

- install the new kernel directly to an NCN (usually ncn-m001)
- copy it to the on-disk bootloader
- copy it to a storage and worker node
- generate new artifacts based on that kernel
- delete the existing artifacts from S3
- upload the new artifacts to S3
- reboot NCNs

# Install the updated kernel packages

- **`TODO`** Extract the tarball(s):
- **`TODO`** Upload the repository to Nexus:

To install the updated kernel packages, the `zypper lock` needs to be removed, the new updates put in place, and then re-add the lock.

```bash
pdsh -b -w $(grep -oP 'ncn-\w\d+' /etc/hosts | sort -u |  tr -t '\n' ',') '
# Remove the zypper lock on the kernel on every NCN
zypper removelock kernel-default

# Add the repo with the new RPMs and refresh it
zypper ar http://slemaster.us.cray.com/SUSE/Updates/SLE-Module-Basesystem/15-SP2/x86_64/update/ internal-sp2-updates
zypper refresh internal-sp2-updates

# Install all of the kernel-related RPMs from the repo above
zypper -n up -y --repo=internal-sp2-updates $(rpm -qa --queryformat="%{NAME}\n" | grep kernel)

# Enable the updates repo
zypper mr --enable SLE-Module-Basesystem15-SP2-Updates
zypper refresh SLE-Module-Basesystem15-SP2-Updates

# Install all of the kernel-related RPMs from the repo above
zypper -n up -y --repo=SLE-Module-Basesystem15-SP2-Updates $(rpm -qa --queryformat='%{NAME}\n' | grep kernel)

# Put the kernel lock back in place
zypper addlock kernel-default
'
```

> **Do not reboot**

> the system may indicate (`zypper`) a reboot is needed but this will happen after uploading to our endpoints

# Install all product streams packages

```bash
zypper update --repo $cray-repo ... # TODO Add other packages or `cat` a list
```

> **Do not reboot**

> the system (`zypper`) may indicate a reboot is needed but this will happen after so do not reboot the NCNs yet

# Patch the Local Bootloader

The local on-disk bootloader still contains the old, vulnerable kernel so it needs to be updated using the script below.

```bash
pdsh -b -w $(grep -oP 'ncn-\w\d+' /etc/hosts | sort -u |  tr -t '\n' ',') '/tmp/kernel-update.sh'
```

```bash
version_full=$(rpm -q --queryformat "%{VERSION}-%{RELEASE}.%{ARCH}\n" kernel-default | tail -n 1)
version_base=${version_full%%-*}
version_suse=${version_full##*-}
version_suse=${version_suse%.*.*}
version="$version_base-$version_suse-default"
initrd="/boot/initrd-$version"
kernel="/boot/vmlinuz-$version"
ls $kernel $initrd # this should return 0 and show the files

# mount the bootraid
BOOTRAID=$(awk '/LABEL=BOOTRAID/ {print $2}' /etc/fstab.metal)
mount -L BOOTRAID -T /etc/fstab.metal
cp -pv $initrd $BOOTRAID/$(grep initrdefi $BOOTRAID/grub2/grub.cfg | awk '{print $2}' | awk -F'/' '{print $NF}')
cp -pv $kernel $BOOTRAID/kernel
```

Update the squashFS storage (do not touch the `*filesystem.squashfs` file).

```bash
# Mount the running system as read/write
mount -o remount,rw /run/initramfs/live
# Make a directory for the assets
mkdir -p /run/initramfs/live/LiveOS-1.4.2a/
# Copy them the kernel into place
cp -pv $BOOTRAID/kernel /run/initramfs/live/LiveOS
# Copy the initrd into place
cp -pv "$BOOTRAID/$(grep -Po "initrd=([\w\.]+)" /proc/cmdline | cut -d "=" -f2)" /run/initramfs/live/LiveOS/
# Remount as read-only
mount -o remount,ro /run/initramfs/live
```

# Rebuild the SquashFS on a worker and a storage node

Run this on any k8s worker node

> master nodes also work but it is easier to build an image where we don't have crypt/luks and since workers and master nodes share the same boot artifacts, this example shows building it on a worker node

```bash
# Fix the script to allow for the existence of the new and old kernels.
kernel_version_full=$(rpm -qa | grep kernel-default | grep -v devel | tail -n 1 | cut -f3- -d'-')
kernel_version=$(ls -1tr /boot/vmlinuz-* | tail -n 1 | cut -d '-' -f2,3,4)
sed -i 's/version_full=.*/version_full='"$kernel_version_full"'/g' /srv/cray/scripts/common/create-kis-artifacts.sh
sed -i 's/kernel_version=.*/kernel_version='"$kernel_version"'/g' /srv/cray/scripts/common/create-kis-artifacts.sh

# disable inclusion of fstab.metal so we make a vanilla/stock image without partition concessions.
rm -f /etc/dracut.conf.d/05-metal.conf

# /var/log is too active on workers, it might cause dracut to re-try perpetually if it changes size.
mkdir -p /mnt/squashfs/var/log
mount -o /tmp /mnt/squashfs/var/log  

# rebuild
# FIXME: All servers have same hostname; ensure no-host
/srv/cray/scripts/common/create-kis-artifacts.sh
ls -l /squashfs
chmod +r /squashfs/*               
umount /mnt/squashfs/var/log
# FIXME: leftover mountpoints
```

Repeat the process on any storage node (ncn-s001 for example):

```bash
# disable inclusion of fstab.metal so we make a vanilla/stock image without partition concessions.
rm -f /etc/dracut.conf.d/05-metal.conf

# rebuild
/srv/cray/scripts/common/create-kis-artifacts.sh
ls -l /squashfs
chmod +r /squashfs/*
```

# Upload the new assets to S3

> **`NOTE`** `csi` is used for this step

> `csi` can be installed from the CSM tarball, or accessed from the USB stick or remoteISO. See here for an example on Accessing [CSI from a USB or RemoteISO](../../../../007-CSM-INSTALL-REBOOT.md#accessing-csi-from-a-usb-or-remoteiso)

## Sync the storage and kubernetes artifacts over

```bash
rsync -aPv --progress ncn-w001:/squashfs/ /squashfs/k8s
rsync -aPv --progress ncn-s001:/squashfs/ /squashfs/ceph
```

## Replace the artifacts on S3

From ncn-m001:

```bash
cray artifacts delete ncn-images k8s-kernel
cray artifacts delete ncn-images k8s-initrd.img.xz
cray artifacts delete ncn-images k8s-filesystem.squashfs
cray artifacts delete ncn-images ceph-initrd.img.xz
cray artifacts delete ncn-images ceph-kernel
cray artifacts delete ncn-images ceph-filesystem.squashfs
# should show empty:
cray artifacts list ncn-images

# authenticate and upload to S3  
ln -snf /etc/kubernetes/admin.conf ~/.kube/config

/tmp/csi handoff ncn-images \
 --k8s-kernel-path $(/squashfs/k8s/*.kernel | tail -n 1) \
 --k8s-initrd-path /squashfs/k8s/initrd* \
 --k8s-squashfs-path /squashfs/k8s/*.squashfs \
 --ceph-kernel-path $(/squashfs/ceph/*.kernel | tail -n 1) \
 --ceph-initrd-path /squashfs/ceph/initrd* \
 --ceph-squashfs-path /squashfs/ceph/*.squashfs
```

# Reboot the NCNs

Reference the [Reboot NCNs procedure](../../../../102-NCN-REBOOTING.md).

Use cray-conman to observe the node as it boots:

```bash
ncn-m001# kubectl exec -it -n services cray-conman-<hash> cray-conman -- /bin/bash
cray-conman# conman -q
cray-conman# conman -j <name of terminal>
```

# Validate the NCN is running the new kernel

Once a system has booted, verify the new kernel is running.  This should match

```bash
uname -a
```
