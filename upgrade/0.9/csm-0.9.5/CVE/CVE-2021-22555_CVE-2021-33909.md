# CVE-2021-22555 CVE-2021-33909

This covers Shasta V1.4.X (upgrades CSM 0.9.5).

This page will direct you in patching and updating runtime endpoints with patched artifacts.

### Install the Patch Packages

1. Apply the update:

    - **`TODO`** Extract the tarball(s):
    - **`TODO`** Upload the repository to Nexus:

    - Install Base Kernel Packages:

        ```bash
        pdsh -b -w $(grep -oP 'ncn-\w\d+' /etc/hosts | sort -u |  tr -t '\n' ',') '
        zypper removelock kernel-default

        zypper ar http://slemaster.us.cray.com/SUSE/Updates/SLE-Module-Basesystem/15-SP2/x86_64/update/ internal-sp2-updates
        zypper refresh internal-sp2-updates

        zypper -n up -y --repo=internal-sp2-updates $(rpm -qa --queryformat="%{NAME}\n" | grep kernel)

        zypper mr --enable SLE-Module-Basesystem15-SP2-Updates
        zypper refresh SLE-Module-Basesystem15-SP2-Updates

        zypper -n up -y --repo=SLE-Module-Basesystem15-SP2-Updates $(rpm -qa --queryformat='%{NAME}\n' | grep kernel)

        zypper addlock kernel-default
        '
        ```

    - Install all product streams packages:

        ```bash
        zypper update --repo $cray-repo ... # TODO Add other packages or `cat` a list
        ```

    - **Do not reboot**, the system may indicate (`zypper`) a reboot is needed but this will happen after uploading to our endpoints

### Patch the Local Bootloader

1. Reload the metal bootloader on each NCN

    - Run this on each NCN to update its fallback bootloader:

      ```bash
      pdsh -b -w $(grep -oP 'ncn-\w\d+' /etc/hosts | sort -u |  tr -t '\n' ',') '/tmp/kernel-update.sh'
      ```

      ```bash
      version_full=$(rpm -q --queryformat "%{VERSION}-%{RELEASE}.%{ARCH}\n" kernel-default | tail -n 1)
      version_base=${version_full%%-*}
      version_suse=${version_full##*-}
      version_suse=${version_suse%.*.*}
      version="$version_base-$version_suse-default"
      initrd="/boot/initrd-$version"
      kernel="/boot/vmlinuz-$version"
      ls $kernel $initrd # this should return 0 and show the files

      # mount the bootraid
      BOOTRAID=$(awk '/LABEL=BOOTRAID/ {print $2}' /etc/fstab.metal)
      mount -L BOOTRAID -T /etc/fstab.metal
      cp -pv $initrd $BOOTRAID/$(grep initrdefi $BOOTRAID/grub2/grub.cfg | awk '{print $2}' | awk -F'/' '{print $NF}')
      cp -pv $kernel $BOOTRAID/kernel
      ```

    - Update the squashFS storage (do not touch the `*filesystem.squashfs` file).

      ```bash
      mount -o remount,rw /run/initramfs/live
      mkdir -p /run/initramfs/live/LiveOS-1.4.2a/
      cp -pv $BOOTRAID/kernel /run/initramfs/live/LiveOS
      cp -pv "$BOOTRAID/$(grep -Po "initrd=([\w\.]+)" /proc/cmdline | cut -d "=" -f2)" /run/initramfs/live/LiveOS/
      mount -o remount,ro /run/initramfs/live
      ```

1. Rebuild the SquashFS on a worker and a storage node

   - Run this on any k8s worker node (master nodes also work, it is easier to build an image where we don't have crypt/luks) rebuild the artifacts:

        ```bash
        # Fix the script to allow for the existence of the new and old kernels.
        kernel_version_full=$(rpm -qa | grep kernel-default | grep -v devel | tail -n 1 | cut -f3- -d'-')
        kernel_version=$(ls -1tr /boot/vmlinuz-* | tail -n 1 | cut -d '-' -f2,3,4)
        sed -i 's/version_full=.*/version_full='"$kernel_version_full"'/g' /srv/cray/scripts/common/create-kis-artifacts.sh
        sed -i 's/kernel_version=.*/kernel_version='"$kernel_version"'/g' /srv/cray/scripts/common/create-kis-artifacts.sh

        # disable inclusion of fstab.metal so we make a vanilla/stock image without partition concessions.
        rm -f /etc/dracut.conf.d/05-metal.conf

        # /var/log is too active on workers, it might cause dracut to re-try perpetually if it changes size.
        mkdir -p /mnt/squashfs/var/log
        mount -o /tmp /mnt/squashfs/var/log  

        # rebuild
        # FIXME: All servers have same hostname; ensure no-host
        /srv/cray/scripts/common/create-kis-artifacts.sh
        ls -l /squashfs
        chmod +r /squashfs/*               
        umount /mnt/squashfs/var/log
        # fixme: leftover mountpoints
        ```

   - On any storage node (ncn-m001 for example), rebuild the artifacts:

        ```bash
        # disable inclusion of fstab.metal so we make a vanilla/stock image without partition concessions.
        rm -f /etc/dracut.conf.d/05-metal.conf

        # rebuild
        /srv/cray/scripts/common/create-kis-artifacts.sh
        ls -l /squashfs
        chmod +r /squashfs/*
        ```

1. Upload to S3 from ncn-m001 using `csi`.

   - Sync the storage and kubernetes artifacts over

        ```bash
        rsync -aPv --progress ncn-w001:/squashfs/ /squashfs/k8s
        rsync -aPv --progress ncn-s001:/squashfs/ /squashfs/ceph
        ```

   - Replace the artifacts on S3
      > **`NOTE`** CSI can be installed from the CSM tarball, or accessed from the USB stick or remoteISO. See here for an example on (accessing CSI after rebooting)[https://stash.us.cray.com/projects/CSM/repos/docs-csm/browse/007-CSM-INSTALL-REBOOT.md?at=refs%2Fheads%2Frelease%2F0.9#accessing-csi-from-a-usb-or-remoteiso].

        ```bash
        cray artifacts delete ncn-images k8s-kernel
        cray artifacts delete ncn-images k8s-initrd.img.xz
        cray artifacts delete ncn-images k8s-filesystem.squashfs
        cray artifacts delete ncn-images ceph-initrd.img.xz
        cray artifacts delete ncn-images ceph-kernel
        cray artifacts delete ncn-images ceph-filesystem.squashfs
        # should show empty:
        cray artifacts list ncn-images

        # authenticate and upload to S3  
        ln -snf /etc/kubernetes/admin.conf ~/.kube/config

        /tmp/csi handoff ncn-images \
         --k8s-kernel-path $(/squashfs/k8s/*.kernel | tail -n 1) \
         --k8s-initrd-path /squashfs/k8s/initrd* \
         --k8s-squashfs-path /squashfs/k8s/*.squashfs \
         --ceph-kernel-path $(/squashfs/ceph/*.kernel | tail -n 1) \
         --ceph-initrd-path /squashfs/ceph/initrd* \
         --ceph-squashfs-path /squashfs/ceph/*.squashfs
        ```

### Reboot One NCN (validation step)

This step is to validate that what we did worked before we embark on rebooting everything we'll sample
the worker node we used in the previous step.

1. Reboot the worker node, set the boot menu to choose the entry the node just used (to guarantee we'll boot over the same medium).

   ```bash
   ncn-w001# efibootmgr -n "$(efibootmgr | grep -i bootcurrent | cut -d " " -f2)"
   ```

1. Initiate a soft power-cycle with ipmitool

   ```bash
   ncn-w001# ipmitool power cycle
   ```

1. Use cray-conman to observe the node

   ```bash
   ncn-m001# kubectl exec -it -n services cray-conman-<hash> cray-conman -- /bin/bash
   cray-conman# conman -q
   cray-conman# conman -j <name of terminal>
   ```

1. On boot up, run `uname -a` to verify the new kernel is running.

### Reboot the Rest of the NCNs

Reference the [Reboot NCNs procedure](../../../../102-NCN-REBOOTING.md).
