# CVE-2021-22555 CVE-2021-33909

This procedure covers patching `CVE-2021-22555` and `CVE-2021-33909` on Shasta V1.4.X (and upgrades CSM to v0.9.5).

A high-level overview of the procedure is as follows:

- install the new kernel directly to an NCN (usually ncn-m001)
- copy it to the on-disk bootloader
- copy it to a storage and worker node
- generate new artifacts based on that kernel
- delete the existing artifacts from S3
- upload the new artifacts to S3
- reboot NCNs

# Install the updated kernel packages

- **`TODO`** Extract the tarball(s):
- **`TODO`** Upload the repository to Nexus:

Remove the `zypper lock`, install the new kernel rpms, and then re-add the lock.

```bash
pdsh -b -w $(grep -oP 'ncn-\w\d+' /etc/hosts | sort -u |  tr -t '\n' ',') '
# Remove the zypper lock on the kernel on every NCN
zypper removelock kernel-default

# Add the repo with the new RPMs and refresh it
zypper ar http://slemaster.us.cray.com/SUSE/Updates/SLE-Module-Basesystem/15-SP2/x86_64/update/ internal-sp2-updates
zypper refresh internal-sp2-updates

# Install all of the kernel-related RPMs from the repo above
zypper -n up -y --repo=internal-sp2-updates $(rpm -qa --queryformat="%{NAME}\n" | grep kernel)

# Enable the updates repo
#zypper mr --enable SLE-Module-Basesystem15-SP2-Updates
#zypper refresh SLE-Module-Basesystem15-SP2-Updates

# Install all of the kernel-related RPMs from the repo above
#zypper -n up -y --repo=SLE-Module-Basesystem15-SP2-Updates $(rpm -qa --queryformat="%{NAME}\n" | grep kernel)

# Put the kernel lock back in place
zypper addlock kernel-default'
```
> **Do not reboot**

> the system (`zypper`) may indicate a reboot is needed but this will happen in a later step so do not reboot the NCNs yet

# Install all product streams packages

```bash
zypper update --repo $cray-repo ... # TODO Add other packages or `cat` a list
```

> **Do not reboot**

> the system (`zypper`) may indicate a reboot is needed but this will happen in a later step so do not reboot the NCNs yet

# Patch the Local Bootloader

The local, on-disk bootloader still contains the old, vulnerable kernel so it needs to be updated using the script below.

Make a here doc to create a script to update the kernel.

```bash
cat << 'EOF' > update-kernel.sh
version_full=$(rpm -q --queryformat '%{VERSION}-%{RELEASE}.%{ARCH}\n' kernel-default | tail -n 1)
version_base="${version_full%%-*}"
version_suse="${version_full##*-}"
version_suse="${version_suse%.*.*}"
version="$version_base-$version_suse-default"
initrd=/boot/initrd-$version
kernel=/boot/vmlinuz-$version
# this should return 0 and show the files;
ls "$kernel" "$initrd"
# mount the bootraid
BOOTRAID=$(awk '/LABEL=BOOTRAID/ {print $2}' /etc/fstab.metal)
mount -L BOOTRAID -T /etc/fstab.metal
cp -pv "$initrd" $BOOTRAID/boot/$(grep initrdefi $BOOTRAID/boot/grub2/grub.cfg | awk '{print $2}' | awk -F'/' '{print $NF}')
cp -pv $kernel $BOOTRAID/boot/kernel
EOF
chmod 755 update-kernel.sh
```

Make a here doc to create a script to update the squashfs.

```bash
cat << 'EOF' > update-squashfs.sh
version_full=$(rpm -q --queryformat '%{VERSION}-%{RELEASE}.%{ARCH}\n' kernel-default | tail -n 1)
version_base="${version_full%%-*}"
version_suse="${version_full##*-}"
version_suse="${version_suse%.*.*}"
version="$version_base-$version_suse-default"
initrd=/boot/initrd-$version
kernel=/boot/vmlinuz-$version
# this should return 0 and show the files;
ls "$kernel" "$initrd"
# mount the bootraid
BOOTRAID=$(awk '/LABEL=BOOTRAID/ {print $2}' /etc/fstab.metal)
# Mount the running system as read/write
mount -o remount,rw /run/initramfs/live
# Make a directory for the assets
mkdir -p /run/initramfs/live/LiveOS-1.4.2a/
# Copy them the kernel into place
cp -pv $BOOTRAID/boot/kernel /run/initramfs/live/LiveOS
# Copy the initrd into place
cp -pv "$BOOTRAID/boot/$(grep -Po "initrd=([\w\.]+)" /proc/cmdline | cut -d "=" -f2)" /run/initramfs/live/LiveOS/
# Remount as read-only
mount -o remount,ro /run/initramfs/live
EOF
chmod 755 update-squashfs.sh
```

Copy the `update-kernel.sh` script and run it:

```bash
for n in $(grep -oP 'ncn-\w\d+' /etc/hosts | sort -u)
do
  scp update-kernel.sh $n:/tmp/update-kernel.sh
  ssh $n "/tmp/update-kernel.sh"
done
```

Copy the `update-squashfs.sh` script and run it:

```bash
for n in $(grep -oP 'ncn-\w\d+' /etc/hosts | sort -u)
do
  scp update-squashfs.sh $n:/tmp/update-squashfs.sh
  ssh $n "/tmp/update-squashfs.sh"
done
```

# Rebuild the SquashFS

Run this step on any k8s worker node.

> master nodes also work but it is easier to build an image where we don't have crypt/luks and since workers and master nodes share the same boot artifacts, this example shows building it on a worker node

```bash
# Fix the script to allow for the existence of the new and old kernels.
kernel_version_full=$(rpm -qa | grep kernel-default | grep -v devel | tail -n 1 | cut -f3- -d'-')
kernel_version=$(ls -1tr /boot/vmlinuz-* | tail -n 1 | cut -d '-' -f2,3,4)
sed -i 's/version_full=.*/version_full='"$kernel_version_full"'/g' /srv/cray/scripts/common/create-kis-artifacts.sh
sed -i 's/kernel_version=.*/kernel_version='"$kernel_version"'/g' /srv/cray/scripts/common/create-kis-artifacts.sh

# disable inclusion of fstab.metal so we make a vanilla/stock image without partition concessions.
mv /etc/dracut.conf.d/05-metal.conf .

# /var/log is too active on workers, it might cause dracut to re-try perpetually if it changes size.
# mkdir -pv /mnt/squashfs/var/log
# mount -o /tmp /mnt/squashfs/var/log  

# rebuild
# FIXME: All servers have same hostname; ensure no-host
/srv/cray/scripts/common/create-kis-artifacts.sh
ls -l /squashfs
chmod +r /squashfs/*               
umount /mnt/squashfs/var/log
# FIXME: leftover mountpoints
```

## Repeat the process on any storage node

Log in to a storage node and repeat the process above.  This is necessary since the storage nodes use a different kernel and initrd.

# Upload the new assets to S3

> **`NOTE`** `csi` is used for this step

> `csi` can be installed from the CSM tarball, or accessed from the USB stick or remoteISO. See here for an example on Accessing [CSI from a USB or RemoteISO](../../../../007-CSM-INSTALL-REBOOT.md#accessing-csi-from-a-usb-or-remoteiso)

## Sync the storage and kubernetes artifacts over

From the worker and storage nodes that you rebuilt the squashfs on (ncn-w001 and ncn-s001 in this example), rsync the assets back to ncn-m001, where they can be uploaded to S3.

From ncn-m001:

```bash
rsync -aPv --progress ncn-w001:/squashfs/ /squashfs/k8s
rsync -aPv --progress ncn-s001:/squashfs/ /squashfs/ceph
```

## Replace the artifacts on S3

From ncn-m001:

```bash
cray artifacts delete ncn-images k8s-kernel
cray artifacts delete ncn-images k8s-initrd.img.xz
cray artifacts delete ncn-images k8s-filesystem.squashfs
cray artifacts delete ncn-images ceph-initrd.img.xz
cray artifacts delete ncn-images ceph-kernel
cray artifacts delete ncn-images ceph-filesystem.squashfs
# This should return nothing (empty)
cray artifacts list ncn-images

# authenticate and upload to S3  
ln -snf /etc/kubernetes/admin.conf ~/.kube/config

/tmp/csi handoff ncn-images \
  --k8s-kernel-path /squashfs/k8s/5.3.18-24.75-default.kernel \
  --k8s-initrd-path /squashfs/k8s/initrd.img.xz \
  --k8s-squashfs-path /squashfs/k8s/filesystem.squashfs \
  --ceph-kernel-path /squashfs/ceph/5.3.18-24.75-default.kernel \
  --ceph-initrd-path /squashfs/ceph/initrd.img.xz \
  --ceph-squashfs-path /squashfs/ceph/filesystem.squashfs
```

# Reboot the NCNs

Reference the [Reboot NCNs procedure](../../../../102-NCN-REBOOTING.md).

Optionally, use cray-conman to observe the node as it boots:

```bash
ncn-m001# kubectl exec -it -n services cray-conman-<hash> cray-conman -- /bin/bash
cray-conman# conman -q
cray-conman# conman -j <name of terminal>
```

# Validate the NCN is running the new kernel

Once a system has booted, verify the new kernel is running.  This should match `4.12.14-197.99.1`, which is the version of the kernel that addresses the CVE.

```bash
uname -a
```
