[
{
	"uri": "/docs-csm/en-12/upgrade/1.2.1/",
	"title": "CSM 1.2.1 Patch Installation Instructions",
	"tags": [],
	"description": "",
	"content": "CSM 1.2.1 Patch Installation Instructions Introduction This document guides an administrator through the patch update to Cray Systems Management v1.2.1 from v1.2.0. If upgrading from CSM v1.0.x directly to v1.2.1, follow the procedures described in Upgrade CSM instead.\nBug Fixes Fixes two issues in CFS, restoring the additional inventory field functionality. Fixes an issue restoring console services functionality on \u0026ldquo;Hill\u0026rdquo; cabinets. Fixes a few issues in PowerDNS where various records were missing in the AXFR transfer. Fixes a rare issue where the Istio container would not be available during a future upgrade to CSM 1.3.0. Fixes an issue where a modified NCN image can no longer boot to disk when specified instead of the default PXE boot. Fixes a rare issue where NCNs booted with a modified image containing Slingshot Host Software had NO-CARRIER on all network interfaces. Fixes an issue where CANU generates incorrect VLANs for switch ports connected to UANs over the CHN. Fixes an issue where dhcp-manager could apply NIC data to the wrong reservation in Kea Known Issues kdump (kernel dump) may hang and fail on NCNs in CSM 1.2 (HPE Cray EX System Software 22.07 release). During the upgrade, a workaround is applied to fix this. The boot order on NCNs may not be correctly set. Because of a bug, the disk entries may be listed ahead of the PXE entries. During the upgrade, a workaround is applied to fix this. This workaround will also fix missing disk entries after an upgrade. Steps Upgrade CSM network configuration Preparation Setup Nexus Upgrade services Verification Complete upgrade Optional: Upgrade CSM network configuration If you are using the CHN network tech preview, upgrade the CSM management network configuration before proceeding with the patch installation.\nDetailed information on the fixes and configuration updates after CANU release 1.6.5 can be found from CANU release notes\nUpgrade CSM network configuration Preparation Start a typescript on ncn-m001 to capture the commands and output from this procedure.\nncn-m001# script -af csm-update.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Download and extract the CSM v1.2.1 release to ncn-m001.\nSee Download and Extract CSM Product Release.\nSet CSM_DISTDIR to the directory of the extracted files.\nIMPORTANT: If necessary, change this command to match the actual location of the extracted files.\nncn-m001# CSM_DISTDIR=\u0026#34;$(pwd)/csm-1.2.1\u0026#34; ncn-m001# echo \u0026#34;${CSM_DISTDIR}\u0026#34; Set CSM_RELEASE_VERSION to the CSM release version.\nncn-m001# export CSM_RELEASE_VERSION=\u0026#34;$(${CSM_DISTDIR}/lib/version.sh --version)\u0026#34; ncn-m001# echo \u0026#34;${CSM_RELEASE_VERSION}\u0026#34; Download and install/upgrade the latest documentation on ncn-m001.\nSee Check for Latest Documentation.\nSetup Nexus Run lib/setup-nexus.sh to configure Nexus and upload new CSM RPM repositories, container images, and Helm charts:\nncn-m001# cd \u0026#34;$CSM_DISTDIR\u0026#34; ncn-m001# ./lib/setup-nexus.sh On success, setup-nexus.sh will output OK on stderr and exit with status code 0. For example:\nncn-m001# ./lib/setup-nexus.sh [... output omitted ...] + Nexus setup complete setup-nexus.sh: OK ncn-m001# echo $? 0 In the event of an error, consult Troubleshoot Nexus to resolve potential problems and then try running setup-nexus.sh again. Note that subsequent runs of setup-nexus.sh may report FAIL when uploading duplicate assets. This is okay as long as setup-nexus.sh outputs setup-nexus.sh: OK and exits with status code 0.\nUpgrade services Run upgrade.sh to deploy upgraded CSM applications and services:\nncn-m001# cd \u0026#34;$CSM_DISTDIR\u0026#34; ncn-m001# ./upgrade.sh Verification Verify that the new CSM version is in the product catalog.\nVerify that the new CSM version is listed in the output of the following command:\nncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r -j - | jq -r \u0026#39;to_entries[] | .key\u0026#39; | sort -V Example output that includes the new CSM version (1.2.1):\n0.9.2 0.9.3 0.9.4 0.9.5 0.9.6 1.0.1 1.0.10 1.2.0 1.2.1 Confirm that the product catalog has an accurate timestamp for the CSM upgrade.\nConfirm that the import_date reflects the timestamp of the upgrade.\nncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r - \u0026#39;\u0026#34;1.2.1\u0026#34;.configuration.import_date\u0026#39; Complete upgrade Remember to exit the typescript that was started at the beginning of the upgrade.\nncn-m001# exit It is recommended to save the typescript file for later reference.\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2.2/",
	"title": "CSM 1.2.2 Patch Installation Instructions",
	"tags": [],
	"description": "",
	"content": "CSM 1.2.2 Patch Installation Instructions Introduction This document guides an administrator through the patch update to Cray Systems Management v1.2.2 from v1.2.0 or 1.2.1. If upgrading from CSM v1.0.x directly to v1.2.2, follow the procedures described in Upgrade CSM instead.\nBug Fixes and Improvements Fixes the incorrect AppVersion that was being reported from csi version report Fixes speed with which the pods are restarted as a part of the precache chart upgrade (for better performance) Increases the timeout for the etcd_database_health check in the ncn-healthcheck Fixes issue with postgres database backups that caused them to fail to restore and cleans up existing (bad) postgres backups on the system Fixes the pod anti-affinity settings and pod disruption budget settings for cray-dns-unbound Fixes problem with unbound forwarding to powerDNS in an air-gapped environment Fixes a race condition between MEDS adding the initial BMC entries and Kea dhcp-helper logic updating IP addresses Fixes an issue where snmp credentials being set on leaf switches were being lost Fixes an issue where cray-hmcollector-poll pod was not collecting river telemetry due to a check the collector does against the SMA kafka instance Fixes CVEs in container images Limits Keycloak access (via ingress OPA policy) to well known OIDC endpoints for Shasta realm and only allow Keycloak administration through CMN Adds remediation for CVE-2020-10770 for keycloak Fixes issue with missing App.version field in csi version command Adds capability to capmc to use the PATCH URI when trying to set multiple controls for Olympus hardware Fixes failure in backing up vcs data when there are extra spaces in the pod name Adds procedure to remediate NCN image access and exposure to security hardening guide Adds documentation for CSM post-install SNMP exporter settings Updates the System Power On documentation to add a rolling restart of spire request-ncn-join-token (to avoid issues with spire tokens) Documents workaround for iLO FW dropping redfish subscriptions Adds improvements to documentation of canu commands Documents known issues for gatekeeper constraint and refused connection Documents syntax error in gateway test example command Documents a workaround for known issue with boot order Improves documentation around CFS image customization session procedure Documents usage of the --no-cache flag when resuming CSM services install Improvements and better documentation for the external SSH test Adds clarity to Site Init documentation on external hosts Adds documentation for the DNS zone forwarding for powerDNS in an air-gapped environment Adds timeout to etcd database health check Fixes the NCN boot artifacts validation test Adds NTP goss test Improvements in pod back-up test scripts Known Issues kdump (kernel dump) may hang and fail on NCNs in CSM 1.2 (HPE Cray EX System Software 22.07 release). During the upgrade, a workaround is applied to fix this. The boot order on NCNs may not be correctly set. Because of a bug, the disk entries may be listed ahead of the PXE entries. During the upgrade, a workaround is applied to fix this. This workaround will also fix missing disk entries after an upgrade. Steps Upgrade CSM network configuration Preparation Setup Nexus Upgrade services Verification Complete upgrade Optional: Upgrade CSM network configuration If you are using the CHN network tech preview, upgrade the CSM management network configuration before proceeding with the patch installation.\nDetailed information on the fixes and configuration updates after CANU release 1.6.5 can be found from CANU release notes\nUpgrade CSM network configuration Preparation Start a typescript on ncn-m001 to capture the commands and output from this procedure.\nncn-m001# script -af csm-update.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Download and extract the CSM v1.2.2 release to ncn-m001.\nSee Download and Extract CSM Product Release.\nSet CSM_DISTDIR to the directory of the extracted files.\nIMPORTANT: If necessary, change this command to match the actual location of the extracted files.\nncn-m001# CSM_DISTDIR=\u0026#34;$(pwd)/csm-1.2.2\u0026#34; ncn-m001# echo \u0026#34;${CSM_DISTDIR}\u0026#34; Set CSM_RELEASE_VERSION to the CSM release version.\nncn-m001# export CSM_RELEASE_VERSION=\u0026#34;$(${CSM_DISTDIR}/lib/version.sh --version)\u0026#34; ncn-m001# echo \u0026#34;${CSM_RELEASE_VERSION}\u0026#34; Download and install/upgrade the latest documentation on ncn-m001.\nSee Check for Latest Documentation.\nSetup Nexus Run lib/setup-nexus.sh to configure Nexus and upload new CSM RPM repositories, container images, and Helm charts:\nncn-m001# cd \u0026#34;$CSM_DISTDIR\u0026#34; ncn-m001# ./lib/setup-nexus.sh On success, setup-nexus.sh will output OK on stderr and exit with status code 0. For example:\nncn-m001# ./lib/setup-nexus.sh [... output omitted ...] + Nexus setup complete setup-nexus.sh: OK ncn-m001# echo $? 0 In the event of an error, consult Troubleshoot Nexus to resolve potential problems and then try running setup-nexus.sh again. Note that subsequent runs of setup-nexus.sh may report FAIL when uploading duplicate assets. This is okay as long as setup-nexus.sh outputs setup-nexus.sh: OK and exits with status code 0.\nUpgrade services Run upgrade.sh to deploy upgraded CSM applications and services:\nncn-m001# cd \u0026#34;$CSM_DISTDIR\u0026#34; ncn-m001# ./upgrade.sh Verification Verify that the new CSM version is in the product catalog.\nVerify that the new CSM version is listed in the output of the following command:\nncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r -j - | jq -r \u0026#39;to_entries[] | .key\u0026#39; | sort -V Example output that includes the new CSM version (1.2.2):\n0.9.2 0.9.3 0.9.4 0.9.5 0.9.6 1.0.1 1.0.10 1.2.0 1.2.1 1.2.2 Confirm that the product catalog has an accurate timestamp for the CSM upgrade.\nConfirm that the import_date reflects the timestamp of the upgrade.\nncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r - \u0026#39;\u0026#34;1.2.2\u0026#34;.configuration.import_date\u0026#39; Complete upgrade Remember to exit the typescript that was started at the beginning of the upgrade.\nncn-m001# exit It is recommended to save the typescript file for later reference.\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/scripts/upgrade/upgrade_automation/",
	"title": "Upgrade Automation",
	"tags": [],
	"description": "",
	"content": "Upgrade Automation This document describes how we automate upgrade process. In a nutshell, an upgrade is:\nUpload new artifacts into a running system Rebuild each NCN with the newly uploaded images Deploy new charts after all NCNs are upgraded. Prerequisites Everything we need before upgrading an NCN.\nNOTE:\nThe prerequisites.sh script is required to run on both ncn-m001 and ncn-m002. If an action is only needed to run once, developers must add logic to avoid running this action both times the script is executed.\nNCN Upgrade Detailed implementation of how each type of node is being upgraded.\nNOTE:\nDepending on the type of node, we have to deal with backup/restore/health check differently. So each type of NCN has its own script for special handling\nCSM Services Upgrade Everything we do after the NCNs are upgraded. The most important part here is to deploy new charts, but depending on the specific upgrade being performed, there may also be other required actions. For example, the kafka cluster might need some extra logic to upgrade which cannot be done within its helm chart. Such logic is done during this portion of the upgrade.\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/scripts/sls/expert_mode/",
	"title": "SLS Updates Expert mode",
	"tags": [],
	"description": "",
	"content": "SLS Updates Expert mode The 1.2 SLS Upgrader aims to make the upgrade of SLS from pre-CMS 1.2 to CSM 1.2 as seamless as possible. However, certain system constraints and subnet sizes quickly necessitate overriding standard input.\nKey takeaways When to use expert mode Minimal input review Forcing preservation of some existing values Expert mode: Overriding defaults and forced guiding of upgrades Expert mode: Prerequisites Expert mode: Process Expert mode: Example Key takeaways Expert mode requires both an iterative (offline) process and command line override values. The output from the upgrade is a file; the configuration is not uploaded live to the system. The output logs from an upgrade run should be reviewed to ensure that input values and upgrader subnetting has the desired effect. When to use expert mode Both external-dns and NCN IP addresses need to be preserved. Subnets created by the upgrader are in the wrong order or are too small, or a subnet size and location in a network needs to be enforced. Running in non-expert mode generates errors. Running in non-expert mode generates warnings for which remediation is desired. Minimal input review The upgrader requires only two inputs:\nSLS input file: This is a file extract of SLS to be upgraded. Bifurcated CAN path for non-administrator traffic: Selection of CAN or CHN, depending on whether user traffic (users running jobs, not administrators) will access the system over the management network CAN or the highspeed network (CHN). An example of this would be:\nncn# ./sls_updater_csm_1.2.py \\ --sls-input-file sls_input_file.json \\ --bican-user-network-name CAN The above example will use default values for all other input values. A list of input parameters and default values can be found by running ./sls_updater_csm_1.2.py --help. Likely using default VLAN and network values will not be what is desired: The CAN or CHN are usually site-routable values.\nIn this case, an example minimal usable input while using CAN could be:\nncn# ./sls_updater_csm_1.2.py \\ --sls-input-file sls_input_file.json \\ --bican-user-network-name CAN \\ --customer-access-network \u0026lt;CAN VLAN ID\u0026gt; \u0026lt;CAN NETWORK CIDR\u0026gt; For CHN, the corresponding minimal usable input would be:\nncn# ./sls_updater_csm_1.2.py \\ --sls-input-file sls_input_file.json \\ --bican-user-network-name CHN \\ --customer-highspeed-network \u0026lt;CHN VLAN ID\u0026gt; \u0026lt;CHN NETWORK CIDR\u0026gt; Forcing preservation of some existing values As part of the upgrade process an existing CAN is migrated to a CSM 1.2 Customer Management Network (CMN). This is because most systems in production use the CAN as an administrative network. Additionally, system IP addresses shared with the site (external DNS, for example) are administrative IP addresses, and it is often more difficult to change these IP addresses than it is to make changes in software and avoid operational changes.\nTo allow some semblance of control over the need to preserve one or more IP addresses (existing CAN, migrated CMN only), the --preserve-existing-subnet-for-cmn was introduced. --preserve-existing-subnet-for-cmn has two possible values:\nexternal-dns: This is the IP address by which customer/site DNS lookups to system internal (Kubernetes services) DNS happen. Often changing this requires operational change control from the site. NOTE: Because of this, --preserve-existing-subnet-for-cmn external-dns is the most frequent use of this command line flag. ncns: During the migration of CAN to CMN, all system switches need to be added to a new network_hardware subnet inside the CMN. Without guidance, NCN IP addresses for the existing CAN (as it is migrated to CMN) will shift to allow room for the new network_hardware subnet. Using --preserve-existing-subnet-for-cmn ncns will prevent changes to CMN NCN IP addresses (managers, workers, and storage only) during the upgrade process. Note that external-dns preservation is mutually exclusive from ncns. This is the last \u0026ldquo;easy button\u0026rdquo; before full expert mode is required.\nFor a system desiring the CHN with no change of the external-dns value, a very common and recommended next step minimal command line is as follows:\nncn# ./sls_updater_csm_1.2.py \\ --sls-input-file sls_input_file.json \\ --bican-user-network-name CHN \\ --customer-highspeed-network \u0026lt;CHN VLAN ID\u0026gt; \u0026lt;CHN NETWORK CIDR\u0026gt; \\ --preserve-existing-subnet-for-cmn external-dns This creates a new CHN and migrates the existing CAN to the new CMN, while maintaining the external-dns IP address in the process. Note that this command line would very likely change existing CAN/CMN addresses on manager, worker, and storage NCNs during the upgrade to CSM 1.2.\nNOTE: The output of the upgrader is to a file, not the screen. The logged output of the upgrader should be used as a guide and reviewed.\nFor example, using:\nncn# ./sls_updater_csm_1.2.py --sls-input-file sls_input_file.json --bican-user-network-name CAN \\ --customer-access-network 6 10.103.11.128/25 --preserve-existing-subnet-for-cmn external-dns The log output for migration/converting the existing CAN to the new CMN looks like:\nConverting existing CAN network to CMN. Attempting to preserve metallb_static pool subnet cmn_metallb_static_pool to pin external-dns IPv4 address Creating subnets in the following order [\u0026#39;metallb_static_pool\u0026#39;, \u0026#39;metallb_address_pool\u0026#39;, \u0026#39;network_hardware\u0026#39;, \u0026#39;bootstrap_dhcp\u0026#39;] Calculating seed/start prefix based on devices in case no further guidance is given INFO: Overrides may be provided on the command line with --\u0026lt;can|cmn\u0026gt;-subnet-override. Preserving cmn_metallb_static_pool with 10.103.11.60/30 Remaining subnets: [\u0026#39;10.103.11.64/26\u0026#39;, \u0026#39;10.103.11.0/27\u0026#39;, \u0026#39;10.103.11.32/28\u0026#39;, \u0026#39;10.103.11.48/29\u0026#39;, \u0026#39;10.103.11.56/30\u0026#39;] Creating cmn_metallb_address_pool with 0 devices: A /31 could work and would hold up to 0 devices (including gateway) Using 10.103.11.64/26 from 10.103.11.0/25 that can hold up to 62 devices (including gateway) Adding gateway IP address Adding IPs for 0 Reservations Remaining subnets: [\u0026#39;10.103.11.0/27\u0026#39;, \u0026#39;10.103.11.32/28\u0026#39;, \u0026#39;10.103.11.48/29\u0026#39;, \u0026#39;10.103.11.56/30\u0026#39;] Subnet network_hardware not found, using HMN as template Creating network_hardware with 3 devices: A /29 could work and would hold up to 6 devices (including gateway) Using 10.103.11.0/27 from 10.103.11.0/25 that can hold up to 30 devices (including gateway) Using VLAN 7 to override templating from HMN Adding gateway IP address Adding IPs for 3 Reservations Remaining subnets: [\u0026#39;10.103.11.32/28\u0026#39;, \u0026#39;10.103.11.48/29\u0026#39;, \u0026#39;10.103.11.56/30\u0026#39;] Creating bootstrap_dhcp with 13 devices: A /28 could work and would hold up to 14 devices (including gateway) Using 10.103.11.32/28 from 10.103.11.0/25 that can hold up to 14 devices (including gateway) Adding gateway IP address Adding IPs for 13 Reservations Setting DHCP Ranges WARNING: Insufficient IPv4 addresses to create DHCP ranges - 13 devices in a subnet supporting 14 devices. Expert mode --\u0026lt;can|cmn\u0026gt;-subnet-override may be used to change this behavior. Remaining subnets: [\u0026#39;10.103.11.48/29\u0026#39;, \u0026#39;10.103.11.56/30\u0026#39;] Applying supernet hack to network_hardware Applying supernet hack to bootstrap_dhcp Cleaning up remnant CAN switch reservations in boostrap_dhcp That is quite a bit of output, but some critical points are:\nOutput Interpretation Attempting to preserve metallb_static pool subnet cmn_metallb_static_pool This confirms that the upgrader knows that it was asked to preserve the external-dns IP address. ['metallb_static_pool', 'metallb_address_pool', 'network_hardware', 'bootstrap_dhcp'] This is the order in which CAN subnets will be migrated to the CMN. Remaining subnets: ['10.103.11.0/27', '10.103.11.32/28', '10.103.11.48/29', '10.103.11.56/30'] After each subnet is allocated, the remaining subnets available for the next step are listed. WARNING: Insufficient IPv4 addresses to create DHCP ranges - 13 devices in a subnet supporting 14 devices. The bootstrap_dhcp subnet (Kubernetes NCNs) is large enough to create the NCNs but NOT large enough to create a DHCP pool effectively. Technically the SLS file generated by this run will work, but some pool of addresses is likely needed in bootstrap_dhcp, so this warning should be remediated. Expert mode: Overriding defaults and forced guiding of upgrades Due to pre-upgrade subnet sizes or multiple constraints, the \u0026ldquo;easy\u0026rdquo; input values might not be sufficient for some systems. This requires \u0026ldquo;expert mode\u0026rdquo;.\nExpert mode: Prerequisites Knowledge of system and site network parameters (IPv4 addresses, subnets, and VLANs). The ability to: Create subnets from a given network. Read output subnet values. Based on upgrade output, make an educated decision on how to modify upgrader input values for the next run. Slow down, be patient. Expert mode: Process Review the existing networks and subnets in the SLS input file. Make an educated guess about how sls_updater_csm_1.2.py should work and run the program. A typical first pass is to --preserve-existing-subnet-for-cmn external-dns. Review the logged output, noting warnings and errors as well as the location and size of subnets. Copy the first entry in the log which says Creating subnets in the following order. If --preserve-existing-subnet-for-cmn external-dns was used, remove the metallb_address_pool from the subnets list. If --preserve-existing-subnet-for-cmn external-dns was used, remove bootstrap_dhcp from the subnets list. Copy the first entry in the log which says Remaining subnets. This provides the canonical list from which remaining subnets in the previous entry can be assigned IP addresses. Add override command line values to override the upgrader\u0026rsquo;s default logic and pin subnet allocations for each network. This is a manual step, but is safe because the upgrader produced both the list of subnets and the subnet IPAM allocations. Users are simply assigning subnets from a fixed list and a pre-allocated list of IPv4 subnets. Re-run the upgrader with the new parameters. Repeat the process, if necessary. Expert mode: Example Consider the following requirements:\nThe user network will be the CHN. The external-dns IP address must be maintained to avoid operational changes to the site/customer network. The upgrade to CSM 1.2 will be a rolling upgrade, not a full outage. In order to prevent race conditions during the rolling upgrade with temporary IP address overlaps, preservation of the Kubernetes NCN IP addresses for the CAN (CMN) is required. The focus of the process that follows will be on the CMN IP address allocations. The same process may be used for the CAN.\nProcess Review of the existing SLS file for CAN shows that NCN addresses are in the range: 10.103.11.2 to 10.103.11.14 with a DHCP pool above this.\nAn educated first pass is to run the updater while preserving the external-dns IP address only and look for the CMN output:\nncn# ./sls_updater_csm_1.2.py \\ --sls-input-file sls_input_file.json \\ --bican-user-network-name CHN \\ --customer-highspeed-network 55 172.16.0.0/16 \\ --preserve-existing-subnet-for-cmn external-dns Example log output:\n...snip... Converting existing CAN network to CMN. Attempting to preserve metallb_static pool subnet cmn_metallb_static_pool to pin external-dns IPv4 address Creating subnets in the following order [\u0026#39;metallb_static_pool\u0026#39;, \u0026#39;metallb_address_pool\u0026#39;, \u0026#39;network_hardware\u0026#39;, \u0026#39;bootstrap_dhcp\u0026#39;] Calculating seed/start prefix based on devices in case no further guidance is given INFO: Overrides may be provided on the command line with --\u0026lt;can|cmn\u0026gt;-subnet-override. Preserving cmn_metallb_static_pool with 10.103.11.60/30 Remaining subnets: [\u0026#39;10.103.11.64/26\u0026#39;, \u0026#39;10.103.11.0/27\u0026#39;, \u0026#39;10.103.11.32/28\u0026#39;, \u0026#39;10.103.11.48/29\u0026#39;, \u0026#39;10.103.11.56/30\u0026#39;] ...snip... Identify and copy the important log output:\nCreating subnets in the following order ['metallb_static_pool', 'metallb_address_pool', 'network_hardware', 'bootstrap_dhcp'] external-dns was preserved, so metallb_static_pool is removed from the available list. Remaining subnets: ['10.103.11.64/26', '10.103.11.0/27', '10.103.11.32/28', '10.103.11.48/29', '10.103.11.56/30'] The task is to map named subnets into IP subnet ranges based on required and desired constraints, and develop --cmn-subnet-override parameters from the mapping.\nAnother immediate constraint is to preserve NCN IP addresses for the CAN as it is transformed into the CMN. NCNs are in the boostrap_dhcp subnet and review of SLS data confirms that NCNs were previously in a range that fits into the new Remaining subnet of 10.103.11.0/27. Generally another desired constraint is to make the service IP address pool allocated to metallb_address_pool as large as possible. After removing NCNs from the previous step, the next largest subnet is 10.103.11.64/26; this will be assigned to metallb_address_pool. The best practice at this point is to manually pin the remaining subnets (in this example, only network_hardware) to provide a full override. Any remaining IP subnet from the list may be used. For the example, this network_hardware is pinned to 10.103.11.32/28. The next run of the upgrader for the example looks like:\nncn# ./sls_updater_csm_1.2.py \\ --sls-input-file sls_input_file.json \\ --bican-user-network-name CHN \\ --customer-highspeed-network 55 172.16.0.0/16 \\ --preserve-existing-subnet-for-cmn external-dns \\ --cmn-subnet-override bootstrap_dhcp 10.103.11.0/27 \\ --cmn-subnet-override cmn_metallb_address_pool 10.103.11.64/26 \\ --cmn-subnet-override network_hardware 10.103.11.32/28 The next run completes successfully and the logs show the following CMN allocations:\nConverting existing CAN network to CMN. Attempting to preserve metallb_static pool subnet cmn_metallb_static_pool to pin external-dns IPv4 address Creating subnets in the following order [\u0026#39;metallb_static_pool\u0026#39;, \u0026#39;metallb_address_pool\u0026#39;, \u0026#39;network_hardware\u0026#39;, \u0026#39;bootstrap_dhcp\u0026#39;] Calculating seed/start prefix based on devices in case no further guidance is given INFO: Overrides may be provided on the command line with --\u0026lt;can|cmn\u0026gt;-subnet-override. Preserving cmn_metallb_static_pool with 10.103.11.60/30 Remaining subnets: [\u0026#39;10.103.11.64/26\u0026#39;, \u0026#39;10.103.11.0/27\u0026#39;, \u0026#39;10.103.11.32/28\u0026#39;, \u0026#39;10.103.11.48/29\u0026#39;, \u0026#39;10.103.11.56/30\u0026#39;] Subnet cmn_metallb_address_pool was assigned 10.103.11.64/26 from the command command line. Creating cmn_metallb_address_pool with 0 devices: A /31 could work and would hold up to 0 devices (including gateway) Using 10.103.11.64/26 from 10.103.11.0/25 that can hold up to 62 devices (including gateway) Adding gateway IP address Adding IPs for 0 Reservations Remaining subnets: [\u0026#39;10.103.11.0/27\u0026#39;, \u0026#39;10.103.11.32/28\u0026#39;, \u0026#39;10.103.11.48/29\u0026#39;, \u0026#39;10.103.11.56/30\u0026#39;] Subnet network_hardware not found, using HMN as template Subnet network_hardware was assigned 10.103.11.32/28 from the command command line. Creating network_hardware with 3 devices: A /29 could work and would hold up to 6 devices (including gateway) Using 10.103.11.32/28 from 10.103.11.0/25 that can hold up to 14 devices (including gateway) Using VLAN 7 to override templating from HMN Adding gateway IP address Adding IPs for 3 Reservations Remaining subnets: [\u0026#39;10.103.11.0/27\u0026#39;, \u0026#39;10.103.11.48/29\u0026#39;, \u0026#39;10.103.11.56/30\u0026#39;] Subnet bootstrap_dhcp was assigned 10.103.11.0/27 from the command command line. Creating bootstrap_dhcp with 13 devices: A /28 could work and would hold up to 14 devices (including gateway) Using 10.103.11.0/27 from 10.103.11.0/25 that can hold up to 30 devices (including gateway) Adding gateway IP address Adding IPs for 13 Reservations Setting DHCP Ranges Remaining subnets: [\u0026#39;10.103.11.48/29\u0026#39;, \u0026#39;10.103.11.56/30\u0026#39;] Applying supernet hack to network_hardware Applying supernet hack to bootstrap_dhcp Cleaning up remnant CAN switch reservations in boostrap_dhcp After review, the allocations for CMN IP addresses in this run are as prescribed. A similar expert override process can be followed if a CAN is desired rather than a CMN.\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/scripts/sls/sls_utils/",
	"title": "sls utils Library",
	"tags": [],
	"description": "",
	"content": "sls_utils Library This is a reusable Python library for safely interacting with SLS network data (in JSON format).\nThe library has been tested against Python version 3.6 and up.\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/scripts/sls/readme.sls_upgrade/",
	"title": "Upgrade SLS Offline from CSM 1.0.x to CSM 1.2",
	"tags": [],
	"description": "",
	"content": "Upgrade SLS Offline from CSM 1.0.x to CSM 1.2 Abstract Prior to updating SLS, at a minimum, answers to the following questions must be known:\nWill user traffic (non-administrative) come in via the CAN or CHN, or is the site air-gapped?\nBy default when upgrading to CSM 1.2, non-administrative user traffic is migrated from the CAN to the CHN while minimizing changes. If instead it is desired that the non-administrative user traffic continue to come in via the CAN, or if the site is air-gapped, then this must be explicitly specified during the SLS update.\nWhat is the internal VLAN and the site-routable IP subnet for the new CAN or CHN?\nIs there a need to preserve any existing IP addresses during the CAN-to-CMN migration?\nOne example is the external-dns IP address used for DNS lookups of system resources from site DNS servers. Changes to external-dns often require changes to site resources with requisite process and timeframes from other groups. For preserving external-dns IP addresses, the flag is --preserve-existing-subnet-for-cmn external-dns.\nWARNING: It is up to the administrator to compare pre-upgraded and post-upgraded SLS files for sanity. Specifically, in the case of preserving external-dns values, the administrator must ensure there are no site-networking changes that might result in NCN IP addresses overlapping during the upgrade process. This requires network subnetting expertise and \u0026ldquo;expert mode\u0026rdquo; (described below).\nIf the external-dns IP address is changed, then the customizations.yaml site_to_system_lookups value must be updated to the new IP address. For instructions on how to do this, see Update customizations.yaml.\nA mutually exclusive example is the need to preserve all NCN IP addresses related to the old CAN while migrating the new CMN. This preservation is not often needed as the transition of NCN IP addresses for the CAN-to-CMN is automatically handled during the upgrade. The flag to preserve CAN-to-CMN NCN IP addresses is mutually exclusive with other preservations and the flag is --preserve-existing-subnet-for-cmn ncns.\nIf no preservation flag is set, then the default behavior is to recalculate every IP address on the existing CAN while migrating to the CMN. The behavior in this case is to calculate the subnet sizes based on number of devices (with a bit of spare room), while maximizing IP address pool sizes for dynamic services.\nPrerequisites The latest CSM documentation RPM must be installed on the node where the procedure is being performed. See Check for Latest Documentation. Procedure Get a token.\nncn# export TOKEN=$(curl -s -k -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Extract SLS data to a file.\nncn# curl -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api-gw-service-nmn.local/apis/sls/v1/dumpstate | jq -S . \u0026gt; sls_input_file.json Upgrade SLS data.\nExample 1: Upgrade, using the CHN as the system default route (will by default output to migrated_sls_file.json).\nncn# /usr/share/doc/csm/upgrade/1.2/scripts/sls/sls_updater_csm_1.2.py --sls-input-file sls_input_file.json \\ --bican-user-network-name CHN \\ --customer-highspeed-network 5 10.103.11.192/26 Example 2: Upgrade, using the CAN as the system default route, keep the generated CHN (for testing), and preserve the existing external-dns entry.\nncn# /usr/share/doc/csm/upgrade/1.2/scripts/sls/sls_updater_csm_1.2.py --sls-input-file sls_input_file.json \\ --bican-user-network-name CAN \\ --customer-highspeed-network 5 10.103.11.192/26 \\ --preserve-existing-subnet-for-cmn external-dns \\ --retain-unused-user-network NOTE: A detailed review of the migrated/upgraded data is strongly recommended. Particularly, ensure that subnet reservations are correct to prevent any data loss.\nUpload migrated SLS file to SLS service.\nncn# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -k -L -X POST \u0026#39;https://api-gw-service-nmn.local/apis/sls/v1/loadstate\u0026#39; -F \u0026#39;sls_dump=@migrated_sls_file.json\u0026#39; SLS Updater help For help and all options, run the following:\nncn# /usr/share/doc/csm/upgrade/1.2/scripts/sls/sls_updater_csm_1.2.py --help Actions and order This migration is performed offline for data security. The running SLS file is first dumped, then the migration script is run and a new, migrated output file is created.\nMigrate switch naming (in order): leaf to leaf-bmc, and agg to leaf. Remove api-gateway entries from HMLB subnets for CSM 1.2 security. Remove kubeapi-vip reservations for all networks except NMN. Create the new BICAN \u0026ldquo;toggle\u0026rdquo; network. Migrate the existing CAN to CMN. Create the CHN. Convert IP addresses of the CAN. Create MetalLB Pools and ASN entries on CMN and NMN. Update uai_macvlan in NMN DHCP ranges and uai_macvlan VLAN. Remove unused user networks (CAN or CHN) if requested (--retain-unused-user-network to keep). Migrate switch names Switch names change in CSM 1.2 and must be applied in the following order:\nleaf switches become leaf-bmc switches. agg switches become leaf switches. This needs to be done in the order listed above.\nRemove api-gateway / istio-ingress-gateway reservations from HMNLB subnets For CSM 1.2, the API gateway will no longer listen on the HMNLB MetalLB address pool. These aliases provided DNS records and are being removed.\nCorrect any erroneous Unbound DNS IPv4 addresses Some systems installed Shasta 1.4 and prior contained a bug in CSI which created reservations with incorrect IP addresses.\nCreate the BICAN network \u0026ldquo;toggle\u0026rdquo; New for CSM 1.2, the BICAN network ExtraProperties value of SystemDefaultRoute is used to point to the CAN, CHN, or CMN, and is used by utilities to systematically toggle routes.\nMigrate existing CAN to new CMN Using the existing CAN as a template, create the CMN. The same IP addresses will be preserved for NCNs (bootstrap_dhcp). A new network_hardware subnet will be created where the end of the previous bootstrap_dhcp subnet existed to contain switching hardware. MetalLB pools in the bootstrap_dhcp subnet will be shifted around to remain at the end of the new bootstrap subnet.\nCreate the CHN With the original CAN as a template, the new CHN network will be created. IP addresses will come from the --customer-highspeed-network \u0026lt;vlan\u0026gt; \u0026lt;ipaddress\u0026gt; (or its defaults). This is be created by default, but can be removed (if not needed or desired) by using the --retain-unused-user-network flag.\nConvert the IP addresses of the CAN Since the original/existing CAN has been converted to the new CMN, the CAN must have new IP addresses. These are provided using the --customer-access-network \u0026lt;vlan\u0026gt; \u0026lt;ipaddress\u0026gt; (or its defaults). This CAN conversion will happen by default, but the new CAN may be removed (if not needed or desired) by using the --retain-unused-user-network flag.\nAdd BGP peering information to CMN and NMN MetalLB and switches now obtain BGP peers using SLS data.\n--bgp-asn INTEGER RANGE The autonomous system number for BGP router [default: 65533;64512\u0026lt;=x\u0026lt;=65534] --bgp-cmn-asn INTEGER RANGE The autonomous system number for CMN BGP clients [default: 65534;64512\u0026lt;=x\u0026lt;=65534] --bgp-nmn-asn INTEGER RANGE The autonomous system number for NMN BGP clients [default: 65533;64512\u0026lt;=x\u0026lt;=65534] In CMN and NMN:\n\u0026#34;Type\u0026#34;: \u0026#34;ethernet\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;CIDR\u0026#34;: \u0026#34;10.102.3.0/25\u0026#34;, \u0026#34;MTU\u0026#34;: 9000, \u0026#34;MyASN\u0026#34;: 65536, \u0026#34;PeerASN\u0026#34;: 65533, \u0026#34;Subnets\u0026#34;: Remove kubeapi-vip reservations for all networks except NMN Self explanatory. This endpoint now exists only on the NMN.\nUpdate uai_macvlan in NMN ranges and uai_macvlan VLAN Self explanatory. Ranges are used for the addresses of UAIs.\nRemove unused user networks (either CAN or CHN) if desired By default the CAN will be removed if --bican-user-network-name CHN is specified, or the CHN will be removed if --bican-user-network-name CAN is specified. In order to prevent this, use the --retain-unused-user-network flag. Retention of the unused network is not normal behavior.\nGenerally production systems will NOT want to use this flag unless active toggling between CAN and CHN is required. This is not usual behavior. Test/development systems may want to have all networks for testing purposes and might want to retain both user networks. For technical details on the SLS update automation, see SLS Updater Technical Details.\nGo back to Stage 0.2 - Update SLS.\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/resource_material/storage/cephadm-reference/",
	"title": "CEPHADM",
	"tags": [],
	"description": "",
	"content": "CEPHADM cephadm is a new function introduced in Ceph Octopus 15. It allows for an easier method to install and manage Ceph nodes.\nTraditional Ceph commands On ncn-s001, ncn-s002, or ncn-s003:\nncn-s# cephadm shell The previous command creates a container and opens an interactive shell with access to run Ceph commands the traditional way.\nOr optionally, you can execute your command as follos:\nncn-s# cephadm shell -- ceph -s Ceph Device Operations There are multiple ways to do Ceph device operations now.\nUsing cephadm ncn-s# cephadm ceph-volume Using cephadm shell ncn-s# cephadm shell -- ceph-volume Optionally you can start a cephadm shell, then run ceph-volume commands from there. The following example shows doing this on ncn-s002:\nncn-s002# cephadm shell Inferring fsid 503633ce-a0ac-11ec-b2ae-b8599ff91d22 Inferring config /var/lib/ceph/503633ce-a0ac-11ec-b2ae-b8599ff91d22/mon.ncn-s002/config Using recent ceph image registry.local/ceph/ceph@sha256:4506cf7b74fd97978cb130cb7a390a9a06d6d68d48c84aa41eb516507b66009c [ceph: root@ncn-s002 /]# ceph-volume Using ceph orch ncn-s# ceph orch device ls Optionally you can specify a single node name to just list that node\u0026rsquo;s drives:\nncn-s# ceph orch device ls ncn-s002 "
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/",
	"title": "CSM 1.0.x to 1.2.x Upgrade Process",
	"tags": [],
	"description": "",
	"content": "CSM 1.0.x to 1.2.x Upgrade Process Introduction This document guides an administrator through the upgrade of Cray Systems Management from v1.0.x to v1.2.x. When upgrading a system, follow this top-level file from top to bottom. The content on this top-level page is meant to be terse. For additional reference material on the upgrade processes and scripts mentioned explicitly on this page, see resource material.\nIf upgrading from CSM 1.0.x to 1.2.0, then instead upgrade directly to the latest released version of CSM 1.2.x.\nA major feature of CSM 1.2.x is the Bifurcated CAN (BICAN). The BICAN is designed to separate administrative network traffic from user network traffic. For more information, see the BICAN Summary. Review the BICAN Summary before continuing with the CSM 1.2.x upgrade.\nFor detailed BICAN documentation, see the BICAN Technical Details page.\nImportant Notes The SMA Grafana service is temporarily inaccessible during the upgrade.\nDuring stage 3 of the CSM 1.2.x upgrade, the SMA Grafana service will become inaccessible at its previous DNS location. It will remain inaccessible until the upgrade to SMA 1.6.x is applied. This is because of a change in DNS names for the service.\nService request adjustments are needed for small systems.\nFor systems with only three worker nodes (typically Testing and Development Systems (TDS)), prior to proceeding with this upgrade, CPU limits MUST be lowered on several services in order for this upgrade to succeed. This step is executed automatically as part of Stage 0.4 of the upgrade. See TDS Lower CPU Requests for more information.\nIndependently, for three-worker systems the customizations.yaml file is edited automatically during the upgrade, prior to deploying new CSM services. These settings are contained in /usr/share/doc/csm/upgrade/1.2/scripts/upgrade/tds_cpu_requests.yaml. This file can be modified (prior to proceeding with this upgrade), if other settings are desired in the customizations.yaml file for this system.\nFor more information about modifying customizations.yaml and tuning for specific systems, see Post Install Customizations.\nKnown issues kdump (kernel dump) may hang and fail on NCNs in CSM 1.2.x (HPE Cray EX System Software 22.07 release). During the upgrade, a workaround is applied to fix this. The boot order on NCNs may not be correctly set. Because of a bug, the disk entries may be listed ahead of the PXE entries. During the upgrade, a workaround is applied to fix this. Plan and coordinate network upgrade Prior to CSM 1.2, the single Customer Access Network (CAN) carried both the administrative network traffic and the user network traffic. CSM 1.2 introduces bifurcated CAN (BICAN), which is designed to separate administrative network traffic and user network traffic.\nPlan and coordinate network upgrade shows the steps that need to be taken in order to prepare for this network upgrade. Follow these steps in order to plan and coordinate the network upgrade with your users, as well as to ensure undisrupted access to UANs during the upgrade.\nUpgrade stages Stage 0 - Prerequisites Stage 1 - Ceph Node Image Upgrade Stage 2 - Kubernetes Upgrade Stage 3 - CSM Services Upgrade Stage 4 - Ceph Upgrade Stage 5 - Perform NCN Personalization Return to Main Page and Proceed to Validate CSM Health Important: Take note of the below content for troubleshooting purposes, in the event that issues are encountered during the upgrade process.\nRelevant troubleshooting links for upgrade-related issues General upgrade troubleshooting\nIf the execution of the upgrade procedure fails, it is safe to rerun the failed script. If a rerun still fails, wait for 10 seconds and then run it again. If the issue persists, then refer to the below troubleshooting procedures.\nGeneral Kubernetes troubleshooting\nFor general Kubernetes commands for troubleshooting, see Kubernetes Troubleshooting Information.\nPXE boot troubleshooting\nIf execution of the upgrade procedures results in NCNs that have errors booting, then refer to the troubleshooting procedures in the PXE Booting Runbook.\nNTP troubleshooting\nDuring upgrades, clock skew may occur when rebooting nodes. If one node is rebooted and its clock differs significantly from those that have not been rebooted, it can cause contention among the other nodes. Waiting for chronyd to slowly adjust the clocks can resolve intermittent clock skew issues. This can take up to 15 minutes or longer. If it does not resolve on its own, then follow the Configure NTP on NCNs procedure to troubleshoot it further.\nBare-metal Etcd recovery\nDuring the upgrade process of the master nodes, if it is found that the bare-metal Etcd cluster (that houses values for the Kubernetes cluster) has a failure, it may be necessary to restore that cluster from backup. See Restore Bare-Metal etcd Clusters from an S3 Snapshot for that procedure.\nBare-metal Etcd certificate\nAfter upgrading, the apiserver-etcd-client certificate may need to been renewed. See Kubernetes and Bare Metal EtcD Certificate Renewal for procedures to check and renew this certificate.\nBack-ups for etcd-operator Clusters\nAfter upgrading, if health checks indicate that Etcd pods are not in a healthy/running state, recovery procedures may be needed. See Backups for etcd-operator Clusters for these procedures.\nRecovering from Postgres database issues\nAfter upgrading, if health checks indicate the Postgres pods are not in a healthy/running state, recovery procedures may be needed. See Troubleshoot Postgres Database for troubleshooting and recovery procedures.\nTroubleshooting Spire pods not starting on NCNs\nSee Troubleshoot Spire Failing to Start on NCNs.\nFixing shared-kafka kafka cluster after upgrade\nSee Kafka Failure after CSM 1.2 Upgrade\nTroubleshoot SLS not working\nSee SLS Not Working During Node Rebuild.\nRerun a step\nWhen running upgrade scripts, each script records what has been done successfully on a node. This is recorded in the /etc/cray/upgrade/csm/{CSM_VERSION}/{NAME_OF_NODE}/state file. If a rerun is required, the recorded steps to be re-run must be removed from this file.\nHere is an example of state file of ncn-m001:\nncn# cat /etc/cray/upgrade/csm/{CSM_VERSION}/ncn-m001/state Example output:\n[2021-07-22 20:05:27] UNTAR_CSM_TARBALL_FILE [2021-07-22 20:05:30] INSTALL_CSI [2021-07-22 20:05:30] INSTALL_WAR_DOC [2021-07-22 20:13:15] SETUP_NEXUS [2021-07-22 20:13:16] UPGRADE_BSS \u0026lt;=== Remove this line if you want to rerun this step [2021-07-22 20:16:30] CHECK_CLOUD_INIT_PREREQ [2021-07-22 20:19:17] APPLY_POD_PRIORITY [2021-07-22 20:19:38] UPDATE_BSS_CLOUD_INIT_RECORDS [2021-07-22 20:19:38] UPDATE_CRAY_DHCP_KEA_TRAFFIC_POLICY [2021-07-22 20:21:03] UPLOAD_NEW_NCN_IMAGE [2021-07-22 20:21:03] EXPORT_GLOBAL_ENV [2021-07-22 20:50:36] PREFLIGHT_CHECK [2021-07-22 20:50:38] UNINSTALL_CONMAN [2021-07-22 20:58:39] INSTALL_NEW_CONSOLE See the inline comment above on how to rerun a single step. In order to rerun the whole upgrade of a node, delete its state file. "
},
{
	"uri": "/docs-csm/en-12/update_product_stream/",
	"title": "Update CSM Product Stream",
	"tags": [],
	"description": "",
	"content": "Update CSM Product Stream The software included in the CSM product stream is released in more than one way. The initial product release may be augmented with patches, documentation updates, or hotfixes after the release.\nThe CSM documentation is included within the CSM product release tarball inside the docs-csm RPM. After the RPM has been installed, the documentation will be available at /usr/share/doc/csm.\nDownload and extract CSM product release Apply patch to CSM release Prerequisites Procedure Check for latest documentation Check for field notices about hotfixes Download and extract CSM product release Acquire a CSM software release tarball for installation on the HPE Cray EX supercomputer.\nThe following procedure should work on any Linux system. If directed here from another procedure, then that source procedure should indicate on which system the CSM release should be downloaded and extracted.\nSet the ENDPOINT variable to the URL of the server hosting the CSM tarball.\nlinux# ENDPOINT=URL_SERVER_Hosting_tarball Set the CSM_RELEASE variable to the version of CSM software to be downloaded.\nlinux# CSM_RELEASE=x.y.z Download the CSM software release tarball.\nlinux# wget ${ENDPOINT}/${CSM_RELEASE}.tar.gz Extract the release distribution.\nlinux# tar -xzvf \u0026#34;${CSM_RELEASE}.tar.gz\u0026#34; Before using this software release, check for any patches available for it.\nIf patches are available, see Apply patch to CSM release.\nApply patch to CSM release Apply a CSM update patch to the expanded CSM release tarball, and then create a new tarball which contains the patched release. This ensures that the latest CSM product artifacts are installed on the HPE Cray EX supercomputer.\nApply patch to CSM release: Prerequisites The following requirements must be met on the system where the procedure is being followed.\nThe expanded CSM release tarball is present.\nBecause the patch is applied to the expanded CSM release tarball, it is simplest to perform this procedure on the same system where the Download and extract CSM product release procedure was followed.\nGit version 2.16.5 or higher must be installed.\nlinux# git version Example output:\ngit version 2.26.2 If the Git version is less than 2.16.15, then update Git to at least that version.\nApply patch to CSM release: Procedure Set the ENDPOINT variable to the URL of the server hosting the CSM patch file.\nlinux# ENDPOINT=URL_SERVER_Hosting_patch Set the CSM_RELEASE variable to the version of CSM software to be patched.\nlinux# CSM_RELEASE=x.y.z Set the PATCH_RELEASE variable to the version of CSM patch.\nlinux# PATCH_RELEASE=x.z.a Download the compressed CSM software package patch file.\nThe file name will be of the form csm-x.y.z-x.z.a.patch.gz. Be sure to modify the following example with the appropriate values.\nlinux# wget \u0026#34;${ENDPOINT}/${CSM_RELEASE}-${PATCH_RELEASE}.patch.gz\u0026#34; Uncompress the patch.\nlinux# gunzip -v \u0026#34;${CSM_RELEASE}-${PATCH_RELEASE}.patch.gz\u0026#34; Apply the patch.\nlinux# git apply -p2 --whitespace=nowarn \\ --directory=\u0026#34;${CSM_RELEASE}\u0026#34; \\ \u0026#34;${CSM_RELEASE}-${PATCH_RELEASE}.patch\u0026#34; Set a variable to reflect the new version.\nlinux# NEW_CSM_RELEASE=\u0026#34;$(./${CSM_RELEASE}/lib/version.sh)\u0026#34; Update the name of the CSM release distribution directory.\nlinux# mv -v \u0026#34;${CSM_RELEASE}\u0026#34; \u0026#34;${NEW_CSM_RELEASE}\u0026#34; Create a tarball from the patched release distribution.\nlinux# tar -cvzf \u0026#34;${NEW_CSM_RELEASE}.tar.gz\u0026#34; \u0026#34;${NEW_CSM_RELEASE}/\u0026#34; This tarball can now be used in place of the original CSM software release tarball.\nCheck for latest documentation Acquire the latest documentation RPM. This may include updates, corrections, and enhancements that were not available until after the software release.\nCheck the version of the currently installed CSM documentation.\nlinux# rpm -q docs-csm Download and upgrade the latest documentation RPM.\nlinux# rpm -Uvh --force https://release.algol60.net/csm-1.2/docs-csm/docs-csm-latest.noarch.rpm If this machine does not have direct internet access, then this RPM will need to be externally downloaded and copied to the system. This example copies it to ncn-m001.\nlinux# wget https://release.algol60.net/csm-1.2/docs-csm/docs-csm-latest.noarch.rpm -O docs-csm-latest.noarch.rpm linux# scp docs-csm-latest.noarch.rpm ncn-m001:/root linux# ssh ncn-m001 ncn-m001# rpm -Uvh --force /root/docs-csm-latest.noarch.rpm Repeat the first step in this procedure to display the version of the CSM documentation after the update.\nCheck for field notices about hotfixes Collect all available field notices about hotfixes which should be applied to this CSM software release. Check with HPE Cray service for more information.\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/resource_material/",
	"title": "Usage",
	"tags": [],
	"description": "",
	"content": "Usage Files in this directory (and its sub-directories) are provided as reference material in support of the automated scripts which are intended to execute as much as possible during the upgrade process.\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/resource_material/k8s/worker-reference/",
	"title": "Worker-Specific Manual Steps",
	"tags": [],
	"description": "",
	"content": "Worker-Specific Manual Steps Determine if the worker being rebuilt is running a cray-cps-cm-pm pod, by running the cray cps deployment list command below. If so, there is a final step to redeploy this pod once the worker is rebuilt. In the example below, nodes ncn-w001, ncn-w002, and ncn-w003 have the pod.\nNOTE: If the command below does not return any pod names, proceed to step 2. NOTE: A 404 error is expected if the COS product is not installed on the system. In this case, proceed to step 2. NOTE: If the cray command is not initialized, see Initialize the CLI Configuration\nncn# cray cps deployment list --format json | jq \u0026#39;.[] | [.node,.podname]\u0026#39; [ \u0026#34;ncn-w003\u0026#34;, \u0026#34;cray-cps-cm-pm-9tdg5\u0026#34; ] [ \u0026#34;ncn-w001\u0026#34;, \u0026#34;cray-cps-cm-pm-fsd8w\u0026#34; ] [ \u0026#34;ncn-w002\u0026#34;, \u0026#34;cray-cps-cm-pm-sg954\u0026#34; ] If the node being rebuilt is in the output from the cray cps deployment list command above, then the cray cps deployment update command should be run after the node has been upgraded and is back online.\nDo not run this command now. It is part of the manual instructions for upgrading a worker node. This example uses ncn-w002.\nncn# cray cps deployment update --nodes \u0026#34;ncn-w002\u0026#34; Confirm the CFS configurationStatus for all worker nodes before shutting down this worker node. If the state is pending, the administrator may want to tail the logs of the CFS pod running on that node to watch the job finish before rebooting this node. If the state is failed for this node, then you will know that the failed CFS job state preceded this worker rebuild, and that can be addressed independent of rebuilding this worker.\nThis example uses ncn-w002.\nncn# export NODE=ncn-w002 ncn# export XNAME=$(ssh $NODE cat /etc/cray/xname) ncn# cray cfs components describe $XNAME --format json { \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/kubernetes/kubernetes_log_file_locations/",
	"title": "Kubernetes Log File Locations",
	"tags": [],
	"description": "",
	"content": "Kubernetes Log File Locations Locations of various K8s log types on the system.\nLog Type Component Purpose Location Kubernetes Master API server Responsible for serving the API kubectl -n kube-system logs -l component=kube-apiserver Scheduler Responsible for making scheduling decisions kubectl -n kube-system logs -l component=kube-scheduler Controller Manages replication controllers kubectl -n kube-system logs -l component=kube-controller-manager Kubernetes Worker Kubelet Responsible for running containers on the node journalctl -xeu kubelet Kube proxy Responsible for service load balancing kubectl -n kube-system logs -l k8s-app=kube-proxy "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/gigabyte_bmc_missing_redfish_data/",
	"title": "Gigabyte BMC Missing Redfish Data",
	"tags": [],
	"description": "",
	"content": "Gigabyte BMC Missing Redfish Data Follow this procedure if you notice data from Gigabyte nodes is missing from Hardware State Manager (HSM) or other CSM tools.\nIf data from Gigabyte nodes is missing from HSM or other CSM tools, check the Redfish endpoint on the BMC to see if the data is present.\nIf the data is not present in the Redfish, then a cold reset of the BMC is needed to refresh the Redfish values.\nIf the data is present in the Redfish, a rediscovery of the BMC may populate the values in HSM.\nPrerequisites Make sure the firmware and BIOS of the Gigabyte node is at the latest supported level.\nReset the Gigabyte BMC Run the command: ipmitool -I lanplus -U admin -P password -H \u0026lt;target bmc ip\u0026gt; mc reset cold\npassword is the admin password \u0026lt;target bmc ip\u0026gt; is the IP address of the BMC After 5 minutes, check to Redfish endpoint once again to verify the data is present.\nRediscover the Gigabyte BMC To rediscover the BMC run the command: cray hsm inventory discover create --xnames \u0026lt;xname of BMC\u0026gt; --force true\n\u0026lt;xname of BMC\u0026gt; is the BMC xname, example: x3000c0s1b0 "
},
{
	"uri": "/docs-csm/en-12/scripts/workarounds/boot-order/",
	"title": "Boot Order Workaround",
	"tags": [],
	"description": "",
	"content": "Boot Order Workaround This directory includes the script and the library necessary for applying the workaround to all reachable NCNs.\nUsage Run run.sh.\nncn# /usr/share/doc/csm/scripts/workarounds/boot-order/run.sh Example output:\nFailed to ping [ncn-w004]; skipping hotfix for [ncn-w004] Uploading new metal-lib.sh to ncn-m001:/srv/cray/scripts/metal/ ... Done Uploading new metal-lib.sh to ncn-m002:/srv/cray/scripts/metal/ ... Done Uploading new metal-lib.sh to ncn-m003:/srv/cray/scripts/metal/ ... Done Uploading new metal-lib.sh to ncn-s001:/srv/cray/scripts/metal/ ... Done Uploading new metal-lib.sh to ncn-s002:/srv/cray/scripts/metal/ ... Done Uploading new metal-lib.sh to ncn-s003:/srv/cray/scripts/metal/ ... Done Uploading new metal-lib.sh to ncn-w001:/srv/cray/scripts/metal/ ... Done Uploading new metal-lib.sh to ncn-w002:/srv/cray/scripts/metal/ ... Done Uploading new metal-lib.sh to ncn-w003:/srv/cray/scripts/metal/ ... Done Refreshing the bootorder on [9] NCNs ... Done The following NCNs contain the boot-order patch: ncn-m001 ncn-m002 ncn-m003 ncn-s001 ncn-s002 ncn-s003 ncn-w001 ncn-w002 ncn-w003 This workaround has completed. Origin This script originated from the metal-provision repository:\nmetal-lib.sh "
},
{
	"uri": "/docs-csm/en-12/scripts/workarounds/kdump/",
	"title": "Kernel Dump Workaround",
	"tags": [],
	"description": "",
	"content": "Kernel Dump Workaround This directory includes the script and the library necessary for applying the workaround to all reachable NCNs.\nUsage Run run.sh.\nncn# /usr/share/doc/csm/scripts/workarounds/kdump/run.sh Example output:\nUploading hotfix files to ncn-m001:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-m002:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-m003:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s001:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s002:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s003:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s004:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w001:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w002:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w003:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w004:/srv/cray/scripts/common/ ... Done Running updated create-kdump-artifacts.sh script on [11] NCNs ... Done The following NCNs contain the kdump patch: ncn-m001 ncn-m002 ncn-m003 ncn-s001 ncn-s002 ncn-s003 ncn-s004 ncn-w001 ncn-w002 ncn-w003 ncn-w004 This workaround has completed. Origin The original script and library used live in metal-provision:\ncreate-kdump-artifacts.sh dracut-lib.sh "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/bmc_ssh_key_manual_fixup/",
	"title": "Manual SSH Key Setting Process",
	"tags": [],
	"description": "",
	"content": "Manual SSH Key Setting Process If for whatever reason this script fails, SSH keys can be set manually using the following process:\nSave the public SSH key for the root user.\nncn# export SCSD_SSH_KEY=$(cat /root/.ssh/id_rsa.pub | sed \u0026#39;s/[[:space:]]*$//\u0026#39;) If a different SSH key is to be used (for example from conman) set the SCSD_SSH_KEY environment variable to that key value.\nGenerate a System Configuration Service configuration via the scsd tool. The admin must be authenticated to the Cray CLI before proceeding.\nncn# cat \u0026gt; scsd_cfg.json \u0026lt;\u0026lt;DATA { \u0026#34;Force\u0026#34;:false, \u0026#34;Targets\u0026#34;: $(cray hsm inventory redfishEndpoints list --format=json | jq \u0026#39;[.RedfishEndpoints[] | .ID]\u0026#39; | sed \u0026#39;s/^/ /\u0026#39;), \u0026#34;Params\u0026#34;:{ \u0026#34;SSHKey\u0026#34;:\u0026#34;$(echo $SCSD_SSH_KEY)\u0026#34; } } DATA Inspect the generated scsd_cfg.json file.\nEnsure the following are true before running the command below:\nThe component name (xname) list looks valid/appropriate The SSHKey settings match the desired public key ncn# cray scsd bmc loadcfg create scsd_cfg.json Check the output to verify all hardware has been set with the correct keys. Passwordless SSH to the root user should now function as expected.\nVerify access to a node controller in a liquid-cooled cabinet.\nSSH into the node controller for the host component name (xname). For example, if the host component name (xname) is x1000c1s0b0n0, the node controller component name (xname) would be x1000c1s0b0.\nIf the node controller is not powered up, this SSH attempt will fail.\nncn-w001# ssh x1000c1s0b0 x1000c1s0b0:\u0026gt; Notice that the command prompt includes the hostname for this node controller\n"
},
{
	"uri": "/docs-csm/en-12/scripts/operations/node_management/add_remove_replace_ncns/sls_utils/",
	"title": "Python Library for SLS Network Data",
	"tags": [],
	"description": "",
	"content": "Python Library for SLS Network Data This is a reusable Python library for safely interacting with SLS network data (in JSON format).\nThe library has been tested against Python version 3.6 and up.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/add_ceph_node/",
	"title": "Adding a Ceph Node to the Ceph Cluster",
	"tags": [],
	"description": "",
	"content": "Adding a Ceph Node to the Ceph Cluster NOTE: This operation can be done to add more than one node at the same time.\nAdd Join Script Copy join script from ncn-m001 to the storage node that was rebuilt or added.\nRun this command on the storage node that was rebuilt or added.\nncn-s# mkdir -pv /usr/share/doc/csm/scripts \u0026amp;\u0026amp; scp -p ncn-m001:/usr/share/doc/csm/scripts/join_ceph_cluster.sh /usr/share/doc/csm/scripts Start monitoring the Ceph health alongside the main procedure.\nIn a separate window, run the following command on ncn-s001, ncn-s002, or ncn-s003 (but not the same node that was rebuilt or added):\nncn-s# watch ceph -s Execute the script from the first step.\nRun this command on the storage node that was rebuilt or added.\nncn-s# /usr/share/doc/csm/scripts/join_ceph_cluster.sh IMPORTANT: In the output from watch ceph -s the health should go to a HEALTH_WARN state. This is expected. Most commonly you will see an alert about failed to probe daemons or devices, but this should clear on its own. In addition, it may take up to 5 minutes for the added OSDs to report as up. This is dependent on the Ceph Orchestrator performing an inventory and completing batch processing to add the OSDs.\nZapping OSDs IMPORTANT: Only do this if you are 100% certain you need to erase data from a previous install.\nNOTE: The commands in this section will need to be run from a node running ceph-mon. Typically ncn-s001, ncn-s002, or ncn-s003.\nFind the devices on the node being rebuilt.\nncn-s# ceph orch device ls $NODE Example Output:\nHostname Path Type Serial Size Health Ident Fault Available ncn-s003 /dev/sdc ssd S455NY0MB42493 1920G Unknown N/A N/A No ncn-s003 /dev/sdd ssd S455NY0MB42482 1920G Unknown N/A N/A No ncn-s003 /dev/sde ssd S455NY0MB42486 1920G Unknown N/A N/A No ncn-s003 /dev/sdf ssd S455NY0MB51808 1920G Unknown N/A N/A No ncn-s003 /dev/sdg ssd S455NY0MB42473 1920G Unknown N/A N/A No ncn-s003 /dev/sdh ssd S455NY0MB42468 1920G Unknown N/A N/A No IMPORTANT: In the above example the drives on our rebuilt node are showing Available = no. This is expected because the check is based on the presence of an LVM on the volume.\nNOTE: The ceph orch device ls $NODE command excludes the drives being used for the OS. Please double check that you are not seeing OS drives. These will have a size of 480G.\nZap the drives.\nncn-s# for drive in $(ceph orch device ls $NODE --format json-pretty |jq -r \u0026#39;.[].devices[].path\u0026#39;) ; do ceph orch device zap $NODE $drive --force done Validate that the drives are being added to the cluster.\nncn-s# watch ceph -s The OSD up and in counts should increase. If the in count increases but does not reflect the amount of drives being added back in, then fail over the ceph-mgr daemon. This is a known bug and is addressed in newer releases.\nIf you need to fail over the ceph-mgr daemon, run:\nncn-s# ceph mgr fail Regenerate Rados-GW Load Balancer Configuration for the Rebuilt Nodes IMPORTANT: radosgw by default is deployed to the first three storage nodes. This includes haproxy and keepalived. This is automated as part of the install, but the configuration may need to be regenerated if not running on the first three storage nodes or all nodes.\nDeploy Rados Gateway containers to the new nodes.\nIf running Rados Gateway on all nodes is the desired configuration, then run:\nncn-s00(1/2/3)# ceph orch apply rgw site1 zone1 --placement=\u0026#34;*\u0026#34; --port=8080 If deploying to select nodes, then instead run:\nncn-s00(1/2/3)# ceph orch apply rgw site1 zone1 --placement=\u0026#34;\u0026lt;num-daemons\u0026gt; \u0026lt;node1 node2 node3 node4 ... \u0026gt;\u0026#34; --port=8080 Verify that Rados Gateway is running on the desired nodes.\nncn-s00(1/2/3)# ceph orch ps --daemon_type rgw Example output:\nNAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID rgw.site1.zone1.ncn-s001.kvskqt ncn-s001 running (41m) 6m ago 41m 15.2.8 registry.local/ceph/ceph:v15.2.8 553b0cb212c 6e323878db46 rgw.site1.zone1.ncn-s002.tisuez ncn-s002 running (41m) 6m ago 41m 15.2.8 registry.local/ceph/ceph:v15.2.8 553b0cb212c 278830a273d3 rgw.site1.zone1.ncn-s003.nnwuqy ncn-s003 running (41m) 6m ago 41m 15.2.8 registry.local/ceph/ceph:v15.2.8 553b0cb212c a9706e6d7a69 Add nodes into HAproxy and KeepAlived. Adjust the command based on the number of storage nodes.\nIf the node was rebuilt:\nncn-s# source /srv/cray/scripts/metal/update_apparmor.sh; reconfigure-apparmor ncn-s# pdsh -w ncn-s00[1-(end node number)] -f 2 \\ \u0026#39;/srv/cray/scripts/metal/generate_haproxy_cfg.sh \u0026gt; /etc/haproxy/haproxy.cfg systemctl restart haproxy.service /srv/cray/scripts/metal/generate_keepalived_conf.sh \u0026gt; /etc/keepalived/keepalived.conf systemctl restart keepalived.service\u0026#39; If the node was added:\nDetermine the IP address of the added node.\nncn-s# cloud-init query ds | jq -r \u0026#34;.meta_data[].host_records[] | select(.aliases[]? == \\\u0026#34;$(hostname)\\\u0026#34;) | .ip\u0026#34; 2\u0026gt;/dev/null Example Output:\n10.252.1.13 Update the HAproxy configuration to include the added node. Select a storage node ncn-s00x from one of the first three storage nodes. This cannot be done from the added node.\nncn-s00x# vi /etc/haproxy/haproxy.cfg This example adds or updates ncn-s004 with the IP address 10.252.1.13 to backend rgw-backend.\n... backend rgw-backend option forwardfor balance static-rr option httpchk GET / server server-ncn-s001-rgw0 10.252.1.6:8080 check weight 100 server server-ncn-s002-rgw0 10.252.1.5:8080 check weight 100 server server-ncn-s003-rgw0 10.252.1.4:8080 check weight 100 server server-ncn-s004-rgw0 10.252.1.13:8080 check weight 100 \u0026lt;--- Added or updated line ... Copy the updated HAproxy configuration to all the storage nodes. Adjust the command based on the number of storage nodes.\nncn-s00x# pdcp -w ncn-s00[1-(end node number)] /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg Configure apparmor and KeepAlived on the added node and restart the services across all the storage nodes.\nncn-s# source /srv/cray/scripts/metal/update_apparmor.sh; reconfigure-apparmor ncn-s# /srv/cray/scripts/metal/generate_keepalived_conf.sh \u0026gt; /etc/keepalived/keepalived.conf ncn-s# export PDSH_SSH_ARGS_APPEND=\u0026#34;-o StrictHostKeyChecking=no\u0026#34; ncn-s# pdsh -w ncn-s00[1-(end node number)] -f 2 \u0026#39;systemctl restart haproxy.service; systemctl restart keepalived.service\u0026#39; Next Step If rebuilding the storage node, then proceed to Storage Node Validation. If adding the storage node, return to Boot NCN - Add storage node to the Ceph cluster for the next step. "
},
{
	"uri": "/docs-csm/en-12/operations/system_management_health/access_system_management_health_services/",
	"title": "Access System Management Health Services",
	"tags": [],
	"description": "",
	"content": "Access System Management Health Services All System Management Health services are exposed outside the cluster through the OAuth2 Proxy and Istio\u0026rsquo;s ingress gateway to enforce the authentication and authorization policies. The URLs to access these services are available on any system with CMN, BGP, MetalLB, and external DNS properly configured.\nPrerequisites System domain name System Management Health service links Prometheus Alertmanager Grafana Kiali Prerequisites Access to the System Management Health web UIs is through Istio\u0026rsquo;s ingress gateway and requires clients (browsers) to set the appropriate HTTP Host header to route traffic to the desired service. Access to these URLs may require administrative privileges on the workstation running the user\u0026rsquo;s web browser. The Customer Management Network (CMN), Border Gateway Protocol (BGP), MetalLB, and external DNS are properly configured. System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system\u0026rsquo;s fully qualified domain name (FQDN).\nThe FQDN can be found by running the following command on any Kubernetes NCN.\nncn-mw# kubectl get secret site-init -n loftsman -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d | yq r - spec.network.dns.external Example output:\nsystem.hpc.amslabs.hpecorp.net Be sure to modify the example URLs on this page by replacing SYSTEM_DOMAIN_NAME with the actual value found using the above command.\nSystem Management Health service links Access any System Management Health service with the provided links.\nWhen accessing the URLs listed below, it will be necessary to accept one or more browser security warnings in order to proceed to the login screen and navigate through the application after successfully logging in. The details of the security warning will indicate that a self-signed certificate/unknown issuer is being used for the site. Support for incorporation of certificates from Trusted Certificate Authorities is planned for a future release.\nPrometheus URL: https://prometheus.cmn.SYSTEM_DOMAIN_NAME/\nCentral Prometheus instance scrapes metrics from Kubernetes, Ceph, and the hosts (part of prometheus-operator Helm chart).\nPrometheus generates alerts based on metrics and reports them to the Alertmanager. The \u0026lsquo;Alerts\u0026rsquo; link at the top of the page will show all of the inactive, pending, and firing alerts on the system. Clicking on any of the alerts will expand them, enabling users to use the \u0026lsquo;Labels\u0026rsquo; data to discern the details of the alert. The details will also show the state of the alert, how long it has been active, and the value for the alert.\nFor more information regarding the use of the Prometheus interface, see Getting Started/ in the Prometheus online documentation.\nSome alerts may be falsely triggered. This occurs if they are alerts which will be improved in the future, or if they are alerts impacted by whether all software products have been installed yet. See Troubleshoot Prometheus Alerts.\nAlertmanager URL: https://alertmanager.cmn.SYSTEM_DOMAIN_NAME/\nCentral Alertmanager instance that manages Prometheus alerts.\nThe Alertmanager manages the alerts it receives and generates notifications to users or applications. For more information about alert-manager, see Getting Started/ in the Prometheus online documentation.\nSome alerts may be falsely triggered. This occurs if they are alerts which will be improved in the future, or if they are alerts impacted by whether all software products have been installed yet. See Troubleshoot Prometheus Alerts.\nGrafana URL: https://grafana.cmn.SYSTEM_DOMAIN_NAME/\nCentral Grafana instance that includes numerous dashboards for visualizing metrics from prometheus and prometheus-istio.\nFor more information, see the Grafana online documentation:\nFor more information about Grafana\u0026rsquo;s features and dashboard creation, see the latest Grafana online documentation. For a description of the Grafana Panel, see Grafana panels. For a description of the Grafana Dashboard, see Grafana dashboards/. Kiali URL: https://kiali-istio.cmn.SYSTEM_DOMAIN_NAME/\nKiali provides real-time introspection into the Istio service mesh using metrics and traces from Istio.\nFor more information about the features of this interface, refer to the Kiali online documentation/.\n"
},
{
	"uri": "/docs-csm/en-12/operations/system_layout_service/add_liquid-cooled_cabinets_to_sls/",
	"title": "Add Liquid-Cooled Cabinets to SLS",
	"tags": [],
	"description": "",
	"content": "Add Liquid-Cooled Cabinets to SLS This procedure adds one or more liquid-cooled cabinets and associated CDU management switches to SLS.\nNOTE: This procedure is intended to be used in conjunction with the top level Add additional Liquid-Cooled Cabinets to a System procedure.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI.\nThe latest CSM documentation is installed on the system. See Check for latest documentation.\nCollect information for the liquid-cooled cabinets being added to the system. For each cabinet collect:\nCabinet component name (xname) (for example x1004) Hardware Management Network (HMN) VLAN ID configured on the CEC (for example 3004) Node Management Network (NMN) VLAN ID configured on the CEC (for example 2004) Starting compute node ID (NID) (for example 2025) Cabinet type: Mountain (8 chassis) or Hill (2 chassis) Collect information for the CDU switches (if any) being added to the system. For each CDU management switch, collect:\nCDU switch component name (xname) (for example d1w1) CDU switch brand (for example Dell or Aruba) CDU switch alias (for example sw-cdu-004) Procedure Perform an SLS dump state operation:\nncn-mw# cray sls dumpstate list --format json \u0026gt; sls_dump.json ncn-mw# cp -v sls_dump.json sls_dump.original.json For each new liquid-cooled cabinet being added to the system, collect the following information about each cabinet:\nCabinet component name (xname) (for example x1004) Hardware Management Network (HMN) VLAN ID configured on the CEC (for example 3004) Node Management Network (NMN) VLAN ID configured on the CEC (for example 2004) Starting compute node ID (NID) (for example 2025) Cabinet type: Mountain (8 chassis) or Hill (2 chassis) The inspect_sls_cabinets.py script can be used to help display information about existing cabinets present in the system:\nncn-mw# /usr/share/doc/csm/scripts/operations/system_layout_service/inspect_sls_cabinets.py sls_dump.json Example output on a system with 1 air-cooled cabinet and 4 liquid-cooled cabinets:\n================================= Cabinet NID Allocations ================================= Cabinet | NID Ranges --------------------|--------------------- x1000 (Mountain) | 1000-1255 x1001 (Mountain) | 1256-1511 x1002 (Mountain) | 1512-1767 x1003 (Mountain) | 1768-2023 x3000 (River) | 100001-100011 ================================= Cabinet Subnet \u0026amp; VLAN Allocations ================================= Cabinet | HMN VLAN | HMN CIDR | NMN VLAN | NMN CIDR --------------------|-----------|---------------------|-----------|--------------------- x1000 (Mountain) | 3000 | 10.104.0.0/22 | 2000 | 10.100.0.0/22 x1001 (Mountain) | 3001 | 10.104.4.0/22 | 2001 | 10.100.4.0/22 x1002 (Mountain) | 3002 | 10.104.8.0/22 | 2002 | 10.100.8.0/22 x1003 (Mountain) | 3003 | 10.104.12.0/22 | 2003 | 10.100.12.0/22 x3000 (River) | 1513 | 10.107.0.0/22 | 1770 | 10.106.0.0/22 For each new liquid-cooled cabinet, add it to the previously taken SLS state dump in ascending order:\nCommand line flags for add_liquid_cooled_cabinet.py:\nArgument Description Example value --cabinet Component name (xname) of the liquid-cooled cabinet to add x1000 --cabinet-type Type of liquid-cooled cabinet to add Mountain, Hill, or EX2500 --cabinet-vlan-hmn Hardware Management Network (HMN) VLAN ID configured on the CEC 3004 --cabinet-vlan-nmn Node Management Network (NMN) VLAN ID configured on the CEC 2004 --starting-nid Starting NID for new cabinet. Each cabinet is allocated 256 NIDs 2024 ncn-mw# /usr/share/doc/csm/scripts/operations/system_layout_service/add_liquid_cooled_cabinet.py sls_dump.json \\ --cabinet x1004 \\ --cabinet-type Mountain \\ --cabinet-vlan-hmn 3004 \\ --cabinet-vlan-nmn 2004 \\ --starting-nid 2024 Example output:\n======================== Configuration ======================== SLS State File: sls_dump.json Starting NID: 2024 Cabinet: x1004 Cabinet Type: Mountain Cabinet VLAN HMN: 3004 Cabinet VLAN NMN: 2004 ======================== Network Configuration ======================== Selecting subnet for x1004 cabinet in HMN_MTN network Found existing subnet cabinet_1000 with CIDR 10.104.0.0/22 Found existing subnet cabinet_1001 with CIDR 10.104.4.0/22 Found existing subnet cabinet_1002 with CIDR 10.104.8.0/22 Found existing subnet cabinet_1003 with CIDR 10.104.12.0/22 10.104.16.0/22 Available for use. Selecting subnet for x1004 cabinet in NMN_MTN network Found existing subnet cabinet_1001 with CIDR 10.100.4.0/22 Found existing subnet cabinet_1002 with CIDR 10.100.8.0/22 Found existing subnet cabinet_1003 with CIDR 10.100.12.0/22 10.100.16.0/22 Available for use. HMN_MTN Subnet CIDR: 10.104.16.0/22 Gateway: 10.104.16.1 DHCP Start: 10.104.16.10 DHCP End: 10.104.19.254 NMN_MTN Subnet CIDR: 10.100.16.0/22 Gateway: 10.100.16.1 DHCP Start: 10.100.16.10 DHCP End: 10.100.19.254 Next available NID 2280 Writing updated SLS state to sls_dump.json Note: If adding more than one cabinet and contiguous NIDs are desired, then the value of the Next available NID 2280 can be used as the value for the --start-nid argument when adding the next cabinet.\nPossible Errors:\nProblem Error Message Resolution Duplicate Cabinet Xname Error x1000 already exists in sls_dump.json! The cabinet has already present in SLS. Ensure the new cabinet has a unique component name (xname), or the cabinet is already present in SLS. Duplicate NID values Error found duplicate NID 3000 Need to choose a different starting NID value for the cabinet that does not overlap with existing nodes. Duplicate Cabinet HMN VLAN ID: Error found duplicate VLAN 3022 with subnet cabinet_1001 in HMN_MTN Ensure that the this new cabinet has an unique HMN VLAN ID. Duplicate Cabinet NMN VLAN ID Error found duplicate VLAN 3023 with subnet cabinet_1001 in NMN_MTN Ensure that the this new cabinet has an unique NMN VLAN ID. Inspect cabinet subnet and VLAN allocations in the system after adding the new cabinets.\nncn-mw# /usr/share/doc/csm/scripts/operations/system_layout_service/inspect_sls_cabinets.py sls_dump.json Example output:\n================================= Cabinet NID Allocations ================================= Cabinet | NID Ranges --------------------|--------------------- x1000 (Mountain) | 1000-1255 x1001 (Mountain) | 1256-1511 x1002 (Mountain) | 1512-1767 x1003 (Mountain) | 1768-2023 x1004 (Mountain) | 2024-2279 x3000 (River) | 100001-100011 ================================= Cabinet Subnet \u0026amp; VLAN Allocations ================================= Cabinet | HMN VLAN | HMN CIDR | NMN VLAN | NMN CIDR --------------------|-----------|---------------------|-----------|--------------------- x1000 (Mountain) | 3000 | 10.104.0.0/22 | 2000 | 10.100.0.0/22 x1001 (Mountain) | 3001 | 10.104.4.0/22 | 2001 | 10.100.4.0/22 x1002 (Mountain) | 3002 | 10.104.8.0/22 | 2002 | 10.100.8.0/22 x1003 (Mountain) | 3003 | 10.104.12.0/22 | 2003 | 10.100.12.0/22 x1004 (Mountain) | 3004 | 10.104.16.0/22 | 2004 | 10.100.16.0/22 x3000 (River) | 1513 | 10.107.0.0/22 | 1770 | 10.106.0.0/22 For each new CDU switch being added to the system, collect the following information about it:\nCDU switch component name (xname) If within a CDU: dDwW - dD : where D is the Coolant Distribution Unit (CDU). - wW : where W is the management switch in a CDU. If within a standard rack: xXcChHsS xX : where X is the River cabinet identification number (the figure above is 3000). cC : where C is the chassis identification number. This should be 0. hH : where H is the slot number in the cabinet (height). sS : where S is the horizontal space number. CDU switch brand (for example Dell or Aruba) CDU switch alias (for example sw-cdu-004 ) For each new CDU switch, add it to the SLS state dump taken in step 1 in ascending order based on the switch alias:\nncn-mw# /usr/share/doc/csm/scripts/operations/system_layout_service/add_cdu_switch.py sls_dump.json \\ --cdu-switch d1w1 \\ --alias sw-cdu-003 \\ --brand Dell Example output:\n======================== Configuration ======================== SLS State File: sls_dump.json CDU Switch: d1w1 Brand: Dell Alias: sw-cdu-003 ================================ CDU Switch Network Configuration ================================ Selecting IP Reservation for d1w1 CDU Switch in HMN\u0026#39;s network_hardware subnet Found existing IP reservation sw-spine-001 with IP 10.254.0.2 Found existing IP reservation sw-spine-002 with IP 10.254.0.3 Found existing IP reservation sw-leaf-bmc-001 with IP 10.254.0.4 Found existing IP reservation sw-leaf-bmc-002 with IP 10.254.0.5 Found existing IP reservation sw-cdu-001 with IP 10.254.0.6 Found existing IP reservation sw-cdu-002 with IP 10.254.0.7 10.254.0.8 Available for use. Selecting IP Reservation for d1w1 CDU Switch in NMN\u0026#39;s network_hardware subnet Found existing IP reservation sw-spine-001 with IP 10.252.0.2 Found existing IP reservation sw-spine-002 with IP 10.252.0.3 Found existing IP reservation sw-leaf-bmc-001 with IP 10.252.0.4 Found existing IP reservation sw-leaf-bmc-002 with IP 10.252.0.5 Found existing IP reservation sw-cdu-001 with IP 10.252.0.6 Found existing IP reservation sw-cdu-002 with IP 10.252.0.7 10.252.0.8 Available for use. Selecting IP Reservation for d1w1 CDU Switch in MTL\u0026#39;s network_hardware subnet Found existing IP reservation sw-spine-001 with IP 10.1.0.2 Found existing IP reservation sw-spine-002 with IP 10.1.0.3 Found existing IP reservation sw-leaf-bmc-001 with IP 10.1.0.4 Found existing IP reservation sw-leaf-bmc-002 with IP 10.1.0.5 Found existing IP reservation sw-cdu-001 with IP 10.1.0.6 Found existing IP reservation sw-cdu-002 with IP 10.1.0.7 10.1.0.8 Available for use. HMN IP: 10.254.0.8 NMN IP: 10.252.0.8 MTL IP: 10.1.0.8 Writing updated SLS state to sls_dump.json Inspect the differences between the original SLS state file and the modified one.\nncn-mw# diff sls_dump.original.json sls_dump.json Perform an SLS load state operation to replace the contents of SLS with the data from the sls_dump.json file.\nGet an API token.\nncn-nw# TOKEN=$(curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Perform the load state operation.\nncn-mw# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -F sls_dump=@sls_dump.json \\ https://api-gw-service-nmn.local/apis/sls/v1/loadstate MEDS will automatically start looking for potential hardware in the newly added liquid-cooled cabinets.\nNote: No hardware in these new cabinets will be discovered until the management network has been reconfigured to support the new cabinets, and routes have been added to the management NCNs in the system.\n"
},
{
	"uri": "/docs-csm/en-12/operations/system_configuration_service/configure_bmc_and_controller_parameters_with_scsd/",
	"title": "Configure BMC and Controller Parameters with SCSD",
	"tags": [],
	"description": "",
	"content": "Configure BMC and Controller Parameters with SCSD The System Configuration Service (SCSD) allows administrators to set various BMC and controller parameters for components in liquid-cooled cabinets. These parameters are typically set during discovery, but this tool enables parameters to be set before or after discovery. The operations to change these parameters are available in the cray CLI under the scsd command.\nThe parameters which can be set are:\nSSH key NTP server Syslog server BMC/Controller passwords SSH console key IMPORTANT: If the scsd tool is used to update the SSHConsoleKey value outside of ConMan, it will disrupt the ConMan connection to the console and collection of console logs. See ConMan for more information about remote consoles and collecting console logs.\nHowever, this procedure only describes how to change the SSH key to enable passwordless SSH for troubleshooting of power down and power up logs on the node BMCs.\nSee Manage Parameters with the SCSD Service for more information about these topics for changing the other parameters:\nRetrieve current information from targets Retrieve information from a single target Set parameters for targets Set parameters for a single BMC or controller Set Redfish credentials for multiple targets Set Redfish credentials for a single target The NTP server and syslog server for BMCs in the liquid-cooled cabinet are typically set by MEDS.\nPrerequisites The latest CSM documentation is installed on the system. See Check for Latest Documentation. Details Setting the SSH keys for Mountain controllers is done by running the /usr/share/doc/csm/scripts/operations/configuration/set_ssh_keys.py script:\nNOTE: The set_ssh_keys.py located in /opt/cray/csm/scripts/admin_access contains a bug and should not be used.\nUsage: set_ssh_keys.py [options] --debug=level Set debug level --dryrun Gather all info but do not set anything in HW. --exclude=list Comma-separated list of target patterns to exclude. Each item in the list is matched on the front of each target component name (xname) and excluded if there is a match. Example: x1000,x3000c0,x9000c1s0 This will exclude all BMCs in cabinet x1000, all BMCs at or below x3000c0, and all BMCs below x9000c1s0. NOTE: --include and --exclude are mutually exclusive. --include=list Comma-separated list of target patterns to include. Each item in the list is matched on the front of each target component name (xname) and included is there is a match. NOTE: --include and --exclude are mutually exclusive. --sshkey=key SSH key to set on BMCs. If none is specified, will use If no command line arguments are needed, SSH keys are set on all discovered Mountain controllers using the root account\u0026rsquo;s public RSA key. Using an alternate key requires the --sshkey=key argument:\n# /usr/share/doc/csm/scripts/operations/configuration/set_ssh_keys.py` --sshkey=\u0026#34;AAAbbCcDddd....\u0026#34; After the script runs, verify that it worked:\nTest access to a node controller in the liquid-cooled cabinet.\nSSH into the node controller for the host component name (xname). For example, if the host component name (xname) is x1000c1s0b0n0, the node controller component name (xname) would be x1000c1s0b0.\nIf the node controller is not powered up, this SSH attempt will fail.\nncn-w001# ssh x1000c1s0b0 Notice that the command prompt (x1000c1s0b0:\u0026gt;) includes the hostname for this node controller.\nThe logs from power actions for node 0 and node 1 on this node controller are in /var/log.\nx1000c1s0b0# cd /var/log x1000c1s0b0# ls -l powerfault_* Expected output looks similar to the following:\n-rw-r--r-- 1 root root 306 May 10 15:32 powerfault_dn.Node0 -rw-r--r-- 1 root root 306 May 10 15:32 powerfault_dn.Node1 -rw-r--r-- 1 root root 5781 May 10 15:36 powerfault_up.Node0 -rw-r--r-- 1 root root 5781 May 10 15:36 powerfault_up.Node1 Debugging if script fails If this script does not achieve the goal of setting SSH keys, then check the following:\nMake sure the SSH key is correct. If --exclude= or --include= was used with the script, ensure the correct component names (xnames) were specified. Re-run the script with --debug=3 for verbose debugging output. Look for things like missing BMCs, bad authentication token, or bad communications with BMCs. If this script fails, then the SSH keys can be set manually using the Manual SSH Key Setting Process.\n"
},
{
	"uri": "/docs-csm/en-12/operations/spire/create_a_backup_of_the_spire_postgres_database/",
	"title": "Create a Backup of the Spire Postgres Database",
	"tags": [],
	"description": "",
	"content": "Create a Backup of the Spire Postgres Database Perform a manual backup of the contents of the Spire Postgres database. This backup can be used to restore the contents of the Spire Postgres database at a later point in time using the Restore Spire Postgres from Backup procedure.\nPrerequisites Healthy Spire Postgres Cluster.\nUse patronictl list on the Spire Postgres cluster to determine the current state of the cluster and note which member is the Leader. A healthy cluster will look similar to the following:\nncn-mw# kubectl exec spire-postgres-0 -n spire -c postgres -it -- patronictl list Example output:\n+ Cluster: spire-postgres (7062403223429402699) -----+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +------------------+--------------+--------+---------+----+-----------+ | spire-postgres-0 | 10.44.43.64 | | running | 12 | 0 | | spire-postgres-1 | 10.33.92.221 | Leader | running | 12 | | | spire-postgres-2 | 10.32.55.219 | | running | 12 | 0 | +------------------+--------------+--------+---------+----+-----------+ Healthy Spire Service.\nVerify all 3 Spire replicas are up and running:\nncn-mw# kubectl -n spire get pods -l application=spilo,cluster-name=spire-postgres Example output:\nNAME READY STATUS RESTARTS AGE spire-postgres-0 3/3 Running 0 11d spire-postgres-1 3/3 Running 0 11d spire-postgres-2 3/3 Running 0 11d Procedure Set the Spire variables including the Leader which for this case is the member spire-postgres-1.\nncn-mw# CLIENT=spire-server ncn-mw# POSTGRESQL=spire-postgres ncn-mw# NAMESPACE=spire ncn-mw# POSTGRES_LEADER=spire-postgres-1 Scale the client service down.\nncn-mw# kubectl scale statefulset ${CLIENT} -n ${NAMESPACE} --replicas=0 # Wait for the pods to terminate ncn-mw# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/instance=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2 done Create a dump of the Spire Postgres database.\nncn-mw# kubectl exec -it ${POSTGRES_LEADER} -n ${NAMESPACE} -c postgres -- pg_dumpall -c -U postgres \u0026gt; \u0026#34;${POSTGRESQL}-dumpall.sql\u0026#34; Copy the ${POSTGRESQL}-dumpall.sql file off of the cluster, and store it in a secure location.\nScale the client service back up.\nncn-mw# kubectl scale statefulset ${CLIENT} -n ${NAMESPACE} --replicas=3 "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/api_authorization/",
	"title": "API Authorization",
	"tags": [],
	"description": "",
	"content": "API Authorization Authorization for REST API calls is only done at the API gateway. This is facilitated through policy checks to the Open Policy Agent (OPA). Every REST API call into the system is sent to the OPA to make an authorization decision. The decision is based on the authenticated JSON Web Token (JWT) passed into the request.\nThis page lists the available personas and the supported REST API endpoints for each.\nadmin user system-pxe system-compute wlm admin Authorized for every possible REST API endpoint.\nuser Authorized for a subset of endpoints to allow users to create and use User Access Instances (UAIs), run jobs, view job results, and use capsules.\nuser UAS endpoints user PALS endpoints user Replicant endpoints user Analytics Capsules endpoints user UAS endpoints REST API endpoints for the user persona for the User Access Service (UAS):\nMethod Endpoint Description GET /apis/uas-mgr/v1/ Get UAS API version GET /apis/uas-mgr/v1/uas List UAIs for current user POST /apis/uas-mgr/v1/uas Create a UAI for current user DELETE /apis/uas-mgr/v1/uas Delete UAIs for current user GET /apis/uas-mgr/v1/images List available UAI images GET /apis/uas-mgr/v1/mgr-info Get UAS service version user PALS endpoints The user persona is authorized to make DELETE, GET, HEAD, PATCH, POST or PUT calls to any Parallel Application Launch Service (PALS) endpoint (/apis/pals/v1/*).\nuser Replicant endpoints REST API endpoints for the user persona for Replicant:\nMethod Endpoint Description GET /apis/rm/v1/report/\u0026lt;id\u0026gt; Get report by ID GET /apis/rm/v1/reports Get reports user Analytics Capsules endpoints The user persona is authorized to make DELETE, GET, HEAD, PATCH, POST or PUT calls to any Analytics Capsules endpoint (/apis/capsules/*).\nsystem-pxe Authorized for endpoints related to booting.\nThe system-pxe persona is authorized to make GET, HEAD, or POST calls to any Boot Script Service (BSS) endpoint (/apis/bss/*).\nsystem-compute Authorized for endpoints required by the Cray Operating System (COS) to manage compute nodes and NCN services.\nThe system-compute persona is authorized to make:\nGET, HEAD, or PATCH calls to any Configuration Framework Service (CFS) endpoint (/apis/cfs/*). GET, HEAD, or POST calls to any Content Projection Service (CPS) endpoint (/apis/v2/cps/*). GET, HEAD, or POST calls to any Heartbeat Tracker Daemon (HBTD) endpoint (/apis/hbtd/*). GET, HEAD, POST, or PUT calls to any Node Memory Dump (NMD) endpoint (/apis/v2/nmd/*). GET or HEAD calls to any Hardware State Manager (HSM) endpoint (/apis/smd/*). DELETE, GET, HEAD, PATCH, or POST calls to any Hardware Management Notification Fanout Daemon (HMNFD) endpoint (apis/hmnfd/*). wlm Authorized for endpoints related to the use of the Slurm or PBS workload managers.\nThe wlm persona is authorized to make:\nDELETE, GET, HEAD, or POST calls to any PALS endpoint (/apis/pals/*). GET, HEAD, or POST calls to any Cray Advanced Platform Monitoring and Control (CAPMC) endpoint (/apis/capmc/*). DELETE, GET, HEAD, PATCH, or POST calls to any Boot Orchestration Service (BOS) endpoint (/apis/bos/*). GET or HEAD calls to any System Layout Service (SLS) endpoint (/apis/sls/*). GET or HEAD calls to any HSM endpoint (/apis/smd/*). DELETE, GET, HEAD, PATCH, POST or PUT calls to any Virtual Network Identifier Daemon (VNID) endpoint (/apis/vnid/*). "
},
{
	"uri": "/docs-csm/en-12/operations/preinstall/change_river_bmc_credentials/",
	"title": "Fresh Install Setting NodeBMC and RouterBMC Redfish Credentials",
	"tags": [],
	"description": "",
	"content": "Fresh Install: Setting NodeBMC and RouterBMC Redfish Credentials These steps are performed before the installation of Shasta System Management or HPCM management software stacks. The goal is to set the BMC Redfish credentials to values that the management software will be expecting so that all software systems work smoothly with the Redfish hardware.\nPrerequisites Before doing these operations, the following is assumed:\nThere is a workstation or laptop which can access all target BMCs. Workstation or laptop has the curl command installed. The hostname or IP address of each BMC is known or obtainable. The default BMC password is obtainable for each target BMC. Set BMC Passwords on All Air-Cooled BMC Hardware This involves interaction with the BMC hardware itself.\nThe BMC factory-default passwords are found on a sticker or card associated with the blades themselves. Some blade enclosures also have a chassis-level management controller (CMC) which may also have its own default password, which would need to be changed as well.\nThe procedure for each blade is to obtain the factory-default password for each blade\u0026rsquo;s BMC, and then use curl to set the root BMC account password to the desired password (which must match the one set in customizations.yaml).\nEach Redfish BMC will have at least one \u0026ldquo;account\u0026rdquo;, and often several accounts. Each account has an ordinal number, and only one of the accounts is the root account. This account is the one that must have its password changed.\nProcedure Use the following procedure for each BMC:\nGet the default BMC password and the hostname or IP address of the BMC.\nDetermine which Redfish account is the root account:\nlinux# curl -s -k -u root:\u0026lt;DFLTPW\u0026gt; https://\u0026lt;BLADENAME_OR_IP\u0026gt;/redfish/v1/AccountSystem/Accounts | jq Example output:\n{ \u0026#34;@odata.context\u0026#34;: \u0026#34;/redfish/v1/$metadata#ManagerAccountCollection.ManagerAccountCollection\u0026#34;, \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts\u0026#34;, \u0026#34;@odata.type\u0026#34;: \u0026#34;#ManagerAccountCollection.ManagerAccountCollection\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;Accounts Collection\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;BMC User Accounts\u0026#34;, \u0026#34;Members@odata.count\u0026#34;: 4, \u0026#34;Members\u0026#34;: [ { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/1\u0026#34; }, { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/2\u0026#34; }, { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/3\u0026#34; }, { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/4\u0026#34; } ] } For each account listed, use curl to find the one which describes the root account (\u0026ldquo;UserName\u0026rdquo;: \u0026ldquo;root\u0026rdquo;).\nNOTES:\nThe root account can be any of the listed accounts \u0026ndash; no guarantees as to which one it will be. If the account information contains an etag entry, note this number, as it will be required when setting the password. linux# curl -s -k -u root:\u0026lt;DFLTPW\u0026gt; https://\u0026lt;BLADENAME_OR_IP\u0026gt;/redfish/v1/AccountSystem/Accounts/1 | jq Example output:\n{ \u0026#34;@odata.context\u0026#34;: \u0026#34;/redfish/v1/$metadata#ManagerAccount.ManagerAccount\u0026#34;, \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/1\u0026#34;, \u0026#34;@odata.type\u0026#34;: \u0026#34;#ManagerAccount.v1_1_1.ManagerAccount\u0026#34;, \u0026#34;@odata.etag\u0026#34;: \u0026#34;W/\\\u0026#34;570254F2\\\u0026#34;\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;User Account\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;User Account\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;Password\u0026#34;: null, \u0026#34;UserName\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;RoleId\u0026#34;: \u0026#34;NoAccess\u0026#34;, \u0026#34;Links\u0026#34;: { \u0026#34;Role\u0026#34;: { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Roles/NoAccess\u0026#34; } } } Set the new password for this account. Use the ETAG in the header if needed.\nlinux# curl -s -k -u root:\u0026lt;DFLTPW\u0026gt; -H \u0026#34;If-None-Match: 570254F2\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -X PATCH -d \u0026#39;{\u0026#34;Password\u0026#34;:\u0026#34;\u0026lt;NEWPW\u0026gt;\u0026#34;}\u0026#39; https://\u0026lt;BLADENAME_OR_IP\u0026gt;/redfish/v1/AccountSystem/Accounts/1 linux# Test to be sure the new password works. If the password set operation did not work, then this will fail.\nlinux# curl -s -k -u root:\u0026lt;NEWPW\u0026gt; https://\u0026lt;BLADENAME_OR_IP\u0026gt;/redfish/v1/AccountSystem Example output:\n{ \u0026#34;@odata.context\u0026#34;: \u0026#34;/redfish/v1/$metadata#ManagerAccount.ManagerAccount\u0026#34;, \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/1\u0026#34;, \u0026#34;@odata.type\u0026#34;: \u0026#34;#ManagerAccount.v1_1_1.ManagerAccount\u0026#34;, \u0026#34;@odata.etag\u0026#34;: \u0026#34;W/\\\u0026#34;570254F2\\\u0026#34;\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;User Account\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;User Account\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;Password\u0026#34;: null, \u0026#34;UserName\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;RoleId\u0026#34;: \u0026#34;NoAccess\u0026#34;, \u0026#34;Links\u0026#34;: { \u0026#34;Role\u0026#34;: { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Roles/NoAccess\u0026#34; } } } Set Default Credentials for High-Speed Network (HSN) Switch BMCs This is the exact same procedure as for node BMCs, except that the source of the default BMC passwords is different.\nThe default passwords for all air-cooled high-speed network switch BMCs is a known factory default and is outside the scope of this document.\nOnce this password, and any HSN BMC IP addresses or hostnames are obtained, the procedure above can be used to set the Redfish root account passwords to the new value.\nNOTE: The default credentials for both air-cooled and liquid-cooled high-speed network switch BMCs should be identical.\n"
},
{
	"uri": "/docs-csm/en-12/operations/resiliency/ntp_resiliency/",
	"title": "NTP Resiliency",
	"tags": [],
	"description": "",
	"content": "NTP Resiliency Synchronize the time on all non-compute nodes (NCNs) via Network Time Protocol (NTP). Avoid a single point of failure for NTP when testing system resiliency.\nPrerequisites This procedure requires administrative privileges.\nProcedure Set the date manually if the time on NCNs is off by more than an a few hours.\nFor example:\nncn-m001# timedatectl set-time \u0026#34;2021-02-19 15:04:00\u0026#34; Configure NTP on the Pre-install Toolkit (PIT).\nncn-m001# /root/bin/configure-ntp.sh Sync NTP on all other nodes.\nIf more than nine NCNs are in use on the system, update the loop in the following command accordingly.\nncn-m002# for i in ncn-{w,s}00{1..3} ncn-m00{2..3}; do echo \u0026#34;------$i--------\u0026#34;; ssh $i \u0026#39;/srv/cray/scripts/common/chrony/csm_ntp.py\u0026#39;; done "
},
{
	"uri": "/docs-csm/en-12/operations/power_management/cray_advanced_platform_monitoring_and_control_capmc/",
	"title": "Cray Advanced Platform Monitoring and Control (CAPMC)",
	"tags": [],
	"description": "",
	"content": "Cray Advanced Platform Monitoring and Control (CAPMC) The Cray Advanced Platform Monitoring and Control (CAPMC) service enables direct hardware control of nodes, compute blades, router modules, and liquid cooled chassis. CAPMC talks to BMCs via Redfish to control power, query status, and manage power capping on target components. These controls enable an administrator and 3rd party software to more intelligently manage state and system-wide power consumption.\nAdministrators can use the cray CLI for power operations from any system that has HTTPS access to the System Management Services.\nThe cray capmc command (see --help) can be used to control power to specific components by specifying the component NID, xname, or group.\nPower control and query by xname Controllable components Air-cooled cabinets Liquid-cooled cabinets Naming convention Examples of valid xnames Power capping Deprecated interfaces Power control and query by NID Power control and query by group Node energy System monitor Others Power control and query by xname xname_on xname_off xname_reinit get_xname_status CAPMC power control assumes that all cabinets and PDUs have been plugged in, breakers are on, and PDU controllers, BMCs, and other embedded controller are on, available, and have been discovered. Components have their power controlled in a pre-defined order to properly handle requests of dependent components.\nImportant: It is recommended to use the Boot Orchestration Service (BOS) to boot (power On), shutdown, and reboot compute nodes.\nControllable components Air-Cooled cabinets Compute Nodes NCNs Liquid-cooled cabinets Chassis Slingshot Switch modules Compute blades Compute nodes Naming convention CAPMC uses xnames to specify entire cabinets or specific components throughout the system. By default, CAPMC controls power to only one component at a time. A --recursive true option can be passed to CAPMC using the cray CLI. When the --recursive true option is included in a request, all of the sub-components of the target component are included in the power command.\nBy the cabinet naming convention, each cabinet in the system is assigned a unique number. Cabinet numbers can range from 0-9999 and contain from 1-4 digits only.\nAlthough manufacturing typically follows a sequential cabinet numbering scheme:\nLiquid-cooled cabinet numbers: x1000–x2999 Air-cooled cabinet numbers: x3000–x4999 Liquid-cooled TDS cabinet numbers: x5000–x5999 Examples of valid xnames Full system: s0, all Cabinet numbers: x1000, x3000, x5000 Chassis numbers 0-7: x1000c7, x3500c0 (Air-cooled cabinets are always chassis 0) Compute blade slots 0-7: x1000c7s3, x3500c0s15 (U15) Compute nodes: x1000c7s3b0n0, x3500c0s15b1n0 NCN slots: x3200c0s9 (U9) NCNs: x3200c0s9b0n0 Power capping get_power_cap_capabilities get_power_cap set_power_cap CAPMC is capable of setting node power limits on all supported compute node hardware in both liquid cooled cabinets and air cooled cabinets. This functionality enables external software to establish an upper bound, or estimate a minimum bound, on the amount of power a system may consume. Separate CAPMC calls are required to power cap different compute node types as each compute node type has its own power capping capabilities.\nNOTE: Power capping is not supported for liquid cooled chassis, switch modules, compute blades, and any non-compute nodes (NCNs) in air cooled cabinets.\nDeprecated interfaces See the CAPMC Deprecation Notice for more information\nPower control and query by NID Use the interfaces from Power control and query by xname:\nnode_on node_off node_reinit get_node_status Power control and query by group Use the interfaces from Power control and query by xname:\ngroup_on group_off group_reinit get_group_status Node energy Use the System Monitoring Application (SMA) Grafana instance:\nget_node_energy get_node_energy_stats get_node_energy_counters System monitor Use the System Monitoring Application (SMA) Grafana instance:\nget_system_parameters get_system_power get_system_power_details Others get_node_rules emergency_power_off get_nid_map "
},
{
	"uri": "/docs-csm/en-12/operations/package_repository_management/manage_repositories_with_nexus/",
	"title": "Manage Repositories with Nexus",
	"tags": [],
	"description": "",
	"content": "Manage Repositories with Nexus This section describes how to connect to Nexus with the Web UI, as well as how to access the REST API from non-compute nodes (NCNs) or compute nodes to manage repositories.\nSystem domain name Access Nexus with the web UI Use Keycloak to create and manage accounts Use the local Nexus admin account Access Nexus with the REST API Pagination Check the status of Nexus List repositories List assets Create a repository Update a repository Delete a repository Create a blob store Delete a blob store Authenticate to access the Rest API System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system\u0026rsquo;s fully qualified domain name (FQDN).\nThe FQDN can be found by running the following command on any Kubernetes NCN.\nncn-mw# kubectl get secret site-init -n loftsman -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d | yq r - spec.network.dns.external Example output:\nsystem.hpc.amslabs.hpecorp.net Be sure to modify the example URLs on this page by replacing SYSTEM_DOMAIN_NAME with the actual value found using the above command.\nAccess Nexus with the web UI Nexus is accessible using a web browser at the following URL: https://nexus.cmn.SYSTEM_DOMAIN_NAME\nUse Keycloak to create and manage accounts In order to log into the Web UI or authenticate with the REST API, a user account with appropriate permissions must be created. Accounts are managed in Keycloak (see Configure Keycloak Accounts). To add administrator permissions for Nexus, add the nx-admin role binding to the user from the system-nexus-client client (see below). To add an anonymous user, add the nx-anonymous role binding to the user from the system-nexus-client client (see below).\nUse the local Nexus admin account During the deployment or update of Nexus, a local admin account is created. To access the local admin account for Nexus on any Kubernetes NCN, run the following commands:\nncn-mw# kubectl -n nexus get secret nexus-admin-credential --template {{.data.username}} | base64 -d; echo ncn-mw# kubectl -n nexus get secret nexus-admin-credential --template {{.data.password}} | base64 -d; echo The first command will print the username of the local admin account. The second command will print the password for the local admin account. (Note that the secret will not update or stay in sync if the username or password of the local account is changed). This account has the same permissions as an account created in Keycloak with the nx-admin role.\nAccess Nexus with the REST API The Nexus REST API is available from NCNs or compute nodes at https://packages.local/service/rest, as well as over the Customer Access Network (CAN) at https://nexus.cmn.SYSTEM_DOMAIN_NAME/service/rest (requires authentication with username and password).\nDownload the Open API document at /service/rest/swagger.json for details about the API, including specific options to available endpoints. By default, the REST API endpoints return (or accept) JSON.\nThe examples in the following sections use curl to exercise the REST API endpoints and jq to parse and manipulate the output. It is reasonable to use curl and jq to facilitate management tasks when necessary, but more complex actions may warrant development of more full-featured tools.\nThe following actions are described in this section:\nPagination Check the status of Nexus List repositories List assets Create a repository Update a repository Delete a repository Create a blob store Delete a blob store Authenticate to access the Rest API Pagination Various API endpoints use the external pagination tool to return results. When a continuationToken is included in the results and is non-null, it indicates additional items are available.\nThe following is some example output:\n{ \u0026#34;items\u0026#34;: [ \u0026#34;...\u0026#34; ], \u0026#34;continuationToken\u0026#34;: \u0026#34;0a1b9d05d7162aa85d7747eaa75f171c\u0026#34; } In this example, the next set of results may be obtained by re-requesting the same URL with the added query parameter continuationToken=0a1b9d05d7162aa85d7747eaa75f171c.\nVarious examples in the following sections may use the paginate helper function to iterate over paginated results:\nfunction paginate() { local url=\u0026#34;$1\u0026#34; local token { token=\u0026#34;$(curl -sSk \u0026#34;$url\u0026#34; | tee /dev/fd/3 | jq -r \u0026#39;.continuationToken // null\u0026#39;)\u0026#34;; } 3\u0026gt;\u0026amp;1 until [[ \u0026#34;$token\u0026#34; == \u0026#34;null\u0026#34; ]]; do { token=\u0026#34;$(curl -sSk \u0026#34;$url\u0026amp;continuationToken=${token}\u0026#34; | tee /dev/fd/3 | jq -r \u0026#39;.continuationToken // null\u0026#39;)\u0026#34;; } 3\u0026gt;\u0026amp;1 done } Check the status of Nexus Send an HTTP GET request to /service/rest/v1/status to check the operating status of Nexus. An HTTP 200 OK response indicates it is healthy:\nncn-mw# curl -sSi https://packages.local/service/rest/v1/status Example output:\nHTTP/2 200 date: Sat, 06 Mar 202117:27:56 GMT server: istio-envoy x-content-type-options: nosniff content-length: 0 x-envoy-upstream-service-time: 6 Before attempting to write to Nexus, it is recommended to check that Nexus is writable by sending an HTTP GET request to /service/rest/v1/status/writable:\nncn-mw# curl -sSi https://packages.local/service/rest/v1/status/writable Example output:\nHTTP/2 200 date: Sat, 06 Mar 202117:28:34 GMT server: istio-envoy x-content-type-options: nosniff content-length: 0 x-envoy-upstream-service-time: 6 List repositories Use the /service/rest/v1/repositories endpoint to get a basic listing of available repositories:\nncn-mw# curl -sSk https://packages.local/service/rest/v1/repositories | jq -r \u0026#39;.[] | .name\u0026#39; The /service/rest/beta/repositories endpoint provides a more detailed listing of available repositories.\nFor example, the following command queries for information about the csm-sle-15sp2 repository:\nncn-mw# curl -sSk https://packages.local/service/rest/beta/repositories | jq -r \u0026#39;.[] | select(.name == \u0026#34;csm-sle-15sp2\u0026#34;)\u0026#39; Example output:\n{ \u0026#34;name\u0026#34;: \u0026#34;csm-sle-15sp2\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://packages.local/repository/csm-sle-15sp2\u0026#34;, \u0026#34;online\u0026#34;: true, \u0026#34;storage\u0026#34;: { \u0026#34;blobStoreName\u0026#34;: \u0026#34;csm\u0026#34;, \u0026#34;strictContentTypeValidation\u0026#34;: false }, \u0026#34;group\u0026#34;: { \u0026#34;memberNames\u0026#34;: [ \u0026#34;csm-0.8.0-sle-15sp2\u0026#34; ] }, \u0026#34;type\u0026#34;: \u0026#34;group\u0026#34; } Neither the v1 or beta/repositories endpoints are paginated.\nList assets Use the /service/rest/v1/components endpoint to list the assets in a specific repository (REPO_NAME). The /service/rest/v1/components endpoint is paginated.\nncn-mw# paginate \u0026#39;https://packages.local/service/rest/v1/components?repository=REPO_NAME\u0026#39; | jq -r \u0026#39;.items[] | .name\u0026#39; For example, to list the names of all components in the csm-sle-15sp2 repository:\nncn-mw# paginate \u0026#34;https://packages.local/service/rest/v1/components?repository=csm-sle-15sp2\u0026#34; | jq -r \u0026#39;.items[] | .name\u0026#39; | sort -u Example output:\nnoarch/basecamp-1.0.1-20210126131805_a665272.noarch.rpm noarch/csm-testing-1.3.2-20210205160852_e012960.noarch.rpm noarch/docs-csm-1.7.4-20210206165423_2fae6fa.noarch.rpm noarch/dracut-metal-dmk8s-1.4.7-20210129115153_7a86571.noarch.rpm noarch/dracut-metal-luksetcd-1.0.2-20210129115153_b34f9a5.noarch.rpm noarch/dracut-metal-mdsquash-1.4.20-20210201222655_e20e2ee.noarch.rpm noarch/goss-servers-1.3.2-20210205160852_e012960.noarch.rpm noarch/hpe-csm-goss-package-0.3.13-20210127124704_aae8d77.noarch.rpm noarch/hpe-csm-scripts-0.0.4-20210125173103_a527e49.noarch.rpm noarch/hpe-csm-yq-package-3.4.1-20210127134802_789be45.noarch.rpm noarch/metal-ipxe-1.4.33-20210127152038_ef91cc8.noarch.rpm noarch/metal-net-scripts-0.0.1-20210204114016_95ab47a.noarch.rpm noarch/nexus-0.5.2-1.20210115090713_aef3950.noarch.rpm noarch/platform-utils-0.1.2-20210115162116_1139af5.noarch.rpm noarch/platform-utils-0.1.5-20210203170424_ca869e9.noarch.rpm repodata/2aadc798a4f7e12e99be79e0faa8bb2c2fe05871295edda8a4045fd371e7a568-primary.xml.gz repodata/452f91a378fa64c52534c984b90cf492e546334732c5b940b8fe5cfe2aebde29-filelists.sqlite.bz2 repodata/8dbbe7d1fceb13ccbae981aa9abe8575004df7bb3c0a74669502b5ea53a5455c-other.xml.gz repodata/a4e95cc8a79f42b150d6c505c3f8e6bf242ee69de7849a2973dd19e0c1d8f07a-filelists.xml.gz repodata/d3a16a9bceebf92fd640d689a8c015984d2963e4c11d7a841ec9b24cc135e99a-primary.sqlite.bz2 repodata/e9e8163a7c956f38eb37d6af3f1ac1bdae8079035843c9cd22ced9e824498da0-other.sqlite.bz2 repodata/repomd.xml x86_64/cfs-state-reporter-1.4.4-20201204120230_c198848.x86_64.rpm x86_64/cfs-state-reporter-1.4.6-20210128142236_6bb340b.x86_64.rpm x86_64/cfs-trust-1.0.2-20201216135115_58f3d86.x86_64.rpm x86_64/cfs-trust-1.0.3-20210125135157_2a234cb.x86_64.rpm ... Each component item has the following structure:\n{ \u0026#34;id\u0026#34;: \u0026#34;Y3NtLXNsZS0xNXNwMjowYTFiOWQwNWQ3MTYyYWE4NWQ3NzQ3ZWFhNzVmMTcxYw\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;csm-sle-15sp2\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;group\u0026#34;: \u0026#34;/noarch\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;noarch/csm-testing-1.3.2-20210205160852_e012960.noarch.rpm\u0026#34;, \u0026#34;version\u0026#34;: null, \u0026#34;assets\u0026#34;: [ { \u0026#34;downloadUrl\u0026#34;: \u0026#34;https://packages.local/repository/csm-sle-15sp2/noarch/csm-testing-1.3.2-20210205160852_e012960.noarch.rpm\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;noarch/csm-testing-1.3.2-20210205160852_e012960.noarch.rpm\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;Y3NtLXNsZS0xNXNwMjpiZDdmNzllMTk2NzMwNTA4NjQ1OTczNzQwYTMwZTRjMg\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;csm-sle-15sp2\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;checksum\u0026#34;: { \u0026#34;sha1\u0026#34;: \u0026#34;daecc7f20e1ddd5dd50b8b40351203882e2ad1c4\u0026#34;, \u0026#34;sha512\u0026#34;: \u0026#34;5343a189a7fb10bd43033f6b36e13cb85d75e705de2fab63a18c7cda4e3e57233ee3bfe55450e497aa0fbbdf2f2d024fb2ef2c3081e529a0bde9fa843d06a288\u0026#34;, \u0026#34;sha256\u0026#34;: \u0026#34;f7f779126031bcbc266c81d5f1546852aee0fb08890b7fba07b6fafd23e79d3b\u0026#34;, \u0026#34;md5\u0026#34;: \u0026#34;2a600edec22b34cbf5886db725389ed0\u0026#34; } } ] } For example, to list the download URLs for each asset in the csm-sle-15sp2 repository:\nncn-mw# paginate \u0026#34;https://packages.local/service/rest/v1/components?repository=csm-sle-15sp2\u0026#34; | jq -r \u0026#39;.items[] | .assets[] | .downloadUrl\u0026#39; | sort -u Example output:\nhttps://packages.local/repository/csm-sle-15sp2/noarch/basecamp-1.0.1-20210126131805_a665272.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/csm-testing-1.3.2-20210205160852_e012960.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/docs-csm-1.7.4-20210206165423_2fae6fa.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/dracut-metal-dmk8s-1.4.7-20210129115153_7a86571.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/dracut-metal-luksetcd-1.0.2-20210129115153_b34f9a5.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/dracut-metal-mdsquash-1.4.20-20210201222655_e20e2ee.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/goss-servers-1.3.2-20210205160852_e012960.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/hpe-csm-goss-package-0.3.13-20210127124704_aae8d77.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/hpe-csm-scripts-0.0.4-20210125173103_a527e49.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/hpe-csm-yq-package-3.4.1-20210127134802_789be45.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/metal-ipxe-1.4.33-20210127152038_ef91cc8.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/metal-net-scripts-0.0.1-20210204114016_95ab47a.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/nexus-0.5.2-1.20210115090713_aef3950.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/platform-utils-0.1.2-20210115162116_1139af5.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/platform-utils-0.1.5-20210203170424_ca869e9.noarch.rpm https://packages.local/repository/csm-sle-15sp2/repodata/2aadc798a4f7e12e99be79e0faa8bb2c2fe05871295edda8a4045fd371e7a568-primary.xml.gz https://packages.local/repository/csm-sle-15sp2/repodata/452f91a378fa64c52534c984b90cf492e546334732c5b940b8fe5cfe2aebde29-filelists.sqlite.bz2 https://packages.local/repository/csm-sle-15sp2/repodata/8dbbe7d1fceb13ccbae981aa9abe8575004df7bb3c0a74669502b5ea53a5455c-other.xml.gz https://packages.local/repository/csm-sle-15sp2/repodata/a4e95cc8a79f42b150d6c505c3f8e6bf242ee69de7849a2973dd19e0c1d8f07a-filelists.xml.gz https://packages.local/repository/csm-sle-15sp2/repodata/d3a16a9bceebf92fd640d689a8c015984d2963e4c11d7a841ec9b24cc135e99a-primary.sqlite.bz2 https://packages.local/repository/csm-sle-15sp2/repodata/e9e8163a7c956f38eb37d6af3f1ac1bdae8079035843c9cd22ced9e824498da0-other.sqlite.bz2 https://packages.local/repository/csm-sle-15sp2/repodata/repomd.xml https://packages.local/repository/csm-sle-15sp2/x86_64/cfs-state-reporter-1.4.4-20201204120230_c198848.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/cfs-state-reporter-1.4.6-20210128142236_6bb340b.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/cfs-trust-1.0.2-20201216135115_58f3d86.x86_64.rpm ... Create a repository Repositories are created by an HTTP POST request to the /service/rest/beta/repositories/\u0026lt;format\u0026gt;/\u0026lt;type\u0026gt; endpoint with an appropriate body that defines the repository settings.\nFor example, to create a hosted yum repository for RPMs using the default blob store, HTTP POST the following body (replace NAME as appropriate) to /service/rest/beta/repositories/yum/hosted:\n{ \u0026#34;name\u0026#34;: \u0026#34;NAME\u0026#34;, \u0026#34;online\u0026#34;: true, \u0026#34;storage\u0026#34;: { \u0026#34;blobStoreName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;strictContentTypeValidation\u0026#34;: true, \u0026#34;writePolicy\u0026#34;: \u0026#34;ALLOW_ONCE\u0026#34; }, \u0026#34;cleanup\u0026#34;: null, \u0026#34;yum\u0026#34;: { \u0026#34;repodataDepth\u0026#34;: 0, \u0026#34;deployPolicy\u0026#34;: \u0026#34;STRICT\u0026#34; }, \u0026#34;format\u0026#34;: \u0026#34;yum\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;hosted\u0026#34; } The storage and yum options are used to control repository behavior.\nTo create a proxy repository to an upstream repository given by URL, HTTP POST the following body (replace NAME and URL as appropriate) to the /service/rest/beta/repositories/raw/proxy endpoint:\n{ \u0026#34;cleanup\u0026#34;: null, \u0026#34;format\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;httpClient\u0026#34;: { \u0026#34;authentication\u0026#34;: null, \u0026#34;autoBlock\u0026#34;: false, \u0026#34;blocked\u0026#34;: false, \u0026#34;connection\u0026#34;: null }, \u0026#34;name\u0026#34;: \u0026#34;NAME\u0026#34;, \u0026#34;negativeCache\u0026#34;: { \u0026#34;enabled\u0026#34;: false, \u0026#34;timeToLive\u0026#34;: 0 }, \u0026#34;online\u0026#34;: true, \u0026#34;proxy\u0026#34;: { \u0026#34;contentMaxAge\u0026#34;: 1440, \u0026#34;metadataMaxAge\u0026#34;: 5, \u0026#34;remoteUrl\u0026#34;: \u0026#34;URL\u0026#34; }, \u0026#34;routingRule\u0026#34;: null, \u0026#34;storage\u0026#34;: { \u0026#34;blobStoreName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;strictContentTypeValidation\u0026#34;: false }, \u0026#34;type\u0026#34;: \u0026#34;proxy\u0026#34; } The proxy, httpClient, and negativeCache options impact the proxy behavior. It may be helpful to create a repository via the Web UI, then retrieve its configuration through the /service/rest/beta/repositories endpoint in order to discover how to set appropriate settings.\nInstallers typically define Nexus repositories in nexus-repositories.yaml and rely on the nexus-repositories-create helper script included in the cray/cray-nexus-setup container image to facilitate creation.\nUpdate a repository Update the configuration for a repository by sending an HTTP PUT request to the /service/rest/beta/repositories/FORMAT/TYPE/NAME endpoint.\nFor example, if the yum hosted repository test is currently online and it needs to be updated to be offline instead, then send an HTTP PUT request to the /service/rest/beta/repositories/yum/hosted/test endpoint after getting the current configuration and changing the online attribute to true:\nncn-mw# curl -sS https://packages.local/service/rest/beta/repositories | jq \u0026#39;.[] | select(.name == \u0026#34;test\u0026#34;)\u0026#39; Example output:\n{ \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://packages.local/repository/test\u0026#34;, \u0026#34;online\u0026#34;: true, \u0026#34;storage\u0026#34;: { \u0026#34;blobStoreName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;strictContentTypeValidation\u0026#34;: true, \u0026#34;writePolicy\u0026#34;: \u0026#34;ALLOW_ONCE\u0026#34; }, \u0026#34;cleanup\u0026#34;: null, \u0026#34;yum\u0026#34;: { \u0026#34;repodataDepth\u0026#34;: 0, \u0026#34;deployPolicy\u0026#34;: \u0026#34;STRICT\u0026#34; }, \u0026#34;format\u0026#34;: \u0026#34;yum\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;hosted\u0026#34; } ncn-mw# curl -sS https://packages.local/service/rest/beta/repositories | \\ jq \u0026#39;.[] | select(.name == \u0026#34;test\u0026#34;) | .online = false\u0026#39; | \\ curl -sSi -X PUT \u0026#39;https://packages.local/service/rest/beta/repositories/yum/hosted/test\u0026#39; -H \u0026#34;Content-Type: application/json\u0026#34; -d @- Example output:\nHTTP/2 204 date: Sat, 06 Mar 202117:55:57 GMT server: istio-envoy x-content-type-options: nosniff x-envoy-upstream-service-time: 9 ncn-mw# curl -sS https://packages.local/service/rest/beta/repositories | jq \u0026#39;.[] | select(.name == \u0026#34;test\u0026#34;)\u0026#39; Example output:\n{ \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://packages.local/repository/test\u0026#34;, \u0026#34;online\u0026#34;: false, \u0026#34;storage\u0026#34;: { \u0026#34;blobStoreName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;strictContentTypeValidation\u0026#34;: true, \u0026#34;writePolicy\u0026#34;: \u0026#34;ALLOW_ONCE\u0026#34; }, \u0026#34;cleanup\u0026#34;: null, \u0026#34;yum\u0026#34;: { \u0026#34;repodataDepth\u0026#34;: 0, \u0026#34;deployPolicy\u0026#34;: \u0026#34;STRICT\u0026#34; }, \u0026#34;format\u0026#34;: \u0026#34;yum\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;hosted\u0026#34; } Delete a repository To delete a repository, send an HTTP DELETE request to the /service/rest/beta/repositories/NAME.\nFor example:\nncn-mw# curl -sfkSL -X DELETE \u0026#34;https://packages.local/service/rest/beta/repositories/NAME\u0026#34; Create a blob store A File type blob store may be created by sending an HTTP POST request to the /service/rest/beta/blobstores/file with the following body (replace NAME as appropriate):\n{ \u0026#34;name\u0026#34;: \u0026#34;NAME\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/nexus-data/blobs/NAME\u0026#34;, \u0026#34;softQuota\u0026#34;: null } Installers typically define Nexus blob stores in nexus-blobstores.yaml and rely on the nexus-blobstores-create helper script included in the cray/cray-nexus-setup container image to facilitate creation.\nDelete a blob store To delete a blob store, send an HTTP DELETE request to the /service/rest/v1/blobstores/NAME endpoint.\nFor example:\nncn-mw# curl -sfkSL -X DELETE \u0026#34;https://packages.local/service/rest/v1/blobstores/NAME\u0026#34; Authenticate to access the REST API An authenticated username and password are required to access some of the REST API functions not listed above. This username and password are the same used to sign into the Web UI. Either the username and password of a properly permissioned Keycloak account or the Nexus local admin account must be used.\nUse the following function to get the Nexus local admin account after a fresh install:\nfunction nexus-get-credential() { if ! command -v kubectl 1\u0026gt;\u0026amp;2 \u0026gt;/dev/null; then echo \u0026#34;Requires kubectl\u0026#34; return 1 fi if ! command -v base64 1\u0026gt;\u0026amp;2 \u0026gt;/dev/null ; then echo \u0026#34;Requires base64\u0026#34; return 1 fi [[ $# -gt 0 ]] || set -- -n nexus nexus-admin-credential kubectl get secret \u0026#34;${@}\u0026#34; \u0026gt;/dev/null || return $? NEXUS_USERNAME=\u0026#34;$(kubectl get secret \u0026#34;${@}\u0026#34; --template {{.data.username}} | base64 -d)\u0026#34; NEXUS_PASSWORD=\u0026#34;$(kubectl get secret \u0026#34;${@}\u0026#34; --template {{.data.password}} | base64 -d)\u0026#34; } Authenticate using either the Keycloak or Nexus account to use the REST API.\nTo authenticate using the Nexus local admin account:\nncn-mw# curl -i -sfv -u \u0026#34;$NEXUS_USERNAME:$NEXUS_PASSWORD\u0026#34; -H \u0026#34;accept: application/json\u0026#34; -X GET https://packages.local/service/rest/beta/security/user-sources To authenticate using a Keycloak account:\nThis example uses a Keycloak account with username USERNAME and password PASSWORD. Replace these values with the proper username and password before running the command.\nncn-mw# curl -i -sfv -u \u0026#34;USERNAME:PASSWORD\u0026#34; -H \u0026#34;accept: application/json\u0026#34; -X GET https://packages.local/service/rest/beta/security/user-sources "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/rebuild_ncns/final_validation_steps/",
	"title": "Final Validation Steps",
	"tags": [],
	"description": "",
	"content": "Final Validation Steps Use this procedure to finish validating the success of rebuilt NCNs.\nProcedure Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig after rebooting the node.\nNOTE: The following command will indicate if a CFS job is currently in progress for this node.\nIMPORTANT: The following command assumes that the variables from the prerequisites section have been set.\nncn# cray cfs components describe $XNAME --format json Example output:\n{ \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, } If the configurationStatus is pending, wait for the job to finish before continuing. If the configurationStatus is failed, this means the failed CFS job configurationStatus should be addressed now for this node. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, this can be ignored.\nIf configurationStatus is failed, See Troubleshoot Ansible Play Failures in CFS Sessions for how to analyze the pod logs from cray-cfs to determine why the configuration may not have completed.\nCollect data about the system management platform health (can be run from a master or worker NCN).\nncn-mw# /opt/cray/platform-utils/ncnHealthChecks.sh ncn-mw# /opt/cray/platform-utils/ncnPostgresHealthChecks.sh Next step Return to the main Rebuild NCNs page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/access_and_update_the_settings_for_replacement_ncns/",
	"title": "Access and Update Settings for Replacement NCNs",
	"tags": [],
	"description": "",
	"content": "Access and Update Settings for Replacement NCNs When a new NCN is added to the system as a hardware replacement, it might use the default credentials. Contact HPE Cray service to learn what these are.\nUse this procedure to verify that the default BMC credentials are set correctly after a replacement NCN is installed, cabled, and powered on.\nAll NCN BMCs must have credentials set up for ipmitool access.\nPrerequisites A new non-compute node (NCN) has been added to the system as a hardware replacement.\nProcedure Determine if ipmitool access is configured for root on the BMC.\nread -s is used to enter the password in order to prevent it from being echoed to the screen or saved in the shell history.\nncn# read -r -s -p \u0026#34;BMC root password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -I lanplus -U root -E -H NCN_NODE-mgmt power status Example output:\nError: Unable to establish IPMI v2 / RMCP+ session Connect to the BMC with the default login credentials. Contact service for the default credentials.\nDefault credentials for the Administrator user on HPE NCNs can be found on the serial label pull out tab attached to the server. See this page for more information. read -s is used to enter the password in order to prevent it from being echoed to the screen or saved in the shell history.\nncn# USERNAME=defaultuser ncn# read -r -s -p \u0026#34;BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN_NODE-mgmt power status Example output:\nChassis Power is on Troubleshooting: Follow the steps below if the credentials are not available:\nTroubleshoot Gigabyte NCNs.\nPower cycle the replacement NCN.\nBoot into Linux.\nUse the factory reset command to regain access to the BMC login credentials.\nlinux# ipmitool raw 0x32 0x66 Troubleshoot HPE NCNs.\nComing soon\nDetermine if the root user is configured.\nIn the example below, the root user does not exist yet.\nncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN_NODE-mgmt user list 1 Example output:\nID Name Callin Link Auth IPMI Msg Channel Priv Limit 1 false false true ADMINISTRATOR 2 admin false false true ADMINISTRATOR 3 ADMIN false false true ADMINISTRATOR 4 true false false NO ACCESS 5 true false false NO ACCESS 6 true false false NO ACCESS 7 true false false NO ACCESS 8 true false false NO ACCESS 9 true false false NO ACCESS 10 true false false NO ACCESS 11 true false false NO ACCESS 12 true false false NO ACCESS 13 true false false NO ACCESS 14 true false false NO ACCESS 15 true false false NO ACCESS 16 true false false NO ACCESS Add the new root user.\nEnable the creation of new credentials.\nncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN_NODE-mgmt user enable 4 Set the new username to root.\nncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN_NODE-mgmt user set name 4 root Set the new password.\nncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN_NODE-mgmt user set password 4 \u0026lt;BMC_root_password\u0026gt; Grant user privileges to the new credentials.\nncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN_NODE-mgmt user priv 4 4 1 Enable messaging for the identified slot and set the privilege level for that slot when it is accessed over LAN.\nncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN_NODE-mgmt channel setaccess 1 4 callin=on ipmi=on link=on Enable access to the serial over LAN (SOL) payload.\nncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN_NODE-mgmt sol payload enable 1 4 Verify that the root credentials have been configured.\nncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN_NODE-mgmt user list 1 Example output:\nID Name Callin Link Auth IPMI Msg Channel Priv Limit 1 false false true ADMINISTRATOR 2 admin false false true ADMINISTRATOR 3 ADMIN false false true ADMINISTRATOR 4 root true true true ADMINISTRATOR 5 true false false NO ACCESS 6 true false false NO ACCESS 7 true false false NO ACCESS 8 true false false NO ACCESS 9 true false false NO ACCESS 10 true false false NO ACCESS 11 true false false NO ACCESS 12 true false false NO ACCESS 13 true false false NO ACCESS 14 true false false NO ACCESS 15 true false false NO ACCESS 16 true false false NO ACCESS Confirm that the new credentials can be used with ipmitool.\nThe new credentials work if the command succeeds and generates output similar to the example below.\nncn# read -r -s -p \u0026#34;BMC root password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -I lanplus -U root -E -H NCN_NODE-mgmt user list 1 Example output:\nID Name Callin Link Auth IPMI Msg Channel Priv Limit 1 false false true ADMINISTRATOR 2 admin false false true ADMINISTRATOR 3 ADMIN false false true ADMINISTRATOR 4 root true true true ADMINISTRATOR 5 true false false NO ACCESS 6 true false false NO ACCESS 7 true false false NO ACCESS 8 true false false NO ACCESS 9 true false false NO ACCESS 10 true false false NO ACCESS 11 true false false NO ACCESS 12 true false false NO ACCESS 13 true false false NO ACCESS 14 true false false NO ACCESS 15 true false false NO ACCESS 16 true false false NO ACCESS Verify that the time is set correctly in the BIOS\nPlease refer to the Ensure Time Is Accurate Before Deploying NCNs procedure.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_remove_replace_ncns/add_ncn_data/",
	"title": "Add NCN Data",
	"tags": [],
	"description": "",
	"content": "Add NCN Data Description Add NCN data to the System Layout Service (SLS), Boot Script Service (BSS), and Hardware State Manager (HSM) as needed, in order to add an NCN to the system.\nScenarios where this procedure is applicable:\nAdding a management NCN that has not previously been in the system: Add an additional NCN to an existing cabinet Add an NCN that is replacing another NCN of the same type and in the same slot Add a new NCN that replaces an NCN removed from the system in a different location Adding a management NCN that has been present in the system previously: Add an NCN that was previously removed from the system to move it to a new location Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. The latest CSM documentation is installed on the system. See Check for latest documentation. Procedure Retrieve an API token:\nncn-mw# export TOKEN=$(curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth \\ -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Collect information from the NCN.\nDetermine the component name (xname) of the NCN, if it has not been determined yet.\nDetermine the xname by referring to the HMN of the system\u0026rsquo;s SHCD file.\nSample row from the HMN tab of an SHCD file:\nSource (J20) Source Rack (K20) Source Location (L20) (M20) Parent (N20) (O20) Source Port (P20) Destination (Q20) Destination Rack (R20) Destination Location (S20) (T20) Destination Port (U20) wn01 x3000 u04 - j3 sw-smn01 x3000 u14 - j48 The Source name for a worker NCN would be in the format wn01; master NCNs have format mn01 and storage NCNs have format sn01.\nNode xname format: xXcCsSbBnN\nSHCD Column to Reference Description X Cabinet number Source Rack (K20) The Cabinet or rack number containing the Management NCN. C Chassis number For air-cooled nodes within a standard rack, the chassis is 0. S Slot/Rack U Source Location (L20) The Slot of the node is determined by the bottom most rack U that node occupies. B BMC number For Management NCNs the BMC number is 0. N Node number For Management NCNs the Node number is 0. ncn-mw# XNAME=x3000c0s4b0n0 Skip if adding ncn-m001: Determine the NCN BMC xname by removing the trailing n0 from the NCN xname:\nncn-mw# BMC_XNAME=x3000c0s4b0 Skip if adding ncn-m001: Determine the xname of the MgmtSwitchConnector (the switch port of the management switch that the BMC is connected to). This is not required for ncn-m001, because its BMC is typically connected to the site network.\nSample row from the HMN tab of an SHCD:\nSource (J20) Source Rack (K20) Source Location (L20) (M20) Parent (N20) (O20) Source Port (P20) Destination (Q20) Destination Rack (R20) Destination Location (S20) (T20) Destination Port (U20) wn01 x3000 u04 - j3 sw-smn01 x3000 u14 - j48 MgmtSwitchConnector xname format: xXcCwWjJ\nSHCD Column to Reference Description X Cabinet number Destination Rack (R20) The Cabinet or rack number containing the management NCN. C Chassis number For air-cooled management switches within standard racks, the chassis is 0. W Slot/Rack U Destination Location (S20) The Slot/Rack U that the management switch occupies. J Switch port number Destination Port (U20) The switch port on the switch that the NCN BMC is cabled to. ncn-mw# MGMT_SWITCH_CONNECTOR=x3000c0w14j48 Skip if adding ncn-m001: Determine the xname of the management switch by removing the trailing jJ from the MgmtSwitchConnector xname.\nncn-mw# MGMT_SWITCH=x3000c0w14 Skip if adding ncn-m001: Collect the BMC MAC address.\nIf the NCN was previously in the system, recall the BMC MAC address recorded from the Remove NCN Data procedure.\nAlternatively, view the MAC address table on the management switch that the BMC is cabled to.\nDetermine the alias of the management switch that is connected to the BMC.\nncn-mw# cray sls hardware describe \u0026#34;${MGMT_SWITCH}\u0026#34; --format json | jq .ExtraProperties.Aliases[] -r Example output:\nsw-leaf-bmc-001 SSH into the management switch that is connected to the BMC.\nncn-mw# ssh admin@sw-leaf-bmc-001.hmn Locate the switch port that the BMC is connected to and record its MAC address.\nIn the commands below, change the value of 1/1/39 to match the BMC switch port number (the BMC switch port number is the J value in the in the MgmtSwitchConnector xname xXwWjJ). For example, with a MgmtSwitchConnector xname of x3000c0w14j39, the switch port number would be 39. In that case, 1/1/39 would be used instead of 1/1/48 in the following commands.\nDell Management Switch\nsw-leaf-bmc# show mac address-table | grep 1/1/48 Example output:\n4 a4:bf:01:65:68:54 dynamic 1/1/48 Aruba Management Switch\nsw-leaf-bmc# show mac-address-table | include 1/1/48 Example output:\na4:bf:01:65:68:54 4 dynamic 1/1/48 Skip if adding ncn-m001: Set the BMC_MAC environment variable to the BMC MAC address.\nncn-mw# BMC_MAC=a4:bf:01:65:68:54 Skip if adding ncn-m001: Determine the current IP address of the NCN BMC.\nQuery Kea for the BMC MAC address to determine its current IP address.\nncn-mw# BMC_IP=$(curl -sk -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; \\ https://api-gw-service-nmn.local/apis/dhcp-kea | jq --arg BMC_MAC \u0026#34;${BMC_MAC}\u0026#34; \\ \u0026#39;.[].arguments.leases[] | select(.\u0026#34;hw-address\u0026#34; == $BMC_MAC).\u0026#34;ip-address\u0026#34;\u0026#39; -r) ncn-mw# echo ${BMC_IP} Example output:\n10.254.1.26 Troubleshooting If the MAC addresses of the BMC are not present in Kea, then check for the following items:\nVerify that the BMC is powered up and has an active connection to the network. Verify that the BMC is set to DHCP instead of a static IP address. Ping the BMC to see if it is reachable.\nncn-mw# ping \u0026#34;${BMC_IP}\u0026#34; Perform this step if adding ncn-m001, otherwise skip: Set the BMC_IP environment variable to the current IP address or hostname of the BMC. This is not the allocated HMN address for the BMC of ncn-m001.\nncn-mw# BMC_IP=10.0.0.10 Collect NCN MAC addresses for the following interfaces if they are present. The collected MAC addresses will be used later in this procedure with the add_management_ncn.py script.\nDepending on the hardware present in the NCN, not all of these interfaces may be present.\nNCNs will have either 1 or 2 management PCIe NIC cards (2 or 4 PCIe NIC ports). It is expected that only worker NCNs have HSN interfaces. NCN with a single PCIe card (1 card with 2 ports):\nInterface CLI Flag Required MAC Address Description mgmt0 --mac-mgmt0 Required First MAC address of Bond 0. mgmt1 --mac-mgmt1 Required Second MAC address of Bond 0. hsn0 --mac-hsn0 Required for Worker NCNs MAC address of the first High Speed Network NIC. Master and Storage NCNs do not have HSN NICs. hsn1 --mac-hsn1 Optional for Worker NCNs MAC address of the second High Speed Network NIC. Master and Storage NCNs do not have HSN NICs. lan0 --mac-lan0 Optional MAC address for the first non-bond or HSN-related interface. lan1 --mac-lan1 Optional MAC address for the second non-bond or HSN-related interface. lan2 --mac-lan2 Optional MAC address for the third non-bond or HSN-related interface. lan3 --mac-lan3 Optional MAC address for the forth non-bond or HSN-related interface. NCN with a dual PCIe cards (2 cards with 2 ports each for 4 ports total):\nInterface CLI Flag Required MAC Address Description mgmt0 --mac-mgmt0 Required First MAC address of Bond 0. mgmt1 --mac-mgmt1 Required Second MAC address of Bond 0. sun0 --mac-sun0 Required First MAC address of Bond 1. sun1 --mac-sun0 Required Second MAC address of Bond 1. hsn0 --mac-hsn0 Required for Worker NCNs MAC address of the first High Speed Network NIC. Master and Storage NCNs do not have HSN NICs. hsn1 --mac-hsn1 Optional for Worker NCNs MAC address of the second High Speed Network NIC. Master and Storage NCNs do not have HSN NICs. lan0 --mac-lan0 Optional MAC address for the first non-bond or HSN-related interface. lan1 --mac-lan1 Optional MAC address for the second non-bond or HSN-related interface. lan2 --mac-lan2 Optional MAC address for the third non-bond or HSN-related interface. lan3 --mac-lan3 Optional MAC address for the forth non-bond or HSN-related interface. If the NCN being added is being moved to a new location in the system, then these MAC addresses can be retrieved from backup files generated by the Remove NCN Data procedure.\nRecall the previous node xname of the NCN being added:\nncn-mw# PREVIOUS_XNAME=REPLACE_WITH_OLD_XNAME Retrieve the MAC address for the NCN from the backup files:\nncn-mw# cat \u0026#34;/tmp/remove_management_ncn/${PREVIOUS_XNAME}/bss-bootparameters-${PREVIOUS_XNAME}.json\u0026#34; | jq .[].params -r | tr \u0026#34; \u0026#34; \u0026#34;\\n\u0026#34; | grep ifname Example output for a worker node with a single management PCIe NIC card:\nifname=hsn0:50:6b:4b:23:9f:7c ifname=lan1:b8:59:9f:d9:9d:e9 ifname=lan0:b8:59:9f:d9:9d:e8 ifname=mgmt0:a4:bf:01:65:6a:aa ifname=mgmt1:a4:bf:01:65:6a:ab Using the example output from above, derive the following CLI flags for a worker NCN:\nInterface MAC Address CLI Flag mgmt0 a4:bf:01:65:6a:aa --mac-mgmt0=a4:bf:01:65:6a:aa mgmt1 a4:bf:01:65:6a:ab --mac-mgmt1=a4:bf:01:65:6a:ab lan0 b8:59:9f:d9:9d:e8 --mac-lan0=b8:59:9f:d9:9d:e8 lan1 b8:59:9f:d9:9d:e9 --mac-lan1=b8:59:9f:d9:9d:e9 hsn0 50:6b:4b:23:9f:7c --mac-hsn0=50:6b:4b:23:9f:7c Otherwise the NCN MAC addresses need to be collected using the Collect NCN MAC Addresses procedure.\nPerform a dry run of the add_management_ncn.py script in order to determine if any validation failures occur:\nUpdate the following command with the MAC addresses and interfaces that were collected from the NCN.\nIf adding a node other than ncn-m001:\nncn-mw# cd /usr/share/doc/csm/scripts/operations/node_management/Add_Remove_Replace_NCNs/ ncn-mw# ./add_management_ncn.py ncn-data \\ --xname \u0026#34;${XNAME}\u0026#34; \\ --alias \u0026#34;${NODE}\u0026#34; \\ --bmc-mgmt-switch-connector \u0026#34;${MGMT_SWITCH_CONNECTOR}\u0026#34; \\ --mac-bmc \u0026#34;${BMC_MAC}\u0026#34; \\ --mac-mgmt0 a4:bf:01:65:6a:aa \\ --mac-mgmt1 a4:bf:01:65:6a:ab \\ --mac-hsn0 50:6b:4b:23:9f:7c \\ --mac-lan0 b8:59:9f:d9:9d:e8 \\ --mac-lan1 b8:59:9f:d9:9d:e9 If adding ncn-m001, omit the --bmc-mgmt-switch-connector and --mac-bmc arguments, because its BMC is connected to the site network:\nncn-mw# cd /usr/share/doc/csm/scripts/operations/node_management/Add_Remove_Replace_NCNs/ ncn-mw# ./add_management_ncn.py ncn-data \\ --xname \u0026#34;${XNAME}\u0026#34; \\ --alias \u0026#34;${NODE}\u0026#34; \\ --mac-mgmt0 a4:bf:01:65:6a:aa \\ --mac-mgmt1 a4:bf:01:65:6a:ab \\ --mac-lan0 b8:59:9f:d9:9d:e8 \\ --mac-lan1 b8:59:9f:d9:9d:e9 Add the NCN to SLS, HSM, and BSS.\nRun the add_management_ncn.py script again, adding the --perform-changes argument to the command run in the previous step:\nFor example:\nncn-mw# ./add_management_ncn.py ncn-data \\ --xname \u0026#34;${XNAME}\u0026#34; \\ --alias \u0026#34;${NODE}\u0026#34; \\ --bmc-mgmt-switch-connector \u0026#34;${MGMT_SWITCH_CONNECTOR}\u0026#34; \\ --mac-bmc \u0026#34;${BMC_MAC}\u0026#34; \\ --mac-mgmt0 a4:bf:01:65:6a:aa \\ --mac-mgmt1 a4:bf:01:65:6a:ab \\ --mac-hsn0 50:6b:4b:23:9f:7c \\ --mac-lan0 b8:59:9f:d9:9d:e8 \\ --mac-lan1 b8:59:9f:d9:9d:e9 \\ --perform-changes Example output:\n... x3000c0s3b0n0 (ncn-m002) has been added to SLS/HSM/BSS WARNING The NCN BMC currently has the IP address: 10.254.1.20, and needs to have IP address 10.254.1.13 ================================= Management NCN IP Allocation ================================= Network | IP Address --------|----------- CMN | 10.103.11.42 CAN | 10.102.4.10 HMN | 10.254.1.14 MTL | 10.1.1.7 NMN | 10.252.1.9 ================================= Management NCN BMC IP Allocation ================================= Network | IP Address --------|----------- HMN | 10.254.1.13 Depending on the networking configuration of the system the CMN or CAN networks may not be present in SLS network data. No IP addresses will be allocated for networks that do not exist in SLS.\nIf the following text is present at the end of the add_management_ncn.py script output, then the NCN BMC was given an IP address by DHCP, and it is not at the expected IP address. Sample output when the BMC has an unexpected IP address.\nx3000c0s3b0n0 (ncn-m002) has been added to SLS/HSM/BSS WARNING The NCN BMC currently has the IP address: \u0026lt;$BMC_IP\u0026gt;, and needs to have IP address X.Y.Z.W Restart the BMC to pick up the expected IP address:\nread -s is used to read the password in order to prevent it from being echoed to the screen or recorded in the shell history.\nncn-mw# read -r -s -p \u0026#34;BMC root password: \u0026#34; IPMI_PASSWORD ncn-mw# export IPMI_PASSWORD ncn-mw# ipmitool -U root -I lanplus -E -H \u0026#34;${BMC_IP}\u0026#34; mc reset cold ncn-mw# sleep 60 Skip if adding ncn-m001: Verify that the BMC is reachable at the expected IP address.\nncn-mw# ping \u0026#34;${NODE}-mgmt\u0026#34; Wait five minutes for Kea and the HSM to sync. If ping continues to fail, then re-run the previous step to restart the BMC.\nRestart the REDS deployment.\nncn-mw# kubectl -n services rollout restart deployment cray-reds Expected output:\ndeployment.apps/cray-reds restarted Wait for REDS to restart.\nncn-mw# kubectl -n services rollout status deployment cray-reds Expected output:\nWaiting for deployment \u0026#34;cray-reds\u0026#34; rollout to finish: 1 old replicas are pending termination... Waiting for deployment \u0026#34;cray-reds\u0026#34; rollout to finish: 1 old replicas are pending termination... deployment \u0026#34;cray-reds\u0026#34; successfully rolled out Skip if adding ncn-m001: Wait for the NCN BMC to get discovered by HSM.\nIf the BMC of ncn-m001 is connected to the site network, then the BMC will not be discovered, because it is not connected via the HMN network.\nncn-mw# watch -n 0.2 \u0026#34;cray hsm inventory redfishEndpoints describe \u0026#39;${BMC_XNAME}\u0026#39; --format json\u0026#34; Wait until the LastDiscoveryAttempt field is DiscoverOK:\n{ \u0026#34;ID\u0026#34;: \u0026#34;x3000c0s38b0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;NodeBMC\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x3000c0s38b0\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;UUID\u0026#34;: \u0026#34;cc48551e-ec22-4bef-b8a3-bb3261749a0d\u0026#34;, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2022-02-28T22:54:08.496898Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;1.7.0\u0026#34; } } Discovery troubleshooting\nThe redfishEndpoint may cycle between DiscoveryStarted and HTTPsGetFailed before the endpoint becomes DiscoverOK. If the BMC is in HTTPSGetFailed for a long period of time, then the following steps may help to determine the cause:\nVerify that the xname of the BMC resolves in DNS.\nncn-mw# nslookup x3000c0s38b0 Expected output:\nServer: 10.92.100.225 Address: 10.92.100.225#53 Name: x3000c0s38b0.hmn Address: 10.254.1.13 Verify that the BMC is reachable at the expected IP address.\nncn-mw# ping \u0026#34;${NODE}-mgmt\u0026#34; Verify that the BMC Redfish v1/Managers endpoint is reachable.\nncn-mw# curl -k -u root:changeme https://x3000c0s38b0/redfish/v1/Managers Verify that the NCN exists under HSM State Components.\nncn-mw# cray hsm state components describe \u0026#34;${XNAME}\u0026#34; --format toml Example output:\nID = \u0026#34;x3000c0s11b0n0\u0026#34; Type = \u0026#34;Node\u0026#34; State = \u0026#34;Off\u0026#34; Flag = \u0026#34;OK\u0026#34; Enabled = true Role = \u0026#34;Management\u0026#34; SubRole = \u0026#34;Worker\u0026#34; NID = 100006 NetType = \u0026#34;Sling\u0026#34; Arch = \u0026#34;X86\u0026#34; Class = \u0026#34;River\u0026#34; Next step Proceed to Update Firmware or return to the main Add, Remove, Replace, or Move NCNs page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/metallb_bgp/check_bgp_status_and_reset_sessions/",
	"title": "Check BGP Status and Reset Sessions",
	"tags": [],
	"description": "",
	"content": "Check BGP Status and Reset Sessions Check the Border Gateway Protocol (BGP) status on the Aruba and Mellanox switches and verify that all sessions are in an Established state. If the state of any session in the table is Idle, then the BGP sessions must be reset.\nPrerequisites Procedure Mellanox Aruba Prerequisites This procedure requires administrative privileges.\nProcedure The following procedures may not resolve the problem after just one attempt. In some cases, the procedures need to be followed multiple times before the situation resolves.\nMellanox Verify that all BGP sessions are in an ESTABLISHED state for the Mellanox spine switches.\nSSH to each spine switch and check the status of all BGP sessions.\nSSH to a spine switch.\nFor example:\nncn# ssh admin@sw-spine-001.hmn View the status of the BGP sessions.\nsw-spine# enable sw-spine# show ip bgp vrf all summary Example output:\nVRF name : CAN BGP router identifier : 10.101.8.2 local AS number : 65533 BGP table version : 1634 Main routing table version: 1634 IPV4 Prefixes : 46 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.101.8.8 4 65536 667385 678016 1634 0 0 6:21:29:59 ESTABLISHED/14 10.101.8.9 4 65536 667177 678199 1634 0 0 6:21:30:04 ESTABLISHED/18 10.101.8.10 4 65536 667359 678211 1634 0 0 6:21:30:16 ESTABLISHED/14 VRF name : default BGP router identifier : 10.252.0.2 local AS number : 65533 BGP table version : 40 Main routing table version: 40 IPV4 Prefixes : 40 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.1.7 4 65533 595814 595793 40 0 0 6:21:29:52 ESTABLISHED/12 10.252.1.8 4 65533 595827 595804 40 0 0 6:21:30:03 ESTABLISHED/16 10.252.1.9 4 65533 595842 595817 40 0 0 6:21:30:16 ESTABLISHED/12 If any of the sessions are in an IDLE state, then proceed to the next step.\nReset BGP to re-establish the sessions.\nSSH to each spine switch.\nFor example:\nncn# ssh admin@sw-spine-001.hmn Enter enable mode.\nsw-spine\u0026gt; enable Verify that BGP is enabled.\nsw-spine# show protocols | include bgp If BGP is enabled, then the output should be similar to the following:\nbgp: enabled Clear the BGP sessions.\nThere are two VRFs that may need to be cleared. Clear the VRF that has the Idle session state.\nDefault VRF:\nsw-spine# clear ip bgp vrf default all Customer VRF:\nsw-spine# clear ip bgp vrf Customer all Check the status of the BGP sessions to see if they are now ESTABLISHED.\nIt may take a few minutes for sessions to become ESTABLISHED.\nsw-spine# show ip bgp vrf all summary Example output:\nVRF name : CAN BGP router identifier : 10.101.8.2 local AS number : 65533 BGP table version : 1634 Main routing table version: 1634 IPV4 Prefixes : 46 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.101.8.8 4 65536 667385 678016 1634 0 0 6:21:29:59 ESTABLISHED/14 10.101.8.9 4 65536 667177 678199 1634 0 0 6:21:30:04 ESTABLISHED/18 10.101.8.10 4 65536 667359 678211 1634 0 0 6:21:30:16 ESTABLISHED/14 VRF name : default BGP router identifier : 10.252.0.2 local AS number : 65533 BGP table version : 40 Main routing table version: 40 IPV4 Prefixes : 40 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.1.7 4 65533 595814 595793 40 0 0 6:21:29:52 ESTABLISHED/12 10.252.1.8 4 65533 595827 595804 40 0 0 6:21:30:03 ESTABLISHED/16 10.252.1.9 4 65533 595842 595817 40 0 0 6:21:30:16 ESTABLISHED/12 Once all sessions are in an ESTABLISHED state, BGP reset is complete.\nAruba Verify that all BGP sessions are in an Established state for the Aruba spine switches.\nSSH to each spine switch and check the status of all BGP sessions.\nSSH to a spine switch.\nncn# ssh admin@sw-spine-001.hmn View the status of the BGP sessions.\nsw-spine# show bgp all-vrf all summary Example output:\nVRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.2.0.2 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 3 Cfg. Keep Alive : 1 Confederation Id : 0 Address-family : IPv4 Unicast ----------------------------- Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.3 65533 571006 571002 06d:14h:38m Established Up 10.252.1.7 65533 451712 451502 03d:09h:34m Established Up 10.252.1.8 65533 450943 450712 03d:09h:36m Established Up 10.252.1.9 65533 451463 451267 03d:09h:35m Established Up Address-family : IPv6 Unicast ----------------------------- Address-family : L2VPN EVPN ----------------------------- VRF : Customer BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.103.15.186 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 3 Cfg. Keep Alive : 1 Confederation Id : 0 Address-family : IPv4 Unicast ----------------------------- Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.103.11.3 65533 500874 500891 00h:00m:11s Established Up 10.103.11.8 65536 374118 374039 03d:09h:35m Established Up 10.103.11.9 65536 373454 373290 03d:09h:35m Established Up 10.103.11.10 65536 374169 374087 03d:09h:34m Established Up Address-family : IPv6 Unicast ----------------------------- If any of the sessions are in an Idle state, then proceed to the next step.\nReset BGP to re-establish the sessions.\nSSH to each spine switch.\nFor example:\nncn# ssh admin@sw-spine-001.hmn Clear the BGP sessions.\nThere are two VRFs that may need to be cleared. Clear the VRF that has the Idle session state.\nDefault VRF:\nsw-spine# clear bgp vrf default * Customer VRF:\nsw-spine# clear bgp vrf Customer * Check the status of the BGP sessions.\nIt may take a few minutes for sessions to become Established.\nsw-spine# show bgp all-vrf all summary Example output:\nVRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.2.0.2 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 3 Cfg. Keep Alive : 1 Confederation Id : 0 Address-family : IPv4 Unicast ----------------------------- Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.3 65533 571006 571002 06d:14h:38m Established Up 10.252.1.7 65533 451712 451502 03d:09h:34m Established Up 10.252.1.8 65533 450943 450712 03d:09h:36m Established Up 10.252.1.9 65533 451463 451267 03d:09h:35m Established Up Address-family : IPv6 Unicast ----------------------------- Address-family : L2VPN EVPN ----------------------------- VRF : Customer BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.103.15.186 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 3 Cfg. Keep Alive : 1 Confederation Id : 0 Address-family : IPv4 Unicast ----------------------------- Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.103.11.3 65533 500874 500891 00h:00m:11s Established Up 10.103.11.8 65536 374118 374039 03d:09h:35m Established Up 10.103.11.9 65536 373454 373290 03d:09h:35m Established Up 10.103.11.10 65536 374169 374087 03d:09h:34m Established Up Address-family : IPv6 Unicast ----------------------------- Once all sessions are in an Established state, BGP reset is complete.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/acl/",
	"title": "Access control lists (ACLs)",
	"tags": [],
	"description": "",
	"content": "Access control lists (ACLs) ACLs are used to help improve network performance and restrict network usage by creating policies to eliminate unwanted IP traffic by filtering packets where they enter the switch on layer 2 and layer 3 interfaces. An ACL is an ordered list of one or more access control list entries (ACEs) prioritized by sequence number. An incoming packet is matched sequentially against each entry in an ACL. When a match is made, the action of that ACE is taken and the packet is not compared against any other ACEs in the list. For ACL filtering to take effect, configure an ACL and then assign it in the inbound or outbound direction on an L2 or L3 interface with IPv4 traffic, and inbound-only for IPv6.\nRelevant Configuration\nCreate an ACL\nswitch (config) mac access-list mac-acl switch (config mac access-list mac-acl) # Add a MAC / IP rule to the appropriate access-list. Run:\nswitch (config mac access-list mac-acl) # seq-number 10 deny 0a:0a:0a:0a:0a:0a mask ff:ff:ff:ff:ff:ff any vlan 6 cos 2 protocol 80 Bind the created access-list to an interface (port or LAG). Run:\nswitch (config) # interface ethernet 1/1 switch (config interface ethernet 1/1) # mac port access-group mac-acl Show Commands to Validate Functionality\nswitch# show ipv4 access-lists \u0026lt;access-list-name\u0026gt; Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/",
	"title": "Management Network User Guide",
	"tags": [],
	"description": "",
	"content": "Management Network User Guide This documentation helps network administrators and support personnel install install and manage Aruba, Dell, and Mellanox network devices in a CSM install.\nThe HPE Cray recommended way of configuring the network is by using the CANU tool. Therefore this guide will not go into detail on how to configure each switch manually using the CLI. Instead, it will give helpful examples of how to configure/use features generated by CANU, in order to provide administrators easy ways to customize their installation.\nAlso included in this guide are the current documented and supported network scenarios.\nNOTE: Not every configuration option is covered here; for any configuration outside of the scope of this document, refer to the official Aruba, Dell, or Mellanox user manuals.\nThis document is intended for network administrators and support personnel.\nNOTE: The display and command lines illustrated in this document are examples and might not exactly match any particular environment. The switch and accessory drawings in this document are for illustration only, and may not exactly match installed products.\nContents Switch Configuration States Starting Points User Guides CANU Useful Pages Products Supported Architecture and Naming Conventions Minimum Software Version Requirements Changes Enhancements Fixes Issues and Workarounds Security Bulletin Subscription Service Switch Configuration States To check the current state of the switches, see the upgrade page.\n1.0 Config: Non-generated switch configurations 1.2 Preconfig: CANU-generated configurations for 1.0 1.2 Config: CANU-generated configurations for 1.2 Starting Points Upgrade CANU to the latest version: Install/Upgrade CANU Current switch configurations are not generated and need to upgrade to generated configurations. This can either be 1.0 generated or 1.2 generated. upgrade Current switch configurations are generated and need to go to 1.2. 1.2 (Preconfig) to 1.2 Switches have no configuration on them. Fresh Install Reinstalling the same CSM version. Re-install New hardware was added to the system. Added Hardware Switch failed and needs to be replaced. Replace Switch User Guides The user guides contain information such as are generic configuration examples, explanation of currently used technologies, topology maps, and a summary of the network design used in the CSM management network.\nAruba Dell Mellanox CANU See CSM Automatic Network Utility (CANU)\nUseful Pages Collect data Input data needed to generate switch configurations. Cable Management Network Servers Shows how servers in CSM should be cabled. SHCD HMN Tab/HMN Connections Rules Shows how the HMN tab on the SHCD should be formatted. SHCD Connection Rules Shows how the all tabs on the SHCD should be formatted EXCEPT the HMN tab. Backup Custom Configuration Shows users how to backup configuration that is custom to their system; this includes site connections and credentials. Configuration Management Shows users how to save a running configuration to the switch for backup purposes or to switch between different switch configurations. Validate Switch Configuration Compares the configuration on a running system to what is generated. Wipe Management Switches Erase the switch configuration, this is useful for fresh installs. Generate Switch Configurations Generate configurations for all management switches. Manual Switch Configurations Some configuration needs to be done manually (authentication/SNMP). Validate SHCD Validate the SHCD against the CSM network architecture. Products Supported This release applies to the following product models:\nAruba 8320 Switch Series Aruba 8325 Switch Series Aruba 8360 Switch Series Aruba 6300 Switch Series Mellanox SN2100 Switch Series Mellanox SN2700 Switch Series Dell S3048-ON Switch Series Dell S4148T-ON Switch Series Dell S4148F-ON Switch Series Architecture and Naming Conventions For architecture and naming convention information, see Cray Network Architecture model.\nMinimum Software Version Requirements Network Firmware Changes These sections list enhancements, fixes, and known issues for this version of the Shasta management network.\nEnhancements Fixes Issues and Workarounds Security Bulletin Subscription Service Enhancements Software enhancements are listed in reverse-chronological order, with the newest on the top of the list.\nUnless otherwise noted, each network version listed includes all enhancements added in earlier versions.\nSpanning-tree In the Shasta management network version 1.2, Spanning-tree configuration is changing from RPVST (RSTP) to MSTP, in order to allow for better vendor interoperability and simplified Spanning-tree configuration. The new default configuration is as follows:\nSpanning-tree BPDU guard is removed from NCN LAG ports. Spanning-tree BPDU filter is removed. Spanning-tree admin-edge port settings are retained for allowing quicker PXE boot. Spanning-tree instance is tied to default (no longer multiple instances or per VLAN). Spanning-tree MSTP configuration name and revision in all configurations generated by CANU are set. Add peering from MetalLB to customer edge router. Fixes Issues and Workarounds The following are known open issues with this branch of the software.\nThe Symptom statement describes what a user might experience if this is seen on the network. The Scenario statement provides additional environment details and trigger summaries. When available, the Workaround statement provides a workaround to the issue.\nKnown and fixed issue can be found from CANU release notes.\nCANU releases\nSecurity Bulletin Subscription Service To initiate a subscription to receive future HPE Security Bulletin alerts via email, see the HPE Support Center.\nA Security Bulletin is the published notification of security vulnerabilities and is the only communication vehicle for security vulnerabilities.\nFixes for security vulnerabilities are not documented in manuals, release notes, or other forms of product documentation. A Security Bulletin is released when all vulnerable products still in support life have publicly available images that contain the fix for the security vulnerability. "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/firmware/update_management_network_firmware/",
	"title": "Update Management Network Firmware",
	"tags": [],
	"description": "",
	"content": "Update Management Network Firmware This page describes how to update firmware on the management network switches. More details and other options to upgrade firmware can be found in the switch External User Guides.\nPrerequisites Access to the switches. Firmware in a location that the switches can reach. All firmware can be found in the HFP package provided with the Shasta release.\nSwitch Firmware Model software version Aruba 8320 Switch Series 10.09.0010 Aruba 8325 Switch Series 10.09.0010 Aruba 8360 Switch Series 10.09.0010 Aruba 6300 Switch Series 10.09.0010 Mellanox SN2100 Switch Series 3.9.3210 Mellanox SN2700 Switch Series 3.9.3210 Dell S3048-ON Switch Series 10.5.1.4 Dell S4148T-ON Switch Series 10.5.1.4 Dell S4148F-ON Switch Series 10.5.1.4 Aruba Firmware Best Practices Aruba software version number explained:\nFor example: 10.06.0120\n10\t= OS\n06\t= Major branch (new features)\n0120\t= CPE release (bug fixes)\nIt is considered to be a best practice to keep all Aruba CX platform devices running the same software version.\nAruba CX devices two software image banks, which means sw images can be pre-staged to the device without booting to the new image.\nIf upgrading to a new major branch, in Aruba identified by the second integer in the software image number.\nWhen upgrading past a major software release, for example, from 10.6 to 10.8 (and skipping 10.7), issue the allow-unsafe-upgrades command to allow any low level firmware/driver upgrades to complete. If going from the 10.6 branch to 10.7 branch, this step can be skipped as the low level firmware/driver upgrade would be automatically completed.\nsw-leaf-001# config sw-leaf-001(config)# allow-unsafe-updates 30 This command will enable non-failsafe updates of programmable devices for the next 30 minutes. First, wait for all line and fabric modules to reach the ready state, and then reboot the switch to begin applying any needed updates. Ensure that the switch will not lose power, be rebooted again, or have any modules removed until all updates have finished and all line and fabric modules have returned to the ready state.\nWARNING: Interrupting these updates may make the product unusable!\nContinue (y/n)? y Unsafe updates : allowed (less than 30 minute(s) remaining) VSX software upgrade command can automatically upgrade both of the peers in VSX topology by staging upgrade and automatically doing traffic shifting between peers to minimize impact to network. The following examples include the option for standalone and vsx-pair upgrade.\nAruba Firmware Update - Standalone Console into the switch being upgraded.\nCheck images\nsw-leaf-001# show images --------------------------------------------------------------------------- ArubaOS-CX Primary Image --------------------------------------------------------------------------- Version : FL.10.06.0010 Size : 658 MB Date : 2020-12-14 11:49:52 PST SHA-256 : 9e03da5697ef40d261b4a2920a19197ab64ea338533578ce576e5ca1a6849285 --------------------------------------------------------------------------- ArubaOS-CX Secondary Image --------------------------------------------------------------------------- Version : FL.10.04.0010 Size : 722 MB Date : 2019-12-03 10:41:01 PST SHA-256 : 2f00ca2d86338701225aadf4b9aa9b076e929b2b4620239b44122f300ff29e2d Default Image : primary Boot Profile Timeout : 5 seconds ------------------------------------------------------ Management Module 1/1 (Active) ------------------------------------------------------ Active Image : primary Service OS Version : FL.01.07.0002 BIOS Version : FL.01.0002 Upload the firmware to the desired image. In this example we are uploading it to the secondary.\nsw-leaf-001# copy sftp://root@10.252.1.12//root/ArubaOS-CX_6400-6300_10_08_1021.swi secondary sw-leaf-001# write mem Copying configuration: [Success] Once the upload is complete, check the images:\nsw-leaf-001# show image --------------------------------------------------------------------------- ArubaOS-CX Primary Image --------------------------------------------------------------------------- Version : FL.10.06.0010 Size : 658 MB Date : 2020-12-14 11:49:52 PST SHA-256 : 9e03da5697ef40d261b4a2920a19197ab64ea338533578ce576e5ca1a6849285 --------------------------------------------------------------------------- ArubaOS-CX Secondary Image --------------------------------------------------------------------------- Version : FL.10.08.1021 Size : 812 MB Date : 2021-11-08 02:09:58 UTC SHA-256 : 3e7f5e22843b49438d2eab19f0e6df8ebccef053e38d6cd65110cfeb37d707fc Default Image : primary Boot Profile Timeout : 5 seconds ------------------------------------------------------ Management Module 1/1 (Active) ------------------------------------------------------ Active Image : primary Service OS Version : FL.01.07.0002 BIOS Version : FL.01.0002 After the firmware is uploaded, boot the switch to the correct image.\nsw-leaf-001# boot system secondary Once the reboot is complete, check and make sure the firmware version is correct.\nsw-leaf-001# show version ----------------------------------------------------------------------------- ArubaOS-CX (c) Copyright 2017-2020 Hewlett Packard Enterprise Development LP ----------------------------------------------------------------------------- Version : FL.10.06.0010 Build Date : 2020-09-29 07:44:16 PDT Build ID : ArubaOS-CX:FL.10.06.0010:3cbfcce60961:202009291304 Build SHA : 3cbfcce609617b0cf84a6b941a2b36c43dfeb2cb Active Image : primary Service OS Version : FL.01.07.0002 BIOS Version : FL.01.0002 Aruba Firmware Update - VSX Software Upgrade Console into both VSX switches and pre-stage the firmware. In this example we are pre-staging the firmware to sw-spine-001 and sw-spine-002\nCheck images first.\nsw-spine-002# show images --------------------------------------------------------------------------- ArubaOS-CX Primary Image --------------------------------------------------------------------------- Version : GL.10.06.0010 Size : 444 MB Date : 2020-12-14 11:55:16 PST SHA-256 : 4157d15a5cad6efce4d0e8b35f75b4d6212de5af0c5c9bf3ad8f74853df67733 --------------------------------------------------------------------------- ArubaOS-CX Secondary Image --------------------------------------------------------------------------- Version : GL.10.02.0020 Size : 360 MB Date : 2019-03-12 09:26:31 PDT SHA-256 : da629a197e6acbdd805bc7cb85f1decff772ce25223ea20f7c55d426df03fcbe Default Image : primary Boot Profile Timeout : 5 seconds ------------------------------------------------------ Management Module 1/1 (Active) ------------------------------------------------------ Active Image : primary Service OS Version : GL.01.08.0002 BIOS Version : GL-01-0013 Upload the firmware to the desired image. In this example we are uploading it to the secondary.\nsw-leaf-001# copy sftp://root@10.252.1.12//var/www/ephemeral/data/network_images/ArubaOS-CX_8325_10_08_1021.swi secondary sw-leaf-001# write mem Copying configuration: [Success] Once the upload is complete, check the images and make sure the version is correct.\nsw-spine-001# show image --------------------------------------------------------------------------- ArubaOS-CX Primary Image --------------------------------------------------------------------------- Version : GL.10.06.0010 Size : 444 MB Date : 2020-12-14 11:55:16 PST SHA-256 : 4157d15a5cad6efce4d0e8b35f75b4d6212de5af0c5c9bf3ad8f74853df67733 --------------------------------------------------------------------------- ArubaOS-CX Secondary Image --------------------------------------------------------------------------- Version : GL.10.08.1021 Size : 473 MB Date : 2021-11-08 01:48:56 UTC SHA-256 : c16dc680333eaf72188061209e56cd24854cb291e6babe2333110ff6029e8227 Default Image : primary Boot Profile Timeout : 5 seconds ------------------------------------------------------ Management Module 1/1 (Active) ------------------------------------------------------ Active Image : primary Service OS Version : GL.01.08.0002 BIOS Version : GL-01-0013 After the firmware is uploaded to both VSX switches, you will need to start the software update from the VSX primary member.\nSince we uploaded to the secondary image, we choose that one to boot to.\nsw-spine-001# vsx update-software boot-bank secondary This will trigger the upgrade process on the VSX pair and it will start the dialogue explaining what will happen next, i.e. if any firmware/driver upgrades are needed (i.e. the unit would reboot twice if this was the case) and it will show you on the screen the current status of the upgrade process. in VSX upgrade process the secondary VSX member will always boot first.\nOnce software update is complete verify the image version on both switches.\nsw-spine-002# show version ----------------------------------------------------------------------------- ArubaOS-CX (c) Copyright 2017-2021 Hewlett Packard Enterprise Development LP ----------------------------------------------------------------------------- Version : GL.10.08.1021 Build Date : 2021-11-08 01:48:56 UTC Build ID : ArubaOS-CX:GL.10.08.1021:befed610d5e5:202111080115 Build SHA : befed610d5e59c29e3cfb6e163fa45af615a2bd3 Active Image : secondary Service OS Version : GL.01.08.0002 BIOS Version : GL-01-0013 Mellanox Firmware Update SSH into the switch being upgraded.\nFetch the image from ncn-m001.\nsw-spine-001 [standalone: master] # image fetch scp://root@10.252.1.4/root/onyx-X86_64-3.9.3210.img Install the image.\nsw-spine-001 [standalone: master] # image install onyx-X86_64-3.9.3210.img Select the image to boot next.\nsw-spine-001 [standalone: master] # image boot next Write memory and reload.\nsw-spine-001 [standalone: master] # write memory sw-spine-001 [standalone: master] # reload Once the switch is available, verify the image is installed.\nsw-spine-001 [standalone: master] # show images Installed images: Partition 1: version: X86_64 3.9.0300 2020-02-26 19:25:24 x86_64 Partition 2: version: X86_64 3.9.1014 2020-08-05 18:06:58 x86_64 Last boot partition: 2 Next boot partition: 1 Images available to be installed: 1: Image : onyx-X86_64-3.9.1014.stable.img Version: X86_64 3.9.1014 2020-08-05 18:06:58 x86_64 Dell Firmware Update SSH into the switch being upgraded.\nFetch the image from ncn-m001.\nsw-leaf-001# image install http://10.252.1.4/fw/network/OS10_Enterprise_10.5.1.4.stable.tar Check the image upload status.\nsw-leaf-001# show image status Image Upgrade State: download ================================================== File Transfer State: download -------------------------------------------------- State Detail: In progress Task Start: 2021-02-08T21:24:14Z Task End: 0000-00-00T00:00:00Z Transfer Progress: 7 % Transfer Bytes: 40949640 bytes File Size: 604119040 bytes Transfer Rate: 869 kbps Reboot after the image is uploaded.\nsw-leaf-001# write memory sw-leaf-001# reload Once the switch is available, verify the image is installed.\nsw-leaf-001# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.4 Build Version: 10.5.1.4.249 "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/acl/",
	"title": "Configure Access Control Links (ACLs)",
	"tags": [],
	"description": "",
	"content": "Configure Access Control Links (ACLs) ACLs are used to help improve network performance and restrict network usage by creating policies to eliminate unwanted IP traffic by filtering packets where they enter the switch on layer 2 and layer 3 interfaces. An ACL is an ordered list of one or more access control list entries (ACEs) prioritized by sequence number. An incoming packet is matched sequentially against each entry in an ACL.\nConfiguration commands Create an ACL:\nswitch(config)# ip access-list name switch(conf-ipv4-acl)# permit ip 1.1.1.0/24 any Show commands to validate functionality:\nswitch# show ip access-list name Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/canu/canu_installation/",
	"title": "CANU Installation",
	"tags": [],
	"description": "",
	"content": "CANU Installation Prerequisites In order to run CANU, both python3 and pip3 must be installed.\nInstallation Install pip3, if it is not already installed.\nlinux# pip3 install --editable Install the development build of CANU.\nlinux# python3 setup.py develop --user Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/8021x/",
	"title": "802.1X",
	"tags": [],
	"description": "",
	"content": "802.1X IEEE 802.1X is an IEEE standard for port-based network access control (PNAC). This standard provides an authentication mechanism to devices wishing to attach to a LAN or WLAN. IEEE 802.1X defines the encapsulation of the Extensible Authentication Protocol (EAP) over IEEE 802, which is known as EAP over LAN (EAPOL).\nNOTE: Port security is a feature of \u0026ldquo;edge\u0026rdquo; switches such as 63/6400, and it is not available on 83xx.\nConfiguration Commands Enter 802.1X authenticator context:\nswitch(config)# aaa authentication port-access dot1x authenticator Enable 802.1X authentication:\nswitch(config-dot1x-auth)# enable Configure 802.1X authentication method:\nswitch(config-dot1x-auth)# chap-radius|eap-radius Configure RADIUS server group for 802.1X:\nswitch(config-dot1x-auth)# radius server-group NAME Enter 802.1X authenticator context on a port:\nswitch(config-if)# aaa authentication port-access dot1x authenticator Enable 802.1X authentication on a port:\nswitch(config-if-dot1x-auth)# enable Enable cached re-authentication on a port:\nswitch(config-if-dot1x-auth)# cached-reauth Configure cached re-authentication period on a port:\nswitch(config-if-dot1x-auth)# cached-reauth-period VALUE Configure maximum authentication attempts on a port:\nswitch(config-if-dot1x-auth)# max-retries VALUE Configure quiet period on a port:\nswitch(config-if-dot1x-auth)# quiet-period VALUE Enable periodic re-authentication on a port:\nswitch(config-if-dot1x-auth)# reauth Configure re-authentication period on a port:\nswitch(config-if-dot1x-auth)# reauth-period VALUE Configure discovery period on a port:\nswitch(config-if-dot1x-auth)# discovery-period VALUE Configure EAPOL timeout on a port:\nswitch(config-if-dot1x-auth)# eapol-timeout VALUE Configure maximum EAPOL requests on a port:\nswitch(config-if-dot1x-auth)# max-eapol-requests VALUE Configure force authorized on a port:\nswitch(config-if-dot1x-auth)# authorized Show commands to validate functionality: :\nswitch# show aaa authentication port-access dot1x authenticator interface \u0026lt;IFACE|all\u0026gt; \u0026lt;port-statistics|client-status\u0026gt; [mac MAC-ADDR] Expected Results Expected outcomes following configuration:\nAdministrators can enable dot1x authentication Administrators are able to authenticate using the specified dot1x authentication method The output of the show commands looks correct Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/1.0_to_1.2_upgrade/",
	"title": "Management Network 1.0 (1.2 Preconfig) to 1.2",
	"tags": [],
	"description": "",
	"content": "Management Network 1.0 (1.2 Preconfig) to 1.2 Prerequisites Mellanox Mellanox Manual Configuration Dell Dell Manual Configuration Aruba Spine Aruba Manual Configuration Aruba Leaf and Leaf BMC Prerequisites System is already running with CANU-generated 1.0 configurations (1.2 Preconfig). Generated switch configurations for 1.2. Generate Switch Configurations CANU installed with version 1.1.11 or greater. Run canu --version to see version. If doing a CSM install or upgrade, a CANU RPM is located in the release tarball. For more information, see Update CANU From CSM Tarball Be sure that the current connection to the system is not through the Spine switches.\nIf it is, then performing this upgrade will cause the connection to the system to be lost.\nCheck the default route from the NCN that has the site connection.\nncn-m001# ip r default via 10.102.3.1 dev vlan007 If the default route is out through the site connection, then skip the rest of the procedure. A default route going out through the site connection looks similar to the following:\ndefault via 172.30.48.1 dev lan0 If the default route is through dev vlan007 or the CAN VLAN, this needs to change in order to prevent the connection loss when moving this VLAN to the Customer VRF on the switches.\nIn this example the site connection is on lan0\nncn-m001# ip a show lan0 29: lan0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether b4:2e:99:3a:26:08 brd ff:ff:ff:ff:ff:ff inet 172.30.52.183/20 brd 172.30.63.255 scope global lan0 valid_lft forever preferred_lft forever inet6 fe80::b62e:99ff:fe3a:2608/64 scope link valid_lft forever preferred_lft forever The default route needs to replaced to route out lan0 Replace the default route with the correct next-hop router for this network.\nncn-m001# ip route replace default via 172.30.48.1 Mellanox Use CANU Validate to see the differences between the 1.0 and 1.2 switch configurations.\nncn-m001# canu validate switch config --running ./1.0/sw-spine-002.cfg --generated ./1.2/sw-spine-002.cfg --vendor mellanox --remediation Output\n- vlan 7 name \u0026#34;CAN\u0026#34; - route-map ncn-w003 permit 10 match ip address pl-can - route-map ncn-w002 permit 10 match ip address pl-can - route-map ncn-w001 permit 10 match ip address pl-can - ip prefix-list pl-can seq 10 permit 10.102.3.0 /25 ge 25 - ip prefix-list pl-can - interface vlan 7 ip dhcp relay instance 2 downstream - interface vlan 7 ip address 10.102.3.3/25 primary - interface vlan 7 - banner motd \u0026#34; ############################################################################### # CSM version: 1.0 # CANU version: 1.1.11 ############################################################################### \u0026#34; + vrf definition Customer rd 7:7 + vrf definition Customer + vlan 7 name \u0026#34;CMN\u0026#34; + vlan 6 name \u0026#34;CAN\u0026#34; + vlan 6 + router ospf 2 vrf Customer router-id 10.2.0.3 + router ospf 2 vrf Customer default-information originate + router ospf 2 vrf Customer + router bgp 65533 vrf Customer router-id 10.2.0.3 force + router bgp 65533 vrf Customer neighbor 10.102.3.9 transport connection-mode passive + router bgp 65533 vrf Customer neighbor 10.102.3.9 timers 1 3 + router bgp 65533 vrf Customer neighbor 10.102.3.9 remote-as 65534 + router bgp 65533 vrf Customer neighbor 10.102.3.8 transport connection-mode passive + router bgp 65533 vrf Customer neighbor 10.102.3.8 timers 1 3 + router bgp 65533 vrf Customer neighbor 10.102.3.8 remote-as 65534 + router bgp 65533 vrf Customer neighbor 10.102.3.10 transport connection-mode passive + router bgp 65533 vrf Customer neighbor 10.102.3.10 timers 1 3 + router bgp 65533 vrf Customer neighbor 10.102.3.10 remote-as 65534 + router bgp 65533 vrf Customer maximum-paths ibgp 32 + router bgp 65533 vrf Customer maximum-paths 32 + router bgp 65533 vrf Customer distance 20 70 20 + router bgp 65533 vrf Customer + route-map ncn-w003 permit 10 match ip address pl-cmn + route-map ncn-w002 permit 10 match ip address pl-cmn + route-map ncn-w001 permit 10 match ip address pl-cmn + ipv4 access-list cmn-can seq-number 30 permit ip any any + ipv4 access-list cmn-can seq-number 20 deny ip 10.102.3.128 mask 255.255.255.192 10.102.3.0 mask 255.255.255.128 + ipv4 access-list cmn-can seq-number 10 deny ip 10.102.3.0 mask 255.255.255.128 10.102.3.128 mask 255.255.255.192 + ipv4 access-list cmn-can bind-point rif + ipv4 access-list cmn-can + ip routing vrf Customer + ip prefix-list pl-cmn seq 10 permit 10.102.3.0 /25 ge 25 + ip prefix-list pl-cmn + interface vlan 7 vrf forwarding Customer + interface vlan 7 ipv4 port access-group cmn-can + interface vlan 7 ip ospf area 0.0.0.0 + interface vlan 7 ip address 10.102.3.99/25 primary + interface vlan 6 vrf forwarding Customer + interface vlan 6 mtu 9184 + interface vlan 6 magp 5 ip virtual-router mac-address 00:00:5E:00:01:05 + interface vlan 6 magp 5 ip virtual-router address 10.102.3.129 + interface vlan 6 magp 5 + interface vlan 6 ipv4 port access-group cmn-can + interface vlan 6 ip address 10.102.3.131/26 primary + interface mlag-port-channel 9 switchport hybrid allowed-vlan add 6 + interface mlag-port-channel 8 switchport hybrid allowed-vlan add 6 + interface mlag-port-channel 7 switchport hybrid allowed-vlan add 6 + interface mlag-port-channel 6 switchport hybrid allowed-vlan add 6 + interface mlag-port-channel 5 switchport hybrid allowed-vlan add 6 + interface mlag-port-channel 4 switchport hybrid allowed-vlan add 6 + interface mlag-port-channel 3 switchport hybrid allowed-vlan add 6 + interface mlag-port-channel 2 switchport hybrid allowed-vlan add 6 + interface mlag-port-channel 151 switchport hybrid allowed-vlan add 7 + interface mlag-port-channel 10 switchport hybrid allowed-vlan add 7 + interface mlag-port-channel 1 switchport hybrid allowed-vlan add 6 + banner motd \u0026#34; ############################################################################### # CSM version: 1.2 # CANU version: 1.1.11 ############################################################################### \u0026#34; ------------------------------------------------------------------------- Config differences between running config and generated config lines that start with a minus \u0026#34;-\u0026#34; and RED: Config that is present in running config but not in generated config lines that start with a plus \u0026#34;+\u0026#34; and GREEN: Config that is present in generated config but not in running config. ------------------------------------------------------------------------- Remediation Config ------------------------------------------------------------------------- banner motd \u0026#34; ############################################################################### # CSM version: 1.2 # CANU version: 1.1.11 ############################################################################### \u0026#34; vlan 6 vlan 7 name \u0026#34;CMN\u0026#34; vlan 6 name \u0026#34;CAN\u0026#34; interface mlag-port-channel 1 switchport hybrid allowed-vlan add 6 interface mlag-port-channel 2 switchport hybrid allowed-vlan add 6 interface mlag-port-channel 3 switchport hybrid allowed-vlan add 6 interface mlag-port-channel 4 switchport hybrid allowed-vlan add 6 interface mlag-port-channel 5 switchport hybrid allowed-vlan add 6 interface mlag-port-channel 6 switchport hybrid allowed-vlan add 6 interface mlag-port-channel 7 switchport hybrid allowed-vlan add 6 interface mlag-port-channel 8 switchport hybrid allowed-vlan add 6 interface mlag-port-channel 9 switchport hybrid allowed-vlan add 6 interface mlag-port-channel 10 switchport hybrid allowed-vlan add 7 interface mlag-port-channel 151 switchport hybrid allowed-vlan add 7 vrf definition Customer vrf definition Customer rd 7:7 ip routing vrf Customer interface vlan 7 vrf forwarding Customer interface vlan 6 vrf forwarding Customer interface vlan 7 ip address 10.102.3.99/25 primary interface vlan 6 ip address 10.102.3.131/26 primary no interface vlan 6 ip icmp redirect interface vlan 6 mtu 9184 ipv4 access-list cmn-can ipv4 access-list cmn-can bind-point rif ipv4 access-list cmn-can seq-number 10 deny ip 10.102.3.0 mask 255.255.255.128 10.102.3.128 mask 255.255.255.192 ipv4 access-list cmn-can seq-number 20 deny ip 10.102.3.128 mask 255.255.255.192 10.102.3.0 mask 255.255.255.128 ipv4 access-list cmn-can seq-number 30 permit ip any any interface vlan 7 ipv4 port access-group cmn-can interface vlan 6 ipv4 port access-group cmn-can router ospf 2 vrf Customer router ospf 2 vrf Customer router-id 10.2.0.3 router ospf 2 vrf Customer default-information originate interface vlan 7 ip ospf area 0.0.0.0 interface vlan 6 magp 5 interface vlan 6 magp 5 ip virtual-router address 10.102.3.129 interface vlan 6 magp 5 ip virtual-router mac-address 00:00:5E:00:01:05 ip prefix-list pl-cmn ip prefix-list pl-cmn seq 10 permit 10.102.3.0 /25 ge 25 route-map ncn-w001 permit 10 match ip address pl-cmn route-map ncn-w002 permit 10 match ip address pl-cmn route-map ncn-w003 permit 10 match ip address pl-cmn router bgp 65533 vrf Customer router bgp 65533 vrf Customer router-id 10.2.0.3 force router bgp 65533 vrf Customer distance 20 70 20 router bgp 65533 vrf Customer maximum-paths ibgp 32 router bgp 65533 vrf Customer maximum-paths 32 router bgp 65533 vrf Customer neighbor 10.102.3.8 remote-as 65534 router bgp 65533 vrf Customer neighbor 10.102.3.9 remote-as 65534 router bgp 65533 vrf Customer neighbor 10.102.3.10 remote-as 65534 router bgp 65533 vrf Customer neighbor 10.102.3.8 timers 1 3 router bgp 65533 vrf Customer neighbor 10.102.3.9 timers 1 3 router bgp 65533 vrf Customer neighbor 10.102.3.10 timers 1 3 router bgp 65533 vrf Customer neighbor 10.102.3.8 transport connection-mode passive router bgp 65533 vrf Customer neighbor 10.102.3.9 transport connection-mode passive router bgp 65533 vrf Customer neighbor 10.102.3.10 transport connection-mode passive Take a close look at the output of this, make sure that all the changes are understood. Copy the remediation configuration into the terminal. sw-spine-002 [mlag-domain: master] (config) # banner motd \u0026#34; \u0026gt; ############################################################################### \u0026gt; # CSM version: 1.2 \u0026gt; # CANU version: 1.1.11 \u0026gt; ############################################################################### \u0026gt; \u0026#34; sw-spine-002 [mlag-domain: master] (config) # vlan 6 sw-spine-002 [mlag-domain: master] (config) # vlan 7 name \u0026#34;CMN\u0026#34; sw-spine-002 [mlag-domain: master] (config) # vlan 6 name \u0026#34;CAN\u0026#34; sw-spine-002 [mlag-domain: master] (config) # interface mlag-port-channel 1 switchport hybrid allowed-vlan add 6 sw-spine-002 [mlag-domain: master] (config) # interface mlag-port-channel 2 switchport hybrid allowed-vlan add 6 sw-spine-002 [mlag-domain: master] (config) # interface mlag-port-channel 3 switchport hybrid allowed-vlan add 6 sw-spine-002 [mlag-domain: master] (config) # interface mlag-port-channel 4 switchport hybrid allowed-vlan add 6 sw-spine-002 [mlag-domain: master] (config) # interface mlag-port-channel 5 switchport hybrid allowed-vlan add 6 sw-spine-002 [mlag-domain: master] (config) # interface mlag-port-channel 6 switchport hybrid allowed-vlan add 6 sw-spine-002 [mlag-domain: master] (config) # interface mlag-port-channel 7 switchport hybrid allowed-vlan add 6 sw-spine-002 [mlag-domain: master] (config) # interface mlag-port-channel 8 switchport hybrid allowed-vlan add 6 sw-spine-002 [mlag-domain: master] (config) # interface mlag-port-channel 9 switchport hybrid allowed-vlan add 6 sw-spine-002 [mlag-domain: master] (config) # interface mlag-port-channel 10 switchport hybrid allowed-vlan add 7 sw-spine-002 [mlag-domain: master] (config) # interface mlag-port-channel 151 switchport hybrid allowed-vlan add 7 sw-spine-002 [mlag-domain: master] (config) # vrf definition Customer sw-spine-002 [mlag-domain: master] (config) # vrf definition Customer rd 7:7 sw-spine-002 [mlag-domain: master] (config) # ip routing vrf Customer sw-spine-002 [mlag-domain: master] (config) # interface vlan 7 vrf forwarding Customer sw-spine-002 [mlag-domain: master] (config) # interface vlan 6 vrf forwarding Customer sw-spine-002 [mlag-domain: master] (config) # interface vlan 7 ip address 10.102.3.99/25 primary sw-spine-002 [mlag-domain: master] (config) # interface vlan 6 ip address 10.102.3.131/26 primary sw-spine-002 [mlag-domain: master] (config) # no interface vlan 6 ip icmp redirect sw-spine-002 [mlag-domain: master] (config) # interface vlan 6 mtu 9184 sw-spine-002 [mlag-domain: master] (config) # ipv4 access-list cmn-can sw-spine-002 [mlag-domain: master] (config) # ipv4 access-list cmn-can bind-point rif sw-spine-002 [mlag-domain: master] (config) # ipv4 access-list cmn-can seq-number 10 deny ip 10.102.3.0 mask 255.255.255.128 10.102.3.128 mask 255.255.255.192 sw-spine-002 [mlag-domain: master] (config) # ipv4 access-list cmn-can seq-number 20 deny ip 10.102.3.128 mask 255.255.255.192 10.102.3.0 mask 255.255.255.128 sw-spine-002 [mlag-domain: master] (config) # ipv4 access-list cmn-can seq-number 30 permit ip any any sw-spine-002 [mlag-domain: master] (config) # interface vlan 7 ipv4 port access-group cmn-can sw-spine-002 [mlag-domain: master] (config) # interface vlan 6 ipv4 port access-group cmn-can sw-spine-002 [mlag-domain: master] (config) # router ospf 2 vrf Customer sw-spine-002 [mlag-domain: master] (config) # router ospf 2 vrf Customer router-id 10.2.0.3 sw-spine-002 [mlag-domain: master] (config) # router ospf 2 vrf Customer default-information originate sw-spine-002 [mlag-domain: master] (config) # interface vlan 7 ip ospf area 0.0.0.0 sw-spine-002 [mlag-domain: master] (config) # interface vlan 6 magp 5 sw-spine-002 [mlag-domain: master] (config) # interface vlan 6 magp 5 ip virtual-router address 10.102.3.129 sw-spine-002 [mlag-domain: master] (config) # interface vlan 6 magp 5 ip virtual-router mac-address 00:00:5E:00:01:05 sw-spine-002 [mlag-domain: master] (config) # ip prefix-list pl-cmn sw-spine-002 [mlag-domain: master] (config) # ip prefix-list pl-cmn seq 10 permit 10.102.3.0 /25 ge 25 sw-spine-002 [mlag-domain: master] (config) # route-map ncn-w001 permit 10 match ip address pl-cmn sw-spine-002 [mlag-domain: master] (config) # route-map ncn-w002 permit 10 match ip address pl-cmn sw-spine-002 [mlag-domain: master] (config) # route-map ncn-w003 permit 10 match ip address pl-cmn sw-spine-002 [mlag-domain: master] (config) # router bgp 65533 vrf Customer sw-spine-002 [mlag-domain: master] (config) # router bgp 65533 vrf Customer router-id 10.2.0.3 force sw-spine-002 [mlag-domain: master] (config) # router bgp 65533 vrf Customer distance 20 70 20 sw-spine-002 [mlag-domain: master] (config) # router bgp 65533 vrf Customer maximum-paths ibgp 32 sw-spine-002 [mlag-domain: master] (config) # router bgp 65533 vrf Customer maximum-paths 32 sw-spine-002 [mlag-domain: master] (config) # router bgp 65533 vrf Customer neighbor 10.102.3.8 remote-as 65534 sw-spine-002 [mlag-domain: master] (config) # router bgp 65533 vrf Customer neighbor 10.102.3.9 remote-as 65534 sw-spine-002 [mlag-domain: master] (config) # router bgp 65533 vrf Customer neighbor 10.102.3.10 remote-as 65534 sw-spine-002 [mlag-domain: master] (config) # router bgp 65533 vrf Customer neighbor 10.102.3.8 timers 1 3 sw-spine-002 [mlag-domain: master] (config) # router bgp 65533 vrf Customer neighbor 10.102.3.9 timers 1 3 sw-spine-002 [mlag-domain: master] (config) # router bgp 65533 vrf Customer neighbor 10.102.3.10 timers 1 3 sw-spine-002 [mlag-domain: master] (config) # router bgp 65533 vrf Customer neighbor 10.102.3.8 transport connection-mode passive sw-spine-002 [mlag-domain: master] (config) # router bgp 65533 vrf Customer neighbor 10.102.3.9 transport connection-mode passive sw-spine-002 [mlag-domain: master] (config) # router bgp 65533 vrf Customer neighbor 10.102.3.10 transport connection-mode passive This should copy into the terminal without any errors, if there are errors stop here and make sure that the generated configuration gets applied correctly.\nMellanox Manual Configuration Because we add the vlan7 interface to the Customer VRF, it removes all previous configuration. This will need to be fixed. ncn-m001# cat sw-spine-002.cfg| grep \u0026#34;interface vlan 7\u0026#34; interface vlan 7 vrf forwarding Customer interface vlan 7 ip address 10.102.4.51/25 primary no interface vlan 7 ip icmp redirect interface vlan 7 mtu 9184 interface vlan 7 ipv4 port access-group cmn-can interface vlan 7 ip ospf area 0.0.0.0 interface vlan 7 magp 4 interface vlan 7 magp 4 ip virtual-router address 10.102.4.1 interface vlan 7 magp 4 ip virtual-router mac-address 00:00:5E:00:01:04 sw-spine-002 [mlag-domain: master] (config) # interface vlan 7 vrf forwarding Customer sw-spine-002 [mlag-domain: master] (config) # interface vlan 7 ip address 10.102.4.51/25 primary sw-spine-002 [mlag-domain: master] (config) # no interface vlan 7 ip icmp redirect sw-spine-002 [mlag-domain: master] (config) # interface vlan 7 mtu 9184 sw-spine-002 [mlag-domain: master] (config) # interface vlan 7 ipv4 port access-group cmn-can sw-spine-002 [mlag-domain: master] (config) # interface vlan 7 ip ospf area 0.0.0.0 sw-spine-002 [mlag-domain: master] (config) # interface vlan 7 magp 4 sw-spine-002 [mlag-domain: master] (config) # interface vlan 7 magp 4 ip virtual-router address 10.102.4.1 sw-spine-002 [mlag-domain: master] (config) # interface vlan 7 magp 4 ip virtual-router mac-address 00:00:5E:00:01:04 Add site connections to Customer VRF.\nThe site connections are found in the SHCD. CAN switch cfcanb6s1 - 31 sw-25g01 x3000 u39 - j16 CAN switch cfcanb6s1 - 46 sw-25g02 x3000 u40 - j16 This example has the site connections on port 16 on both spine switches.\nGet the current configuration from port 16 on both switches. This needs to be done on both Spine switches. sw-spine-001 [mlag-domain: master] # show run int ethernet 1/16 interface ethernet 1/16 speed 10G force interface ethernet 1/16 mtu 1500 force interface ethernet 1/16 no switchport force interface ethernet 1/16 ip address 10.102.255.10/30 primary Attach the interface to the Customer VRF. The IP address will need to be added back, because that configuration is wiped when a VRF gets attached to an interface. sw-spine-001 [mlag-domain: master] (config) # int ethernet 1/16 vrf forwarding Customer sw-spine-001 [mlag-domain: master] (config) # interface ethernet 1/16 ip address 10.102.255.10/30 primary Add the default route to the Customer VRF and delete the old one. sw-spine-001 [mlag-domain: master] (config) # show run | include \u0026#34;ip route\u0026#34; ip route vrf default 0.0.0.0/0 10.102.255.9 sw-spine-001 [mlag-domain: master] (config) # no ip route vrf default 0.0.0.0/0 sw-spine-001 [mlag-domain: master] (config) # ip route vrf Customer 0.0.0.0/0 10.102.255.9 Save this configuration to a new configuration file with the CSM and CANU versions. sw-spine-001 [mlag-domain: master] (config) # show banner Banners: Message of the Day (MOTD): ############################################################################### # CSM version: 1.2 # CANU version: 1.1.11 ############################################################################### Login: NVIDIA Onyx Switch Management Logout: sw-spine-001 [mlag-domain: master] (config) # configuration write to csm1.2-canu1.1.11 Dell Use canu validate to see the differences between the 1.0 and 1.2 switch configurations. ncn-m001# canu validate switch config --running 1.0/sw-leaf-bmc-001.cfg --generated 1.2/sw-leaf-bmc-001.cfg --vendor dell --remediation + ip vrf Customer + interface vlan7 + description CMN + no shutdown + ip vrf forwarding Customer + mtu 9216 + ip address 10.102.3.100/25 + ip access-group cmn-can in + ip access-group cmn-can out + ip ospf 2 area 0.0.0.0 interface port-channel100 - switchport trunk allowed vlan 2,4 + switchport trunk allowed vlan 2,4,7 + ip access-list cmn-can + seq 10 deny ip 10.102.3.0/25 10.102.3.128/26 + seq 20 deny ip 10.102.3.128/26 10.102.3.0/25 + seq 30 permit ip any any + router ospf 2 vrf Customer + router-id 10.2.0.4 - banner motd ^ ############################################################################### # CSM version: 1.0 # CANU version: 1.1.11 ############################################################################### ^ + banner motd ^ ############################################################################### # CSM version: 1.2 # CANU version: 1.1.11 ############################################################################### ^ ------------------------------------------------------------------------- Config differences between running config and generated config lines that start with a minus \u0026#34;-\u0026#34; and RED: Config that is present in running config but not in generated config lines that start with a plus \u0026#34;+\u0026#34; and GREEN: Config that is present in generated config but not in running config. ------------------------------------------------------------------------- Remediation Config ------------------------------------------------------------------------- no banner motd ip vrf Customer interface vlan7 description CMN no shutdown ip vrf forwarding Customer mtu 9216 ip address 10.102.3.100/25 ip access-group cmn-can in ip access-group cmn-can out ip ospf 2 area 0.0.0.0 interface port-channel100 switchport trunk allowed vlan 2,4,7 ip access-list cmn-can seq 10 deny ip 10.102.3.0/25 10.102.3.128/26 seq 20 deny ip 10.102.3.128/26 10.102.3.0/25 seq 30 permit ip any any router ospf 2 vrf Customer router-id 10.2.0.4 banner motd ^ ############################################################################### # CSM version: 1.2 # CANU version: 1.1.11 ############################################################################### ^ Dell Manual Configuration Exit the ip vrf Customer sub-menu.\nsw-leaf-bmc-001(config)# ip vrf Customer sw-leaf-bmc-001(conf-vrf)# exit banner motd will need to be applied manually.\nExample:\nsw-leaf-bmc-001(config)# no banner motd sw-leaf-bmc-001(config)# ip vrf Customer sw-leaf-bmc-001(conf-vrf)# exit sw-leaf-bmc-001(config)# interface vlan7 sw-leaf-bmc-001(conf-if-vl-7)# description CMN sw-leaf-bmc-001(conf-if-vl-7)# no shutdown sw-leaf-bmc-001(conf-if-vl-7)# ip vrf forwarding Customer sw-leaf-bmc-001(conf-if-vl-7)# mtu 9216 sw-leaf-bmc-001(conf-if-vl-7)# ip address 10.102.3.100/25 sw-leaf-bmc-001(conf-if-vl-7)# ip access-group cmn-can in sw-leaf-bmc-001(conf-if-vl-7)# ip access-group cmn-can out sw-leaf-bmc-001(conf-if-vl-7)# ip ospf 2 area 0.0.0.0 sw-leaf-bmc-001(conf-if-vl-7)# interface port-channel100 sw-leaf-bmc-001(conf-if-po-100)# switchport trunk allowed vlan 2,4,7 sw-leaf-bmc-001(conf-if-po-100)# ip access-list cmn-can sw-leaf-bmc-001(config-ipv4-acl)# seq 10 deny ip 10.102.3.0/25 10.102.3.128/26 sw-leaf-bmc-001(config-ipv4-acl)# seq 20 deny ip 10.102.3.128/26 10.102.3.0/25 sw-leaf-bmc-001(config-ipv4-acl)# seq 30 permit ip any any sw-leaf-bmc-001(config-ipv4-acl)# router ospf 2 vrf Customer sw-leaf-bmc-001(config-router-ospf-2)# router-id 10.2.0.4 sw-leaf-bmc-001(config-router-ospf-2)# banner motd ^ Enter TEXT message. End with the character \u0026#39;^\u0026#39;. ############################################################################### # CSM version: 1.2 # CANU version: 1.1.11 ############################################################################### ^ Save configuration.\nsw-leaf-bmc-001(config)# copy config://startup.xml config://csm1.2-canu1.1.11 Copy completed Aruba Spine Use canu validate to see the differences between the 1.0 and 1.2 switch configurations. ncn-m001# canu validate switch config --running ./1.0/sw-spine-002.cfg --generated ./1.2/sw-spine-002.cfg --vendor aruba --remediation + vrf Customer + ssh server vrf Customer access-list ip mgmt - 10 comment ALLOW SSH, HTTPS, AND SNMP ON HMN SUBNET + 10 comment ALLOW SSH, HTTPS, AND SNMP ON HMN SUBNET and CMN + 60 permit tcp 10.103.11.0/255.255.255.128 any eq ssh + 70 permit tcp 10.103.11.0/255.255.255.128 any eq https + 80 permit udp 10.103.11.0/255.255.255.128 any eq snmp + 90 permit udp 10.103.11.0/255.255.255.128 any eq snmp-trap - 60 comment ALLOW SNMP FROM HMN METALLB SUBNET + 100 comment ALLOW SNMP FROM HMN METALLB SUBNET - 70 permit udp 10.94.100.0/255.255.255.0 any eq snmp + 110 permit udp 10.94.100.0/255.255.255.0 any eq snmp - 80 permit udp 10.94.100.0/255.255.255.0 any eq snmp-trap + 120 permit udp 10.94.100.0/255.255.255.0 any eq snmp-trap - 90 comment BLOCK SSH, HTTPS, AND SNMP FROM EVERYWHERE ELSE + 130 comment BLOCK SSH, HTTPS, AND SNMP FROM EVERYWHERE ELSE - 100 deny tcp any any eq ssh + 140 deny tcp any any eq ssh - 110 deny tcp any any eq https + 150 deny tcp any any eq https - 120 deny udp any any eq snmp + 160 deny udp any any eq snmp - 130 deny udp any any eq snmp-trap + 170 deny udp any any eq snmp-trap - 140 comment ALLOW ANYTHING ELSE + 180 comment ALLOW ANYTHING ELSE - 150 permit any any any + 190 permit any any any + access-list ip cmn-can + 10 deny any 10.103.11.0/255.255.255.128 10.103.11.128/255.255.255.192 + 20 deny any 10.103.11.128/255.255.255.192 10.103.11.0/255.255.255.128 + 30 permit any any any vlan 7 + name CMN + apply access-list ip cmn-can in + apply access-list ip cmn-can out + vlan 6 name CAN + apply access-list ip cmn-can in + apply access-list ip cmn-can out interface lag 1 multi-chassis + vlan trunk allowed 1-2,4,6-7 + interface lag 3 multi-chassis + vlan trunk allowed 1-2,4,6-7 + interface lag 5 multi-chassis + vlan trunk allowed 1-2,4,6-7 + interface lag 7 multi-chassis + vlan trunk allowed 1-2,4,6-7 + interface lag 8 multi-chassis + vlan trunk allowed 1-2,4,6-7 + interface lag 9 multi-chassis + vlan trunk allowed 1-2,4,6-7 + interface lag 11 multi-chassis + vlan trunk allowed 1-2,4,6-7 + interface lag 13 multi-chassis + vlan trunk allowed 1-2,4,6-7 + interface lag 15 multi-chassis + vlan trunk allowed 1-2,4,6-7 + interface lag 151 multi-chassis vlan trunk allowed 1-2,4,7 - interface lag 3 multi-chassis - vlan trunk allowed 1-2,4,7 - interface lag 5 multi-chassis - vlan trunk allowed 1-2,4,7 - interface lag 7 multi-chassis - vlan trunk allowed 1-2,4,7 - interface lag 8 multi-chassis - vlan trunk allowed 1-2,4,7 - interface lag 9 multi-chassis - vlan trunk allowed 1-2,4,7 - interface lag 11 multi-chassis - vlan trunk allowed 1-2,4,7 - interface lag 13 multi-chassis - vlan trunk allowed 1-2,4,7 - interface lag 15 multi-chassis - vlan trunk allowed 1-2,4,7 - interface lag 151 multi-chassis - vlan trunk allowed 1-2,4 - banner motd ^ ############################################################################### # CSM version: 1.0 # CANU version: 1.1.20~develop ############################################################################### ^ + banner motd ^ ############################################################################### # CSM version: 1.2 # CANU version: 1.1.11 ############################################################################### ^ route-map ncn-w003 permit seq 40 - match ip address prefix-list pl-can + match ip address prefix-list pl-cmn - set ip next-hop 10.103.11.8 + set ip next-hop 10.103.11.39 route-map ncn-w002 permit seq 40 - match ip address prefix-list pl-can + match ip address prefix-list pl-cmn - set ip next-hop 10.103.11.9 + set ip next-hop 10.103.11.40 route-map ncn-w001 permit seq 40 - match ip address prefix-list pl-can + match ip address prefix-list pl-cmn - set ip next-hop 10.103.11.10 + set ip next-hop 10.103.11.41 interface vlan 7 + vrf attach Customer + description CMN + ip ospf 2 area 0.0.0.0 + interface vlan 6 + vrf attach Customer description CAN - ip helper-address 10.92.100.222 + ip mtu 9198 + ip address 10.103.11.131/26 + active-gateway ip mac 12:00:00:00:6b:00 + active-gateway ip 10.103.11.129 - ip dns server-address 10.92.100.74 + ip dns server-address 10.92.100.225 - ip prefix-list pl-can seq 10 permit 10.103.11.0/25 ge 25 + ip prefix-list pl-cmn seq 10 permit 10.103.11.0/25 ge 25 + router ospf 2 vrf Customer + router-id 10.2.0.3 + default-information originate + area 0.0.0.0 + router bgp 65533 + vrf Customer + bgp router-id 10.2.0.3 + maximum-paths 8 + timers bgp 1 3 + distance bgp 20 70 + neighbor 10.103.11.2 remote-as 65533 + neighbor 10.103.11.39 remote-as 65532 + neighbor 10.103.11.39 passive + neighbor 10.103.11.40 remote-as 65532 + neighbor 10.103.11.40 passive + neighbor 10.103.11.41 remote-as 65532 + neighbor 10.103.11.41 passive + address-family ipv4 unicast + neighbor 10.103.11.2 activate + neighbor 10.103.11.39 activate + neighbor 10.103.11.40 activate + neighbor 10.103.11.41 activate + https-server vrf Customer ------------------------------------------------------------------------- Config differences between running config and generated config lines that start with a minus \u0026#34;-\u0026#34; and RED: Config that is present in running config but not in generated config lines that start with a plus \u0026#34;+\u0026#34; and GREEN: Config that is present in generated config but not in running config. ------------------------------------------------------------------------- Remediation Config no ip dns server-address 10.92.100.74 no ip prefix-list pl-can seq 10 permit 10.103.11.0/25 ge 25 banner motd ^ ############################################################################### # CSM version: 1.2 # CANU version: 1.1.11 ############################################################################### ^ vrf Customer ssh server vrf Customer access-list ip mgmt no 10 comment ALLOW SSH, HTTPS, AND SNMP ON HMN SUBNET no 60 comment ALLOW SNMP FROM HMN METALLB SUBNET no 70 permit udp 10.94.100.0/255.255.255.0 any eq snmp no 80 permit udp 10.94.100.0/255.255.255.0 any eq snmp-trap no 90 comment BLOCK SSH, HTTPS, AND SNMP FROM EVERYWHERE ELSE no 100 deny tcp any any eq ssh no 110 deny tcp any any eq https no 120 deny udp any any eq snmp no 130 deny udp any any eq snmp-trap no 140 comment ALLOW ANYTHING ELSE no 150 permit any any any 10 comment ALLOW SSH, HTTPS, AND SNMP ON HMN SUBNET and CMN 60 permit tcp 10.103.11.0/255.255.255.128 any eq ssh 70 permit tcp 10.103.11.0/255.255.255.128 any eq https 80 permit udp 10.103.11.0/255.255.255.128 any eq snmp 90 permit udp 10.103.11.0/255.255.255.128 any eq snmp-trap 100 comment ALLOW SNMP FROM HMN METALLB SUBNET 110 permit udp 10.94.100.0/255.255.255.0 any eq snmp 120 permit udp 10.94.100.0/255.255.255.0 any eq snmp-trap 130 comment BLOCK SSH, HTTPS, AND SNMP FROM EVERYWHERE ELSE 140 deny tcp any any eq ssh 150 deny tcp any any eq https 160 deny udp any any eq snmp 170 deny udp any any eq snmp-trap 180 comment ALLOW ANYTHING ELSE 190 permit any any any access-list ip cmn-can 10 deny any 10.103.11.0/255.255.255.128 10.103.11.128/255.255.255.192 20 deny any 10.103.11.128/255.255.255.192 10.103.11.0/255.255.255.128 30 permit any any any vlan 7 name CMN apply access-list ip cmn-can in apply access-list ip cmn-can out vlan 6 name CAN apply access-list ip cmn-can in apply access-list ip cmn-can out interface lag 1 multi-chassis no vlan trunk allowed 1-2,4,7 vlan trunk allowed 1-2,4,6-7 interface lag 3 multi-chassis no vlan trunk allowed 1-2,4,7 vlan trunk allowed 1-2,4,6-7 interface lag 5 multi-chassis no vlan trunk allowed 1-2,4,7 vlan trunk allowed 1-2,4,6-7 interface lag 7 multi-chassis no vlan trunk allowed 1-2,4,7 vlan trunk allowed 1-2,4,6-7 interface lag 8 multi-chassis no vlan trunk allowed 1-2,4,7 vlan trunk allowed 1-2,4,6-7 interface lag 9 multi-chassis no vlan trunk allowed 1-2,4,7 vlan trunk allowed 1-2,4,6-7 interface lag 11 multi-chassis no vlan trunk allowed 1-2,4,7 vlan trunk allowed 1-2,4,6-7 interface lag 13 multi-chassis no vlan trunk allowed 1-2,4,7 vlan trunk allowed 1-2,4,6-7 interface lag 15 multi-chassis no vlan trunk allowed 1-2,4,7 vlan trunk allowed 1-2,4,6-7 interface lag 151 multi-chassis no vlan trunk allowed 1-2,4 vlan trunk allowed 1-2,4,7 interface vlan 7 no ip helper-address 10.92.100.222 vrf attach Customer description CMN ip ospf 2 area 0.0.0.0 interface vlan 6 vrf attach Customer description CAN ip mtu 9198 ip address 10.103.11.131/26 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 10.103.11.129 ip dns server-address 10.92.100.225 ip prefix-list pl-cmn seq 10 permit 10.103.11.0/25 ge 25 route-map ncn-w003 permit seq 40 no match ip address prefix-list pl-can no set ip next-hop 10.103.11.8 match ip address prefix-list pl-cmn set ip next-hop 10.103.11.39 route-map ncn-w002 permit seq 40 no match ip address prefix-list pl-can no set ip next-hop 10.103.11.9 match ip address prefix-list pl-cmn set ip next-hop 10.103.11.40 route-map ncn-w001 permit seq 40 no match ip address prefix-list pl-can no set ip next-hop 10.103.11.10 match ip address prefix-list pl-cmn set ip next-hop 10.103.11.41 router ospf 2 vrf Customer router-id 10.2.0.3 default-information originate area 0.0.0.0 router bgp 65533 vrf Customer bgp router-id 10.2.0.3 maximum-paths 8 timers bgp 1 3 distance bgp 20 70 neighbor 10.103.11.2 remote-as 65533 neighbor 10.103.11.39 remote-as 65532 neighbor 10.103.11.39 passive neighbor 10.103.11.40 remote-as 65532 neighbor 10.103.11.40 passive neighbor 10.103.11.41 remote-as 65532 neighbor 10.103.11.41 passive address-family ipv4 unicast neighbor 10.103.11.2 activate neighbor 10.103.11.39 activate neighbor 10.103.11.40 activate neighbor 10.103.11.41 activate https-server vrf Customer Using the remediation config, apply prefix-list and route-maps first. sw-spine-002(config)# ip prefix-list pl-cmn seq 10 permit 10.103.11.0/25 ge 25 sw-spine-002(config)# route-map ncn-w003 permit seq 40 sw-spine-002(config-route-map-ncn-w003-40)# no match ip address prefix-list pl-can sw-spine-002(config-route-map-ncn-w003-40)# no set ip next-hop 10.103.11.8 sw-spine-002(config-route-map-ncn-w003-40)# match ip address prefix-list pl-cmn sw-spine-002(config-route-map-ncn-w003-40)# set ip next-hop 10.103.11.39 sw-spine-002(config-route-map-ncn-w003-40)# route-map ncn-w002 permit seq 40 sw-spine-002(config-route-map-ncn-w002-40)# no match ip address prefix-list pl-can sw-spine-002(config-route-map-ncn-w002-40)# no set ip next-hop 10.103.11.9 sw-spine-002(config-route-map-ncn-w002-40)# match ip address prefix-list pl-cmn sw-spine-002(config-route-map-ncn-w002-40)# set ip next-hop 10.103.11.40 sw-spine-002(config-route-map-ncn-w002-40)# route-map ncn-w001 permit seq 40 sw-spine-002(config-route-map-ncn-w001-40)# no match ip address prefix-list pl-can sw-spine-002(config-route-map-ncn-w001-40)# no set ip next-hop 10.103.11.10 sw-spine-002(config-route-map-ncn-w001-40)# match ip address prefix-list pl-cmn sw-spine-002(config-route-map-ncn-w001-40)# set ip next-hop 10.103.11.41 Copy in the remaining configuration. sw-spine-002(config-route-map-ncn-w001-40)# no ip dns server-address 10.92.100.74 sw-spine-002(config)# no ip prefix-list pl-can seq 10 permit 10.103.11.0/25 ge 25 sw-spine-002(config)# banner motd ^ sw-spine-002(config-banner-motd)# ############################################################################### sw-spine-002(config-banner-motd)# # CSM version: 1.2 sw-spine-002(config-banner-motd)# # CANU version: 1.1.11 sw-spine-002(config-banner-motd)# ############################################################################### sw-spine-002(config-banner-motd)# ^ sw-spine-002(config)# vrf Customer sw-spine-002(config-vrf)# ssh server vrf Customer sw-spine-002(config)# access-list ip mgmt sw-spine-002(config-acl-ip)# no 10 comment ALLOW SSH, HTTPS, AND SNMP ON HMN SUBNET sw-spine-002(config-acl-ip)# no 60 comment ALLOW SNMP FROM HMN METALLB SUBNET sw-spine-002(config-acl-ip)# no 70 permit udp 10.94.100.0/255.255.255.0 any eq snmp sw-spine-002(config-acl-ip)# no 80 permit udp 10.94.100.0/255.255.255.0 any eq snmp-trap sw-spine-002(config-acl-ip)# no 90 comment BLOCK SSH, HTTPS, AND SNMP FROM EVERYWHERE ELSE sw-spine-002(config-acl-ip)# no 100 deny tcp any any eq ssh sw-spine-002(config-acl-ip)# no 110 deny tcp any any eq https sw-spine-002(config-acl-ip)# no 120 deny udp any any eq snmp sw-spine-002(config-acl-ip)# no 130 deny udp any any eq snmp-trap sw-spine-002(config-acl-ip)# no 140 comment ALLOW ANYTHING ELSE sw-spine-002(config-acl-ip)# no 150 permit any any any sw-spine-002(config-acl-ip)# 10 comment ALLOW SSH, HTTPS, AND SNMP ON HMN SUBNET and CMN sw-spine-002(config-acl-ip)# 60 permit tcp 10.103.11.0/255.255.255.128 any eq ssh sw-spine-002(config-acl-ip)# 70 permit tcp 10.103.11.0/255.255.255.128 any eq https sw-spine-002(config-acl-ip)# 80 permit udp 10.103.11.0/255.255.255.128 any eq snmp sw-spine-002(config-acl-ip)# 90 permit udp 10.103.11.0/255.255.255.128 any eq snmp-trap sw-spine-002(config-acl-ip)# 100 comment ALLOW SNMP FROM HMN METALLB SUBNET sw-spine-002(config-acl-ip)# 110 permit udp 10.94.100.0/255.255.255.0 any eq snmp sw-spine-002(config-acl-ip)# 120 permit udp 10.94.100.0/255.255.255.0 any eq snmp-trap sw-spine-002(config-acl-ip)# 130 comment BLOCK SSH, HTTPS, AND SNMP FROM EVERYWHERE ELSE sw-spine-002(config-acl-ip)# 140 deny tcp any any eq ssh sw-spine-002(config-acl-ip)# 150 deny tcp any any eq https sw-spine-002(config-acl-ip)# 160 deny udp any any eq snmp sw-spine-002(config-acl-ip)# 170 deny udp any any eq snmp-trap sw-spine-002(config-acl-ip)# 180 comment ALLOW ANYTHING ELSE sw-spine-002(config-acl-ip)# 190 permit any any any sw-spine-002(config-acl-ip)# access-list ip cmn-can sw-spine-002(config-acl-ip)# 10 deny any 10.103.11.0/255.255.255.128 10.103.11.128/255.255.255.192 sw-spine-002(config-acl-ip)# 20 deny any 10.103.11.128/255.255.255.192 10.103.11.0/255.255.255.128 sw-spine-002(config-acl-ip)# 30 permit any any any sw-spine-002(config-acl-ip)# vlan 7 sw-spine-002(config-vlan-7)# name CMN sw-spine-002(config-vlan-7)# apply access-list ip cmn-can in sw-spine-002(config-vlan-7)# apply access-list ip cmn-can out sw-spine-002(config-vlan-7)# vlan 6 sw-spine-002(config-vlan-6)# name CAN sw-spine-002(config-vlan-6)# apply access-list ip cmn-can in sw-spine-002(config-vlan-6)# apply access-list ip cmn-can out sw-spine-002(config-vlan-6)# interface lag 1 multi-chassis sw-spine-002(config-lag-if)# vlan trunk allowed 1-2,4,6-7 sw-spine-002(config-lag-if)# interface lag 3 multi-chassis sw-spine-002(config-lag-if)# vlan trunk allowed 1-2,4,6-7 sw-spine-002(config-lag-if)# interface lag 5 multi-chassis sw-spine-002(config-lag-if)# vlan trunk allowed 1-2,4,6-7 sw-spine-002(config-lag-if)# interface lag 7 multi-chassis sw-spine-002(config-lag-if)# vlan trunk allowed 1-2,4,6-7 sw-spine-002(config-lag-if)# interface lag 8 multi-chassis sw-spine-002(config-lag-if)# vlan trunk allowed 1-2,4,6-7 sw-spine-002(config-lag-if)# interface lag 9 multi-chassis sw-spine-002(config-lag-if)# vlan trunk allowed 1-2,4,6-7 sw-spine-002(config-lag-if)# interface lag 11 multi-chassis sw-spine-002(config-lag-if)# vlan trunk allowed 1-2,4,6-7 sw-spine-002(config-lag-if)# interface lag 13 multi-chassis sw-spine-002(config-lag-if)# vlan trunk allowed 1-2,4,6-7 sw-spine-002(config-lag-if)# interface lag 15 multi-chassis sw-spine-002(config-lag-if)# vlan trunk allowed 1-2,4,6-7 sw-spine-002(config-lag-if)# interface lag 151 multi-chassis sw-spine-002(config-lag-if)# vlan trunk allowed 1-2,4,7 sw-spine-002(config-lag-if)# interface vlan 7 sw-spine-002(config-if-vlan)# no ip helper-address 10.92.100.222 sw-spine-002(config-if-vlan)# vrf attach Customer sw-spine-002(config-if-vlan)# description CMN sw-spine-002(config-if-vlan)# ip ospf 2 area 0.0.0.0 sw-spine-002(config-if-vlan)# interface vlan 6 sw-spine-002(config-if-vlan)# vrf attach Customer sw-spine-002(config-if-vlan)# description CAN sw-spine-002(config-if-vlan)# ip mtu 9198 sw-spine-002(config-if-vlan)# ip address 10.103.11.131/26 sw-spine-002(config-if-vlan)# active-gateway ip mac 12:00:00:00:6b:00 sw-spine-002(config-if-vlan)# active-gateway ip 10.103.11.129 sw-spine-002(config-if-vlan)# router ospf 2 vrf Customer sw-spine-002(config-ospf-2)# router-id 10.2.0.3 sw-spine-002(config-ospf-2)# default-information originate sw-spine-002(config-ospf-2)# area 0.0.0.0 sw-spine-002(config-ospf-2)# router bgp 65533 sw-spine-002(config-bgp)# vrf Customer sw-spine-002(config-bgp-vrf)# bgp router-id 10.2.0.3 sw-spine-002(config-bgp-vrf)# maximum-paths 8 sw-spine-002(config-bgp-vrf)# timers bgp 1 3 sw-spine-002(config-bgp-vrf)# distance bgp 20 70 sw-spine-002(config-bgp-vrf)# neighbor 10.103.11.2 remote-as 65533 sw-spine-002(config-bgp-vrf)# neighbor 10.103.11.39 remote-as 65532 sw-spine-002(config-bgp-vrf)# neighbor 10.103.11.39 passive sw-spine-002(config-bgp-vrf)# neighbor 10.103.11.40 remote-as 65532 sw-spine-002(config-bgp-vrf)# neighbor 10.103.11.40 passive sw-spine-002(config-bgp-vrf)# neighbor 10.103.11.41 remote-as 65532 sw-spine-002(config-bgp-vrf)# neighbor 10.103.11.41 passive sw-spine-002(config-bgp-vrf)# address-family ipv4 unicast sw-spine-002(config-bgp-vrf-ipv4-uc)# neighbor 10.103.11.2 activate sw-spine-002(config-bgp-vrf-ipv4-uc)# neighbor 10.103.11.39 activate sw-spine-002(config-bgp-vrf-ipv4-uc)# neighbor 10.103.11.40 activate sw-spine-002(config-bgp-vrf-ipv4-uc)# neighbor 10.103.11.41 activate sw-spine-002(config-bgp-vrf-ipv4-uc)# https-server vrf Customer Paste in the vlan 7 interface information. Aruba Manual Configuration Because we add the vlan7 interface to the Customer VRF, it removes all previous configuration. This will need to be fixed. Get the interface vlan 7 configuration\nncn-m001# grep -A 7 \u0026#34;interface vlan 7\u0026#34; sw-spine-002.cfg interface vlan 7 vrf attach Customer description CMN ip mtu 9198 ip address 10.103.11.3/25 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 10.103.11.1 ip ospf 2 area 0.0.0.0 Apply VLAN interface configuration to the switch.\nsw-spine-002(config)# interface vlan 7 sw-spine-002(config-if-vlan)# vrf attach Customer sw-spine-002(config-if-vlan)# description CMN sw-spine-002(config-if-vlan)# ip mtu 9198 sw-spine-002(config-if-vlan)# ip address 10.103.11.3/25 sw-spine-002(config-if-vlan)# active-gateway ip mac 12:00:00:00:6b:00 sw-spine-002(config-if-vlan)# active-gateway ip 10.103.11.1 sw-spine-002(config-if-vlan)# ip ospf 2 area 0.0.0.0 Add site connections to Customer VRF.\nThe site connections are in the SHCD. CAN switch cfcanb6s1 - 31 sw-25g01 x3000 u39 - j36 CAN switch cfcanb6s1 - 46 sw-25g02 x3000 u40 - j36 This example has the site connections on port 36 on both spine switches.\nAdd the interface to the Customer VRF and re-add the IP address. sw-spine-002(config)# show run interface 1/1/36 interface 1/1/36 no shutdown ip address 10.103.15.190/30 exit sw-spine-002(config)# int 1/1/36 sw-spine-002(config-if)# vrf attach Customer sw-spine-002(config-if)# ip address 10.103.15.190/30 Move the default route to the Customer VRF. sw-spine-002(config)# show run | include \u0026#34;ip route\u0026#34; ip route 0.0.0.0/0 10.103.15.189 sw-spine-002(config)# no ip route 0.0.0.0/0 10.103.15.189 sw-spine-002(config)# ip route 0.0.0.0/0 10.103.15.189 vrf Customer Save the configuration and create a checkpoint using the CSM version and the CANU version\nsw-spine-002(config)# show banner motd ############################################################################### # CSM version: 1.2 # CANU version: 1.1.11 ############################################################################### sw-spine-002(config)# write mem Copying configuration: [Success] sw-spine-002(config)# copy running-config checkpoint CSM1_2_CANU_1_1_11 Aruba Leaf and Leaf BMC Use CANU Validate to see the differences between the 1.0 and 1.2 switch configurations.\nncn-m001# canu validate switch config --running surtur/1.0/sw-leaf-bmc-001.cfg --generated surtur/1.2/sw-leaf-bmc-001.cfg --vendor aruba --remediation Remediation Config This feature is experimental and has limited testing. banner motd ^ ############################################################################### # CSM version: 1.2 # CANU version: 1.1.11 ############################################################################### ^ vrf Customer ssh server vrf Customer access-list ip mgmt no 10 comment ALLOW SSH, HTTPS, AND SNMP ON HMN SUBNET no 60 comment ALLOW SNMP FROM HMN METALLB SUBNET no 70 permit udp 10.94.100.0/255.255.255.0 any eq snmp no 80 permit udp 10.94.100.0/255.255.255.0 any eq snmp-trap no 90 comment BLOCK SSH, HTTPS, AND SNMP FROM EVERYWHERE ELSE no 100 deny tcp any any eq ssh no 110 deny tcp any any eq https no 120 deny udp any any eq snmp no 130 deny udp any any eq snmp-trap no 140 comment ALLOW ANYTHING ELSE no 150 permit any any any 10 comment ALLOW SSH, HTTPS, AND SNMP ON HMN SUBNET and CMN 60 permit tcp 10.103.11.0/255.255.255.128 any eq ssh 70 permit tcp 10.103.11.0/255.255.255.128 any eq https 80 permit udp 10.103.11.0/255.255.255.128 any eq snmp 90 permit udp 10.103.11.0/255.255.255.128 any eq snmp-trap 100 comment ALLOW SNMP FROM HMN METALLB SUBNET 110 permit udp 10.94.100.0/255.255.255.0 any eq snmp 120 permit udp 10.94.100.0/255.255.255.0 any eq snmp-trap 130 comment BLOCK SSH, HTTPS, AND SNMP FROM EVERYWHERE ELSE 140 deny tcp any any eq ssh 150 deny tcp any any eq https 160 deny udp any any eq snmp 170 deny udp any any eq snmp-trap 180 comment ALLOW ANYTHING ELSE 190 permit any any any access-list ip cmn-can 10 deny any 10.103.11.0/255.255.255.128 10.103.11.128/255.255.255.192 20 deny any 10.103.11.128/255.255.255.192 10.103.11.0/255.255.255.128 30 permit any any any vlan 7 name CMN apply access-list ip cmn-can in apply access-list ip cmn-can out interface lag 255 vlan trunk allowed 1-2,4,7 interface vlan 7 vrf attach Customer description CMN ip mtu 9198 ip address 10.103.11.4/25 ip ospf 2 area 0.0.0.0 router ospf 2 vrf Customer router-id 10.2.0.4 area 0.0.0.0 https-server vrf Customer Copy in the entire remediation configuration block.\nsw-leaf-bmc-001(config)# banner motd ^ sw-leaf-bmc-001(config-banner-motd)# ############################################################################### sw-leaf-bmc-001(config-banner-motd)# # CSM version: 1.2 sw-leaf-bmc-001(config-banner-motd)# # CANU version: 1.1.11 sw-leaf-bmc-001(config-banner-motd)# ############################################################################### sw-leaf-bmc-001(config-banner-motd)# ^ sw-leaf-bmc-001(config)# vrf Customer sw-leaf-bmc-001(config-vrf)# ssh server vrf Customer sw-leaf-bmc-001(config)# access-list ip mgmt sw-leaf-bmc-001(config-acl-ip)# no 10 comment ALLOW SSH, HTTPS, AND SNMP ON HMN SUBNET sw-leaf-bmc-001(config-acl-ip)# no 60 comment ALLOW SNMP FROM HMN METALLB SUBNET sw-leaf-bmc-001(config-acl-ip)# no 70 permit udp 10.94.100.0/255.255.255.0 any eq snmp sw-leaf-bmc-001(config-acl-ip)# no 80 permit udp 10.94.100.0/255.255.255.0 any eq snmp-trap sw-leaf-bmc-001(config-acl-ip)# no 90 comment BLOCK SSH, HTTPS, AND SNMP FROM EVERYWHERE ELSE sw-leaf-bmc-001(config-acl-ip)# no 100 deny tcp any any eq ssh sw-leaf-bmc-001(config-acl-ip)# no 110 deny tcp any any eq https sw-leaf-bmc-001(config-acl-ip)# no 120 deny udp any any eq snmp sw-leaf-bmc-001(config-acl-ip)# no 130 deny udp any any eq snmp-trap sw-leaf-bmc-001(config-acl-ip)# no 140 comment ALLOW ANYTHING ELSE sw-leaf-bmc-001(config-acl-ip)# no 150 permit any any any sw-leaf-bmc-001(config-acl-ip)# 10 comment ALLOW SSH, HTTPS, AND SNMP ON HMN SUBNET and CMN sw-leaf-bmc-001(config-acl-ip)# 60 permit tcp 10.103.11.0/255.255.255.128 any eq ssh sw-leaf-bmc-001(config-acl-ip)# 70 permit tcp 10.103.11.0/255.255.255.128 any eq https sw-leaf-bmc-001(config-acl-ip)# 80 permit udp 10.103.11.0/255.255.255.128 any eq snmp sw-leaf-bmc-001(config-acl-ip)# 90 permit udp 10.103.11.0/255.255.255.128 any eq snmp-trap sw-leaf-bmc-001(config-acl-ip)# 100 comment ALLOW SNMP FROM HMN METALLB SUBNET sw-leaf-bmc-001(config-acl-ip)# 110 permit udp 10.94.100.0/255.255.255.0 any eq snmp sw-leaf-bmc-001(config-acl-ip)# 120 permit udp 10.94.100.0/255.255.255.0 any eq snmp-trap sw-leaf-bmc-001(config-acl-ip)# 130 comment BLOCK SSH, HTTPS, AND SNMP FROM EVERYWHERE ELSE sw-leaf-bmc-001(config-acl-ip)# 140 deny tcp any any eq ssh sw-leaf-bmc-001(config-acl-ip)# 150 deny tcp any any eq https sw-leaf-bmc-001(config-acl-ip)# 160 deny udp any any eq snmp sw-leaf-bmc-001(config-acl-ip)# 170 deny udp any any eq snmp-trap sw-leaf-bmc-001(config-acl-ip)# 180 comment ALLOW ANYTHING ELSE sw-leaf-bmc-001(config-acl-ip)# 190 permit any any any sw-leaf-bmc-001(config-acl-ip)# access-list ip cmn-can sw-leaf-bmc-001(config-acl-ip)# 10 deny any 10.103.11.0/255.255.255.128 10.103.11.128/255.255.255.192 sw-leaf-bmc-001(config-acl-ip)# 20 deny any 10.103.11.128/255.255.255.192 10.103.11.0/255.255.255.128 sw-leaf-bmc-001(config-acl-ip)# 30 permit any any any sw-leaf-bmc-001(config-acl-ip)# vlan 7 sw-leaf-bmc-001(config-vlan-7)# name CMN sw-leaf-bmc-001(config-vlan-7)# apply access-list ip cmn-can in sw-leaf-bmc-001(config-vlan-7)# apply access-list ip cmn-can out sw-leaf-bmc-001(config-vlan-7)# interface lag 255 sw-leaf-bmc-001(config-lag-if)# vlan trunk allowed 1-2,4,7 sw-leaf-bmc-001(config-lag-if)# interface vlan 7 sw-leaf-bmc-001(config-if-vlan)# vrf attach Customer sw-leaf-bmc-001(config-if-vlan)# description CMN sw-leaf-bmc-001(config-if-vlan)# ip mtu 9198 sw-leaf-bmc-001(config-if-vlan)# ip address 10.103.11.4/25 sw-leaf-bmc-001(config-if-vlan)# ip ospf 2 area 0.0.0.0 sw-leaf-bmc-001(config-if-vlan)# router ospf 2 vrf Customer sw-leaf-bmc-001(config-ospf-2)# router-id 10.2.0.4 sw-leaf-bmc-001(config-ospf-2)# area 0.0.0.0 sw-leaf-bmc-001(config-ospf-2)# https-server vrf Customer This should copy into the terminal without any errors, if there are errors stop here and make sure that the generated configuration gets applied correctly.\nSave the running configuration and create a checkpoint using the CSM version and the CANU version.\nsw-leaf-bmc-001(config)# show banner motd ############################################################################### # CSM version: 1.2 # CANU version: 1.1.11 ############################################################################### sw-leaf-bmc-001(config)# write mem Copying configuration: [Success] sw-leaf-bmc-001(config)# copy running-config checkpoint CSM1_2_CANU_1_1_11 "
},
{
	"uri": "/docs-csm/en-12/operations/network/dns/dns/",
	"title": "Domain Name Service (DNS) Overview",
	"tags": [],
	"description": "",
	"content": "Domain Name Service (DNS) Overview DNS Architecture This diagram shows how the various components of the DNS infrastructure interact.\nDNS Components The DNS infrastructure is comprised of a number of components.\nUnbound (cray-dns-unbound) Unbound is a caching DNS resolver which is also used as the primary DNS server.\nThe DNS records served by Unbound include system component names (xnames), node hostnames, and service names. These records are read from the cray-dns-unbound ConfigMap which is populated by cray-dns-unbound-manager.\nThe DNS server functionality will be migrated to PowerDNS in a future release leaving Unbound acting purely as a caching DNS resolver.\nUnbound also forwards queries to PowerDNS or the site DNS server if the query cannot be answered by local data.\nUnbound Manager (cray-dns-unbound-manager) The cray-dns-unbound-manager cron job runs every three minutes and queries the System Layout Service (SLS), the Hardware State Manager (HSM), and the Kea DHCP server for new or changed hardware components and creates DNS records for these components in the cray-dns-unbound ConfigMap.\nThis job also initiates a rolling restart of Unbound if the cray-dns-unbound ConfigMap was modified.\nKubernetes DNS (coredns) Kubernetes creates DNS records for services and pods. A CoreDNS server running in the kube-system namespace is used for this purpose.\nThe CoreDNS service is also configured to forward DNS requests to Unbound in order to allow pods to resolve system hardware components and other services. This configuration is performed by the cray-dns-unbound-coredns job which is invoked whenever the cray-dns-unbound Helm chart is deployed or upgraded.\nSee the Kubernetes documentation for more information.\nExternalDNS (cray-externaldns-external-dns) ExternalDNS creates DNS records for services that are intended to be accessible via the Customer Access Network (CAN), Customer Management Network (CMN), and Customer High-Speed Network (CHN). For example, grafana.cmn.wasp.dev.cray.com.\nKubernetes Services annotated with external-dns.alpha.kubernetes.io/hostname have DNS records created.\nStarting with CSM version 1.2 these DNS records are created in the PowerDNS server. Earlier versions of CSM used a dedicated CoreDNS server for ExternalDNS.\nOnly DNS A records are created as ExternalDNS currently does not support the creation of the PTR records required for reverse lookup.\nPowerDNS (cray-dns-powerdns) PowerDNS is an authoritative DNS server which over the next few CSM releases will replace Unbound as the primary DNS server within a CSM system.\nPowerDNS is able to respond to queries for services accessible via the CAN, CMN, or CHN. Records are externally accessible via the Kubernetes LoadBalancer IP address specified for the CSI --cmn-external-dns option.\nAs with earlier CSM releases it is possible to delegate to PowerDNS to resolve services and it is also possible to configure zone transfer to sync the DNS records from PowerDNS to Site DNS.\nPowerDNS Manager (cray-powerdns-manager) The PowerDNS Manager serves a similar purpose to the Unbound Manager. It runs in the background and periodically queries the SLS, HSM, and the Kea DHCP server for new or changed hardware components and creates DNS records for these components in PowerDNS.\nThe PowerDNS Manager also configures the PowerDNS server for zone transfer and DNSSEC if required.\nSite DNS This term is used to refer the external DNS server specified the CSI --site-dns option.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/external_dns/external_dns/",
	"title": "External DNS",
	"tags": [],
	"description": "",
	"content": "External DNS External DNS, along with the customer accessible networks CMN and CAN/CHN, PowerDNS, Border Gateway Protocol (BGP), and MetalLB, makes it simpler to access the HPE Cray EX API and system management services. Services are accessible directly from a laptop without needing to tunnel into a non-compute node (NCN) or override /etc/hosts settings. Some services may require a JSON Web Token (JWT) to access them, while others may require OAuth2 to login using a DC LDAP password.\nThe following services are currently available:\nHPE Cray EX API (requires valid JWT) Keycloak Ceph RADOS gateway (requires valid JWT) Nexus System Management Health Prometheus (redirects to OAuth2 Proxy for SSO) System Management Health Grafana (redirects to OAuth2 Proxy for SSO) System Management Health Alertmanager (redirects to OAuth2 Proxy for SSO) Kiali, for Istio service mesh visibility (redirects to OAuth2 Proxy for SSO) Jaeger, for Istio tracing (redirects to OAuth2 Proxy for SSO) In general, external hostnames should resolve to a external IP address for one of the following services:\nistio-system/istio-ingressgateway-cmn - Istio\u0026rsquo;s ingress gateway on CMN. istio-system/istio-ingressgateway-can - Istio\u0026rsquo;s ingress gateway on CAN. istio-system/istio-ingressgateway-chn - Istio\u0026rsquo;s ingress gateway on CHN. services/cray-oauth2-proxies-customer-access-ingress - OAuth2 Proxy\u0026rsquo;s ingress on CMN that redirects browsers to Keycloak for log in, and then to Istio\u0026rsquo;s ingress gateway with a valid JWT for authorized access. services/cray-oauth2-proxies-customer-management-ingress - OAuth2 Proxy\u0026rsquo;s ingress on CAN that redirects browsers to Keycloak for log in, and then to Istio\u0026rsquo;s ingress gateway with a valid JWT for authorized access. services/cray-oauth2-proxies-customer-high-speed-ingress - OAuth2 Proxy\u0026rsquo;s ingress on CHN that redirects browsers to Keycloak for log in, and then to Istio\u0026rsquo;s ingress gateway with a valid JWT for authorized access. This can be verified using the dig command to resolve the external hostname and compare it with Kubernetes.\nWhat Happens if External DNS is not Used? Without forwarding to External DNS, administrators will not have the ability to use the externally exposed services, such as Prometheus, Grafana, the HPE Cray EX REST API, and more. See Externally Exposed Services for more information.\nAccessing most of these services by IP address will not work because the Ingress Gateway uses the name to direct requests to the appropriate service.\nDNS for HPE Cray EX Systems There is a separate set of DNS instances within HPE Cray EX that is used by the nodes and pods within the system for resolving names.\nUnbound\nThe unbound DNS instance is used to resolve names for the physical equipment on the management networks within HPE Cray EX, such as NCNs, UANs, switches, compute nodes, and more. This instance is accessible only within the system.\nKubernetes CoreDNS\nThere is a CoreDNS instance within Kubernetes that is used by Kubernetes pods to resolve names for internal pods and services. This instance is accessible only within the HPE Cray EX Kubernetes cluster.\nConnect Customer DNS to PowerDNS The DNS instance at the customer site should use DNS forwarding to forward the subdomain specified by the system-name and site-domain values (combined to make the system-name.site-domain value) to the IP address specified by the cmn-external-dns value. These values are defined with the csi config init command. The specifics on how to do the forwarding configuration is dependent on the type of DNS used by the customer.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/access_to_system_management_services/",
	"title": "Access to System Management Services",
	"tags": [],
	"description": "",
	"content": "Access to System Management Services The standard configuration for System Management Services (SMS) is the containerized REST micro-service with a public API. All of the micro-services provide an HTTP interface and are collectively exposed through a single gateway URL. The API gateway for the system is available at a well known URL based on the domain name of the system. It acts as a single HTTPS endpoint for terminating Transport Layer Security (TLS) using the configured certificate authority. All services and the API gateway are not dependent on any single node. This resilient arrangement ensures that services remain available during possible underlying hardware and network failures.\nAccess to individual APIs through the gateway is controlled by a policy-driven access control system. Administrators and users must retrieve a token for authentication before attempting to access APIs through the gateway and present a valid token with each API call. The authentication and authorization decisions are made at the gateway level which prevent unauthorized API calls from reaching the underlying micro-services. Refer to Retrieve an Authentication Token for more detail on the process of obtaining tokens and user management.\nThe APIs are accessible on multiple networks. Individual APIs may only be accessible on some of the networks depending on the nature of the API. For example, administrative APIs will not be available on networks that do not allow administrative users. The authentication token must be retrieved from the same network.\nThe available networks are:\nCustomer Management Network (cmn) external network administrative and user APIs Customer Access Network (can) and Customer High-Speed Network (chn) external network only user APIs Node Management Network - Load Balancers (nmnlb) internal network administrative and user APIs Hardware Management Network - Load Balancers (hmnlb) internal network only the cray-hms-hmcollector-ingress service is available Review the API documentation in the supplied container before attempting to use the API services. This container is generated with the release using the most current API descriptions in OpenAPI 2.0 format. Because this file serves as both an internal definition of the API contract and the external documentation of the API function, it is the most up-to-date reference available.\nThe API Gateway URL for accessing the APIs on a site-specific system is https://api.NETWORK.SYSTEM-NAME_DOMAIN-NAME/apis/.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/customer_accessible_networks/connect_to_the_cmn_can/",
	"title": "Connect to the CMN and CAN",
	"tags": [],
	"description": "",
	"content": "Connect to the CMN and CAN How to connect to the CMN and CAN physically and via layer 3.\nThere are multiple ways to connect to the Customer Management Network (CMN) and Customer Access Network (CAN), both physically and via a layer 3 connection.\nPhysical Connection to the CMN and CAN The physical connection to the CMN and CAN is made via the load balancer or the spine switches. The uplink connection from the system to the customer network is achieved by using the highest numbered port(s). Customer can select a single uplink port or multiple uplink ports.\nIn the example below, a Mellanox SN2700 with a single uplink connection is being used. The cable would connect to port 32 as shown in the diagram below:\nLayer 3 Connection to the CMN and CAN The CMN and CAN clients in the system require a routing topology that is setup to route traffic to the customer network. This can be done in a variety of ways and will vary depending on the system setup and configuration. The different options for connecting to the CMN and CAN from layer 3 are described below.\nOption 1: Point-to-Point This option provides a point-to-point routing topology between the customer switch and the HPE Cray EX TOR Spine Switch. See CMN/CAN with Dual-Spine Configuration for more information on using this topology for a dual-spine configuration.\nThe diagram below shows how the point-to-point routing topology works:\nOption 2: Single Gateway The single gateway options requires the customer to provide an IP address that is on a /24 network. This IP address will act as the gateway for traffic bound to the HPE Cray EX CMN and CAN.\nFor example, a customer could use the 192.168.30.0/24 network to connect via the HPE Cray EX CMN and CAN uplink connection. The customer also needs to provide an IP address on this network, such as 192.168.30.253. This IP address will be assigned to the uplink port on the HPE Cray EX TOR Spine Switch.\nFor a dual-spine configuration, the admin would need to extended the customer network to both switches using one IP address for each switch. After extending the network, two equal routes need to be configured. The spine switches are configured to support multi-chassis link aggregation group (MLAG) from NCNs and UANs. These nodes are configured for bonding mode layer 2 and layer 3. See CMN/CAN with Dual-Spine Configuration for more information.\nThe diagram below shows how the connection is established:\nOption 3: Customer Specific The system can be setup using other customer requirements. In order to do so, the following information will be needed:\nItem Description Customer network IP address Specifies an IP address on the customer network that is routable to the customer network gateway CMN IP address space Specifies a dedicated network (typically /24) that is routable from the customer network to be used for the CMN CAN IP address space Specifies a dedicated network that is routable from the customer network to be used for the CAN (optional) System name Specifies the system name to be used and configured Domain namespace extension Specifies the domain extension to be used; for example, customer.com Customer network IP address for ncn-m001 Specifies an IP address for administrative access on the customer network for ncn-m001 (Optional) BMC customer network IP address for ncn-m001 Specifies an IP address for BMC access on the customer network for ncn-m001 "
},
{
	"uri": "/docs-csm/en-12/operations/network/dhcp/dhcp/",
	"title": "DHCP",
	"tags": [],
	"description": "",
	"content": "DHCP The Dynamic Host Configuration Protocol (DHCP) service on the HPE Cray EX system uses the Internet Systems Consortium (ISC) Kea tool. Kea provides more robust management capabilities for DHCP servers.\nFor more information: https://www.isc.org/kea/.\nThe following improvements to the DHCP service are included:\nAPI access to manage DHCP Scalable pod that uses MetalLB instead of host networking Options for updates to HPE Cray EX management system IP addresses DHCP Helper Workflow The DHCP-Helper uses the following workflow:\nThe order can vary in the workflow outlined below, and the services in this workflow can run asynchronously.\nRetrieve the network information. Query the System Layout Service (SLS) Update Kea Query Kea for active leases to check if SMD knows about the NIC. Check MACs in KEA active lease against SMD known NIC MACs. Query SMD for all known NICs and create DHCP leases. Each NIC in SMD create DHCP reservation in Kea Create DHCP reservation in Kea Create hostname/MAC/IP address DHCP reservations Create hostname(alias)/MAC in DHCP reservation Check to see if the NIC in SMD needs to have an IP added. Update the IP address from Kea if NIC in SMD does not have an IP address set Remove any potential race conditions between the DHCP reservation in Kea and the NIC information in SMD. Compare information in NIC information in SMD and Kea DHCP reservation configurations Delete any active leases that did not match SMD "
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/about_kubernetes_taints_and_labels/",
	"title": "About Kubernetes Taints and Labels",
	"tags": [],
	"description": "",
	"content": "About Kubernetes Taints and Labels Kubernetes labels control node affinity, which is the property of pods that attracts them to a set of nodes. On the other hand, Kubernetes taints enable a node to repel a set of pods. In addition, pods can have tolerances for taints to allow them to run on nodes with certain taints.\nTaints are controlled with the kubectl taint nodes command, while node labels for various nodes can be customized with a configmap that contains the desired values. For a description of how to modify the default node labels, refer to the Customer Access Network (CAN) documentation.\nThe list of existing labels can be retrieved using the following command:\nncn# kubectl get nodes --show-labels To learn more, refer to https://kubernetes.io/.\n"
},
{
	"uri": "/docs-csm/en-12/operations/image_management/build_a_new_uan_image_using_the_default_recipe/",
	"title": "Build a New UAN Image Using the Default Recipe",
	"tags": [],
	"description": "",
	"content": "Build a New UAN Image Using the Default Recipe Build or rebuild the User Access Node (UAN) image using either the default UAN image or image recipe. Both of these are supplied by the UAN product stream installer.\nPrerequisites Overview Remove Slingshot Diagnostics RPM From Default UAN Recipe Build the UAN Image Automatically Using IMS Build the UAN Image By Customizing It Manually Prerequisites Both the Cray Operation System (COS) and UAN product streams must be installed. The Cray administrative CLI must be initialized. Overview The Cray EX User Access Node (UAN) recipe currently requires the Slingshot Diagnostics package, which is not installed with the UAN product itself. Therefore, the UAN recipe can only be built after either the Slingshot product is installed, or the Slingshot Diagnostics package is removed from the recipe.\nFirst, determine if the Slingshot product stream is installed on the HPE Cray EX system. The Slingshot Diagnostics RPM must be removed from the default recipe if the Slingshot product is not installed.\nRemove Slingshot Diagnostics RPM From Default UAN Recipe This procedure does not need to be followed if the Slingshot package is installed.\nPerform Upload and Register an Image Recipe procedure to download and extract the UAN image recipe, cray-sles15sp1-uan-cos, but stop before the step that modifies the recipe.\nEdit the file config-template.xml.j2 within the recipe by removing these lines:\n\u0026lt;!-- SECTION: Slingshot Diagnostic package --\u0026gt; \u0026lt;package name=\u0026#34;cray-diags-fabric\u0026#34;/\u0026gt; Resume the Upload and Register an Image Recipe procedure, starting with the step that locates the directory that contains the Kiwi-NG image description files.\nThe next step requires the id of the new image recipe record.\nPerform the Build an Image Using IMS REST Service procedure in order to build the UAN image from the modified recipe. Use the id of the new image recipe.\nBuild the UAN Image Automatically Using IMS This procedure does not need to be followed if choosing to build the UAN image manually.\nIdentify the UAN image recipe.\nncn# cray ims recipes list --format json | jq \u0026#39;.[] | select(.name | contains(\u0026#34;uan\u0026#34;))\u0026#39; Example output:\n{ \u0026#34;created\u0026#34;: \u0026#34;2021-02-17T15:19:48.549383+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;4a5d1178-80ad-4151-af1b-bbe1480958d1\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;3c3b292364f7739da966c9cdae096964\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://ims/recipes/4a5d1178-80ad-4151-af1b-bbe1480958d1/recipe.tar.gz\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;linux_distribution\u0026#34;: \u0026#34;sles15\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cray-shasta-uan-cos-sles15sp1.x86_64-@product_version@\u0026#34;, \u0026#34;recipe_type\u0026#34;: \u0026#34;kiwi-ng\u0026#34; } Save the ID of the IMS recipe in an environment variable.\nncn# IMS_RECIPE_ID=4a5d1178-80ad-4151-af1b-bbe1480958d1 Using the saved IMS recipe ID, follow the Build an Image Using IMS REST Service procedure to build the UAN image.\nBuild the UAN Image By Customizing It Manually This procedure does not need to be followed if the previous procedure was used to build the UAN image automatically.\nIdentify the base UAN image to customize.\nncn# cray ims images list --format json | jq \u0026#39;.[] | select(.name | contains(\u0026#34;uan\u0026#34;))\u0026#39; Example output:\n{ \u0026#34;created\u0026#34;: \u0026#34;2021-02-18T17:17:44.168655+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;6d46d601-c41f-444d-8b49-c9a2a55d3c21\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;371b62c9f0263e4c8c70c8602ccd5158\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/6d46d601-c41f-444d-8b49-c9a2a55d3c21/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;uan-PRODUCT_VERSION-image\u0026#34; } Save the ID of the IMS image in an environment variable.\nncn# IMS_IMAGE_ID=4a5d1178-80ad-4151-af1b-bbe1480958d1 Using the saved IMS image ID, follow the Customize an Image Root Using IMS procedure to build the UAN image.\n"
},
{
	"uri": "/docs-csm/en-12/operations/hmcollector/adjust_hmcollector_resource_limits_requests/",
	"title": "Adjust HM Collector resource limits and requests",
	"tags": [],
	"description": "",
	"content": "Adjust HM Collector resource limits and requests Inspect current resource usage Inspect pods for OOMKilled events How to adjust resource limits Inspect current resource usage View resource usage of the containers in the cray-hms-hmcollector-ingress pods:\nncn-mw# kubectl -n services top pod -l app.kubernetes.io/name=cray-hms-hmcollector-ingress --containers Example output:\nPOD NAME CPU(cores) MEMORY(bytes) cray-hms-hmcollector-ingress-554bb46784-dvjzq cray-hms-hmcollector-ingress 7m 99Mi cray-hms-hmcollector-ingress-554bb46784-dvjzq istio-proxy 5m 132Mi cray-hms-hmcollector-ingress-554bb46784-hctwm cray-hms-hmcollector-ingress 4m 82Mi cray-hms-hmcollector-ingress-554bb46784-hctwm istio-proxy 4m 120Mi cray-hms-hmcollector-ingress-554bb46784-zdhwc cray-hms-hmcollector-ingress 5m 97Mi cray-hms-hmcollector-ingress-554bb46784-zdhwc istio-proxy 4m 133Mi The default resource limits for the cray-hms-hmcollector-ingress containers are:\nCPU: 4 or 4000m Memory: 5Gi The default resource limits for the istio-proxy containers are:\nCPU: 2 or 2000m Memory: 1Gi Inspect pods for OOMKilled events Describe the cray-hms-hmcollector-ingress pods to determine if any have been OOMKilled in the recent past:\nncn-mw# kubectl -n services describe pod -l app.kubernetes.io/name=cray-hms-hmcollector-ingress In the command output, look for the cray-hms-hmcollector-ingress and isitio-proxy containers. Check their Last State (if present) in order to see if the container has been previously terminated because it ran out of memory:\n[...] Containers: cray-hms-hmcollector-ingress: Container ID: containerd://a35853bacdcea350e70c57fe1667b5b9d3c82d41e1e7c1f901832bae97b722fb Image: artifactory.algol60.net/csm-docker/stable/hms-hmcollector:2.17.0 Image ID: artifactory.algol60.net/csm-docker/stable/hms-hmcollector@sha256:43aa7b7c2361a47e56d2ee05fbe37ace1faedc5292bbce4da5d2e79826a45f81 Ports: 80/TCP, 443/TCP Host Ports: 0/TCP, 0/TCP State: Running Started: Tue, 21 Sep 2021 20:52:13 +0000 Last State: Terminated Reason: OOMKilled Exit Code: 137 Started: Tue, 21 Sep 2021 20:51:08 +0000 Finished: Tue, 21 Sep 2021 20:52:12 +0000 [...] istio-proxy: Container ID: containerd://f7e778cf91eedfa86382aabe2c43f3ae1fcf8fea166013c96b8c6794a53cfe1e Image: artifactory.algol60.net/csm-docker/stable/istio/proxyv2:1.8.6-cray2-distroless Image ID: artifactory.algol60.net/csm-docker/stable/istio/proxyv2@sha256:824b59554d6e9765f6226faeaf78902e1df2206b747c05f5b8eb23933eb2e85d Port: 15090/TCP Host Port: 0/TCP Args: proxy sidecar --domain $(POD_NAMESPACE).svc.cluster.local --serviceCluster cray-hms-hmcollector.services --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --trust-domain=cluster.local --concurrency 2 State: Running Started: Tue, 21 Sep 2021 20:51:09 +0000 Last State: Terminated Reason: OOMKilled Exit Code: 137 Started: Tue, 21 Sep 2021 20:51:08 +0000 Finished: Tue, 21 Sep 2021 20:52:12 +0000 [...] In the above example output, the cray-hms-hmcollector-ingress and the istio-proxy containers were previously OOMKilled, but both containers are currently running.\nHow to adjust resource limits If the cray-hms-hmcollector-ingress containers are hitting their CPU limit and memory usage is steadily increasing until they get OOMKilled, then their CPU limit should be increased. It can be increased in increments of 8 or 8000m. This is a situation were the collector is unable to process events fast enough and they start to build up inside of it. If the cray-hms-hmcollector-ingress containers are consistently hitting their CPU limit, then their CPU limit should be increased. It can be increased in increments of 8 or 8000m. If the cray-hms-hmcollector-ingress containers are consistently hitting their memory limit, then their memory limit should be increased. It can be increased in increments of 5Gi. If the istio-proxy container is getting OOMKilled, then its memory limit should be increased. It can be increased in increments of 5Gi. Otherwise, if the cray-hms-hmcollector-ingress and istio-proxy containers are not hitting their CPU or memory limits, then nothing should be changed. For reference, on a system with four fully populated liquid-cooled cabinets, a single cray-hms-hmcollector-ingress pod (with a single replica) was consuming about 5000m of CPU and about 300Mi of memory.\nFollow the Redeploying a Chart procedure with the following specifications:\nChart name: cray-hms-hmcollector\nBase manifest name: sysmgmt\n(ncn-mw#) When reaching the step to update the customizations, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nUpdate customizations.yaml with the existing cray-hms-hmcollector-ingress resource settings.\nPersist resource requests and limits from the cray-hms-hmcollector-ingress deployment.\nncn-mw# kubectl -n services get deployments cray-hms-hmcollector-ingress \\ -o jsonpath=\u0026#39;{.spec.template.spec.containers[].resources}\u0026#39; | yq r -P - | \\ yq w -f - -i ./customizations.yaml spec.kubernetes.services.cray-hms-hmcollector.collectorIngressConfig.resources Persist annotations manually added to cray-hms-hmcollector-ingress deployment.\nncn-mw# kubectl -n services get deployments cray-hms-hmcollector-ingress \\ -o jsonpath=\u0026#39;{.spec.template.metadata.annotations}\u0026#39; | \\ yq d -P - \u0026#39;\u0026#34;traffic.sidecar.istio.io/excludeOutboundPorts\u0026#34;\u0026#39; | \\ yq w -f - -i ./customizations.yaml spec.kubernetes.services.cray-hms-hmcollector.podAnnotations View the updated overrides added to customizations.yaml.\nIf the value overrides look different to the sample output below, then the resource limits and requests have been manually modified in the past.\nncn-mw# yq r ./customizations.yaml spec.kubernetes.services.cray-hms-hmcollector Example output:\nhmcollector_external_ip: \u0026#39;{{ network.netstaticips.hmn_api_gw }}\u0026#39; collectorIngressConfig: resources: limits: cpu: \u0026#34;4\u0026#34; memory: 5Gi requests: cpu: 500m memory: 256Mi podAnnotations: {} If desired, adjust the resource limits and requests for cray-hms-hmcollector-ingress.\nOtherwise this step can be skipped.\nThe value overrides for the cray-hms-hmcollector-ingress Helm chart are defined at spec.kubernetes.services.cray-hms-hmcollector.collectorIngressConfig.\nAdjust the resource limits and requests for the cray-hms-hmcollector-ingress deployment in customizations.yaml:\ncray-hms-hmcollector: hmcollector_external_ip: \u0026#39;{{ network.netstaticips.hmn_api_gw }}\u0026#39; collectorIngressConfig: resources: limits: cpu: \u0026#34;4\u0026#34; memory: 5Gi requests: cpu: 500m memory: 256Mi In order to specify a non-default memory limit for the Istio proxy used by all cray-hms-hmcollector-* pods, add sidecar.istio.io/proxyMemoryLimit under podAnnotations. By default, the Istio proxy memory limit is 1Gi.\ncray-hms-hmcollector: podAnnotations: sidecar.istio.io/proxyMemoryLimit: 5Gi Review the changes to customizations.yaml.\nVerify that baseline system customizations and any customer-specific settings are correct.\nUpdate the site-init sealed secret in the loftsman namespace.\nncn-mw# kubectl delete secret -n loftsman site-init ncn-mw# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml If this document was referenced during an upgrade procure, then skip the rest of the redeploy procedure and also skip the rest of this page.\nWhen reaching the step to validate that the redeploy was successful, there are no validation steps to perform.\n"
},
{
	"uri": "/docs-csm/en-12/operations/hpe_pdu/hpe_pdu_admin_procedures/",
	"title": "HPE PDU Admin Procedures",
	"tags": [],
	"description": "",
	"content": "HPE PDU Admin Procedures The following procedures are used to manage the HPE Power Distribution Unit (PDU):\nVerify PDU vendor Connect to HPE PDU web interface HPE PDU initial set-up Update HPE PDU firmware Change HPE PDU user passwords Update Vault credentials Discover HPE PDU after upgrading CSM IMPORTANT: Because of the polling method used to process sensor data from the HPE PDU, telemetry data may take up to six minutes to refresh; this includes the outlet status reported by the Hardware State Manager (HSM).\nVerify PDU vendor If the PDU is accessible over the network, the following can be used to determine the vendor of the PDU.\nncn-mw# PDU=x3000m0 ncn-mw# curl -k -s --compressed https://$PDU -i | grep Server: Example ServerTech output:\nServer: ServerTech-AWS/v8.0v Example HPE output:\nServer: HPE/1.4.0 This document covers HPE PDU procedures.\nConnect to HPE PDU web interface Connect and log in to the HPE PDU web interface. Access to the HPE PDU web interface is required for the other administrative procedures in this section.\nPrerequisites The following is needed before running this procedure:\nIP address or domain name of ncn-m001 Component name (xname) of the HPE PDU root password for ncn-m001 admin password for HPE PDU (default: 12345678) Create an SSH tunnel from a local PC/MAC/Linux machine:\nexternal# ssh -L 8443:{PDU_xname}:443 -N root@{ncn-m001_ip} In this example, {PDU_xname} is the component name (xname) of the PDU and {ncn-m001_ip} is the IP address of ncn-m001.\nEnter the root password for ncn-m001 when prompted.\nThis command will not complete. It should be left running until the SSH tunnel is no longer needed. At that point, it can be exited with control-C.\nConnect to https://localhost:8443 using a web browser.\nThis must be done on the system where the SSH tunnel was created in the previous step.\nLog in with the admin username.\nEnter the admin password. If the admin password has never been changed, then there will be a prompt to change it.\nHPE PDU initial set-up Set up an HPE PDU for administrative use by completing the following tasks:\nEnsure that Redfish is enabled Add the default user Enable outlet control Connect to the HPE PDU Web Interface and log in as admin.\nSee Connect to HPE PDU web interface.\nEnsure that Redfish is enabled Use the Settings icon (gear in computer monitor in top right corner) to navigate to Network Settings.\nVerify that there is a check next to RESTapi Access.\nIf there is not, then click the Edit icon (pencil) and enable.\nAdd the default user Use the admin menu (top right corner) to navigate to User Accounts. Click on the Add User button. Use the form to add the username and password for the default River user. Assign the role Administrator to that user. Enable outlet control Using the Home icon (house in top right corner) navigate to Control \u0026amp; Manage. Verify that the Outlet Control Enable switch on the top of the page is selected (green). Update HPE PDU firmware Verify that the firmware version for the HPE PDU is 2.0.0.L. If it is not, then a firmware update is required.\nCheck firmware version Connect to the HPE PDU Web Interface and log in as admin.\nSee Connect to HPE PDU web interface.\nSelect the Home icon (house in the top right corner) and navigate to Identification.\nThe version will be displayed. If the version is not 2.0.0.L, then Update firmware.\nUpdate firmware Download version 2.0.0.L firmware from HPE Support.\nThis will download an .exe file, which is a self-extracting zip archive.\nExtract the firmware files.\nIf using a Windows system, run the .exe file to extract the files. Otherwise use an unzip program on the file.\nOne of the files extracted will be named HPE.FW. That is the firmware file needed for uploading.\nConnect to the HPE PDU Web Interface and log in as admin.\nSee Connect to HPE PDU web interface.\nUse the Settings icon (gear in computer monitor in top right corner) to navigate to System Management.\nClick the Update Firmware button.\nClick Choose File and select the HPE.FW file that was downloaded.\nClick Upload button.\nThe firmware will be updated and the PDU management processor will restart.\nChange HPE PDU user passwords Change the password of any existing user account using the HPE PDU web interface.\nConnect to the HPE PDU Web Interface and log in as admin.\nSee Connect to HPE PDU web interface.\nUse the admin menu (top right corner) to navigate to User Accounts.\nClick on the Edit icon (pencil) next to the user.\nEnter the new password and make any other desired changes for that user account.\nClick the Save button.\nDiscover HPE PDU after upgrading CSM Use the following procedure to ensure that the hms-discovery job and Redfish Translation Service (RTS) correctly discover HPE PDUs when upgrading to CSM 1.2 from an earlier release.\nIMPORTANT: This procedure is only needed when upgrading CSM, not performing a fresh install. Run this procedure after CSM has been fully upgraded, including the discovery job.\nIn CSM 1.0 and earlier releases, the hms-discovery job and RTS treated all PDUs as if were made by ServerTech.\nAfter the upgrade to CSM 1.2, RTS will still think that the HPE PDUs in the system are ServerTech PDUs. Remove these erroneous HPE PDU entries for RTS from Vault.\nGet Vault password and create Vault alias.\nncn-mw# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-mw# alias vault=\u0026#39;kubectl -n vault exec -i cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 VAULT_FORMAT=json vault\u0026#39; Identify HPE PDUs known by RTS.\nncn-mw# vault kv list secret/pdu-creds Example output:\n[ \u0026#34;global/\u0026#34;, \u0026#34;x3000m0\u0026#34;, \u0026#34;x3000m1\u0026#34; ] Remove each identified HPE PDU from Vault.\nRepeat the following command for each HPE PDU identified in the output of the previous sub-step.\nncn-mw# PDU=x3000m0 ncn-mw# vault kv delete secret/pdu-creds/$PDU Restart the Redfish Translation Service (RTS).\nncn-mw# kubectl -n services rollout restart deployment cray-hms-rts ncn-mw# kubectl -n services rollout status deployment cray-hms-rts Find the list of PDU MAC addresses.\nThe ID field in each element is the normalized MAC address of each PDU:\nncn-mw# cray hsm inventory ethernetInterfaces list --type CabinetPDUController Use the returned ID from the previous step to delete each HPE PDU MAC address from HSM.\nncn-mw# cray hsm inventory ethernetInterfaces delete {ID} On the next hms-discovery job run, it should relocate the deleted PDUs and discover them correctly as HPE PDUs.\nAfter waiting five minutes, verify that the Ethernet interfaces that were previously deleted are now present:\nncn-mw# sleep 300 ncn-mw# cray hsm inventory ethernetInterfaces list --type CabinetPDUController Verify that the Redfish endpoints for the PDUs exist and are DiscoverOK.\nncn-mw# cray hsm inventory redfishEndpoints list --type CabinetPDUController "
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/add_a_switch_to_the_hsm_database/",
	"title": "Add a Switch to the HSM Database",
	"tags": [],
	"description": "",
	"content": "Add a Switch to the HSM Database Manually add a switch to the Hardware State Manager (HSM) database. Switches need to be in the HSM database in order to update their firmware with the Firmware Action Service (FAS).\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. The component name (xname), IP address, user name, and password are known for the switch being added. Procedure Add the switch to the HSM database.\nThe --rediscover-on-update true flag forces HSM to discover the switch.\nncn-mw# cray hsm inventory redfishEndpoints create --id XNAME --fqdn IP_ADDRESS --user USERNAME \\ --password PASSWORD --rediscover-on-update true --format toml Example output:\n[[results]] URI = \u0026#34;/hsm/v2/Inventory/RedfishEndpoints/x3000c0r41b0\u0026#34; Verify that HSM successfully discovered the switch.\nncn-mw# cray hsm inventory redfishEndpoints list --id XNAME --format toml Example output:\n[[RedfishEndpoints]] Domain = \u0026#34;\u0026#34; RediscoverOnUpdate = true Hostname = \u0026#34;10.254.2.17\u0026#34; Enabled = true FQDN = \u0026#34;10.254.2.17\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34;\u0026#34; Type = \u0026#34;RouterBMC\u0026#34; ID = \u0026#34;x3000c0r41b0\u0026#34; [RedfishEndpoints.DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2020-02-05T18:41:08.823059Z\u0026#34; RedfishVersion = \u0026#34;1.2.0\u0026#34; LastDiscoveryStatus = \u0026#34;DiscoverOK\u0026#34; The switch is now discovered by the HSM.\n"
},
{
	"uri": "/docs-csm/en-12/operations/firmware/fas_admin_procedures/",
	"title": "FAS Admin Procedures",
	"tags": [],
	"description": "",
	"content": "FAS Admin Procedures Procedures for leveraging the Firmware Action Service (FAS) CLI to manage firmware.\nTopics Warning for non-compute nodes (NCNs) Declarative vs imperative FAS updates Exclude nodes from an update Ignore management nodes within FAS Override an image for an update Check for new firmware versions with a dry-run Load firmware from Nexus Load firmware from RPM or ZIP file Warning for non-compute nodes (NCNs) NCNs and their BMCs must be locked with the HSM locking API to ensure they are not unintentionally updated by FAS. Research Lock and Unlock Management Nodes for more information. Failure to lock the NCNs could result in unintentional update of the NCNs if FAS is not used correctly; this will lead to system instability problems.\nNOTE: Any node that is locked remains in the state inProgress with the stateHelper message of \u0026quot;failed to lock\u0026quot; until the action times out, or the lock is released. If the action is timed out, these nodes report as failed with the stateHelper message of \u0026quot;time expired; could not complete update\u0026quot;. This includes NCNs which are manually locked to prevent accidental rebooting and firmware updates.\nDeclarative vs imperative FAS updates In most cases, FAS will update firmware on each node to the required version. This version is determined by comparing the semantic versions for all valid images for each node and selecting the highest value. Sometimes however, a different version is required for updating a node. FAS can update a node to any uploaded version using the Override an Image for an Update procedure. If the required image is not installed, obtain the RPM or ZIP file and use the Load Firmware from RPM or ZIP file procedure.\nIgnore management nodes within FAS The default configuration of FAS no longer ignores management nodes, which prevents FAS from firmware updating the NCNs. To reconfigure the FAS deployment to exclude non-compute nodes (NCNs) and ensure they cannot have their firmware upgraded, the NODE_BLACKLIST value must be manually enabled\nPreferred Method: Nodes can also be locked with the Hardware State Manager (HSM) API. Refer to Lock and Unlock Management Nodes for more information.\nProcedure to ignore management nodes Check that there are no FAS actions running.\nncn-mw# cray fas actions list Edit the cray-fas deployment.\nncn-mw# kubectl -n services edit deployment cray-fas Change the NODE_BLACKLIST value from ignore_ignore_ignore to management.\nSave and quit the deployment. This will restart FAS.\nOverride an image for an update In order to update to a firmware image other than the latest image found by FAS, the image for the update can be overridden. This is also useful if an update fails because of \u0026quot;No Image available\u0026quot;, which happens when FAS is unable to match the data on the node to an image in the image list.\nWARNING: Make sure to select the correct image because FAS will force a flash of the device \u0026ndash; using incorrect firmware may make it inoperable.\nIt is strongly recommended to do a dryrun first and check the output.\nProcedure to override an image Find the available image in FAS.\nChange TARGETNAME to the actual target being searched.\nncn-mw# cray fas images list --format json | jq \u0026#39;.[] | .[] | select(.target==\u0026#34;TARGETNAME\u0026#34;)\u0026#39; To narrow down the selection, update the select field to match multiple items. For example:\nncn-mw# cray fas images list --format json | jq \u0026#39;.[] | .[] | select(.target==\u0026#34;BMC\u0026#34; and .manufacturer==\u0026#34;cray\u0026#34; and .deviceType==\u0026#34;NodeBMC\u0026#34;)\u0026#39; The example command displays one or more images available for updates.\n{ \u0026#34;imageID\u0026#34;: \u0026#34;ff268e8a-8f73-414f-a9c7-737a34bb02fc\u0026#34;, \u0026#34;createTime\u0026#34;: \u0026#34;2021-02-24T02:25:03Z\u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;nodeBMC\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34;, \u0026#34;models\u0026#34;: [ \u0026#34;HPE Cray EX235n\u0026#34;, \u0026#34;GrizzlyPkNodeCard_REV_B\u0026#34; ], \u0026#34;softwareIds\u0026#34;: [ \u0026#34;fgpa:NVIDIA.HGX.A100.4.GPU:*:*\u0026#34; ], \u0026#34;target\u0026#34;: \u0026#34;Node0.AccFPGA0\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;default\u0026#34; ], \u0026#34;firmwareVersion\u0026#34;: \u0026#34;2.7\u0026#34;, \u0026#34;semanticFirmwareVersion\u0026#34;: \u0026#34;2.7.0\u0026#34;, \u0026#34;pollingSpeedSeconds\u0026#34;: 30, \u0026#34;s3URL\u0026#34;: \u0026#34;s3:/fw-update/80a62641764711ebabe28e2b78a05899/accfpga_nvidia_2.7.tar.gz\u0026#34; } If the firmwareVersion from the FAS image matches the fromFirmwareVersion from the FAS action, the firmware is at the latest version and no update is needed.\nUse the imageID from the cray images list in the previous step and add the following line to the action JSON file.\nReplace IMAGEID with the actual imageID value. In this example, the value would be: ff268e8a-8f73-414f-a9c7-737a34bb02fc.\n\u0026#34;imageFilter\u0026#34;: { \u0026#34;imageID\u0026#34;:\u0026#34;IMAGEID\u0026#34;, \u0026#34;overrideImage\u0026#34;:true } Example actions JSON file with imageFilter added:\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;:[\u0026#34;nodeBMC\u0026#34;] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;:\u0026#34;cray\u0026#34; }, \u0026#34;imageFilter\u0026#34;: { \u0026#34;imageID\u0026#34;:\u0026#34;ff268e8a-8f73-414f-a9c7-737a34bb02fc\u0026#34;, \u0026#34;overrideImage\u0026#34;:true }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;:[\u0026#34;Node0.AccFPGA0\u0026#34;,\u0026#34;Node1.AccFPGA0\u0026#34;] }, \u0026#34;command\u0026#34;: { \u0026#34;overrideDryrun\u0026#34;:false, \u0026#34;restoreNotPossibleOverride\u0026#34;:true, \u0026#34;overwriteSameImage\u0026#34;:false } } Verify that the correct image ID was found.\nncn-mw# cray fas images describe {imageID} WARNING: FAS will force a flash of the device \u0026ndash; using incorrect firmware may make it inoperable.\nRe-run the FAS actions command using the updated JSON file. It is strongly recommended to run a dry-run (overrideDryrun=false) first and check the actions output.\nCheck for new firmware versions with a dry-run Use the Firmware Action Service (FAS) dry-run feature to determine what firmware can be updated on the system. Dry-runs are enabled by default, and can be configured with the overrideDryrun parameter. A dry-run will create a query according to the filters requested by the administrator. It will initiate an update sequence to determine what firmware is available, but will not actually change the state of the firmware.\nWARNING: It is crucial that an administrator is familiar with the release notes of any firmware. The release notes will indicate what new features the firmware provides and if there are any incompatibilities. FAS does not know about incompatibilities or dependencies between versions. The administrator assumes full responsibility for this knowledge.\nIt is likely that when performing a firmware update, that the current version of firmware will not be available. This means that after successfully upgrading, the firmware cannot be downgraded.\nThis procedure includes information on how check the firmware versions for the entire system, as well as how to target specific manufacturers, component names (xnames), and targets.\nProcedure to check for new firmware versions Run a dry-run firmware update.\nThe following command parameters should be included in dry-run JSON files:\noverrideDryrun: The overrideDryrun parameter is set to false by default. FAS will only update the system if this is parameter is set to true. restoreNotPossibleOverride: FAS will not perform an update if the currently running firmware is not available in the images repository. Set this parameter to true in order to allow FAS to update firmware even if the current firmware is unavailable on the system. description: A brief description that helps administrators distinguish between actions. version: Determines if the firmware should be set to the latest, the earliest semantic version, or set to a specific firmware version. Use one of the options below to run on a dry-run on every system device or on targeted devices:\nOption 1: Determine the available firmware for every device on the system:\nCreate a JSON file for the command parameters.\n{ \u0026#34;command\u0026#34;: { \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;full system dryrun 2020623_0\u0026#34; } } Run the dry-run for the full system.\nncn-mw# cray fas actions create COMMAND.json Proceed to the next step to determine if any firmware needs to be updated.\nOption 2: Determine the available firmware for specific devices:\nCreate a JSON file with the specific device information to target when doing a dry-run.\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;xnames\u0026#34;: [ \u0026#34;x9000c1s3b1\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;Node1.BIOS\u0026#34;, \u0026#34;Node0.BIOS\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;dryrun upgrade of x9000c1s3b1 Nodex.BIOS to WNC 1.1.2\u0026#34; } Run a dry-run on the targeted devices.\nncn-mw# cray fas actions create CUSTOM_DEVICE_PARAMETERS.json Proceed to the next step to determine if any firmware needs to be updated.\nView the status of the dry-run to determine if any firmware updates can be made.\nThe following returned messages will help determine if a firmware update is needed.\nnoOperation: Nothing to do; already at the requested version. noSolution: No image is available or data is missing. succeeded: A firmware version that FAS can update the firmware to is available and it should work when actually updating the firmware. failed: There is something that FAS could do, but it likely would fail (most likely because the file is missing). Get a high-level summary of the FAS job to determine if there are any upgradable firmware images available.\nUse the returned actionID from the cray fas actions create command.\nIn the example below, there are two operations in the succeeded state, indicating there is an available firmware version that FAS can use to update firmware.\nncn-mw# cray fas actions status list {actionID} --format toml Example output:\nactionID = \u0026#34;e6dc14cd-5e12-4d36-a97b-0dd372b0930f\u0026#34; snapshotID = \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; startTime = \u0026#34;2021-09-07 16:43:04.294233199 +0000 UTC\u0026#34; endTime = \u0026#34;2021-09-07 16:53:09.363233482 +0000 UTC\u0026#34; state = \u0026#34;completed\u0026#34; blockedBy = [] [command] overrideDryrun = false restoreNotPossibleOverride = true overwriteSameImage = false timeLimit = 2000 version = \u0026#34;latest\u0026#34; tag = \u0026#34;default\u0026#34; description = \u0026#34;Dryrun upgrade of Gigabyte node BMCs\u0026#34; [operationCounts] total = 14 initial = 0 configured = 0 blocked = 0 needsVerified = 0 verifying = 0 inProgress = 0 failed = 0 succeeded = 8 noOperation = 6 noSolution = 0 aborted = 0 unknown = 0 The action is still in progress if the state field is not completed or aborted.\nView the details of an action to get more information on each operation in the FAS action.\nIn the example below, there is an operation for a component name (xname) in the failed state, indicating there is something that FAS could do, but it likely would fail. A common cause for an operation failing is due to a missing firmware image file.\nncn-mw# cray fas actions describe {actionID} --format json Example output:\n{ \u0026#34;parameters\u0026#34;: { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;dryrun\u0026#34;: false, \u0026#34;description\u0026#34;: \u0026#34;upgrade of nodeBMCs for cray\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;imageFilter\u0026#34;: { \u0026#34;imageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] } }, \u0026#34;blockedBy\u0026#34;: [], \u0026#34;state\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;command\u0026#34;: { \u0026#34;dryrun\u0026#34;: false, \u0026#34;description\u0026#34;: \u0026#34;upgrade of nodeBMCs for cray\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;actionID\u0026#34;: \u0026#34;e0cdd7c2-32b1-4a25-9b2a-8e74217eafa7\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-06-26 20:03:37.316932354 +0000 UTC\u0026#34;, \u0026#34;snapshotID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2020-06-26 20:04:07.118243184 +0000 UTC\u0026#34;, \u0026#34;operationSummary\u0026#34;: { \u0026#34;succeeded\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;verifying\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;unknown\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;configured\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;initial\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;failed\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [ { \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x5000c1r7b0\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;0796eed0-e95d-45ea-bc71-8903d52cffde\u0026#34; }, ] }, \u0026#34;noSolution\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;aborted\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;needsVerified\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;noOperation\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;inProgress\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;blocked\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] } } } View the details for a specific operation.\nIn this example, there is a device that is available for a firmware upgrade because the operation being viewed is a succeeded operation.\nncn-mw# cray fas operations describe {operationID} --format json Example output:\n{ \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fromTag\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fromImageURL\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2020-06-24 14:23:37.544814197 +0000 UTC\u0026#34;, \u0026#34;actionID\u0026#34;: \u0026#34;f48aabf1-1616-49ae-9761-a11edb38684d\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-06-24 14:19:15.10128214 +0000 UTC\u0026#34;, \u0026#34;fromSemanticFirmwareVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;toImageURL\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;WindomNodeCard_REV_D\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;24a5e5fb-5c4f-4848-bf4e-b071719c1850\u0026#34;, \u0026#34;fromImageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;toImageID\u0026#34;: \u0026#34;71c41a74-ab84-45b2-95bd-677f763af168\u0026#34;, \u0026#34;toSemanticFirmwareVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refreshTime\u0026#34;: \u0026#34;2020-06-24 14:23:37.544824938 +0000 UTC\u0026#34;, \u0026#34;blockedBy\u0026#34;: [], \u0026#34;toTag\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected nc.1.3.8-shasta-release.arm.2020-06-15T22:57:31+00:00.b7f0725 got: nc.1.2.25-shasta-release.arm.2020-05-15T17:27:16+00:00.0cf7f51\u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;expirationTime\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x9000c1s3b1\u0026#34;, \u0026#34;toFirmwareVersion\u0026#34;: \u0026#34;\u0026#34; } Update the firmware on any devices indicating a new version is needed.\nLoad firmware from Nexus This procedure will read all RPMs in the Nexus repository and upload firmware images to S3 and create image records for firmware not already in FAS.\nCheck the loader status.\nncn-mw# cray fas loader list --format toml | grep loaderStatus This will return a ready or busy status.\nloaderStatus = \u0026#34;ready\u0026#34; The loader can only run one job at a time, if the loader is busy, it will return an error on any attempt to create an additional job.\nRun the loader Nexus command.\nncn-mw# cray fas loader nexus create --format toml This will return an ID which will be used to check the status of the run.\nloaderRunID = \u0026#34;7b0ce40f-cd6d-4ff0-9b71-0f3c9686f5ce\u0026#34; NOTE Depending on how many files are in Nexus and how large those files are, the loader may take several minutes to complete.\nCheck the results of the loader run.\nncn-mw# cray fas loader describe {loaderRunID} --format json NOTE {loadRunID} is the ID from step #2 above \u0026ndash; in that case 7b0ce40f-cd6d-4ff0-9b71-0f3c9686f5ce. Use the --format json to make it easier to read.\nExample output:\n{ \u0026#34;loaderRunOutput\u0026#34;: [ \u0026#34;2021-07-20T18:17:58Z-FWLoader-INFO-Starting FW Loader, LOG_LEVEL: INFO; value: 20\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-FWLoader-INFO-urls: {\u0026#39;fas\u0026#39;: \u0026#39;http://cray-fas\u0026#39;, \u0026#39;fwloc\u0026#39;: \u0026#39;file://download/\u0026#39;}\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-INFO: LOG_LEVEL: DEBUG; value: 10\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-INFO: NEXUS_ENDPOINT: http://nexus.nexus.svc.cluster.local\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-INFO: NEXUS_REPO: shasta-firmware\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-INFO: Repomd URL: http://nexus.nexus.svc.cluster.local/repository/shasta-firmware/repodata/repomd.xml\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-DEBUG: Starting new HTTP connection (1): nexus.nexus.svc.cluster.local:80\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-DEBUG: http://nexus.nexus.svc.cluster.local:80 \\\u0026#34;GET /repository/shasta-firmware/repodata/repomd.xml HTTP/1.1\\\u0026#34; 200 3080\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-INFO: Packages URL: http://nexus.nexus.svc.cluster.local/repository/shasta-firmware/repodata/7f727fc9c4a8d0df528798dc85f1c5178128f3e00a0820a4d07bf9842ddcb6e1-primary.xml.gz\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-DEBUG: Starting new HTTP connection (1): nexus.nexus.svc.cluster.local:80\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-DEBUG: http://nexus.nexus.svc.cluster.local:80 \\\u0026#34;GET /repository/shasta-firmware/repodata/7f727fc9c4a8d0df528798dc85f1c5178128f3e00a0820a4d07bf9842ddcb6e1-primary.xml.gz HTTP/1.1\\\u0026#34; 200 6137\u0026#34;, ... ... \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for e7b20c7ae98611eb880aa2c40cff7c62/nc-1.5.15.itb\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for ec324d05e98611ebbb9da2c40cff7c62/rom.ima_enc\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for e74c5977e98611eb8e9aa2c40cff7c62/cc-1.5.15.itb\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for e1feb6d6e98611eb877aa2c40cff7c62/accfpga_nvidia_2.7.tar.gz\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for dc830cb2e98611ebb4d2a2c40cff7c62/A48_2.40_02_24_2021.signed.flash\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for e28626d4e98611ebb0a7a2c40cff7c62/wnc.i210-p2sn01.tar.gz\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for eee87acde98611eba8f4a2c40cff7c62/image.RBU\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for de3d01bee98611eb9affa2c40cff7c62/A47_2.40_02_23_2021.signed.flash\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for e8ab6f61e98611eb913fa2c40cff7c62/ex235n.bios-1.1.1.tar.gz\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for e8ab6f61e98611eb913fa2c40cff7c62/ex235n.bios-1.1.1.tar.gz\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-finished updating images ACL\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-*** Number of Updates: 24 ***\u0026#34; ] } A successful run will end with *** Number of Updates: x ***.\nNOTE The FAS loader will not overwrite image records already in FAS. Number of Updates will be the number of new images found in Nexus. If the number is 0, all images were already in FAS.\nLoad firmware from RPM or ZIP file This procedure will read a single local RPM (or ZIP) file and upload firmware images to S3 and create image records for firmware not already in FAS.\nCopy the file to ncn-m001 or one of the other NCNs.\nCheck the loader status:\nncn-mw# cray fas loader list --format toml | grep loaderStatus This will return a ready or busy status.\nloaderStatus = \u0026#34;ready\u0026#34; The loader can only run one job at a time, if the loader is busy, it will return an error on any attempt to create an additional job.\nRun the loader command.\nfirmware.rpm is the name of the RPM. If the file is not in the current directory, add the path to the filename.\nncn-mw# cray fas loader create --file firmware.RPM This will return an ID which will be used to check the status of the run.\nloaderRunID = \u0026#34;7b0ce40f-cd6d-4ff0-9b71-0f3c9686f5ce\u0026#34; Check the results of the loader run.\nncn-mw# cray fas loader describe {loaderRunID} --format json NOTE {loadRunID} is the ID from step #2 above \u0026ndash; in that case 7b0ce40f-cd6d-4ff0-9b71-0f3c9686f5ce. Use the --format json to make it easier to read.\nExample output:\n{ \u0026#34;loaderRunOutput\u0026#34;: [ \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Starting FW Loader, LOG_LEVEL: INFO; value: 20\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-urls: {\u0026#39;fas\u0026#39;: \u0026#39;http://localhost:28800\u0026#39;, \u0026#39;fwloc\u0026#39;: \u0026#39;file://download/\u0026#39;}\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Using local file: /ilo5_241.zip\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-unzip /ilo5_241.zip\u0026#34;, \u0026#34;Archive: /ilo5_241.zip\u0026#34;, \u0026#34; inflating: ilo5_241.bin\u0026#34;, \u0026#34; inflating: ilo5_241.json\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Processing files from file://download/\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-get_file_list(file://download/)\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Processing File: file://download/ ilo5_241.json\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Uploading b73a48cea82f11eb8c8a0242c0a81003/ilo5_241.bin\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Metadata {\u0026#39;imageData\u0026#39;: \\\u0026#34;{\u0026#39;deviceType\u0026#39;: \u0026#39;nodeBMC\u0026#39;, \u0026#39;manufacturer\u0026#39;: \u0026#39;hpe\u0026#39;, \u0026#39;models\u0026#39;: [\u0026#39;ProLiant XL270d Gen10\u0026#39;, \u0026#39;ProLiant DL325 Gen10\u0026#39;, \u0026#39;ProLiant DL325 Gen10 Plus\u0026#39;, \u0026#39;ProLiant DL385 Gen10\u0026#39;, \u0026#39;ProLiant DL385 Gen10 Plus\u0026#39;, \u0026#39;ProLiant XL645d Gen10 Plus\u0026#39;, \u0026#39;ProLiant XL675d Gen10 Plus\u0026#39;], \u0026#39;targets\u0026#39;: [\u0026#39;iLO 5\u0026#39;], \u0026#39;tags\u0026#39;: [\u0026#39;default\u0026#39;], \u0026#39;firmwareVersion\u0026#39;: \u0026#39;2.41 Mar 08 2021\u0026#39;, \u0026#39;semanticFirmwareVersion\u0026#39;: \u0026#39;2.41.0\u0026#39;, \u0026#39;pollingSpeedSeconds\u0026#39;: 30, \u0026#39;fileName\u0026#39;: \u0026#39;ilo5_241.bin\u0026#39;}\\\u0026#34;}\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-IMAGE: {\\\u0026#34;s3URL\\\u0026#34;: \\\u0026#34;s3:/fw-update/b73a48cea82f11eb8c8a0242c0a81003/ilo5_241.bin\\\u0026#34;, \\\u0026#34;target\\\u0026#34;: \\\u0026#34;iLO 5\\\u0026#34;, \\\u0026#34;deviceType\\\u0026#34;: \\\u0026#34;nodeBMC\\\u0026#34;, \\\u0026#34;manufacturer\\\u0026#34;: \\\u0026#34;hpe\\\u0026#34;, \\\u0026#34;models\\\u0026#34;: [\\\u0026#34;ProLiant XL270d Gen10\\\u0026#34;, \\\u0026#34;ProLiant DL325 Gen10\\\u0026#34;, \\\u0026#34;ProLiant DL325 Gen10 Plus\\\u0026#34;, \\\u0026#34;ProLiant DL385 Gen10\\\u0026#34;, \\\u0026#34;ProLiant DL385 Gen10 Plus\\\u0026#34;, \\\u0026#34;ProLiant XL645d Gen10 Plus\\\u0026#34;, \\\u0026#34;ProLiant XL675d Gen10 Plus\\\u0026#34;], \\\u0026#34;softwareIds\\\u0026#34;: [], \\\u0026#34;tags\\\u0026#34;: [\\\u0026#34;default\\\u0026#34;], \\\u0026#34;firmwareVersion\\\u0026#34;: \\\u0026#34;2.41 Mar 08 2021\\\u0026#34;, \\\u0026#34;semanticFirmwareVersion\\\u0026#34;: \\\u0026#34;2.41.0\\\u0026#34;, \\\u0026#34;allowableDeviceStates\\\u0026#34;: [], \\\u0026#34;needManualReboot\\\u0026#34;: false, \\\u0026#34;pollingSpeedSeconds\\\u0026#34;: 30}\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-Number of Updates: 1\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-Iterate images\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for 5ab9f804a82b11eb8a700242c0a81003/wnc.bios-1.1.2.tar.gz\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for 5ab9f804a82b11eb8a700242c0a81003/wnc.bios-1.1.2.tar.gz\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for 53c060baa82a11eba26c0242c0a81003/controllers-1.3.317.itb\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for b73a48cea82f11eb8c8a0242c0a81003/ilo5_241.bin\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-finished updating images ACL\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-removing local file: /ilo5_241.zip\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-*** Number of Updates: 1 ***\u0026#34; ] } A successful run will end with *** Number of Updates: x ***.\nNOTE The FAS loader will not overwrite image records already in FAS. Number of Updates will be the number of new images found in the RPM. If the number is 0, all images were already in FAS.\n"
},
{
	"uri": "/docs-csm/en-12/operations/conman/access_compute_node_logs/",
	"title": "Access Compute Node Logs",
	"tags": [],
	"description": "",
	"content": "Access Compute Node Logs This procedure shows how the ConMan utility can be used to retrieve compute node logs.\nPrerequisites The user performing this procedure needs to have access permission to the cray-console-operator pod.\nLimitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.\nProcedure Note: this procedure has changed since the CSM 0.9 release.\nLog on to a Kubernetes master or worker node.\nFind the cray-console-operator pod.\nncn# OP_POD=$(kubectl get pods -n services \\ -o wide|grep cray-console-operator|awk \u0026#39;{print $1}\u0026#39;) ncn# echo $OP_POD Example output:\ncray-console-operator-6cf89ff566-kfnjr Log on to the pod.\nncn# kubectl exec -it -n services $OP_POD -- sh The console log file for each node is labeled with the component name (xname) of that node.\nList the log directory contents.\n# ls -la /var/log/conman total 44 -rw------- 1 root root 1415 Nov 30 20:00 console.XNAME ... The log directory is also accessible from the cray-console-node pods.\nThe log files are plain text files which can be viewed with commands like cat or tail.\n# tail /var/log/conman/console.XNAME Exit out of the pod.\n# exit ncn# "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/ansible_execution_environments/",
	"title": "Ansible Execution Environments",
	"tags": [],
	"description": "",
	"content": "Ansible Execution Environments Configuration Framework Service (CFS) sessions are comprised of a single Kubernetes pod with several containers. Inventory and Git clone setup containers run first, and a teardown container runs last (if the session is running an image customization).\nThe containers that run the Ansible code cloned from the Git repositories in the configuration layers are Ansible Execution Environments (AEE). The AEE is provided as a SLES-based docker image, which includes Ansible version 2.9.11 installed using Python 3.6. In addition to the base Ansible installation, CFS also includes several Ansible modules and plug-ins that are required for CFS and Ansible to work properly on the system.\nThe following modules and plug-ins are available:\ncfs_aggregator.py Callback Plug-in\nThis callback plug-in is included to relay playbook execution results back to CFS for the purpose of tracking session status and component state.\nWARNING: This plug-in is required for CFS to function properly and must not be removed from the ansible.cfg file.\ncfs_linear and cfs_free Strategy Plug-ins\nCFS provides two strategy plug-ins, cfs_linear and cfs_free, which should be used in place of the stock Ansible free and linear playbook execution strategies.\nFor more information about Ansible strategies, see the external Ansible playbook strategies documentation.\nshasta_s3_cred.py Module\nThis module is provided to obtain access to S3 credentials stored in Kubernetes secrets in the cluster, specifically secrets with names such as \u0026lt;service\u0026gt;-s3-credentials.\nAn example of using this module is as follows:\n- name: Retrieve credentials from abc-s3-credentials k8s secret shasta_s3_creds: k8s_secret: abc-s3-credentials k8s_namespace: services register: creds no_log: true The access key is available at \u0026ldquo;{{ creds.access_key }}\u0026rdquo; and the secret key is at \u0026ldquo;{{ creds.secret_key }}\u0026rdquo;.\n"
},
{
	"uri": "/docs-csm/en-12/operations/compute_rolling_upgrades/crus_workflow/",
	"title": "CRUS Workflow",
	"tags": [],
	"description": "",
	"content": "CRUS Workflow Note: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality. See Deprecated features.\nThe following workflow is intended to be a high-level overview of how to upgrade compute nodes. This workflow depicts how services interact with each other during the compute node upgrade process, and helps to provide a quicker and deeper understanding of how the system functions.\nUse cases Administrator upgrades select compute nodes to a newer compute image by using Compute Rolling Upgrade Service (CRUS).\nRequirements The compute nodes are up and running. Compute node workloads are managed by Slurm. Components This workflow is based on the interaction of CRUS with Boot Orchestration Service (BOS) and Slurm (Workload Manager).\nThe following terms are mentioned in this workflow:\nCompute Rolling Upgrade Service (CRUS) allows an administrator to modify the boot image and/or configuration on a set of compute nodes without the need to take the entire set of nodes out of service at once. It manages the workload management status of nodes, quiescing each node before taking the node out of service, upgrading the node, rebooting the node into the upgraded state and then returning the node to service within the workload manager. Boot Orchestration Service (BOS) is responsible for booting, configuring, and shutting down collections of nodes. The Boot Orchestration Service has the following components: Boot Orchestration Session Template is a collection of one or more boot set objects. A boot set defines a collection of nodes and the information about the boot artifacts and parameters. Boot Orchestration Session carries out an operation. The possible operations in a session are boot, shutdown, reboot, and configure. Boot Orchestration Agent (BOA) is automatically launched to execute the session. A BOA executes the given operation, and if the operation is a boot or a reboot, it also configures the nodes post-boot (if configure is enabled). The Slurm control daemon (slurmctld) is the central management daemon of Slurm. It runs on non-compute nodes in a container. It monitors all other Slurm daemons and resources, accepts jobs, and allocates resources to those jobs. The Slurm daemon (slurmd) monitors all tasks running on compute nodes, accepts tasks, launches tasks, and kills running tasks upon request. It runs on compute nodes. Workflow The following sequence of steps occur during this workflow.\n1. Administrator creates HSM groups and populates the starting group Create three HSM groups with starting, failed, and upgrading labels.\nAny names can be used for these groups. For this example: crus_starting, crusfailed, and crusupgrading, respectively.\nAdd all of the compute nodes to be updated to the crus_starting group.\nLeave the crusfailed, and crusupgrading groups empty.\n2. Administrator creates a session template A session template is a collection of metadata for a group of nodes and their desired configuration.\nCreate a BOS session template which points to the new image, the desired CFS configuration, and with a boot set which includes all the compute nodes to be updated. The boot set can include additional nodes, but it must contain all the nodes that need to be updated. The BOS session template should specify crusupgrading in the node_groups field of one of its boot sets.\nThis example will use the BOS session template named newcomputetemplate.\n3. Administrator creates a CRUS session A new upgrade session is launched as a result of this call.\nSpecify the following parameters:\nParameter Example Meaning failed_label crusfailed An empty Hardware State Manager (HSM) group which CRUS will populate with any nodes that fail their upgrades. starting_label crus_starting An HSM group which contains the total set of nodes to be upgraded. upgrading_label crusupgrading An empty HSM group which CRUS will use to boot and configure subsets of the compute nodes. upgradestepsize 50 The number of nodes to include in each discrete upgrade step.* upgradetemplateid newcomputetemplate The name of the BOS session template to use for the upgrades. workloadmanagertype slurm Only Slurm is supported. * Each group of concurrent upgrades will never exceed this number of compute nodes, although in some cases they may be smaller.\n4. CRUS to HSM CRUS calls HSM to find the nodes in the crus_starting group.\n5. CRUS to HSM CRUS selects a number of these nodes equal to upgradestepsize and calls HSM to put them into the crusupgrading group.\n6. CRUS to Slurm CRUS tells Slurm to quiesce these nodes. As each node is quiesced, Slurm puts the node offline.\n7. Slurm to CRUS Slurm reports back to CRUS that all of the nodes as offline.\n8. CRUS to BOS CRUS calls BOS to create a session with the following arguments:\nParameter Value operation reboot templateUuid newcomputetemplate limit crusupgrading 9. CRUS retrieves the BOA job details from BOS CRUS retrieves the BOS session to get the BOA job name.\nCRUS waits for the BOA job to finish.\nCRUS looks at the exit code of the BOA job to determine whether or not there were errors.\nIf there were errors, CRUS adds the nodes from the crusupgrading group into the crusfailed group.\nCRUS calls HSM to empty the crusupgrading group.\n10. CRUS repeats steps for remaining nodes, then updates status CRUS repeats steps 5-9 until all of the nodes from the crus_starting group have gone through these steps.\nCRUS marks the session status as complete.\n"
},
{
	"uri": "/docs-csm/en-12/operations/artifact_management/artifact_management/",
	"title": "Artifact Management",
	"tags": [],
	"description": "",
	"content": "Artifact Management The Ceph Object Gateway Simple Storage Service (S3) API is used for artifact management. The RESTful API that Ceph provides via the gateway is compatible with the basic data access model of the Amazon S3 API. See the https://docs.ceph.com/en/pacific/radosgw/s3/ for more information about compatibility. The object gateway is also referred to as the RADOS gateway or simply RGW.\nS3 is an object storage service that provides high-level performance, scalability, security, and data availability. S3 exposes a rudimentary data model, similar to a file system, where buckets (directories) store objects (files). Bucket- and object-level Access Control Lists (ACL) can be provided for flexible access authorization to artifacts stored in S3.\nRGW on HPE Cray EX Systems RGW is installed as a part of the HPE Cray EX Stage 3 deployment. The S3 API is available on systems at the following location:\nhttps://rgw-vip.local The RGW administrative interface (radosgw-admin) is available on non-compute nodes (NCNs).\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/bos_workflows/",
	"title": "BOS Workflows",
	"tags": [],
	"description": "",
	"content": "BOS Workflows The following workflows present a high-level overview of common Boot Orchestration Service (BOS) operations. These workflows depict how services interact with each other when booting, configuring, or shutting down nodes. They also help provide a quicker and deeper understanding of how the system functions.\nTerminology Workflows Boot and configure nodes Reconfigure nodes Power off nodes Terminology The following are mentioned in the workflows:\nBoot Orchestration Service (BOS) is responsible for booting, configuring, and shutting down collections of nodes. The Boot Orchestration Service has the following components: A BOS session template is a collection of one or more boot sets. A boot set defines a collection of nodes and the information about the boot artifacts and parameters. Session templates also include information on which Configuration Framework Service (CFS) configuration should be applied. BOS sessions provide a way to apply a template across a group of nodes and monitor the progress of those nodes as they move toward their desired state. BOS Operators interact with other services to perform actions on nodes, moving them toward their desired state. BOS operators are used only for BOS v2 operations. Boot Orchestration Agent (BOA) is automatically launched to execute the session. A BOA executes the given operation; if the operation is a boot or a reboot, it also configures the nodes post-boot (if enabled). Cray Advanced Platform Monitoring and Control (CAPMC) service provides system-level power control for nodes in the system. CAPMC interfaces directly with the Redfish APIs to the controller infrastructure to effect power and environmental changes on the system. Hardware State Manager (HSM) tracks the state of each node and its group and role associations. Boot Script Service (BSS) stores per-node information about the iPXE boot script. When booting or rebooting, nodes consult BSS for boot artifacts (kernel, initrd, image root) and boot parameters. Simple Storage Service (S3) is an artifact repository that stores boot artifacts. CFS configures nodes using the configuration framework. Launches and aggregates the status from one or more Ansible instances against nodes (node personalization) or images (image customization). Workflows The set of allowed operations for a session are:\nboot – Boot nodes that are powered off configure – Reconfigure the nodes using the Configuration Framework Service (CFS) reboot – Gracefully power down nodes that are on and then power them back up shutdown – Gracefully power down nodes that are on The following workflows are included in this document:\nBoot and configure nodes Reconfigure nodes Power off nodes Boot and configure nodes Use case: Administrator powers on and configures select compute nodes.\nComponents: This workflow is based on the interaction of the BOS with other services during the boot process:\nWorkflow overview: The following sequence of steps occurs during this workflow.\nAdministrator creates a configuration.\nAdd a configuration to CFS.\nncn-mw# cray cfs configurations update sample-config --file configuration.json --format json Example output:\n{ \u0026#34;lastUpdated\u0026#34;: \u0026#34;2020-09-22T19:56:32Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/configmanagement.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;01b8083dd89c394675f3a6955914f344b90581e2\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yaml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;sample-config\u0026#34; } Administrator creates a session template.\nA session template is a collection of data specifying a group of nodes, as well as the boot artifacts and configuration that should be applied to them. A session template can be created from a JSON structure. It returns a session template ID if successful.\nSee Manage a session template for more information.\nAdministrator creates a session.\nCreate a session to perform the operation specified in the operation request parameter on the boot set defined in the session template. For this use case, the administrator creates a session with operation as boot and specifies the session template ID.\nncn-mw# cray bos session create --template-uuid SESSIONTEMPLATE_NAME --operation boot Launch BOA.\nThe creation of a session results in the creation of a Kubernetes BOA job to complete the operation. BOA coordinates with other services to complete the requested operation.\nBOA to HSM.\nBOA coordinates with HSM to validate node group and node status.\nBOA to S3.\nBOA coordinates with S3 to verify boot artifacts like kernel, initrd, and root file system.\nBOA to BSS.\nBOA updates BSS with boot artifacts and kernel parameters for each node.\nBOA to CAPMC.\nBOA coordinates with CAPMC to power-on the nodes.\nCAPMC boots nodes.\nCAPMC interfaces directly with the Redfish APIs and powers on the selected nodes.\nBSS interacts with the nodes.\nBSS generates iPXE boot scripts based on the image content and boot parameters that have been assigned to a node. Nodes download the iPXE boot script from BSS.\nS3 interacts with the nodes.\nNodes download the boot artifacts. The nodes boot using the boot artifacts pulled from S3.\nBOA to HSM.\nBOA waits for the nodes to boot up and be accessible via SSH. This can take up to 30 minutes. BOA coordinates with HSM to ensures that nodes are booted and Ansible can SSH to them.\nBOA to CFS.\nBOA directs CFS to apply post-boot configuration.\nCFS applies configuration.\nCFS runs Ansible on the nodes and applies post-boot configuration (also called node personalization). CFS then communicates the results back to BOA.\nReconfigure nodes Use case: Administrator reconfigures compute nodes that are already booted and configured.\nComponents: This workflow is based on the interaction of the BOS with other services during the reconfiguration process.\nWorkflow overview: The following sequence of steps occurs during this workflow.\nAdministrator creates a session template.\nA session template is a collection of data specifying a group of nodes, as well as the boot artifacts and configuration that should be applied to them. A session template can be created from a JSON structure. It returns a session template ID if successful.\nSee Manage a session template for more information.\nAdministrator creates a session.\nCreate a session to perform the operation specified in the operation request parameter on the boot set defined in the session template. For this use case, the administrator creates a session with operation as configure and specifies the session template ID.\nncn-mw# cray bos session create --template-uuid SESSIONTEMPLATE_NAME --operation configure Launch BOA.\nThe creation of a session results in the creation of a Kubernetes BOA job to complete the operation. BOA coordinates with the underlying subsystem to complete the requested operation.\nBOA to HSM.\nBOA coordinates with HSM to validate node group and node status.\nBOA to CFS.\nBOA directs CFS to apply post-boot configuration.\nCFS applies configuration.\nCFS runs Ansible on the nodes and applies post-boot configuration (also called node personalization).\nCFS to BOA.\nCFS then communicates the results back to BOA.\nPower off nodes Use cases: Administrator powers off selected compute nodes.\nComponents: This workflow is based on the interaction of the Boot Orchestration Service (BOS) with other services during the node shutdown process:\nWorkflow overview: The following sequence of steps occurs during this workflow.\nAdministrator creates a session template.\nA session template is a collection of data specifying a group of nodes, as well as the boot artifacts and configuration that should be applied to them. A session template can be created from a JSON structure. It returns a session template ID if successful.\nSee Manage a session template for more information.\nAdministrator creates a session.\nCreate a session to perform the operation specified in the operation request parameter on the boot set defined in the session template. For this use case, the administrator creates a session with operation as shutdown and specifies the session template ID.\nncn-mw# cray bos session create --template-uuid SESSIONTEMPLATE_NAME --operation shutdown Launch BOA.\nThe creation of a session results in the creation of a Kubernetes BOA job to complete the operation. BOA coordinates with the underlying subsystem to complete the requested operation.\nBOA to HSM.\nBOA coordinates with HSM to validate node group and node status.\nBOA to CAPMC.\nBOA directs CAPMC to power off the nodes.\nCAPMC to the nodes.\nCAPMC interfaces directly with the Redfish APIs and powers off the selected nodes.\nCAPMC to BOA.\nCAPMC communicates the results back to BOA.\n"
},
{
	"uri": "/docs-csm/en-12/operations/access_livecd_usb_device_after_reboot/",
	"title": "Accessing LiveCD USB Device After Reboot",
	"tags": [],
	"description": "",
	"content": "Accessing LiveCD USB Device After Reboot This is a procedure that only applies to the LiveCD USB device after the PIT node has been rebooted.\nUSB ONLY If the installation above was done from a Remote ISO.\nAfter deploying the LiveCD\u0026rsquo;s NCN, the LiveCD USB itself is unharmed and available to an administrator.\nProcedure Mount and view the USB device.\nncn-m001# mkdir -pv /mnt/{cow,pitdata} ncn-m001# mount -vL cow /mnt/cow ncn-m001# mount -vL PITDATA /mnt/pitdata ncn-m001# ls -ld /mnt/cow/rw/* Example output:\ndrwxr-xr-x 2 root root 4096 Jan 28 15:47 /mnt/cow/rw/boot drwxr-xr-x 8 root root 4096 Jan 29 07:25 /mnt/cow/rw/etc drwxr-xr-x 3 root root 4096 Feb 5 04:02 /mnt/cow/rw/mnt drwxr-xr-x 3 root root 4096 Jan 28 15:49 /mnt/cow/rw/opt drwx------ 10 root root 4096 Feb 5 03:59 /mnt/cow/rw/root drwxrwxrwt 13 root root 4096 Feb 5 04:03 /mnt/cow/rw/tmp drwxr-xr-x 7 root root 4096 Jan 28 15:40 /mnt/cow/rw/usr drwxr-xr-x 7 root root 4096 Jan 28 15:47 /mnt/cow/rw/var Look at the contents of /mnt/pitdata.\nncn-m001# ls -ld /mnt/pitdata/* Example output:\ndrwxr-xr-x 2 root root 4096 Feb 3 04:32 /mnt/pitdata/configs drwxr-xr-x 14 root root 4096 Feb 3 07:26 /mnt/pitdata/csm-0.7.29 -rw-r--r-- 1 root root 22159328586 Feb 2 22:18 /mnt/pitdata/csm-0.7.29.tar.gz drwxr-xr-x 4 root root 4096 Feb 3 04:25 /mnt/pitdata/data drwx------ 2 root root 16384 Jan 28 15:41 /mnt/pitdata/lost+found drwxr-xr-x 5 root root 4096 Feb 3 04:20 /mnt/pitdata/prep drwxr-xr-x 2 root root 4096 Jan 28 16:07 /mnt/pitdata/static Unmount the USB device to avoid corruption.\nThe corruption risk is low, but varies if large data use was done to or on the USB.\nncn-m001# umount -v /mnt/cow /mnt/pitdata Remove the USB device after it has been unmounted.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/add_a_volume_to_uas/",
	"title": "Add a Volume to UAS",
	"tags": [],
	"description": "",
	"content": "Add a Volume to UAS This procedure registers and configures a volume in UAS so that the volume can be mounted in UAIs.\nSee List Volumes Registered in UAS for examples of valid volume configurations. Refer to Elements of a UAI for descriptions of the volume configuration fields and values.\nNote the following caveats about adding volumes to UAS:\nA volume description may specify an underlying directory that is NFS-mounted on the UAI host nodes. Hard-mounted NFS file systems will stop responding indefinitely on references to their mount points if the NFS server fails or becomes unreachable from the UAI host node. This will cause new UAI creation and migration of existing UAIs to stop responding as well until the NFS issue is remedied. Multiple volumes can be configured in UAS with the same mount_path. UAS cannot create a UAI if that UAI has more than one volume specified for a given mount_path. If multiple volumes with the same mount_path exist in the UAS configuration all UAIs must be created using UAI classes that specify a workable subset of volumes. A UAI created without a UAI Class under such a UAS configuration will try to use all configured volumes and creation will fail. The volumename is a string that can describe or name the volume. It must be composed of only lowercase letters, numbers, and dashes (\u0026rsquo;-\u0026rsquo;). The volumename also must begin and end with an alphanumeric character. As with UAI images, registering a volume with UAS creates the configuration that will be used to create a UAI. If the underlying object referred to by the volume does not exist at the time the UAI is created, the UAI will, in most cases, wait until the object becomes available before starting up. This will be visible in the UAI state which will eventually move to and remain in Waiting until the underlying issue is resolved. Prerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The underlying resources for the volume should be available in the Kubernetes namespace where they will be referenced Procedure To create a volume, follow this procedure.\nUse the cray CLI to create the volume, specifying volumename, mount_path, and volume_description.\nNote difference between the UAS name for the volume type and the Kubernetes name for that type. Kubernetes uses camelCase for its type names, while UAS uses lower_case_with_underscores.\nncn-m001-pit# cray uas admin config volumes create --mount-path \u0026lt;path in UAI\u0026gt; --volume-description \u0026#39;{\u0026#34;\u0026lt;volume-kind\u0026gt;\u0026#34;: \u0026lt;k8s-volume-description\u0026gt;}\u0026#39; --volumename \u0026#39;\u0026lt;string\u0026gt;\u0026#39; For example:\nncn-m001-pit# cray uas admin config volumes create --mount-path /host_files/host_passwd --volume-description \u0026#39;{\u0026#34;host_path\u0026#34;: {\u0026#34;path\u0026#34;: \u0026#34;/etc/passwd\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34;}}\u0026#39; --volumename \u0026#39;my-volume-with-passwd-from-the-host-node\u0026#39; The example above will create a directory /host_files in every UAI configured to use this volume and mount the file /etc/passwd from the host node into that directory as a file named host_passwd. Notice the form of the --volume-description argument. It is a JSON string encapsulating an entire volume_description field as shown in the JSON output in the previous section.\nPerform List Volumes Registered in UAS to verify that the new volume is configured.\nThe new volume appears in the output of the cray uas admin config volumes list command.\nTop: User Access Service (UAS)\nNext Topic: Obtain Configuration of a UAS Volume\n"
},
{
	"uri": "/docs-csm/en-12/operations/csm_product_management/apply_security_hardening/",
	"title": "Security Hardening",
	"tags": [],
	"description": "",
	"content": "Security Hardening This is an overarching guide to further harden the security posture of a Cray System Management (CSM) system.\nIf a subset of the steps in this procedure were completed as a consequence of an install, upgrade, or other guidance, then it is safe to skip that subset following a review.\nPrerequisites None.\nProcedure Change passwords and credentials.\nPerform procedure(s) in Change Passwords and Credentials.\nRestrict access to ncn-images S3 Bucket.\nPerform procedure(s) in Restrict Access to ncn-images S3 Bucket.\nLimit Kubernetes Audit Log Retention.\nIf Kubernetes API Auditing was enabled at install, perform procedure(s) in Limit Kubernetes API Audit Log Maximum Backups.\nFailure to apply the referenced configuration could result in NCN disk space exhaustion on Kubernetes Master Nodes.\nUpdate IMS Job Access Network.\nPerform procedure(s) in Update IMS Job Access Network.\n(Optional) Change Keycloak OAuth token lifetime.\nPerform procedure(s) in Change Keycloak token lifetime.\n(Optional) Remove Kiali.\nPerform procedure(s) in Remove Kiali.\n"
},
{
	"uri": "/docs-csm/en-12/introduction/capmc_deprecation/",
	"title": "CAPMC Deprecation Notice many CAPMC v1 features are being partially deprecated",
	"tags": [],
	"description": "",
	"content": "CAPMC Deprecation Notice: many CAPMC v1 features are being partially deprecated Deprecated Features in CSM 1.0 Many CAPMC v1 REST API and CLI features are being deprecated as part of CSM version 1.0; Full removal of the following deprecated CAPMC features will happen in CSM version 1.3. Further development of CAPMC service or CLI has stopped. CAPMC has entered end-of-life but will still be generally available. CAPMC is going to be replaced with the Power Control Service (PCS) in a future release. The current API/CLI portfolio for CAPMC are being pruned to better align with the future direction of PCS. More information about PCS and the CAPMC transition will be released as part of subsequent CSM releases.\nThe API endpoints that remain un-deprecated will remain supported until their \u0026lsquo;phased transition\u0026rsquo; into PCS (e.g. Power Capping is not \u0026lsquo;deprecated\u0026rsquo; and will be supported in PCS; As PCS is developed, CAPMC\u0026rsquo;s Powercapping and PCS\u0026rsquo;s Powercapping will both function, eventually callers of the CAPMC power capping API/CLI will need to will need transition to call PCS as the API will be different.)\nHere is a list of deprecated API (CLI) endpoints:\nnode control /get_node_rules /get_node_status /node_on /node_off /node_reinit group control /group_reinit /get_group_status /group_on /group_off node energy /get_node_energy /get_node_energy_stats /get_node_energy_counter system monitor /get_system_parameters /get_system_power /get_system_power_details EPO /emergency_power_off utilities /get_nid_map "
},
{
	"uri": "/docs-csm/en-12/install/set_gigabyte_node_bmc_to_factory_defaults/",
	"title": "Set Gigabyte Node BMC to Factory Defaults",
	"tags": [],
	"description": "",
	"content": "Set Gigabyte Node BMC to Factory Defaults There are cases when a Gigabyte node BMC must be reset to its factory default settings. This page describes when this reset is appropriate, and how to use management scripts and text files to do the reset.\nSet the BMC to the factory default settings in the following cases:\nThere are problems using the ipmitool command and Redfish does not respond. There are problems using the ipmitool command and Redfish is running. When BIOS or BMC flash procedures fail using Redfish. Procedure This section refers to scripts that exist only in the PIT environment. If necessary, copy the LiveCD data from a different machine to get these scripts.\nNote: When BIOS or BMC flash procedures fail using Redfish:\nRun the do_bmc_factory_default.sh script Run ipmitool -I lanplus -U admin -P password -H BMC_or_CMC_IP mc reset cold and flash it again after five minutes. If booted from the PIT node:\nThe firmware packages are located in the HPE Cray EX HPC Firmware Pack (HFP) provided with the Shasta release. The required scripts are located in /var/www/fw/river/sh-svr-scripts Create a node.txt file and add the target node information as shown:\nExample node.txt file with two nodes:\n10.254.1.11 x3000c0s9b0 ncn-w002 10.254.1.21 x3000c0s27b0 uan01 Example node.txt file with one node:\n10.254.1.11 x3000c0s9b0 ncn-w002 Use Redfish to reset the BMC to factory default.\nOption 1: If the BMC is running version 12.84.01 or later, then run:\nncn# sh do_Redfish_BMC_Factory.sh Option 2: Use ipmitool to reset the BMC to factory defaults:\nncn# sh do_bmc_factory_default.sh Option 3: Use the power control script:\nncn# sh do_bmc_power_control.sh raw 0x32 0x66 (raw 0x32 0x66 are Gigabyte/AMI vendor-specific IPMI commands to reset to factory defaults.)\nWait five minutes (300 seconds) for the BMC and Redfish to initialize.\nncn# sleep 300 Add the default login and password to the BMC.\nncn# sh do_bmc_root_account.sh Add the default login and password to Redfish.\nIMPORTANT: If the BMC is version 12.84.01 or later, then skip this step.\nncn# sh do_Redfish_credentials.sh Make sure the BMC is not in failover mode.\nRun the script with the read option to check the BMC status:\nncn# sh do_bmc_change_mode_to_manual.sh read Example output:\n--------------------------------------------------- [ BMC: 172.30.48.33 ] =\u0026gt; Manual mode (O) The BMC is in failover mode if the previous command includes output similar to the following:\n[ BMC: 172.30.48.33 ] ==\u0026gt; Failover mode (X) \u0026lt;== If the BMC is in failover mode, then change the BMC back to manual mode:\nncn# sh do_bmc_change_mode_to_manual.sh change If the BMC is in a booted management NCN running Shasta v1.3 or later, then reapply the static IP address and clear the DHCP address from HSM/KEA.\nDetermine the MAC address in HSM for the DHCP address for the BMC, delete it from HSM, and restart KEA.\nReboot or power cycle the target nodes.\nAfter the BMC is reset to factory defaults, wait 300 seconds for BMC and Redfish initialization.\nncn# sleep 300 Add the default login and password to the BMC.\nncn# sh do_bmc_root_account.sh "
},
{
	"uri": "/docs-csm/en-12/install/boot_livecd_virtual_iso/",
	"title": "Boot LiveCD Virtual ISO",
	"tags": [],
	"description": "",
	"content": "Boot LiveCD Virtual ISO This page will walk-through booting the LiveCD .iso file directly onto a BMC.\nTopics Boot LiveCD Virtual ISO Topics Details Prerequisites BMCs\u0026rsquo; Virtual Mounts HPE iLO BMCs Gigabyte BMCs Configuring Backing up the Overlay COW FS Restoring from an Overlay COW FS Backup Details Prerequisites A Cray Pre-Install Toolkit ISO is required for this process. This ISO can be obtained from:\nThe Cray Pre-Install Toolkit ISO included in a CSM release tar file. It will have a filename similar to cray-pre-install-toolkit-sle15sp2.x86_64-1.4.10-20210514183447-gc054094.iso BMCs\u0026rsquo; Virtual Mounts Most BMCs offer a Web Interface for controlling the node and for providing access to its BIOS and firmware.\nRefer to the following pages based on your node vendor for help mounting an ISO image:\nHPE iLO BMCs Gigabyte HPE iLO BMCs HPE iLO BMCs allow for booting directly from an HTTP-accessible ISO location.\nEnter the Virtual Media URL, select Boot on Next Reset, and click Insert Media.\nReboot by selecting Reset in the top right power menu.\nOpen the virtual terminal by choosing the HTML5 Console option when clicking the terminal image in the bottom left corner.\nNOTE: It may appear that the boot is stalled at a line of EXT4-fs (loop1): mounted ... or Starting dracut pre-mount hook.... This is the step when it actually begins downloading the ISO\u0026rsquo;s SquashFS root file system and can take a few minutes\nGigabyte BMCs Gigabyte BMCs allow for booting over HTTP.\nWARNING: Do not try to boot over NFS or CIFS because of problems in the Gigabyte firmware.\nGo to the BMC settings and setup the remote ISO for the protocol and node.\nAccess the BMC\u0026rsquo;s web interface and navigate to Settings -\u0026gt; Media Redirection Settings -\u0026gt; General Settings.\nEnable Remote Media Support and Mount CD/DVD and then fill in the server IP address or DNS name and the path to the server.\nNOTE: The Gigabyte URL appears to not allow certain characters and has a limit on path length. You may need to move or rename the ISO to a location with a smaller file name.\nNavigate to Image Redirection -\u0026gt; Remote Images.\nClick on the Start button to start the Virtual ISO mount.\nReboot the node and select the Virtual CDROM option from the manual boot options.\nConfiguring Boot LiveCD Virtual ISO Topics Details Prerequisites BMCs\u0026rsquo; Virtual Mounts HPE iLO BMCs Gigabyte BMCs Configuring Backing up the Overlay COW FS Restoring from an Overlay COW FS Backup The ISO boots with no password, requiring one be set on first login. Continue the bootstrap process by setting the root password following the procedure First Login.\nNOTE: The root OS / directory is writable without persistence. This means that restarting the machine will result in all changes being lost. Before restarting, consider following Backing up the Overlay COW FS and the accompanying Restoring from an Overlay COW FS Backup section.\nBacking up the Overlay COW FS Backup the writable overlay upper-dir so that changes are not lost after a reboot or when updating the ISO.\nThis requires a location to scp a tar file as a backup.\ntar czf /run/overlay.tar.gz -C /run/overlayfs/rw . scp /run/overlay.tar.gz \u0026lt;somelocation\u0026gt; NOTE: To reduce the size of the backup, delete any SquashFS files first, or exclude them in the tar command using --exclude='*.squashfs'. Those will need to be repopulated after you restoring the backup.\nRestoring from an Overlay COW FS Backup Restore a backed up tar file from the previous command with the following:\nscp \u0026lt;somelocation\u0026gt; /run/overlay.tar.gz tar xf /run/overlay.tar.gz -C /run/overlayfs/rw mount -o remount / If the squashfs files were excluded from the backup, repopulate them following the configuration section.\n"
},
{
	"uri": "/docs-csm/en-12/",
	"title": "Cray System Management Documentation",
	"tags": [],
	"description": "",
	"content": "Cray System Management Documentation Scope and audience Table of contents Copyright and license Scope and audience The documentation included here describes the Cray System Management (CSM) software, how to install or upgrade CSM software, and related supporting operational procedures to manage an HPE Cray EX system. CSM software is the foundation upon which other software product streams for the HPE Cray EX system depend.\nThe CSM installation prepares and deploys a distributed system across a group of management nodes organized into a Kubernetes cluster which uses Ceph for utility storage. These nodes perform their function as Kubernetes master nodes, Kubernetes worker nodes, or utility storage nodes with the Ceph storage.\nSystem services on these nodes are provided as containerized micro-services packaged for deployment via Helm charts. Kubernetes orchestrates these services and schedules them on Kubernetes worker nodes with horizontal scaling. Horizontal scales increases or decreases the number of service instances as demand for them varies, such as when booting many compute nodes or application nodes.\nThis information is intended for system installers, system administrators, and network administrators of the system. It assumes some familiarity with standard Linux and open source tools, such as shell scripts, revision control with git, configuration management with Ansible, YAML, JSON, and TOML file formats, etc.\nTable of contents Introduction to CSM Installation\nThis chapter provides an introduction to using the CSM software to manage the HPE Cray EX system which also describes the scenarios for installation and upgrade of CSM software, how product stream updates for CSM are delivered, the operational activities done after installation for on-going management of the HPE Cray EX system, differences between previous release and this release, and conventions used in this documentation.\nPre-Install Steps\nThis chapter outlines how to set up default credentials for River BMCs and ServerTech PDUs, which must be done before the initial installation of CSM, in order to enable HSM software to interact with River Redfish BMCs and PDUs.\nUpdate CSM Product Stream\nThis chapter explains how to get the CSM product release, get any patches, update to the latest documentation, and check for any Field Notices or Hotfixes.\nInstall CSM\nThis chapter provides an order list of procedures which can be used for CSM software installation or reinstall that indicate when to do operational tasks as part of the installation workflow. Updating software is in another chapter. Installation of the CSM product stream has many steps in multiple procedures which should be done in a specific order. Information about the HPE Cray EX system and the site is used to prepare the configuration payload. The initial node used to bootstrap the installation process is called the PIT node because the Pre-Install Toolkit is installed there. Once the management network switches have been configured, the other management nodes can be deployed with an operating system and the software to create a Kubernetes cluster utilizing Ceph storage. The CSM services provide essential software infrastructure including the API gateway and many micro-services with REST APIs for managing the system. Once administrative access has been configured, the installation of CSM software and nodes can be validated with health checks before doing operational tasks like the check and update of firmware on system components or the preparation of compute nodes.\nUpgrade CSM\nThis chapter provides an order list of procedures which can be used to update CSM software that indicate when to do operational tasks as part of the software upgrade workflow. There are procedures to prepare the HPE Cray system for the upgrade, and update the management network, the management nodes, and the CSM services. After the upgrade of CSM software, the CSM health checks are used to validate the system before doing any other operational tasks like the check and update of firmware on system components.\nCSM Operational Activities\nThis chapter provides an unordered set of administrative procedures required to operate an HPE Cray EX system with CSM software and grouped into several major areas:\nCSM Product Management Artifact Management Boot Orchestration Compute Rolling Upgrade Configuration Management Console Management Firmware Management Hardware State Manager Image Management Kubernetes Network Management Node Management Package Repository Management Power Management Resiliency River Endpoint Discovery Service Security And Authentication System Configuration Service System Layout Service System Management Health UAS User And Admin Topics Utility Storage Validate CSM Health CSM Troubleshooting Information\nThis chapter provides information about some known issues in the system and tips for troubleshooting Kubernetes.\nCSM Background Information\nThis chapter provides background information about the NCNs (non-compute nodes) which function as management nodes for the HPE Cray EX system. This information is not normally needed to install or upgrade software, but provides background which might be helpful for troubleshooting an installation.\nGlossary\nThis chapter provides explanations of terms and acronyms used throughout the rest of this documentation.\nCopyright and license See LICENSE.\n"
},
{
	"uri": "/docs-csm/en-12/background/certificate_authority/",
	"title": "Certificate Authority",
	"tags": [],
	"description": "",
	"content": "Certificate Authority While a system is being installed for the first time, a certificate authority (CA) is needed. This can be generated for a system, or one can be supplied from a customer intermediate CA. Outside of a new installation, there is no supported method to rotate or change the platform CA in this release.\nTopics Overview Use default platform-generated CA Customize platform-generated CA Use external CA Overview At install time, a PKI certificate authority can either be generated for a system, or a customer can opt to supply their own intermediate CA.\nOutside of a new installation, there is currently no supported method to rotate (change) the platform CA. The ability to rotate CAs is anticipated as part of a future release.\nSealed Secrets, part of shasta-cfg, are used by the installation process to inject CA material in an encrypted form. Vault (cray-vault instance) ultimately sources and stores the CA from a Kubernetes secret (the result of decrypting the corresponding Sealed Secret).\nThe resulting CA will be used to sign multiple workloads on the platform (such as ingress, mTLS for PostgreSQL Clusters, Spire, etc.).\nManagement of Sealed Secrets should ideally take place on a secure workstation.\nUse default platform-generated CA In shasta-cfg, there is a Sealed Secret generator named platform_ca. By default, the customizations.yaml file will contain a generation template to use this generator, and will create a Sealed Secret named generated-platform-ca-1. The cray-vault overrides in customizations.yaml contain both of the following:\nA templated reference to expand the generated-platform-ca-1 Sealed Secret. Directives instructing Vault to load the CA material on start-up \u0026ndash; ultimately initializing a HashiCorp Vault PKI Engine instance with the material. Note: The intermediate CA gets installed into Vault, not the root CA (as generated). Use of a root CA is not recommended.\nThe resulting default configuration (prior to seeding customizations) should look like the following customizations.yaml snippet:\nspec: # ...lines omitted... kubernetes: sealed_secrets: # ...lines omitted... gen_platform_ca_1: generate: name: generated-platform-ca-1 data: - type: platform_ca args: root_days: 3651 int_days: 3650 root_cn: \u0026#34;Platform CA\u0026#34; int_cn: \u0026#34;Platform CA - L1\u0026#34; services: # ...lines omitted... cray-vault: sealedSecrets: - \u0026#34;{{ kubernetes.sealed_secrets.gen_platform_ca_1 | toYaml }}\u0026#34; pki: customCA: enabled: true secret: generated-platform-ca-1 private_key: int_ca.key certificate: int_ca.crt ca_bundle: root_ca.crt # ...lines omitted... The platform_ca generator produces RSA CAs with a 3072-bit modulus, using SHA256 as the base signature algorithm.\nCustomize platform-generated CA The platform_ca generator inputs can be customized, if desired. Notably, the root_days, int_days, root_cn, and int_cn fields can be modified. While the shasta-cfg documentation on the use of generators supplies additional detail, the *_days settings control the validity period and the *_cn settings control the common name value for the resulting CA certificates. Ensure that the Sealed Secret name reference in spec.kubernetes.services.cray-vault.sealedSecrets is updated if opting to use a different name.\nOutside of a new installation, there is currently no supported method to rotate (change) the platform CA. Set validity periods accordingly. The ability to rotate CAs is anticipated as part of a future release.\nUse external CA The static_platform_ca generator, part of shasta-cfg, can be used to supply an external CA private key, certificate, and associated upstream CAs that form the trust chain. The generator attempts to prevent a root CA from being supplied. The entire trust chain up to the root CA certificate must also be supplied.\nOutside of a new installation, there is currently no supported method to rotate (change) the platform CA. Ensure that validity periods are set accordingly for external CAs used in this process. The ability to rotate CAs is anticipated as part of a future release.\nHere is an example customizations.yaml snippet illustrating the generator input to inject a static CA:\nspec: # ...lines omitted... kubernetes: sealed_secrets: # ...lines omitted... external_platform_ca_1: generate: name: external-platform-ca-1 data: - type: static_platform_ca args: key: |- -----BEGIN PRIVATE KEY----- MIIG/gIBADANBgkqhkiG9w0BAQEFAASCBugwggbkAgEAAoIBgQDvhzXCUmGalTDo uswnppXbM+E+OwU79xvaZBsiGEDPpERPZfizpSO3/6IWnYvCUCrb1V4rIhkSKGYq LLVMhmEkfiEImDnx+ksbZau3/w23ogP4qj+BpbTRF707//IOfXgRSD1Q+mVQ7MVo crOt8e/hR4DqZjbkWOrw9pdrfvV159o6x9RVpip33BkAtDzONYApY6ePhzS1BFmo I9R0zMGNeVpy7I2m47YUwpyGAWjRoof0P2BFHX7vdEoJE/TWAlbbiqlM9OHmR85J I/O0MwP63C2Eqn9HajbF1GPVw2IvGN6fE3THtmVDVwxD17cFsKxtVl8gMHljkw9V I+U5piuIfDPvaCoUIC3hlv7jsQs9j52LyZZF3sOKP3xsGG4a5ThqK08EKEgrFovg MYsQrt8aSx7o/7K6IzDOD9QVf7dmkFVxlbPGAjR6nlQ5aW7gFEOAr1CbbZFS+lKi KGjHGraIv93MTqqToE7yRJ6Sv0yP7U9clCi6MNi89AWFfZDkLAsCAwEAAQKCAYAW R61odeE+T8JM45M53PTzfs/kyfiiq0mb9tPPSBI/Pjhcak/H5gR8iPq6v8zQNkTG TgKEYJeUaM2X/rCefaFrk4/fDMnXCEEUO1DNvJu6CQf1iWB+3rsC+AJSImyRjHou oVmSvrfN3zg9ju3HsElv2wbSxs80TlEMOOO8zAJpBTf3X78QeHRa0c5BkoJVbASP 1QUxBJKSg+UTDsIkWydl0XPoXLiQXX4CUFfe3yKw3T1oKrz5sNSt0VNRpNmRToY3 s96Teuv2iBUnN4UciuFajgjlP0Wt2YvntWoYcwJ7mOjwo6Ru5IXdPMeLBx/xKeLF j2SnPiozSAg2OV8G+yffOIcV7598s2Jh9LpgEX0S2NWPdSrjp33IWM9clivzQXaV fFZtFcb3dkrXTt2jVuj6hQR5dsVMC/D/sfORPuAudejmUkAYmozTI9vgcOJpWw3h AT8KBZ6xR3ifr3/GwJk9eosFMeLCTnUprhgbMzM9sde31NOzgYPhiPrN4GJRp4EC gcEA+e3m7HNrSY766GOaiYwiVdzLftL7i6Ie0QTHqJLLESu2/XyxuoML6IRXc+Df A/HVtuwJMqxEe3APvOcwS/Qs6qnPhh0WNz9vJ+3D/uo7Om3cbIR8J6QlsQID9Kas /OAOqxcbtedkkiDSzVM1SPzNh+R85FBDK2xBM433Eu9xET0V8YZegT99SWg72l8+ M37/EhGvtyQpYpY8lYs8pI3Xj7IRLt+jkPKu59uDdATMvVntOMheddpTwYW7XdUI M67VAoHBAPVYodD9Hoe5AcUBrahM7trGzAw3z8fom5lf/wmzJ6Mow8lgH6tliwCs 4NS5PR45olONhK7o7vd/PXvzP1QSIHLNbInveCH29O0ZmBasDlF/eDT+Hcdzq0sw YWUR+9mX5kNS3DuZaWy6f2PDQC+mzPn1yxGmwL2yW0sY6ExfKjmFVSjqG7Mt/oMo BriKaANd3ctge3aRm2MHniXOPq+jC2Zq1rRopWgWIWDzchQsyl4e6iHs5s80nQsE R9nrC6CfXwKBwQDMlwLB7HmW7YRXV7HZhu1UfDnYx71CwKOZVuBaDlBM7gwN1VVn 6H6HCE7OfPYStJTN+MpOwNYOdd1sNZRDmM5sCjXnA0h8UWEcvnYC5ps1aVlXO9ym VqjEDXJPg2F4X7GiPHhin9ikBlqJ2eN0q/1TkKbr/wf9M9Dr8vqedYOJKQgdfnE+ PErDHKBiUjUI0pzanb/Jm8CFA5b0k9ZAnhwndQy74jZzITYsdnVVM9il6EdYhC1P LDoD4QVP+mOMa0ECgcEA0ZCKb4O1j0Kk000ysx47q53A7vLBRUVXmzOXGgbwZXpN efXkNze9+q6wQKOVI/sgv3OTEQAgFkGWGAjXYA03sDftbQiiOYjC/r8s3LjMZiqW V9VzREl11/yURIuO7vbDlV/yg+nvVhMa+vDtI4a7cQrVENe5rI7rUgMNcSacX5OX ASKu1GcGDaujyf9XBwEnkS9xZf7LllQMbshzXPzMoQfDK0hzeKvmiPSIzdjQZoLL hHzhTb3oIl/eq7IMNX/LAoHAYuVeWbSXROyXITXrYcYMwgtYjjUWThQmrLQImJjj HDUNMqq8w8OaQsV+JpZ0lwukeYst3d8vH8Eb4UczUaR+oJpBeEmXjXCGYG4Ec1EQ H72VrrZoJowoqORDSp88h+akcF6+vPJPuNC/Ea7+eAeiYqgxOX5nc2uLjZxBt4OC AhKMY5mnBN2pfAkGVpuyUw3dqGctTSCT0jnxvFPXpldgdAmXi2NTPqPd0IzmLKNG jja1TCeqn9XRTy+EArf1bYi+ -----END PRIVATE KEY----- cert: |- -----BEGIN CERTIFICATE----- MIIEZTCCAs2gAwIBAgIJAKnqv1FyMOp/MA0GCSqGSIb3DQEBCwUAMFsxDzANBgNV BAoMBlNoYXN0YTERMA8GA1UECwwIUGxhdGZvcm0xGjAYBgNVBAMMEVJvb3QgR2Vu ZXJhdGVkIENBMRkwFwYDVQQDDBBQbGF0Zm9ybSBSb290IENBMB4XDTIwMDcwMTIz MjU1MVoXDTIwMDcxMTIzMjU1MVowJDEPMA0GA1UECgwGU2hhc3RhMREwDwYDVQQL DAhQbGF0Zm9ybTCCAaIwDQYJKoZIhvcNAQEBBQADggGPADCCAYoCggGBAO+HNcJS YZqVMOi6zCemldsz4T47BTv3G9pkGyIYQM+kRE9l+LOlI7f/ohadi8JQKtvVXisi GRIoZiostUyGYSR+IQiYOfH6Sxtlq7f/DbeiA/iqP4GltNEXvTv/8g59eBFIPVD6 ZVDsxWhys63x7+FHgOpmNuRY6vD2l2t+9XXn2jrH1FWmKnfcGQC0PM41gCljp4+H NLUEWagj1HTMwY15WnLsjabjthTCnIYBaNGih/Q/YEUdfu90SgkT9NYCVtuKqUz0 4eZHzkkj87QzA/rcLYSqf0dqNsXUY9XDYi8Y3p8TdMe2ZUNXDEPXtwWwrG1WXyAw eWOTD1Uj5TmmK4h8M+9oKhQgLeGW/uOxCz2PnYvJlkXew4o/fGwYbhrlOGorTwQo SCsWi+AxixCu3xpLHuj/srojMM4P1BV/t2aQVXGVs8YCNHqeVDlpbuAUQ4CvUJtt kVL6UqIoaMcatoi/3cxOqpOgTvJEnpK/TI/tT1yUKLow2Lz0BYV9kOQsCwIDAQAB o2MwYTAPBgNVHRMBAf8EBTADAQH/MA4GA1UdDwEB/wQEAwIBBjAdBgNVHQ4EFgQU uNa6qcbJsHdxo6k8kaR5o53DNbIwHwYDVR0jBBgwFoAU/SFNwDBMcAYWBC2SCsDf OyZJbEMwDQYJKoZIhvcNAQELBQADggGBAD8O1Vg9WLFem0RZiZWjtXiNOTZmaksE +a49CE7yGqyETljlVOvbkTUTr4eJnzq2prYJUF8QavSBs38OahcxkTU2GOawZa09 hFc1aBiGSPAxTxJqdHV+G3QZcce1CG2e9VyrxqNudosNRNBEPMOsgg4LpvlRqMfm QhPEJcfvVaCopDZBFXLBPxqmt9BckWFmTSsK09xnrCE/40YD69hdUQ6USJaz9/cd UfNm0HIugRUMvFUP2ytdJmbV+1YQbfVsFrKU4aClrMg+ECX83od5N1TUNQwMePLh IizLGoGDF353eRVKxlzyI724Ni9W82rMW66TQdA7vU6liItHYrhDmcZ+mK2R0F5B ZuYjsLf/BCQ1uDv/bsVG40ogjH/eI/qfhRIzbgVVTF74uKG97pOakp2iQaG9USFd 9/s6ouQQXfkDZ2a/vzs8SBD4eIx7vmeABPRqlHTE8VzohxugxMbJNMdZRPGrEeH6 uddqVNpMH9ehQtsDdt0nmfVIy9/An3BKFw== -----END CERTIFICATE----- ca_bundle: |- -----BEGIN CERTIFICATE----- MIIEezCCAuOgAwIBAgIJAMjuQjQKUpUtMA0GCSqGSIb3DQEBCwUAMFsxDzANBgNV BAoMBlNoYXN0YTERMA8GA1UECwwIUGxhdGZvcm0xGjAYBgNVBAMMEVJvb3QgR2Vu ZXJhdGVkIENBMRkwFwYDVQQDDBBQbGF0Zm9ybSBSb290IENBMB4XDTIwMDcwMTIz MjU1MVoXDTIwMDcxMTIzMjU1MVowWzEPMA0GA1UECgwGU2hhc3RhMREwDwYDVQQL DAhQbGF0Zm9ybTEaMBgGA1UEAwwRUm9vdCBHZW5lcmF0ZWQgQ0ExGTAXBgNVBAMM EFBsYXRmb3JtIFJvb3QgQ0EwggGiMA0GCSqGSIb3DQEBAQUAA4IBjwAwggGKAoIB gQDQ0DTdZmqCOfrWb8KTXJ0hT1r2G51rRE5eAp8d/PoVCgV1gg5h1+jbiv3yYd2R BgM/CPZPvEJaL03wR1gO9NiGEXh1ALd8+yv1O1VRKNb6JuB5cPZFHE3Z8El6aGMc zrqN1ZekRPrZMM1W5Iw78olOMZvsxYw0ZIJqfKOWYB9jYUNM1KohHVj65f/HD/Em kC+9VFhepRV9z21q6fBU13bMz6/NlW19omvbTMwrVSPbYi2nSzqOfi00GXmVh/9Q WElBrAeiGLOsjWkeQ8sFF8ab4SSvzLAAilyQqkBhz2jIxB4L7iG+b9KEgVLeOoMH 1Rs7RhduOMEQypZGVA/vsu/86/5ctM1Cu60mZP+s5B7oT2rwypz0ihLiVCaDCcS5 lDK7PPT5GxZPD8TAqX0SgtaxJnSB/RzavGPSS7efFvlWXh18frwlwa+FgOnyCw1/ qR3BHarcZX9XZivBQSupxQAaUNPMlk0N4wYi6oWrmf21zwd7NtZAinxC2F98J1sn sK8CAwEAAaNCMEAwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwHQYD VR0OBBYEFP0hTcAwTHAGFgQtkgrA3zsmSWxDMA0GCSqGSIb3DQEBCwUAA4IBgQAp ApgLdQBK6fZ7CWlEWwXSKxcjv3akuSqf1NXfn/J9e1rAqqyYoTDE9DXG9dYHL9OA p78KLsLy9fQmrLMmjacXw49bpXDG6XN1WLJfhgQg3j7lXvOvXyxynOgKDtBlroiU nMoK+or9lF2lBIuY34GPyZCL/+vB8s1tu0dGBDgHMUL8/k5d27sdGZZUljC7CgcC k+ABrv19IygDpZpZ6m5N27xajnKpJSjXOfpMCPdhCuNRMgMTX6x8bxZzVAx9ogQ8 16ZzAziB4iMXeCggaY/+YnoEstzTDPXB8FuqeGEVt63Y9ZA7NgWYvVExtKFGGhOL lnEhCLjQyu6/LgOJNfNM9EofaE/IU+i0talgFA+ygSChmYdXzFJn4EfAY9XbwEwV Pw+NHbkpv82jIpc+mopuMRdDO5OyFb+IGkn7ITUFE9N+u97oz2PjD5nQ/Z5DGjBu y3sefnrlqaRanHYkmOnOBTwImPSq8RE8eJP2aRrnu+2YrnoACXxS+XWUXtNhXJ4= -----END CERTIFICATE----- services: # ...lines omitted... cray-vault: sealedSecrets: - \u0026#34;{{ kubernetes.sealed_secrets.external_platform_ca_1 | toYaml }}\u0026#34; pki: customCA: enabled: true secret: external-platform-ca-1 private_key: int_ca.key certificate: int_ca.crt ca_bundle: ca_bundle.crt # ...lines omitted... Only RSA-based CAs with 3072- or 4096-bit moduli, using RSA256 as a signature/digest algorithm, have been tested and are supported. Also note, the generator does not support password-protected private keys.\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2.1/upgrade_network/",
	"title": "Upgrade and validate Switch Configurations",
	"tags": [],
	"description": "",
	"content": "Upgrade and validate Switch Configurations Prerequisites SSH access to the switches or the running configuration file Upgrade or install latest version of CANU Install/Upgrade CANU Run canu --version to see version CANU installed with version 1.6.13 or greater. Use the paddle file (CCJ) if available, or validate an up-to-date SHCD Validate SHCD System Layout Service (SLS) input file. Collect Data CAUTION: All of these steps should be done using an out-of-band connection. This process is disruptive and will require downtime.\nGenerate Configuration Files Ensure that the correct architecture (-a parameter) is selected for the setup in use.\nThe following are the different architectures that can be specified:\nTds – Aruba-based Test and Development System. These are small systems characterized by Kubernetes NCNs cabled directly to the spine switches. Full – Aruba-based Leaf-Spine systems. These are usually customer production systems. V1 – Any Dell and Mellanox based systems. Generating a configuration file can be done for a single switch, or for the full system. Below are example commands for both scenarios:\nImportant: Modify the following items in your command:\n--csm : Which CSM version configuration do you want to use? For example, 1.2 or 1.0 NOTE: Only major and minor versions of CSM are tracked at this time. CANU bug fixes are captured in the latest CANU version and do not align with CSM bug fix versions.\n--a : What is the system architecture? (See above)\n--ccj : Match the ccj.json file to the one you created for your system.\n--sls : Match the sls_file.json to the one you created for your system.\n--custom-config : Pass in a switch configuration file that CANU will inject into the generated configuration. For more information, see the CANU documentation.\nGenerate switch configuration files for the entire system:\nncn# canu generate network config --csm 1.2 -a full --ccj system-ccj.json --sls-file sls_file.json --custom-config system-custom-config.yaml --folder generated Compare the generated CSM 1.2 switch configurations with running configurations Compare the current running configuration with the generated configuration.\nEssentially this would be to backup current configuration to your workstation and comparing it against the CANU generated configuration.\nExample of CANU pulling configuration.\nncn# canu validate switch config --vendor \u0026lt;aruba/dell/mellanox\u0026gt; --ip \u0026lt;192.168.1.1\u0026gt; --username USERNAME --password PASSWORD --generated ./generated/sw-spine-001.cfg Doing file comparisons on your local machine:\nComparing configuration file for single switch: ncn# canu validate switch config --running ./running/sw-spine-001.cfg --generated ./generated/sw-spine-001.cfg Example of output of the validate switch configuration: ncn# canu validate switch config --running ./running/sw-spine-001.cfg --generated ./generated/sw-spine-001.cfg Please enter the vendor (Aruba, Dell, Mellanox): Mellanox - interface mlag-port-channel 6 shutdown - interface mlag-port-channel 5 shutdown + interface mlag-port-channel 6 no shutdown + interface mlag-port-channel 5 no shutdown ------------------------------------------------------------------------- Config differences between running config and generated config lines that start with a minus \u0026#34;-\u0026#34; and RED: Config that is present in running config but not in generated config lines that start with a plus \u0026#34;+\u0026#34; and GREEN: Config that is present in generated config but not in running config. CANU-generated switch configurations will not include any ports or devices not defined in the model. These were previously discussed in the \u0026ldquo;Validate the SHCD section\u0026rdquo; but include edge uplinks (CAN/CMN) and custom configurations applied by the customer. When looking at the generated configurations being applied against existing running configurations CANU will recommend removal of some critical configurations. It is vital that these devices and configurations be identified and protected. This can be accomplished in three ways:\nGenerate Switch configuration including custom configurations. Custom configuration\nBased on experienced networking knowledge, manually reorder the proposed upgrade configurations. This may require manual exclusion of required configurations which the CANU analysis says to remove.\nSome devices may be used by multiple sites and may not currently be in the CANU architecture and configuration. If a device type is more universally used on several sites, then it should be added to the architectural and configuration definitions via the CANU code and Pull Request (PR) process.\nAnalyze CSM 1.2.1 configuration upgrade Configuration updates depend on the current version of network configuration. Upgrading from CSM 1.2 configuration to CSM 1.2.1 should be fairly straight forward.\nAlways before making configuration changes, analyze the changes shown in the above configuration diff section.\n:exclamation: All of these steps should be done using an out of band connection. This process is disruptive and will require downtime :exclamation:\nCaveats and known issues Mellanox and Dell configuration remediation support is limited. Some configuration may need to be applied in a certain order. Example: Customer VRF needs to be applied before adding interfaces/routes to the VRF. When applying certain configuration it may wipe out pre-existing configuration. An example of this would be adding a VRF to a port. Warnings Understanding the switch configuration changes is critical. The following configurations risk a network outage, if not applied correctly:\nGenerating switch configuration without preserving site-specific values (by using the --custom-configuration flag). Changes to ISL (MAGP, VSX, etc.) configurations. Changes to Spanning Tree. Changes to ACLs or ACL ordering. Changes to VRF. Changes to default route. Changes to MLAG/LACP. Return to CSM 1.2.1 Patch Installation Instructions\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2.2/upgrade_network/",
	"title": "Upgrade and validate Switch Configurations",
	"tags": [],
	"description": "",
	"content": "Upgrade and validate Switch Configurations Prerequisites SSH access to the switches or the running configuration file Upgrade or install latest version of CANU Install/Upgrade CANU Run canu --version to see version CANU installed with version 1.6.13 or greater. Use the paddle file (CCJ) if available, or validate an up-to-date SHCD Validate SHCD System Layout Service (SLS) input file. Collect Data CAUTION: All of these steps should be done using an out-of-band connection. This process is disruptive and will require downtime.\nGenerate Configuration Files Ensure that the correct architecture (-a parameter) is selected for the setup in use.\nThe following are the different architectures that can be specified:\nTds – Aruba-based Test and Development System. These are small systems characterized by Kubernetes NCNs cabled directly to the spine switches. Full – Aruba-based Leaf-Spine systems. These are usually customer production systems. V1 – Any Dell and Mellanox based systems. Generating a configuration file can be done for a single switch, or for the full system. Below are example commands for both scenarios:\nImportant: Modify the following items in your command:\n--csm : Which CSM version configuration do you want to use? For example, 1.2 or 1.0 NOTE: Only major and minor versions of CSM are tracked at this time. CANU bug fixes are captured in the latest CANU version and do not align with CSM bug fix versions.\n--a : What is the system architecture? (See above)\n--ccj : Match the ccj.json file to the one you created for your system.\n--sls : Match the sls_file.json to the one you created for your system.\n--custom-config : Pass in a switch configuration file that CANU will inject into the generated configuration. For more information, see the CANU documentation.\nGenerate switch configuration files for the entire system:\nncn# canu generate network config --csm 1.2 -a full --ccj system-ccj.json --sls-file sls_file.json --custom-config system-custom-config.yaml --folder generated Compare the generated CSM 1.2 switch configurations with running configurations Compare the current running configuration with the generated configuration.\nEssentially this would be to backup current configuration to your workstation and comparing it against the CANU generated configuration.\nExample of CANU pulling configuration.\nncn# canu validate switch config --vendor \u0026lt;aruba/dell/mellanox\u0026gt; --ip \u0026lt;192.168.1.1\u0026gt; --username USERNAME --password PASSWORD --generated ./generated/sw-spine-001.cfg Doing file comparisons on your local machine:\nComparing configuration file for single switch: ncn# canu validate switch config --running ./running/sw-spine-001.cfg --generated ./generated/sw-spine-001.cfg Example of output of the validate switch configuration: ncn# canu validate switch config --running ./running/sw-spine-001.cfg --generated ./generated/sw-spine-001.cfg Please enter the vendor (Aruba, Dell, Mellanox): Mellanox - interface mlag-port-channel 6 shutdown - interface mlag-port-channel 5 shutdown + interface mlag-port-channel 6 no shutdown + interface mlag-port-channel 5 no shutdown ------------------------------------------------------------------------- Config differences between running config and generated config lines that start with a minus \u0026#34;-\u0026#34; and RED: Config that is present in running config but not in generated config lines that start with a plus \u0026#34;+\u0026#34; and GREEN: Config that is present in generated config but not in running config. CANU-generated switch configurations will not include any ports or devices not defined in the model. These were previously discussed in the \u0026ldquo;Validate the SHCD section\u0026rdquo; but include edge uplinks (CAN/CMN) and custom configurations applied by the customer. When looking at the generated configurations being applied against existing running configurations CANU will recommend removal of some critical configurations. It is vital that these devices and configurations be identified and protected. This can be accomplished in three ways:\nGenerate Switch configuration including custom configurations. Custom configuration\nBased on experienced networking knowledge, manually reorder the proposed upgrade configurations. This may require manual exclusion of required configurations which the CANU analysis says to remove.\nSome devices may be used by multiple sites and may not currently be in the CANU architecture and configuration. If a device type is more universally used on several sites, then it should be added to the architectural and configuration definitions via the CANU code and Pull Request (PR) process.\nAnalyze CSM 1.2.2 configuration upgrade Configuration updates depend on the current version of network configuration. Upgrading from CSM 1.2 or CSM 1.2.1 configuration to CSM 1.2.2 should be fairly straight forward.\nAlways before making configuration changes, analyze the changes shown in the above configuration diff section.\n:exclamation: All of these steps should be done using an out of band connection. This process is disruptive and will require downtime :exclamation:\nCaveats and known issues Mellanox and Dell configuration remediation support is limited. Some configuration may need to be applied in a certain order. Example: Customer VRF needs to be applied before adding interfaces/routes to the VRF. When applying certain configuration it may wipe out pre-existing configuration. An example of this would be adding a VRF to a port. Warnings Understanding the switch configuration changes is critical. The following configurations risk a network outage, if not applied correctly:\nGenerating switch configuration without preserving site-specific values (by using the --custom-configuration flag). Changes to ISL (MAGP, VSX, etc.) configurations. Changes to Spanning Tree. Changes to ACLs or ACL ordering. Changes to VRF. Changes to default route. Changes to MLAG/LACP. Return to CSM 1.2.2 Patch Installation Instructions\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/scripts/sls/sls_updater.py_technical_details/",
	"title": "sls updater.py Technical Details",
	"tags": [],
	"description": "",
	"content": "sls_updater.py Technical Details No action needed. Informational purposes only.\nActions and order This migration is performed offline for data security. The running SLS file is first dumped, then the migration script is run and a new, migrated output file is created.\nMigrate switch naming (in order): leaf to leaf-bmc, and agg to leaf. Remove api-gateway entries from HMLB subnets for CSM 1.2 security. Remove kubeapi-vip reservations for all networks except NMN. Create the new BICAN \u0026ldquo;toggle\u0026rdquo; network. Migrate the existing CAN to CMN. Create the CHN network. Convert IP addresses of the CAN network. Create MetalLB Pools and ASN entries on CMN and NMN networks. Update uai_macvlan in NMN DHCP ranges and uai_macvlan VLAN. Remove unused user networks (CAN or CHN) if requested (--retain-unused-user-network to keep). Migrate switch names Switch names change in CSM 1.2 and must be applied in the following order:\nleaf switches become leaf-bmc switches. agg switches become leaf switches. This needs to be done in the order listed above.\nRemove api-gateway / istio-ingress-gateway reservations from HMNLB subnets For CSM 1.2, the API gateway no longer listens on the HMNLB MetalLB address pool. These aliases provided DNS records and have been removed in CSM 1.2.\nCreate the BICAN network \u0026ldquo;toggle\u0026rdquo; New for CSM 1.2: The BICAN network ExtraProperties value of SystemDefaultRoute is used to point to the CAN, CHN, or CMN, and is used by utilities to systematically toggle routes.\nMigrate existing CAN to new CMN Using the existing CAN as a template, create the CMN. The same IP addresses will be preserved for NCNs (bootstrap_dhcp). A new network_hardware subnet will be created where the end of the previous bootstrap_dhcp subnet existed to contain switching hardware. MetalLB pools in the bootstrap_dhcp subnet will be shifted around to remain at the end of the new bootstrap subnet.\nCreate the CHN With the original CAN as a template, the new CHN will be created. IP addresses will come from the --customer-highspeed-network \u0026lt;vlan\u0026gt; \u0026lt;ipaddress\u0026gt; (or its defaults). This is be created by default, but can be removed (if not needed or desired) by using the --retain-unused-user-network flag.\nConvert the IP addresses of the CAN Since the original/existing CAN has been converted to the new CMN, the CAN must have new IP addresses. These are provided using the --customer-access-network \u0026lt;vlan\u0026gt; \u0026lt;ipaddress\u0026gt; (or its defaults). This CAN conversion will happen by default, but the new CAN may be removed (if not needed or desired) by using the --retain-unused-user-network flag.\nAdd BGP peering information to CMN and NMN MetalLB and switches now obtain BGP peers using SLS data.\n--bgp-asn INTEGER RANGE The autonomous system number for BGP router [default: 65533;64512\u0026lt;=x\u0026lt;=65534] --bgp-cmn-asn INTEGER RANGE The autonomous system number for CMN BGP clients [default: 65534;64512\u0026lt;=x\u0026lt;=65534] --bgp-nmn-asn INTEGER RANGE The autonomous system number for NMN BGP clients [default: 65533;64512\u0026lt;=x\u0026lt;=65534] In CMN and NMN:\n\u0026#34;Type\u0026#34;: \u0026#34;ethernet\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;CIDR\u0026#34;: \u0026#34;10.102.3.0/25\u0026#34;, \u0026#34;MTU\u0026#34;: 9000, \u0026#34;MyASN\u0026#34;: 65536, \u0026#34;PeerASN\u0026#34;: 65533, \u0026#34;Subnets\u0026#34;: Remove kubeapi-vip reservations for all networks except NMN Self explanatory. This endpoint now exists only on the NMN.\nUpdate uai_macvlan in NMN ranges and uai_macvlan VLAN Self explanatory. Ranges are used for the addresses of UAIs.\nRemove unused user networks (either CAN or CHN) if desired By default, the CAN will be removed if --bican-user-network-name CHN is specified, or the CHN will be removed if --bican-user-network-name CAN is specified. In order to keep a network from being removed, use the --retain-unused-user-network flag. Retention of the unused network is not normal behavior.\nGenerally production systems will NOT want to use this flag unless active toggling between CAN and CHN is required. This is not usual behavior. Test/development systems may want to have all networks for testing purposes and might want to retain both user networks. Go Back to README.SLS_Upgrade page.\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/stage_0_prerequisites/",
	"title": "Stage 0 - Prerequisites and Preflight Checks",
	"tags": [],
	"description": "",
	"content": "Stage 0 - Prerequisites and Preflight Checks Reminders:\nCSM 1.0.1 or higher is required in order to upgrade to CSM 1.2.x. If any problems are encountered and the procedure or command output does not provide relevant guidance, see Relevant troubleshooting links for upgrade-related issues. Abstract (Stage 0) Stage 0 has several critical procedures which prepares and verify if the environment is ready for upgrade. First, the latest documentation RPM is installed; it includes critical install scripts used in the upgrade procedure. Next, the current configuration of the System Layout Service (SLS) is updated to have necessary information for CSM 1.2.x. The management network configuration is also upgraded. Towards the end, prerequisite checks are performed to ensure that the upgrade is ready to proceed. Finally, a backup of Workload Manager configuration data and files is created. Once complete, the upgrade proceeds to Stage 1.\nStages Stage 0.1 - Prepare assets Stage 0.2 - Update SLS Stage 0.3 - Upgrade Management Network Stage 0.4 - Prerequisites Check Stage 0.5 - Backup Workload Manager Data Stage completed Stage 0.1 - Prepare assets Set the CSM_RELEASE variable to the target CSM version of this upgrade.\nThe following command is just an example. Be sure to set the appropriate CSM_RELEASE version for the upgrade being performed.\nncn-m001# CSM_RELEASE=csm-1.2.2 If there are space concerns on the node, then add an rbd device on the node for the CSM tarball.\nCreate a storage pool.\nCreate and map an rbd device.\nMount an rbd device.\nNote: This same rbd device can be remapped to ncn-m002 later in the upgrade procedure, when the CSM tarball is needed on that node. However, the prepare-assets.sh script will delete the CSM tarball in order to free space on the node. If using an rbd device, this is not necessary or desirable, as it will require the CSM tarball to be downloaded again later in the procedure. Therefore, if using an rbd device to store the CSM tarball, then copy the tarball to a different location and point to that location when running the prepare-assets.sh script.\nFollow either the Direct download or Manual copy procedure.\nIf there is a URL for the CSM tar file that is accessible from ncn-m001, then the Direct download procedure may be used. Alternatively, the Manual copy procedure may be used, which includes manually copying the CSM tar file to ncn-m001. Direct download Download and install the latest documentation RPM.\nImportant: The upgrade scripts expect the docs-csm RPM to be located at /root/docs-csm-latest.noarch.rpm; that is why this command copies it there.\nncn-m001# wget https://release.algol60.net/csm-1.2/docs-csm/docs-csm-latest.noarch.rpm \\ -O /root/docs-csm-latest.noarch.rpm \u0026amp;\u0026amp; rpm -Uvh --force /root/docs-csm-latest.noarch.rpm Set the ENDPOINT variable to the URL of the directory containing the CSM release tar file.\nIn other words, the full URL to the CSM release tar file must be ${ENDPOINT}${CSM_RELEASE}.tar.gz\nNOTE This step is optional for Cray/HPE internal installs, if ncn-m001 can reach the internet.\nncn-m001# ENDPOINT=https://put.the/url/here/ Run the script.\nNOTE For Cray/HPE internal installs, if ncn-m001 can reach the internet, then the --endpoint argument may be omitted.\nncn-m001# /usr/share/doc/csm/upgrade/1.2/scripts/upgrade/prepare-assets.sh --csm-version ${CSM_RELEASE} --endpoint \u0026#34;${ENDPOINT}\u0026#34; Skip the Manual copy subsection and proceed to Stage 0.2 - Update SLS.\nManual copy Copy the docs-csm RPM package and CSM release tar file to ncn-m001.\nSee Update Product Stream.\nCopy the documentation RPM to /root and install it.\nImportant:\nReplace the PATH_TO_DOCS_RPM below with the location of the RPM on ncn-m001. The upgrade scripts expect the docs-csm RPM to be located at /root/docs-csm-latest.noarch.rpm; that is why this command copies it there. ncn-m001# cp PATH_TO_DOCS_RPM /root/docs-csm-latest.noarch.rpm \u0026amp;\u0026amp; rpm -Uvh --force /root/docs-csm-latest.noarch.rpm Set the CSM_TAR_PATH variable to the full path to the CSM tar file on ncn-m001.\nThe prepare-assets.sh script will delete the CSM tarball in order to free space on the node. If using an rbd device to store the CSM tarball (or if not wanting the tarball file deleted for other reasons), then be sure to copy the tarball file to a different location, and set the CSM_TAR_PATH to point to this new location.\nncn-m001# CSM_TAR_PATH=/path/to/${CSM_RELEASE}.tar.gz Run the script.\nncn-m001# /usr/share/doc/csm/upgrade/1.2/scripts/upgrade/prepare-assets.sh --csm-version ${CSM_RELEASE} --tarball-file \u0026#34;${CSM_TAR_PATH}\u0026#34; Stage 0.2 - Update SLS Abstract (Stage 0.2) CSM 1.2.x introduces the bifurcated CAN (BICAN) as well as network configuration controlled by data in SLS. An offline upgrade of SLS data is performed. For more details on the upgrade and its sequence of events, see the SLS upgrade README.\nThe SLS data upgrade is a critical step in moving to CSM 1.2.x. Upgraded SLS data is used in DNS and management network configuration. For details to aid in understanding and decision making, see the Management Network User Guide.\nOne detail which must not be overlooked is that the existing Customer Access Network (CAN) will be migrated or retrofitted into the new Customer Management Network (CMN) while minimizing changes. A new CAN (or CHN) network is then created. Pivoting the existing CAN to the new CMN allows administrative traffic (already on the CAN) to remain as-is while moving standard user traffic to a new site-routable network. You can read more about this, as well as steps to ensure undisrupted access to UANs during upgrade, in Plan and coordinate network upgrade.\nImportant: If this is the first time performing the SLS update to CSM 1.2.x, review the SLS upgrade README in order to ensure the correct options for the specific environment are used. Two examples are given below. To see all options from the update script, run ./sls_updater_csm_1.2.py --help.\nRetrieve SLS data as JSON Obtain a token.\nncn-m001# export TOKEN=$(curl -s -k -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Create a working directory.\nncn-m001# mkdir /root/sls_upgrade \u0026amp;\u0026amp; cd /root/sls_upgrade Extract SLS data to a file.\nncn-m001# curl -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api-gw-service-nmn.local/apis/sls/v1/dumpstate | jq -S . \u0026gt; sls_input_file.json Migrate SLS data JSON to CSM 1.2.x Example 1: The CHN as the system default route (will by default output to migrated_sls_file.json).\nncn-m001# export DOCDIR=/usr/share/doc/csm/upgrade/1.2/scripts/sls ncn-m001# ${DOCDIR}/sls_updater_csm_1.2.py --sls-input-file sls_input_file.json \\ --bican-user-network-name CHN \\ --customer-highspeed-network REPLACE_CHN_VLAN REPLACE_CHN_IPV4_SUBNET Example 2: The CAN as the system default route, keep the generated CHN (for testing), and preserve the existing external-dns entry.\nncn-m001# export DOCDIR=/usr/share/doc/csm/upgrade/1.2/scripts/sls ncn-m001# ${DOCDIR}/sls_updater_csm_1.2.py --sls-input-file sls_input_file.json \\ --bican-user-network-name CAN \\ --customer-access-network REPLACE_CHN_VLAN REPLACE_CHN_IPV4_SUBNET \\ --preserve-existing-subnet-for-cmn external-dns Note:: A detailed review of the migrated/upgraded data (using vimdiff or otherwise) for production systems and for systems which have many add-on components (UANs, login nodes, storage integration points, etc.) is strongly recommended. Particularly, ensure that subnet reservations are correct in order to prevent any data mismatches.\nUpload migrated SLS file to SLS service If the following command does not complete successfully, check if the TOKEN environment variable is set correctly.\nncn-m001# curl --fail -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -k -L -X POST \u0026#39;https://api-gw-service-nmn.local/apis/sls/v1/loadstate\u0026#39; -F \u0026#39;sls_dump=@migrated_sls_file.json\u0026#39; Stage 0.3 - Upgrade management network Verify that switches have 1.2 configuration in place Log in to each management switch.\nlinux# ssh admin@1.2.3.4 Examine the text displayed when logging in to the switch.\nSpecifically, look for output similar to the following:\n################################################################################## # CSM version: 1.2 # CANU version: 1.3.2 ################################################################################## Output like the above text means that the switches have a CANU-generated configuration for CSM 1.2 in place. In this case, follow the steps in Management Network 1.0 (1.2 Preconfig) to 1.2. If the banner does NOT contain text like the above, then contact support in order to get the 1.2 Preconfig applied to the system. See the Management Network User Guide for more information on the management network. With the 1.2 switch configuration in place, users will only be able to SSH into the switches over the HMN. Stage 0.4 - Prerequisites check Set the SW_ADMIN_PASSWORD environment variable.\nSet it to the password for admin user on the switches. This is needed for preflight tests within the check script.\nNOTE: read -s is used to prevent the password from being written to the screen or the shell history.\nncn-m001# read -s SW_ADMIN_PASSWORD ncn-m001# export SW_ADMIN_PASSWORD Prevent the use of the rpcrdma module.\nThis step is required. The rpcrdma kernel module needs to be ignored so that it does not interfere with Slingshot Host Software.\nRun the following script to add the necessary parameters to the kernel command line on the worker nodes.\nncn-m001# /usr/share/doc/csm/upgrade/1.2/scripts/k8s/blacklist-kernel-modules.sh Set the NEXUS_PASSWORD variable only if needed.\nIMPORTANT: If the password for the local Nexus admin account has been changed from the default admin123 (not typical), then set the NEXUS_PASSWORD environment variable to the correct admin password and export it, before running prerequisites.sh.\nFor example:\nread -s is used to prevent the password from being written to the screen or the shell history.\nncn-m001# read -s NEXUS_PASSWORD ncn-m001# export NEXUS_PASSWORD Otherwise, a random 32-character base-64-encoded string will be generated and updated as the default admin password when Nexus is upgraded.\nRun the script.\nncn-m001# /usr/share/doc/csm/upgrade/1.2/scripts/upgrade/prerequisites.sh --csm-version ${CSM_RELEASE} If the script ran correctly, it should end with the following output:\n[OK] - Successfully completed If the script does not end with this output, then try rerunning it. If it still fails, see Upgrade Troubleshooting. If the failure persists, then open a support ticket for guidance before proceeding.\nprerequisites.sh clears the existing CFS configuration for each Management node. As each worker node is upgraded, the documentation will refer to the CFS configuration that should be assigned to the node at that time. If any worker node is unexpectedly rebooted prior to this, or if any other type of Management node is unexpectedly rebooted prior to the end of the CSM upgrade, then CFS will not automatically personalize the node after it has booted.\nUnset the NEXUS_PASSWORD variable, if it was set in the earlier step.\nncn-m001# unset NEXUS_PASSWORD (Optional) Commit changes to customizations.yaml.\ncustomizations.yaml has been updated in this procedure. If using an external Git repository for managing customizations as recommended, then clone a local working tree and commit appropriate changes to customizations.yaml.\nFor example:\nncn-m001# git clone \u0026lt;URL\u0026gt; site-init ncn-m001# cd site-init ncn-m001# kubectl -n loftsman get secret site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d - \u0026gt; customizations.yaml ncn-m001# git add customizations.yaml ncn-m001# git commit -m \u0026#39;CSM 1.2 upgrade - customizations.yaml\u0026#39; ncn-m001# git push Check available space in Nexus, and free up space if needed.\nSee Nexus Space Cleanup.\nStage 0.5 - Backup workload manager data To prevent any possibility of losing workload manager configuration data or files, a backup is required. Execute all backup procedures (for the workload manager in use) located in the Troubleshooting and Administrative Tasks sub-section of the Install a Workload Manager section of the HPE Cray Programming Environment Installation Guide: CSM on HPE Cray EX. The resulting backup data should be stored in a safe location off of the system.\nStage completed This stage is completed. Continue to Stage 1 - Ceph image upgrade.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/kubernetes/kubernetes_troubleshooting_information/",
	"title": "Kubernetes Troubleshooting Information",
	"tags": [],
	"description": "",
	"content": "Kubernetes Troubleshooting Information Commands for performing basic Kubernetes cluster troubleshooting.\nAccess pod logs Describe a node Describe a pod Open a shell on a pod Run a single command on a pod Connect to a running container Scale a deployment Remove a deployment with the manifest and reapply the deployment Delete a pod Access pod logs Use one of the following commands to retrieve pod-related logs:\nncn-mw# kubectl logs POD_NAME ncn-mw# kubectl logs POD_NAME -c CONTAINER_NAME If the pods keeps crashing, open a log for the previous instance using the following command:\nncn-mw# kubectl logs -p POD_NAME Describe a node Use the following command to retrieve information about a node\u0026rsquo;s condition, such as OutOfDisk, MemoryPressure, DiskPressure, etc.\nncn-mw# kubectl describe node NODE_NAME Describe a pod Use the following command to retrieve information that can help debug pod-related errors.\nncn-mw# kubectl describe pod POD_NAME Use the following command to list all of the containers in a pod, as shown in the following example:\nncn-mw# kubectl describe pod/cray-tftp-6f85767d76-b28gc -n default Open a shell on a pod Use the following command to connect to a pod:\nncn-mw# kubectl exec -it POD_NAME -c CONTAINER_NAME /bin/sh Run a single command on a pod Use the following command to execute a command inside a pod:\nncn-mw# kubectl exec POD_NAME ls / Connect to a running container Use the following command to connect to a currently running container:\nncn-mw# kubectl attach POD_NAME -i Scale a deployment Use the deployment command to scale a deployment up or down, as shown in the following examples:\nncn-mw# kubectl scale deployment APPLICATION_NAME --replicas=0 ncn-mw# kubectl scale deployment APPLICATION_NAME --replicas=3 Remove a deployment with the manifest and reapply the deployment Use the following command to remove components of the deployment\u0026rsquo;s manifest, such as services, network policies, and more:\nncn-mw# kubectl delete –f APPLICATION_NAME.yaml Use the following command to reapply the deployment:\nncn-mw# kubectl apply –f APPLICATION_NAME.yaml Delete a pod Pods can be configured to restart after getting deleted. Use the following command to delete a pod:\nncn-mw# kubectl delete pod POD_NAME CAUTION: It is recommended to be careful while deleting deployments or pods, because doing so can have an effect on other pods.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/hang_listing_bos_sessions/",
	"title": "Hang Listing BOS Sessions",
	"tags": [],
	"description": "",
	"content": "Hang Listing BOS Sessions Overview Symptoms Remedy Prevention Overview BOS v1 loses the ability to list its sessions after too many of them exist in its database. This has only been observed happening when the total number of sessions in the database is well over 1000.\nSymptoms When this situation occurs, attempts to list BOS sessions using the API or CLI will hang. This may also be noticed when performing the Validate CSM Health procedure \u0026ndash; the cmsdev test tool will exhibit the same hang when it tries to query BOS for a session list.\nIn order to confirm that these hangs are the result of this problem, run the following command:\nncn-mw# kubectl -n services logs --max-log-requests 50 -l app.kubernetes.io/name=cray-bos | \\ grep -C4 \u0026#39;Received message larger than max\u0026#39; If the problem being described on this page is happening on the system, then the output should contain something similar to the following:\nWARNING:bos.server.dbclient:Connect failed to cray-bos-etcd-client. Caught \u0026lt;_InactiveRpcError of RPC that terminated with: status = StatusCode.RESOURCE_EXHAUSTED details = \u0026#34;Received message larger than max (4870618 vs. 4194304)\u0026#34; debug_error_string = \u0026#34;{\u0026#34;created\u0026#34;:\u0026#34;@1682101300.469469814\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Error received from peer ipv4:10.26.231.219:2379\u0026#34;,\u0026#34;file\u0026#34;:\u0026#34;src/core/lib/surface/call.cc\u0026#34;,\u0026#34;file_line\u0026#34;:966,\u0026#34;grpc_message\u0026#34;:\u0026#34;Received message larger than max (4870618 vs. 4194304)\u0026#34;,\u0026#34;grpc_status\u0026#34;:8}\u0026#34; Remedy The remedy to the situation is to manually delete the BOS v1 sessions from the underlying BOS etcd database.\nIdentify the name of a BOS etcd pod.\nncn-mw# BOS_ETCD_POD=$(kubectl get pods -n services -l app=etcd,etcd_cluster=cray-bos-etcd \\ | grep Running | head -1 | awk \u0026#39;{ print $1 }\u0026#39;) ncn-mw# echo \u0026#34;${BOS_ETCD_POD}\u0026#34; Example output:\ncray-bos-etcd-hwb88pqklg Optionally, list the number of entries in the BOS v1 session database.\nNote: This number does not equal the number of BOS v1 sessions, because each session creates multiple entries in the database.\nncn-mw# kubectl exec -n services -it \u0026#34;${BOS_ETCD_POD}\u0026#34; -c etcd -- sh -c \\ \u0026#39;ETCDCTL_API=3 etcdctl get /session/ --prefix --keys-only\u0026#39; | grep \u0026#34;^/session/\u0026#34; | wc -l Example output:\n10836 (Delete all of the BOS v1 sessions from its database.\nThe output of this command is the number of entries deleted from the database.\nncn-mw# kubectl exec -n services -it ${BOS_ETCD_POD} -c etcd -- sh -c \\ \u0026#39;ETCDCTL_API=3 etcdctl del /session/ --prefix\u0026#39; Example output:\n10836 Verify that BOS v1 sessions are now able to be listed.\nncn-mw# cray bos session list --format json Expected output:\n[] Prevention This situation can be prevented by periodically deleting completed BOS v1 sessions.\nIn CSM 1.3.0 and later, using BOS v2 will also prevent the problem, because BOS v2 is not subject to this limitation; in addition, BOS v2 includes automatic cleanup of old completed sessions.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/cms_barebones_image_boot/",
	"title": "Troubleshoot the CMS Barebones Image Boot Test",
	"tags": [],
	"description": "",
	"content": "Troubleshoot the CMS Barebones Image Boot Test Verify that the CSM services needed to boot a node are available and working properly. This section describes how the barebonesImageTest script works and how to interpret the results. If the script is unavailable, the manual steps for reproducing the Barebones image boot test are provided.\nTopics 1. Steps the Script Performs 2. Controlling Which Node Is Used 3. Controlling Test Script Output Level 4. Manual Steps To Reproduce This Script 4.1 Locate CSM Barebones Image in IMS 4.2 Create a BOS Session Template for the CSM Barebones Image 4.3 Find an Available Compute Node 4.4 Reboot the Node Using a BOS Session Template 4.5 Connect to the Node\u0026rsquo;s Console and Watch the Boot 1. Steps the Script Performs The script file is: /opt/cray/tests/integration/csm/barebonesImageTest\nThis script automates the following steps.\nObtain the Kubernetes API gateway access token Find the existing Barebones boot image using IMS Create a BOS session template for the Barebones boot image Find an enabled compute node using HSM Watch the console log for the target compute node using console services Create a BOS session to reboot the target compute node Wait for the console output to show an error or successfully reach dracut If the script fails, investigate the underlying service to ensure it is operating correctly and examine the detailed log file to find information on the exact error and cause of failure.\nThe boot may take up to 10 or 15 minutes. The image being booted does not support a complete boot, so the node will not boot fully into an operating system. This test is merely to verify that the CSM services needed to boot a node are available and working properly. This boot test is considered successful if the boot reaches the dracut stage.\n2. Controlling Which Node Is Used By default, the script will gather all enabled compute nodes that are present in HSM and choose one at random to perform the boot test. This may be overridden with a command line option to choose which compute node is rebooted using the --xname option. The input compute node must be enabled and present in HSM to be used. If the input compute node is not available, a warning will be issued and the test will continue with a valid compute node instead of the user selected node.\nncn# /opt/cray/tests/integration/csm/barebonesImageTest --xname x3000c0s10b1n0 3. Controlling Test Script Output Level Output is directed to both the console calling the script as well as a log file that will hold more detailed information on the run and any potential problems found. The log file is written to /tmp/cray.barebones-boot-test.log and will overwrite any existing file at that location on each new run of the script.\nThe messages output to the console and the log file may be controlled separately through environment variables. To control the information being sent to the console, set the variable CONSOLE_LOG_LEVEL. To control the information being sent to the log file, set the variable FILE_LOG_LEVEL. Valid values in increasing levels of detail are: CRITICAL, ERROR, WARNING, INFO, DEBUG. The default for the console output is INFO and the default for the log file is DEBUG.\nHere is an example of running the script with more information displayed on the console during the execution of the test:\nncn# CONSOLE_LOG_LEVEL=DEBUG /opt/cray/tests/integration/csm/barebonesImageTest cray.barebones-boot-test: INFO Barebones image boot test starting cray.barebones-boot-test: INFO For complete logs look in the file /tmp/cray.barebones-boot-test.log cray.barebones-boot-test: DEBUG Found boot image: cray-shasta-csm-sles15sp2-barebones.x86_64-shasta-1.5 cray.barebones-boot-test: DEBUG Creating bos session template with etag:bc390772fbe67107cd58b3c7c08ed92d, path:s3://boot-images/e360fae1-7926-4dee-85bb-f2b4eb216d9c/manifest.json 4. Manual Steps To Reproduce This Script The following manual steps may be performed to reproduce the actions of this script. The result should be the same as running the script.\n1. Steps the Script Performs 2. Controlling Which Node Is Used 3. Controlling Test Script Output Level 4. Manual Steps To Reproduce This Script 4.1 Locate CSM Barebones Image in IMS 4.2 Create a BOS Session Template for the CSM Barebones Image 4.3 Find an Available Compute Node 4.4 Reboot the Node Using a BOS Session Template 4.5 Connect to the Node\u0026rsquo;s Console and Watch the Boot 4.1 Locate CSM Barebones Image in IMS Locate the CSM Barebones image and note the etag and path fields in the output.\nncn# cray ims images list --format json | jq \u0026#39;.[] | select(.name | contains(\u0026#34;barebones\u0026#34;))\u0026#39; Expected output is similar to the following:\n{ \u0026#34;created\u0026#34;: \u0026#34;2021-01-14T03:15:55.146962+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;293b1e9c-2bc4-4225-b235-147d1d611eef\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;6d04c3a4546888ee740d7149eaecea68\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/293b1e9c-2bc4-4225-b235-147d1d611eef/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;cray-shasta-csm-sles15sp1-barebones.x86_64-shasta-1.4\u0026#34; } 4.2 Create a BOS Session Template for the CSM Barebones Image The session template below can be copied and used as the basis for the BOS session template. As noted below, make sure the S3 path for the manifest matches the S3 path shown in the Image Management Service (IMS).\nCreate the sessiontemplate.json file.\nncn# vi sessiontemplate.json The session template should contain the following:\n{ \u0026#34;boot_sets\u0026#34;: { \u0026#34;compute\u0026#34;: { \u0026#34;boot_ordinal\u0026#34;: 2, \u0026#34;etag\u0026#34;: \u0026#34;etag_value_from_cray_ims_command\u0026#34;, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=340M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell turbo_boost_limit=999 spire_join_token=${SPIRE_JOIN_TOKEN}\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;node_roles_groups\u0026#34;: [ \u0026#34;Compute\u0026#34; ], \u0026#34;path\u0026#34;: \u0026#34;path_value_from_cray_ims_command\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:nmn0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;cfs\u0026#34;: { \u0026#34;configuration\u0026#34;: \u0026#34;cos-integ-config-1.4.0\u0026#34; }, \u0026#34;enable_cfs\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;shasta-1.4-csm-bare-bones-image\u0026#34; } NOTE: Be sure to replace the values of the etag and path fields with the ones noted earlier in the cray ims images list command.\nCreate the BOS session template using the following file as input:\nncn# cray bos sessiontemplate create --file sessiontemplate.json --name shasta-1.4-csm-bare-bones-image The expected output is:\n/sessionTemplate/shasta-1.4-csm-bare-bones-image 4.3 Find an Available Compute Node To list the compute nodes managed by HSM:\nncn-mw# cray hsm state components list --role Compute --enabled true --format toml Example output:\n[[Components]] ID = \u0026#34;x3000c0s17b1n0\u0026#34; Type = \u0026#34;Node\u0026#34; State = \u0026#34;On\u0026#34; Flag = \u0026#34;OK\u0026#34; Enabled = true Role = \u0026#34;Compute\u0026#34; NID = 1 NetType = \u0026#34;Sling\u0026#34; Arch = \u0026#34;X86\u0026#34; Class = \u0026#34;River\u0026#34; [[Components]] ID = \u0026#34;x3000c0s17b2n0\u0026#34; Type = \u0026#34;Node\u0026#34; State = \u0026#34;On\u0026#34; Flag = \u0026#34;OK\u0026#34; Enabled = true Role = \u0026#34;Compute\u0026#34; NID = 2 NetType = \u0026#34;Sling\u0026#34; Arch = \u0026#34;X86\u0026#34; Class = \u0026#34;River\u0026#34; Troubleshooting: If any compute nodes are missing from HSM database, refer to 2.3.2 Known Issues to troubleshoot any Node BMCs that have not been discovered.\nChoose a node from those listed and set XNAME to its component name (xname). In this example, x3000c0s17b2n0:\nncn-mw# export XNAME=x3000c0s17b2n0 4.4 Reboot the Node Using a BOS Session Template Create a BOS session to reboot the chosen node using the BOS session template that was created:\nncn-mw# cray bos session create --template-uuid shasta-1.4-csm-bare-bones-image --operation reboot --limit $XNAME --format toml Expected output looks similar to the following:\nlimit = \u0026#34;x3000c0s17b2n0\u0026#34; operation = \u0026#34;reboot\u0026#34; templateUuid = \u0026#34;shasta-1.4-csm-bare-bones-image\u0026#34; [[links]] href = \u0026#34;/v1/session/8f2fc013-7817-4fe2-8e6f-c2136a5e3bd1\u0026#34; jobId = \u0026#34;boa-8f2fc013-7817-4fe2-8e6f-c2136a5e3bd1\u0026#34; rel = \u0026#34;session\u0026#34; type = \u0026#34;GET\u0026#34; [[links]] href = \u0026#34;/v1/session/8f2fc013-7817-4fe2-8e6f-c2136a5e3bd1/status\u0026#34; rel = \u0026#34;status\u0026#34; type = \u0026#34;GET\u0026#34; 4.5 Connect to the Node\u0026rsquo;s Console and Watch the Boot The boot may take up to 10 or 15 minutes. The image being booted does not support a complete boot, so the node will not boot fully into an operating system. This test is merely to verify that the CSM services needed to boot a node are available and working properly.\nConnect to the node\u0026rsquo;s console. See Manage Node Consoles for information on how to connect to the node\u0026rsquo;s console (and for instructions on how to close it later).\nMonitor the boot. This boot test is considered successful if the boot reaches the dracut stage. You know this has happened if the console output has something similar to the following somewhere within the final 20 lines of its output:\n[ 7.876909] dracut: FATAL: Don\u0026#39;t know how to handle \u0026#39;root=craycps-s3:s3://boot-images/e3ba09d7-e3c2-4b80-9d86-0ee2c48c2214/rootfs:c77c0097bb6d488a5d1e4a2503969ac0-27:dvs:api-gw-service-nmn.local:300:nmn0\u0026#39; [ 7.898169] dracut: Refusing to continue NOTE: As long as the preceding text is found near the end of the console output, the test is considered successful. It is normal (and not indicative of a test failure) to see something similar to the following at the very end of the console output:\nStarting Dracut Emergency Shell... [ 11.591948] device-mapper: uevent: version 1.0.3 [ 11.596657] device-mapper: ioctl: 4.40.0-ioctl (2019-01-18) initialised: dm-devel@redhat.com Warning: dracut: FATAL: Don\u0026#39;t know how to handle Press Enter for maintenance (or press Control-D to continue): Exit the console.\ncray-console-node# \u0026amp;. The test is complete.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/add_ceph_osds/",
	"title": "Add Ceph OSDs",
	"tags": [],
	"description": "",
	"content": "Add Ceph OSDs IMPORTANT: This document is addressing how to add an OSD when the OSD auto-discovery fails to add in new drives.\nCheck to ensure you have OSD auto-discovery enabled.\nncn-s00(1/2/3)# ceph orch ls osd Example output:\nNAME RUNNING REFRESHED AGE PLACEMENT IMAGE NAME IMAGE ID osd.all-available-devices 9/9 4m ago 3d * registry.local/ceph/ceph:v15.2.8 5553b0cb212c NOTE: Ceph version 15.2.x and newer will utilize the ceph orchestrator to add any available drives on the storage nodes to the OSD pool. The process below is in the event that the orchestrator did not add the available drives into the cluster\nPrerequisites This procedure requires administrative privileges and will require at least two windows.\nProcedure In the first window, log in as root on the first master node (ncn-m001).\nncn-w001# ssh ncn-m001 Watch the status of the cluster to monitor the progress of the drives being added.\nThe following example shows only six OSDs in use.\nncn-m001# watch -n 10 ceph -s Example output:\ncluster: id: 5b359a58-e6f7-4f0c-98b8-f528f620896a health: HEALTH_OK services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby osd: 18 osds: 18 up (since 20h), 18 in (since 9d) rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) data: pools: 11 pools, 220 pgs objects: 826.23k objects, 944 GiB usage: 2.0 TiB used, 61 TiB / 63 TiB avail pgs: 220 active+clean io: client: 1.7 KiB/s rd, 12 MiB/s wr, 1 op/s rd, 1.32k op/s wr In the second window, log into ncn-s00(1/2/3) or an ncn-m node and fail over the mgr process.\nThere is an issue where orchestration tasks can get hung up and the failover will clear that up. ncn-w001# ceph mgr fail $(ceph mgr dump | jq -r .active_name) In the second window, list your available drives on the node(s) where the OSDs are missing\nThe following example is utilizing ncn-s001. Ensure the correct host for the situation is used.\nncn-s001# ceph orch device ls Example output:\nceph orch device ls ncn-s001 Hostname Path Type Serial Size Health Ident Fault Available ncn-s001 /dev/sdb hdd f94bd091-cc25-476b-9 48.3G Unknown N/A N/A No NOTE: The drive in question is reporting available. The following steps are going to erase that drive so PLEASE make sure to verify that drive is not being used.\nncn-s001# podman ps Example output:\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 596d1c235da8 registry.local/ceph/ceph:v15.2.8 -n client.rgw.sit... Less than a second ago Up Less than a second ago ceph-11d5d552-cfac-11eb-ab69-fa163ec012bf-rgw.site1.zone1.ncn-s001.oztynu eecfac35fe7c registry.local/ceph/ceph:v15.2.8 -n mon.ncn-s001 -... 2 seconds ago Up 2 seconds ago ceph-11d5d552-cfac-11eb-ab69-fa163ec012bf-mon.ncn-s001 3140f5062945 registry.local/ceph/ceph:v15.2.8 -n mgr.ncn-s001.b... 17 seconds ago Up 17 seconds ago ceph-11d5d552-cfac-11eb-ab69-fa163ec012bf-mgr.ncn-s001.bfdept 3d25564047e1 registry.local/ceph/ceph:v15.2.8 -n mds.cephfs.ncn... 3 days ago Up 3 days ago ceph-11d5d552-cfac-11eb-ab69-fa163ec012bf-mds.cephfs.ncn-s001.juehkw 4ebd6db27d08 registry.local/ceph/ceph:v15.2.8 -n osd.2 -f --set... 4 days ago Up 4 days ago ceph-11d5d552-cfac-11eb-ab69-fa163ec012bf-osd.2 96c6e11677f0 registry.local/ceph/ceph:v15.2.8 -n client.crash.n... 4 days ago Up 4 days ago ceph-11d5d552-cfac-11eb-ab69-fa163ec012bf-crash.ncn-s001 If you find an Running OSD container then we should assume that the drive is being used or might have critical data on it. If you know this to 100% not be the case (example a rebuild), then you can proceed.\nRepeat this step for all drives on the storage node(s) that have unused storage which should be added to Ceph.\nncn-s001# ceph orch device zap ncn-s001 /dev/sdb (optional --force) Proceed to the next step after all of the OSDs have been added to the storage nodes.\nIn the first window, check how many OSDs are available.\nThe following example shows 18 OSDs in use.\ncluster: id: 5b359a58-e6f7-4f0c-98b8-f528f620896a health: HEALTH_ERR Degraded data redundancy (low space): 3 pgs backfill_toofull services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby {0=ncn-m002=up:active} 2 up:standby osd: 18 osds: 18 up (since 20h), 18 in (since 9d) rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) data: pools: 11 pools, 204 pgs objects: 70.98k objects, 241 GiB usage: 547 GiB used, 62 TiB / 63 TiB avail pgs: 39582/212949 objects misplaced (18.588%) 163 active+clean 36 active+remapped+backfill_wait 3 active+remapped+backfill_wait+backfill_toofull 2 active+remapped+backfilling io: client: 20 MiB/s wr, 0 op/s rd, 807 op/s wr recovery: 559 MiB/s, 187 objects/s ncn-s001# ceph orch ps --daemon_type osd ncn-s001 Example output:\nNAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID osd.2 ncn-s001 running (4d) 20s ago 4d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 4ebd6db27d08 Reset the pool quotas.\nThis step is only necessary when the cluster capacity has increased.\nncn-s00(1/2/3)# source /srv/cray/scripts/common/fix_ansible_inv.sh ncn-s00(1/2/3)# fix_inventory ncn-s00(1/2/3)# source /etc/ansible/boto3_ansible/bin/activate ncn-s00(1/2/3)# ansible-playbook /etc/ansible/ceph-rgw-users/ceph-pool-quotas.yml ncn-s00(1/2/3)# deactivate "
},
{
	"uri": "/docs-csm/en-12/operations/system_management_health/configure_prometheus_email_alert_notifications/",
	"title": "Configure Prometheus Email Alert Notifications",
	"tags": [],
	"description": "",
	"content": "Configure Prometheus Email Alert Notifications Configure an email alert notification for all Prometheus Postgres replication alerts: PostgresReplicationLagSMA, PostgresReplicationServices, PostgresqlFollowerReplicationLagSMA, and PostgresqlFollowerReplicationLagServices.\nSystem domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system\u0026rsquo;s fully qualified domain name (FQDN).\nThe FQDN can be found by running the following command on any Kubernetes NCN.\nncn-mw# kubectl get secret site-init -n loftsman -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d | yq r - spec.network.dns.external Example output:\nsystem.hpc.amslabs.hpecorp.net Be sure to modify the example URLs on this page by replacing SYSTEM_DOMAIN_NAME with the actual value found using the above command.\nProcedure This procedure can be performed on any master or worker NCN.\nSave the current alert notification configuration, in case a rollback is needed.\nncn-mw# kubectl get secret -n sysmgmt-health alertmanager-cray-sysmgmt-health-promet-alertmanager \\ -ojsonpath=\u0026#39;{.data.alertmanager.yaml}\u0026#39; | base64 --decode \u0026gt; /tmp/alertmanager-default.yaml Create a secret and an alert configuration that will be used to add email notifications for the alerts.\nCreate the secret file.\nCreate a file named /tmp/alertmanager-secret.yaml with the following contents:\napiVersion: v1 data: alertmanager.yaml: ALERTMANAGER_CONFIG kind: Secret metadata: labels: app: prometheus-operator-alertmanager chart: prometheus-operator-8.15.4 heritage: Tiller release: cray-sysmgmt-health name: alertmanager-cray-sysmgmt-health-promet-alertmanager namespace: sysmgmt-health type: Opaque Create the alert configuration file.\nIn the following example file, the Gmail SMTP server is used in this example to relay the notification to receiver-email@yourcompany.com. Update the fields under email_configs: to reflect the desired configuration.\nCreate a file named /tmp/alertmanager-new.yaml with the following contents:\nglobal: resolve_timeout: 5m route: group_by: - job group_interval: 5m group_wait: 30s receiver: \u0026#34;null\u0026#34; repeat_interval: 12h routes: - match: alertname: Watchdog receiver: \u0026#34;null\u0026#34; - match: alertname: PostgresqlReplicationLagSMA receiver: email-alert - match: alertname: PostgresqlReplicationLagServices receiver: email-alert - match: alertname: PostgresqlFollowerReplicationLagSMA receiver: email-alert - match: alertname: PostgresqlFollowerReplicationLagServices receiver: email-alert receivers: - name: \u0026#34;null\u0026#34; - name: email-alert email_configs: - to: receiver-email@yourcompany.com from: sender-email@gmail.com # Your smtp server address smarthost: smtp.gmail.com:587 auth_username: sender-email@gmail.com auth_identity: sender-email@gmail.com auth_password: xxxxxxxxxxxxxxxx Replace the alert notification configuration based on the files created in the previous steps.\nncn-mw# sed \u0026#34;s/ALERTMANAGER_CONFIG/$(cat /tmp/alertmanager-new.yaml \\ | base64 -w0)/g\u0026#34; /tmp/alertmanager-secret.yaml \\ | kubectl replace --force -f - Validate the configuration changes.\nView the current configuration.\nncn-mw# kubectl exec alertmanager-cray-sysmgmt-health-promet-alertmanager-0 \\ -n sysmgmt-health -c alertmanager -- cat /etc/alertmanager/config/alertmanager.yaml If the configuration does not look accurate, check the logs for errors.\nncn-mw# kubectl logs -f -n sysmgmt-health pod/alertmanager-cray-sysmgmt-health-promet-alertmanager-0 alertmanager An email notification will be sent once either of the alerts set in this procedure is FIRING in Prometheus. See https://prometheus.cmn.SYSTEM_DOMAIN_NAME/alerts for more information.\nIf an alert is received, then refer to Troubleshoot Postgres Database for more information about recovering replication.\n"
},
{
	"uri": "/docs-csm/en-12/operations/system_layout_service/add_uan_can_ip_addresses_to_sls/",
	"title": "Add UAN CAN IP Addresses to SLS",
	"tags": [],
	"description": "",
	"content": "Add UAN CAN IP Addresses to SLS Add the Customer Access Network (CAN) IP addresses for User Access Nodes (UANs) to the IP address reservations in the System Layout Service (SLS). Adding these IP addresses will propagate the data needed for the Domain Name Service (DNS).\nFor more information on CAN IP addresses, refer to the Customer Accessible Networks.\nPrerequisites This procedure requires administrative privileges.\nProcedure Retrieve the SLS data for the CAN.\nncn-m001# export TOKEN=$(curl -s -k -S -d grant_type=client_credentials \\ -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth \\ -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) ncn-m001# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ https://api-gw-service-nmn.local/apis/sls/v1/networks/CAN|jq \u0026gt; CAN.json ncn-m001# cp CAN.json CAN.json.bak Edit the CAN.json file and add the desired UAN CAN IP addresses in the ExtraProperties.Subnets section.\nThis subsection is located under the CAN Bootstrap DHCP Subnet section. The IP address reservations array needs to be added in the following JSON format:\n{ \u0026#34;Comment\u0026#34;: \u0026#34;x3000c0s23b0n0\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.103.2.20\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;uan01\u0026#34; }, If multiple alias are required, the JSON format would be:\n{ \u0026#34;Aliases\u0026#34;: [ \u0026#34;uan01-can\u0026#34;, \u0026#34;uan01-slurm\u0026#34; ], \u0026#34;Comment\u0026#34;: \u0026#34;x3000c0s23b0n0\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.103.2.20\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;uan01\u0026#34; }, IMPORTANT: There must be an alias or name defined in a format that matches the hostname of the UAN. This is required by the CFS play uan_interfaces that configures the CAN interface on UANs. If the CAN is not being configured for a particular UAN, then this requirement is not needed.\nUpload the updated CAN.json file to SLS.\nncn-m001# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; --header \\ \u0026#34;Content-Type: application/json\u0026#34; --request PUT --data @CAN.json \\ https://api-gw-service-nmn.local/apis/sls/v1/networks/CAN Verify that DNS records were created.\nIt will take about five minutes before any records will show up.\nFor example:\nncn-m001# nslookup uan01.can Example output:\nServer: 10.92.100.225 Address: 10.92.100.225#53 Name: uan01.can Address: 10.103.2.24 ncn-m001# nslookup uan01-can.can Example output:\nServer: 10.92.100.225 Address: 10.92.100.225#53 Name: uan01-can.can Address: 10.103.2.24 As stated above, the UAN play uan_interfaces will attempt to nslookup the hostname of the node with with \u0026ldquo;.can\u0026rdquo; appended. Make sure this alias resolves if the CAN is going to be configured on that particular UAN. In certain upgrade scenarios, the expected alias may not have been added by default.\n"
},
{
	"uri": "/docs-csm/en-12/operations/system_configuration_service/manage_parameters_with_the_scsd_service/",
	"title": "Manage Parameters with the scsd Service",
	"tags": [],
	"description": "",
	"content": "Manage Parameters with the scsd Service The System Configuration Service commands below enable administrators to set various BMC and controller parameters. These parameters are controlled with the scsd command in the Cray CLI.\nRetrieve Current Information from Targets Get the network protocol parameters (NTP/syslog server, SSH keys) and boot order for the targets in the payload. All fields are only applicable to Liquid Cooled controllers. Attempts to set them for Air Cooled BMCs will be ignored, and retrieving them for Air Cooled BMCs will return empty strings.\nCommand line options can be used to set parameters as desired. For example, if only the NTP server info is to be set, only the \u0026ldquo;NTPServer\u0026rdquo; value has to be present.\nThe following is an example payload file that was used to generate the output in the command below:\n{ \u0026#34;Force\u0026#34;: true, \u0026#34;Targets\u0026#34;: [ \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;x0c0s1b0\u0026#34; ], \u0026#34;Params\u0026#34;: [ \u0026#34;NTPServerInfo\u0026#34;, \u0026#34;SyslogServerInfo\u0026#34;, \u0026#34;SSHKey\u0026#34;, \u0026#34;SSHConsoleKey\u0026#34;, \u0026#34;BootOrder\u0026#34; ] } To retrieve information from the targets:\nncn-m001# cray scsd bmc dumpcfg create PAYLOAD_FILE --format json { \u0026#34;Targets\u0026#34;: [ { \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;Params\u0026#34;: { \u0026#34;NTPServerInfo\u0026#34;: { \u0026#34;NTPServers\u0026#34;: [\u0026#34;sms-ncn-w001\u0026#34;], \u0026#34;Port\u0026#34;: 123, \u0026#34;ProtocolEnabled\u0026#34;: true }, \u0026#34;SyslogServerInfo\u0026#34;: { \u0026#34;SyslogServers\u0026#34;: [\u0026#34;sms-ncn-w001\u0026#34;], \u0026#34;Port\u0026#34;:514, \u0026#34;ProtocolEnabled\u0026#34;: true }, \u0026#34;SSHKey\u0026#34;: \u0026#34;xxxxyyyyzzzz\u0026#34;, \u0026#34;SSHConsoleKey\u0026#34;: \u0026#34;aaaabbbbcccc\u0026#34;, \u0026#34;BootOrder\u0026#34;: [\u0026#34;Boot0\u0026#34;,Boot1\u0026#34;,Boot2\u0026#34;,Boot3\u0026#34;] } }, { \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;Params\u0026#34;: { \u0026#34;NTPServerInfo\u0026#34;: { \u0026#34;NTPServers\u0026#34;: [\u0026#34;sms-ncn-w001\u0026#34;], \u0026#34;Port\u0026#34;: 123, \u0026#34;ProtocolEnabled\u0026#34;: true }, \u0026#34;SyslogServerInfo\u0026#34;: { \u0026#34;SyslogServers\u0026#34;: [\u0026#34;sms-ncn-w001\u0026#34;], \u0026#34;Port\u0026#34;:514, \u0026#34;ProtocolEnabled\u0026#34;: true }, \u0026#34;SSHKey\u0026#34;: \u0026#34;xxxxyyyyzzzz\u0026#34;, \u0026#34;SSHConsoleKey\u0026#34;: \u0026#34;aaaabbbbcccc\u0026#34;, \u0026#34;BootOrder\u0026#34;: [\u0026#34;Boot0\u0026#34;,Boot1\u0026#34;,Boot2\u0026#34;,Boot3\u0026#34;] } } ] } Retrieve Information from a Single Target Retrieve NTP server information, syslog information, or the SSH key from a single target.\nncn-m001# cray scsd bmc cfg describe XNAME --format json Example output:\n{ \u0026#34;Force\u0026#34;: false, \u0026#34;Params\u0026#34;: { \u0026#34;SSHConsoleKey\u0026#34;: \u0026#34;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCg1yUfF5zPwOWjp3B/4LtuEGdbwo23L8BkQwhBz/4lVX2K2viYhGBAGwzqWMe1OQjSz3cUiSE/A6Kr7tKwB77j4U51XLbTRy5tcqzhIVFb7kFgdSqyUxfv+5s0aNLkwpI00w2TVVSp7xy8t+CLHwgdC7RXtWHOmI35NdUc8y8monn+q4mQV0ms29h1/tpHETocPQjCMwIsOtvUyS91XAn72Va1Xe8uTaAO+SqTZMYVTOxfLeLTg6QLox8PBXpVz422E4bcKZOYT68s1DxL5Rtz7HB6iKtXOvLaJSe8S5AUEe1G4eojQ/NEHcNobZkO00wSIzce2TwZV1il7410yGle1njnWLZBSpYmfH8d2joX434IEdESTwgdrYBEBAtOe7yvXu+2Qiux4AFaQwI0Aiif2Q5FndgqUiN6pD1IkVkInBYGFR5La8ZdZAgUdptvIZNJE67D3aGj0cseJFMHY4hfLEK34xne5yvL3OqpyjSS/0oPd1kLk4BgA8npGroLP+bP2GH6fMe7Wu9Sk/UUoM1W6N7127xVlvIogKxTG27zes8LSw7R/vOpVnWqJ2/BVIblTkMV45lCBQXaj4xG8ju8Zofh23BMusTthu8Q+T48k6H17g2dVlYTuUN+/i1KnSMPI2+dbOyV+X/maW+TBS8zK1pV5VTptg0UZgaZim+WIQ== \u0026#34;, \u0026#34;SyslogServerInfo\u0026#34;: { \u0026#34;ProtocolEnabled\u0026#34;: true, \u0026#34;SyslogServers\u0026#34;: [ \u0026#34;rsyslog_agg_service_hmn.local\u0026#34; ], \u0026#34;Port\u0026#34;: 514, \u0026#34;Transport\u0026#34;: \u0026#34;udp\u0026#34; }, \u0026#34;NTPServerInfo\u0026#34;: { \u0026#34;NTPServers\u0026#34;: [ \u0026#34;10.254.0.4\u0026#34; ], \u0026#34;ProtocolEnabled\u0026#34;: true, \u0026#34;Port\u0026#34;: 123 }, \u0026#34;SSHKey\u0026#34;: \u0026#34;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCg1yUfF5zPwOWjp3B/4LtuEGdbwo23L8BkQwhBz/4lVX2K2viYhGBAGwzqWMe1OQjSz3cUiSE/A6Kr7tKwB77j4U51XLbTRy5tcqzhIVFb7kFgdSqyUxfv+5s0aNLkwpI00w2TVVSp7xy8t+CLHwgdC7RXtWHOmI35NdUc8y8monn+q4mQV0ms29h1/tpHETocPQjCMwIsOtvUyS91XAn72Va1Xe8uTaAO+SqTZMYVTOxfLeLTg6QLox8PBXpVz422E4bcKZOYT68s1DxL5Rtz7HB6iKtXOvLaJSe8S5AUEe1G4eojQ/NEHcNobZkO00wSIzce2TwZV1il7410yGle1njnWLZBSpYmfH8d2joX434IEdESTwgdrYBEBAtOe7yvXu+2Qiux4AFaQwI0Aiif2Q5FndgqUiN6pD1IkVkInBYGFR5La8ZdZAgUdptvIZNJE67D3aGj0cseJFMHY4hfLEK34xne5yvL3OqpyjSS/0oPd1kLk4BgA8npGroLP+bP2GH6fMe7Wu9Sk/UUoM1W6N7127xVlvIogKxTG27zes8LSw7R/vOpVnWqJ2/BVIblTkMV45lCBQXaj4xG8ju8Zofh23BMusTthu8Q+T48k6H17g2dVlYTuUN+/i1KnSMPI2+dbOyV+X/maW+TBS8zK1pV5VTptg0UZgaZim+WIQ== \u0026#34; } } Individual parameters can be specified in the command line with the --param option. Multiple parameters can be specified by using a comma-separated list with the --params option. This makes it easier to find information for certain parameters. For example, to only view the NTP server information, the following option can be used:\nncn-m001# cray scsd bmc cfg describe --param NTPServerInfo \\ XNAME --format json { \u0026#34;Force\u0026#34;: false, \u0026#34;Params\u0026#34;: { \u0026#34;NTPServerInfo\u0026#34;: { \u0026#34;NTPServers\u0026#34;: [ \u0026#34;10.254.0.4\u0026#34; ], \u0026#34;ProtocolEnabled\u0026#34;: true, \u0026#34;Port\u0026#34;: 123 } } Set Parameters for Targets Set syslog, NTP server information, or SSH key for a set of targets.\nThe following is an example payload file that was used to generate the output in the command below:\n{ \u0026#34;Force\u0026#34;: false, \u0026#34;Targets\u0026#34;: [ \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;x0c0s1b0\u0026#34; ], \u0026#34;Params\u0026#34;: { \u0026#34;NTPServerInfo\u0026#34;: { \u0026#34;NTPServers\u0026#34;: [\u0026#34;sms-ncn-w001\u0026#34;], \u0026#34;Port\u0026#34;: 123, \u0026#34;ProtocolEnabled\u0026#34;: true }, \u0026#34;SyslogServerInfo\u0026#34;: { \u0026#34;SyslogServers\u0026#34;: [\u0026#34;sms-ncn-w001\u0026#34;], \u0026#34;Port\u0026#34;:514, \u0026#34;ProtocolEnabled\u0026#34;: true }, \u0026#34;SSHKey\u0026#34;: \u0026#34;xxxxyyyyzzzz\u0026#34;, \u0026#34;SSHConsoleKey\u0026#34;: \u0026#34;aaaabbbbcccc\u0026#34;, \u0026#34;BootOrder\u0026#34;: [\u0026#34;Boot0\u0026#34;,\u0026#34;Boot1\u0026#34;,\u0026#34;Boot2\u0026#34;,\u0026#34;Boot3\u0026#34;] } } To set parameters for the specified targets:\nncn-w001# cray scsd bmc loadcfg create PAYLOAD_FILE --format json { \u0026#34;Targets\u0026#34;: [ { \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s1b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 405, \u0026#34;StatusMsg\u0026#34;: \u0026#34;Only GET operations permitted\u0026#34; } ] } Set Parameters for a Single BMC or Controller Set the BMC configuration for a single target using a specific component name (xname). If no form data is specified, all network protocol data is returned for the target; otherwise, only the requested data is returned.\nThe following is an example payload file that was used to generate the output in the command below:\n{ \u0026#34;Force\u0026#34;: true, \u0026#34;Params\u0026#34;: { \u0026#34;NTPServerInfo\u0026#34;: { \u0026#34;NTPServers\u0026#34;: [\u0026#34;sms-ncn-w001\u0026#34;], \u0026#34;Port\u0026#34;: 123, \u0026#34;ProtocolEnabled\u0026#34;: true }, \u0026#34;SyslogServerInfo\u0026#34;: { \u0026#34;SyslogServers\u0026#34;: [\u0026#34;sms-ncn-w001\u0026#34;], \u0026#34;Port\u0026#34;:514, \u0026#34;ProtocolEnabled\u0026#34;: true }, \u0026#34;SSHKey\u0026#34;: \u0026#34;xxxxyyyyzzzz\u0026#34;, \u0026#34;SSHConsoleKey\u0026#34;: \u0026#34;aaaabbbbcccc\u0026#34;, \u0026#34;BootOrder\u0026#34;: [\u0026#34;Boot0\u0026#34;,\u0026#34;Boot1\u0026#34;,\u0026#34;Boot2\u0026#34;,\u0026#34;Boot3\u0026#34;] } } To set the parameters for a single BMC or controller:\nncn-m001# cray scsd bmc cfg create XNAME --format json { \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; } Set Redfish Credentials for Multiple Targets Use the following command to set Redfish credentials for BMCs and controllers. Note that this is different than SSH keys, which are only used on controllers. These credentials are for Redfish access, not SSH access into a controller.\nThe API allows for different credentials to be set for each target within one call. It is not possible to retrieve credentials with this command. Only setting them is allowed for security reasons.\nThe payload for this API is amenable to setting different credentials for different targets all in one call. To set credentials for a group of controllers, set up a group in HSM and use the group ID.\nThe following is an example payload file that was used to generate the output in the command below:\n{ \u0026#34;Force\u0026#34;: false, \u0026#34;Targets\u0026#34;: [ { \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;Creds\u0026#34;: { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;admin-pw\u0026#34; } }, { \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s1b0\u0026#34;, \u0026#34;Creds\u0026#34;: { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;admin-pw\u0026#34; } } ] } To set the Redfish credentials for multiple targets:\nncn-m001# cray scsd bmc discreetcreds create PAYLOAD_FILE --format json { \u0026#34;Targets\u0026#34;: [ { \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s1b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; } ] } Set Redfish Credentials for a Single Target Set Redfish credentials for a single target. This command is similar to the cray scsd bmc discreetcreds create command, except it cannot be used to set different credentials for multiple targets.\nThe following is an example payload file that was used to generate the output in the command below:\n{ \u0026#34;Force\u0026#34;: true, \u0026#34;Creds\u0026#34;: { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;admin-pw\u0026#34; } } To set the Redfish credentials for a single target:\nncn-m001# cray scsd bmc creds create XNAME --format json Example output:\n{ \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; } "
},
{
	"uri": "/docs-csm/en-12/operations/spire/restore_missing_spire_metadata/",
	"title": "Restore missing Spire metadata",
	"tags": [],
	"description": "",
	"content": "Restore missing Spire metadata If the Boot Script Service (BSS) metadata server does contain the proper Spire metadata, then the computes will fail to boot. This is due to dracut pulling server data from the metadata during startup. To fix this issue, the spire-update-bss job needs to be rerun.\nError [ 557.513984] Apr 22 18:02:02 nid000004 dracut-initqueue[4177]: time=\u0026#34;2022-04-22T18:02:02Z\u0026#34; level=info msg=\u0026#34;SVID is not found. Starting node attestation\u0026#34; subsystem_name=attestor trust_domain_id=\u0026#34;spiffe://null\u0026#34; [ 557.514000] Apr 22 18:02:07 nid000004 dracut-initqueue[4194]: Agent is unavailable. [ 557.514017] Apr 22 18:02:07 nid000004 dracut-initqueue[4174]: Warning: Spire-agent healthcheck failed, return code 1 Check Run the goss-spire-bss-metadata-exist test.\nncn-mw# goss -g /opt/cray/tests/install/ncn/tests/goss-spire-bss-metadata-exist.yaml v Example output:\nFailures/Skipped: Title: Kubernetes Query BSS Cloud-init for spire meta data Meta: desc: Kubernetes Query BSS Cloud-init for spire meta data sev: 0 Command: spire_in_bss_cloudinit: exit-status: Expected \u0026lt;int\u0026gt;: 1 to equal \u0026lt;int\u0026gt;: 0 Total Duration: 0.387s Count: 1, Failed: 1, Skipped: 0 Solution Run the following command on a master node to restart the job that populates the metadata server with the correct Spire information.\nRe-run the spire-update-bss job.\nncn-mw# JOB=$(kubectl get jobs -n spire -l app.kubernetes.io/name=spire-update-bss --no-headers -oname | sort -u | tail -n1) ncn-mw# kubectl get -n spire $JOB -o json | jq \u0026#39;del(.spec.selector,.spec.template.metadata.labels)\u0026#39; | kubectl replace --force -f - "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/access_the_keycloak_user_management_ui/",
	"title": "Access the Keycloak User Management UI",
	"tags": [],
	"description": "",
	"content": "Access the Keycloak User Management UI This procedure can be used to access the interface to manage Keycloak users. Users can be added with this interface. See Create Internal User Accounts in the Keycloak Shasta Realm.\nPrerequisites This procedure uses SYSTEM_DOMAIN_NAME as an example for the DNS name of the non-compute node (NCN). Replace this name with the actual NCN\u0026rsquo;s DNS name while executing this procedure. This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process. The password can be obtained with the following command:\nncn-mw# kubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode Procedure Point a browser at https://auth.cmn.SYSTEM_DOMAIN_NAME/keycloak/, replacing SYSTEM_DOMAIN_NAME with the actual NCN\u0026rsquo;s DNS name. Use of the auth.cmn. sub-domain is required for administrative access to Keycloak.\nThe following is an example URL for a system: https://auth.cmn.system1.us.cray.com/keycloak/\nThe browser may return an error message similar to the following when auth.cmn.SYSTEM_DOMAIN_NAME/keycloak is launched for the first time:\nThis Connection Is Not Private This website may be impersonating \u0026#34;hostname\u0026#34; to steal your personal or financial information. You should go back to the previous page. See Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster for more information on getting the Certificate Authority (CA) certificate on the system.\nClick the Administration Console link.\nLog in as the admin user for the Master realm.\nEnsure that the selected Realm is Shasta.\nClick the Users link under the Manage menu on the left side of the screen.\nNew users can be added with this interface. See Create Internal User Accounts in the Keycloak Shasta Realm.\n"
},
{
	"uri": "/docs-csm/en-12/operations/preinstall/change_servertech_pdu_credentials/",
	"title": "Change Credentials on ServerTech PDUs",
	"tags": [],
	"description": "",
	"content": "Change Credentials on ServerTech PDUs This procedure changes password used by the admn user on ServerTech PDUs. This procedure should be used to update all ServerTech PDUs in the system to the same global credentials.\nNOTE: This procedure only updates the default credentials on the ServerTech PDU hardware. No credentials are set in any management software.\nPrerequisites The ServerTech PDUs must be accessible via a workstation or laptop. Workstation or laptop has the curl command installed. The ServerTech PDU hostnames or IP addresses must be known. The default admn user password must be known for each PDU. Procedure For each ServerTech PDU:\nChange password for the admn user on the ServerTech PDU.\nlinux# curl -i -k -u admn:\u0026lt;OLD-PDU-PASSWORD\u0026gt; -X PATCH \\ https://\u0026lt;PDU_IP_OR_HOSTNAME\u0026gt;/jaws/config/users/local/admn \\ -d \u0026#34;{ \\\u0026#34;password\\\u0026#34;: \\\u0026#34;\u0026lt;NEW_PDU_PASSWORD\u0026gt;\\\u0026#34; }\u0026#34; Expected output upon a successful password change:\nHTTP/1.1 204 No Content Content-Type: text/html Transfer-Encoding: chunked Server: ServerTech-AWS/v8.0p Set-Cookie: C5=1883488164; path=/ Connection: close Pragma: JAWS v1.01 NOTE: After 5 minutes, the previous credential should stop working as the existing session timed out.\nVerify that the new password works:\nlinux# curl -i -k -u admn:\u0026lt;NEW-PDU-PASSWORD\u0026gt; \\ https://\u0026lt;PDU_IP_OR_HOSTNAME\u0026gt;/jaws/config/banner Expected output upon a successful password change:\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 23 Server: ServerTech-AWS/v8.0p Set-Cookie: C5=241418521; path=/ Allow: GET, PATCH CacheControl: no-cache,no-store Expires: Thu, 26 Oct 1995 00:00:00 GMT Pragma: JAWS v1.01 { \u0026#34;message\u0026#34; : \u0026#34;\u0026#34; } "
},
{
	"uri": "/docs-csm/en-12/operations/resiliency/recreate_statefulset_pods_on_another_node/",
	"title": "Recreate StatefulSet Pods on Another Node",
	"tags": [],
	"description": "",
	"content": "Recreate StatefulSet Pods on Another Node Some pods are members of StatefulSets, meaning that there is a very specific number of them, each likely running on a different node. Similar to DaemonSets, these pods will never be recreated on another node as long as they are sitting in a Terminating state.\nWarning: This procedure should only be done for pods that are known to no longer be running. Corruption may occur if the worker node is still running when deleting the deleting the StatefulSet pod in a Terminating state.\nThe design of StatefulSet pods prohibits the creation of new pods on another node. This is the expected behavior for StatefulSet pods. In many cases, the StatefulSet is a set of only one pod. If a StatefulSet pod needs to be recreated on another node, the Terminating pod needs to be force deleted and then it will automatically recreate.\nThis procedure prevents services from being taken out of service when a node goes down.\nPrerequisites A StatefulSet pod failed to be moved to another healthy non-compute node (NCN) acting as a worker node. If the network is being brought down temporarily during testing, ignore any issues with StatefulSet pods until the testing is complete. These errors should be ignored because the nodes could have the network restored, and then the Terminating pod may go ACTIVE temporarily (causing a race and corruption) if it comes up at the same time as another StatefulSet running the pod. This is much more likely to occur during testing of a network outage. Procedure View the StatefulSet pods on the system.\nAny of the pods with x/1 in the READY column could be on a node that goes down, at which point that service will no longer be available.\nncn-mw# kubectl get statefulsets -A Example output:\nNAMESPACE NAME READY AGE backups benji-k8s-postgresql 1/1 2d14h nexus nexus 1/1 2d14h services cray-dhcp-kea-postgres 3/3 2d14h services cray-hms-badger-postgres 3/3 2d14h services cray-keycloak 3/3 2d14h services cray-rm-pals-postgres 3/3 2d14h services cray-shared-kafka-kafka 3/3 2d14h services cray-shared-kafka-zookeeper 3/3 2d14h services cray-sls-postgres 3/3 2d14h services cray-smd-postgres 3/3 2d14h services gitea-vcs-postgres 1/1 2d14h services keycloak-postgres 3/3 2d14h services slingshot-controllers-kafka 3/3 2d14h services slingshot-controllers-zookeeper 3/3 2d14h sma cluster-kafka 3/3 2d14h sma cluster-zookeeper 3/3 2d14h sma elasticsearch-master 3/3 2d14h sma mysql 1/1 2d14h sma sma-ldms-aggr-compute 1/1 2d14h sma sma-ldms-aggr-ncn 1/1 2d14h sma sma-monasca-agent 1/1 2d14h sma sma-monasca-mysql 1/1 2d14h sma sma-monasca-notification 1/1 2d14h sma sma-monasca-zoo-entrance 1/1 2d14h sma sma-postgres-cluster 2/2 96s sysmgmt-health alertmanager-cray-sysmgmt-health-promet-alertmanager 1/1 2d14h sysmgmt-health prometheus-cray-sysmgmt-health-promet-prometheus 1/1 2d14h vault cray-vault 3/3 2d14h Describe a service to find the StatefulSet pod name.\nThe StatefulSet pod will have a name in the StatefulSet-0 format. Vault is the service being described in the example below. The StatefulSet pod name is cray-vault-0.\nncn-mw# kubectl get pods -A -o wide | grep SERVICE_NAME Example output:\noperators cray-vault-operator-57dbbb7db5-9lr6z 1/1 Running 0 5d18h 10.40.0.11 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services cray-meds-vault-loader-bzgbd 0/2 Completed 0 5d18h 10.40.0.24 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services cray-reds-vault-loader-d9cn9 0/2 Completed 0 5d18h 10.40.0.25 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault cray-vault-0 4/4 Terminating 7 5d18h 10.42.0.21 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault cray-vault-configurer-68754d5b69-knrsc 2/2 Running 0 92m 10.40.0.103 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault cray-vault-configurer-68754d5b69-mwmlr 2/2 Terminating 0 5d18h 10.42.0.22 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault cray-vault-etcd-4b2t84tp79 1/1 Terminating 0 5d18h 10.42.0.27 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault cray-vault-etcd-699ncq8s9c 1/1 Running 0 5d18h 10.40.0.17 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault cray-vault-etcd-hvmtrwjpsw 1/1 Running 0 92m 10.47.0.110 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault cray-vault-etcd-mx8qc596t9 1/1 Running 0 5d18h 10.47.0.23 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Delete the StatefulSet pod in a Terminating state.\nThe StatefulSet (the controller) will recreate the pod on a working node when the pod or the node it sits on is deleted. The command below assumes the pod is located on the downed node and is currently in a Terminating state.\nncn-mw# kubectl delete pod -n NAMESPACE POD_NAME --force --grace-period 0 For example:\nncn-mw# kubectl delete pod -n vault cray-vault-0 --force --grace-period 0 The StatefulSet will then recreate cray-vault-0 on a node that is in Ready state.\n"
},
{
	"uri": "/docs-csm/en-12/operations/power_management/ignore_nodes_with_capmc/",
	"title": "Ignore Nodes with CAPMC",
	"tags": [],
	"description": "",
	"content": "Ignore Nodes with CAPMC Update the Cray Advanced Platform Monitoring and Control (CAPMC) ConfigMap to ignore non-compute nodes (NCNs) and ensure that they cannot be powered off or reset.\nModifying the CAPMC ConfigMap to ignore nodes can prevent them from accidentally being power cycled.\nNodes can also be locked with the Hardware State Manager (HSM) API. Refer to Lock and Unlock Management Nodes for more information.\nPrerequisites This procedure requires administrative privileges.\nProcedure Edit the CAPMC ConfigMap.\nncn-m001# kubectl -n services edit configmaps cray-capmc-configuration Uncomment the # BlockRole = [\u0026quot;Management\u0026quot;] value in the [PowerControls.Off], [PowerControls.On], and [PowerControls.ForceRestart] sections.\nSave and quit the ConfigMap.\nCAPMC restarts using the new ConfigMap.\n"
},
{
	"uri": "/docs-csm/en-12/operations/package_repository_management/nexus_configuration/",
	"title": "Nexus Configuration",
	"tags": [],
	"description": "",
	"content": "Nexus Configuration Expect each product to create and use its own File type blob store. For example, the Cray System Management (CSM) product uses csm.\nThe default blob store is also available, but Cray products are discouraged from using it.\nRepositories CSM creates the registry (format docker) and charts (format helm) repositories for managing container images and Helm charts across all Cray products. However, each product\u0026rsquo;s release may contain a number of RPM repositories that are added to Nexus. RPM repositories are created in Nexus as raw repositories to support signed repository metadata and to enable client GPG checks.\nRepository naming conventions RPM repositories should be named in the \u0026lt;product\u0026gt;[-\u0026lt;product-version\u0026gt;]-\u0026lt;os-dist\u0026gt;-\u0026lt;os-version\u0026gt;[-compute][-\u0026lt;arch\u0026gt;] format. The following is a description of each component in an RPM repository name:\nComponent Description \u0026lt;product\u0026gt; The product. For example, cos, csm, or sma. \u0026lt;product-version\u0026gt; The product version. For example, 1.4.0, latest, or stable. Type hosted repositories must specify the version relative to the patch release. Type group or proxy repositories whose sole member is a hosted repository (for instance, if it serves as an alias) may use a more generic version, such as 1.4, or omit the version altogether if it represents the currently active version. \u0026lt;os-dist\u0026gt; The OS distribution, such as sle. \u0026lt;os-version\u0026gt; The OS version, such as 15sp1 or 15sp2. compute Must be specified if the repository contains RPMs specific to compute nodes and omitted otherwise. There is no suffix for repositories containing NCN RPMs. \u0026lt;arch\u0026gt; Must be specified if the repository contains RPMs specific to a system architecture other than x86_64, such as -aarch64. "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/rebuild_ncns/identify_nodes_and_update_metadata/",
	"title": "Identify Nodes and Update Metadata",
	"tags": [],
	"description": "",
	"content": "Identify Nodes and Update Metadata Use the following procedure to inspect and modify the Boot Script Service (BSS) boot parameters JSON file.\nThis section applies to all node types. The commands in this section assume the variables from the prerequisites section have been set.\nProcedure Generate the BSS boot parameters JSON file.\nRun the following commands from a node that has cray CLI initialized:\nncn# cray bss bootparameters list --name $XNAME --format=json | jq .[] \u0026gt; ${XNAME}.json Modify the JSON file and set the kernel parameters to wipe the disk.\nLocate the portion of the line that contains \u0026quot;metal.no-wipe\u0026quot; and ensure it is set to zero \u0026quot;metal.no-wipe=0\u0026quot;.\nRe-apply the boot parameters list for the node using the JSON file.\nGet a token to interact with BSS using the REST API.\nncn# TOKEN=$(curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth \\ -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Do a PUT action for the new JSON file.\nncn# curl -i -s -H \u0026#34;Content-Type: application/json\u0026#34; -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ \u0026#34;https://api-gw-service-nmn.local/apis/bss/boot/v1/bootparameters\u0026#34; -X PUT -d @./${XNAME}.json IMPORTANT: Ensure a good response (HTTP CODE 200) is returned in the output.\nVerify the bss bootparameters list command returns the expected information.\nExport the list from BSS to a file with a different name.\nncn# cray bss bootparameters list --name ${XNAME} --format=json |jq .[]\u0026gt; ${XNAME}.check.json Compare the new JSON file with what was PUT to BSS.\nncn# diff ${XNAME}.json ${XNAME}.check.json The files should be identical\nNext Step Proceed to the next step to Wipe Drives or return to the main Rebuild NCNs page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_remove_replace_ncns/add_remove_replace_ncns/",
	"title": "Alpha Framework to Add, Remove, Replace, or Move NCNs",
	"tags": [],
	"description": "",
	"content": "Alpha Framework to Add, Remove, Replace, or Move NCNs Add, remove, replace, or move non-compute nodes (NCNs). This applies to worker, storage, or master nodes. Use this procedure in the event that:\nWorker, storage, or master nodes are being replaced and the MAC address is changing. Worker or storage nodes are being added. Worker, storage, or master nodes are being moved to a different cabinet. IMPORTANT: Always maintain at least two of the first three worker, storage, and master nodes when adding, removing, replacing, or moving NCNs.\nThe following workflows are available:\nPrerequisites Add worker, storage, or master NCNs Add NCN prerequisites Add NCN procedure Remove worker, storage, or master NCNs Remove NCN prerequisites Remove NCN procedure Replace or move worker, storage, or master NCNs Replace NCN procedure Prerequisites The system is fully installed and has transitioned off of the LiveCD.\nAll activities required for site maintenance are complete.\nThe latest CSM documentation has been installed on the master nodes. See Check for Latest Documentation.\nRun ncn_add_pre-req.py to adjust the network.\nncn-m# cd /usr/share/doc/csm/scripts/operations/node_management/Add_Remove_Replace_NCNs/ ncn-m# ./ncn_add_pre-req.py The script will ask the following question:\nHow many NCNs would you like to add? Do not include NCNs to be removed or moved. Example output:\nThe prerequisite script prepares adding NCNs by adjusting SLS network configurations. Please enter answer as an integer. How many NCNs would you like to add? Do not include NCNs to be removed or moved. 10 You are about to make DESTRUCTIVE changes to the system and will need to restart DVS. If you are sure you want to proceed. Please type: PROCEED If you want to stop. Type: exit or press ctrl-c PROCEED Checking HMN. last_reserved_ip: 10.254.1.20 start_dhcp_pool:10.254.1.26 The space between last_reserved_ip and start_dhcp_pool is 6 IP. There is not enough static IP space to add an NCN.Adjusting DHCP pool start. {\u0026#39;HMN\u0026#39;: {\u0026#39;10.254.1.33\u0026#39;, \u0026#39;10.254.1.35\u0026#39;, \u0026#39;10.254.1.36\u0026#39;, \u0026#39;10.254.1.38\u0026#39;, \u0026#39;10.254.1.34\u0026#39;, \u0026#39;10.254.1.39\u0026#39;, \u0026#39;10.254.1.22\u0026#39;, \u0026#39;10.254.1.29\u0026#39;, \u0026#39;10.254.1.40\u0026#39;, \u0026#39;10.254.1.27\u0026#39;, \u0026#39;10.254.1.21\u0026#39;, \u0026#39;10.254.1.28\u0026#39;, \u0026#39;10.254.1.26\u0026#39;, \u0026#39;10.254.1.30\u0026#39;, \u0026#39;10.254.1.31\u0026#39;, \u0026#39;10.254.1.32\u0026#39;, \u0026#39;10.254.1.25\u0026#39;, \u0026#39;10.254.1.24\u0026#39;, \u0026#39;10.254.1.23\u0026#39;, \u0026#39;10.254.1.37\u0026#39;}} add_ncn_count: 10 ip_dhcp_pool_start: {\u0026#39;MTL\u0026#39;: \u0026#39;10.1.1.17\u0026#39;, \u0026#39;NMN\u0026#39;: \u0026#39;10.252.1.20\u0026#39;, \u0026#39;CAN\u0026#39;: \u0026#39;10.102.4.22\u0026#39;, \u0026#39;HMN\u0026#39;: \u0026#39;10.254.1.26\u0026#39;} new_ip_dhcp_pool_start: {\u0026#39;HMN\u0026#39;: \u0026#39;10.254.1.47\u0026#39;} Checking CAN. last_reserved_ip: 10.102.4.14 start_dhcp_pool:10.102.4.22 The space between last_reserved_ip and start_dhcp_pool is 8 IP. There is not enough static IP space to add an NCN.Adjusting DHCP pool start. {\u0026#39;HMN\u0026#39;: {\u0026#39;10.254.1.33\u0026#39;, \u0026#39;10.254.1.35\u0026#39;, \u0026#39;10.254.1.36\u0026#39;, \u0026#39;10.254.1.38\u0026#39;, \u0026#39;10.254.1.34\u0026#39;, \u0026#39;10.254.1.39\u0026#39;, \u0026#39;10.254.1.22\u0026#39;, \u0026#39;10.254.1.29\u0026#39;, \u0026#39;10.254.1.40\u0026#39;, \u0026#39;10.254.1.27\u0026#39;, \u0026#39;10.254.1.21\u0026#39;, \u0026#39;10.254.1.28\u0026#39;, \u0026#39;10.254.1.26\u0026#39;, \u0026#39;10.254.1.30\u0026#39;, \u0026#39;10.254.1.31\u0026#39;, \u0026#39;10.254.1.32\u0026#39;, \u0026#39;10.254.1.25\u0026#39;, \u0026#39;10.254.1.24\u0026#39;, \u0026#39;10.254.1.23\u0026#39;, \u0026#39;10.254.1.37\u0026#39;}, \u0026#39;CAN\u0026#39;: {\u0026#39;10.102.4.23\u0026#39;, \u0026#39;10.102.4.15\u0026#39;, \u0026#39;10.102.4.16\u0026#39;, \u0026#39;10.102.4.24\u0026#39;, \u0026#39;10.102.4.19\u0026#39;, \u0026#39;10.102.4.21\u0026#39;, \u0026#39;10.102.4.20\u0026#39;, \u0026#39;10.102.4.17\u0026#39;, \u0026#39;10.102.4.22\u0026#39;, \u0026#39;10.102.4.18\u0026#39;}} add_ncn_count: 10 ip_dhcp_pool_start: {\u0026#39;MTL\u0026#39;: \u0026#39;10.1.1.17\u0026#39;, \u0026#39;NMN\u0026#39;: \u0026#39;10.252.1.20\u0026#39;, \u0026#39;CAN\u0026#39;: \u0026#39;10.102.4.22\u0026#39;, \u0026#39;HMN\u0026#39;: \u0026#39;10.254.1.26\u0026#39;} new_ip_dhcp_pool_start: {\u0026#39;HMN\u0026#39;: \u0026#39;10.254.1.47\u0026#39;, \u0026#39;CAN\u0026#39;: \u0026#39;10.102.4.33\u0026#39;} Checking MTL. last_reserved_ip: 10.1.1.10 start_dhcp_pool:10.1.1.17 The space between last_reserved_ip and start_dhcp_pool is 7 IP. There is not enough static IP space to add an NCN.Adjusting DHCP pool start. {\u0026#39;HMN\u0026#39;: {\u0026#39;10.254.1.33\u0026#39;, \u0026#39;10.254.1.35\u0026#39;, \u0026#39;10.254.1.36\u0026#39;, \u0026#39;10.254.1.38\u0026#39;, \u0026#39;10.254.1.34\u0026#39;, \u0026#39;10.254.1.39\u0026#39;, \u0026#39;10.254.1.22\u0026#39;, \u0026#39;10.254.1.29\u0026#39;, \u0026#39;10.254.1.40\u0026#39;, \u0026#39;10.254.1.27\u0026#39;, \u0026#39;10.254.1.21\u0026#39;, \u0026#39;10.254.1.28\u0026#39;, \u0026#39;10.254.1.26\u0026#39;, \u0026#39;10.254.1.30\u0026#39;, \u0026#39;10.254.1.31\u0026#39;, \u0026#39;10.254.1.32\u0026#39;, \u0026#39;10.254.1.25\u0026#39;, \u0026#39;10.254.1.24\u0026#39;, \u0026#39;10.254.1.23\u0026#39;, \u0026#39;10.254.1.37\u0026#39;}, \u0026#39;CAN\u0026#39;: {\u0026#39;10.102.4.23\u0026#39;, \u0026#39;10.102.4.15\u0026#39;, \u0026#39;10.102.4.16\u0026#39;, \u0026#39;10.102.4.24\u0026#39;, \u0026#39;10.102.4.19\u0026#39;, \u0026#39;10.102.4.21\u0026#39;, \u0026#39;10.102.4.20\u0026#39;, \u0026#39;10.102.4.17\u0026#39;, \u0026#39;10.102.4.22\u0026#39;, \u0026#39;10.102.4.18\u0026#39;}, \u0026#39;MTL\u0026#39;: {\u0026#39;10.1.1.16\u0026#39;, \u0026#39;10.1.1.18\u0026#39;, \u0026#39;10.1.1.20\u0026#39;, \u0026#39;10.1.1.12\u0026#39;, \u0026#39;10.1.1.17\u0026#39;, \u0026#39;10.1.1.13\u0026#39;, \u0026#39;10.1.1.19\u0026#39;, \u0026#39;10.1.1.15\u0026#39;, \u0026#39;10.1.1.14\u0026#39;, \u0026#39;10.1.1.11\u0026#39;}} add_ncn_count: 10 ip_dhcp_pool_start: {\u0026#39;MTL\u0026#39;: \u0026#39;10.1.1.17\u0026#39;, \u0026#39;NMN\u0026#39;: \u0026#39;10.252.1.20\u0026#39;, \u0026#39;CAN\u0026#39;: \u0026#39;10.102.4.22\u0026#39;, \u0026#39;HMN\u0026#39;: \u0026#39;10.254.1.26\u0026#39;} new_ip_dhcp_pool_start: {\u0026#39;HMN\u0026#39;: \u0026#39;10.254.1.47\u0026#39;, \u0026#39;CAN\u0026#39;: \u0026#39;10.102.4.33\u0026#39;, \u0026#39;MTL\u0026#39;: \u0026#39;10.1.1.28\u0026#39;} Checking NMN. last_reserved_ip: 10.252.1.12 start_dhcp_pool:10.252.1.20 The space between last_reserved_ip and start_dhcp_pool is 8 IP. There is not enough static IP space to add an NCN.Adjusting DHCP pool start. {\u0026#39;HMN\u0026#39;: {\u0026#39;10.254.1.33\u0026#39;, \u0026#39;10.254.1.35\u0026#39;, \u0026#39;10.254.1.36\u0026#39;, \u0026#39;10.254.1.38\u0026#39;, \u0026#39;10.254.1.34\u0026#39;, \u0026#39;10.254.1.39\u0026#39;, \u0026#39;10.254.1.22\u0026#39;, \u0026#39;10.254.1.29\u0026#39;, \u0026#39;10.254.1.40\u0026#39;, \u0026#39;10.254.1.27\u0026#39;, \u0026#39;10.254.1.21\u0026#39;, \u0026#39;10.254.1.28\u0026#39;, \u0026#39;10.254.1.26\u0026#39;, \u0026#39;10.254.1.30\u0026#39;, \u0026#39;10.254.1.31\u0026#39;, \u0026#39;10.254.1.32\u0026#39;, \u0026#39;10.254.1.25\u0026#39;, \u0026#39;10.254.1.24\u0026#39;, \u0026#39;10.254.1.23\u0026#39;, \u0026#39;10.254.1.37\u0026#39;}, \u0026#39;CAN\u0026#39;: {\u0026#39;10.102.4.23\u0026#39;, \u0026#39;10.102.4.15\u0026#39;, \u0026#39;10.102.4.16\u0026#39;, \u0026#39;10.102.4.24\u0026#39;, \u0026#39;10.102.4.19\u0026#39;, \u0026#39;10.102.4.21\u0026#39;, \u0026#39;10.102.4.20\u0026#39;, \u0026#39;10.102.4.17\u0026#39;, \u0026#39;10.102.4.22\u0026#39;, \u0026#39;10.102.4.18\u0026#39;}, \u0026#39;MTL\u0026#39;: {\u0026#39;10.1.1.16\u0026#39;, \u0026#39;10.1.1.18\u0026#39;, \u0026#39;10.1.1.20\u0026#39;, \u0026#39;10.1.1.12\u0026#39;, \u0026#39;10.1.1.17\u0026#39;, \u0026#39;10.1.1.13\u0026#39;, \u0026#39;10.1.1.19\u0026#39;, \u0026#39;10.1.1.15\u0026#39;, \u0026#39;10.1.1.14\u0026#39;, \u0026#39;10.1.1.11\u0026#39;}, \u0026#39;NMN\u0026#39;: {\u0026#39;10.252.1.21\u0026#39;, \u0026#39;10.252.1.14\u0026#39;, \u0026#39;10.252.1.19\u0026#39;, \u0026#39;10.252.1.18\u0026#39;, \u0026#39;10.252.1.16\u0026#39;, \u0026#39;10.252.1.15\u0026#39;, \u0026#39;10.252.1.13\u0026#39;, \u0026#39;10.252.1.22\u0026#39;, \u0026#39;10.252.1.20\u0026#39;, \u0026#39;10.252.1.17\u0026#39;}} add_ncn_count: 10 ip_dhcp_pool_start: {\u0026#39;MTL\u0026#39;: \u0026#39;10.1.1.17\u0026#39;, \u0026#39;NMN\u0026#39;: \u0026#39;10.252.1.20\u0026#39;, \u0026#39;CAN\u0026#39;: \u0026#39;10.102.4.22\u0026#39;, \u0026#39;HMN\u0026#39;: \u0026#39;10.254.1.26\u0026#39;} new_ip_dhcp_pool_start: {\u0026#39;HMN\u0026#39;: \u0026#39;10.254.1.47\u0026#39;, \u0026#39;CAN\u0026#39;: \u0026#39;10.102.4.33\u0026#39;, \u0026#39;MTL\u0026#39;: \u0026#39;10.1.1.28\u0026#39;, \u0026#39;NMN\u0026#39;: \u0026#39;10.252.1.31\u0026#39;} 2022-04-01 21:21:08,859 - __main__ - WARNING - Deleting {\u0026#34;ID\u0026#34;: \u0026#34;a4bf013efa5d\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;a4:bf:01:3e:fa:5d\u0026#34;, \u0026#34;IPAddress\u0026#34;: []} from SMD EthernetInterfaces. 2022-04-01 21:21:08,859 - __main__ - WARNING - Deleting 10.254.1.39 from kea active leases. 2022-04-01 21:21:08,930 - __main__ - WARNING - Deleting {\u0026#34;ID\u0026#34;: \u0026#34;a4bf0138ed46\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;a4:bf:01:38:ed:46\u0026#34;, \u0026#34;IPAddress\u0026#34;: []} from SMD EthernetInterfaces. 2022-04-01 21:21:08,931 - __main__ - WARNING - Deleting 10.254.1.29 from kea active leases. 2022-04-01 21:21:09,001 - __main__ - WARNING - Deleting {\u0026#34;ID\u0026#34;: \u0026#34;a4bf013ef0c6\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;a4:bf:01:3e:f0:c6\u0026#34;, \u0026#34;IPAddress\u0026#34;: []} from SMD EthernetInterfaces. 2022-04-01 21:21:09,001 - __main__ - WARNING - Deleting 10.254.1.27 from kea active leases. 2022-04-01 21:21:09,067 - __main__ - WARNING - Deleting {\u0026#34;ID\u0026#34;: \u0026#34;a4bf01656337\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;a4:bf:01:65:63:37\u0026#34;, \u0026#34;IPAddress\u0026#34;: []} from SMD EthernetInterfaces. 2022-04-01 21:21:09,068 - __main__ - WARNING - Deleting 10.254.1.28 from kea active leases. 2022-04-01 21:21:09,117 - __main__ - WARNING - Deleting {\u0026#34;ID\u0026#34;: \u0026#34;a4bf01656854\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;a4:bf:01:65:68:54\u0026#34;, \u0026#34;IPAddress\u0026#34;: []} from SMD EthernetInterfaces. 2022-04-01 21:21:09,118 - __main__ - WARNING - Deleting 10.254.1.26 from kea active leases. 2022-04-01 21:21:09,229 - __main__ - WARNING - Deleting {\u0026#34;ID\u0026#34;: \u0026#34;a4bf013eeb53\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;a4:bf:01:3e:eb:53\u0026#34;, \u0026#34;IPAddress\u0026#34;: []} from SMD EthernetInterfaces. 2022-04-01 21:21:09,229 - __main__ - WARNING - Deleting 10.254.1.25 from kea active leases. 2022-04-01 21:21:09,327 - __main__ - WARNING - Deleting {\u0026#34;ID\u0026#34;: \u0026#34;a4bf013edd72\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;a4:bf:01:3e:dd:72\u0026#34;, \u0026#34;IPAddress\u0026#34;: []} from SMD EthernetInterfaces. 2022-04-01 21:21:09,327 - __main__ - WARNING - Deleting 10.254.1.37 from kea active leases. 2022-04-01 21:21:09,703 - __main__ - WARNING - Deleting {\u0026#34;ID\u0026#34;: \u0026#34;a4bf013edd6e\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;a4:bf:01:3e:dd:6e\u0026#34;, \u0026#34;IPAddress\u0026#34;: []} from SMD EthernetInterfaces. 2022-04-01 21:21:09,703 - __main__ - WARNING - Deleting 10.252.1.21 from kea active leases. 2022-04-01 21:21:09,869 - __main__ - WARNING - Deleting {\u0026#34;ID\u0026#34;: \u0026#34;a4bf013ef0c2\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;a4:bf:01:3e:f0:c2\u0026#34;, \u0026#34;IPAddress\u0026#34;: []} from SMD EthernetInterfaces. 2022-04-01 21:21:09,870 - __main__ - WARNING - Deleting 10.252.1.22 from kea active leases. 2022-04-01 21:21:09,934 - __main__ - WARNING - Deleting {\u0026#34;ID\u0026#34;: \u0026#34;a4bf013eeb4f\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;a4:bf:01:3e:eb:4f\u0026#34;, \u0026#34;IPAddress\u0026#34;: []} from SMD EthernetInterfaces. 2022-04-01 21:21:09,935 - __main__ - WARNING - Deleting 10.252.1.20 from kea active leases. Please restart DVS and rebooting the following nodes before proceeding to the next step.:[\u0026#34;x3000c0s21b4\u0026#34;, \u0026#34;x3000c0s19b0\u0026#34;, \u0026#34;x3000c0s21b3\u0026#34;, \u0026#34;x3000c0s21b1\u0026#34;, \u0026#34;x3000c0s21b2\u0026#34;, \u0026#34;x3000c0s21b2n0\u0026#34;, \u0026#34;x3000c0s21b3n0\u0026#34;, \u0026#34;x3000c0s21b1n0\u0026#34;] prerequisite to prepare NCNs for removal, move and add Network expansion COMPLETED Log and backup of SLS, BSS and SMD can be found at: /tmp/ncn_task_backups2022-04-01_21-21-04 Restarting cray-dhcp-kea When adding new NCNs, there will be network configuration changes that will impact changing IP addresses on computes.\nThat will require a DVS restart to update the IP addresses in the DVS node_map.\nncn_add_pre-req.py will make the network adjustments and will list the component names (xnames) that will need to be rebooted after DVS is restarted. See example below:\nPlease restart DVS and rebooting the following nodes before proceeding to the next step.:[\u0026#34;x3000c0s21b4\u0026#34;, \u0026#34;x3000c0s19b0\u0026#34;, \u0026#34;x3000c0s21b3\u0026#34;, \u0026#34;x3000c0s21b1\u0026#34;, \u0026#34;x3000c0s21b2\u0026#34;, \u0026#34;x3000c0s21b2n0\u0026#34;, \u0026#34;x3000c0s21b3n0\u0026#34;, \u0026#34;x3000c0s21b1n0\u0026#34;] prerequisite to prepare NCNs for removal, move and add Network expansion COMPLETED Log and backup of SLS, BSS and SMD can be found at: /tmp/ncn_task_backups2022-04-01_21-21-04 Add worker, storage, or master NCNs Use this procedure to add a worker, storage, or master NCN.\nAdd NCN prerequisites For several of the commands in this section, variables must be set with the name of the node being added and its component name (xname).\nSet NODE to the hostname of the node being added (for example ncn-w001, ncn-s002, etc).\nncn-m# NODE=ncn-x00n If the component name (xname) is known, then set it now. Otherwise it will be determined in a later step.\nncn-m# XNAME=\u0026lt;xname\u0026gt; IMPORTANT: Ensure that the node being added to the system has been properly configured. If the node being added to the system has not been previously in the system, several settings need to be verified.\nEnsure that the NCN device to be added has been racked and cabled per the SHCD.\nEnsure that the NCN BMC is configured with the expected root user credentials.\nThe NCN BMC credentials need to match the current global air-cooled BMC default credentials. These can be viewed with the following commands:\nncn-m# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-m# kubectl -n vault exec -it cray-vault-0 -c vault -- env \\ VAULT_TOKEN=\u0026#34;${VAULT_PASSWD}\u0026#34; VAULT_ADDR=http://127.0.0.1:8200 \\ vault kv get secret/reds-creds/defaults Example output:\n==== Data ==== Key Value --- ----- Cray map[password:foobar username:root] If adding an NCN that was not previously in the system, follow the Access and Update the Settings for Replacement NCNs procedure.\nEnsure the NCN BMC is configured to use DHCP.\nThis does not apply to the BMC for ncn-m001, because it is statically configured for the site. Ensure that the NCN is configured to boot over the PCIe NICs instead of the Onboard 1 Gig NICs.\nSee the Switch PXE Boot from Onboard NIC to PCIe procedure. If adding an HPE NCN, then ensure that IPMI is enabled.\nCheck to see if IPMI is enabled:\nread -s is used to read the password in order to prevent it from being echoed to the screen or saved in the shell history. Note that the subsequent curl commands will do both of these things. If this is not desired, then the call should be made in another way.\nncn-m# NCN_BMC=ncn_bmc_hostname_or_ip_address ncn-m# read -r -s -p \u0026#34;${NCN_BMC} root password: \u0026#34; IPMI_PASSWORD ncn-m# export IPMI_PASSWORD ncn-m# curl -k -u root:\u0026#34;${IPMI_PASSWORD}\u0026#34; \u0026#34;https://${NCN_BMC}/redfish/v1/Managers/1/NetworkProtocol\u0026#34; | jq .IPMI Expected output:\n{ \u0026#34;Port\u0026#34;: 623, \u0026#34;ProtocolEnabled\u0026#34;: true } If IPMI is disabled, then enable IPMI and restart the BMC.\nEnable IPMI.\nncn-m# curl -k -u root:\u0026#34;${IPMI_PASSWORD}\u0026#34; -X PATCH -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;IPMI\u0026#34;: {\u0026#34;Port\u0026#34;: 623, \u0026#34;ProtocolEnabled\u0026#34;: true}}\u0026#39; \\ \u0026#34;https://${NCN_BMC}/redfish/v1/Managers/1/NetworkProtocol\u0026#34; | jq Expected output:\n{ \u0026#34;error\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;iLO.0.10.ExtendedInfo\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;See @Message.ExtendedInfo for more information.\u0026#34;, \u0026#34;@Message.ExtendedInfo\u0026#34;: [ { \u0026#34;MessageId\u0026#34;: \u0026#34;iLO.2.14.ResetRequired\u0026#34; } ] } } Restart the BMC.\nncn-m# curl -k -u root:\u0026#34;${IPMI_PASSWORD}\u0026#34; -X POST -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;ResetType\u0026#34;: \u0026#34;GracefulRestart\u0026#34;}\u0026#39; \\ \u0026#34;https://${NCN_BMC}/redfish/v1/Managers/1/Actions/Manager.Reset\u0026#34; | jq Expected output:\n{ \u0026#34;error\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;iLO.0.10.ExtendedInfo\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;See @Message.ExtendedInfo for more information.\u0026#34;, \u0026#34;@Message.ExtendedInfo\u0026#34;: [ { \u0026#34;MessageId\u0026#34;: \u0026#34;iLO.2.14.ResetInProgress\u0026#34; } ] } } Add NCN procedure The following is a high-level overview of the add NCN workflow:\nAllocate NCN IP Addresses. Add Switch Configuration. Add NCN data for SLS, BSS and HSM. Update Firmware via FAS. Boot NCN and Configure. Redeploy Services. Validate NCN. Validate Health. Remove worker, storage, or master NCNs Use this procedure to remove a worker, storage, or master NCN.\nRemove NCN prerequisites Open two sessions: one on the node that is to be removed and another on a different master or worker node.\nFor several of the commands in this section, variables must be set with the name of the node being removed and its component name (xname). Set NODE to the hostname of the node being removed (for example ncn-w001, ncn-s002, etc). Set XNAME to the xname of that node.\nncn# NODE=ncn-x00n ncn# XNAME=$(ssh ${NODE} cat /etc/cray/xname) ncn# echo \u0026#34;${XNAME}\u0026#34; Remove NCN procedure The following is a high-level overview of the remove NCN workflow:\nRemove NCN from Role, Wipe the Disks, and Power Down. Remove NCN data from SLS, BSS and HSM. Remove Switch Configuration. Redeploy Services. Validate Health. IMPORTANT: Update the SHCD to remove the device. This is only needed if no NCN device will be added back to same location with the same cabling.\nReplace or move worker, storage, or master NCNs Replacing an NCN is defined as removing an NCN of a given type and adding a different NCN of the same type (but with different MAC addresses) back into the same cabinet slot. Moving an NCN is defined as removing an NCN of a given type from one cabinet and adding it back into a different cabinet.\nIn general, scaling master nodes is not recommended because it can cause Etcd latency.\nReplace NCN procedure The following is a high-level overview of the replace NCN workflow:\nRemove Worker, Storage, or Master NCNs Add Worker, Storage, or Master NCNs "
},
{
	"uri": "/docs-csm/en-12/operations/network/metallb_bgp/metallb_configuration/",
	"title": "MetalLB Configuration",
	"tags": [],
	"description": "",
	"content": "MetalLB Configuration MetalLB provides a more robust configuration for the Node Management Network (NMNLB), Hardware Management Network (HMNLB), Customer Management Network (CMN), Customer High-Speed Network (CHN), and Customer Access Network (CAN). This configuration is generated from the csi config init input values.\nMetalLB Peer Configuration The content for metallb_bgp_peers is generated by the csi config init command. In addition to the MetalLB configuration, there is configuration needed on the spine switches to set up the BGP router on these switches. If the system is configured to use the CHN for the user network, then configuration is also needed on the edge switches.\nThe MetalLB ConfigMap can also be viewed:\npeers: - peer-address: 10.103.11.2 peer-asn: 65533 my-asn: 65532 - peer-address: 10.103.11.3 peer-asn: 65533 my-asn: 65532 - peer-address: 10.252.0.1 peer-asn: 65533 my-asn: 65533 - peer-address: 10.252.0.3 peer-asn: 65533 my-asn: 65533 To retrieve data about BGP peers:\nncn-m001# kubectl get cm metallb -n metallb-system -o yaml The speakers get their peering configuration from the MetalLB ConfigMap. This configuration specifies the IP address of the spine or aggregate switch, as well as the Autonomous System Number (ASN) for the speaker and the switch with which it is peering.\nCMN Configuration This network has two MetalLB address pools, one for static IP allocation and the other for dynamic IP allocation. Static allocation guarantees the same IP allocation for services using this pool across deployment and installations. Dynamic allocations means that the allocated IP addresses will be in this pool, but may change depending on the timing and ordering of the IP allocation.\nView the address pool configurations in the MetalLB ConfigMap after system installation. The MetalLB ConfigMap should not be edited directly or they may be overwritten in a later update. The following is an example of the values for the CMN address pools in the ConfigMap:\n- name: customer-management protocol: bgp addresses: **- 10.102.5.64/26** - name: customer-management-static protocol: bgp addresses: **- 10.102.5.60/30** The CMN configuration is set in the csi config init input:\nlinux# csi config init . . . --cmn-cidr 10.102.5.0/25 --cmn-gateway 10.102.5.1 --cmn-static-pool 10.102.5.60/30 --cmn-dynamic-pool 10.102.5.64/26 . . . CAN or CHN Configuration There will be either CAN or CHN configured, not both. This network has one MetalLB address pool for dynamic IP allocation. A static pool is not needed for this network.\nThe following is an example of the values for the CAN address pool in the ConfigMap:\n- name: customer-access protocol: bgp addresses: **- 10.102.5.160/27** The following is an example of the values for the CHN address pool in the ConfigMap:\n- name: customer-high-speed protocol: bgp addresses: **- 10.102.6.160/27** The CAN or CHN configuration is set in the csi config init input:\nlinux# csi config init . . . --can-cidr 10.102.5.128/26 --can-gateway 10.102.5.129 --can-dynamic-pool 10.102.5.160/27 --chn-cidr 10.102.6.128/26 --chn-gateway 10.102.6.129 --chn-dynamic-pool 10.102.6.160/27 . . . "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/arp/",
	"title": "Address resolution protocol (ARP)",
	"tags": [],
	"description": "",
	"content": "Address resolution protocol (ARP) ARP is commonly used for mapping IPv4 addresses to MAC addresses. Static ARP addresses only supported in management interfaces;\nRelevant Configuration\nConfigure static ARP on an interface\nSwitch (config) # interface mgmt0 switch(config interface mgmt0)# arp ipv4 IP-ADDR mac MAC-ADDR Show Commands to Validate Functionality\nswitch# show ip arp Expected Results\nStep 1: You are able to ping the connected device Step 2: You can view the ARP entries Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/load_saved_switch_config/",
	"title": "Load Saved Switch Configuration",
	"tags": [],
	"description": "",
	"content": "Load Saved Switch Configuration This procedure is intended for internal use only.\nThis procedure switches between already saved switch configurations. It is used to quickly switch between configurations that are already loaded on the switches.\nTo save switch configurations, refer to the Configuration Management procedure.\nWhen switching between configurations, the procedure must be followed on all management switches.\nSpine switches have three total configuration files/checkpoints.\n1.2 fresh install 1.2 upgrade 1.0 Leaf-BMC switches have have two configuration files/checkpoints.\n1.2 1.0 The procedure depends on the switch manufacturer:\nAruba Dell Mellanox Aruba List the checkpoint files.\nEnsure that the proper checkpoint files exist.\nsw# show checkpoint | include CSM Example output:\nCSM1_2_FRESH_INSTALL_CANU_1_3_2 latest User 2022-04-01T20:11:57Z GL.10.09.0010 CSM1_2_UPGRADE_CANU_1_3_2 checkpoint User 2022-04-01T18:57:06Z GL.10.09.0010 CSM1_0_CANU_1_2_4 checkpoint User 2022-03-15T21:37:11Z GL.10.09.0010 Rollback to desired checkpoint.\nsw# checkpoint rollback CSM1_2_UPGRADE_CANU_1_3_2 Reset ACLs.\nIn some rare cases, ACLs will not work as expected. In order to prevent this, run the following command after switching checkpoints.\nsw# access-list all reset Dell List the configuration files.\nEnsure that the proper configuration files exist.\nsw# dir config Example output:\nDirectory contents for folder: config Date (modified) Size (bytes) Name --------------------- ------------ ------------------------------------------ 2022-02-08T16:31:42Z 112189 csm1.0.xml 2022-02-08T16:28:31Z 112189 csm1.2.xml 2022-02-08T16:30:23Z 112189 startup.xml Copy the desired configuration to the startup configuration.\nsw# copy config://csm1.0.xml config://startup.xml Copy completed will be returned if successful.\nReboot the switch without saving configuration.\nsw# reload Example output:\nSystem configuration has been modified. Save? [yes/no]:no Mellanox View the configuration files.\nEnsure that the proper backup configurations exist.\nsw# show configuration files Example output:\ncsm1.0.canu1.1.21 (active) csm1.0.canu1.1.21.bak csm1.2.fresh_install_canu1.1.21 csm1.2.fresh_install_canu1.1.21.bak csm1.2.upgrade_canu1.1.21 csm1.2.upgrade_canu1.1.21.bak initial initial.bak Active configuration: csm1.0.canu1.1.21 Unsaved changes : yes Switch to the desired configuration.\nsw# configuration switch-to csm1.2.upgrade_canu1.1.21 Example output:\nThis requires a reboot. Type \u0026#39;yes\u0026#39; to confirm: yes The switch will then reboot to chosen configuration.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/fresh_install/",
	"title": "Fresh Install",
	"tags": [],
	"description": "",
	"content": "Fresh Install Use this procedure for either a first-time install or in the event a previous CSM was wiped and requires a new install.\nBefore continuing with install, make sure that CANU is running the most current version:\nInstall/Upgrade CANU\nProcedure CAUTION: All of the following steps should be done using an out-of-band connection. This process is disruptive and will require downtime.\nUpgrade switch firmware to specified firmware version.\nRefer to Update Management Network Firmware.\nIf the system had a previous version of CSM on it, you need to backup all custom configuration and credential configuration.\nRefer to Backup a Custom Configuration.\nIf the switches have any configuration, it is recommenced to erase it before adding any new configuration.\nRefer to Wipe Management Switch Configuration.\nValidate the SHCD.\nThe SHCD defines the topology of a Shasta system, this is needed when generating switch configurations. Refer to Validate the SHCD.\nValidate cabling between SHCD generated data and actual switch configuration.\nRefer to Validate Cabling.\nGenerate the switch configuration file(s).\nRefer to Generate Switch Configurations.\nApply the configuration to switch.\nRefer to Apply Switch Configurations.\nApply the custom configuration to switch, which includes site connection and credential info.\nRefer to one of the following procedures:\nApply Custom Switch Configurations 1.0 Apply Custom Switch Configurations 1.2\nSetup connection to the site.\nRefer to Setup Site Connection.\nCheck the differences between the generated configurations and the configurations on the system.\nRefer to Validate Switch Configurations.\nRun a suite of tests against the management network switches.\nRefer to Network Tests.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/arp/",
	"title": "Configure Address Resolution Protocol (ARP)",
	"tags": [],
	"description": "",
	"content": "Configure Address Resolution Protocol (ARP) ARP is commonly used for mapping IPv4 addresses to MAC addresses.\nConfiguration Commands Configure static ARP on an interface:\nswitch(config-if)# ip arp ipv4 IP-ADDR mac MAC-ADDR Show commands to validate functionality:\nswitch# show ip arp Expected Results Administrators are able to ping the connected device Administrators can view the ARP entries Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/canu/canu_validation_error/",
	"title": "Troubleshoot CANU Validation Errors",
	"tags": [],
	"description": "",
	"content": "Troubleshoot CANU Validation Errors Typical CANU validation errors and how to fix them.\nExample 1 validate_shcd - CRITICAL: A port number must be specified. Please correct the SHCD for HMN:V36 with an empty value SOLUTION: Blank cell. Minimally the Source or Destination and Port needs to be specified.\nExample 2 Tab PDU not found in ./HPE System Hela CCD.revA27.xlsx Available tabs: [\u0026#39;Config. Summary\u0026#39;, \u0026#39;HPE Cables\u0026#39;, \u0026#39;RiverRackLayout \u0026#39;, \u0026#39;Arista\u0026#39;, \u0026#39;River Device Diagrams\u0026#39;, \u0026#39;HPE Devices\u0026#39;, \u0026#39;SCT pt_pt\u0026#39;, \u0026#39;yaml\u0026#39;, \u0026#39;Mountain-TDS-Management\u0026#39;, \u0026#39;MTN Rack Layout\u0026#39;, \u0026#39;10G_25G_40G_100G\u0026#39;, \u0026#39;NMN\u0026#39;, \u0026#39;HMN\u0026#39;, \u0026#39;PDU \u0026#39;] SOLUTION: PDU has an extra space in the tab name.\nExample 3 validate_shcd - ERROR: On tab PDU, header column Slot not found. SOLUTION: Make sure the header descriptions include the name Slot.\nExample 4 validate_shcd - CRITICAL: On tab PDU, the header is formatted incorrectly. Columns must exist in the following order, but may have other columns in between: [0, 1, 2, \u0026#39;Slot\u0026#39;, \u0026#39;Port\u0026#39;, \u0026#39;Destination\u0026#39;, \u0026#39;Rack\u0026#39;, \u0026#39;Location\u0026#39;, \u0026#39;Port\u0026#39;] SOLUTION: Fix the header naming to match the expected output.\nExample 5 network_modeling.NetworkNode: No available ports found for slot bmc and speed 25 in node sw-leaf-002 validate_shcd - CRITICAL: None Failed to connect sw-leaf-002 to sw-leaf-bmc-002 bi-directionally while working on sheet HMN, row 25. validate_shcd - CRITICAL: None SOLUTION: Remove the connections going from sw-leaf-002 to sw-leaf-bmc-002, because at this time CSM does not utilize these ports.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/acl/",
	"title": "Access Control Lists (ACLs)",
	"tags": [],
	"description": "",
	"content": "Access Control Lists (ACLs) ACLs are used to help improve network performance and restrict network usage by creating policies to eliminate unwanted IP traffic by filtering packets where they enter the switch on layer 2 and layer 3 interfaces. An ACL is an ordered list of one or more access control list entries (ACEs) prioritized by sequence number. An incoming packet is matched sequentially against each entry in an ACL.\nWhen a match is made, the action of that ACE is taken and the packet is not compared against any other ACEs in the list.\nFor ACL filtering to take effect, configure an ACL and then assign it in the inbound or outbound direction on an L2 or L3 interface with IPV4 traffic, and inbound-only for IPv6.\nConfiguration Commands Create an ACL:\nswitch(config)# access-list \u0026lt;ip|ipv6|mac\u0026gt; ACL Copy an existing ACL:\nswitch(config)# access-list \u0026lt;ip|ipv6|mac\u0026gt; ACL copy NEW-ACL Resequence an ACL:\nswitch(config)# access-list \u0026lt;ip|ipv6|mac\u0026gt; ACL resequence VALUE INC Apply an ACL to the control plane:\nswitch(config)# apply access-list \u0026lt;ip|ipv6\u0026gt; ACL control-plane [vrf VRF] Add ACEs in the appropriate order:\nswitch(config-acl-ip)# [SEQ] \u0026lt;deny|permit\u0026gt; \u0026lt;any|PROTOCOL\u0026gt; \u0026lt;any|SRC\u0026gt; \u0026lt;any|DST\u0026gt; [count] [log] switch(config-acl-ip)# [SEQ] comment TEXT Apply the ACL to a physical interface, a logical interface or a VLAN (please note: ACLs on L3 VLAN interfaces are not supported):\nswitch(config-if)# apply access-list \u0026lt;ip|ipv6|mac\u0026gt; ACL \u0026lt;in|out\u0026gt; switch(config-vlan)# apply access-list \u0026lt;ip|ipv6|mac\u0026gt; ACL \u0026lt;in|out\u0026gt; Show commands to validate functionality: :\nswitch# show access-list [hitcounts] [ip|ipv6|mac ACL] [control-plane vrf VRF] Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/added_hardware/",
	"title": "Added Hardware",
	"tags": [],
	"description": "",
	"content": "Added Hardware Follow this procedure when new hardware is added to the system.\nProcedure Validate the SHCD.\nThe SHCD defines the topology of a Shasta system, this is needed when generating switch configurations.\nRefer to Validate the SHCD.\nGenerate the switch configuration file(s).\nRefer to Generate Switch Configs.\nCheck the differences between the generated configurations and the configurations on the system.\nRefer to Validate Switch Configs.\nRun a suite of tests against the management network switches.\nRefer to Network Tests.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/dns/enable_ncsd_on_uans/",
	"title": "Enable ncsd on UANs",
	"tags": [],
	"description": "",
	"content": "Enable ncsd on UANs Configure User Access Nodes (UANs) to start the ncsd service at boot time.\nThe nscd service is not currently enabled by default and systemd does not start it at boot time. There are two ways to start nscd on UAN nodes: manually starting the service or enabling the service in the UAN image. While restarting nscd manually has to be performed each time the UAN is rebooted, enabling nscd in the image only has to be done once. Then all UANs that use the image will have nscd started automatically on boot.\nStart ncsd manually on each UAN Enable ncsd in the UAN image Start ncsd manually on each UAN Log into a UAN.\nStart ncsd using systemctl.\nuan# systemctl start nscd Repeat the previous two steps for every UAN.\nEnable ncsd in the UAN image Determine the ID of the image used by the UAN.\nThis ID can be found in the BOS session template used to boot the UAN:\n{ \u0026#34;boot_sets\u0026#34;: { \u0026#34;uan\u0026#34;: { \u0026#34;boot_ordinal\u0026#34;: 2, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=340M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=nmn0:dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y quiet rd.neednet=1 rd.retry=10 rd.shell turbo_boost_limit=999 ifmap=net2:nmn0,lan0:hsn0,lan1:hsn1 spire_join_token=${SPIRE_JOIN_TOKEN}\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;LIST_OF_APPLICATION_NODES\u0026#34; ], \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/IMS_IMAGE_ID/manifest.json\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:nmn0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;cfs\u0026#34;: { \u0026#34;configuration\u0026#34;: \u0026#34;uan-config-PRODUCT_VERSION\u0026#34; }, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;uan-sessiontemplate-PRODUCT_VERSION\u0026#34; } The image ID is found in the path field inside the boot_sets object.\nEnable the ncsd service in the image.\nUse the procedure Customize an Image Root Using IMS and run the following commands in the image chroot:\nchroot# systemctl enable nscd.service chroot# /tmp/images.sh Obtain the new resultant image ID from the previous step.\nUpdate the UAN BOS session template with the new image ID.\nSee Create UAN Boot Images for instructions on updating the BOS session template.\nReboot the UANs with the updated session template.\nSee Boot UANs.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/external_dns/external_dns_failing_to_discover_services_workaround/",
	"title": "External DNS Failing to Discover Services Workaround",
	"tags": [],
	"description": "",
	"content": "External DNS Failing to Discover Services Workaround Many external DNS issues can be worked around by directly connecting to the desired backend service. This can circumvent authentication and authorization protections, but it may be necessary to access specific services when mitigating critical issues.\nIstio\u0026rsquo;s ingress gateway uses Gateway and VirtualService objects to configure how traffic is routed to backend services. Currently, there are three gateways supporting the externally accessible services: services-gateway, customer-admin-gateway, and customer-user-gateway. They are configured to support traffic on any host depending on the network over which the services are accessed. It is the VirtualService objects that ultimately control routing based on hostname.\nUse this procedure to resolve any external DNS routing issues with backend services.\nProcedure Search for the VirtualService object that corresponds to the desired service.\nThe command below will list all external hostnames.\nncn-mw# kubectl get vs -A | grep -v \u0026#39;[*]\u0026#39; Example output:\nNAMESPACE NAME GATEWAYS HOSTS AGE istio-system kiali [services/services-gateway] [kiali-istio.cmn.SYSTEM_DOMAIN_NAME] 2d16h istio-system tracing [services/services-gateway] [jaeger-istio.cmn.SYSTEM_DOMAIN_NAME] 2d16h nexus nexus [services/services-gateway] [packages.local registry.local nexus.cmn.SYSTEM_DOMAIN_NAME] 2d16h services gitea-vcs-external [services/services-gateway] [vcs.cmn.SYSTEM_DOMAIN_NAME] 2d16h services sma-grafana [services-gateway] [sma-grafana.cmn.SYSTEM_DOMAIN_NAME] 2d16h services sma-kibana [services-gateway] [sma-kibana.cmn.SYSTEM_DOMAIN_NAME] 2d16h sysmgmt-health cray-sysmgmt-health-alertmanager [services/services-gateway] [alertmanager.cmn.SYSTEM_DOMAIN_NAME] 2d16h sysmgmt-health cray-sysmgmt-health-grafana [services/services-gateway] [grafana.cmn.SYSTEM_DOMAIN_NAME] 2d16h sysmgmt-health cray-sysmgmt-health-prometheus [services/services-gateway] [prometheus.cmn.SYSTEM_DOMAIN_NAME] 2d16h Inspect the VirtualService objects to learn the destination service and port.\nUse the NAME value returned in the previous step. The following example is for the cray-sysmgmt-health-prometheus service.\nncn-mw# kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus -o yaml Example output:\napiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: creationTimestamp: \u0026#34;2020-07-09T17:49:07Z\u0026#34; generation: 1 labels: app: cray-sysmgmt-health-prometheus app.kubernetes.io/instance: cray-sysmgmt-health app.kubernetes.io/managed-by: Tiller app.kubernetes.io/name: cray-sysmgmt-health app.kubernetes.io/version: 8.15.4 helm.sh/chart: cray-sysmgmt-health-0.3.1 name: cray-sysmgmt-health-prometheus namespace: sysmgmt-health resourceVersion: \u0026#34;41620\u0026#34; selfLink: /apis/networking.istio.io/v1beta1/namespaces/sysmgmt-health/virtualservices/cray-sysmgmt-health-prometheus uid: d239dfcc-a827-4a51-9b73-6eccfb937088 spec: gateways: - services/services-gateway hosts: - prometheus.cmn.SYSTEM_DOMAIN_NAME http: - match: - authority: exact: prometheus.cmn.SYSTEM_DOMAIN_NAME route: - destination: host: cray-sysmgmt-health-promet-prometheus port: number: 9090 From the VirtualService data, it is straightforward to see how traffic will be routed. In this example, connections to prometheus.cmn.SYSTEM_DOMAIN_NAME will be routed to the cray-sysmgmt-health-prometheus service in the sysmgmt-health namespace on port 9090.\nExternal DNS will now be connected to the backend service.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/dhcp/troubleshoot_dhcp_issues/",
	"title": "Troubleshoot DHCP Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot DHCP Issues There are several things to check for when troubleshooting issues with Dynamic Host Configuration Protocol (DHCP) servers.\nIncorrect DHCP IP addresses One of the most common issues is when the DHCP IP addresses are not matching in the Domain Name Service (DNS).\nCheck to make sure cray-dhcp is not running in Kubernetes:\nncn-mw# kubectl get pods -A | grep cray-dhcp Example output:\nservices cray-dhcp-5f8c8767db-hg6ch 1/1 Running 0 35d If the cray-dhcp pod is running, use the following command to shut down the pod:\nncn-mw# kubectl scale deploy cray-dhcp --replicas=0 If the IP addresses are still not lining up with DNS and cray-dhcp is confirmed not running, then wait 800 seconds for DHCP leases to expire and renew.\nVerify the status of the cray-dhcp-kea pods and services Check to see if the Kea DHCP services are running:\nncn-mw# kubectl get services -n services | grep kea Example output:\ncray-dhcp-kea-api ClusterIP 10.26.142.204 \u0026lt;none\u0026gt; 8000/TCP 5d23h cray-dhcp-kea-tcp-hmn LoadBalancer 10.24.79.120 10.94.100.222 67:32120/TCP 5d23h cray-dhcp-kea-tcp-nmn LoadBalancer 10.19.139.179 10.92.100.222 67:31652/TCP 5d23h cray-dhcp-kea-udp-hmn LoadBalancer 10.25.203.31 10.94.100.222 67:30840/UDP 5d23h cray-dhcp-kea-udp-nmn LoadBalancer 10.19.187.168 10.92.100.222 67:31904/UDP 5d23h If the services shown in the output above are not present, then it could be an indication that something is not working correctly.\nTo check to see if the Kea pods are running:\nncn-mw# kubectl get pods -n services -o wide | grep kea Example output:\ncray-dhcp-kea-788b4c899b-x6ltd 3/3 Running 0 36h 10.40.3.183 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; The pods should be in a Running state. The output above will also indicate which worker node the kea-dhcp pod is currently running on.\nTo restart the pods:\nncn-mw# kubectl delete pods -n services -l app.kubernetes.io/name=cray-dhcp-kea Use the command mentioned above to verify the pods are running again after restarting the pods.\nCheck the current DHCP leases Use the Kea API to retrieve data from the DHCP lease database. An authentication token will be needed to access the Kea API.\nTo retrieve a token:\nncn# export TOKEN=$(curl -s -k -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Once a token has been generated, the DHCP lease database can be viewed. The commands below are the most effective way to check the current DHCP leases:\nView all leases:\nncn# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; \\ https://api-gw-service-nmn.local/apis/dhcp-kea | jq View the total number of leases:\nncn# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; \\ https://api-gw-service-nmn.local/apis/dhcp-kea | jq \u0026#39;.[].text\u0026#39; Use an IP address to search for a hostname or MAC address:\nncn# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ], \u0026#34;arguments\u0026#34;: { \u0026#34;ip-address\u0026#34;: \u0026#34;x.x.x.x\u0026#34; } }\u0026#39; \\ https://api-gw-service-nmn.local/apis/dhcp-kea | jq Use a MAC address to find a hostname or IP address:\nncn# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; \\ https://api-gw-service-nmn.local/apis/dhcp-kea | jq \u0026#39;.[].arguments.leases[] | \\ select(.\u0026#34;hw-address\u0026#34;==\u0026#34;XX:XX:XX:XX:XX:5d\u0026#34;)\u0026#39; Use a hostname to find a MAC address or IP address:\nncn# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; \\ https://api-gw-service-nmn.local/apis/dhcp-kea | jq \u0026#39;.[].arguments.leases[] | \\ select(.\u0026#34;hostname\u0026#34;==\u0026#34;xNAME\u0026#34;)\u0026#39; Check the Hardware State Manager (HSM) for issues The HSM includes two important components:\nSystems Layout Service (SLS): This is the expected state of the system. State Manager Daemon (SMD): This is the discovered or active state of the system during runtime. To view the information stored in SLS for a specific component name (xname):\nncn-mw# cray sls hardware describe XNAME To view the information in SMD:\nncn-mw# cray hsm inventory ethernetInterfaces describe XNAME View the cray-dhcp-kea logs To view the Kea logs:\nncn-mw# kubectl logs -n services -l app.kubernetes.io/instance=cray-dhcp-kea -c cray-dhcp-kea Example output:\n2020-08-03 21:47:50.580 INFO [kea-dhcp4.dhcpsrv/10] DHCPSRV_MEMFILE_LEASE_FILE_LOAD loading leases from file /cray-dhcp-kea-socket/dhcp4.leases 2020-08-03 21:47:50.580 INFO [kea-dhcp4.dhcpsrv/10] DHCPSRV_MEMFILE_LFC_SETUP setting up the Lease File Cleanup interval to 3600 sec 2020-08-03 21:47:50.580 WARN [kea-dhcp4.dhcpsrv/10] DHCPSRV_OPEN_SOCKET_FAIL failed to open socket: the interface eth0 has no usable IPv4 addresses configured 2020-08-03 21:47:50.580 WARN [kea-dhcp4.dhcpsrv/10] DHCPSRV_NO_SOCKETS_OPEN no interface configured to listen to DHCP traffic 2020-08-03 21:48:00.602 INFO [kea-dhcp4.commands/10] COMMAND_RECEIVED Received command \u0026#39;lease4-get-all\u0026#39; {\u0026#34;Dhcp4\u0026#34;: {\u0026#34;control-socket\u0026#34;: {\u0026#34;socket-name\u0026#34;: \u0026#34;/cray-dhcp-kea-socket/cray-dhcp-kea.socket\u0026#34;, \u0026#34;socket-type\u0026#34;: \u0026#34;unix\u0026#34;}, \u0026#34;hooks-libraries\u0026#34;: [{\u0026#34;library\u0026#34;: \u0026#34;/usr/local/lib/kea/hooks/libdhcp_lease_cmds.so\u0026#34;}, ...SNIP... waiting 10 seconds for any leases to be given out... [{\u0026#39;arguments\u0026#39;: {\u0026#39;leases\u0026#39;: []}, \u0026#39;result\u0026#39;: 3, \u0026#39;text\u0026#39;: \u0026#39;0 IPv4 lease(s) found.\u0026#39;}] 2020-08-03 21:48:22.734 INFO [kea-dhcp4.commands/10] COMMAND_RECEIVED Received command \u0026#39;config-get\u0026#39; tcpdump If a host is not getting an IP address, then run a packet capture to see if DHCP traffic is being transmitted.\nncn# tcpdump -w dhcp.pcap -envli bond0.nmn0 port 67 or port 68 This will create a file named dhcp.pcap in the current directory. It will collect all DHCP traffic on the specified port. In this example. it would be the DHCP traffic on interface bond0.nmn0 (10.252.0.0/17).\nTo view the DHCP traffic:\nncn# tcpdump -r dhcp.pcap -v -n The output may be very long, so use any desired filters to narrow the results.\nTo do a tcpdump for a certain MAC address:\nncn# tcpdump -i eth0 -vvv -s 1500 \u0026#39;((port 67 or port 68) and (udp[38:4] = 0x993b7030))\u0026#39; This example is using the MAC of b4:2e:99:3b:70:30. It will show the output on the terminal and will not save to a file.\nVerify that MetalLB/BGP peering and routes are correct Log in to the spine switches and check that MetalLB is peering to the spines via BGP.\nCheck both spines if they are available and powered up. All worker nodes should be peered with the spine BGP.\nsw-spine# show ip bgp neighbors Example output:\nBGP neighbor: 10.252.0.4, remote AS: 65533, link: internal: Route-map (in/out) : rm-ncn-w001 BGP version : 4 Configured hold time in seconds : 180 keepalive interval in seconds (configured) : 60 keepalive interval in seconds (established with peer): 30 Minimum holdtime from neighbor in seconds : 90BGP neighbor: 10.252.0.5, remote AS: 65533, link: internal: Route-map (in/out) : rm-ncn-w002 BGP version : 4 Configured hold time in seconds : 180 keepalive interval in seconds (configured) : 60 keepalive interval in seconds (established with peer): 30 Minimum holdtime from neighbor in seconds : 90BGP neighbor: 10.252.0.6, remote AS: 65533, link: internal: Route-map (in/out) : rm-ncn-w003 BGP version : 4 Configured hold time in seconds : 180 keepalive interval in seconds (configured) : 60 keepalive interval in seconds (established with peer): 30 Minimum holdtime from neighbor in seconds : 90 Confirm that routes to Kea (10.92.100.222) via all the NCN worker nodes are available:\nsw-spine# show ip route 10.92.100.222 Example output:\nFlags: F: Failed to install in H/W B: BFD protected (static route) i: BFD session initializing (static route) x: protecting BFD session failed (static route) c: consistent hashing p: partial programming in H/W VRF Name default: ------------------------------------------------------------------------------------------------------ Destination Mask Flag Gateway Interface Source AD/M ------------------------------------------------------------------------------------------------------ default 0.0.0.0 c 10.102.255.9 eth1/16 static 1/1 10.92.100.222 255.255.255.255 c 10.252.0.4 vlan2 bgp 200/0 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 "
},
{
	"uri": "/docs-csm/en-12/operations/network/connect_to_the_hpe_cray_ex_environment/",
	"title": "Connect to the HPE Cray EX Environment",
	"tags": [],
	"description": "",
	"content": "Connect to the HPE Cray EX Environment The HPE Cray EX Management Network (SMNet) has multiple separate physical and logical links that are used to segregate traffic.\nThe diagram below shows the available connections from within the SMNet, as well as the connections to the customer network:\nThere are multiple ways to connect to the HPE Cray EX environment. The various methods are described in the following table:\nRole Description Administrative External customer network connection to the worker node\u0026rsquo;s hardware management and administrative port Application node access External customer network connection to an Application Node Customer Access Network (CAN) Customer connection to the CAN gateway to access the HPE Cray EX CAN There are also several ways to physically connect to the nodes on the system. The following table describes the physical connections to the administrative and application nodes:\nRole Description Administrative node 1GB copper connection made from the customer network that provides administrative access and hardware management Application node Different interconnect types are available from the customer network based on customer requirements For more information on connecting to the CAN, see Connect to the CMN and CAN.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/customer_accessible_networks/customer_accessible_networks/",
	"title": "Customer Accessible Networks",
	"tags": [],
	"description": "",
	"content": "Customer Accessible Networks There are generally two networks accessible by devices outside of the CSM cluster. One network is for administrators managing the cluster and one is for users accessing user services provided by the cluster.\nCustomer Management Network The Customer Management Network (CMN) provides access from outside the customer network to administrative services and non-compute nodes (NCNs). This allows for the following:\nAdministrator clients outside of the system: Log in to NCNs. Access administrative web UIs within the system (e.g. Prometheus, Grafana, and more). Access the administrative REST APIs. Access a DNS server within the system for resolution of names for the webUI and REST API services. Run administrative Cray CLI commands from outside the system. NCNs to access systems outside the cluster (e.g. LDAP, license servers, and more). Services within the cluster to access systems outside the cluster. These nodes and services need an IP address that routes to the customer\u0026rsquo;s network in order to be accessed from outside the network.\nImplications if CAN is not configured No direct access to the NCNs other than ncn-m001. Will need to hop through ncn-m001 to get to the rest of the NCNs. No direct access to the UANs unless the UAN has a direct connection to the customer network. NCNs other than ncn-m001 do not have access to services outside of the system (e.g. LDAP, license servers, and more). These nodes will not have an interface on any network with access outside of the HPE Cray EX system. These nodes will not have a default route. This includes access to any of the externally exposed services from these nodes. Pods running on NCNs other than ncn-m001 will not have access to services outside of the system. No access to externally exposed services. See Externally Exposed Services for more information. Customer User Networks (CAN/CHN) The CSM cluster can be configured with a user network that uses either the management network or the high-speed network. The cluster cannot have both CAN and CHN.\nThe Customer Access Network (CAN) will use a VLAN on the management switches. The Customer High-Speed Network (CHN) will use the high-speed network.\nThe user network will allow for the following:\nUser clients outside of the system: Log in to UANs. Access user web UIs within the system (e.g. Capsules). Access the user REST APIs. Run user Cray CLI commands from outside the system. Access the User Access Instances (UAI). UANs to access systems outside the cluster (e.g. LDAP, license servers, and more). Subnet configuration CMN subnets CMN IP addresses are allocated from a single IP subnet that is configured as the cmn-cidr value in the csi config init input. This subnet is further divided into three smaller subnets:\nSubnet for NCNs and switches. Subnet for the MetalLB static pool (cmn-static-pool). This is used for services that need to be pinned to the same IP address. For example, the PowerDNS service that needs to be configured in the upstream DNS server. This subnet currently needs only a few IP addresses. Subnet for the MetalLB dynamic pool (cmn-dynamic-pool). This is used for the rest of the externally exposed services and are allocated dynamically. These IP addresses can be allocated differently across deployments because these services are accessed by DNS name rather than by IP address. The minimum size for the CMN subnet is /25. The CMN /25 subnet allows for the following:\n16 IP addresses for NCNs 16 IP addresses for switches. 4 IP addresses for the CMN static service IP addresses. 64 IP addresses for the rest of the external CMN services. 6 of these IP addresses are used as standard CMN service IP addresses and the remaining 58 IP addresses are for IMS services. If there are more IP addresses needed for any of those sections, then the CMN subnet will need to be larger than a /25.\nCAN/CHN subnets CAN or CHN IP addresses are allocated from a single IP subnet that is configured as the can-cidr or chn-cidr value in the csi config init input. Only one of these two networks should be defined. The user network subnet is further divided into two smaller subnets:\nSubnet for NCNs, UANs, and switches. Subnet for the MetalLB dynamic pool (can-dynamic-pool) or (chn-dynamic-pool). This is used for all of the externally exposed services and are allocated dynamically. These IP addresses can be allocated differently across deployments because these services are accessed by DNS name rather than by IP address. The minimum size for the CAN or CHN subnet is /27. The /27 subnet allows for the following:\n16 IP addresses for NCNs, UANs, and Switches 16 IP addresses for the external CAN or CHN services. 2 of these IP addresses are used as standard CAN/CHN service IP addresses and the remaining 14 IP addresses are for UAI services. If there are more than 16 IP addresses needed for either of those sections, then the CAN/CHN subnet will need to be larger than a /27.\nCustomer variables The following variables are defined in the csi config init input. These examples use values for the layouts described above. cmn-external-dns must be an IP address within the cmn-static-pool CIDR.\nbican-user-network-name specifies whether the user network is on the management network (CAN) or the high-speed network (CHN).\nlinux# csi config init Example output with CAN:\n[...] --system-name testsystem --site-domain example.com --bican-user-network-name CAN --cmn-cidr 10.102.5.0/25 --cmn-gateway 10.102.5.1 --cmn-static-pool 10.102.5.60/30 --cmn-dynamic-pool 10.102.5.64/26 --cmn-external-dns 10.102.5.61 --can-cidr 10.102.6.0/27 --can-gateway 10.102.6.1 --can-dynamic-pool 10.102.6.16/28 [...] Example output with CHN:\n[...] --system-name testsystem --site-domain example.com --bican-user-network-name CHN --cmn-cidr 10.102.5.0/25 --cmn-gateway 10.102.5.1 --cmn-static-pool 10.102.5.60/30 --cmn-dynamic-pool 10.102.5.64/26 --cmn-external-dns 10.102.5.61 --chn-cidr 10.102.6.0/27 --chn-gateway 10.102.6.1 --chn-dynamic-pool 10.102.6.16/28 [...] "
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/about_postgres/",
	"title": "About Postgres",
	"tags": [],
	"description": "",
	"content": "About Postgres The system uses PostgreSQL (known as Postgres) as a database solution. Postgres databases use SQL language to store and manage databases on the system.\nTo learn more about Postgres, see https://www.postgresql.org/docs/.\nThe Patroni tool can be used to manage and maintain information in a Postgres database. It handles tasks such as listing cluster members and the replication status, configuring and restarting databases, and more. For more information about this tool, refer to Troubleshoot Postgres Database.\n"
},
{
	"uri": "/docs-csm/en-12/operations/image_management/build_an_image_using_ims_rest_service/",
	"title": "Build an Image Using IMS REST Service",
	"tags": [],
	"description": "",
	"content": "Build an Image Using IMS REST Service Create an image root from an IMS recipe.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCN) and include the following deployments: cray-ims, the Image Management Service (IMS) cray-nexus, the Nexus repository manager service The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system. kubectl is installed locally and configured to point at the SMS Kubernetes cluster. A Kiwi image recipe uploaded as a gzipped tar file and registered with IMS. See Upload and Register an Image Recipe. A token providing Simple Storage Service (S3) credentials has been generated. Limitations The commands in this procedure must be run as the root user.\nProcedure Prepare to create the image Check for an existing IMS public key ID.\nSkip this step if it is known that a public key associated with the user account being used was not previously uploaded to the IMS service.\nThe following query may return multiple public key records. The correct one will have a name value including the current username in use.\nncn# cray ims public-keys list Example output excerpt:\n[[results]] public_key = \u0026#34;ssh-rsa AAAAB3NzaC1yc2EA ... AsVruw1Zeiec2IWt\u0026#34; id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; name = \u0026#34;username public key\u0026#34; created = \u0026#34;2018-11-21T17:19:07.830000+00:00\u0026#34; If a public key associated with the username in use is not returned, proceed to the next step. If a public key associated with the username does exist, create a variable for the IMS public key id value in the returned data and then proceed to step 3.\nncn# IMS_PUBLIC_KEY_ID=a252ff6f-c087-4093-a305-122b41824a3e Upload the SSH public key to the IMS service.\nSkip this step if an IMS public key record has already been created for the account being used.\nThe IMS debug/configuration shell relies on passwordless SSH. This SSH public key needs to be uploaded to IMS to enable interaction with the image customization environment later in this procedure.\nReplace the username value with the actual username being used on the system when setting the public key name.\nncn# cray ims public-keys create --name \u0026#34;username public key\u0026#34; --public-key ~/.ssh/id_rsa.pub Example output:\npublic_key = \u0026#34;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCl50gK4l9uupxC2KHxMpTNxPTJbnwEdWy1jst5W5LqJx9fdTrc9uNJ33HAq+WIOhPVGbLm2N4GX1WTUQ4+wVOSmmBBJnlu/l5rmO9lEGT6U8lKG8dA9c7qhguGHy7M7WBgdW/gWA16gwE/u8Qc2fycFERRKmFucL/Er9wA0/Qvz7/U59yO+HOtk5hvEz/AUkvaaoY0IVBfdNBCl59CIdZHxDzgXlXzd9PAlrXZNO8jDD3jyFAOvMMRG7py78zj2NUngvsWYoBcV3FcREZJU529uJ0Au8Vn9DRADyB4QQS2o+fa6hG9i2SzfY8L6vAVvSE7A2ILAsVruw1Zeiec2IWt\u0026#34; id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; name = \u0026#34;username public key\u0026#34; created = \u0026#34;2018-11-21T17:19:07.830000+00:00\u0026#34; If successful, create a variable for the IMS public key id value in the returned data.\nncn# IMS_PUBLIC_KEY_ID=a252ff6f-c087-4093-a305-122b41824a3e Get the IMS recipe to build Locate the IMS recipe needed to build the image.\nncn# cray ims recipes list Example output excerpt:\n[[results]] id = \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34; name = \u0026#34;my_recipe.tgz\u0026#34; created = \u0026#34;2020-02-05T19:24:22.621448+00:00\u0026#34; [results.link] path = \u0026#34;s3://ims/recipes/2233c82a-5081-4f67-bec4-4b59a60017a6/my_recipe.tgz\u0026#34; etag = \u0026#34;28f3d78c8cceca2083d7d3090d96bbb7\u0026#34; type = \u0026#34;s3\u0026#34; If successful, create a variable for the IMS recipe id in the returned data.\nncn# IMS_RECIPE_ID=2233c82a-5081-4f67-bec4-4b59a60017a6 Submit the Kubernetes image create job Create an IMS job record and start the image creation job.\nAfter building an image, IMS will automatically upload any build artifacts (root file system, kernel and initrd) to the artifact repository, and associate them with IMS. IMS is not able to dynamically determine the Linux kernel and initrd to look for because the file name for these vary depending upon Linux distribution, Linux version, dracut configuration, and more. Thus, the user must pass the name of the kernel and initrd that IMS will look for in the resultant image root\u0026rsquo;s /boot directory.\nUse the following table to help determine the default kernel and initrd file names to specify when submitting the job to customize an image. These are just default names. Please consult with the site administrator to determine if these names have been changed for a given image or recipe.\nRecipe Recipe Name Kernel File Name initrd File Name SLES 15 SP3 Barebones cray-sles15sp3-barebones vmlinuz initrd COS cray-shasta-compute-sles15sp3.x86_64-1.4.66 vmlinuz initrd ncn# cray ims jobs create \\ --job-type create \\ --image-root-archive-name cray-sles15-barebones \\ --artifact-id $IMS_RECIPE_ID \\ --public-key-id $IMS_PUBLIC_KEY_ID \\ --enable-debug False Example output:\nstatus = \u0026#34;creating\u0026#34; enable_debug = false kernel_file_name = \u0026#34;vmlinuz\u0026#34; artifact_id = \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34; build_env_size = 10 job_type = \u0026#34;create\u0026#34; kubernetes_service = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34; kubernetes_job = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-create\u0026#34; id = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34; image_root_archive_name = \u0026#34;cray-sles15-barebones\u0026#34; initrd_file_name = \u0026#34;initrd\u0026#34; created = \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34; public_key_id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; kubernetes_configmap = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34; If successful, create variables for the IMS job id and kubernetes_job values in the returned data.\nncn# IMS_JOB_ID=ad5163d2-398d-4e93-94f0-2f439f114fe7 ncn# IMS_KUBERNETES_JOB=cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-create Describe the image create job.\nncn# kubectl -n ims describe job $IMS_KUBERNETES_JOB Example output:\nName: ims-myimage-create Namespace: default [...] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 4m job-controller Created pod: cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-create-lt69t If successful, create a variable for the pod name that was created above, displayed in the Events section.\nncn# POD=cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-create-lt69t Watch the logs from the fetch-recipe, wait-for-repos, build-ca-rpm, build-image, and buildenv-sidecar containers to monitor the image creation process.\nUse kubectl and the returned pod name from the previous step to retrieve this information.\nThe fetch-recipe container is responsible for fetching the recipe archive from S3 and uncompressing the recipe.\nncn# kubectl -n ims logs -f $POD -c fetch-recipe Example output:\nINFO:/scripts/fetch.py:IMS_JOB_ID=ad5163d2-398d-4e93-94f0-2f439f114fe7 INFO:/scripts/fetch.py:Setting job status to \u0026#39;fetching_recipe\u0026#39;. INFO:ims_python_helper:image_set_job_status: {{ims_job_id: ad5163d2-398d-4e93-94f0-2f439f114fe7, job_status: fetching_recipe}} INFO:ims_python_helper:PATCH https://api-gw-service-nmn.local/apis/ims/jobs/ad5163d2-398d-4e93-94f0-2f439f114fe7 status=fetching_recipe INFO:/scripts/fetch.py:Fetching recipe http://rgw.local:8080/ims/recipes/2233c82a-5081-4f67-bec4-4b59a60017a6/my_recipe.tgz?AWSAccessKeyId=GQZKV1HAM80ZFDZJFFS7\u0026amp;Expires=1586891507\u0026amp;Signature=GzRzuTWo3p5CoKHzT2mIuPQXLGM%3D INFO:/scripts/fetch.py:Saving file as \u0026#39;/mnt/recipe/recipe.tgz\u0026#39; INFO:/scripts/fetch.py:Verifying md5sum of the downloaded file. INFO:/scripts/fetch.py:Successfully verified the md5sum of the downloaded file. INFO:/scripts/fetch.py:Uncompressing recipe into /mnt/recipe INFO:/scripts/fetch.py:Deleting compressed recipe /mnt/recipe/recipe.tgz INFO:/scripts/fetch.py:Done The wait-for-repos container will ensure that any HTTP/HTTPS repositories referenced by the Kiwi-NG recipe can be accessed and are available. This helps ensure that the image will be built successfully. If 301 responses are returned instead of 200 responses, that does not indicate an error.\nncn# kubectl -n ims logs -f $POD -c wait-for-repos Example output:\n2019-05-17 09:53:47,381 - INFO - __main__ - Recipe contains the following repos: [\u0026#39;http://api-gw-service-nmn.local/repositories/sle15-Module-Basesystem/\u0026#39;, \u0026#39;http://api-gw-service-nmn.local/repositories/sle15-Product-SLES/\u0026#39;, \u0026#39;http://api-gw-service-nmn.local/repositories/cray-sle15\u0026#39;] 2019-05-17 09:53:47,381 - INFO - __main__ - Attempting to get http://api-gw-service-nmn.local/repositories/sle15-Module-Basesystem/repodata/repomd.xml 2019-05-17 09:53:47,404 - INFO - __main__ - 200 response getting http://api-gw-service-nmn.local/repositories/sle15-Module-Basesystem/repodata/repomd.xml 2019-05-17 09:53:47,404 - INFO - __main__ - Attempting to get http://api-gw-service-nmn.local/repositories/sle15-Product-SLES/repodata/repomd.xml 2019-05-17 09:53:47,431 - INFO - __main__ - 200 response getting http://api-gw-service-nmn.local/repositories/sle15-Product-SLES/repodata/repomd.xml 2019-05-17 09:53:47,431 - INFO - __main__ - Attempting to get http://api-gw-service-nmn.local/repositories/cray-sle15/repodata/repomd.xml 2019-05-17 09:53:47,458 - INFO - __main__ - 200 response getting http://api-gw-service-nmn.local/repositories/cray-sle15/repodata/repomd.xml The build-ca-rpm container creates an RPM with the private root CA certificate for the system. This RPM is installed automatically by Kiwi-NG to ensure that Kiwi can securely talk to the Nexus repositories when building the image root.\nncn# kubectl -n ims logs -f $POD -c build-ca-rpm Example output:\ncray_ca_cert-1.0.0/ cray_ca_cert-1.0.0/etc/ cray_ca_cert-1.0.0/etc/cray/ cray_ca_cert-1.0.0/etc/cray/ca/ cray_ca_cert-1.0.0/etc/cray/ca/certificate_authority.crt Executing(%prep): /bin/sh -e /var/tmp/rpm-tmp.pgDBLk + umask 022 + cd /root/rpmbuild/BUILD + cd /root/rpmbuild/BUILD + rm -rf cray_ca_cert-1.0.0 + /bin/gzip -dc /root/rpmbuild/SOURCES/cray_ca_cert-1.0.0.tar.gz + /bin/tar -xof - + STATUS=0 + \u0026#39;[\u0026#39; 0 -ne 0 ] + cd cray_ca_cert-1.0.0 + /bin/chmod -Rf a+rX,u+w,g-w,o-w . + exit 0 Executing(%build): /bin/sh -e /var/tmp/rpm-tmp.gILKaJ + umask 022 + cd /root/rpmbuild/BUILD + cd cray_ca_cert-1.0.0 + exit 0 Executing(%install): /bin/sh -e /var/tmp/rpm-tmp.PGhobB + umask 022 + cd /root/rpmbuild/BUILD + cd cray_ca_cert-1.0.0 + install -d /root/rpmbuild/BUILDROOT/cray_ca_cert-1.0.0-1.x86_64/usr/share/pki/trust/anchors + install -m 644 /etc/cray/ca/certificate_authority.crt /root/rpmbuild/BUILDROOT/cray_ca_cert-1.0.0-1.x86_64/usr/share/pki/trust/anchors/cray_certificate_authority.crt + /usr/lib/rpm/brp-compress + /usr/lib/rpm/brp-strip /usr/bin/strip + /usr/lib/rpm/brp-strip-static-archive /usr/bin/strip find: file: No such file or directory + /usr/lib/rpm/brp-strip-comment-note /usr/bin/strip /usr/bin/objdump Processing files: cray_ca_cert-1.0.0-1.x86_64 Provides: cray_ca_cert = 1.0.0-1 cray_ca_cert(x86-64) = 1.0.0-1 Requires(interp): /bin/sh Requires(rpmlib): rpmlib(CompressedFileNames) \u0026lt;= 3.0.4-1 rpmlib(PayloadFilesHavePrefix) \u0026lt;= 4.0-1 Requires(post): /bin/sh Checking for unpackaged file(s): /usr/lib/rpm/check-files /root/rpmbuild/BUILDROOT/cray_ca_cert-1.0.0-1.x86_64 Wrote: /root/rpmbuild/RPMS/x86_64/cray_ca_cert-1.0.0-1.x86_64.rpm Executing(%clean): /bin/sh -e /var/tmp/rpm-tmp.jHaFMC + umask 022 + cd /root/rpmbuild/BUILD + cd cray_ca_cert-1.0.0 + exit 0 The build-image container builds the recipe using the Kiwi-NG tool.\nncn# kubectl -n ims logs -f $POD -c build-image Example output:\n+ RECIPE_ROOT_PARENT=/mnt/recipe + IMAGE_ROOT_PARENT=/mnt/image + PARAMETER_FILE_BUILD_FAILED=/mnt/image/build_failed + PARAMETER_FILE_KIWI_LOGFILE=/mnt/image/kiwi.log [...] + kiwi-ng --logfile=/mnt/image/kiwi.log --type tbz system build --description /mnt/recipe --target /mnt/image [ INFO ]: 16:14:31 | Loading XML description [ INFO ]: 16:14:31 | --\u0026gt; loaded /mnt/recipe/config.xml [ INFO ]: 16:14:31 | --\u0026gt; Selected build type: tbz [ INFO ]: 16:14:31 | Preparing new root system [ INFO ]: 16:14:31 | Setup root directory: /mnt/image/build/image-root [ INFO ]: 16:14:31 | Setting up repository http://api-gw-service-nmn.local/repositories/sle15-Module-Basesystem/ [ INFO ]: 16:14:31 | --\u0026gt; Type: rpm-md [ INFO ]: 16:14:31 | --\u0026gt; Translated: http://api-gw-service-nmn.local/repositories/sle15-Module-Basesystem/ [ INFO ]: 16:14:31 | --\u0026gt; Alias: SLES15_Module_Basesystem [ INFO ]: 16:14:32 | Setting up repository http://api-gw-service-nmn.local/repositories/sle15-Product-SLES/ [ INFO ]: 16:14:32 | --\u0026gt; Type: rpm-md [ INFO ]: 16:14:32 | --\u0026gt; Translated: http://api-gw-service-nmn.local/repositories/sle15-Product-SLES/ [ INFO ]: 16:14:32 | --\u0026gt; Alias: SLES15_Product_SLES [ INFO ]: 16:14:32 | Setting up repository http://api-gw-service-nmn.local/repositories/cray-sle15 [ INFO ]: 16:14:32 | --\u0026gt; Type: rpm-md [ INFO ]: 16:14:32 | --\u0026gt; Translated: http://api-gw-service-nmn.local/repositories/cray-sle15 [ INFO ]: 16:14:32 | --\u0026gt; Alias: DST_built_rpms [...] [ INFO ]: 16:19:19 | Calling images.sh script [ INFO ]: 16:19:55 | Creating system image [ INFO ]: 16:19:55 | Creating XZ compressed tar archive [ INFO ]: 16:21:31 | --\u0026gt; Creating archive checksum [ INFO ]: 16:21:51 | Export rpm packages metadata [ INFO ]: 16:21:51 | Export rpm verification metadata [ INFO ]: 16:22:09 | Result files: [ INFO ]: 16:22:09 | --\u0026gt; image_packages: /mnt/image/cray-sles15-barebones.x86_64-1.0.1.packages [ INFO ]: 16:22:09 | --\u0026gt; image_verified: /mnt/image/cray-sles15-barebones.x86_64-1.0.1.verified [ INFO ]: 16:22:09 | --\u0026gt; root_archive: /mnt/image/cray-sles15-barebones.x86_64-1.0.1.tar.xz [ INFO ]: 16:22:09 | --\u0026gt; root_archive_md5: /mnt/image/cray-sles15-barebones.x86_64-1.0.1.md5 + rc=0 + \u0026#39;[\u0026#39; 0 -ne 0 \u0026#39;]\u0026#39; + exit 0 The buildenv-sidecar container determines if the Kiwi-NG build was successful or not.\nIf the Kiwi-NG build completed successfully, the image root, kernel, and initrd artifacts are uploaded to the artifact repository. If the Kiwi-NG build failed to complete successfully, an optional SSH debug shell is enabled so the image build can be debugged. ncn# kubectl -n ims logs -f $POD -c buildenv-sidecar Example output:\nNot running user shell for successful create action Copying SMS CA Public Certificate to target image root + IMAGE_ROOT_PARENT=/mnt/image + IMAGE_ROOT_DIR=/mnt/image/build/image-root + KERNEL_FILENAME=vmlinuz + INITRD_FILENAME=initrd + IMAGE_ROOT_ARCHIVE_NAME=sles15_barebones_image + echo Copying SMS CA Public Certificate to target image root + mkdir -p /mnt/image/build/image-root/etc/cray + cp -r /etc/cray/ca /mnt/image/build/image-root/etc/cray/ + mksquashfs /mnt/image/build/image-root /mnt/image/sles15_barebones_image.sqsh Parallel mksquashfs: Using 4 processors Creating 4.0 filesystem on /mnt/image/sles15_barebones_image.sqsh, block size 131072. [===========================================================\\] 26886/26886 100% Exportable Squashfs 4.0 filesystem, gzip compressed, data block size 131072 compressed data, compressed metadata, compressed fragments, compressed xattrs [...] + python -m ims_python_helper image upload_artifacts sles15_barebones_image 7de80ccc-1e7d-43a9-a6e4-02cad10bb60b \\ -v -r /mnt/image/sles15_barebones_image.sqsh -k /mnt/image/image-root/boot/vmlinuz -i /mnt/image/image-root/boot/initrd { \u0026#34;ims_image_artifacts\u0026#34;: [ { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;4add976679c7e955c4b16d7e2cfa114e-32\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/rootfs\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;94165af4373e5ace3e817eb4baba2284\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.rootfs.squashfs\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;f836412241aae79d160556ed6a4eb4d4\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/kernel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;f836412241aae79d160556ed6a4eb4d4\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.kernel\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;ec8793c07f94e59a2a30abdb1bd3d35a-4\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/initrd\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;86832ee3977ca0515592e5d00271d2fe\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.initrd\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;13af343f3e76b0f8c7fbef7ee3588ac1\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;13af343f3e76b0f8c7fbef7ee3588ac1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/json\u0026#34; } ], \u0026#34;ims_image_record\u0026#34;: { \u0026#34;created\u0026#34;: \u0026#34;2018-12-17T22:59:43.264129+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;d88521c3-b339-43bc-afda-afdfda126388\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;sles15_barebones_image\u0026#34; \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;13af343f3e76b0f8c7fbef7ee3588ac1\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, }, \u0026#34;ims_job_record\u0026#34;: { \u0026#34;artifact_id\u0026#34;: \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34;, \u0026#34;build_env_size\u0026#34;: 10, \u0026#34;created\u0026#34;: \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34;, \u0026#34;enable_debug\u0026#34;: false, \u0026#34;id\u0026#34;: \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34;, \u0026#34;image_root_archive_name\u0026#34;: \u0026#34;sles15_barebones_image\u0026#34;, \u0026#34;initrd_file_name\u0026#34;: \u0026#34;initrd\u0026#34;, \u0026#34;job_type\u0026#34;: \u0026#34;create\u0026#34;, \u0026#34;kernel_file_name\u0026#34;: \u0026#34;vmlinuz\u0026#34;, \u0026#34;kubernetes_configmap\u0026#34;: \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34;, \u0026#34;kubernetes_job\u0026#34;: \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-create\u0026#34;, \u0026#34;kubernetes_service\u0026#34;: \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34;, \u0026#34;public_key_id\u0026#34;: \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34;, \u0026#34;resultant_image_id\u0026#34;: \u0026#34;d88521c3-b339-43bc-afda-afdfda126388\u0026#34;, \u0026#34;ssh_port\u0026#34;: 0, \u0026#34;status\u0026#34;: \u0026#34;creating\u0026#34; }, \u0026#34;result\u0026#34;: \u0026#34;success\u0026#34; } [...] IMPORTANT: The IMS image creation workflow automatically copies the NCN Certificate Authority\u0026rsquo;s public certificate to /etc/cray/ca/certificate_authority.crt within the image root being built. This can be used to enable secure communications between the NCN and the client node.\nIf the image creation operation fails, the build artifacts will not be uploaded to S3. If enable_debug is set to true, then the IMS creation job will enable a debug SSH shell that is accessible by one or more dynamic host names. The user needs to know if they will SSH from inside or outside the Kubernetes cluster to determine which host name to use. Typically, customers access the system from outside the Kubernetes cluster using the Customer Access Network (CAN).\nIf no errors are observed, skip to the 1. Verify that the new image was created correctly step.\nOtherwise, proceed to the following step to debug the failure.\nUse the IMS_JOB_ID to look up the ID of the newly created image.\nThere may be multiple records returned. Ensure that the correct record is selected in the returned data.\nncn# cray ims jobs describe $IMS_JOB_ID Example output:\nstatus = \u0026#34;waiting_on_user\u0026#34; enable_debug = false kernel_file_name = \u0026#34;vmlinuz\u0026#34; artifact_id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; build_env_size = 10 job_type = \u0026#34;create\u0026#34; kubernetes_service = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34; kubernetes_job = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-create\u0026#34; id = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34; image_root_archive_name = \u0026#34;my_customized_image\u0026#34; initrd_file_name = \u0026#34;initrd\u0026#34; created = \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34; kubernetes_namespace = \u0026#34;ims\u0026#34; public_key_id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; kubernetes_configmap = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34; [[ssh_containers]] status = \u0026#34;pending\u0026#34; jail = false name = \u0026#34;debug\u0026#34; [ssh_containers.connection_info.\u0026#34;cluster.local\u0026#34;] host = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service.ims.svc.cluster.local\u0026#34; port = 22 [ssh_containers.connection_info.customer_access] host = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7.ims.cmn.shasta.cray.com\u0026#34; \u0026lt;\u0026lt;-- Note this host port = 22 \u0026lt;\u0026lt;-- Note this port If successful, create variables for the SSH connection information.\nncn# IMS_SSH_HOST=ad5163d2-398d-4e93-94f0-2f439f114fe7.ims.cmn.shasta.cray.com ncn# IMS_SSH_PORT=22 Connect to the IMS debug shell.\nTo access the debug shell, SSH to the container using the private key that matches the public key used to create the IMS job.\nIMPORTANT: The following command will not work when run on a node within the Kubernetes cluster.\nncn# ssh -p IMS_SSH_PORT root@IMS_SSH_HOST Example output:\nLast login: Tue Sep 4 18:06:27 2018 from gateway [root@POD ~]# Investigate using the IMS debug shell.\nChange to the /mnt/image/ directory.\n[root@POD image]# cd /mnt/image/ Access the image root.\n[root@POD image]# chroot image-root/ Investigate inside the image debug shell.\nExit the image root.\n:/ # exit [root@POD image]# Touch the complete file once investigations are complete.\n[root@POD image]# touch /mount/image/complete Verify that the new image was created correctly.\nncn# cray ims jobs describe $IMS_JOB_ID Example output:\nstatus = \u0026#34;success\u0026#34; enable_debug = false kernel_file_name = \u0026#34;vmlinuz\u0026#34; artifact_id = \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34; build_env_size = 10 job_type = \u0026#34;create\u0026#34; kubernetes_service = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34; kubernetes_job = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-customize\u0026#34; id = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34; image_root_archive_name = \u0026#34;sles15_barebones_image\u0026#34; resultant_image_id = d88521c3-b339-43bc-afda-afdfda126388 initrd_file_name = \u0026#34;initrd\u0026#34; created = \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34; kubernetes_namespace = \u0026#34;ims\u0026#34; public_key_id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; kubernetes_configmap = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34; If successful, create a variable for the IMS resultant_image_id.\nncn# IMS_RESULTANT_IMAGE_ID=d88521c3-b339-43bc-afda-afdfda126388 Verify that the new IMS image record exists.\nncn# cray ims images describe $IMS_RESULTANT_IMAGE_ID Example output:\ncreated = \u0026#34;2018-12-17T22:59:43.264129+00:00\u0026#34; id = \u0026#34;d88521c3-b339-43bc-afda-afdfda126388\u0026#34; name = \u0026#34;sles15_barebones_image\u0026#34; [link] path = \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/manifest.json\u0026#34; etag = \u0026#34;180883770442235de747e9d69855f269\u0026#34; type = \u0026#34;s3\u0026#34; Clean up the creation environment Delete the IMS job record.\nncn# cray ims jobs delete $IMS_JOB_ID Deleting the job record will delete the underlying Kubernetes job, service, and ConfigMap that were created when the job record was submitted.\nImages built by IMS contain only the packages and settings that are referenced in the Kiwi-NG recipe used to build the image. The only exception is that IMS will dynamically install the system\u0026rsquo;s root CA certificate to allow Zypper (via Kiwi-NG) to talk securely with the required Nexus RPM repositories. Images that are intended to be used to boot a CN or other node must be configured with DNS and other settings that enable the image to talk to vital services. A base level of customization is provided by the default Ansible plays used by the Configuration Framework Service (CFS) to enable DNS resolution; these plays are typically run against an image after it is built by IMS.\nWhen customizing an image via Customize an Image Root Using IMS, once in the image root using chroot (or if using a `jailed` environment), the image will only have access to whatever configuration the image already contains. In order to talk to services, including Nexus RPM repositories, the image root must first be configured with DNS and other settings. That base level of customization is provided by the default Ansible plays used by CFS to enable DNS resolution.\n"
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/add_an_ncn_to_the_hsm_database/",
	"title": "Add an NCN to the HSM Database",
	"tags": [],
	"description": "",
	"content": "Add an NCN to the HSM Database This procedure details how to customize the bare-metal non-compute node (NCN) on a system and add the NCN to the Hardware State Manager (HSM) database.\nThe examples in this procedure use ncn-w0003-nmn as the Customer Access Node (CAN). Use the correct CAN for the system.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. The initial CSM software installation is complete. Keycloak authentication is complete. See Configure Keycloak Account. Procedure Locate the component name (xname) of the NCN.\nThe xname is located in the /etc/hosts file. This example shows the xname for ncn-w003.\nncn-mw# grep ncn-w003-nmn /etc/hosts Example output:\n10.252.1.15 ncn-w003.local ncn-w003 ncn-w003-nmn ncn-w003-nmn.local sms03-nmn x3000c0s24b0n0 #-label-10.252.1.15 Define the get_token helper function.\nncn-mw# function get_token () { curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39; } Create an entry with the following keypairs.\nThe get_token function adds the authorization required by the HTTPS security token. The -H options tell the REST API to accept the data as JSON and that the information is for a JSON-enabled application.\nBe sure to modify the example below to use the xname identified in the previous step.\nncn-mw# curl -X POST -k https://api-gw-service-nmn.local/apis/smd/hsm/v2/State/Components \\ -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; -H \u0026#34;accept: application/json\u0026#34; -H \\ \u0026#34;Content-Type: application/json\u0026#34; -d \\ \u0026#39;{\u0026#34;Components\u0026#34;:[{\u0026#34;ID\u0026#34;:\u0026#34;x3000c0s24b0\u0026#34;,\u0026#34;State\u0026#34;:\u0026#34;On\u0026#34;,\u0026#34;NetType\u0026#34;:\u0026#34;Sling\u0026#34;,\u0026#34;Arch\u0026#34;:\u0026#34;X86\u0026#34;,\u0026#34;Role\u0026#34;:\u0026#34;Management\u0026#34;}]}\u0026#39; List HSM state components and verify that information is correct.\nBe sure to modify the example below to use the xname identified in the previous step.\nncn-mw# cray hsm state components list --id x3000c0s24b0 --format toml Example output:\n[[Components]] Arch = \u0026#34;X86\u0026#34; Enabled = true Flag = \u0026#34;OK\u0026#34; State = \u0026#34;On\u0026#34; Role = \u0026#34;Management\u0026#34; NetType = \u0026#34;Sling\u0026#34; Type = \u0026#34;NodeBMC\u0026#34; ID = \u0026#34;x3000c0s24b0\u0026#34; Find the daemonset pod that is running on the NCN being added to the HSM database.\nncn-mw# kubectl get pods -l app.kubernetes.io/instance=ncn-customization -n services -o wide Example output:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ncn-customization-cray-service-4tqcg 2/2 Running 2 4d2h 10.47.0.3 ncn-m001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ncn-customization-cray-service-dh8gb 2/2 Running 1 4d2h 10.42.0.4 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ncn-customization-cray-service-gwxc2 2/2 Running 2 4d2h 10.40.0.8 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ncn-customization-cray-service-rjms5 2/2 Running 2 4d2h 10.35.0.3 ncn-w004 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ncn-customization-cray-service-wgl44 2/2 Running 2 4d2h 10.39.0.3 ncn-w005 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Delete the daemonset pod identified in the previous step.\nDeleting the pod will restart it and enable the changes to be picked up.\nBe sure to modify the example below to use the pod name identified in the previous step.\nncn-mw# kubectl -n services delete pod ncn-customization-cray-service-dh8gb Verify the daemonset restarts on the NCN with the CAN configuration.\nRetrieve the new pod name.\nBe sure to modify the example below to use the name of the NCN being added.\nncn-mw# kubectl get pods -l app.kubernetes.io/instance=ncn-customization -n services -o wide | grep ncn-w003 Example output:\nncn-customization-cray-service-dh8gb 2/2 Running 2 22d 10.36.0.119 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Wait for the daemonset pod to cycle through the unload session.\nThis may take up to 5 minutes.\nncn-mw# cray cfs sessions list --format toml | grep \u0026#34;name =\u0026#34; Example output:\nname = \u0026#34;ncn-customization-ncn-w003-unload\u0026#34; Wait for the daemonset pod to cycle through the load session.\nThis may take up to 5 minutes.\nncn-mw# cray cfs sessions list --format toml | grep \u0026#34;name =\u0026#34; Example output:\nname = \u0026#34;ncn-customization-ncn-w003-load\u0026#34; Once the load job completes, if there are no errors returned, the session is removed.\nRunning cray cfs sessions list --format toml | grep \u0026quot;name =\u0026quot; again should return with no sessions active. If Ansible errors were encountered during the unload or load sessions, then the dormant CFS session artifacts remain for CFS Ansible failure troubleshooting.\n"
},
{
	"uri": "/docs-csm/en-12/operations/firmware/fas_cli/",
	"title": "FAS CLI",
	"tags": [],
	"description": "",
	"content": "FAS CLI This section describes the basic capabilities of the Firmware Action Service (FAS) CLI commands. These commands can be used to manage firmware for system hardware supported by FAS. Refer to the prerequisites section before proceeding to any of the sections for the supported operations.\nThe following CLI operations are described:\nPrerequisites Actions Execute an action Procedure Abort an action Procedure Describe an action Interpreting output Procedure Get high level summary Get details of action Get details of operation Snapshots Create a snapshot Procedure List snapshots Procedure View snapshots Procedure Update a firmware image Procedure FAS loader commands Loader status Load firmware from Nexus Load individual RPM or ZIP into FAS Display results of loader run Delete loader run data Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI.\nActions Execute an action Use FAS to execute an action. An action produces a set of firmware operations. Each operation represents a component name (xname) + target on that component name (xname) that will be targeted for update. There are two of firmware action modes: : dryrun or liveupdate; the parameters used when creating either are completely identical except for the overrideDryrun setting. overrideDryrun will determine if feature to determine what firmware can be updated on the system. Dry-runs are enabled by default, and can be configured with the overrideDryrun parameter. A dry-run will create a query according to the filters requested by the admin. It will initiate an update sequence to determine what firmware is available, but will not actually change the state of the firmware.\nWARNING: It is crucial that an administrator is familiar with the release notes of any firmware. The release notes will indicate what new features the firmware provides and if there are any incompatibilities. FAS does not know about incompatibilities or dependencies between versions. The administrator assumes full responsibility for this knowledge. It is also likely that when performing a firmware update, the current version of firmware will not be available. This means that after successfully upgrading, the firmware cannot be reverted or downgraded to a previous version.\nExecute an action: Procedure This covers the generic process for executing an action. For more specific examples and detailed explanations of options, see FAS Recipes and FAS Filters.\nIdentify the selection of filters to apply.\nFilters narrow the scope of FAS to target specific component names (xnames), manufacturers, targets, and so on. For this example, FAS will run with no selection filters applied.\nCreate a JSON file.\nTo make this a live update set \u0026quot;overrideDryrun\u0026quot;: true.\n{ \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;dryrun of full system\u0026#34; } } Execute the dry-run.\nModify the example command to specify the JSON file created in the previous step.\nncn-mw# cray fas actions create filename.json --format json Example output:\n{ \u0026#34;actionID\u0026#34;: \u0026#34;e0cdd7c2-32b1-4a25-9b2a-8e74217eafa7\u0026#34;, \u0026#34;overrideDryun\u0026#34;: false } Note the returned actionID.\nSee Describe an action: Interpreting output for more information.\nAbort an action Firmware updates can be stopped if required. This is useful because only one action can be run at a time. This is to protect hardware from multiple actions trying to modify it at the same time.\nIMPORTANT: If a Redfish update is already in progress, the abort will not stop that process on the device. It is likely the device will update. If the device needs to be manually power cycled (needManualReboot), then it is possible that the device will update, but not actually apply the update until its next reboot. Administrators must verify the state of the system after an abort. Only perform an abort if truly necessary. The best way to check the state of the system is to do a snapshot or do a dry-run of an update.\nAbort an action: Procedure Issue the abort command to the action.\nModify the example command to specify the actionID of the action being aborted.\nncn-mw# cray fas actions instance delete {actionID} The action could take up to a minute to fully abort.\nDescribe an action There are several ways to get more information about a firmware update. An actionID and operationIDs are generated when a live update or dry-run is created. These values can be used to learn more about what is happening on the system during an update.\nDescribe an action: Interpreting output For the steps below, the following returned messages will help determine if a firmware update is needed. The following are end states for operations. The Firmware action itself should be in completed once all operations have finished.\nNoOp: Nothing to do, already at version. NoSol: No image is available. succeeded: If dryrun: The operation should succeed if performed as a live update. succeeded means that FAS identified that it COULD update a component name (xname) + target with the declared strategy. If live update: The operation succeeded, and has updated the component name (xname) + target to the identified version. failed: If dryrun: There is something that FAS could do, but it likely would fail; most likely because the file is missing. If live update: The operation failed; the identified version could not be put on the component name (xname) + target. NOTE: Any node which is locked will remain in the state inProgress with the stateHelper message of \u0026quot;failed to lock\u0026quot; until the action times out, or the lock is released. These nodes will report as failed with the stateHelper message of \u0026quot;time expired; could not complete update\u0026quot; if action times out. This includes NCNs which are manually locked to prevent accidental rebooting and firmware updates.\nData can be viewed at several levels of information:\nDescribe an action: Procedure Get high level summary To view counts of operations, what state they are in, the overall state of the action, and what parameters were used to create the action:\nModify the following command to specify the actual actionID of the action to be examined.\nncn-mw# cray fas actions status list {actionID} --format toml Example output:\nactionID = \u0026#34;e6dc14cd-5e12-4d36-a97b-0dd372b0930f\u0026#34; snapshotID = \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; startTime = \u0026#34;2021-09-07 16:43:04.294233199 +0000 UTC\u0026#34; endTime = \u0026#34;2021-09-07 16:53:09.363233482 +0000 UTC\u0026#34; state = \u0026#34;completed\u0026#34; blockedBy = [] [command] overrideDryrun = false restoreNotPossibleOverride = true overwriteSameImage = false timeLimit = 2000 version = \u0026#34;latest\u0026#34; tag = \u0026#34;default\u0026#34; description = \u0026#34;Dryrun upgrade of Gigabyte node BMCs\u0026#34; [operationCounts] total = 14 initial = 0 configured = 0 blocked = 0 needsVerified = 0 verifying = 0 inProgress = 0 failed = 0 succeeded = 8 noOperation = 6 noSolution = 0 aborted = 0 unknown = 0 IMPORTANT: The action is still in progress unless the action\u0026rsquo;s state is completed or aborted.\nGet details of action Modify the following command to specify the actual actionID of the action to be examined.\nncn-mw# cray fas actions describe {actionID} --format json Example output:\n{ \u0026#34;parameters\u0026#34;: { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;dryrun\u0026#34;: false, \u0026#34;description\u0026#34;: \u0026#34;upgrade of nodeBMCs for cray\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;imageFilter\u0026#34;: { \u0026#34;imageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] } }, \u0026#34;blockedBy\u0026#34;: [], \u0026#34;state\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;command\u0026#34;: { \u0026#34;dryrun\u0026#34;: false, \u0026#34;description\u0026#34;: \u0026#34;upgrade of nodeBMCs for cray\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;actionID\u0026#34;: \u0026#34;e0cdd7c2-32b1-4a25-9b2a-8e74217eafa7\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-06-26 20:03:37.316932354 +0000 UTC\u0026#34;, \u0026#34;snapshotID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2020-06-26 20:04:07.118243184 +0000 UTC\u0026#34;, \u0026#34;operationSummary\u0026#34;: { \u0026#34;succeeded\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;verifying\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;unknown\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;configured\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;initial\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;failed\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [ { \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x5000c1r7b0\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;0796eed0-e95d-45ea-bc71-8903d52cffde\u0026#34; }, ] }, \u0026#34;noSolution\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;aborted\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;needsVerified\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;noOperation\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;inProgress\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;blocked\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] } } } Get details of operation Using the operationID listed in the actions array, see the full detail of the operation.\nModify the following command to specify the actual operationID of the operation to be examined.\nncn-mw# cray fas operations describe {operationID} --format json Example output:\n{ \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fromTag\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fromImageURL\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2020-06-24 14:23:37.544814197 +0000 UTC\u0026#34;, \u0026#34;actionID\u0026#34;: \u0026#34;f48aabf1-1616-49ae-9761-a11edb38684d\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-06-24 14:19:15.10128214 +0000 UTC\u0026#34;, \u0026#34;fromSemanticFirmwareVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;toImageURL\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;WindomNodeCard_REV_D\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;24a5e5fb-5c4f-4848-bf4e-b071719c1850\u0026#34;, \u0026#34;fromImageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;toImageID\u0026#34;: \u0026#34;71c41a74-ab84-45b2-95bd-677f763af168\u0026#34;, \u0026#34;toSemanticFirmwareVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refreshTime\u0026#34;: \u0026#34;2020-06-24 14:23:37.544824938 +0000 UTC\u0026#34;, \u0026#34;blockedBy\u0026#34;: [], \u0026#34;toTag\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected nc.1.3.8-shasta-release.arm.2020-06-15T22:57:31+00:00.b7f0725 got: nc.1.2.25-shasta-release.arm.2020-05-15T17:27:16+00:00.0cf7f51\u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;expirationTime\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x9000c1s3b1\u0026#34;, \u0026#34;toFirmwareVersion\u0026#34;: \u0026#34;\u0026#34; } Snapshots FAS includes a snapshot feature to record the firmware value for each device (type and target) on the system into the FAS database.\nCreate a snapshot Similar to the FAS actions described above, FAS provides a lot of flexibility for taking snapshots.\nA snapshot of the system captures the firmware version for every device that is in the Hardware State Manager (HSM) Redfish Inventory.\nCreate a snapshot: Procedure Determine the desired snapshot level.\nCreate a JSON file based on the desired level.\nFull system\n{ \u0026#34;name\u0026#34;:\u0026#34;fullSystem_20200701\u0026#34; } Partial system\n{ \u0026#34;name\u0026#34;: \u0026#34;20200402_all_xnames\u0026#34;, \u0026#34;expirationTime\u0026#34;: \u0026#34;2020-06-26T16:32:53.275Z\u0026#34;, \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;partitions\u0026#34;: [ \u0026#34;p1\u0026#34; ], \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;gigabyte\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] } } Create the snapshot.\nModify the example command to specify the JSON file created in the previous step.\nncn-mw# cray fas snapshots create {file.json} Use the snapshot name to query the snapshot. This is a long-running operation, so monitor the state field to determine if the snapshot is complete.\nList snapshots A list of all snapshots can be viewed on the system. Any of the snapshots listed can be used to restore the firmware on the system.\nList snapshots: Procedure List the snapshots.\nncn-mw# cray fas snapshots list --format json Example output:\n{ \u0026#34;snapshots\u0026#34;: [ { \u0026#34;ready\u0026#34;: true, \u0026#34;captureTime\u0026#34;: \u0026#34;2020-06-25 22:47:11.072268274 +0000 UTC\u0026#34;, \u0026#34;relatedActions\u0026#34;: [], \u0026#34;name\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;uniqueDeviceCount\u0026#34;: 9 }, { \u0026#34;ready\u0026#34;: true, \u0026#34;captureTime\u0026#34;: \u0026#34;2020-06-25 22:49:13.314876084 +0000 UTC\u0026#34;, \u0026#34;relatedActions\u0026#34;: [], \u0026#34;name\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;uniqueDeviceCount\u0026#34;: 9 }, { \u0026#34;ready\u0026#34;: true, \u0026#34;captureTime\u0026#34;: \u0026#34;2020-06-26 22:38:12.309979483 +0000 UTC\u0026#34;, \u0026#34;relatedActions\u0026#34;: [], \u0026#34;name\u0026#34;: \u0026#34;adn0\u0026#34;, \u0026#34;uniqueDeviceCount\u0026#34;: 6 } ] } View snapshots View a snapshot to see which versions of firmware are set for each target.\nView snapshots: Procedure View a snapshot.\nModify the following command to specify the actual name of the snapshot to be examined.\nncn-mw# cray fas snapshots describe {snapshot_name} --format json Example output:\n{ \u0026#34;relatedActions\u0026#34;: [], \u0026#34;name\u0026#34;: \u0026#34;all\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;stateComponentFilter\u0026#34;: {}, \u0026#34;targetFilter\u0026#34;: {}, \u0026#34;name\u0026#34;: \u0026#34;all\u0026#34;, \u0026#34;inventoryHardwareFilter\u0026#34;: {} }, \u0026#34;ready\u0026#34;: true, \u0026#34;captureTime\u0026#34;: \u0026#34;2020-06-26 19:13:53.755350771 +0000 UTC\u0026#34;, \u0026#34;devices\u0026#34;: [ { \u0026#34;xname\u0026#34;: \u0026#34;x3000c0s19b4\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;BIOS\u0026#34;, \u0026#34;firmwareVersion\u0026#34;: \u0026#34;C12\u0026#34;, \u0026#34;imageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;firmwareVersion\u0026#34;: \u0026#34;12.03.3\u0026#34;, \u0026#34;imageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; } ] }, { \u0026#34;xname\u0026#34;: \u0026#34;x3000c0s1b0\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;BPB_CPLD1\u0026#34;, \u0026#34;firmwareVersion\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;imageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;firmwareVersion\u0026#34;: \u0026#34;12.03.3\u0026#34;, \u0026#34;imageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;BIOS\u0026#34;, \u0026#34;firmwareVersion\u0026#34;: \u0026#34;C12\u0026#34;, \u0026#34;imageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;BPB_CPLD2\u0026#34;, \u0026#34;firmwareVersion\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;imageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; } ] } ] } Update a firmware image If FAS indicates that hardware is in a nosolution state as a result of a dry-run or update, it is an indication that there is no matching image available to update firmware. A missing image is highly possible, but the issue could also be that the hardware has inconsistent model names in the image file.\nGiven the nature of the model field and its likelihood to not be standardized, it may be necessary to update the image to include an image that is not currently present.\nUpdate a firmware image: Procedure List the existing firmware images to find the imageID of the desired firmware image.\nncn-mw# cray fas images list Describe the image using the imageID.\nModify the following command to specify the actual imageID of the image to be examined.\nncn-mw# cray fas images describe {imageID} --format json Example output:\n{ \u0026#34;semanticFirmwareVersion\u0026#34;: \u0026#34;0.2.6\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;Node0.BIOS\u0026#34;, \u0026#34;waitTimeBeforeManualRebootSeconds\u0026#34;: 0, \u0026#34;tags\u0026#34;: [ \u0026#34;default\u0026#34; ], \u0026#34;models\u0026#34;: [ \u0026#34;GrizzlyPeak-Rome\u0026#34; ], \u0026#34;updateURI\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;waitTimeAfterRebootSeconds\u0026#34;: 0, \u0026#34;imageID\u0026#34;: \u0026#34;efa4c2bc-06b9-4e88-8098-8d6778c1db52\u0026#34;, \u0026#34;s3URL\u0026#34;: \u0026#34;s3:/fw-update/794c47d1b7e011ea8d20569839947aa5/gprnc.bios-0.2.6.tar.gz\u0026#34;, \u0026#34;forceResetType\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;nodeBMC\u0026#34;, \u0026#34;pollingSpeedSeconds\u0026#34;: 30, \u0026#34;createTime\u0026#34;: \u0026#34;2020-06-26T19:08:52Z\u0026#34;, \u0026#34;firmwareVersion\u0026#34;: \u0026#34;gprnc.bios-0.2.6\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; } Describe the FAS action and compare it to the image from the previous step.\nLook at the hardware models to see if some of the population is in a noSolution state, while others are in a succeeded state. If that is the case, then view the operation data and examine the models.\nModify the following command to specify the actual actionID of the action to be examined.\nncn-mw# cray fas actions describe {actionID} --format json Example output:\n\u0026#34;parameters\u0026#34;: { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;dryrun\u0026#34;: false, \u0026#34;description\u0026#34;: \u0026#34;upgrade of nodeBMCs for cray\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;imageFilter\u0026#34;: { \u0026#34;imageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] } }, \u0026#34;blockedBy\u0026#34;: [], \u0026#34;state\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;command\u0026#34;: { \u0026#34;dryrun\u0026#34;: false, \u0026#34;description\u0026#34;: \u0026#34;upgrade of nodeBMCs for cray\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;actionID\u0026#34;: \u0026#34;e0cdd7c2-32b1-4a25-9b2a-8e74217eafa7\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-06-26 20:03:37.316932354 +0000 UTC\u0026#34;, \u0026#34;snapshotID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2020-06-26 20:04:07.118243184 +0000 UTC\u0026#34;, \u0026#34;operationSummary\u0026#34;: { \u0026#34;succeeded\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;verifying\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;unknown\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;configured\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;initial\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;failed\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [ { \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x5000c1r7b0\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;0796eed0-e95d-45ea-bc71-8903d52cffde\u0026#34; }, { \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x5000c3r7b0\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;11421f0b-1fde-4917-ba56-c42b321fc833\u0026#34; }, { \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x5000c3r3b0\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;21e04403-f89f-4a9f-9fd6-5affc9204689\u0026#34; }, { \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x5000c1r5b0\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;3a13a459-2102-4ee5-b516-62880baa132d\u0026#34; }, { \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x5000c1r1b0\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;80fafbdd-9bac-407d-b28a-ad47c197bbc1\u0026#34; }, { \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x5000c3r5b0\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;a86e8e04-81cc-40ad-ac62-438ae73e033a\u0026#34; }, { \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x5000c1r3b0\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;dd0e8b62-8894-4751-bd22-a45506a2a50a\u0026#34; }, { \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x5000c3r1b0\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;f87bff63-d231-403e-b6b6-fc09e4dc7d11\u0026#34; } ] }, \u0026#34;noSolution\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;aborted\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;needsVerified\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;noOperation\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;inProgress\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;blocked\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] } } } View the operation data.\nIf the model name is different between identical hardware, it may be appropriate to update the image model with the model of the noSolution hardware.\nModify the following command to specify the actual operationID of the operation to be examined.\nncn-mw# cray fas operations describe {operationID} --format json Example output:\n{ \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;fromTag\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fromImageURL\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2020-06-26 20:15:38.535719717 +0000 UTC\u0026#34;, \u0026#34;actionID\u0026#34;: \u0026#34;e0cdd7c2-32b1-4a25-9b2a-8e74217eafa7\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-06-26 20:03:39.44911099 +0000 UTC\u0026#34;, \u0026#34;fromSemanticFirmwareVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;toImageURL\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;ColoradoSwitchBoard_REV_A\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;f87bff63-d231-403e-b6b6-fc09e4dc7d11\u0026#34;, \u0026#34;fromImageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;toImageID\u0026#34;: \u0026#34;1540ce48-91db-4bbf-a0cf-5cf936c30fbc\u0026#34;, \u0026#34;toSemanticFirmwareVersion\u0026#34;: \u0026#34;1.4.35\u0026#34;, \u0026#34;refreshTime\u0026#34;: \u0026#34;2020-06-26 20:15:38.535722248 +0000 UTC\u0026#34;, \u0026#34;blockedBy\u0026#34;: [], \u0026#34;toTag\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;RouterBMC\u0026#34;, \u0026#34;expirationTime\u0026#34;: \u0026#34;2020-06-26 20:20:19.44911275 +0000 UTC\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x5000c3r1b0\u0026#34;, \u0026#34;toFirmwareVersion\u0026#34;: \u0026#34;sc.1.4.35-prod-master.arm64.2020-06-26T08:36:42+00:00.0c2bb02\u0026#34; } Update the firmware image file.\nThis step should be skipped if there is no clear evidence of a missing image or incorrect model name.\nWARNING: The administrator needs to be certain the firmware is compatible before proceeding.\nDump the content of the firmware image to a JSON file.\nModify the following command to specify the actual imageID of the image to be updated.\nncn-mw# cray fas images describe {imageID} --format json \u0026gt; imagedata.json Edit the new imagedata.json file.\nUpdate any incorrect firmware information, such as the model name.\nUpdate the firmware image.\nModify the following command to specify the actual imageID of the image to be updated, and be sure that the filename matches the edited file from the previous step.\nncn-mw# cray fas images update imagedata.json {imageID} FAS loader commands Loader status To check if the loader is currently busy and receive a list of loader run IDs:\nncn-mw# cray fas loader list --format toml Example output:\nloaderStatus = \u0026#34;ready\u0026#34; [[loaderRunList]] loaderRunID = \u0026#34;770af5a4-15bf-4e9f-9983-03069479dc23\u0026#34; [[loaderRunList]] loaderRunID = \u0026#34;8efb19c4-77a2-41da-9a8f-fccbfe06f674\u0026#34; The loader can only run one job at a time. If the loader is busy, then it will return an error on any attempt to create an additional job.\nLoad firmware from Nexus Firmware may be released and placed into the Nexus repository.\nTo load the firmware from Nexus into FAS, use the following command:\nncn-mw# cray fas loader nexus create --format toml Example output:\nloaderRunID = \u0026#34;c2b7e9bb-f428-4e4c-aa83-d8fd8bcfd820\u0026#34; Use the loaderRunID to check the results of the loader run.\nSee Load Firmware from Nexus.\nLoad individual RPM or ZIP into FAS Copy the RPM or ZIP file to one of the master or worker NCNs.\nLoad the firmware into FAS.\nBe sure to update the example command with the actual path and filename of the RPM or ZIP file to be loaded.\nncn-mw# cray fas loader create --file firmware.rpm --format toml Example output:\nloaderRunID = \u0026#34;dd37dd45-84ec-4bd6-b3c9-7af480048966\u0026#34; Use the loaderRunID to check the results of the loader run.\nSee Load Firmware from RPM or ZIP file.\nDisplay results of loader run Using the loaderRunID returned from the loader upload command, run the following command to get the output from the upload.\nBe sure to update the example command with the actual loaderRunID whose output is to be checked.\nncn-mw# cray fas loader describe dd37dd45-84ec-4bd6-b3c9-7af480048966 --format json Example output:\n{ \u0026#34;loaderRunOutput\u0026#34;: [ \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Starting FW Loader, LOG_LEVEL: INFO; value: 20\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-urls: {\u0026#39;fas\u0026#39;: \u0026#39;http://localhost:28800\u0026#39;, \u0026#39;fwloc\u0026#39;: \u0026#39;file://download/\u0026#39;}\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Using local file: /ilo5_241.zip\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-unzip /ilo5_241.zip\u0026#34;, \u0026#34;Archive: /ilo5_241.zip\u0026#34;, \u0026#34; inflating: ilo5_241.bin\u0026#34;, \u0026#34; inflating: ilo5_241.json\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Processing files from file://download/\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-get_file_list(file://download/)\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Processing File: file://download/ ilo5_241.json\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Uploading b73a48cea82f11eb8c8a0242c0a81003/ilo5_241.bin\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Metadata {\u0026#39;imageData\u0026#39;: \\\u0026#34;{\u0026#39;deviceType\u0026#39;: \u0026#39;nodeBMC\u0026#39;, \u0026#39;manufacturer\u0026#39;: \u0026#39;hpe\u0026#39;, \u0026#39;models\u0026#39;: [\u0026#39;ProLiant XL270d Gen10\u0026#39;, \u0026#39;ProLiant DL325 Gen10\u0026#39;, \u0026#39;ProLiant DL325 Gen10 Plus\u0026#39;, \u0026#39;ProLiant DL385 Gen10\u0026#39;, \u0026#39;ProLiant DL385 Gen10 Plus\u0026#39;, \u0026#39;ProLiant XL645d Gen10 Plus\u0026#39;, \u0026#39;ProLiant XL675d Gen10 Plus\u0026#39;], \u0026#39;targets\u0026#39;: [\u0026#39;iLO 5\u0026#39;], \u0026#39;tags\u0026#39;: [\u0026#39;default\u0026#39;], \u0026#39;firmwareVersion\u0026#39;: \u0026#39;2.41 Mar 08 2021\u0026#39;, \u0026#39;semanticFirmwareVersion\u0026#39;: \u0026#39;2.41.0\u0026#39;, \u0026#39;pollingSpeedSeconds\u0026#39;: 30, \u0026#39;fileName\u0026#39;: \u0026#39;ilo5_241.bin\u0026#39;}\\\u0026#34;}\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-IMAGE: {\\\u0026#34;s3URL\\\u0026#34;: \\\u0026#34;s3:/fw-update/b73a48cea82f11eb8c8a0242c0a81003/ilo5_241.bin\\\u0026#34;, \\\u0026#34;target\\\u0026#34;: \\\u0026#34;iLO 5\\\u0026#34;, \\\u0026#34;deviceType\\\u0026#34;: \\\u0026#34;nodeBMC\\\u0026#34;, \\\u0026#34;manufacturer\\\u0026#34;: \\\u0026#34;hpe\\\u0026#34;, \\\u0026#34;models\\\u0026#34;: [\\\u0026#34;ProLiant XL270d Gen10\\\u0026#34;, \\\u0026#34;ProLiant DL325 Gen10\\\u0026#34;, \\\u0026#34;ProLiant DL325 Gen10 Plus\\\u0026#34;, \\\u0026#34;ProLiant DL385 Gen10\\\u0026#34;, \\\u0026#34;ProLiant DL385 Gen10 Plus\\\u0026#34;, \\\u0026#34;ProLiant XL645d Gen10 Plus\\\u0026#34;, \\\u0026#34;ProLiant XL675d Gen10 Plus\\\u0026#34;], \\\u0026#34;softwareIds\\\u0026#34;: [], \\\u0026#34;tags\\\u0026#34;: [\\\u0026#34;default\\\u0026#34;], \\\u0026#34;firmwareVersion\\\u0026#34;: \\\u0026#34;2.41 Mar 08 2021\\\u0026#34;, \\\u0026#34;semanticFirmwareVersion\\\u0026#34;: \\\u0026#34;2.41.0\\\u0026#34;, \\\u0026#34;allowableDeviceStates\\\u0026#34;: [], \\\u0026#34;needManualReboot\\\u0026#34;: false, \\\u0026#34;pollingSpeedSeconds\\\u0026#34;: 30}\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-Number of Updates: 1\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-Iterate images\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for 5ab9f804a82b11eb8a700242c0a81003/wnc.bios-1.1.2.tar.gz\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for 5ab9f804a82b11eb8a700242c0a81003/wnc.bios-1.1.2.tar.gz\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for 53c060baa82a11eba26c0242c0a81003/controllers-1.3.317.itb\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for b73a48cea82f11eb8c8a0242c0a81003/ilo5_241.bin\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-finished updating images ACL\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-removing local file: /ilo5_241.zip\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-*** Number of Updates: 1 ***\u0026#34; ] } A successful run will end with *** Number of Updates: x ***.\nNOTE The FAS loader will not overwrite image records already in FAS. Number of Updates will be the number of new images found in the RPM. If the number is 0, all images were already in FAS.\nDelete loader run data To delete the output from a loader run and remove it from the loader run list:\nBe sure to update the example command with the actual loaderRunID whose output should be deleted.\nncn-mw# cray fas loader delete dd37dd45-84ec-4bd6-b3c9-7af480048966 The delete command does not return anything if successful.\nNOTE The loader delete command does not delete any images from FAS; it only deletes the loader run saved status and removes the ID from the loader run list.\n"
},
{
	"uri": "/docs-csm/en-12/operations/conman/access_console_log_data_via_the_system_monitoring_framework_smf/",
	"title": "Access Console Log Data Via the System Monitoring Framework (SMF)",
	"tags": [],
	"description": "",
	"content": "Access Console Log Data Via the System Monitoring Framework (SMF) Console log data is collected by SMF and can be queried through the Kibana UI or Elasticsearch. Each line of the console logs are an individual record in the SMF database.\nPrerequisites System domain name Procedure Prerequisites This procedure requires the Kibana service to be up and running on a non-compute node (NCN).\nSystem domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system\u0026rsquo;s fully qualified domain name (FQDN).\nThe FQDN can be found by running the following command on any Kubernetes NCN.\nncn-mw# kubectl get secret site-init -n loftsman -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d | yq r - spec.network.dns.external Example output:\nsystem.hpc.amslabs.hpecorp.net Be sure to modify the example URLs on this page by replacing SYSTEM_DOMAIN_NAME with the actual value found using the above command.\nProcedure Navigate to the following URL in a web browser: https://sma-kibana.cmn.SYSTEM_DOMAIN_NAME/app/kibana\nIf this appears Do this the Keycloak login page Supply valid credentials, then wait to be redirected to the Kibana dashboard before continuing to the next step. the error Kibana did not load properly. Check the server output for more information. Clear browser cookies for https://sma-kibana.cmn.SYSTEM_DOMAIN_NAME The Kibana dashboard (see example below) Proceed to next step When the Kibana dashboard loads, the web UI displays the Discover page by default. Note that even without entering a search pattern, an index pattern shows entries ordered in time.\nSelect the Shasta index for the type of logs desired from the drop-down list to search that data source.\nIdentify the component name (xname) for individual consoles to search for specific logs.\nEach line of the log data is prepended with console.hostname: XNAME where XNAME is the name of the node for the console log. This information can be used to identify each individual console.\nFor example, the following is the console log for x3000c0s19b4n0:\nconsole.hostname: x3000c0s19b4n0 \u0026lt;ConMan\u0026gt; Console [x3000c0s19b4n0] joined by \u0026lt;root@localhost\u0026gt; on pts/0 at 10-09 15:11. console.hostname: x3000c0s19b4n0 2020-10-09 15:11:39 Keepalived_vrrp[38]: bogus VRRP packet received on bond0.nmn0 !!! Enter Search terms for the specific console component name (xname) using the console.hostname: XNAME string.\nClick the time range drop-down menu to select the time period for which logs are displayed.\nUsing a time range for these searches is important to limit the scope and number of records returned, as well as limiting the time required to perform the search.\nThe default time range is 15 minutes.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/ansible_inventory/",
	"title": "Ansible Inventory",
	"tags": [],
	"description": "",
	"content": "Ansible Inventory The Configuration Framework Service (CFS) provides several options for targeting nodes or boot images for configuration by Ansible. The contents of the Ansible inventory determine which nodes are available for configuration in each CFS session and how default configuration values can be customized. For more information on what it means to define an inventory, see Specifying Hosts and Groups.\nThe following are the inventory options provided by CFS:\nDynamic inventory Static inventory Image customization Dynamic inventory and host groups Dynamic inventory is the default inventory when creating a CFS session. CFS automatically generates an Ansible hosts file with data provided by the Hardware State Manager (HSM) when using a dynamic inventory. CFS automatically generates Ansible hosts groups for each group defined in HSM and creates Ansible host groups for nodes based on hardware roles and sub-roles.\nRetrieve a list of HSM groups with the following command:\nncn-mw# cray hsm groups list --format json | jq .[].label These groups can be referenced in Ansible plays or when creating a CFS session directly.\nHardware roles and sub-roles are available as \u0026lt;Role\u0026gt; and \u0026lt;Role\u0026gt;_\u0026lt;Subrole\u0026gt; Ansible host groups. For instance, if targeting just the nodes with the Application role, the host group name is Application. If targeting just the sub-role UAN, which is a sub-role of the Application role, the host group name provided by CFS is Application_UAN. See HSM Roles and Subroles for more information.\nConsult the cray-hms-base-config Kubernetes ConfigMap in the services namespace for a listing of the available roles and sub-roles on the system.\nDuring a CFS session, the dynamic inventory is generated and placed in the hosts/01-cfs-generated.yaml file, relative to the configuration management repository root defined in the current configuration layer. Refer to the external Ansible Inventory documentation for more information about managing inventory, as well as variable precedence within multiple inventory files.\nCFS prefixes its dynamic inventory file with 01- so that its variables can be easily overridden as needed because Ansible reads inventory files in lexicographic order.\nStatic inventory It is also possible for users to bypass HSM and specify their own hosts and groups using a static inventory file. This replaces the dynamic inventory, so each host and group that is going to be targeted must be listed in an inventory file by the user. This approach is useful for testing configuration changes on a small scale.\nCreate a static inventory file in a hosts directory at the root of the configuration management repository in Ansible INI format. For example, create it in a branch and persist the changes.\nIn the following example, this is done for a single node in static inventory:\nncn# mkdir -p hosts; cd hosts; cat \u0026gt; static \u0026lt;\u0026lt;EOF [test_nodes] x3000c0s25b0n0 EOF ncn# cd ..; git add hosts/static ncn# git commit -m \u0026#34;Added a single test node to static inventory\u0026#34; ncn# git push The process can be used to include any nodes in the system reachable over the Node Management Network (NMN), which contains the public SSH key pair provisioned by the install process. This inventory information will only be located in the repository to which it is added. If the desired configuration contains multiple layers, use the additionalInventoryUrl option in CFS to provide inventory information on a per-session level instead of a per-repository level. Refer to Manage Multiple Inventories in a Single Location for more information.\nImage customization Inventory for image customization is also provided by the user. This type of configuration session does not target live nodes, so HSM has no knowledge of either the host or the groups it belongs to. Instead, when creating a configuration session meant to customize a boot image, the Image Management Service (IMS) image IDs are used as hosts and grouped according to user input to the session creation.\nSee Create an Image Customization CFS Session for more information.\n"
},
{
	"uri": "/docs-csm/en-12/operations/compute_rolling_upgrades/compute_rolling_upgrades/",
	"title": "Compute Rolling Upgrades",
	"tags": [],
	"description": "",
	"content": "Compute Rolling Upgrades Note: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality. See Deprecated features.\nThe Compute Rolling Upgrade Service (CRUS) upgrades sets of compute nodes without requiring an entire set of nodes to be out of service at once. CRUS manages the workload management status of nodes, handling each of the following steps required to upgrade compute nodes:\nQuiesce each node before taking the node out of service. Upgrade the node. Reboot the node into the upgraded state. Return the node to service within its respective workload manager. CRUS enables administrators to limit the impact on production caused from upgrading compute nodes by working through one step of the upgrade process at a time. The nodes in each step are first taken out of service in the workload manager to prevent work from being scheduled. They are then upgraded, rebooted, and put back into service in the workload manager.\nCRUS is built upon a few basic features of the system:\nThe grouping of nodes by label provided by the Hardware State Manager (HSM) groups mechanism. Workload management that can gracefully take nodes out of service (quiesce nodes), declare nodes as failed, and return nodes to service. The Boot Orchestration Service (BOS) and boot session templates. "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/boot_issue_symptom_node_console_or_logs_indicate_that_the_server_response_has_timed_out/",
	"title": "Compute Node Boot Issue Symptom Node Console or Logs Indicate that the Server Response has Timed Out",
	"tags": [],
	"description": "",
	"content": "Compute Node Boot Issue Symptom: Node Console or Logs Indicate that the Server Response has Timed Out If the TFTP request is able to access the TFTP service pod but is unable to find its way back to the node, it may be because the kernel is not tracking established TFTP connections.\nSymptoms Problem detection Resolution Symptoms The following image is tcpdump data from within the TFTP pod. It shows what happens when the TFTP request cannot find a route back to the node that sent the request. The node IP address is 10.32.0.1, which is the IP address of the Kubernetes weave network. It is forwarding the node\u0026rsquo;s TFTP to this pod. The server IP address is 10.32.0.13.\nLine 1: The read request from the node arrives. Line 2: The TFTP server attempts to acknowledge the read request. Line 3: ICMP complains about an unreachable destination/port. Lines 4-6: The TFTP server cannot locate the route to the node. It issues an ARP request for 10.32.0.1, but that does not have any effect. Lines 8 and 11: After waiting, the client resends a read request and eventually times out. This repeated request causes a repeat of lines 2-6 as seen in lines 9, 12, and 14-17. Problem detection Check if the nf_nat_tftp kernel module has been loaded. The kernel module is loaded on all ingress points in the Kubernetes cluster, so there will likely be no missing kernel modules.\nResolution Load nf_nat_tftp, if it has not been loaded already.\nncn-m001# modprobe nf_nat_tftp Restart the cray-tftp service.\nncn-m001# kubectl delete pod cray-tftp-885cc65c4-fk8bm Example output:\npod \u0026#34;cray-tftp-885cc65c4-fk8bm\u0026#34; deleted "
},
{
	"uri": "/docs-csm/en-12/operations/artifact_management/generate_temporary_s3_credentials/",
	"title": "Generate Temporary S3 Credentials",
	"tags": [],
	"description": "",
	"content": "Generate Temporary S3 Credentials Cray provides a simple token service (STS) via the API gateway for administrators to generate temporary Simple Storage Service (S3) credentials for use with S3 buckets. Temporary S3 credentials are generated using either cURL or Python.\nThe generated S3 credentials will expire after one hour.\nRetrieve temporary S3 credentials with cURL Retrieve temporary S3 credentials with Python Retrieve temporary S3 credentials with cURL Obtain a JWT token.\nSee Retrieve an Authentication Token for more information.\nGenerate temporary S3 credentials.\nThe following command to call STS assumes that the environment variable $TOKEN contains the JWT.\nncn-mw# curl -X PUT -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; https://api-gw-service-nmn.local/apis/sts/token Example output:\n{ \u0026#34;Credentials\u0026#34;: { \u0026#34;AccessKeyId\u0026#34;: \u0026#34;KtSRFzmAkoDfgCnBLYt\u0026#34;, \u0026#34;EndpointURL\u0026#34;: \u0026#34;http://rgw.local:8080\u0026#34;, \u0026#34;Expiration\u0026#34;: \u0026#34;2019-10-14T15:15:43.480741+00:00\u0026#34;, \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;6CD15EIY6DQOD3DMN0VZPV1XP3W9N4FFPRI0300\u0026#34;, \u0026#34;SessionToken\u0026#34;: \u0026#34;qbwVvv6w1ec/NwI0VzzOXuzFVczjdVICcij0s7kmqKvyZ59RrHJWjLKvmUhGeBATMtkEK72s+qL7Tdn06tPMCQr04MEOpyeUOLmfFyKN3Awm0/7Rlx7rKVaOejpeYaRzO2kWDu3llrpZOONSMPYfck6KjAfvqg/ZJPGEJ5Mzb9YfeSCBq0ghj3G51o9V4DhjjL0YoA/XARMnN0NTHav+OIUHBkXcxZIfT+ti9bSjmz6ExKsJj8zPLvGMK2TIo/Xp\u0026#34; } } Retrieve temporary S3 credentials with Python The following is an example of a Python script that retrieves temporary S3 credentials.\n#!/usr/bin/env python3 # s3creds.py - Generate a temporary S3 token from the Cray Simple Token Service import os import oauthlib.oauth2 import requests_oauthlib realm = \u0026#39;shasta\u0026#39; client_id = \u0026#39;shasta\u0026#39; username = \u0026#39;testuser\u0026#39; # Provide a user here password = os.environ.get(\u0026#39;TESTUSER_PASSWORD\u0026#39;) # Obtain the password from the env, or elsewhere token_url = \u0026#39;https://api-gw-service-nmn.local/keycloak/realms/%s/protocol/openid-connect/token\u0026#39; % realm sts_url = \u0026#39;https://api-gw-service-nmn.local/apis/sts/token\u0026#39; # Create an OAuth2Session and request a token oauth_client = oauthlib.oauth2.LegacyApplicationClient(client_id=client_id) session = requests_oauthlib.OAuth2Session( client=oauth_client, token_updater=lambda t: None, auto_refresh_url=token_url, auto_refresh_kwargs={\u0026#39;client_id\u0026#39;: client_id} ) session.fetch_token( token_url=token_url, client_id=client_id, username=username, password=password ) # Retrieve S3 credentials from STS sts_response = session.put(sts_url) sts_response.raise_for_status() if sts_response.ok: creds = sts_response.json()[\u0026#39;Credentials\u0026#39;] creds_kwargs = { \u0026#39;aws_access_key_id\u0026#39;: creds[\u0026#39;AccessKeyId\u0026#39;], \u0026#39;aws_secret_access_key\u0026#39;: creds[\u0026#39;SecretAccessKey\u0026#39;], \u0026#39;aws_session_token\u0026#39;: creds[\u0026#39;SessionToken\u0026#39;], \u0026#39;endpoint_url\u0026#39;: creds[\u0026#39;EndpointURL\u0026#39;], } The mapping creds_kwargs can now be used for further interaction with S3 in Python.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/broker_mode_uai_management/",
	"title": "Broker Mode UAI Management",
	"tags": [],
	"description": "",
	"content": "Broker Mode UAI Management A Broker UAI is a special kind of UAI whose job is not to host users directly but to accept attempts to reach a UAI, locate or create a UAI for the user making the attempt, and then pass the user\u0026rsquo;s connection on to the correct UAI. Multiple Broker UAIs can be created, each serving users with UAIs of a different classes. This makes it possible to set up UAIs for varying workflows and environments as needed. The following illustrates a system using the Broker mode of UAI management:\nUnlike in the Legacy Mode, in the Broker Mode users log into their UAIs through the Broker UAI. The logic in the Broker UAI authenticates the user and assigns the user an End-User UAI. The Broker UAI then forwards the SSH session to the End-User UAI. This is seamless from the user\u0026rsquo;s perspective, as the SSH session is carried through the Broker UAI and into the End-User UAI.\nTo make all of this work, the administrator must define at least one UAI Class containing the configuration for the End-User UAIs to be created by the Broker UAI and one UAI class containing the Broker UAI configuration itself. The Broker UAI should be configured by the site to permit authentication of users. Refer to the example in Configure a Broker UAI Class for more information. The necessary Broker UAI customization can be achieved using volumes to place configuration files as needed in the file system namespace of the Broker UAI. Finally, once all of this is prepared, the administrator launches the Broker UAI, and makes the IP address of the Broker UAI available for users to log into.\nTop: User Access Service (UAS)\nNext Topic: Configure End-User UAI Classes for Broker Mode\n"
},
{
	"uri": "/docs-csm/en-12/operations/csm_product_management/change_passwords_and_credentials/",
	"title": "Change Passwords and Credentials",
	"tags": [],
	"description": "",
	"content": "Change Passwords and Credentials This is an overarching procedure to change all credentials managed by Cray System Management (CSM) in HPE Cray EX system to new values.\nThere are many passwords and credentials used in different contexts to manage the system. These can be changed as needed. Their initial settings are documented, so it is recommended to change them during or soon after a CSM software installation.\nPrerequisites Review procedures in Manage System Passwords. Procedure Change hardware credentials Change Cray EX Liquid-Cooled Cabinet Global Default Password Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials Change Air-Cooled Node BMC Credentials Change SNMP Credentials on Leaf BMC Switches Update Default ServerTech PDU Credentials used by the Redfish Translation Service (RTS) Change Credentials on ServerTech PDUs Change node credentials Update NCN Passwords Change Root Passwords for Compute Nodes Change service credentials Change the Keycloak Admin Password "
},
{
	"uri": "/docs-csm/en-12/introduction/csm_overview/",
	"title": "CSM Overview",
	"tags": [],
	"description": "",
	"content": "CSM Overview This CSM Overview describes the Cray System Management ecosystem with its hardware, software, and network. It describes how to access these services and components.\nThe CSM installation prepares and deploys a distributed system across a group of management nodes organized into a Kubernetes cluster which uses Ceph for utility storage. These nodes perform their function as Kubernetes master nodes, Kubernetes worker nodes, or utility storage nodes with the Ceph storage.\nSystem services on these nodes are provided as containerized micro-services packaged for deployment via Helm charts. Kubernetes orchestrates these services and schedules them on Kubernetes worker nodes with horizontal scaling. Horizontal scaling increases or decreases the number of services\u0026rsquo; instances as demand for them varies, such as when booting many compute nodes or application nodes.\nTopics System Nodes and Networks Default IP Address Ranges Resilience of System Management Services Access to System Management Services Details 1. System Nodes and Networks The HPE Cray EX system has two types of nodes:\nCompute Nodes, where high performance computing applications are run, have hostnames in the form of nidXXXXXX, that is, nid followed by six digits. These six digits will be padded with zeroes at the beginning. All other nodes provide supporting functions to these compute nodes. Non-Compute Nodes (NCNs), which carry out system functions and come in many types: Management nodes in a Kubernetes cluster which host system services. Kubernetes master nodes, with names in the form of ncn-mXXX. Every system has three or more master nodes. Kubernetes worker nodes, with names in the form of ncn-wXXX. Every system has three or more worker nodes. Utility Storage nodes providing Ceph storage to Kubernetes nodes, with names in the form of ncn-sXXX. Every system has three or more storage nodes. Application nodes (ANs) which are not part of the Kubernetes management cluster User Access Nodes (UANs), known by some as login or front-end nodes Other site-defined types: Gateway nodes Data Mover nodes Visualization nodes The following system networks connect the devices listed:\nNetworks external to the system: Customer Network (Data Center) ncn-m001 BMC is connected by the customer network switch to the customer management network ClusterStor System Management Unit (SMU) interfaces User Access Nodes (UANs) System networks: Hardware Management Network (HMN) BMCs for Admin tasks Power distribution units (PDU) Keyboard/video/mouse (KVM) Node Management Network (NMN) All NCNs and compute nodes ClusterStor Management Network ClusterStor controller management interfaces of all ClusterStor components (SMU, Metadata Management Unit (MMU), and Scalable Storage Unit (SSU)) High-Speed Network (HSN), which connects the following devices: Kubernetes worker nodes UANs ClusterStor controller data interfaces of all ClusterStor components (SMU, MMU, and SSU) During initial installation, several of those networks are created with default IP address ranges. See Default IP Address Ranges\nThe network management system (NMS) data model and REST API enable customer sites to construct their own \u0026ldquo;networks\u0026rdquo; of nodes within the high-speed fabric, where a \u0026ldquo;network\u0026rdquo; is a collection of nodes that share a VLAN and an IP subnet.\nThe low-level network management components (switch, DHCP service, ARP service) of the management nodes and ClusterStor interfaces are configured to serve one particular network (the \u0026ldquo;supported network\u0026rdquo;) on the high-speed fabric. As part of the initial installation, the supported network is created to include all of the compute nodes, thereby enabling those compute nodes to access the gateway, user access services, and ClusterStor devices.\nA site may create other networks as well, but it is only the supported network that is served by those devices.\n2. Default IP Address Ranges The initial installation of the system creates default networks with default settings and with no external exposure. These IP address default ranges ensure that no nodes in the system attempt to use the same IP address as a Kubernetes service or pod, which would result in undefined behavior that is extremely difficult to reproduce or debug.\nThe following table shows the default IP address ranges\nNetwork IP Address Range Kubernetes service network 10.16.0.0/12 Kubernetes pod network 10.32.0.0/12 Install Network (MTL) 10.1.0.0/16 Node Management Network (NMN) 10.252.0.0/17 High Speed Network (HSN) 10.253.0.0/16 Hardware Management Network (HMN) 10.254.0.0/17 Mountain NMN (see note below table) 10.100.0.0/17 Mountain HMN (see note below table) 10.104.0.0/17 River NMN 10.106.0.0/17 River HMN 10.107.0.0/17 Load Balanced NMN 10.92.100.0/24 Load Balanced HMN 10.94.100.0/24 For the Mountain NMN:\nAllocate a /22 from this range per liquid-cooled cabinet. For example, the following cabinets would be given the following IP addresses in the allocated ranges:\ncabinet 1 = 10.100.0.0/22 cabinet 2 = 10.100.4.0/22 cabinet 3 = 10.100.8.0/22 \u0026hellip; For the Mountain HMN:\nAllocate a /22 from this range per liquid-cooled cabinet. For example, the following cabinets would be given the following IP addresses in the allocated ranges:\ncabinet 1 = 10.104.0.0/22 cabinet 2 = 10.104.4.0/22 cabinet 3 = 10.104.8.0/22 \u0026hellip; The above values could be modified prior to install if there is a need to ensure that there are no conflicts with customer resources, such as LDAP or license servers. If a customer has more than one HPE Cray EX system, these values can be safely reused across them all. Contact customer support for this site if it is required to change the IP address range for Kubernetes services or pods; for example, if the IP addresses within those ranges must be used for something else. The cluster must be fully reinstalled if either of those ranges are changed.\nThere are several network values and other pieces of system information that are unique to the customer system.\nIP addresses and the network(s) for ncn-m001 and the BMC on ncn-m001.\nThe main Customer Management Network (CMN) subnet. The two address pools mentioned below need to be part of this subnet.\nFor more information on the CMN, see Customer Accessible Networks.\nSubnet for the MetalLB static address pool (cmn-static-pool), which is used for services that need to be pinned to the same IP address, such as the system DNS service. Subnet for the MetalLB dynamic address pool (cmn-dynamic-pool), which is used for services such as Prometheus and Nexus that can be reached by DNS. HPE Cray EX Domain: The value of the subdomain that is used to access externally exposed services. For example, if the system is named TestSystem, and the site is example.com, the HPE Cray EX domain would be testsystem.example.com. Central DNS would need to be configured to delegate requests for addresses in this domain to the HPE Cray EX DNS IP address for resolution.\nHPE Cray EX DNS IP: The IP address used for the HPE Cray EX DNS service. Central DNS delegates the resolution for addresses in the HPE Cray EX Domain to this server. The IP address will be in the cmn-static-pool subnet.\nCMN gateway IP address: The IP address assigned to a specific port on the spine switch, which will act as the gateway between the CMN and the rest of the customer\u0026rsquo;s internal networks. This address would be the last hop route to the CMN network.\nThe User Network subnet which will be either the Customer Access Network (CAN) or Customer High-speed Network (CHN). The address pool mentioned below needs to be part of this subnet.\nFor more information on the CAN and CHN, see Customer Accessible Networks.\nSubnet for the MetalLB dynamic address pool (can-dynamic-pool) or (chn-dynamic-pool), which is used for services such as User Access Instances (UAIs) that can be reached by DNS. 3. Resilience of System Management Services HPE Cray EX systems are designed so that system management services (SMS) are fully resilient and that there is no single point of failure. The design of the system allows for resiliency in the following ways:\nThree management nodes are configured as Kubernetes master nodes. When one master goes down, operations (such as jobs running across compute nodes) are expected to continue. At least three utility storage nodes provide persistent storage for the services running on the Kubernetes management nodes. When one of the utility storage nodes goes down, operations (such as jobs running across compute nodes) are expected to continue. At least three management nodes are configured as Kubernetes worker nodes. If one of only three Kubernetes worker nodes were to go down, it would be much more difficult for the remaining two worker nodes to handle the total balance of pods. It is less significant to lose one of the worker nodes if the system has more than three worker nodes because there are more worker nodes able to handle the pod load. The state and configuration of the Kubernetes cluster are stored in an etcd cluster distributed across the Kubernetes master nodes. This cluster is also backed up on an interval, and backups are pushed to the local cluster\u0026rsquo;s Ceph Rados Gateway (S3). A micro-service can run on any node that meets the requirements for that micro-service, such as appropriate hardware attributes, which are indicated by Kubernetes labels and taints. All micro-services have shared persistent storage so that they can be restarted on any worker node in the Kubernetes management cluster without losing state. Kubernetes is designed to ensure that the desired number of deployments of a micro-service are always running on one or more worker nodes. In addition, it ensures that if one worker node becomes unresponsive, the micro-services that were running on it are migrated to another worker node that is up and meets the requirements of those micro-services.\nFor more information about resiliency topics see Resilience of System Management Services.\n4. Access to System Management Services The standard configuration for System Management Services (SMS) is the containerized REST micro-service with a public API. All of the micro-services provide an HTTP interface and are collectively exposed through a single gateway URL. The API gateway for the system is available at a well known URL based on the domain name of the system. It acts as a single HTTPS endpoint for terminating Transport Layer Security (TLS) using the configured certificate authority. All services and the API gateway are not dependent on any single node. This resilient arrangement ensures that services remain available during possible underlying hardware and network failures.\nAccess to individual APIs through the gateway is controlled by a policy-driven access control system. Administrators and users must retrieve a token for authentication before attempting to access APIs through the gateway and present a valid token with each API call. The authentication and authorization decisions are made at the gateway level which prevent unauthorized API calls from reaching the underlying micro-services. For more detail on the process of obtaining tokens and user management, see System Security and Authentication.\nReview the API documentation in the supplied container before attempting to use the API services. This container is generated with the release using the most current API descriptions in OpenAPI 2.0 format. Because this file serves as both an internal definition of the API contract and the external documentation of the API function, it is the most up-to-date reference available.\nThe API Gateway URL for accessing the APIs on a site-specific system is https://api.NETWORK.SYSTEM-NAME.DOMAIN-NAME/apis/.\n"
},
{
	"uri": "/docs-csm/en-12/install/shcd_hmn_connections_rules/",
	"title": "SHCD HMN Tab/HMN Connections Rules",
	"tags": [],
	"description": "",
	"content": "SHCD HMN Tab/HMN Connections Rules Introduction Compute node Dense four node chassis - Gigabyte or Intel chassis Single node chassis - Apollo 6500 XL675D Dual node chassis - Apollo 6500 XL645D Chassis Management Controller (CMC) Management node Master Worker Storage Application node Single node chassis Building component names (xnames) for nodes in a single application node chassis Dual node chassis Building component names (xnames) for nodes in a dual application node chassis Columbia Slingshot switch PDU cabinet controller Cooling door Management switches Introduction The HMN tab of the SHCD describes the air-cooled hardware present in the system and how these devices are connected to the Hardware Management Network (HMN). This information is required by CSM to perform hardware discovery and geolocation of air-cooled hardware in the system. The HMN tab may contain other hardware that is not managed by CSM, but is connected to the HMN.\nThe hmn_connections.json file is derived from the HMN tab of a system SHCD, and is one of the seed files required by the Cray Site Init (CSI) tool to generate configuration files required to install CSM. The hmn_connections.json file is almost a one-to-one copy of the right-hand table in the HMN tab of the SHCD. It is an array of JSON objects, and each object represents a row from the HMN tab. Any row that is not understood by CSI will be ignored; this includes any additional devices connected to the HMN that are not managed by CSM.\nThe System Layout Service (SLS) contains data about what hardware is in the system and how it is connected to the HMN network. This data is generated when the CSI tool generates configurations files for system. For air-cooled hardware, SLS will contain the SLS representation of the device and a Management Switch Connector object that describes what device is plugged into a particular management switch port.\nColumn mapping from SHCD to hmn_connections.json:\nSHCD Column SHCD Column Name hmn_connections Field Description J20 Source Source Name of the device connected to the HMN network K20 Rack SourceRack Source rack; matches regular expression x\\d+ L20 Location SourceLocation For nodes (management, compute, application), this is bottom rack slot that the node occupies, and can be extracted by [a-zA-Z]*(\\d+)([a-zA-Z]*). For other device types, this is ignored. M20 SourceSubLocation For compute nodes, this can be L, l, R, r, or blank. For other device types, this is ignored. N20 Parent SourceParent O20 not used P20 Port not used Q20 Destination not used R20 Rack DestinationRack Rack of the management switch S20 Location DestinationLocation Rack slot of the management switch T20 not used U20 Port DestinationPort Switch port on the management switch Only J20 needs to have the column name of Source. There are no requirements on what the other columns should be named.\nSome conventions for this document:\nAll Source names from the SHCD are converted to lowercase before being processed by the CSI tool. Throughout this document, the field names from the hmn_connections.json file will be used to referenced values from the SHCD. Each device type has an example of how it is represented in the HMN tab of the SHCD, the hmn_connections.json file, and in SLS. Compute node The Source field needs to match these conditions in order to be considered a compute node:\nHas one of the following prefixes:\nnid cn Ends with an integer that matches this regular expression: (\\d+$)\nThis integer is the Node ID (NID) for the node Each node should have a unique NID value For example, the following are valid Source field values for compute nodes:\nnid000001 cn1 cn-01 Depending on the type of compute node, additional rules may apply. Compute nodes in the follow sections will use the nid prefix.\nDense four node chassis - Gigabyte or Intel chassis Apollo 2000 compute nodes are not currently supported by CSM.\nAir-cooled compute nodes are typically in a 2U chassis that contains four compute nodes. Each of the compute nodes in the chassis gets its own row in the HMN tab, plus a parent row.\nThe value of the SourceParent field is used to group together the 4 nodes that are contained within the same chassis, and it is used to reference another row in the SHCD HMN tab. The referenced SourceParent row is used to determine the rack slot that the compute nodes occupy. The SourceParent row can be a Chassis Management Controller, which can be used to control devices underneath it. This device typically will have a connection to the HMN. A Gigabyte CMC is an example of a CMC. If a CMC is not connected to the HMN network, this will prevent CSM services from managing that device. The SourceParent row can be a virtual parent that is used to group the compute nodes together symbolically into a chassis. It does not need to not have a connection to the HMN. The rack slot that a compute node occupies is determined by the rack slot of the SourceParent. The SourceLocation of the parent is the bottom unit number of the chassis. All four nodes in the same chassis receive a component name (xname) with the bottom unit number of the chassis.\nThe BMC ordinal for a node\u0026rsquo;s BMC is derived from the NID of the node by applying a modulo of four and then adding one. For example, a node with NID 17 will have a BMC ordinal of (17 modulo 4) + 1 == 1 + 1 == 2. Therefore a node with NID 17 in slot 10 in cabinet 3000 will have the component name (xname) of x3000s10b2n0.\nCompute: Four node chassis: SHCD Example: Four compute nodes in the same chassis with a CMC connected to the network. The compute node chassis is located in slot 17 of cabinet 3000, and the compute node BMCs are connected to ports 33-36 in the management leaf-bmc-bmc switch in slot 14 of cabinet 3000. Port 32 on the leaf-bmc-bmc switch is for the CMC in the chassis. Refer to Chassis Management Controller section for additional details.\nSource Rack Location Parent Port Destination Rack Location Port nid000001 x3000 u17 R SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j36 nid000002 x3000 u18 R SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j35 nid000003 x3000 u18 L SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j34 nid000004 x3000 u17 L SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j33 SubRack-001-CMC x3000 u17 - cmc sw-smn01 x3000 u14 - j32 Note: Source names like cn1 and cn-01 are equivalent to the value nid000001.\nExample: Four compute nodes in the same chassis without a CMC connected to the HMN network.\nSource Rack Location Parent Port Destination Rack Location Port nid000001 x3000 u17 R SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j36 nid000002 x3000 u18 R SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j35 nid000003 x3000 u18 L SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j34 nid000004 x3000 u17 L SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j33 SubRack-001-CMC x3000 u17 - Note: Source names like cn1 and cn-01 are equivalent to the value nid000001.\nCompute: Four node chassis: HMN connections Example Four compute nodes in the same chassis with the a CMC connected to the network. The compute node chassis is located in slot 17 of cabinet 3000, and the compute node BMCs are connected to ports 33-36 in the management leaf-bmc-bmc switch in slot 14 of cabinet 3000. The SourceParent for the compute nodes SubRack-001-CMC is connected to the port 32 on the leaf-bmc-bmc switch.\n{\u0026#34;Source\u0026#34;:\u0026#34;nid000001\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u17\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;R\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j36\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;nid000002\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u18\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;R\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j35\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;nid000003\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u18\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;L\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j34\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;nid000004\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u17\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;L\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j33\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u17\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j32\u0026#34;} Note: Source values like cn1 and cn-01 are equivalent to the value nid000001.\nExample: Four compute nodes in the same chassis without a CMC connected to the HMN network.\nThe SourceParent for the compute nodes SubRack-001-CMC is not connected the HMN network.\n{\u0026#34;Source\u0026#34;:\u0026#34;nid000001\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u17\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;R\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j36\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;nid000002\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u18\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;R\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j35\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;nid000003\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u18\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;L\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j34\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;nid000004\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u17\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;L\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j33\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u17\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34; \u0026#34;} Note: Source values like cn1 and cn-01 are equivalent to the value nid000001.\nCompute: Four node chassis: SLS The CSI tool will generate the following SLS representations for compute nodes and their BMC connections to the HMN network.\nCompute: Four node chassis: SLS: Compute node with NID 1 Node\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s17b1\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s17b1n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 1, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000001\u0026#34; ] } } Management switch connector\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j36\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s17b1\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/36\u0026#34; } } For Aruba leaf-bmc switches, the VendorName value will be 1/1/36. Dell leaf-bmc switches will have value ethernet1/1/36.\nCompute: Four node chassis: SLS: Compute node with NID 2 Node:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s17b2\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s17b2n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 2, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000002\u0026#34; ] } } Management switch connector\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j35\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s17b2\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/35\u0026#34; } } For Aruba leaf-bmc switches, the VendorName value will be 1/1/35. Dell leaf-bmc switches will have value ethernet1/1/35.\nCompute: Four node chassis: SLS: Compute node with NID 3 Node\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s17b3\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s17b3n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 3, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000003\u0026#34; ] } } Management switch connector\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j34\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s17b3\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/34\u0026#34; } } For Aruba leaf-bmc switches, the VendorName value will be 1/1/34. Dell leaf-bmc switches will have value ethernet1/1/34.\nCompute: Four node chassis: SLS: Compute node with NID 4 Node\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s17b4\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s17b4n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 4, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000004\u0026#34; ] } } Management switch connector\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j33\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s17b4\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/33\u0026#34; } } For Aruba leaf-bmc switches, the VendorName value will be 1/1/33. Dell leaf-bmc switches will have value ethernet1/1/33.\nSingle node chassis - Apollo 6500 XL675D A single compute node chassis needs to match these additional conditions:\nNo SourceParent defined No SourceSubLocation defined This convention applies to all compute nodes that have a single node in a chassis, such as the Apollo XL675D.\nCompute: Single node chassis: SHCD Example: A single chassis node with NID 1 located in slot 2 of cabinet 3000. The node\u0026rsquo;s BMC is connected to port 36 of the management leaf-bmc switch in slot 40 of cabinet 3000.\nSource Rack Location Parent Port Destination Rack Location Port nid000001 x3000 u02 - j03 sw-smn01 x3000 u40 - j36 Note: Source values like cn1 and cn-01 are equivalent to the value nid000001.\nCompute: Single node chassis: HMN connections The HMN connections representation for the two SHCD table rows above:\n{\u0026#34;Source\u0026#34;:\u0026#34;nid000001\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u02\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u40\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j36\u0026#34;} Note: Source values like cn1 and cn-01 are equivalent to the value nid000001.\nCompute: Single node chassis: SLS Compute node:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s2b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s2b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 1, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000001\u0026#34; ] } } Management switch connector\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w40\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w40j36\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s2b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/36\u0026#34; } } For Aruba leaf-bmc switches, the VendorName value will be 1/1/36. Dell leaf-bmc switches will have value ethernet1/1/36.\nDual node chassis - Apollo 6500 XL645D Additional matching conditions:\nSourceSubLocation field contains one of: L, l, R, r. In addition to the top-level compute node naming requirements, when there are two nodes in a single chassis, the SourceSubLocation is required. The SourceSubLocation can contain one of the following values: L, l, R, r. These values are used to determine the BMC ordinal for the node.\nL or l translates into the component name (xname) having b1. For example, x3000c0s10b1b0 R or r translates into the component name (xname) having b2. For example, x3000c0s10b1b0 This convention applies to all compute nodes that have two nodes in a chassis, such as the Apollo XL645D.\nCompute: Dual node chassis: SHCD Example: A compute node chassis with 2 nodes located in slot 8 of cabinet 3000. NID 1 is on the left side of the chassis, and NID 2 is on the right side. The two node BMCs are connected to ports 37 and 38 of the management leaf-bmc switch in slot 40 of cabinet 3000.\nSource Rack Location Parent Port Destination Rack Location Port nid000001 x3000 u08 L - j03 sw-smn01 x3000 u40 - j38 nid000002 x3000 u08 R - j03 sw-smn01 x3000 u40 - j37 Note: Source values like cn1 and cn-01 are equivalent to the value nid000001.\nCompute: Dual node chassis: HMN connections The HMN connections representation for the two SHCD table rows above:\n{\u0026#34;Source\u0026#34;:\u0026#34;nid000001\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u08\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;L\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u40\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j37\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;nid000002\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u08\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;R\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u40\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j38\u0026#34;} Note: Source values like cn1 and cn-01 are equivalent to the value nid000001.\nCompute: Dual node chassis: SLS Compute: Dual node chassis: SLS: Compute node with NID 1 Node:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s8b1\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s8b1n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 3, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000003\u0026#34; ] } } Management switch connector\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w40\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w40j38\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s8b1\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/38\u0026#34; } } For Aruba leaf-bmc switches, the VendorName value will be 1/1/38. Dell leaf-bmc switches will have value ethernet1/1/38.\nCompute: Dual node chassis: SLS: Compute node with NID 2 Node\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s8b2\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s8b2n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 2, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000002\u0026#34; ] } } Management switch connector\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w40\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w40j37\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s8b2\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/37\u0026#34; } } For Aruba leaf-bmc switches, the VendorName value will be 1/1/37. Dell leaf-bmc switches will have value ethernet1/1/37.\nChassis Management Controller (CMC) Note: This is not the same as an RCM (Rack Consolidation Module) that is present in Apollo 2000 chassis.\nMatching conditions:\nThis row is referenced as a SourceParent of another row Source field contains cmc or CMC A Chassis Management Controller (CMC) is a device which can be used to control the BMCs underneath it. This device typically has a connection to the HMN. A Gigabyte CMC is an example of a CMC. If a CMC is not connected to the HMN network, this will prevent CSM services from managing that device.\nThese devices will have the BMC ordinal of 999 for their component names (xnames). For example, x3000c0s10b999.\nCMC: SHCD Example: The CMC for the chassis in slot 28 of cabinet 3000 is connected to port 32 of the management leaf-bmc switch in slot 22 of cabinet 3000.\nSource Rack Location Parent Port Destination Rack Location Port SubRack-002-cmc x3000 u28 - cmc sw-smn01 x3000 u22 - j42 CMC: HMN Connections The HMN connections representation for the SHCD table row above:\n{\u0026#34;Source\u0026#34;:\u0026#34;SubRack-002-cmc\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u28\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u22\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j42\u0026#34;} CMC: SLS Chassis Management Controller:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s17b999\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_chassis_bmc\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;ChassisBMC\u0026#34; } Management switch connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j32\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s17b999\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/32\u0026#34; } } For Aruba leaf-bmc switches, the VendorName value will be 1/1/32. Dell leaf-bmc switches will have value ethernet1/1/32.\nManagement node Master The Source field needs to match both of the following conditions:\nmn prefix Integer immediately after the prefix; can be padded with 0 characters The integer after the prefix is used to determine the hostname of the master node. For example, mn02 corresponds to hostname ncn-m002.\nTypically, the BMC of the first master node is not connected to the HMN, as its BMC is connected to the site network.\nMaster: SHCD Example: master node where its BMC is connected to the HMN. The master node is in slot 2 in cabinet 3000, and its BMC is connected to port 25 in the management leaf-bmc switch in slot 14 of cabinet 3000.\nSource Rack Location Parent Port Destination Rack Location Port mn02 x3000 u02 - j3 sw-smn01 x3000 u14 - j25 Example: master node where its BMC is connected to the site network.\nSource Rack Location Parent Port Destination Rack Location Port mn01 x3000 u01 - j3 Master: HMN Connections Example: master node where its BMC is connected to the HMN\n{\u0026#34;Source\u0026#34;:\u0026#34;mn02\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u02\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j25\u0026#34;} Example: master node where its BMC is connected to the site network, and no connection to the HMN\n{\u0026#34;Source\u0026#34;:\u0026#34;mn01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u01\u0026#34;} The following is also equivalent to a master node with not connection to the HMN. The values DestinationRack, DestinationLocation, and DestinationPort can all contain whitespace and it is still considered to have no connection the HMN.\n{\u0026#34;Source\u0026#34;:\u0026#34;mn01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u01\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34; \u0026#34;} Master: SLS Management master node:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s2b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s2b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 100008, \u0026#34;Role\u0026#34;: \u0026#34;Management\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;Master\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;ncn-m002\u0026#34; ] } } Management switch connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j25\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s2b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/25\u0026#34; } } For Aruba leaf-bmc switches, the VendorName value will be 1/1/25. Dell leaf-bmc switches will have value ethernet1/1/25.\nWorker The Source field needs to match both of the following conditions:\nwn prefix Integer immediately after the prefix; can be padded with 0 characters The integer after the prefix is used to determine the hostname of the worker node. For example, wn01 corresponds to hostname ncn-w001.\nWorker: SHCD Example: The worker node is in slot 4 of cabinet 3000, and its BMC is connected to port 48 of management leaf-bmc switch in slot 14 of cabinet 3000.\nSource Rack Location Parent Port Destination Rack Location Port wn01 x3000 u04 - j3 sw-smn01 x3000 u14 - j48 Worker: HMN connections The HMN connections representation for the SHCD table row above:\n{\u0026#34;Source\u0026#34;:\u0026#34;wn01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u04\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j48\u0026#34;} Worker: SLS Management worker node:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s4b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s4b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 100006, \u0026#34;Role\u0026#34;: \u0026#34;Management\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;Worker\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;ncn-w001\u0026#34; ] } } Management switch connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j48\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s4b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/48\u0026#34; } } For Aruba leaf-bmc switches, the VendorName value will be 1/1/48. Dell leaf-bmc switches will have value ethernet1/1/48.\nStorage The Source field needs to match both of the following conditions:\nsn prefix Integer immediately after the prefix; can be padded with 0 characters The integer after the prefix is used to determine the hostname of the storage node. For example, sn01 corresponds to hostname ncn-s001.\nStorage: SHCD Example: The storage node is in slot 4 of cabinet 3000, and its BMC is connected to port 29 of management leaf-bmc switch in slot 14 of cabinet 3000.\nSource Rack Location Parent Port Destination Rack Location Port sn01 x3000 u07 - j3 sw-smn01 x3000 u14 - j29 Storage: HMN connections The HMN connections representation for the SHCD table row above:\n{\u0026#34;Source\u0026#34;:\u0026#34;sn01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u07\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;} Storage: SLS Management storage node:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s7b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 100003, \u0026#34;Role\u0026#34;: \u0026#34;Management\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;Storage\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;ncn-s001\u0026#34; ] } } Management switch connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j29\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s7b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/29\u0026#34; } } For Aruba leaf-bmc switches, the VendorName value will be 1/1/29. Dell leaf-bmc switches will have value ethernet1/1/29.\nApplication node The Source field needs to match these conditions to be considered an application node:\nHas one of the following prefixes: uan gn ln Note: The naming conventions for application nodes can be unique to a system. Refer to the Create Application Node Configuration YAML procedure for the process to add additional Source name prefixes for application nodes.\nSingle node chassis A single application node chassis needs to match these additional conditions:\nNo SourceParent defined No SourceSubLocation defined This convention applies to all application nodes that have a single node in a chassis.\nApplication node: Single node chassis: SHCD Example: application node is in slot 4 of cabinet 3000, and its BMC is connected to port 25 of management leaf-bmc switch in slot 14 of cabinet 3000.\nSource Rack Location Parent Port Destination Rack Location Port uan01 x3000 u04 - j3 sw-smn01 x3000 u14 - j25 Application node: Single node chassis: HMN connections The HMN connections representation for the SHCD table row above:\n{\u0026#34;Source\u0026#34;:\u0026#34;uan01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u04\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j25\u0026#34;} Building component names (xnames) for nodes in a single application node chassis The component name (xname) format for nodes takes the form of xXcCsSbBnN:\nxX: where X is the cabinet or rack identification number. cC: where C is the chassis identification number. This should be 0. sS: where S is the lowest slot the node chassis occupies. bB: where B is the ordinal of the node BMC. This should be 0. nN: where N is the ordinal of the node This should be 0. For example, if an application node is in slot 4 of air-cooled cabinet 3000, then it would have x3000c0s4b0n0 as its component name (xname).\nDual node chassis Additional matching conditions:\nSourceSubLocation field contains one of: L, l, R, r. In addition to the top-level compute node naming requirements, when there are two nodes in a single chassis, the SourceSubLocation is required. The SourceSubLocation can contain one of the following values: L, l, R, r. These values are used to determine the BMC ordinal for the node.\nL or l translates into the component name (xname) having b1\nFor example, x3000c0s10b1b0 R or r translates into the component name (xname) having b2\nFor example, x3000c0s10b1b0 This convention applies to all application nodes that have two nodes in a single chassis.\nApplication node: Dual node chassis: SHCD Example: An application node chassis with 2 nodes located in slot 8 of cabinet 3000. uan01 is on the left side of the chassis, and uan02 is on the right side. The two node BMCs are connected to ports 37 and 38 of the management leaf-bmc switch in slot 40 of cabinet 3000.\nSource Rack Location Parent Port Destination Rack Location Port uan01 x3000 u08 L - j03 sw-smn01 x3000 u40 - j38 uan02 x3000 u08 R - j03 sw-smn01 x3000 u40 - j37 Note: Source values like cn1 and cn-01 are equivalent to the value nid000001.\nApplication node: Dual node chassis: HMN connections The HMN connections representation for the two SHCD table rows above:\n{\u0026#34;Source\u0026#34;:\u0026#34;uan01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u08\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;L\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u40\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j37\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;uan02\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u08\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;R\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u40\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j38\u0026#34;} Building component names (xnames) for nodes in a dual application node chassis The component name (xname) format for nodes takes the form of xXcCsSbBnN:\nxX: where X is the Cabinet or Rack identification number. cC: where C is the chassis identification number. This should be 0. sS: where S is the lowest slot the node chassis occupies. bB: where B is the ordinal of the node BMC. If the SourceSubLocation is L or l, then this should be 1. If the SourceSubLocation is R or r, then this should be 2. nN: where N is the ordinal of the node This should be 0. For example:\nIf an application node is in slot 8 of cabinet 3000 with a SourceSubLocation of L, then it would have x3000c0s8b1n0 as its component name (xname). If an application node is in slot 8 of cabinet 3000 with a SourceSubLocation of R, then it would have x3000c0s8b2n0 as its component name (xname). Columbia Slingshot switch The Source field needs to matching one of the following conditions:\nPrefixed with sw-hsn Equal to columbia or Columbia The following are examples of valid matches:\nsw-hsn01 Columbia columbia Columbia Slingshot switch: SHCD Example: A Columbia Slingshot switch in slot 42 of cabinet 3000. Its BMC is connected to port 45 of the leaf-bmc switch in slot 38 of cabinet 3000.\nSource Rack Location Parent Port Destination Rack Location Port sw-hsn01 x3000 u42 - j3 sw-smn01 x3000 u38 - j45 Note: Source values like Columbia or columbia are also valid.\nColumbia Slingshot switch: HMN connections The HMN connections representation for the SHCD table row above:\n{\u0026#34;Source\u0026#34;:\u0026#34;sw-hsn01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u42\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u38\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j45\u0026#34;} Columbia Slingshot switch: SLS Router BMC:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0r42b0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_rtr_bmc\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;RouterBMC\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Username\u0026#34;: \u0026#34;vault://hms-creds/x3000c0r42b0\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;vault://hms-creds/x3000c0r42b0\u0026#34; } } Management switch connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w38\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w38j45\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0r42b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/45\u0026#34; } } For Aruba leaf-bmc switches, the VendorName value will be 1/1/45. Dell leaf-bmc switches will have value ethernet1/1/45.\nPDU cabinet controller A PDU cabinet controller is the device that is connected to the HMN network and manages PDU underneath it.\nThe Source field for a PDU Cabinet Controller needs to match the following regular expression: (x\\d+p|pdu)(\\d+). This regular expression matches the following 2 patterns:\nxXpP where X is the cabinet number and P is the ordinal of the PDU controller in the cabinet pduP where P is the ordinal of the PDU controller in the cabinet The following are examples of valid matches:\nx3000p0 pdu0 PDU cabinet controller: SHCD Example: PDU controller for cabinet 3000 is connected port 41 of the leaf-bmc switch in slot 38 of cabinet 3000.\nSource Rack Location Parent Port Destination Rack Location Port x3000p0 x3000 - i0 sw-smn01 x3000 u38 - j41 Alternative naming convention for the same HMN connection.\nSource Rack Location Parent Port Destination Rack Location Port pdu0 x3000 pdu0 - i0 sw-smn01 x3000 u38 - j41 PDU cabinet controller: HMN connections The HMN connections representation for the first SHCD table above.\n{\u0026#34;Source\u0026#34;:\u0026#34;x3000p0\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u38\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j41\u0026#34;} The HMN connections representation for alternative naming convention.\n{\u0026#34;Source\u0026#34;:\u0026#34;pdu0\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;pdu0\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u38\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j41\u0026#34;} PDU cabinet controller: SLS Cabinet PDU controller:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000m0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_cab_pdu_controller\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;CabinetPDUController\u0026#34; } Management switch connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w38\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w38j41\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000m0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/41\u0026#34; } } For Aruba leaf-bmc switches, the VendorName value will be 1/1/41. Dell leaf-bmc switches will have value ethernet1/1/41.\nCooling door The Source field for a cooling door contains door.\nCooling doors in an air-cooled cabinet are not currently supported by CSM software and are ignored.\nCooling door: SHCD Example: Cooling door for cabinet 3000 is connected to port 27 of the leaf-bmc switch in slot 36 of cabinet 3000.\nSource Rack Location Parent Port Destination Rack Location Port x3000door-Motiv x3000 - j1 sw-smn04 x3000 u36 - j27 Cooling door: HMN connections The HMN connections representation for the SHCD table row above:\n{\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;} Cooling door: SLS Cooling doors are not currently supported by HMS services, and are not present in SLS.\nManagement switches The Source field has one of the following prefixes:\nsw-agg sw-25g sw-40g sw-100g sw-smn Any management switch that is found in the HMN tab of the SHCD will be ignored by CSI.\nManagement switches: SHCD Example: Management switch in slot 12 of cabinet 3000, its management port is connected to port 41 of the leaf-bmc management switch in slot 14 of cabinet 3000.\nSource Rack Location Parent Port Destination Rack Location Port sw-25g01 x3000 u12 - j1 sw-smn01 x3000 u14 - j41 Management switches: HMN connections The HMN connections representation for the SHCD table row above:\n{\u0026#34;Source\u0026#34;:\u0026#34;sw-25g01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u12\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j41\u0026#34;} Management switches: SLS The management switches in SLS are not populated by hmn_connections.json, but instead from switch_metadata.csv.\n"
},
{
	"uri": "/docs-csm/en-12/install/bootstrap_livecd_remote_iso/",
	"title": "Bootstrap PIT Node from LiveCD Remote ISO",
	"tags": [],
	"description": "",
	"content": "Bootstrap PIT Node from LiveCD Remote ISO The Pre-Install Toolkit (PIT) node needs to be bootstrapped from the LiveCD. There are two media available to bootstrap the PIT node: the RemoteISO or a bootable USB device. This procedure describes using the USB device. If not using the RemoteISO, see Bootstrap PIT Node from LiveCD USB\nThe installation process is similar to the USB-based installation, with adjustments to account for the lack of removable storage.\nImportant: Before starting this procedure be sure to complete the procedure to Prepare Configuration Payload for the relevant installation scenario.\nTopics Known compatibility issues Attaching and booting the LiveCD with the BMC First login Configure the running LiveCD Generate installation files Subsequent installs (reinstalls) Initial installs (bare-metal) Verify and backup system_config.yaml Prepare Site Init Bring up the PIT services and validate PIT health Next topic 1. Known compatibility issues The LiveCD Remote ISO has known compatibility issues for nodes from certain vendors.\nIntel nodes should not attempt to bootstrap using the LiveCD Remote ISO method. Instead use Bootstrap PIT Node from LiveCD USB Gigabyte nodes should not attempt to bootstrap using the LiveCD Remote ISO method. Instead use Bootstrap PIT Node from LiveCD USB 2. Attaching and booting the LiveCD with the BMC Warning: If this is a re-installation on a system that still has a USB device from a prior installation, then that USB device must be wiped before continuing. Failing to wipe the USB, if present, may result in confusion. If the USB is still booted, then it can wipe itself using the basic wipe from Wipe NCN Disks for Reinstallation. If it is not booted, please do so and wipe it or disable the USB ports in the BIOS (not available for all vendors).\nObtain and attach the LiveCD cray-pre-install-toolkit ISO file to the BMC. Depending on the vendor of the node, the instructions for attaching to the BMC will differ.\nDownload the CSM software release and extract the LiveCD remote ISO image.\nImportant: Ensure that you have the CSM release plus any patches or hotfixes by following the instructions in Update CSM Product Stream\nThe cray-pre-install-toolkit ISO and other files are now available in the directory from the extracted CSM tar file. The ISO will have a name similar to cray-pre-install-toolkit-sle15sp3.x86_64-1.5.8-20211203183315-geddda8a.iso\nThis ISO file can be extracted from the CSM release tar file using the following command:\nlinux# tar --wildcards --no-anchored -xzvf \u0026lt;csm-release\u0026gt;.tar.gz \u0026#39;cray-pre-install-toolkit-*.iso\u0026#39; Prepare a server on the network to host the cray-pre-install-toolkit ISO file.\nPlace the cray-pre-install-toolkit ISO file on a server which the BMC of the PIT node will be able to contact using HTTP or HTTPS.\nNote: A short URL is better than a long URL for the PIT file on the webserver.\nSee the respective procedure below to attach an ISO.\nHPE iLO BMCs Gigabyte BMCs Do not use the RemoteISO method. See Bootstrap PIT Node from LiveCD USB Intel BMCs Do not use the RemoteISO method. See Bootstrap PIT Node from LiveCD USB The chosen procedure should have rebooted the server. Observe the server boot into the LiveCD.\n3. First login On first login (over SSH or at local console) the LiveCD will prompt the administrator to change the password.\nThe initial password is empty; enter the username of root and press return twice.\npit login: root Expected output looks similar to the following:\nPassword: \u0026lt;-------just press Enter here for a blank password You are required to change your password immediately (administrator enforced) Changing password for root. Current password: \u0026lt;------- press Enter here, again, for a blank password New password: \u0026lt;------- type new password Retype new password:\u0026lt;------- retype new password Welcome to the CRAY Pre-Install Toolkit (LiveOS) 4. Configure the running LiveCD Start a typescript to record this section of activities done on ncn-m001 while booted from the LiveCD.\npit# script -af ~/csm-install-remoteiso.$(date +%Y-%m-%d).txt pit# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Print information about the booted PIT image.\nThere is nothing in the output that needs to be verified. This is run in order to ensure the information is recorded in the typescript file, in case it is needed later. For example, this information is useful to include in any bug reports or service queries for issues encountered on the PIT node.\nNOTE The App. Version will report incorrectly in CSM 1.2. Please obtain the version information by running the step below and by invoking rpm -q cray-site-init.\npit# /root/bin/metalid.sh Expected output looks similar to the following:\n= PIT Identification = COPY/CUT START ======================================= VERSION=1.5.7 TIMESTAMP=20211028194247 HASH=ge4aceb1 CRAY-Site-Init build signature... Build Commit : a6c8dddf9df1a9fc7f8c4f17cb26568a8b41d433-main Build Time : 2021-12-01T16:16:41Z Go Version : go1.16.10 Git Version : a6c8dddf9df1a9fc7f8c4f17cb26568a8b41d433 Platform : linux/amd64 App. Version : 1.12.2 metal-net-scripts-0.0.2-1.noarch metal-basecamp-1.1.9-1.x86_64 metal-ipxe-2.0.10-1.noarch pit-init-1.2.12-1.noarch = PIT Identification = COPY/CUT END ========================================= Find a local disk for storing product installers.\npit# disk=\u0026#34;$(lsblk -l -o SIZE,NAME,TYPE,TRAN | grep -E \u0026#39;(sata|nvme|sas)\u0026#39; | sort -h | awk \u0026#39;{print $2}\u0026#39; | head -n 1 | tr -d \u0026#39;\\n\u0026#39;)\u0026#34; pit# echo $disk pit# parted --wipesignatures -m --align=opt --ignore-busy -s /dev/$disk -- mklabel gpt mkpart primary ext4 2048s 100% pit# mkfs.ext4 -L PITDATA \u0026#34;/dev/${disk}1\u0026#34; pit# mount -vL PITDATA The parted command may give an error similar to the following:\nError: Partition(s) 4 on /dev/sda have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use. As a result, the old partition(s) will remain in use. You should reboot now before making further changes. In that case, the following steps may resolve the problem without needing to reboot. These commands remove volume groups and raid arrays that may be using the disk. These commands only need to be run if the earlier parted command failed.\npit# RAIDS=$(grep \u0026#34;${disk}[0-9]\u0026#34; /proc/mdstat | awk \u0026#39;{ print \u0026#34;/dev/\u0026#34;$1 }\u0026#39;) ; echo ${RAIDS} pit# VGS=$(echo ${RAIDS} | xargs -r pvs --noheadings -o vg_name 2\u0026gt;/dev/null) ; echo ${VGS} pit# echo ${VGS} | xargs -r -t -n 1 vgremove -f -v pit# echo ${RAIDS} | xargs -r -t -n 1 mdadm -S -f -v After running the above procedure, retry the parted command which failed. If it succeeds, resume the install from that point.\nSet up the site-link, enabling SSH to work. You can reconnect with SSH after this step.\nNote: If your site\u0026rsquo;s network authority or network administrator has already provisioned a DHCP IPv4 address for your master node\u0026rsquo;s external NIC(s), then skip this step.\nSet networking variables.\nIf you have previously created the system_config.yaml file for this system, the values for these variables are in it. The following table lists the variables being set, their corresponding system_config.yaml fields, and a description of what they are.\nVariable system_config.yaml Description site_ip site-ip The IPv4 address and CIDR netmask for the node\u0026rsquo;s external interface(s) site_gw site-gw The IPv4 gateway address for the node\u0026rsquo;s external interface(s) site_dns site-dns The IPv4 domain name server address for the site site_nics site-nic The actual NIC name(s) for the external site interface(s) If the system_config.yaml file has not yet been generated for this system, the values for site_ip, site_gw, and site_dns should be provided by the site\u0026rsquo;s network administrator or network authority. The site_nics interface(s) are typically the first onboard adapter or the first copper 1 GbE PCIe adapter on the PIT node. If multiple interfaces are specified, they must be separated by spaces (for example, site_nics='p2p1 p2p2 p2p3').\npit# site_ip=172.30.XXX.YYY/20 pit# site_gw=172.30.48.1 pit# site_dns=172.30.84.40 pit# site_nics=em1 Run the csi-setup-lan0.sh script to set up the site link.\nNote: All of the /root/bin/csi-* scripts are harmless to run without parameters; doing so will print usage statements.\npit# /root/bin/csi-setup-lan0.sh $site_ip $site_gw $site_dns $site_nics Verify that lan0 has an IP address and attempt to auto-set the hostname based on DNS.\nThe script appends -pit to the end of the hostname as a means to reduce the chances of confusing the PIT node with an actual, deployed NCN.\npit# ip a show lan0 pit# /root/bin/csi-set-hostname.sh # this will attempt to set the hostname based on the site\u0026#39;s own DNS records. Add helper variables to PIT node environment.\nImportant: All CSM install procedures on the PIT node assume that these variables are set and exported.\nSet helper variables.\npit# CSM_RELEASE=csm-x.y.z pit# SYSTEM_NAME=eniac pit# PITDATA=$(lsblk -o MOUNTPOINT -nr /dev/disk/by-label/PITDATA) Add variables to the PIT environment.\nBy adding these to the /etc/environment file of the PIT node, these variables will be automatically set and exported in shell sessions on the PIT node.\nThe echo prepends a newline to ensure that the variable assignment occurs on a unique line, and not at the end of another line.\npit# echo \u0026#34; CSM_RELEASE=${CSM_RELEASE} PITDATA=${PITDATA} CSM_PATH=${PITDATA}/${CSM_RELEASE} SYSTEM_NAME=${SYSTEM_NAME}\u0026#34; | tee -a /etc/environment Exit the typescript, exit the console session, and log in again using SSH.\npit# exit # exit the typescript started earlier pit# exit # log out of the pit node # Close the console session by entering \u0026amp;. or ~. # Then ssh back into the PIT node external# ssh root@${SYSTEM_NAME}-ncn-m001 After reconnecting, resume the typescript (the -a appends to an existing script).\npit# script -af $(ls -tr ~/csm-install-remoteiso*.txt | head -n 1) pit# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Verify that expected environment variables are set in the new login shell.\npit# echo -e \u0026#34;CSM_PATH=${CSM_PATH}\\nCSM_RELEASE=${CSM_RELEASE}\\nPITDATA=${PITDATA}\\nSYSTEM_NAME=${SYSTEM_NAME}\u0026#34; Check hostname.\npit# hostnamectl Note:\nThe hostname should be similar to eniac-ncn-m001-pit when booted from the LiveCD, but it will be shown as pit# in the documentation command prompts from this point onward. If the hostname returned by the hostnamectl command is pit, then re-run the csi-set-hostname.sh script with the same parameters. Otherwise, an administrator should set the hostname manually with hostnamectl. In the latter case, do not confuse other administrators by using the hostname ncn-m001. Append the -pit suffix, indicating that the node is booted from the LiveCD. Create necessary directories.\npit# mkdir -pv ${PITDATA}/{admin,configs} ${PITDATA}/prep/{admin,logs} ${PITDATA}/data/{k8s,ceph} Relocate the typescript to the newly mounted PITDATA directory.\nQuit the typescript session with the exit command.\nCopy the typescript file to its new location.\npit# cp -v ~/csm-install-remoteiso.*.txt ${PITDATA}/prep/admin Restart the typescript, appending to the previous file.\npit# script -af $(ls -tr ${PITDATA}/prep/admin/csm-install-remoteiso*.txt | head -n 1) pit# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Download the CSM software release to the PIT node.\nSet variable to URL of CSM tarball.\npit# URL=https://arti.dev.cray.com/artifactory/shasta-distribution-stable-local/csm/${CSM_RELEASE}.tar.gz Fetch the release tarball.\npit# wget ${URL} -O ${CSM_PATH}.tar.gz Expand the tarball on the PIT node.\nNote: Expansion of the tarball may take more than 45 minutes.\npit# tar -C ${PITDATA} -zxvf ${CSM_PATH}.tar.gz \u0026amp;\u0026amp; ls -l ${CSM_PATH} Copy the artifacts into place.\npit# rsync -a -P --delete ${CSM_PATH}/images/kubernetes/ ${PITDATA}/data/k8s/ \u0026amp;\u0026amp; rsync -a -P --delete ${CSM_PATH}/images/storage-ceph/ ${PITDATA}/data/ceph/ Note: The PIT ISO, Helm charts/images, and bootstrap RPMs are now available in the extracted CSM tar file.\nInstall the latest version of CSI tool.\npit# rpm -Uvh --force $(find ${CSM_PATH}/rpm/ -name \u0026#34;cray-site-init-*.x86_64.rpm\u0026#34; | sort -V | tail -1) Install the latest documentation RPM.\nSee Check for Latest Documentation\nShow the version of CSI installed.\nNOTE The App. Version will report incorrectly in CSM 1.2.0 and CSM 1.2.1. Please obtain the version information by running the step below and by invoking rpm -q cray-site-init.\npit# /root/bin/metalid.sh Expected output looks similar to the following:\n= PIT Identification = COPY/CUT START ======================================= VERSION=1.5.7 TIMESTAMP=20211028194247 HASH=ge4aceb1 CRAY-Site-Init build signature... Build Commit : a6c8dddf9df1a9fc7f8c4f17cb26568a8b41d433-main Build Time : 2021-12-01T16:16:41Z Go Version : go1.16.10 Git Version : a6c8dddf9df1a9fc7f8c4f17cb26568a8b41d433 Platform : linux/amd64 App. Version : 1.12.2 metal-net-scripts-0.0.2-1.noarch metal-basecamp-1.1.9-1.x86_64 metal-ipxe-2.0.10-1.noarch pit-init-1.2.12-1.noarch = PIT Identification = COPY/CUT END ========================================= 4.1 Generate installation files Some files are needed for generating the configuration payload. See the Command Line Configuration Payload and Configuration Payload Files topics if one has not already prepared the information for this system.\nCommand Line Configuration Payload Configuration Payload Files Create the hmn_connections.json file by following the Create HMN Connections JSON procedure. Return to this section when completed.\nCreate the configuration input files if needed and copy them into the preparation directory.\nThe preparation directory is ${PITDATA}/prep.\nCopy these files into the preparation directory, or create them if this is an initial install of the system:\napplication_node_config.yaml (optional - see below) cabinets.yaml (optional - see below) hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml (only available after first-install generation of system files) The optional application_node_config.yaml file may be provided for further definition of settings relating to how application nodes will appear in HSM for roles and subroles. See Create Application Node YAML.\nThe optional cabinets.yaml file allows cabinet naming and numbering as well as some VLAN overrides. See Create Cabinets YAML.\nThe system_config.yaml file is generated by the csi tool during the first install of a system, and can later be used for reinstalls of the system. For the initial install, the information in it must be provided as command line arguments to csi config init.\nAfter gathering the files into this working directory, move on to Subsequent Fresh-Installs (Re-Installs).\nProceed to the appropriate next step.\nIf this is the initial install of the system, then proceed to Initial Installs (bare-metal). If this is a reinstall of the system, then proceed to Subsequent Installs (Reinstalls). 4.1.a Subsequent installs (reinstalls) For subsequent fresh-installs (re-installs) where the system_config.yaml parameter file is available, generate the updated system configuration (see Cray Site Init Files).\nWarning: If the system_config.yaml file is unavailable, then skip this step and proceed to Initial Installs (bare-metal).\nCheck for the configuration files. The needed files should be in the preparation directory.\npit# ls -1 ${PITDATA}/prep Expected output looks similar to the following:\napplication_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml Generate the system configuration.\nNote: Ensure that you specify a reachable NTP pool or server using the ntp-pools or ntp-servers fields, respectively. Adding an unreachable server can cause clock skew as chrony tries to continually reach out to a server it can never reach.\npit# cd ${PITDATA}/prep \u0026amp;\u0026amp; csi config init A new directory matching the system-name field in system_config.yaml will now exist in the working directory.\nNote: These warnings from csi config init for issues in hmn_connections.json can be ignored.\nThe node with the external connection (ncn-m001) will have a warning similar to this because its BMC is connected to the site and not the HMN like the other management NCNs. It can be ignored.\n\u0026#34;Couldn\u0026#39;t find switch port for NCN: x3000c0s1b0\u0026#34; An unexpected component may have this message. If this component is an application node with an unusual prefix, it should be added to the application_node_config.yaml file. Then rerun csi config init. See the procedure to Create Application Node Config YAML\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1610405168.8705149,\u0026#34;msg\u0026#34;:\u0026#34;Found unknown source prefix! If this is expected to be an Application node, please update application_node_config.yaml\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u33\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3002\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u48\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;}} If a cooling door is found in hmn_connections.json, there may be a message like the following. It can be safely ignored.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1612552159.2962296,\u0026#34;msg\u0026#34;:\u0026#34;Cooling door found, but xname does not yet exist for cooling doors!\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;}} Skip the next step and continue to verify and backup system_config.yaml.\n4.1.b Initial installs (bare-metal) For first-time/initial installs (without a system_config.yamlfile), generate the system configuration. See below for an explanation of the command line parameters and some common settings.\nCheck for the configuration files. The needed files should be in the preparation directory.\npit# ls -1 ${PITDATA}/prep Expected output looks similar to the following:\napplication_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv Generate the system configuration.\nNotes:\nRun csi config init --help to print a full list of parameters that must be set. These will vary significantly depending on the system and site configuration. Ensure that you specify a reachable NTP pool or server using the --ntp-pools or --ntp-servers flags, respectively. Adding an unreachable server can cause clock skew as chrony tries to continually reach out to a server it can never reach. pit# cd ${PITDATA}/prep \u0026amp;\u0026amp; csi config init \u0026lt;options\u0026gt; A new directory matching the --system-name argument will now exist in the working directory.\nImportant: After generating a configuration, a visual audit of the generated files for network data should be performed.\nSpecial Notes: Certain parameters to csi config init may be hard to grasp on first-time configuration generations:\nThe optional application_node_config.yaml file is used to map prefixes in hmn_connections.csv to HSM subroles. A command line option is required in order for csi to use the file. See Create Application Node YAML. The bootstrap-ncn-bmc-user and bootstrap-ncn-bmc-pass must match what is used for the BMC account and its password for the management NCNs. Set site parameters (site-domain, site-ip, site-gw, site-nic, site-dns) for the network information which connects ncn-m001 (the PIT node) to the site. The site-nic is the interface on ncn-m001 that is connected to the site network. There are other interfaces possible, but the install-ncn-bond-members are typically: p1p1,p10p1 for HPE nodes p1p1,p1p2 for Gigabyte nodes p801p1,p801p2 for Intel nodes If not using a cabinets-yaml file, then set the three cabinet parameters (mountain-cabinets, hill-cabinets, and river-cabinets) to the quantity of each cabinet type included in this system. The starting cabinet number for each type of cabinet (for example, starting-mountain-cabinet) has a default that can be overridden. See the csi config init --help. For systems that use non-sequential cabinet ID numbers, use the cabinets-yaml argument to include the cabinets.yaml file. This file gives the ability to explicitly specify the ID of every cabinet in the system. When specifying a cabinets.yaml file with the cabinets-yaml argument, other command line arguments related to cabinets will be ignored by csi. See Create Cabinets YAML. An override to default cabinet IPv4 subnets can be made with the hmn-mtn-cidr and nmn-mtn-cidr parameters. Note: These warnings from csi config init for issues in hmn_connections.json can be ignored.\nThe node with the external connection (ncn-m001) will have a warning similar to this because its BMC is connected to the site and not the HMN like the other management NCNs. It can be ignored.\n\u0026#34;Couldn\u0026#39;t find switch port for NCN: x3000c0s1b0\u0026#34; An unexpected component may have this message. If this component is an application node with an unusual prefix, it should be added to the application_node_config.yaml file. Then rerun csi config init. See the procedure to Create Application Node Config YAML\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1610405168.8705149,\u0026#34;msg\u0026#34;:\u0026#34;Found unknown source prefix! If this is expected to be an Application node, please update application_node_config.yaml\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u33\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3002\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u48\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;}} If a cooling door is found in hmn_connections.json, there may be a message like the following. It can be safely ignored.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1612552159.2962296,\u0026#34;msg\u0026#34;:\u0026#34;Cooling door found, but xname does not yet exist for cooling doors!\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;}} Link the generated system_config.yaml file into the prep/ directory. This is needed for pit-init to find and resolve the file.\nNOTE This step is needed only for fresh installs where system_config.yaml is missing from the prep/ directory.\npit# cd ${PITDATA}/prep \u0026amp;\u0026amp; ln ${SYSTEM_NAME}/system_config.yaml Continue to the next step to verify and backup system_config.yaml.\n4.2 Verify and backup system_config.yaml Verify that the newly generated system_config.yaml matches the current version of CSI.\nView the new system_config.yaml file and note the CSI version reported near the end of the file.\npit# cat ${PITDATA}/prep/${SYSTEM_NAME}/system_config.yaml Note the version reported by the csi tool.\nNOTE The App. Version will report incorrectly in CSM 1.2.0 and CSM 1.2.1. Please obtain the version information by running the step below and by invoking rpm -q cray-site-init.\npit# csi version The two versions should match. If they do not, determine the cause and regenerate the file.\nCopy the new system_config.yaml file somewhere safe to facilitate re-installs.\nContinue to the next step to Prepare Site Init.\n4.3 Prepare Site Init Important: Although the command prompts in this procedure are linux#, the procedure should be performed on the PIT node.\nPrepare the site-init directory by performing the Prepare Site Init procedures.\n5. Bring up the PIT services and validate PIT health Initialize the PIT.\nThe pit-init.sh script will prepare the PIT server for deploying NCNs.\nSet the USERNAME and IPMI_PASSWORD variables to the credentials for the BMC of the PIT node.\nread -s is used in order to prevent the credentials from being displayed on the screen or recorded in the shell history.\npit# USERNAME=root pit# read -s IPMI_PASSWORD pit# export USERNAME IPMI_PASSWORD ; /root/bin/pit-init.sh Install csm-testing and hpe-csm-goss-package.\nThe following assumes the CSM_PATH environment variable is set to the absolute path of the unpacked CSM release.\npit# rpm -Uvh --force $(find ${CSM_PATH}/rpm/ -name \u0026#34;csm-testing*.rpm\u0026#34; | sort -V | tail -1) $(find ${CSM_PATH}/rpm/ -name \u0026#34;hpe-csm-goss-package*.rpm\u0026#34; | sort -V | tail -1) Next topic After completing this procedure, proceed to configure the management network switches.\nSee Configure Management Network Switches\n"
},
{
	"uri": "/docs-csm/en-12/release_notes/",
	"title": "Cray System Management (CSM) - Release Notes",
	"tags": [],
	"description": "",
	"content": "Cray System Management (CSM) - Release Notes CSM 1.2 contains approximately 2000 changes spanning bug fixes, new feature development, and documentation improvements. This page lists some of the highlights.\nNew Monitoring New network traffic dashboard New Kubernetes and microservice health dashboard New boot dashboard New command line dashboard for critical services like smd, smd-postgres, capmc, and hbtd New Grafana dashboard for critical services like smd, capmc, smd-postgres, and hbtd Management nodes sample SMART data and publish it to SMA/SMF Support for HPE PDU telemetry Networking Release Cray Automated Network Utility (CANU) V1.0.0\nPerformance improvements to Unbound and DHCP Helper\nInitial Release of Bifurcated CAN\nBICAN summary page BICAN technical details The user and administrative traffic segregation introduced by Bifurcated CAN has changed the URLs for certain services as it is now necessary to include the network path in the fully qualified domain name. Access to administrative services is now restricted to the Customer Management Network (CMN). API access is available via the Customer Management Network (CMN), Customer Access Network (CAN), and Customer Highspeed Network (CHN).\nThe following table assumes the system was configured with a system-name of shasta and a site-domain of dev.cray.com.\nOld Name New Name auth.shasta.dev.cray.com auth.cmn.shasta.dev.cray.com nexus.shasta.dev.cray.com nexus.cmn.shasta.dev.cray.com grafana.shasta.dev.cray.com grafana.cmn.shasta.dev.cray.com prometheus.shasta.dev.cray.com prometheus.cmn.shasta.dev.cray.com alertmanager.shasta.dev.cray.com alertmanager.cmn.shasta.dev.cray.com vcs.shasta.dev.cray.com vcs.cmn.shasta.dev.cray.com kiali-istio.shasta.dev.cray.com kiali-istio.cmn.shasta.dev.cray.com s3.shasta.dev.cray.com s3.cmn.shasta.dev.cray.com sma-grafana.shasta.dev.cray.com sma-grafana.cmn.shasta.dev.cray.com sma-kibana.shasta.dev.cray.com sma-kibana.cmn.shasta.dev.cray.com api.shasta.dev.cray.com api.cmn.shasta.dev.cray.com, api.chn.shasta.dev.cray.com, api.can.shasta.dev.cray.com PowerDNS authoritative DNS server\nSupports zone transfer to external DNS servers via AXFR query and DNSSEC Refer to the PowerDNS Migration Guide and PowerDNS Configuration Guide for further information. Management network switch hostname changes\nThe management network switch hostnames have changed in CSM 1.2 to more accurately reflect the usage of each switch type.\nOld Name New Name Usage sw-spine Unchanged Network spine that links to other switches. sw-agg sw-leaf NMN connections for NCNs and application nodes. sw-leaf sw-leaf-bmc BMC connections, PDUs, Slingshot switches, cooling doors Miscellaneous functionality SLES15 SP3 support for NCNs, UANs, Compute Nodes, and barebones validation image S3FS added to master and worker nodes for storing SDU dumps and CPS content Improved FAS (Firmware Action Service) error reporting CFS State Reporter added to storage nodes Numerous new tests added along with improved error logging CAPMC support for HPE Apollo 6500 power capping CAPMC support for new power schema for BardPeak power capping CAPMC support for HPE G2 Metered 3Ph 39.9kVA 60A 480/277V FIO PDU Improved CAPMC error handling in BOA root user password and SSH keys now handled by NCN personalization after initial install; locations of data changed in HashiCorp Vault from previous releases Generic Ansible passthrough parameter added to CFS session API Improved CFS session resiliency after power outages Pod priority class additions to improve upgrades and fail-over New procedure for exporting and restoring Nexus data Nexus Export and Restore New recommendation to take and save off cluster an export of all data using the procedure New hardware support Olympus Bard Peak Blade (AMD Trento with AMD MI200) with Slingshot 11 - Compute Node Olympus Grizzly Peak NVidia A100 80GB GPU - Compute Node Milan-Based DL385 Gen10+ with AMD Mi100 GPU - UAN and Application Node Milan-Based Apollo 6500/XL675d Gen10+ with NVIDIA A100 40GB - Compute Node Milan-Based Apollo 6500/XL645d Gen10+ with NVIDIA A100 80GB - Compute Node HPE G2 Metered 3Ph 39.9kVA 60A 480/277V FIO PDU Automation improvements Automated validation of CSM health in several areas Automated administrative access configuration Automated installation of CFS set-up of passwordless SSH Automated validation of Management Network cabling Automated firmware check on PIT node keycloak-installer is released CSM install and upgrade automation improvements Base platform component upgrades Platform Component Version Ceph 15.2.15 containerd 1.5.7 CoreDNS 1.7.0 Etcd for Kubernetes 3.5.0 Etcd cluster 3.3.22 Helm 3.2.4 Istio 1.8 Keepalived 2.0.19 Kiali 1.28.1 Kubernetes 1.20.13 Loftsman 1.2.0-1 MetalLB 0.11.0 Multus 3.7 PostgreSQL 12.11 Strimzi Operator 0.27.1 Vault 1.5.5 Vault Operator 1.8.0 Zookeeper 3.5.9 Security improvements Switch to non-root containers A significant number of root user container images have been removed The remainder have been identified for removal in a future release Verification of signed RPMs CVE remediation A significant number of CVEs have been addressed, including a majority of the critical and high CVEs, like polkit and log4j Updates to Nexus require authentication Removal of code injection vulnerability in commit and cloneURL fields of CFS configuration API Further restrictions on allowed HTTP verbs in API requests coming from Compute Nodes Option to restrict Compute Nodes to only call URIs by machine identity Customer-requested enhancements Ability to turn off slots without hms-discovery powering them on Resilient way to reboot a Compute Node into its current configuration with a single API call Bug fixes Documented optimized BIOS boot order for NCNs Fixed: Slingshot switches attempting DHCP renewal to unreachable address Fixed: Node will not reboot following upgrade of BMC and BIOS Fixed: Worker node container /var/lib/containerd is full and pods stuck in ContainerCreating state Fixed: Incorrect data or bad monitor filters in sysmgmt-health namespace Fixed: Hardware State Manager showing compute nodes in standby after cabinet-level power down procedure Fixed: Cray HSM inventory reports incorrect DIMM Id and Name Fixed: Image customization CFS jobs do not set an Ansible limit when customizing Fixed: No /proc available in CFS image container Fixed: ConMan reconnects to nodes every hour, reissuing old messages with updated time stamps Fixed: CFS can leave sessions pending after a power outage Fixed: sonar-jobs-watcher not stopping orphaned CFS pods Fixed: PXE boot failures during installs, upgrades, and NCN rebuilds Fixed: cray-powerdns-manager not correctly creating CAN reverse DNS records. Deprecations CRUS is deprecated in CSM 1.2.0. It will be removed in CSM 1.5.0 and replaced with BOS V2, which will provide similar functionality. PowerDNS will replace Unbound as the authoritative DNS source in a future CSM release. The cray-dns-unbound-manager CronJob will be deprecated in a future release once all DNS records are migrated to PowerDNS. The introduction of PowerDNS and Bifurcated CAN will introduce some node and service naming changes. See the PowerDNS Migration Guide for more information. SLS support for downloading and uploading credentials in the SLS dumpstate and loadstate REST APIs is deprecated. See Deprecated features.\nRemovals The V1 version of the CFS API has been removed The cray-externaldns-coredns, cray-externaldns-etcd, and cray-externaldns-wait-for-etcd pods have been removed. PowerDNS is now the provider of the external DNS service. Known issues Security vulnerability exceptions A great deal of emphasis was placed on elimination or reduction of critical or high security vulnerabilities of container images included in the CSM 1.2 release. There remain, however, a small number of exceptions that are listed below. General reasons for carrying exceptions include needing to version pin certain core components, upstream fixes not being available, or new vulnerability detection or fixes occurring after release content is frozen. A new effort to track and address security vulnerabilities of container images spins up with each major CSM release.\nImage Reason csm-dckr/stable/dckr.io/ceph/ceph:v15.2.8 This image is needed for the procedure to upgrade to CSM 1.2, but is purged afterwards. csm-dckr/stable/quay.io/ceph/ceph:v15.2.15 This version of Ceph (Octopus) is pinned for the CSM 1.2 release. The next major version of CSM will support Ceph (Pacific). csm-dckr/stable/quay.io/cephcsi/cephcsi:v3.5.1 Upstream fixes became available after CSM 1.2 release content was frozen. csm-dckr/stable/csm-config:1.9.31 The vulnerability was discovered after CSM 1.2 release content was frozen and will be addressed in the next major CSM release. csm-dckr/stable/dckr.io/bitnami/external-dns:0.10.2-debian-10-r23 Upstream fixes are needed and are not yet available. csm-dckr/stable/quay.io/kiali/kiali:v1.28.1 Upstream fixes are needed and are not yet available. There is a procedure to Remove Kiali if desired. csm-dckr/stable/k8s.gcr.io/kube-proxy:v1.20.13 Upstream fixes are needed and are not yet available for the 1.20.13 version of Kubernetes included in CSM 1.2. csm-dckr/stable/dckr.io/nfvpe/multus:v3.1 Upstream fixes are needed for resolution. However, this image is only needed for the upgrade to CSM 1.2 and is purged afterwards. csm-dckr/stable/dckr.io/nfvpe/multus:v3.7 Upstream fixes are needed and are not yet available. quay.io/oauth2-proxy/oauth2-proxy:v7.2.1 The latest tagged image was pinned to use alpine:3.15.0 and was not addressed upstream until after CSM 1.2 release content was frozen. "
},
{
	"uri": "/docs-csm/en-12/background/",
	"title": "CSM Background Information",
	"tags": [],
	"description": "",
	"content": "CSM Background Information This document provides background information about the NCNs (non-compute nodes) which function as management nodes for the HPE Cray EX system. This information is not normally needed to install software, but provides background which may be helpful for troubleshooting an installation.\nCray Site Init files Certificate authority NCN images NCN boot workflow NCN networking NCN mounts and file systems NCN packages NCN operating system releases cloud-init Basecamp configuration Cray Site Init files The Cray Site Init (csi) command has several files which describe pre-configuration data needed during the installation process:\napplication_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv In addition, after being run with those files, csi creates an output system_config.yaml file which can be used as an input to csi when reinstalling this software release.\nCertificate authority While a system is being installed for the first time, a certificate authority (CA) is needed. This can be generated for a system, or one can be supplied from a customer intermediate CA. Outside of a new installation, there is no supported method to rotate or change the platform CA in this release.\nFor more information about these topics, see Certificate Authority.\nNCN images The management nodes boot from NCN images which are created as layers on top of a common base image. The common image is customized with a Kubernetes layer for the master nodes and worker nodes. The common image is also customized with a storage/Ceph layer for the utility storage nodes. Three artifacts are needed to boot the management nodes.\nFor more information, see NCN Images.\nNCN boot workflow The boot workflow for management nodes (NCNs) is different from compute nodes or application nodes. They can PXE boot over the network or from local storage.\nSee NCN Boot Workflow for more information.\nNCN networking Non-compute nodes and compute nodes have different network interfaces used for booting.\nFor more information, see NCN Networking.\nNCN mounts and file systems The management nodes have specific file systems and mounts and use overlayfs.\nFor information, see NCN Mounts and File Systems.\nNCN packages The management nodes boot from images which have many (RPM) packages installed. The packages installed differ between the Kubernetes master and worker nodes versus the utility storage nodes.\nFor more information, see NCN Packages.\nNCN operating system releases All management nodes have an operating system based on SLE_HPC (SuSE High Performance Computing).\nFor more information, see NCN Operating System Releases.\ncloud-init Basecamp configuration Metal Basecamp is a cloud-init DataSource available on the LiveCD. Basecamp\u0026rsquo;s configuration file offers many inputs for various cloud-init scripts embedded within the NCN images.\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/",
	"title": "Upgrade CSM",
	"tags": [],
	"description": "",
	"content": "Upgrade CSM The process for upgrading Cray Systems Management (CSM) has many steps in multiple procedures which should be done in a specific order.\nAfter the upgrade of CSM software, the CSM health checks will validate the system before doing any other operational tasks like the check and update of firmware on system components. Once the CSM upgrade has completed, other product streams for the HPE Cray EX system can be installed or upgraded.\nPrepare for upgrade Upgrade management nodes and CSM services Validate CSM health Next topic Note: If problems are encountered during the upgrade, some of the topics do have their own troubleshooting sections, but there is also a general troubleshooting topic.\nPrepare for upgrade\nSee Prepare for Upgrade\nUpgrade management nodes and CSM services\nThe upgrade of CSM software will do a controlled, rolling reboot of all management nodes before updating the CSM services.\nThe upgrade is a guided process starting with Upgrade Management Nodes and CSM Services.\nValidate CSM health\nNOTE:\nBefore performing the health validation, be sure that at least 15 minutes have elapsed since the CSM services were upgraded. This allows the various Kubernetes resources to initialize and start. If the site does not use UAIs, skip UAS and UAI validation. If UAIs are used, there are products that configure UAS like Cray Analytics and Cray Programming Environment that must be working correctly with UAIs, and should be validated (the procedures for this are beyond the scope of this document) prior to validating UAS and UAI. Failures in UAI creation that result from incorrect or incomplete installation of these products will generally take the form of UAIs stuck in waiting state trying to set up volume mounts. Performing the Booting CSM barebones image test may be skipped if no compute nodes are available (that is, if all compute nodes are active running application workloads). See Validate CSM Health\nNext topic\nAfter completion of the validation of CSM health, the CSM product stream has been fully upgraded and configured. Refer to the HPE Cray EX System Software Getting Started Guide (S-8000) 22.07 on the HPE Customer Support Center for more information on other product streams to be upgraded and configured after CSM.\nNote: If a newer version of the HPE Cray EX HPC Firmware Pack (HFP) is available, then the next step would be to install HFP which will inform the Firmware Action Services (FAS) of the newest firmware available. Once FAS is aware that new firmware is available, then see Update Firmware with FAS.\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/stage_1/",
	"title": "Stage 1 - Ceph image upgrade",
	"tags": [],
	"description": "",
	"content": "Stage 1 - Ceph image upgrade Reminder: If any problems are encountered and the procedure or command output does not provide relevant guidance, see Relevant troubleshooting links for upgrade-related issues.\nProcedure Apply a workaround for the boot order.\nncn-m001# /usr/share/doc/csm/scripts/workarounds/boot-order/run.sh Run ncn-upgrade-ceph-nodes.sh for ncn-s001. Follow output of the script carefully. The script will pause for manual interaction.\nncn-m001# /usr/share/doc/csm/upgrade/1.2/scripts/upgrade/ncn-upgrade-ceph-nodes.sh ncn-s001 NOTE: The root password for the node may need to be reset after it is rebooted.\nKnown Issues:\nIf the below error is observed, then re-run the same command for the node upgrade. It will pick up at that point and continue.\n====\u0026gt; REDEPLOY_CEPH ... /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \u0026#34;/root/ceph.pub\u0026#34;Number of key(s) added: 1Now try logging into the machine, with: \u0026#34;ssh \u0026#39;root@ncn-s003\u0026#39;\u0026#34; and check to make sure that only the key(s) you wanted were added.Error EINVAL: Traceback (most recent call last): During the storage node rebuild, Ceph health may report HEALTH_WARN 1 daemons have recently crashed. This occurs occasionally as part of the shutdown process of the node being rebuilt. See Dump Ceph Crash Data.\nThe CSM validation test for detecting clock skew (Title: Check clock skew on k8s and storage nodes) can have a false failure. To determine if the clock for an NCN is in sync, run the following command:\nncn# timedatectl | awk /synchronized:/\u0026#39;{print $NF}\u0026#39; If the clock is in sync, the output will be yes; otherwise the output will be no. If the output is no, then wait a few minutes and try again. If it is still no after a few minutes, refer to Fix Broken Configuration.\nIMPORTANT: Ensure that the Ceph cluster is healthy prior to continuing.\nIf there are processes not running, then see Utility Storage Operations for operational and troubleshooting procedures.\nRepeat the previous steps for each other storage node, one at a time.\nAfter ncn-upgrade-ceph-nodes.sh has successfully run for all storage nodes, then rescan the SSH keys on all storage nodes.\nncn-m001# grep -oP \u0026#34;(ncn-s\\w+)\u0026#34; /etc/hosts | sort -u | xargs -t -i ssh {} \u0026#39;truncate --size=0 ~/.ssh/known_hosts\u0026#39; ncn-m001# grep -oP \u0026#34;(ncn-s\\w+)\u0026#34; /etc/hosts | sort -u | xargs -t -i ssh {} \u0026#39;grep -oP \u0026#34;(ncn-s\\w+|ncn-m\\w+|ncn-w\\w+)\u0026#34; /etc/hosts | sort -u | xargs -t -i ssh-keyscan -H \\{\\} \u0026gt;\u0026gt; /root/.ssh/known_hosts\u0026#39; Deploy node-exporter and alertmanager.\nNOTE: This procedure must run on a node running ceph-mon, which in most cases will be ncn-s001, ncn-s002, and ncn-s003. It only needs to be run once, not on every one of these nodes.\nDeploy node-exporter and alertmanager.\nncn-s# ceph orch apply node-exporter \u0026amp;\u0026amp; ceph orch apply alertmanager Expected output looks similar to the following:\nScheduled node-exporter update... Scheduled alertmanager update... Verify that node-exporter is running.\nIMPORTANT: There should be one node-exporter container per Ceph node.\nncn-s# ceph orch ps --daemon_type node-exporter Expected output on a system with three Ceph nodes should look similar to the following:\nNAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID node-exporter.ncn-s001 ncn-s001 running (57m) 3m ago 67m 0.18.1 registry.local/quay.io/prometheus/node-exporter:v1.2.2 53b6486665ad 8455ee5bfcd2 node-exporter.ncn-s002 ncn-s002 running (57m) 3m ago 67m 0.18.1 registry.local/quay.io/prometheus/node-exporter:v1.2.2 53b6486665ad d37aece375e1 node-exporter.ncn-s003 ncn-s003 running (57m) 3m ago 67m 0.18.1 registry.local/quay.io/prometheus/node-exporter:v1.2.2 53b6486665ad cb3ce40c10c0 The VERSION may be reported as \u0026lt;unknown\u0026gt;. This is not an error. The three things to verify in the output are:\nThe number of node-exporter pods matches the number of Ceph nodes. The STATUS for each pod is running. The REFRESHED time for each pod is low enough that it indicates the refresh did not happen before the ceph orch apply commands issued earlier in this procedure. Verify that alertmanager is running.\nIMPORTANT: There should be a single alertmanager container for the cluster.\nncn-s# ceph orch ps --daemon_type alertmanager Expected output looks similar to the following:\nNAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID alertmanager.ncn-s001 ncn-s001 running (66m) 3m ago 68m 0.21.0 registry.local/quay.io/prometheus/alertmanager:v0.21.0 926ce25ce099 58bceaf8577b The VERSION may be reported as \u0026lt;unknown\u0026gt;. This is not an error. The three things to verify in the output are:\nThere is exactly one alertmanager pod. The STATUS for each pod is running. The REFRESHED time for each pod is low enough that it indicates the refresh did not happen before the ceph orch apply commands issued earlier in this procedure. Update BSS to ensure that the Ceph images are loaded if a node is rebuilt.\nncn-m001# . /usr/share/doc/csm/upgrade/1.2/scripts/ceph/lib/update_bss_metadata.sh ncn-m001# update_bss_storage Stage completed All the Ceph nodes have been rebooted into the new image.\nThis stage is completed. Continue to Stage 2.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/kubernetes/troubleshoot_kubernetes_node_notready/",
	"title": "Troubleshoot Kubernetes Master or Worker node in NotReady state",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Kubernetes Master or Worker node in NotReady state Use this procedure to check if a Kubernetes master or worker node is in a NotReady state.\nIdentify the node in question Identify the node in NotReady state.\nncn-mw# kubectl get nodes Example output:\nNAME STATUS ROLES AGE VERSION ncn-m001 Ready control-plane,master 27h v1.20.13 ncn-m002 Ready control-plane,master 8d v1.20.13 ncn-m003 Ready control-plane,master 8d v1.20.13 ncn-w001 NotReady \u0026lt;none\u0026gt; 8d v1.20.13 ncn-w002 Ready \u0026lt;none\u0026gt; 8d v1.20.13 ncn-w003 Ready \u0026lt;none\u0026gt; 8d v1.20.13 Recovery steps Ensure that the node does not have an intentional NoSchedule taint.\nSee About Kubernetes Taints and Labels for more information about tainting and untainting a node.\nIf the node in question is not intentionally tainted causing the NotReady state, then proceed to the next step and attempt to restart the kubelet.\nRestart the kubelet.\nRun the following command on the node in a NotReady state.\nncn-mw# systemctl restart kubelet Ensure that the node is now in a Ready state.\nncn-mw# kubectl get nodes Example output:\nNAME STATUS ROLES AGE VERSION ncn-m001 Ready control-plane,master 27h v1.20.13 ncn-m002 Ready control-plane,master 8d v1.20.13 ncn-m003 Ready control-plane,master 8d v1.20.13 ncn-w001 Ready \u0026lt;none\u0026gt; 8d v1.20.13 ncn-w002 Ready \u0026lt;none\u0026gt; 8d v1.20.13 ncn-w003 Ready \u0026lt;none\u0026gt; 8d v1.20.13 "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/multiple_console_node_pods_on_the_same_worker/",
	"title": "Multiple Console Node Pods on the Same Worker",
	"tags": [],
	"description": "",
	"content": "Multiple Console Node Pods on the Same Worker In versions before CSM v1.3.0, there is no anti-affinity specified for the cray-console-node pods. This leads to the possibility of several pods running on the same worker node. This can be inconvenient during worker reboot operations and can reduce service reliability.\nManually edit deployment Pod scheduling behavior Manually edit deployment This procedure implements anti-affinity Kubernetes scheduling in versions prior to CSM v1.3.0 by manually editing the cray-console-node deployment. This will remain in effect until the service is reinstalled, downgraded, or upgraded. In CSM v1.3.0, the cray-console-node deployment already includes anti-affinity, so after an upgrade to that CSM version, no manual editing is required in order to implement pod anti-affinity for this deployment.\nBring up the deployment in an editor.\nncn-mw# kubectl -n services edit statefulset cray-console-node Find the spec.template.spec section.\nIt will look similar to the following:\nspec: podManagementPolicy: OrderedReady replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: app.kubernetes.io/instance: cray-console-node app.kubernetes.io/name: cray-console-node serviceName: cray-console-node template: metadata: annotations: service.cray.io/public: \u0026#34;true\u0026#34; creationTimestamp: null labels: app.kubernetes.io/instance: cray-console-node app.kubernetes.io/name: cray-console-node spec: containers: - env: - name: LOG_ROTATE_ENABLE value: \u0026#34;True\u0026#34; - name: LOG_ROTATE_FILE_SIZE Add a new affinity section before containers.\nThe new section contents are:\naffinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: app.kubernetes.io/name operator: In values: - cray-console-node topologyKey: kubernetes.io/hostname weight: 100 After the addition, the deployment should look similar to the following:\nspec: podManagementPolicy: OrderedReady replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: app.kubernetes.io/instance: cray-console-node app.kubernetes.io/name: cray-console-node serviceName: cray-console-node template: metadata: annotations: service.cray.io/public: \u0026#34;true\u0026#34; creationTimestamp: null labels: app.kubernetes.io/instance: cray-console-node app.kubernetes.io/name: cray-console-node spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: app.kubernetes.io/name operator: In values: - cray-console-node topologyKey: kubernetes.io/hostname weight: 100 containers: - env: - name: LOG_ROTATE_ENABLE value: \u0026#34;True\u0026#34; - name: LOG_ROTATE_FILE_SIZE Save the deployment and exit the editor.\nThe cray-console-node pods should restart one at a time until all have restarted. As they restart, Kubernetes will try to schedule them on different worker nodes.\nPod scheduling behavior The above manually edited deployment only prefers to schedule the pods on different nodes, meaning that if there are not enough valid nodes running at the time they are scheduled, then there still may be more than one running on a single node. If this is the case, then look at the health of the worker nodes.\nThe anti-affinity property is only examined when a new pod is started. If there are not enough healthy workers for all of the cray-console-node pods requested, causing multiple pods to run on the same worker, then these pods will not be automatically moved later to rebalance the deployment. If more healthy workers are added later, then the extra pods will need to be deleted manually in order to have them shifted to a different worker node.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/hms_ct_manual_run/",
	"title": "Running CT Tests Manually",
	"tags": [],
	"description": "",
	"content": "Running CT Tests Manually To run the tests manually:\nncn# /opt/cray/tests/ncn-resources/hms/hms-test/hms_run_ct_smoke_tests_ncn-resources.sh Examine the output. If one or more failures occur, investigate the cause of each failure. See the interpreting_hms_health_check_results documentation for more information.\nOtherwise, run the HMS functional tests.\nncn# /opt/cray/tests/ncn-resources/hms/hms-test/hms_run_ct_functional_tests_ncn-resources.sh Examine the output. If one or more failures occur, investigate the cause of each failure. See the interpreting_hms_health_check_results documentation for more information.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/adjust_ceph_pool_quotas/",
	"title": "Adjust Ceph Pool Quotas",
	"tags": [],
	"description": "",
	"content": "Adjust Ceph Pool Quotas Ceph pools are used for storing data. Use this procedure to set the Ceph pool quotas to determine the wanted number of bytes per pool. The smf Ceph pool now has replication factor of two.\nResolve Ceph health issues caused by a pool reaching its quota.\nPrerequisites This procedure requires administrative privileges.\nLimitations Currently, only smf includes a quota.\nProcedure Log in as root on ncn-m001.\nDetermine the available space.\nIn the following example, the 3.5 TiB is 33 percent of the 21 TiB total. Ceph keeps three copies of data, so a 3.5 TiB quota is actually provisioning 7.0 TiB of storage, which is 33 percent of 21 TiB.\nncn-m001# ceph df detail Example output:\nRAW STORAGE: CLASS SIZE AVAIL USED RAW USED %RAW USED ssd 21 TiB 21 TiB 122 GiB 134 GiB 0.62 TOTAL **21 TiB** 21 TiB 122 GiB 134 GiB 0.62 POOLS: POOL ID STORED OBJECTS USED %USED MAX AVAIL QUOTA OBJECTS QUOTA BYTES DIRTY USED COMPR UNDER COMPR cephfs_data 1 1.0 MiB 74 6.2 MiB 0 6.6 TiB N/A N/A 74 0 B 0 B cephfs_metadata 2 22 MiB 28 68 MiB 0 6.6 TiB N/A N/A 28 0 B 0 B .rgw.root 3 3.5 KiB 8 384 KiB 0 6.6 TiB N/A N/A 8 0 B 0 B default.rgw.buckets.data 4 22 GiB 22.57k 65 GiB 0.32 6.6 TiB N/A N/A 22.57k 0 B 0 B default.rgw.control 5 0 B 8 0 B 0 6.6 TiB N/A N/A 8 0 B 0 B default.rgw.buckets.index 6 197 KiB 13 197 KiB 0 6.6 TiB N/A N/A 13 0 B 0 B default.rgw.meta 7 19 KiB 107 4.2 MiB 0 6.6 TiB N/A N/A 107 0 B 0 B default.rgw.log 8 0 B 207 0 B 0 6.6 TiB N/A N/A 207 0 B 0 B kube 9 15 GiB 6.48k 28 GiB 0.14 6.6 TiB N/A N/A 6.48k 17 GiB 33 GiB smf 10 19 TiB 7.88k 28 GiB 0.14 9.9 TiB N/A **3.5 TiB** 7.88k 9.4 GiB 19 GiB default.rgw.buckets.non-ec 11 0 B 0 0 B 0 9.9 TiB N/A N/A 0 0 B 0 B Determine the maximum quota percentage.\n6TiB must be left for Kubernetes, Ceph RGW, and other services. To calculate the quota percentage, use the following equation:\n(TOTAL_SIZE-6)/TOTAL_SIZE Using the example output in step 2, the following would be the quota percentage:\n(21-6)/21 = .71 Edit the quota percentage as wanted.\nDo not exceed the percentage determined in the previous step.\nncn-s001# vim /etc/ansible/ceph-rgw-users/ceph-pool-quotas.yml Example ceph-pool-quotas.yml:\nceph_pool_quotas: - pool_name: smf percent_of_total: .71 \u0026lt;-- Change this to desired percentage replication_factor: 2.0 Run the ceph-pool-quotas.yml playbook from ncn-s001.\nncn-s001# ansible-playbook /etc/ansible/ceph-rgw-users/ceph-pool-quotas.yml View the quota/pool usage.\nLook at the USED and QUOTA BYTES columns to view usage and the new quota setting.\nncn-m001# ceph df detail Example output:\nRAW STORAGE: CLASS SIZE AVAIL USED RAW USED %RAW USED ssd 21 TiB 21 TiB 122 GiB 134 GiB 0.62 TOTAL **21 TiB** 21 TiB 122 GiB 134 GiB 0.62 POOLS: POOL ID STORED OBJECTS USED %USED MAX AVAIL QUOTA OBJECTS QUOTA BYTES DIRTY USED COMPR UNDER COMPR cephfs_data 1 1.0 MiB 74 6.2 MiB 0 6.6 TiB N/A N/A 74 0 B 0 B cephfs_metadata 2 22 MiB 28 68 MiB 0 6.6 TiB N/A N/A 28 0 B 0 B .rgw.root 3 3.5 KiB 8 384 KiB 0 6.6 TiB N/A N/A 8 0 B 0 B default.rgw.buckets.data 4 22 GiB 22.57k 65 GiB 0.32 6.6 TiB N/A N/A 22.57k 0 B 0 B default.rgw.control 5 0 B 8 0 B 0 6.6 TiB N/A N/A 8 0 B 0 B default.rgw.buckets.index 6 197 KiB 13 197 KiB 0 6.6 TiB N/A N/A 13 0 B 0 B default.rgw.meta 7 19 KiB 107 4.2 MiB 0 6.6 TiB N/A N/A 107 0 B 0 B default.rgw.log 8 0 B 207 0 B 0 6.6 TiB N/A N/A 207 0 B 0 B kube 9 15 GiB 6.48k 28 GiB 0.14 6.6 TiB N/A N/A 6.48k 17 GiB 33 GiB smf 10 19 TiB 7.88k 28 GiB 0.14 9.9 TiB N/A **7.4 TiB** 7.88k 9.4 GiB 19 GiB default.rgw.buckets.non-ec 11 0 B 0 0 B 0 9.9 TiB N/A N/A 0 0 B 0 B "
},
{
	"uri": "/docs-csm/en-12/operations/system_management_health/grafana_dashboards_by_component/",
	"title": "Grafana Dashboards by Component",
	"tags": [],
	"description": "",
	"content": "Grafana Dashboards by Component This document show the dashboards that are available for each component in Grafana.\nA Grafana dashboard is a powerful open source analytical and visualization tool that consists of multiple individual panels arranged in a grid. The panels interact with configured data sources, including the following:\nAWS CloudWatch Prometheus NOTE: For known issues around some of the Grafana dashboards, see Troubleshoot Grafana Dashboards.\nCeph Cluster Host Overview Details MDS Performance OSD Overview Device Details Pool Overview Details RBD Overview RGW Overview Instance Details CoreDNS CoreDNS ETCD Main Clusters Istio Mesh Performance Pilot Service Workload Kea DHCP Kubernetes API server Compute Resources Compute Resources / Cluster Compute Resources / Namespace (Pods) Compute Resources / Namespace (Workloads) Compute Resources / Node (Pods) Compute Resources / Pod Compute Resources / Workload Controller Manager kubelet Networking Networking / Cluster Networking / Namespace (Pods) Networking / Namespace (Workload) Networking / Pod Networking / Workload Persistent Volumes Proxy Scheduler StatefulSets Nodes Exporter Full Main Postgres PostgreSQL Statistics Prometheus Prometheus Use Method Cluster Node "
},
{
	"uri": "/docs-csm/en-12/operations/system_layout_service/add_an_alias_to_a_service/",
	"title": "Add an alias to a service",
	"tags": [],
	"description": "",
	"content": "Add an alias to a service Add an alias for an existing service to the IP address reservations in the System Layout Service (SLS). Adding these IP addresses will propagate the data needed for the Domain Name Service (DNS).\nPrerequisites This procedure requires administrative privileges.\nProcedure This example will add an alias to the pbs_service in the Node Management Network (NMN).\nGet an API token.\nncn-mw# TOKEN=$(curl -s -k -S -d grant_type=client_credentials \\ -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth \\ -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Retrieve the SLS data for the network the service resides in.\nncn-mw# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api-gw-service-nmn.local/apis/sls/v1/networks/NMN|jq \u0026gt; NMN.json Make a backup copy of the file.\nncn-mw# cp -v NMN.json NMN.json.bak Edit the NMN.json file and add the desired alias in the ExtraProperties.Subnets section.\nFor example, after editing it may look similar to the following:\n{ \u0026#34;Aliases\u0026#34;: [ \u0026#34;pbs-service\u0026#34;, \u0026#34;pbs-service-nmn\u0026#34;, \u0026#34;pbs_service.local\u0026#34;, \u0026#34;test-alias\u0026#34; ], \u0026#34;Comment\u0026#34;: \u0026#34;pbs-service,pbs-service-nmn\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.252.2.5\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;pbs_service\u0026#34; } Upload the updated NMN.json file to SLS.\nncn-mw# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; --header \u0026#34;Content-Type: application/json\u0026#34; --request PUT \\ --data @NMN.json https://api-gw-service-nmn.local/apis/sls/v1/networks/NMN Verify that DNS records were created.\nIt will take about five minutes before any records will show up.\nFor example:\nncn-mw# nslookup test-alias Example output:\nServer: 10.92.100.225 Address: 10.92.100.225#53 Name: test-alias Address: 10.252.2.5 "
},
{
	"uri": "/docs-csm/en-12/operations/spire/restore_spire_postgres_without_a_backup/",
	"title": "Restore Spire Postgres without an Existing Backup",
	"tags": [],
	"description": "",
	"content": "Restore Spire Postgres without an Existing Backup Reinstall the Spire Helm chart in the event that spire-postgres databases cannot be restored from a backup.\nUninstall Spire Uninstall the Spire Helm chart.\nncn-mw# helm uninstall -n spire spire Wait for the pods in the spire namespace to terminate. Once that is done, remove the spire-data-server PVCs.\nncn-mw# kubectl get pvc -n spire | grep spire-data-spire-server | awk \u0026#39;{print $1}\u0026#39; | xargs kubectl delete -n spire pvc Disable spire-agent on all of the Kubernetes NCNs (all worker nodes and master nodes) and delete the join data.\nncn-mw# for ncn in $(kubectl get nodes -o name | cut -d\u0026#39;/\u0026#39; -f2); do ssh \u0026#34;${ncn}\u0026#34; systemctl stop spire-agent; ssh \u0026#34;${ncn}\u0026#34; rm /root/spire/data/svid.key /root/spire/agent_svid.der /root/spire/bundle.der; done Re-install the Spire Helm Chart Follow the Redeploying a Chart procedure with the following specifications:\nName of chart to be redeployed: spire\nBase name of manifest: sysmgmt\nWhen reaching the step to update customizations, no edits need to be made to the customizations file.\nWhen reaching the step to validate that the redeploy was successful, perform the following step:\nOnly follow this step as part of the previously linked chart redeploy procedure.\nVerify that all Spire pods have started.\nThis step may take a few minutes due to a number of pods requiring other pods to be up.\nncn-mw# kubectl get pods -n spire Restart all compute nodes and User Access Nodes (UANs).\nCompute nodes and UANs get their join token on boot from the Boot Script Service (BSS). Their old SVID data is no longer valid and a reboot is required in order for them to re-join Spire.\n"
},
{
	"uri": "/docs-csm/en-12/operations/system_configuration_service/set_bmc_credentials/",
	"title": "Set BMC Credentials",
	"tags": [],
	"description": "",
	"content": "Set BMC Credentials Use the System Configuration Service (SCSD) to set the BMCs credentials to unique values, or set them all to the same value. Redfish BMCs get installed into the system with default credentials. Once the machine is shipped, the Redfish credentials must be changed on all BMCs. This is done using System Configuration Service (SCSD) through the Cray CLI.\nImportant: If the credentials for other devices need to be changed, refer to the following device-specific credential changing procedures:\nTo change liquid-cooled BMC credentials, refer to Change Cray EX Liquid-Cooled Cabinet Global Default Password. To change air-cooled Node BMC credentials, refer to Change Air-Cooled Node BMC Credentials. To change ServerTech PDU credentials, refer to Change Credentials on ServerTech PDUs. To change Slingshot switch BMC credentials, refer to \u0026ldquo;Change Rosetta Login and Redfish API Credentials\u0026rdquo; in the Slingshot Operations Guide (\u0026gt; 1.6.0). Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Procedure Determine the live BMCs in the system.\nThe list of BMCs must include air-cooled compute BMCs, air-cooled High Speed Network (HSN) switch BMCs, liquid-cooled compute BMCs, liquid-cooled switch BMCs, and liquid-cooled chassis BMCs.\nncn-m001# for fff in `cray hsm inventory redfishEndpoints list --format json \\ | jq \u0026#39;.RedfishEndpoints[] | select(.FQDN | contains(\u0026#34;-rts\u0026#34;) | not) | \\ select(.DiscoveryInfo.LastDiscoveryStatus == \u0026#34;DiscoverOK\u0026#34;) | select(.Enabled==true) | .ID\u0026#39; \\ | sed \u0026#39;s/\u0026#34;//g\u0026#39;` do echo \u0026#34;Pinging ${fff}...\u0026#34; ; xxx=`curl -k https://${fff}/redfish/v1/` [[ \u0026#34;${xxx}\u0026#34; != \u0026#34;\u0026#34; ]] \u0026amp;\u0026amp; echo \u0026#34;PRESENT\u0026#34; || echo \u0026#34;NOT PRESENT\u0026#34; done Create a new JSON file containing the BMC credentials for all BMCs returned in the previous step.\nSelect one of the options below to set the credentials for the BMCs:\nSet all BMCs with the same credentials.\nExample file contents:\n{ \u0026#34;Force\u0026#34;: false, \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;new.root.password\u0026#34; \u0026#34;Targets\u0026#34;: [ \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;x0c0s1b0\u0026#34; ] } Set all BMCs with different credentials.\nExample file contents:\n{ \u0026#34;Force\u0026#34;: true, \u0026#34;Targets\u0026#34;: [ { \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;Creds\u0026#34;: { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;pw-x0c0s0b0\u0026#34; } }, { \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s0b1\u0026#34;, \u0026#34;Creds\u0026#34;: { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;pw-x0c0s0b1\u0026#34; } } ] } Apply the new BMC credentials.\nUse only one of the following options depending on how the credentials are being set:\nApply global credentials to BMCs with the same credentials.\nncn-m001# cray scsd bmc globalcreds create ./bmc_creds_glb.json Apply discrete credentials to BMCs with different credentials.\nncn-m001# cray scsd bmc discreetcreds create ./bmc_creds_dsc.json Troubleshooting: If either command has any components that do not have the status of OK, they must be retried until they work, or the retries are exhausted and noted as failures. Failed modules need to be taken out of the system until they are fixed.\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/add_ldap_user_federation/",
	"title": "Add LDAP User Federation",
	"tags": [],
	"description": "",
	"content": "Add LDAP User Federation Add LDAP user federation using the Keycloak localization tool.\nPrerequisites System domain name Procedure Prerequisites LDAP user federation is not currently configured in Keycloak. For example, if it was not configured in Keycloak when the system was initially installed or the LDAP user federation was removed.\nSystem domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system\u0026rsquo;s fully qualified domain name (FQDN).\nThe FQDN can be found by running the following command on any Kubernetes NCN.\nncn-mw# kubectl get secret site-init -n loftsman -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d | yq r - spec.network.dns.external Example output:\nsystem.hpc.amslabs.hpecorp.net Be sure to modify the example URLs on this page by replacing SYSTEM_DOMAIN_NAME with the actual value found using the above command.\nProcedure Prepare to edit the customizations.yaml file.\nIf the customizations.yaml file is managed in an external Git repository (as recommended), then clone a local working tree. Replace the \u0026lt;URL\u0026gt; value in the following command before running it.\nncn-mw# git clone \u0026lt;URL\u0026gt; /root/site-init ncn-mw# cd /root/site-init If there is not a backup of site-init, perform the following steps to create a new one using the values stored in the Kubernetes cluster.\nSet the CSM_DISTDIR variable to the path to the unpacked CSM release tarball.\nSee Download and extract CSM product release.\nCreate a new site-init directory using the CSM tarball.\nncn-mw# cp -r ${CSM_DISTDIR}/shasta-cfg/* /root/site-init ncn-mw# cd /root/site-init Extract customizations.yaml from the site-init Kubernetes secret.\nncn-mw# kubectl -n loftsman get secret site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d - \u0026gt; customizations.yaml Extract the certificate and key used to create the sealed secrets.\nncn-mw# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39; | base64 -d - \u0026gt; certs/sealed_secrets.crt ncn-mw# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.key}\u0026#39; | base64 -d - \u0026gt; certs/sealed_secrets.key NOTE All subsequent steps of this procedure should be performed within the /root/site-init directory created in this step.\nRepopulate the keycloak_users_localize and cray-keycloak sealed secrets in the customizations.yaml file with the desired configuration.\nUpdate the LDAP settings with the desired configuration. LDAP connection information is stored in the keycloak-users-localize secret in the customizations.yaml file.\nThe ldap_connection_url key is required and is set to an LDAP URL. The ldap_bind_dn and ldap_bind_credentials keys are optional. If the LDAP server requires authentication. then the bind DN and credentials are set in these keys respectively. For example:\ncray-keycloak: generate: name: keycloak-certs data: - type: static_b64 args: name: certs.jks value: /u3+7QAAAAIAAAAA5yXvSDt11bGXyBA9M2iy0/5i1Tg= keycloak_users_localize: generate: name: keycloak-users-localize data: - type: static args: name: ldap_connection_url value: \u0026#34;ldaps://my_ldap.my_org.test\u0026#34; - type: static args: name: ldap_bind_dn value: \u0026#34;cn=my_admin\u0026#34; - type: static args: name: ldap_bind_credentials value: \u0026#34;my_ldap_admin_password\u0026#34; The example above puts an empty certs.jks in the cray-keycloak sealed secret. The next step will generate certs.jks.\nOther LDAP configuration settings are set in the spec.kubernetes.services.cray-keycloak-users-localize field in the customizations.yaml file.\nA list of the fields follows. The format of the entries in this list is:\n* \u0026lt;cray-keycloak-users-localize chart option name\u0026gt; : \u0026lt;description\u0026gt; - default: \u0026lt;the default value if not overridden in customizations.yaml\u0026gt; - type: \u0026lt;type that the value in customizations.yaml has to be. e.g., if type is string and a number is entered then you need to quote it\u0026gt; - allowed values: \u0026lt;if only certain values are allowed they are listed here\u0026gt; The fields are:\n* ldapProviderId : The Keycloak provider ID for the component. This must be \u0026#34;ldap\u0026#34; - default: ldap - type: string * ldapFederationName : The name of the LDAP provider in Keycloak. If a provider with this name already exists then this tool will not create a new provider. - default: shasta-user-federation-ldap - type: string * ldapPriority : The priority of this provider when looking up users or adding a user. - default: 1 - type: string * ldapEditMode : If you want to be able to create or change users in Keycloak and have them created or modified in the LDAP server, and the LDAP server allows it, then this can be changed. - default: READ_ONLY - type: string - allowed values: `READ_ONLY`, `WRITEABLE`, or `UNSYNCED` * ldapSyncRegistrations : If true, then newly created users will be created in the LDAP server. - default: false - type: string - allowed values: true or false * ldapVendor: This determines some defaults for what mappers are created by default. - default: other - type: string - allowed values: Active Directory, Red Hat Directory Server, Tivoli, Novell eDirectory, or other * ldapUsernameLDAPAttribute: The LDAP attribute to map to the username in Keycloak. - default: uid - type: string * ldapRdnLDAPAttribute: The LDAP attribute being used as the users RDN. - default: uid - type: string * ldapUuidLDAPAttribute: The LDAP attribute being used as a unique ID. - default: uid - type: string * ldapUserObjectClasses: The object classes for user entries. - default: posixAccount - type: comma-separated string * ldapAuthType: Set to \u0026#34;none\u0026#34; if the LDAP server allows anonymous search for users and groups, otherwise set to \u0026#34;simple\u0026#34; to bind. - default: none - type: string - allowed values: none or simple * ldapSearchBase: The DN for the base entry to search for users and groups. - default: cn=default - type: string * ldapSearchScope: The search scope to use when searching for users or groups: 2 for subtree, 1 for onelevel - default: 2 - type: string - allowed values: 1 or 2 * ldapUseTruststoreSpi: Determines if the truststore is used to validate the server certificate when connecting to the server. - default: ldapsOnly - type: string - allowed values: ldapsOnly, always, never * ldapConnectionPooling: If true then Keycloak will use a connection pool of LDAP connections. - default: true - type: string - allowed values: true or false * ldapPagination: Set to true if the LDAP server supports or requires use of the paging extension. - default: true - type: string - allowed values: true or false * ldapAllowKerberosAuthentication: - Set to true to enable HTTP authentication of users with SPNEGO/Kerberos tokens. - default: false - type: string * ldapBatchSizeForSync: Count of LDAP users to be imported from LDAP to Keycloak in a single transaction. - default: 4000 - type: string * ldapFullSyncPeriod: If a positive number, this is the number of seconds between automatic full user synchronization operations; if negative then full user synchronization operations will not be done automatically. - default: -1 - type: string * ldapChangedSyncPeriod: f a positive number, this is the number of seconds between automatic changed user synchronization operations; if negative then changed user synchronization operations will not be done automatically. - default: -1 - type: string * ldapDebug: Set to true to enable extra logging of LDAP operations. - default: true - allowed values: true or false * ldapUserAttributeMappers: Extra attribute mappers to create so that users have attributes required by Shasta software. The Keycloak attribute that the LDAP attribute maps to will be the same. - default: [uidNumber, gidNumber, loginShell, homeDirectory] - type: list of strings * ldapUserAttributeMappersToRemove: These attribute mappers will be removed, to be used in the case where the default attribute mappers are not appropriate. For example, this could be used to remove the email mapper if email addresses are not unique. - default: [] - type: list of strings * ldapGroupNameLDAPAttribute: The LDAP attribute to map to the group name in Keycloak. - default: cn - type: string * ldapGroupObjectClass: The object classes for group entries. - default: posixGroup - type: comma-separated string * ldapPreserveGroupInheritance: Whether group inheritance should be propagated to Keycloak or not. - default: false - type: string - allowed values: true or false * ldapMembershipLDAPAttribute: Name of the LDAP attribute that refers to the group members. - default: memberUid - type: string * ldapMembershipAttributeType: If the member attribute contains the DN for the user, then set this to DN. If the member attribute is the UID of the entry then set this to UID. - default: UID - type: string - allowed values: UID or DN * ldapMembershipUserLDAPAttribute: If the ldapMembershipAttributeType is UID then this is the LDAP attribute containing the UID value, otherwise this is ignored. - default: uid - type: string * ldapGroupsLDAPFilter: Extra filter to include when searching for group entries. If this is not the empty string the value must start with the ( character and end with ). - default: \u0026#34;\u0026#34; - type: string * ldapUserRolesRetrieveStrategy: Defines how to retrieve groups for a user. - default: LOAD_GROUPS_BY_MEMBER_ATTRIBUTE - type: string - allowed values: LOAD_GROUPS_BY_MEMBER_ATTRIBUTE, GET_GROUPS_FROM_USER_MEMBEROF_ATTRIBUTE, LOAD_GROUPS_BY_MEMBER_ATTRIBUTE_RECURSIVELY * ldapMappedGroupAttributes: Attributes of the group that will be added as attributes of the user. Some Shasta REST API operations require the user to have a gidNumber and this adds that attribute from the LDAP group. - default: cn,gidNumber,memberUid - type: comma-separated string * ldapDropNonExistingGroupsDuringSync: If true, groups that are not in LDAP will be deleted when synchronizing. - default: false - type: string - allowed values: true or false * ldapDoFullSync: Tells the HPE Cray EX Keycloak localization tool to perform an immediate full user synchronization after configuring the LDAP integration. - default: true - type: string * ldapRoleMapperDn: If this is an empty string then a role mapper is not created, otherwise this the the DN used as the search base to find role entries. - default: \u0026#34;\u0026#34; - type: string * ldapRoleMapperRoleNameLDAPAttribute: The LDAP attribute to map to the role name in Keycloak. - default: cn - type: string * ldapRoleMapperRoleObjectClasses: The object classes for role entries. - default: groupOfNames - type: string * ldapRoleMapperLDAPAttribute: Name of the LDAP attribute that refers to the group members. - default: member - type: string * ldapRoleMapperMemberAttributeType: If the member attribute contains the DN for the user, then set this to DN. If the member attribute is the UID of the entry then set this to UID. - default: DN - type: string - allowed values: UID or DN * ldapRoleMapperUserLDAPAttribute: If the ldapRoleMapperMemberAttributeType is UID then this is the LDAP attribute containing the UID value, otherwise this is ignored. - default: sAMAccountName - type: string * ldapRoleMapperRolesLDAPFilter: Extra filter to include when searching for group entries. If this is not the empty string the value must start with the ( character and end with ). - default: \u0026#34;\u0026#34; - type: string * ldapRoleMapperMode: Specifies how to retrieve roles for the user. - default: READ_ONLY - allowed values: READ_ONLY, LDAP_ONLY, or IMPORT * ldapRoleMapperStrategy: Defines how to retrieve roles for a user. - default: LOAD_ROLES_BY_MEMBER_ATTRIBUTE - type: string - allowed values: LOAD_ROLES_BY_MEMBER_ATTRIBUTE, GET_ROLES_FROM_MEMBEROF_ATTRIBUTE, or LOAD_ROLES_BY_MEMBER_ATTRIBUTE_RECURSIVELY * ldapRoleMapperMemberOfLDAPAttribute: Only used when ldapRoleMapperStrategy is GET_ROLES_FROM_MEMBEROF_ATTRIBUTE where it is the LDAP attribute in the user entry that contains the roles that the user has. - default: memberOf - type: string * ldapRoleMapperUseRealmRolesMapping: If true then LDAP role mappings will be mapped to realm role mappings in Keycloak, otherwise the LDAP role mappings will be mapped to client role mappings. - default: false - type: string - allowed values: true or false * ldapRoleMapperClientId: If ldapRoleMapperUseRealmRolesMapping is false then this is the client ID to apply the roles to. - default: shasta - type: string (Optional) Add the LDAP CA certificate in the certs.jks section of customizations.yaml.\nIf LDAP requires TLS (recommended), update the cray-keycloak sealed secret value by supplying a base-64-encoded Java KeyStore (JKS) that contains the CA certificate that signed with the LDAP server\u0026rsquo;s host key. The password for the JKS file must be password.\nAdministrators may use the keytool command from the openjdk:11-jre-slim container image packaged with CSM to create a JKS file that includes a PEM-encoded CA certificate to verify the LDAP host(s).\nLoad the openjdk container image.\nNOTE Requires a properly configured Docker or Podman environment.\nncn-mw# ${CSM_DISTDIR}/hack/load-container-image.sh artifactory.algol60.net/csm-docker/stable/docker.io/library/openjdk:11-jre-slim Troubleshooting:\nIf the output shows the skopeo.tar file cannot be found, then ensure that the $CSM_DISTDIR directory looks contains the artifactory.algol60.net directory which includes the originally installed docker images.\nThe following is an example of the skopeo.tar file not being found:\n++ podman load -q -i ./hack/../vendor/skopeo.tar ++ sed -e \u0026#39;s/^.*: //\u0026#39; + SKOPEO_IMAGE= If the following overlay error is returned, it could be caused by an earlier podman invocation using a different configuration:\n\u0026#34;ERRO[0000] [graphdriver] prior storage driver overlay failed: \u0026#39;overlay\u0026#39; is not supported over overlayfs, a mount_program is required: backing file system is unsupported for this graph driver\u0026#34; To recover podman, move the overlay directories to a backup folder as follows:\nncn-mw# mkdir /var/lib/containers/storage/backup ncn-mw# mv /var/lib/containers/storage/overlay* /var/lib/containers/storage/backup This should allow load-container-images.sh to succeed.\nCreate (or update) cert.jks with the PEM-encoded CA certificate for an LDAP host.\nIMPORTANT: Replace \u0026lt;ca-cert.pem\u0026gt; and \u0026lt;alias\u0026gt; before running the command.\nncn-mw# podman run --rm -v \u0026#34;$(pwd):/data\u0026#34; artifactory.algol60.net/csm-docker/stable/docker.io/library/openjdk:11-jre-slim keytool \\ -importcert -trustcacerts -file /data/\u0026lt;ca-cert.pem\u0026gt; -alias \u0026lt;alias\u0026gt; -keystore /data/certs.jks \\ -storepass password -noprompt Set variables for the LDAP server.\nIn the following example, the LDAP server has the hostname dcldap2.hpc.amslabs.hpecorp.net and is using the port 636.\nncn-mw# LDAP=dcldap2.hpc.amslabs.hpecorp.net ncn-mw# PORT=636 Get the issuer certificate for the LDAP server at port 636.\nUse openssl s_client to connect and show the certificate chain returned by the LDAP host.\nncn-mw# openssl s_client -showcerts -connect $LDAP:${PORT} \u0026lt;/dev/null Generate cacert.pem containing the issuer\u0026rsquo;s certificate.\nEither manually extract (cut/paste) the issuer\u0026rsquo;s certificate into cacert.pem, or try the following commands to create it automatically.\nNOTE The following commands were verified using OpenSSL version 1.1.1d and use the -nameopt RFC2253 option to ensure consistent formatting of distinguished names. Older versions of OpenSSL may not support -nameopt on the s_client command or may use a different default format. However, administrators should be able to extract the issuer certificate manually from the output of the above openssl s_client example, if necessary.\nObserve the issuer\u0026rsquo;s DN.\nFor example:\nncn-mw# openssl s_client -showcerts -nameopt RFC2253 -connect $LDAP:${PORT} \u0026lt;/dev/null 2\u0026gt;/dev/null | grep issuer= | sed -e \u0026#39;s/^issuer=//\u0026#39; Example output:\nCN=DigiCert Global G2 TLS RSA SHA256 2020 CA1,O=DigiCert Inc,C=US Extract the issuer\u0026rsquo;s certificate using the awk command.\nNOTE The issuer DN is properly escaped as part of the awk pattern below. If the value being used is different, then be sure to escape it properly!\nncn-mw# openssl s_client -showcerts -nameopt RFC2253 -connect $LDAP:${PORT} \u0026lt;/dev/null 2\u0026gt;/dev/null | awk \u0026#39;/s:CN=DigiCert Global G2 TLS RSA SHA256 2020 CA1,O=DigiCert Inc,C=US/,/END CERTIFICATE/\u0026#39; | awk \u0026#39;/BEGIN CERTIFICATE/,/END CERTIFICATE/\u0026#39; \u0026gt; cacert.pem Verify the issuer\u0026rsquo;s certificate was properly extracted and saved in cacert.pem.\nncn-mw# cat cacert.pem Expected output looks similar to the following:\n-----BEGIN CERTIFICATE----- MIIDvTCCAqWgAwIBAgIUYxrG/PrMcmIzDuJ+U1Gh8hpsU8cwDQYJKoZIhvcNAQEL BQAwbjELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAldJMQwwCgYDVQQKDANIUEUxEDAO BgNVBAsMB0hQQy9NQ1MxFDASBgNVBAMMC0RhdGEgQ2VudGVyMRwwGgYJKoZIhvcN AQkBFg1kY29wc0BocGUuY29tMB4XDTIwMTEyNDIwMzM0MVoXDTMwMTEyMjIwMzM0 MVowbjELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAldJMQwwCgYDVQQKDANIUEUxEDAO BgNVBAsMB0hQQy9NQ1MxFDASBgNVBAMMC0RhdGEgQ2VudGVyMRwwGgYJKoZIhvcN AQkBFg1kY29wc0BocGUuY29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC AQEAuBIZkKitHHVQHymtaQt4D8ZhG4qNJ0cTsLhODPMtVtBjPZp59e+PWzbc9Rj5 +wfjLGteK6/fNJsJctWlS/ar4jw/xBIPMk5pg0dnkMT2s7lkSCmyd9Uib7u6y6E8 yeGoGcb7I+4ZI+E3FQV7zPact6b17xmajNyKrzhBGEjYucYJUL5iTgZ6a7HOZU2O aQSXe7ctiHBxe7p7RhHCuKRrqJnxoohakloKwgHHzDLFQzX/5ADp1hdJcduWpaXY RMBu6b1mhmwo5vmc+fDnfUpl5/X4i109r9VN7JC7DQ5+JX8u9SHDGLggBWkrhpvl bNXMVCnwnSFfb/rnmGO7rdJSpwIDAQABo1MwUTAdBgNVHQ4EFgQUVg3VYExUAdn2 WE3e8Xc8HONy/+4wHwYDVR0jBBgwFoAUVg3VYExUAdn2WE3e8Xc8HONy/+4wDwYD VR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAWLDQLB6rrmK+gwUY+4B7 0USbQK0JkLWuc0tCfjTxNQTzFb75PeH+GH21QsjUI8VC6QOAAJ4uzIEV85VpOQPp qjz+LI/Ej1xXfz5ostZQu9rCMnPtVu7JT0B+NV7HvgqidTfa2M2dw9yUYS2surZO 8S0Dq3Bi6IEhtGU3T8ZpbAmAp+nNsaJWdUNjD4ECO5rAkyA/Vu+WyMz6F3ZDBmRr ipWM1B16vx8rSpQpygY+FNX4e1RqslKhoyuzXfUGzyXux5yhs/ufOaqORCw3rJIx v4sTWGsSBLXDsFM3lBgljSAHfmDuKdO+Qv7EqGzCRMpgSciZihnbQoRrPZkOHUxr NA== -----END CERTIFICATE----- Create certs.jks.\nncn-mw# podman run --rm -v \u0026#34;$(pwd):/data\u0026#34; artifactory.algol60.net/csm-docker/stable/docker.io/library/openjdk:11-jre-slim keytool -importcert \\ -trustcacerts -file /data/cacert.pem -alias cray-data-center-ca -keystore /data/certs.jks \\ -storepass password -noprompt Create certs.jks.b64 by base-64 encoding certs.jks.\nncn-mw# base64 certs.jks \u0026gt; certs.jks.b64 Inject and encrypt certs.jks.b64 into customizations.yaml.\nncn-mw# cat \u0026lt;\u0026lt;EOF | yq w - \u0026#39;data.\u0026#34;certs.jks\u0026#34;\u0026#39; \u0026#34;$(\u0026lt;certs.jks.b64)\u0026#34; | \\ yq r -j - | /root/site-init/utils/secrets-encrypt.sh | \\ yq w -f - -i /root/site-init/customizations.yaml \\ spec.kubernetes.sealed_secrets.cray-keycloak { \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;keycloak-certs\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: {} } EOF Prepare to generate sealed secrets.\nSecrets are stored in customizations.yaml as SealedSecret resources (encrypted secrets), which are deployed by specific charts and decrypted by the sealed secrets operator. In order for that to happen, those Secrets must first be seeded, generated, and encrypted.\nncn-mw# ./utils/secrets-reencrypt.sh customizations.yaml ./certs/sealed_secrets.key ./certs/sealed_secrets.crt Encrypt the static values in the customizations.yaml file after making changes.\nThe following command must be run within the site-init directory.\nncn-mw# ./utils/secrets-seed-customizations.sh customizations.yaml Expected output looks similar to:\nCreating Sealed Secret keycloak-certs Generating type static_b64... Creating Sealed Secret keycloak-master-admin-auth Generating type static... Generating type static... Generating type randstr... Generating type static... Creating Sealed Secret cray_reds_credentials Generating type static... Generating type static... Creating Sealed Secret cray_meds_credentials Generating type static... Creating Sealed Secret cray_hms_rts_credentials Generating type static... Generating type static... Creating Sealed Secret vcs-user-credentials Generating type randstr... Generating type static... Creating Sealed Secret generated-platform-ca-1 Generating type platform_ca... Creating Sealed Secret pals-config Generating type zmq_curve... Generating type zmq_curve... Creating Sealed Secret munge-secret Generating type randstr... Creating Sealed Secret slurmdb-secret Generating type static... Generating type static... Generating type randstr... Generating type randstr... Creating Sealed Secret keycloak-users-localize Generating type static... Decrypt the sealed secret to verify it was generated correctly.\nncn-mw# ./utils/secrets-decrypt.sh keycloak_users_localize | jq -r \u0026#39;.data.ldap_connection_url\u0026#39; | base64 --decode Expected output looks similar to the following:\nldaps://my_ldap.my_org.test Upload the modified customizations.yaml file to Kubernetes.\nncn-mw# kubectl delete secret -n loftsman site-init ncn-mw# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml Re-apply the cray-keycloak Helm chart with the updated customizations.yaml file.\nFollow the Redeploying a Chart procedure with the following specifications:\nName of chart to be redeployed: cray-keycloak\nBase name of manifest: platform\nInstead of downloading the customizations from Kubernetes, use the updated customizations.yaml file.\nWhen reaching the step to validate that the redeploy was successful, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nWait for the keycloak-certs secret to reflect the new cert.jks.\nRun the following command until there is a non-empty value in the secret (this can take a minute or two):\nncn-mw# kubectl get secret -n services keycloak-certs -o yaml | grep certs.jks Example output:\ncerts.jks: \u0026lt;REDACTED\u0026gt; Restart the cray-keycloak- pods.\nncn-mw# kubectl rollout restart statefulset -n services cray-keycloak Wait for the Keycloak pods to restart.\nkubectl rollout status statefulset -n services cray-keycloak Make sure to perform the entire linked procedure, including the step to save the updated customizations.\nUninstall the current cray-keycloak-users-localize chart.\nncn-mw# helm del cray-keycloak-users-localize -n services Re-apply the cray-keycloak-users-localize Helm chart with the updated customizations.yaml file.\nFollow the Redeploying a Chart procedure with the following specifications:\nName of chart to be redeployed: cray-keycloak-users-localize\nBase name of manifest: platform\nInstead of downloading the customizations from Kubernetes, use the updated customizations.yaml file.\nWhen reaching the step to validate that the redeploy was successful, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nWatch the pod to check the status of the job.\nThe pod will go through the normal Kubernetes states. It will stay in a Running state for a while, and then it will go to Completed.\nncn-mw# kubectl get pods -n services | grep keycloak-users-localize Example output:\nkeycloak-users-localize-1-sk2hn 0/2 Completed 0 2m35s Check the pod\u0026rsquo;s logs.\nReplace the KEYCLOAK_POD_NAME value with the pod name from the previous step.\nncn-mw# kubectl logs -n services KEYCLOAK_POD_NAME keycloak-localize Example log entry showing that it has updated the \u0026ldquo;s3\u0026rdquo; objects and ConfigMaps:\n2020-07-20 18:26:15,774 - INFO - keycloak_localize - keycloak-localize complete Sync the users and groups from Keycloak to the compute nodes.\nGet the crayvcs password.\nncn-mw# kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode Check out content from the cos-config-management VCS repository.\nncn-mw# git clone https://api-gw-service-nmn.local/vcs/cray/cos-config-management.git ncn-mw# cd cos-config-management ncn-mw# git checkout integration Create the group_vars/Compute/keycloak.yaml file.\nThe file contents should be:\n--- keycloak_config_computes: True Push the changes to VCS with the crayvcs username.\nncn-mw# git add group_vars/Compute/keycloak.yaml ncn-mw# git commit -m \u0026#34;Configure keycloak on computes\u0026#34; ncn-mw# git push origin integration Update the Configuration Framework Service (CFS) configuration.\nncn-mw# cray cfs configurations update configurations-example --file ./configurations-example.json --format json Example output:\n{ \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:30:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; } Reboot with the Boot Orchestration Service (BOS).\nncn-mw# cray bos session create --template-uuid BOS_TEMPLATE --operation reboot Validate that LDAP integration was added successfully.\nRetrieve the admin user\u0026rsquo;s password for Keycloak.\nncn-mw# kubectl get secrets -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d Log in to the Keycloak UI using the admin user and the password obtained in the previous step.\nThe Keycloak UI URL is typically similar to the following: https://auth.cmn.SYSTEM_DOMAIN_NAME/keycloak\nClick on the Users tab in the navigation pane on the left.\nClick on the View all users button and verify that the LDAP users appear in the table.\nVerify that a token can be retrieved from Keycloak using an LDAP user/password.\nIn the example below, replace myuser, mypass, and shasta in the cURL command with site-specific values. The shasta client is created during the SMS install process.\nIn the following example, the jq command is not required; it is simply used to format the output for readability.\nncn-mw# curl -s \\ -d grant_type=password \\ -d client_id=shasta \\ -d username=myuser \\ -d password=mypass \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq Expected output:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;ey...IA\u0026#34;, \u0026lt;\u0026lt;-- NOTE this value, used in the following step \u0026#34;expires_in\u0026#34;: 300, \u0026#34;not-before-policy\u0026#34;: 0, \u0026#34;refresh_expires_in\u0026#34;: 1800, \u0026#34;refresh_token\u0026#34;: \u0026#34;ey...qg\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;profile email\u0026#34;, \u0026#34;session_state\u0026#34;: \u0026#34;10c7d2f7-8921-4652-ad1e-10138ec6fbc3\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34; } Validate that the access_token looks correct.\nCopy the access_token from the previous step and open a browser window. Navigate to http://jwt.io and paste the token in the Encoded field.\nVerify that the preferred_username is the expected LDAP user, and that the role is admin (or other role appropriate for the user).\n"
},
{
	"uri": "/docs-csm/en-12/operations/resiliency/resilience_of_system_management_services/",
	"title": "Resilience of System Management Services",
	"tags": [],
	"description": "",
	"content": "Resilience of System Management Services HPE Cray EX systems are designed so that system management services (SMS) are fully resilient and that there is no single point of failure. The design of the system allows for resiliency in the following ways:\nThree non-compute nodes (NCNs) are configured as Kubernetes master nodes. When one master goes down, operations (such as jobs running across compute nodes) are expected to continue. At least three utility storage nodes provide persistent storage for the services running on the Kubernetes management nodes. When one of the utility storage nodes goes down, operations (such as jobs running across compute nodes) are expected to continue. At least three NCNs are configured as Kubernetes worker nodes. If one of only three Kubernetes worker nodes were to go down, it would be much more difficult for the remaining two NCN worker nodes to handle the total balance of pods. It is less significant to lose one of the NCN worker nodes if the system has more than three NCN worker nodes because there are more worker nodes able to handle the pod load. The state and configuration of the Kubernetes cluster are stored in an etcd cluster distributed across the Kubernetes master nodes. This cluster is also backed up on an interval, and backups are pushed to Ceph Rados Gateway (S3). A micro-service can run on any node that meets the requirements for that micro-service, such as appropriate hardware attributes, which are indicated by labels and taints. All micro-services have shared persistent storage so that they can be restarted on any NCN in the Kubernetes management cluster without losing state. Kubernetes is designed to ensure that the wanted number of deployments of a micro-service are always running on one or more worker nodes. In addition, it ensures that if one worker node becomes unresponsive, the micro-services that were running on it are migrated to another NCN that is up and meets the requirements of those micro-services.\nSee Restore System Functionality if a Kubernetes Worker Node is Down for more information.\nResiliency Improvements To increase the overall resiliency of system management services and software within the system, the following improvements were made:\nCapsules services have implemented replicas for added resiliency. Added support for new storage class that supports Read-Write-Many and in doing so, eliminated some of the errors we encountered on pods which could not seamlessly start up on other worker NCNs upon termination (because of a PVC unmount error). Modified procedures for reloading DVS on NCNs to reduce DVS service interruptions. DVS now retries DNS queries in dvs_generate_map, which improves boot resiliency at scale. Additional retries implemented in the BOS, IMS, and CFS services for increased protection around outages of dependent services. Image-based installs emphasizing \u0026ldquo;non-special\u0026rdquo; node types eliminated single points of failure previously encountered with DNS, administrative and installation tooling, and gathering of Cray System Management (CSM) logging for Ceph and Kubernetes. Expected Resiliency Behavior In addition, the following general criteria describe the expected behavior of the system if a single Kubernetes node (master, worker, or storage) goes down temporarily:\nOnce a job has been launched and is executing on the compute plane, it is expected that it will continue to run without interruption during planned or unplanned outages characterized by the loss of an NCN master, worker, or storage node. Applications launched through PALS may show error messages and lost output if a worker node goes down during application runtime.\nIf an NCN worker node goes down, it will take between 4 and 5 minutes before most of the pods which had been running on the downed NCN will begin terminating. This is a predefined Kubernetes behavior, not something inherent to HPE Cray EX.\nWithin around 20 minutes or less, it should be possible to launch a job using a UAI or UAN after planned or unplanned outages characterized by the loss of an NCN master, worker, or storage node.\nIn the case of a UAN, the recovery time is expected to be quicker. However, launching a UAI after an NCN outage means that some UAI pods may need to relocate to other NCN worker nodes. The status of those new UAI pods will remain unready until all necessary content has been loaded on the new NCN that the UAI is starting up on. This process can take approximately 10 minutes. Within around 20 minutes or less, it should be possible to boot and configure compute nodes after planned or unplanned outages characterized by the loss of an NCN master, worker, or storage node.\nAt least three utility storage nodes provide persistent storage for the services running on the Kubernetes management nodes. When one of the utility storage nodes goes down, critical operations such as job launch, application run, or compute node boot are expected to continue to work.\nNot all pods running on a downed NCN worker node are expected to migrate to a remaining NCN worker node. There are some pods which are configured with anti-affinity such that if the pod exists on another NCN worker node, it will not start another of those pods on that same NCN worker node. At this time, this mostly only applies to etcd clusters running in the cluster. It is optimal to have those pods balanced across the NCN worker nodes (and not have multiple etcd pods, from the same etcd cluster, running on the same NCN worker node). Thus, when an NCN worker node goes down, the etcd pods running on it will remain in terminated state and will not attempt to relocate to another NCN worker node. This should be fine as there should be at least two other etcd pods (from the cluster of 3) running on other NCN worker nodes. Additionally, any pods that are part of a stateful set will not migrate off a worker node when it goes down. Those are expected to stay on the node and also remain in the terminated state until the NCN worker nodes comes back up or unless deliberate action is taken to force that pod off the NCN worker node which is down.\nThe cps-cm-pm pods are part of a daemonset and they only run on designated nodes. When the node comes back up the containers will be restarted and service restored. Refer to \u0026ldquo;Content Projection Service (CPS)\u0026rdquo; in the Cray Operating System (COS) product stream documentation for more information on changing node assignments. After an NCN worker, storage, or master node goes down, if there are issues with launching a UAI session or booting compute nodes, that does not necessarily mean that the problem is due to a worker node being down. If possible, it is advised to also check the relevant \u0026ldquo;Compute Node Boot Troubleshooting Information\u0026rdquo; and User Access Service (specifically with respect to Troubleshoot UAS Issues) procedures. Those sections can give guidance around general known issues and how to troubleshoot them. For any customer support ticket opened on these issues, however, it would be an important piece of data to include in that ticket if the issue was encountered while one or more of the NCNs were down.\nKnown Issues and Workarounds Though an effort was made to increase the number of pod replicas for services that were critical to system operations such as booting computes, launching jobs, and running applications across the compute plane, there are still some services that remain with single copies of their pods. In general, this does not result in a critical issue if these singleton pods are on an NCN worker node that goes down. Most micro-services should (after being terminated by Kubernetes), simply be rescheduled onto a remaining NCN worker node. That assumes that the remaining NCN worker nodes have sufficient resources available and meet the hardware/network requirements of the pods.\nHowever, it is important to note that some pods, when running on a worker NCN that goes down, may require some manual intervention to be rescheduled. Note the workarounds in this section for such pods. Work is ongoing to correct these issues in a future release.\nNexus pod\nThe nexus pod is a single pod deployment and serves as our image repository. If it is on an NCN worker node that goes down, it will attempt to start up on another NCN worker node. However, it is likely that it can also encounter the Multi-Attach error for volume error that can be seen in the kubectl describe output for the pod that is trying to come up on the new node.\nTo determine if this is happening, run the following:\nncn# kubectl get pods -n nexus | grep nexus Describe the pod obtained in the previous step\nncn# kubectl describe pod -n nexus NEXUS_FULL_POD_NAME If the event data at the bottom of the kubectl describe command output indicates that a Multi-Attach PVC error has occurred, then see the Troubleshoot Pods Multi-Attach Error procedure to unmount the PVC. This will allow the Nexus pod to begin successfully running on the new NCN worker node.\nHigh-speed network resiliency after ncn-w001 goes down\nThe slingshot-fabric-manager pod running on one of NCNs does not rely on ncn-w001. If ncn-w001 goes down, the slingshot-fabric-manager pods should not be impacted as the pod is runs on other NCNs, such as ncn-w002.\nThe slingshot-fabric-manager pod relies on Kubernetes to launch the new pod on another NCN if the slingshot-fabric-manager pod is running on ncn-w001 when it is brought down.\nUse the following command and check the NODE column to check which NCN the pod is running on:\nncn# kubectl get pod -n services -o wide | awk \u0026#39;NR == 1 || /slingshot-fabric-manager/\u0026#39; When the slingshot-fabric-manager pod goes down, the switches will continue to run. Even if the status of the switches changes, those changes will be picked up after the slingshot-fabric-manager pod is brought back up and the sweeping process restarts. The slingshot-fabric-manager relies on data in persistent storage. The data is persistent across upgrades but when the pods are deleted, the data is also deleted. Future Resiliency Improvements In a future release, strides will be made to further improve the resiliency of the system. These improvements may include one or more of the following:\nFurther emphasis on eliminating singleton system management pods. Reduce time delays for individual service responsiveness after its pods are terminated because of it running on a worker node that has gone down. Rebalancing of pods/workloads after an NCN worker node comes back up after being down. Analysis/improvements with respect to outages of the Node Management Network (NMN) and the impact to critical system management services. Expanded analysis/improvements of resiliency of noncritical services (those that are not directly related to job launch, application run, or compute boot). "
},
{
	"uri": "/docs-csm/en-12/operations/preinstall/preinstall/",
	"title": "Pre-Install Steps",
	"tags": [],
	"description": "",
	"content": "Pre-Install Steps This section provides information on what needs to be done before an initial install of CSM.\nAir-Cooled BMC Credentials It is necessary to set the default credentials of all air-cooled BMCs so that CSM Hardware Management can interact with Redfish.\nThis procedure is outlined in Change Air-Cooled BMC Credentials.\nLiquid-Cooled BMC Credentials As with air-cooled BMCs, liquid-cooled BMCs also need their credentials changed.\nThis procedure is outlined in Change Liquid-Cooled BMC Credentials.\nServerTech PDU Default Credentials ServerTech PDUs must have their default credentials set as well. These are not native Redfish devices; they have a proprietary interface which is abstracted by the HMS Redfish Translation Service.\nTo set ServerTech PDU default credentials, follow the procedure outlined in Change ServerTech PDU Credentials.\n"
},
{
	"uri": "/docs-csm/en-12/operations/power_management/liquid_cooled_node_card_power_management/",
	"title": "Liquid Cooled Node Power Management",
	"tags": [],
	"description": "",
	"content": "Liquid Cooled Node Power Management Liquid Cooled AMD EPYC compute blade node card power capabilities and limits.\nLiquid Cooled cabinet node card power features are supported by the node controller (nC) firmware and CPU vendor. The nC exposes the power control API for each node via the node\u0026rsquo;s Redfish Control schema. Out-of-band power management data is produced and collected by the nC hardware and firmware. This data can be published to a collector using the Redfish EventService, or retrieved on-demand from the Redfish ChassisSensors resource.\nRequirements Hardware State Manager (cray-hms-smd) at least v1.30.16 CAPMC (cray-hms-capmc) at least 1.31.0 Cray CLI at least 0.44.0 Deprecated Interfaces Set the CAPMC Deprecation Notice for more information.\nget_node_energy get_node_energy_stats get_system_power Redfish API The Redfish API for Liquid Cooled compute blades is the node\u0026rsquo;s Control resource which is presented by the nC. The Control resource presents the various power management capabilities for the node and any associated accelerator cards.\nEach node has one or more power control resource that can be modified:\nNode power control (host CPU and memory) Accelerator power control (one resource per accelerator connected to the node) The Control resources will only manifest in the nC\u0026rsquo;s Redfish endpoint after a node has been powered on and background processes have discovered the node\u0026rsquo;s power management capabilities.\nPower Limiting CAPMC power limit controls for compute nodes can query component capabilities and manipulate the node power constraints. This functionality enables external software to establish an upper bound, or estimate a minimum bound, on the amount of power a system or a select subset of the system may consume.\nCAPMC API calls provide means for third party software to implement advanced power management strategies using JSON data structures.\nThe AMD EPYC node card supports these power limiting and monitoring API calls:\nget_power_cap_capabilities get_power_cap set_power_cap Power limit control will only be valid on a compute node when power limiting is enabled, the node is booted, and the node is in the Ready state as seen via the Hardware State Manager.\nCray CLI Examples for Liquid Cooled Compute Node Power Management Get Node Power Control and Limit Settings ncn# cray capmc get_power_cap create –-nids NID_LIST --format json Return the current power cap settings for a node and any accelerators that are installed. Valid settings are only returned if power limiting is enabled on the target nodes, those nodes are booted, and the nodes are in the Ready state.\nncn# cray capmc get_power_cap create --nids 1160 --format json Example output:\n{ \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;nids\u0026#34;: [ { \u0026#34;nid\u0026#34;: 1160, \u0026#34;controls\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Node Power Limit\u0026#34;, \u0026#34;val\u0026#34;: 1000 }, { \u0026#34;name\u0026#34;: \u0026#34;Accelerator3 Power Limit\u0026#34;, \u0026#34;val\u0026#34;: 200 }, { \u0026#34;name\u0026#34;: \u0026#34;Accelerator2 Power Limit\u0026#34;, \u0026#34;val\u0026#34;: 200 }, { \u0026#34;name\u0026#34;: \u0026#34;Accelerator0 Power Limit\u0026#34;, \u0026#34;val\u0026#34;: 200 }, { \u0026#34;name\u0026#34;: \u0026#34;Accelerator1 Power Limit\u0026#34;, \u0026#34;val\u0026#34;: 200 } ] } ] } Get Power Limit Capabilities ncn# cray capmc get_power_cap_capabilities create –-nids NID_LIST --format json Return the min and max power cap settings for the node list and any accelerators that are installed.\nncn# cray capmc get_power_cap_capabilities create --nids 1160 --format json Example output:\n{ \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;groups\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;3_AuthenticAMD_64c_256GiB_3200MHz_NodeAccel.NVIDIA.6922G5060202000.1321020042737\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;3_AuthenticAMD_64c_256GiB_3200MHz_NodeAccel.NVIDIA.6922G5060202000.1321020042737\u0026#34;, \u0026#34;host_limit_max\u0026#34;: 1985, \u0026#34;host_limit_min\u0026#34;: 595, \u0026#34;static\u0026#34;: 0, \u0026#34;supply\u0026#34;: 1985, \u0026#34;powerup\u0026#34;: 0, \u0026#34;nids\u0026#34;: [ 1160 ], \u0026#34;controls\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Node Power Limit\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;Node Power Limit\u0026#34;, \u0026#34;max\u0026#34;: 1985, \u0026#34;min\u0026#34;: 595 }, { \u0026#34;name\u0026#34;: \u0026#34;Accelerator0 Power Limit\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;Accelerator0 Power Limit\u0026#34;, \u0026#34;max\u0026#34;: 400, \u0026#34;min\u0026#34;: 100 }, { \u0026#34;name\u0026#34;: \u0026#34;Accelerator1 Power Limit\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;Accelerator1 Power Limit\u0026#34;, \u0026#34;max\u0026#34;: 400, \u0026#34;min\u0026#34;: 100 }, { \u0026#34;name\u0026#34;: \u0026#34;Accelerator2 Power Limit\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;Accelerator2 Power Limit\u0026#34;, \u0026#34;max\u0026#34;: 400, \u0026#34;min\u0026#34;: 100 }, { \u0026#34;name\u0026#34;: \u0026#34;Accelerator3 Power Limit\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;Accelerator3 Power Limit\u0026#34;, \u0026#34;max\u0026#34;: 400, \u0026#34;min\u0026#34;: 100 } ] } ] } Set Node Power Limit ncn# cray capmc set_power_cap create --nids NID_LIST --control CONTROL_NAME VALUE --format json Set the total power limit of the node by using the name of the node control. The power provided to the host CPU and memory is the total node power limit minus the power limits of each of the accelerators installed on the node.\nncn# cray capmc set_power_cap create --nids 1160 --control \u0026#34;Node Power Limit\u0026#34; 1785 Example output:\n{ \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;nids\u0026#34;: [ { \u0026#34;nid\u0026#34;: 1160, \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34; } ] } Multiple controls can be set at the same time on multiple nodes, but all target nodes must have the same set of controls available, otherwise the call will fail.\nncn# cray capmc set_power_cap create \\ --nids [1160-1163] \\ --control \u0026#34;Node Power Limit\u0026#34; 1785 \\ --control \u0026#34;Accelerator0 Power Limit\u0026#34; 300 \\ --control \u0026#34;Accelerator1 Power Limit\u0026#34; 300 \\ --control \u0026#34;Accelerator2 Power Limit\u0026#34; 300 \\ --control \u0026#34;Accelerator3 Power Limit\u0026#34; 300 \\ --format json Example output:\n{ \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;nids\u0026#34;: [ { \u0026#34;nid\u0026#34;: 1160, \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;nid\u0026#34;: 1161, \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;nid\u0026#34;: 1162, \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;nid\u0026#34;: 1163, \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34; } ] } Remove Node Power Limit (Set to Default) ncn# cray capmc set_power_cap create --nids NID_LIST --control CONTROL_NAME 0 --format json Reset the power limit to the default maximum. Alternatively, using the max value returned from get_power_cap_capabilities may also be used. Multiple controls can be set at the same time on multiple nodes, but all target nodes must have the same set of controls available, otherwise the call will fail.\nncn# cray capmc set_power_cap create --nids 1160 --control \u0026#34;Node Power Limit\u0026#34; 0 --format json Example output:\n{ \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;nids\u0026#34;: [ { \u0026#34;nid\u0026#34;: 1160, \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34; } ] } Enable and Disable Power Limiting Enable Power Limiting Determine the valid power limit range for the target control by using the get_power_cap_capabilities Cray CLI option.\nncn# cray capmc get_power_cap_capabilities create –-nids NID_LIST --format json For example:\nncn# cray capmc get_power_cap_capabilities create --nids 1160 --format json Example output:\n{ \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;groups\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;3_AuthenticAMD_64c_256GiB_3200MHz_NodeAccel.NVIDIA.6922G5060202000.1321020042737\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;3_AuthenticAMD_64c_256GiB_3200MHz_NodeAccel.NVIDIA.6922G5060202000.1321020042737\u0026#34;, \u0026#34;host_limit_max\u0026#34;: 1985, \u0026#34;host_limit_min\u0026#34;: 595, \u0026#34;static\u0026#34;: 0, \u0026#34;supply\u0026#34;: 1985, \u0026#34;powerup\u0026#34;: 0, \u0026#34;nids\u0026#34;: [ 1160 ], \u0026#34;controls\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Node Power Limit\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;Node Power Limit\u0026#34;, \u0026#34;max\u0026#34;: 1985, \u0026#34;min\u0026#34;: 595 }, { \u0026#34;name\u0026#34;: \u0026#34;Accelerator0 Power Limit\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;Accelerator0 Power Limit\u0026#34;, \u0026#34;max\u0026#34;: 400, \u0026#34;min\u0026#34;: 100 }, { \u0026#34;name\u0026#34;: \u0026#34;Accelerator1 Power Limit\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;Accelerator1 Power Limit\u0026#34;, \u0026#34;max\u0026#34;: 400, \u0026#34;min\u0026#34;: 100 }, { \u0026#34;name\u0026#34;: \u0026#34;Accelerator2 Power Limit\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;Accelerator2 Power Limit\u0026#34;, \u0026#34;max\u0026#34;: 400, \u0026#34;min\u0026#34;: 100 }, { \u0026#34;name\u0026#34;: \u0026#34;Accelerator3 Power Limit\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;Accelerator3 Power Limit\u0026#34;, \u0026#34;max\u0026#34;: 400, \u0026#34;min\u0026#34;: 100 } ] } ] } Selecting a value that is in the min to max range, make a curl call to the Redfish endpoint to enable power limiting for each control. Be aware that the power limit for accelerators will be much lower than the power limit for the node.\nncn# limit=1985 ncn# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; -X PATCH \\ https://${BMC}/redfish/v1/Chassis/${node}/Controls/NodePowerLimit \\ -d \u0026#39;{\u0026#34;ControlMode\u0026#34;:\u0026#34;Automatic\u0026#34;,\u0026#34;SetPoint\u0026#34;:\u0026#39;${limit}\u0026#39;}\u0026#39; If there are accelerators installed, enabled power limiting on those as well.\nncn# limit=400 ncn# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; -X PATCH \\ https://${BMC}/redfish/v1/Chassis/${node}/Controls/Accelerator0PowerLimit \\ -d \u0026#39;{\u0026#34;ControlMode\u0026#34;:\u0026#34;Automatic\u0026#34;,\u0026#34;SetPoint\u0026#34;:\u0026#39;${limit}\u0026#39;}\u0026#39; ncn# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; -X PATCH \\ https://${BMC}/redfish/v1/Chassis/${node}/Controls/Accelerator1PowerLimit \\ -d \u0026#39;{\u0026#34;ControlMode\u0026#34;:\u0026#34;Automatic\u0026#34;,\u0026#34;SetPoint\u0026#34;:\u0026#39;${limit}\u0026#39;}\u0026#39; ncn# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; -X PATCH \\ https://${BMC}/redfish/v1/Chassis/${node}/Controls/Accelerator2PowerLimit \\ -d \u0026#39;{\u0026#34;ControlMode\u0026#34;:\u0026#34;Automatic\u0026#34;,\u0026#34;SetPoint\u0026#34;:\u0026#39;${limit}\u0026#39;}\u0026#39; ncn# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; -X PATCH \\ https://${BMC}/redfish/v1/Chassis/${node}/Controls/Accelerator3PowerLimit \\ -d \u0026#39;{\u0026#34;ControlMode\u0026#34;:\u0026#34;Automatic\u0026#34;,\u0026#34;SetPoint\u0026#34;:\u0026#39;${limit}\u0026#39;}\u0026#39; Disable Power Limiting Each control at the Redfish endpoint needs to be disabled.\nncn# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; \\ -X PATCH https://${BMC}/redfish/v1/Chassis/${node}/Controls/NodePowerLimit \\ -d \u0026#39;{\u0026#34;ControlMode\u0026#34;:\u0026#34;Disabled\u0026#34;}\u0026#39; If there are accelerators installed, disable power limiting on those as well.\nncn# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; -X PATCH \\ https://${BMC}/redfish/v1/Chassis/${node}/Controls/Accelerator0PowerLimit \\ -d \u0026#39;{\u0026#34;ControlMode\u0026#34;:\u0026#34;Disabled\u0026#34;}\u0026#39; ncn# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; -X PATCH \\ https://${BMC}/redfish/v1/Chassis/${node}/Controls/Accelerator1PowerLimit \\ -d \u0026#39;{\u0026#34;ControlMode\u0026#34;:\u0026#34;Disabled\u0026#34;}\u0026#39; ncn# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; -X PATCH \\ https://${BMC}/redfish/v1/Chassis/${node}/Controls/Accelerator2PowerLimit \\ -d \u0026#39;{\u0026#34;ControlMode\u0026#34;:\u0026#34;Disabled\u0026#34;}\u0026#39; ncn# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; -X PATCH \\ https://${BMC}/redfish/v1/Chassis/${node}/Controls/Accelerator3PowerLimit \\ -d \u0026#39;{\u0026#34;ControlMode\u0026#34;:\u0026#34;Disabled\u0026#34;}\u0026#39; "
},
{
	"uri": "/docs-csm/en-12/operations/package_repository_management/nexus_deployment/",
	"title": "Nexus Deployment",
	"tags": [],
	"description": "",
	"content": "Nexus Deployment Nexus is deployed with the cray-nexus chart to the nexus namespace as part of the Cray System Management (CSM) release. Nexus is deployed after critical platform services are up and running. Product installers configure and populate Nexus blob stores and repositories using the cray-nexus-setup container image. As a result, there is no singular product that provides all Nexus repositories or assets; instead, individual products must be installed. However, CSM configures the charts Helm repository and the registry Docker repository, which all products may use.\nCustomizations Common Nexus deployments Bootstrap registry Product installers Customizations For a complete set of available settings, consult the values.yaml file for the cray-nexus chart. The most common customizations to set are specified in the following table. They must be set in the customizations.yaml file under the spec.kubernetes.services.cray-nexus setting.\nCustomization Default Description istio.ingress.hosts.ui.enabled true Enables ingress from the CAN (default chart value is false) istio.ingress.hosts.ui.authority nexus.cmn.{{ network.dns.external }} Sets the CAN hostname (default chart value is nexus.local) sonatype-nexus.persistence.storageSize 1000Gi Nexus storage size, may be increased after installation; critical if spec.kubernetes.services.cray-nexus-setup.s3.enabled is false If modifying the customizations.yaml file, be sure to upload the new file to Kubernetes, otherwise the changes will not persist through future installs or upgrades.\nncn-mw# kubectl delete secret -n loftsman site-init \u0026amp;\u0026amp; kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml Common Nexus deployments A typical deployment will look similar to the output of the following command:\nncn-mw# kubectl -n nexus get all Example output:\nNAME READY STATUS RESTARTS AGE pod/cray-precache-images-6tp2c 2/2 Running 0 20d pod/cray-precache-images-dnwdx 2/2 Running 0 20d pod/cray-precache-images-jgvx8 2/2 Running 0 20d pod/cray-precache-images-n2clw 2/2 Running 0 20d pod/cray-precache-images-v8ntg 2/2 Running 0 17d pod/cray-precache-images-xmg6d 2/2 Running 0 20d pod/nexus-55d8c77547-xcc2f 2/2 Running 0 19d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nexus ClusterIP 10.23.120.95 \u0026lt;none\u0026gt; 80/TCP,5003/TCP 19d NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/cray-precache-images 6 6 6 6 6 \u0026lt;none\u0026gt; 20d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nexus 1/1 1 1 19d NAME DESIRED CURRENT READY AGE replicaset.apps/nexus-55d8c77547 1 1 1 19dd The cray-precache-images DaemonSet is used to keep select container images resident in the image cache on each worker node to ensure Nexus resiliency. It is deployed as a critical platform component prior to Nexus.\nWARNING: The cray-nexus chart deploys Nexus with a single replica and the corresponding nexus-data PVC with RWX access mode. Nexus should NEVER be scaled to more than one replica; otherwise, the instance data in nexus-data PV will most likely be corrupted. Using RWX access mode enables Nexus to quickly restart on another worker node in the event of a node failure and avoid additional delay because of volume multi-attach errors.\nBootstrap registry During installation, a Nexus instance is run on the PIT node at port 8081 to facilitate cluster bootstrap. It is only configured with a Docker registry available at http://pit.nmn:5000, which is populated with container images included in the CSM release.\nOn the PIT node, http://pit.nmn:5000 is the default mirror configured in /etc/containerd/config.toml. However, once the PIT node is rebooted as ncn-m001, it will no longer be available.\nProduct installers Product installers vendor the dtr.dev.cray.com/cray/cray-nexus-setup:0.4.0 container image, which includes helper scripts for working with the Nexus REST API to update and modify repositories. Product release distributions will include the nexus-blobstores.yaml and nexus-repositories.yaml files, which define the Nexus blob stores and repositories required for that version of the product. Also, expect to find directories that include specific types of assets:\nrpm/ - RPM repositories docker/ - Container images helm/ - Helm charts Prior to deploying Helm charts to the system management Kubernetes cluster, product installers will set up repositories in Nexus and then upload assets to them. Typically, all of this is automated in the beginning of a product\u0026rsquo;s install.sh script, and will look something like the following:\nROOTDIR=\u0026#34;$(dirname \u0026#34;${BASH_SOURCE[0]}\u0026#34;)\u0026#34;source\u0026#34;${ROOTDIR}/lib/version.sh\u0026#34;source\u0026#34;${ROOTDIR}/lib/install.sh\u0026#34;# Load vendored tools into install environment load-install-deps # Upload the contents of an RPM repository named $repo nexus-upload raw \u0026#34;${ROOTDIR}/rpm/${repo}\u0026#34;\u0026#34;${RELEASE_NAME}-${RELEASE_VERSION}-${repo}\u0026#34;# Setup Nexus nexus-setup blobstores \u0026#34;${ROOTDIR}/nexus-blobstores.yaml\u0026#34; nexus-setup repositories \u0026#34;${ROOTDIR}/nexus-repositories.yaml\u0026#34;# Upload container images to registry.local skopeo-sync \u0026#34;${ROOTDIR}/docker\u0026#34;# Upload charts to the \u0026#34;charts\u0026#34; repository nexus-upload helm \u0026#34;${ROOTDIR}/helm\u0026#34; charts # Remove vendored tools from install environment clean-install-deps Product installers also load and clean up the install tools used to facilitate installation. By convention, vendored tools will be in the vendor directory. In case something goes wrong, it may be useful to manually load them into the install environment to help with debugging.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/rebuild_ncns/post_rebuild_storage_node_validation/",
	"title": "Post Rebuild Storage Node Validation",
	"tags": [],
	"description": "",
	"content": "Post Rebuild Storage Node Validation Validate the storage node rebuilt successfully.\nSkip this section if a master or worker node was rebuilt.\nProcedure Verify there are 3 mons, 3 mds, 3 mgr processes, and rgws.\nncn-m# ceph -s Example output:\ncluster: id: 22d01fcd-a75b-4bfc-b286-2ed8645be2b5 health: HEALTH_OK services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 4m) mgr: ncn-s001(active, since 19h), standbys: ncn-s002, ncn-s003 mds: cephfs:1 {0=ncn-s001=up:active} 2 up:standby osd: 12 osds: 12 up (since 2m), 12 in (since 2m) rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) task status: scrub status: mds.ncn-s001: idle data: pools: 10 pools, 480 pgs objects: 926 objects, 31 KiB usage: 12 GiB used, 21 TiB / 21 TiB avail pgs: 480 active+clean Verify the OSDs are back in the cluster.\nncn-m# ceph osd tree Example output:\nID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 20.95917 root default -3 6.98639 host ncn-s001 2 ssd 1.74660 osd.2 up 1.00000 1.00000 5 ssd 1.74660 osd.5 up 1.00000 1.00000 8 ssd 1.74660 osd.8 up 1.00000 1.00000 11 ssd 1.74660 osd.11 up 1.00000 1.00000 -7 6.98639 host ncn-s002 0 ssd 1.74660 osd.0 up 1.00000 1.00000 4 ssd 1.74660 osd.4 up 1.00000 1.00000 7 ssd 1.74660 osd.7 up 1.00000 1.00000 10 ssd 1.74660 osd.10 up 1.00000 1.00000 -5 6.98639 host ncn-s003 1 ssd 1.74660 osd.1 up 1.00000 1.00000 3 ssd 1.74660 osd.3 up 1.00000 1.00000 6 ssd 1.74660 osd.6 up 1.00000 1.00000 9 ssd 1.74660 osd.9 up 1.00000 1.00000 Verify the radosgw and haproxy are correct.\nThere will be an output (without an error) returned if radosgw and haproxy are correct.\nncn# curl -k https://rgw-vip.nmn Example output:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt;\u0026lt;ListAllMyBucketsResult xmlns=\u0026#34;http://s3.amazonaws.com/doc/2006-03-01/ \u0026#34;\u0026gt;\u0026lt;Owner\u0026gt;\u0026lt;ID\u0026gt;anonymous\u0026lt;/ID\u0026gt;\u0026lt;DisplayName\u0026gt;\u0026lt;/DisplayName\u0026gt;\u0026lt;/Owner\u0026gt;\u0026lt;Buckets\u0026gt;\u0026lt;/Buckets\u0026gt;\u0026lt;/ListAllMyBucketsResult Next Step Return to the main Rebuild NCNs page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_remove_replace_ncns/add_switch_config/",
	"title": "Add Switch Configuration for NCN",
	"tags": [],
	"description": "",
	"content": "Add Switch Configuration for NCN Description Update the network switches for the NCN that is being added.\nProcedure Update Networking to Add NCN Details coming soon.\nNext Step Proceed to the next step to Add NCN Data or return to the main Add, Remove, Replace, or Move NCNs page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/metallb_bgp/metallb_in_bgp-mode/",
	"title": "MetalLB in BGP-Mode",
	"tags": [],
	"description": "",
	"content": "MetalLB in BGP-Mode MetalLB is a component in Kubernetes that manages access to LoadBalancer services from outside the Kubernetes cluster. There are LoadBalancer services on the Node Management Network (NMNLB), Hardware Management Network (HMNLB), Customer Management Network (CMN), Customer High-Speed Network (CHN), and Customer Access Network (CAN).\nMetalLB can run in either Layer 2 mode or BGP mode for each address pool it manages. BGP mode is used for the NMNLB, HMNLB, and CAN. This enables true load balancing (Layer 2 mode does failover, not load balancing) and allows for a more robust layer 3 configuration for these networks.\nIn BGP mode, the MetalLB speakers will peer with the BGP router on the spine switches and advertise the service LoadBalancer IP addresses. If the system is configured to use the CHN for the user network, then the speakers will also peer with the BGP router on the edge switches. The BGP routers will accept those advertised prefixes and add them to the route table. The spine and edge switches are configured with Equal-Cost Multi-Path (ECMP), meaning that each of these BGP route prefixes will load balance to any of the workers that has advertised the prefix. This process allows clients outside the cluster with access to the NMNLB, HMNLB, CMN, CHN or CAN to be able to route to these Kubernetes services.\nBGP peering is only between the MetalLB speakers and the spine/edge switches. It does not do any peering beyond that.\nThe routes in the BGP route table will only be the IP addresses of the Kubernetes LoadBalancer services. This is the fifth column displayed in the output of the following command:\nncn-w001# kubectl get service -A | grep LoadBalancer For example:\nNAMESPACE NAME TYPE CLUSTER-IP **EXTERNAL-IP** PORT(S) AGE ceph-rgw cray-s3 LoadBalancer 10.31.54.80 10.102.10.129 8080:31003/TCP 36d ims cray-ims-40f523ac-9b99-4f76-bb37-df6eb62540c8-service LoadBalancer 10.21.156.88 10.102.10.134 22:31604/TCP 35d ims cray-ims-50287398-b877-4a2b-bf18-c3618583c66f-service LoadBalancer 10.29.254.221 10.102.10.167 22:30314/TCP 12d ims cray-ims-577dec6e-dbac-4363-a423-bf39ed9b9e32-service LoadBalancer 10.22.200.115 10.102.10.158 22:32672/TCP 15d ims cray-ims-5b05e86e-f65b-4a5f-b5eb-2c31f0458722-service LoadBalancer 10.25.162.244 10.102.10.160 22:32707/TCP 14d ims cray-ims-7ffaf10f-75ca-4ccb-b10d-1c7cd31b3d4b-service LoadBalancer 10.20.16.190 10.102.10.132 22:31934/TCP 35d ims cray-ims-b1cd0827-bb51-4bcd-ac25-f64d5f7d0c44-service LoadBalancer 10.26.69.180 10.102.10.131 22:31281/TCP 35d ims cray-ims-bd0698b4-a104-48eb-9714-5b5889ad7b52-service LoadBalancer 10.18.114.136 10.102.10.135 22:31701/TCP 35d MetalLB does not manage access to any of the NCNs, UANs, or compute nodes.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/backup/",
	"title": "Backing up switch configuration",
	"tags": [],
	"description": "",
	"content": "Backing up switch configuration Backing up current configuration of the switch in text format Example\nTo create a new text-based configuration file, complete the following steps:\nLog in to the switch as Admin.\nType the following command:\nswitch (config) # configuration text generate active running save my-filename To upload a text-based configuration file from a switch to an external file server, complete the following steps:\nswitch (config) # configuration text file my-filename upload scp://root@my-server/root/tmp/my-filename Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/manual_switch_config/",
	"title": "Manual Switch Configuration",
	"tags": [],
	"description": "",
	"content": "Manual Switch Configuration Some of the switch configuration is not generated by CANU and needs to be manually applied.\nPrerequisites The custom switch configuration is backed up. See Manual Switch Config.\nSNMP Configuration SNMP is currently only used on sw-leaf-bmc switches, these credentials can be retrieved from Vault. More information on SNMP credentials can be found in the Change SNMP Credentials on Leaf-BMC Switches procedure.\nOnce these credentials are retrieved from vault, fill in the xxxxxx fields below and paste the commands into the switch.\nAruba sw-leaf-001# conf t Example output:\nsnmp-server vrf default snmpv3 user testuser auth md5 auth-pass plaintext xxxxxx priv des priv-pass plaintext xxxxxx Dell sw-leaf-001# conf t Example output:\nsnmp-server group cray-reds-group 3 noauth read cray-reds-view snmp-server user xxxxxx cray-reds-group 3 auth md5 xxxxxx priv des xxxxxx snmp-server view cray-reds-view 1.3.6.1.2 included Authentication Config This is for all switches.\nAruba sw-leaf-bmc-001# conf t Example output:\nuser admin group administrators password plaintext xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Dell sw-leaf-001# conf t Example output:\nsystem-user linuxadmin password xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx username admin password xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx role sysadmin priv-lvl 15 Mellanox sw-spine-001 [standalone: master] # conf t Example output:\nusername admin password 0 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx username monitor password 0 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/generate_switch_configs/",
	"title": "Generate Switch Configurations",
	"tags": [],
	"description": "",
	"content": "Generate Switch Configurations Generating configuration files can be done for a single switch or for the full system.\nFor example, if there is a suspected configuration issue on single switch, a configuration file can be generated for only that switch in order to simplify debugging.\nPrerequisites CANU installed with 1.1.11 or later versions. Run canu --version to see version. If doing a CSM install or upgrade, a CANU RPM is located in the release tarball. See Update CANU From CSM Tarball. Alternatively, upgrade or install the latest version of CANU from GitHub. See Install/Upgrade CANU. Validated SHCD. See Validate SHCD. JSON output from validated SHCD. See Validate SHCD. System Layout Service (SLS) input file. If generating CSM 1.2 configurations, the SLS file must be updated prior to generating configurations. See Collect Data. Generate configuration files Ensure that the correct architecture (-a parameter) is selected for the setup in use.\nThe following are the different architectures that can be specified:\nTds – Aruba-based Test and Development System. These are small systems characterized by Kubernetes NCNs cabled directly to the spine. Full – Aruba-based Leaf-Spine systems. These are usually customer production systems. V1 – Any Dell and Mellanox-based systems. Generating a configuration file can be done for a single switch, or for the full system. Below are example commands for both scenarios:\nImportant: Modify the following items in the command:\n--csm : Which CSM version configuration do you want to use? For example, 1.2 or 1.0 --a : What is the system architecture? (See above) --ccj : Match the ccj.json file to the one you created for your system. --sls : Match the sls_file.json to the one you created for your system. --custom-config : Pass in a switch configuration file that CANU will inject into the generated configuration. More documentation can be found from the official CANU documentation. ncn# canu generate network config --csm 1.2 -a full --ccj system-ccj.json --sls-file sls_file.json --custom-config system-custom-config.yaml --folder generated "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/backup/",
	"title": "Back Up a Switch Configuration",
	"tags": [],
	"description": "",
	"content": "Back Up a Switch Configuration The following command copies the running configuration or the startup configuration to a remote location as a file.\nswitch# copy running-configuration {config://filepath | home://filepath | ftp://userid:passwd@hostip/filepath | scp://userid:passwd@hostip/filepath | sftp://userid:passwd@hostip/filepath | tftp://hostip/filepath} Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/canu/canu_verify_generate_compare_switch_configuration/",
	"title": "Use CANU to Verify, Generate, or Compare Switch Configurations",
	"tags": [],
	"description": "",
	"content": "Use CANU to Verify, Generate, or Compare Switch Configurations Common CANU Arguments SHCD-Related Arguments CSI and SLS API Input to CANU CSI Input SLS API Input Check Single Switch Firmware Check Firmware of Multiple Switches JSON Output Check Single Switch Cabling Check Cabling of Multiple Switches Validate SHCD Validate Cabling Validate SHCD and Cabling Validate BGP Configuration Creation For BGP Generate Switch Configurations Common CANU Arguments The following CANU flags are used for multiple different actions.\nIn order to have CANU output to a file, specify the desired output file with the --out flag. IPv4 addresses can be read from a file or specified in a comma-separated list with the --ips flag. When passing in the IP addresses in a file, the file must have one IP address per line, and the filename is specified with the --ips-file flag. The --architecture / -a flag is used to set the architecture of the system: TDS or Full. SHCD-Related Arguments Some of the CANU flags are specific to SHCD (Shasta Cabling Diagram) input files.\nThe --shcd flag specifies the path to the SHCD file. The --tabs flag selects which tabs on the SHCD spreadsheet will be included. The --corners flag is used to input the upper left and lower right corners of the table on each tab of the SHCD. The table should contain the 11 headers: Source, Rack, Location, Slot, Blank, Port, Destination, Rack, Location, Blank, Port. If the corners are not specified, CANU will prompt for the columns for each tab. CSI and SLS API Input to CANU In some cases, CANU accepts input from a CSI-generated file or from the SLS API. The following two sections go over these options.\nCSI Input In order for CANU to parse CSI output, use the --csi-folder flag specify the directory containing the CSI-generated sls_input_file.json file.\nThe sls_input_file.json file is generally stored in one of two places, depending on how far the system is in the install process.\nEarly in the install process, when running off of the LiveCD, the sls_input_file.json file is normally found on the PIT node in the the /var/www/ephemeral/prep/SYSTEMNAME/ directory. Later in the install process, after the PIT node has been redeployed, the sls_input_file.json file is generally found on ncn-m001 or ncn-m003 in the /metal/bootstrap/prep/SYSTEMNAME/ directory. SLS API Input In order for CANU to get input from the SLS API, the CSM install must be completed at least to the point where the CSM Services have been successfully deployed.\nIn order to have CANU use the SLS API as the source, the path to a token file must be passed in using the --auth-token flag. Tokens are typically stored in the ~./config/cray/tokens/ directory.\nInstead of passing in a token file, the environment variable SLS_TOKEN can be used.\nThe SLS address is by default set to api-gw-service-nmn.local. If needed, a different SLS address can be specified using the --sls-address flag.\nCheck Single Switch Firmware To check the firmware of a single switch, run the following:\nlinux# canu --shasta 1.4 switch firmware --ip 192.168.1.1 --username USERNAME --password PASSWORD Expected output:\n🛶 - Pass - IP: 192.168.1.1 Hostname:test-switch-spine01 Firmware: GL.10.06.0130 Check Firmware of Multiple Switches Multiple Aruba switches on a network can be checked for their firmware versions. An example of checking the firmware of multiple switches:\nlinux# canu --shasta 1.4 network firmware --ips 192.168.1.1,192.168.1.2 --username USERNAME --password PASSWORD linux# canu --shasta 1.4 network firmware --ips 192.168.1.1,192.168.1.2,192.168.1.3,192.168.1.4 --username USERNAME --password PASSWORD Expected Output\n------------------------------------------------------------------ STATUS IP HOSTNAME FIRMWARE ------------------------------------------------------------------ 🛶 Pass 192.168.1.1 test-switch-spine01 GL.10.06.0010 🛶 Pass 192.168.1.2 test-switch-leaf01 FL.10.06.0010 ❌ Fail 192.168.1.3 test-wrong-version FL.10.05.0001 Firmware should be in range [\u0026#39;FL.10.06.0001\u0026#39;] 🔺 Error 192.168.1.4\u0026#39; Errors ------------------------------------------------------------------ 192.168.1.4 - HTTP Error. Check that this IP is an Aruba switch, or check the username and password Summary ------------------------------------------------------------------ 🛶 Pass - 2 switches ❌ Fail - 1 switches 🔺 Error - 1 switches GL.10.06.0010 - 1 switches FL.10.06.0010 - 1 switches FL.10.05.0010 - 1 switches When using the network firmware commands, the table will show either: 🛶 Pass, ❌ Fail, or 🔺 Error. The switch will pass or fail based on whether or not the switch firmware matches the canu.yaml file.\nJSON Output To get the JSON output from a single switch, or from multiple switches, make sure to use the --json flag. An example JSON output is below.\nlinux# canu --shasta 1.4 network firmware --ips 192.168.1.1,192.168.1.2 --username USERNAME --password PASSWORD –json { \u0026#34;192.168.1.1\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;Pass\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;test-switch-spine01\u0026#34;, \u0026#34;platform_name\u0026#34;: \u0026#34;8325\u0026#34;, \u0026#34;firmware\u0026#34;: { \u0026#34;current_version\u0026#34;: \u0026#34;GL.10.06.0010\u0026#34;, \u0026#34;primary_version\u0026#34;: \u0026#34;GL.10.06.0010\u0026#34;, \u0026#34;secondary_version\u0026#34;: \u0026#34;GL.10.05.0020\u0026#34;, \u0026#34;default_image\u0026#34;: \u0026#34;primary\u0026#34;, \u0026#34;booted_image\u0026#34;: \u0026#34;primary\u0026#34;, }, }, \u0026#34;192.168.1.2\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;Pass\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;test-switch-leaf01\u0026#34;, \u0026#34;platform_name\u0026#34;: \u0026#34;6300\u0026#34;, \u0026#34;firmware\u0026#34;: { \u0026#34;current_version\u0026#34;: \u0026#34;FL.10.06.0010\u0026#34;, \u0026#34;primary_version\u0026#34;: \u0026#34;FL.10.06.0010\u0026#34;, \u0026#34;secondary_version\u0026#34;: \u0026#34;FL.10.05.0020\u0026#34;, \u0026#34;default_image\u0026#34;: \u0026#34;primary\u0026#34;, \u0026#34;booted_image\u0026#34;: \u0026#34;primary\u0026#34;, }, }, } Check Single Switch Cabling CANU can also use LLDP to check the cabling status of a switch. To check the cabling of a single switch, run the following:\nlinux# canu --shasta 1.5 switch cabling --ip 192.168.1.1 --username USERNAME --password PASSWORD Expected results:\nSwitch: test-switch-spine01 (192.168.1.1) Aruba 8325 ------------------------------------------------------------------------------------------- ----------------------------------------------- PORT NEIGHBOR NEIGHBOR PORT PORT DESCRIPTION DESCRIPTION ------------------------------------------------------------------------------------------- ----------------------------------------------- 1/1/1 ==\u0026gt; 00:00:00:00:00:01 No LLDP data, check ARP vlan info. 192.168.1.20:vlan1, 192.168.2.12:vlan2 1/1/3 ==\u0026gt; ncn-test2 00:00:00:00:00:02 mgmt0 Linux ncn-test2 1/1/5 ==\u0026gt; ncn-test3 00:00:00:00:00:03 mgmt0 Linux ncn-test3 1/1/7 ==\u0026gt; 00:00:00:00:00:04 No LLDP data, check ARP vlan info. 192.168.1.10:vlan1, 192.168.2.9:vlan2 1/1/51 ==\u0026gt; test-spine02 1/1/51 Aruba JL635A GL.10.06.0010 1/1/52 ==\u0026gt; test-spine02 1/1/52 Aruba JL635A GL.10.06.0010 Sometimes when checking cabling using LLDP, the neighbor does not return any information except a MAC address. When that is the case, CANU looks up the MAC address in the ARP table, and displays the IP addresses and VLAN information associated with that MAC.\nEntries in the table will be colored based on what they are. For example: Neighbors that have ncn in their name will be colored blue. Neighbors that have a port labeled (not a MAC address) are generally switches and are labeled green. Ports that are duplicated will be bright white.\nCheck Cabling of Multiple Switches The cabling of multiple Aruba switches on a network can be checked at the same time using LLDP.\nAn example of checking the cabling of multiple switches:\nlinux# canu --shasta 1.5 network cabling --ips 192.168.1.1,192.168.1.2 --username USERNAME --password PASSWORD There are two different --view options: switch and equipment:\n--view switch option: Displays a table for every switch IP address passed in showing connections. This is the same view as shown in the above example of checking single switch cabling. --view equipment option: Displays a table for each MAC address connection. This means that servers and switches will both display incoming and outgoing connections. An example of checking the cabling of multiple switches and displaying with the equipment view:\nlinux# canu --shasta 1.5 network cabling --ips 192.168.1.1,192.168.1.2 --username USERNAME --password PASSWORD --view equipment` linux# canu --shasta 1.4 network cabling --ips 192.168.1.1,192.168.1.2 --username USERNAME --password PASSWORD --view equipment Expected results:\nsw-spine01 Aruba JL635A GL.10.06.0010 aa:aa:aa:aa:aa:aa ------------------------------------------------------------------------------------------- 1/1/1 \u0026lt;==\u0026gt; sw-spine02 1/1/1 Aruba JL635A GL.10.06.0010 1/1/3 ===\u0026gt; 00:00:00:00:00:00 mgmt1 1/1/4 ===\u0026gt; ncn-test bb:bb:bb:bb:bb:bb mgmt1 Linux ncn-test sw-spine02 Aruba JL635A GL.10.06.0010 bb:bb:bb:bb:bb:bb ------------------------------------------------------------------------------------------- 1/1/1 \u0026lt;==\u0026gt; sw-spine01 1/1/1 Aruba JL635A GL.10.06.0010 00:00:00:00:00:00 192.168.2.2:vlan3, 192.168.1.2:vlan1 Validate SHCD CANU can be used to perform basic validation of an SHCD (Shasta Cabling Diagram) file.\nIn order to check an SHCD, run the following:\nlinux# canu -s 1.5 validate shcd -a tds --shcd FILENAME.xlsx --tabs 25G_10G,NMN,HMN --corners I14,S25,I16,S22,J20,T39 Expected results:\nSHCD Node Connections ------------------------------------------------------------ 0: sw-spine-001 connects to 6 nodes: [1, 2, 3, 4, 5, 6] 1: sw-spine-002 connects to 6 nodes: [0, 2, 3, 4, 5, 6] 2: sw-leaf-bmc-001 connects to 2 nodes: [0, 1] 3: uan001 connects to 2 nodes: [0, 1] 4: ncn-s001 connects to 2 nodes: [0, 1] 5: ncn-w001 connects to 2 nodes: [0, 1] 6: ncn-m001 connects to 2 nodes: [0, 1] Warnings Node type could not be determined for the following ------------------------------------------------------------ CAN switch Validate Cabling CANU can be used to perform basic validation of network cabling.\nIn order to validate the cabling, run the following:\nlinux# canu -s 1.4 validate cabling -a tds --ips 192.168.1.1,192.168.1.2 --username USERNAME --password PASSWORD linux# canu -s 1.4 validate cabling -a tds --ips 192.168.1.1,192.168.1.2 --username USERNAME --password PASSWORD Expected results:\nCabling Node Connections ------------------------------------------------------------ 0: sw-spine-001 connects to 10 nodes: [1, 2, 3, 4] 1: ncn-m001 connects to 2 nodes: [0, 4] 2: ncn-w001 connects to 2 nodes: [0, 4] 3: ncn-s001 connects to 2 nodes: [0, 4] 4: sw-spine-002 connects to 10 nodes: [0, 1, 2, 3 ] Warnings Node type could not be determined for the following ------------------------------------------------------------ sw-leaf-001 sw-spine-001 1/1/1 ===\u0026gt; aa:aa:aa:aa:aa:aa sw-spine-001 1/1/2 ===\u0026gt; 1/1/1 CFCANB4S1 Aruba JL479A TL.10.03.0081 sw-spine-001 1/1/3 ===\u0026gt; 1/1/3 sw-leaf-001 Aruba JL663A FL.10.06.0010 sw-spine-002 1/1/4 ===\u0026gt; bb:bb:bb:bb:bb:bb sw-spine-002 1/1/5 ===\u0026gt; 1/1/2 CFCANB4S1 Aruba JL479A TL.10.03.0081 sw-spine-002 1/1/6 ===\u0026gt; 1/1/6 sw-leaf-001 Aruba JL663A FL.10.06.0010 Nodes that show up as MAC addresses might need to have LLDP enabled. The following nodes should be renamed ------------------------------------------------------------ sw-leaf01 should be renamed (could not identify node) sw-spine01 should be renamed sw-spine-001 sw-spine02 should be renamed sw-spine-002 If there are any nodes that cannot be determined or should be renamed, there will be warning tables that show the details.\nValidate SHCD and Cabling CANU can be used to validate an SHCD against the current network cabling.\nIn order to validate an SHCD against the cabling, run the following:\nlinux# canu -s 1.5 validate shcd-cabling -a tds --shcd FILENAME.xlsx --tabs 25G_10G,NMN --corners I14,S49,I16,S22 --ips 192.168.1.1,192.168.1.2 --username USERNAME --password PASSWORD` linux# canu -s 1.5 validate shcd-cabling -a tds --shcd FILENAME.xlsx --tabs 25G_10G,NMN --corners I14,S49,I16,S22 --ips 192.168.1.1,192.168.1.2 --username USERNAME --password PASSWORD Expected results:\n==================================================================================================== SHCD ==================================================================================================== SHCD Node Connections ------------------------------------------------------------ 0: sw-spine-001 connects to 6 nodes: [1, 2, 3, 4, 5, 6] 1: sw-spine-002 connects to 6 nodes: [0, 2, 3, 4, 5, 6] 2: sw-leaf-bmc-001 connects to 2 nodes: [0, 1] 3: uan001 connects to 2 nodes: [0, 1] 4: ncn-s001 connects to 2 nodes: [0, 1] 5: ncn-w001 connects to 2 nodes: [0, 1] 6: ncn-m001 connects to 2 nodes: [0, 1] Warnings Node type could not be determined for the following ------------------------------------------------------------ CAN switch ==================================================================================================== Cabling ==================================================================================================== Cabling Node Connections ------------------------------------------------------------ 0: sw-spine-001 connects to 10 nodes: [1, 2, 3, 4] 1: ncn-m001 connects to 2 nodes: [0, 4] 2: ncn-w001 connects to 2 nodes: [0, 4] 3: ncn-s001 connects to 2 nodes: [0, 4] 4: sw-spine-002 connects to 10 nodes: [0, 1, 2, 3 ] Warnings Node type could not be determined for the following ------------------------------------------------------------ sw-leaf-001 sw-spine-001 1/1/1 ===\u0026gt; aa:aa:aa:aa:aa:aa sw-spine-001 1/1/2 ===\u0026gt; 1/1/1 CFCANB4S1 Aruba JL479A TL.10.03.0081 sw-spine-001 1/1/3 ===\u0026gt; 1/1/3 sw-leaf-001 Aruba JL663A FL.10.06.0010 sw-spine-002 1/1/4 ===\u0026gt; bb:bb:bb:bb:bb:bb sw-spine-002 1/1/5 ===\u0026gt; 1/1/2 CFCANB4S1 Aruba JL479A TL.10.03.0081 sw-spine-002 1/1/6 ===\u0026gt; 1/1/6 sw-leaf-001 Aruba JL663A FL.10.06.0010 Nodes that show up as MAC addresses might need to have LLDP enabled. The following nodes should be renamed ------------------------------------------------------------ sw-leaf01 should be renamed (could not identify node) sw-spine01 should be renamed sw-spine-001 sw-spine02 should be renamed sw-spine-002 ==================================================================================================== SHCD vs Cabling ==================================================================================================== SHCD / Cabling Comparison ------------------------------------------------------------ sw-spine-001 : Found in SHCD and on the network, but missing the following connections on the network that were found in the SHCD: [\u0026#39;sw-leaf-bmc-001\u0026#39;, \u0026#39;uan001\u0026#39;] sw-spine-002 : Found in SHCD and on the network, but missing the following connections on the network that were found in the SHCD: [\u0026#39;sw-leaf-bmc-001\u0026#39;, \u0026#39;uan001\u0026#39;] sw-leaf-bmc-001 : Found in SHCD but not found on the network. uan001 : Found in SHCD but not found on the network. The output of the validate shcd-cabling command will show the results for validate shcd, validate cabling, and a comparison of the two results. A node will be displayed in blue if it is found in the SHCD but not the network, or vice versa. If a node is found on both the network and in the SHCD, but the connections are not the same, that node will be shown in green, and the missing connections will be shown.\nValidate BGP CANU can be used to validate BGP neighbors. All neighbors of a switch must return status Established or the verification will fail.\nThe default asn is set to 65533. If needed, use the --asn flag to set a different number.\nIn order to see the individual status of all the neighbors of a switch, use the --verbose flag.\nIn order to validate BGP, run the following command:\nlinux# canu -s 1.5 validate bgp --ips 192.168.1.1,192.168.1.2 --username USERNAME --password PASSWORD linux# canu -s 1.4 validate bgp --ips 192.168.1.1,192.168.1.2 --username USERNAME --password PASSWORD Expected results:\nBGP Neighbors Established -------------------------------------------------- PASS - IP: 192.168.1.1 Hostname: sw-spine01 PASS - IP: 192.168.1.2 Hostname: sw-spine01 If any of the spine switch neighbors for a connection other than Established, the switch will fail validation.\nIf a switch that is not a spine switch is tested, it will show in the results table as SKIP.\nConfiguration Creation For BGP CANU can be used to configure BGP for a pair of switches.\nWARNING: This command will remove the previous configuration (BGP, prefix lists, route maps), then add prefix lists, create route maps, update BGP neighbors, and write it all to the switch memory.\nThe network and NCN data can be read from one of two sources: the SLS API or a file generated by CSI. See CSI and SLS API Input to CANU.\nIn order to configure BGP, run the following:\nlinux# canu -s 1.5 config bgp --ips 192.168.1.1,192.168.1.2 --username USERNAME --password PASSWORD linux# canu -s 1.4 config bgp --ips 192.168.1.1,192.168.1.2 --username USERNAME --password PASSWORD Expected. Results:\nBGP Updated -------------------------------------------------- 192.168.1.1 192.168.1.2 To print extra details (prefixes, NCN names, IP addresses), add the --verbose flag.\nGenerate Switch Configurations CANU can be used to generate switch configurations.\nIn order to generate a switch configuration, a valid SHCD must be passed in and system variables must be read in from either CSI output or the SLS API. See CSI and SLS API Input to CANU.\nIn order to generate a configuration for a specific switch, a hostname must be passed in using the --name flag.\nIn order to generate a switch configuration, run the following:\nlinux# canu -s 1.5 switch config -a full --shcd FILENAME.xlsx --tabs \u0026#39;INTER_SWITCH_LINKS,NON_COMPUTE_NODES,HARDWARE_MANAGEMENT,COMPUTE_NODES\u0026#39; --corners \u0026#39;J14,T44,J14,T48,J14,T24,J14,T23\u0026#39; --csi-folder /CSI/OUTPUT/FOLDER/ADDRESS --name SWITCH_HOSTNAME --out FILENAME linux# canu -s 1.4 switch config -a full --shcd FILENAME.xlsx --tabs INTER_SWITCH_LINKS,NON_COMPUTE_NODES,HARDWARE_MANAGEMENT,COMPUTE_NODES --corners J14,T44,J14,T48,J14,T24,J14,T23 --csi-folder /CSI/OUTPUT/FOLDER/ADDRESS --name sw-spine-001 Expected results: \u0026lt;snippet\u0026gt; hostname sw-spine-001 user admin group administrators password plaintext bfd no ip icmp redirect vrf CAN vrf keepalive ... .. \u0026lt;/Snippet\u0026gt; "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/arp/",
	"title": "Address Resolution Protocol (ARP)",
	"tags": [],
	"description": "",
	"content": "Address Resolution Protocol (ARP) ARP is commonly used for mapping IPv4 addresses to MAC addresses.\nProcedure Configure static ARP on an interface.\nswitch(config-if)# arp ipv4 IP-ADDR mac MAC-ADDR Show commands to validate functionality: .\nswitch# show arp Expected Results Administrators are able to ping the connected device Administrators can view the ARP entries Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/apply_custom_config_1.0/",
	"title": "Apply Custom Switch Configurations for CSM 1.0",
	"tags": [],
	"description": "",
	"content": "Apply Custom Switch Configurations for CSM 1.0 Apply the backed up site connection configuration with a couple modifications. Since virtual routing and forwarding (VRF) is now used to separate customer traffic, the site ports and default routes must be added to that VRF.\nPrerequisites Access to the switches Custom switch configurations Backup Custom Configurations Generated switch configurations already applied Apply Switch Configurations Aruba apply configurations sw-spine-001# conf t interface 1/1/36 no shutdown description to:CANswitch_cfcanb6s1-31:from:sw-25g01_x3000u39-j36 ip address 10.101.15.142/30 exit sw-spine-001# conf t sw-spine-001(config)# system interface-group 3 speed 10g sw-spine-002# conf t interface 1/1/36 no shutdown description to:CANswitch_cfcanb6s1-46:from:sw-25g02_x3000u40-j36 ip address 10.101.15.190/30 exit If the switch had system interface-group commands those would be added here.\nsw-spine-001(config)# system interface-group 3 speed 10g sw-spine-001# conf t sw-spine-001(config)# ip route 0.0.0.0/0 10.101.15.141 vrf default sw-spine-002# conf t sw-spine-002(config)# ip route 0.0.0.0/0 10.101.15.189 vrf default Mellanox apply configurations sw-spine-001 [mlag-domain: master] # conf t interface ethernet 1/16 speed 10G force interface ethernet 1/16 mtu 1500 force interface ethernet 1/16 no switchport force interface ethernet 1/16 ip address 10.102.255.10/30 primary sw-spine-002 [mlag-domain: master] # conf t interface ethernet 1/16 speed 10G force interface ethernet 1/16 mtu 1500 force interface ethernet 1/16 no switchport force interface ethernet 1/16 ip address 10.102.255.86/30 primary sw-spine-001 [mlag-domain: master] # conf t ip route vrf default 0.0.0.0/0 10.102.255.9 sw-spine-002 [mlag-domain: master] # conf t ip route vrf default 0.0.0.0/0 10.102.255.85 Apply Users/Password All that is required to re-apply the users is to get into global configuration mode with conf t and to paste in the configuration that was copied from the previous step.\nAruba credentials sw-leaf-bmc-001# conf t user admin group administrators password ciphertext xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Dell credentials sw-leaf-001# conf t system-user linuxadmin password xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx username admin password xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx role sysadmin priv-lvl 15 Mellanox credentials sw-spine-001 [standalone: master] # conf t username admin password 7 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx username monitor password 7 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Apply SNMP credentials Dell SNMP sw-leaf-bmc-001# conf t snmp-server group cray-reds-group 3 noauth read cray-reds-view snmp-server user testuser cray-reds-group 3 auth md5 xxxxxxxx priv des xxxxxxx snmp-server view cray-reds-view 1.3.6.1.2 included Aruba SNMP sw-leaf-bmc-001# conf t snmp-server vrf default snmpv3 user testuser auth md5 auth-pass plaintext xxxxxx priv des priv-pass plaintext xxxxx For more information on SNMP credentials, see Change SNMP Credentials on Leaf-BMC Switches and Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials.\nWrite memory Save the configuration once the configuration is applied. See Saving Configuration.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/external_dns/external_dns_csi_config_init_input_values/",
	"title": "External DNS CSI Input Values",
	"tags": [],
	"description": "",
	"content": "External DNS CSI Input Values External DNS requires the system-name, site-domain, and cmn-external-dns values that are defined with the csi config init command. These values are used to customize the External DNS configuration during installation.\nThe system-name and site-domain values The system-name and site-domain values specified as part of the csi config init are used together in the system-name.site-domain format, creating the external domain for external hostnames for services accessible from the Customer Management Network (CMN). Changing this value requires updating all impacted external-dns.alpha.kubernetes.io/hostname annotations, VirtualService and possibly Gateway objects, the CoreDNS ConfigMap, Keycloak settings for valid OAuth callback URLs, OAuth2 Proxy configuration, and generating new certificates.\nWarning: Changing the system-name.site-domain value post-installation is not recommended because of the complexity of changes required.\nInput for csi config init:\n--system-name testsystem --site-domain example.com The cmn-external-dns value The cmn-external-dns value is the IP address that DNS queries under the combined system-name.site-domain values need to be delegated.\nThis will be the shared IP address for services/cray-dns-powerdns-cmn-tcp and services/cray-dns-powerdns-cmn-udp services, which must be an IP address in the cmn-static-pool subnet defined in the csi config init input. See Customer Accessible Networks for more information.\nChanging this value requires updating the loadBalancerIP value of the services/cray-dns-powerdns-cmn-tcp and services/cray-dns-powerdns-cmn-udp services.\nInput for csi config init:\n--cmn-external-dns 10.102.5.30 This input is the CMN IP address for resolution of system services.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/dns/manage_the_dns_unbound_resolver/",
	"title": "Manage the DNS Unbound Resolver",
	"tags": [],
	"description": "",
	"content": "Manage the DNS Unbound Resolver The unbound DNS instance is used to resolve names for the physical equipment on the management networks within the system, such as NCNs, UANs, switches, and compute nodes. This instance is accessible only within the HPE Cray EX system.\nCheck the status of the cray-dns-unbound pods Unbound logs View manager (DNS Helper) logs Restart Unbound Clear bad data in the Unbound ConfigMap Change the site DNS server Check the status of the cray-dns-unbound pods Check the status of the pods:\nncn-mw# kubectl get -n services pods | grep unbound Example output:\ncray-dns-unbound-696c58647f-26k4c 2/2 Running 0 121m cray-dns-unbound-696c58647f-rv8h6 2/2 Running 0 121m cray-dns-unbound-coredns-q9lbg 0/2 Completed 0 121m cray-dns-unbound-manager-1596149400-5rqxd 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-8ppv4 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-cwksv 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-dtm9p 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-hckmp 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-t24w6 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-vzxnp 0/2 Completed 0 20h cray-dns-unbound-manager-1596222000-bcsk7 0/2 Completed 0 2m48s cray-dns-unbound-manager-1596222060-8pjx6 0/2 Completed 0 118s cray-dns-unbound-manager-1596222120-hrgbr 0/2 Completed 0 67s cray-dns-unbound-manager-1596222180-sf46q 1/2 NotReady 0 7s For more information about the pods displayed in the output above:\ncray-dns-unbound-xxx - These are the main unbound pods. cray-dns-unbound-manager-yyy - These are job pods that run periodically to update DNS from DHCP (Kea) and the SLS/SMD content for the Hardware State Manager (HSM). Pods will go into the Completed status, and then independently be reaped later by Kubernetes. cray-dns-unbound-coredns-zzz - This pod is run one time during installation of Unbound and reconfigures CoreDNS/ExternalDNS to point to Unbound for all site/internet lookups. The table below describes what the status of each pod means for the health of the cray-dns-unbound services and pods. The Init and NotReady states are not necessarily bad; they mean that the pod is being started or is processing. The cray-dns-manager and cray-dns-coredns pods for cray-dns-unbound are job pods that run periodically.\nPod Healthy Status Error Status Other cray-dns-unbound Running CrashBackOffLoop cray-dns-coredns Completed CrashBackOffLoop InitNotReady cray-dns-manager Completed CrashBackOffLoop InitNotReady Unbound logs Logs for the Unbound pods will show the status and health of actual DNS lookups. Any logs with ERROR or Exception are an indication that the Unbound service is not healthy.\nncn-mw# kubectl logs -n services -l app.kubernetes.io/instance=cray-dns-unbound -c unbound Example output:\n[1596224129] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224129] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224135] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224135] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224140] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224140] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224145] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224145] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224149] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224149] unbound[8:0] debug: using localzone health.check.unbound. transparent ...snip... [1597020669] unbound[8:0] error: error parsing local-data at 33 \u0026#39;69.0.254.10.in-addr.arpa. PTR .local\u0026#39;: Empty label [1597020669] unbound[8:0] error: Bad local-data RR 69.0.254.10.in-addr.arpa. PTR .local [1597020669] unbound[8:0] fatal error: Could not set up local zones Troubleshooting Unbound log errors If there are any errors in the Unbound logs:\nThe using localzone health.check.unbound. transparent log is not an issue. Typically, any error seen in Unbound, including the example above, falls under one of two categories: A bad configuration from a misconfiguration in the Helm chart. Currently, only the site/external DNS lookup can be at fault. ACTION: See the customization.yaml file and look at the system_to_site_lookup values. Ensure that the external lookup values are valid and working. Bad data (as shown in the above example) comes only from the DNS Helper and can be seen in the manager logs. ACTION: See View manager (DNS Helper) logs. View manager (DNS Helper) logs Manager logs will show the status of the latest \u0026ldquo;true up\u0026rdquo; of DNS with respect to DHCP actual leases and SLS/SMD status.\nThe following command shows the last four lines of the last Manager run, and can be adjusted as needed.\nncn-mw# kubectl logs -n services pod/$(kubectl get -n services pods | grep unbound | tail -n 1 | cut -f 1 -d \u0026#39; \u0026#39;) -c manager | tail -n4 Example output:\nuid: bc1e8b7f-39e2-49e5-b586-2028953d2940 Comparing new and existing DNS records. No differences found. Skipping DNS update Any log with ERROR or Exception is an indication that DNS is not healthy. The above example includes one of two possible reports for a healthy manager run. The healthy states are described below, as long as the write to the ConfigMap has not failed:\nNo differences found. Skipping DNS update Differences found. Writing new DNS records to our configmap. Troubleshooting the Manager The Manager runs periodically, about once every minute. Check if this is a one-time occurrence or if it is a recurring issue.\nIf the error shows in one Manager log, but not during the next one, then this is likely a one-time failure. Check to see if the record exists in DNS, and if so, move on. If several or all Manager logs show errors, particularly the same error, then this could be one of several sources: Bad network connections to DHCP or SLS/SMD. ACTION: Capture as much log data as possible and contact customer support. Bad data from DHCP or SLS/SMD. ACTION: If connections to DHCP (Kea) are involved, then refer to Troubleshoot DHCP Issues. Restart Unbound If any errors discovered in the sections above have been deemed transient or have not been resolved, then restart the Unbound pods.\nUse the following command to restart the pods:\nncn-mw# kubectl -n services rollout restart deployment cray-dns-unbound A rolling restart of the Unbound pods will occur; old pods will not be terminated and new pods will not be added to the load balancer until the new pods have successfully loaded the DNS records.\nClear bad data in the Unbound ConfigMap Unbound stores records it obtains from DHCP, SLS, and SMD via the Manager job in a ConfigMap. It is possible to clear this ConfigMap and allow the next Manager job to regenerate the content.\nThis is useful in the following cases:\nA transient failure in any Unbound process or required services has left the configuration data in a bad state. SLS and SMD data needed to be reset because of bad or incorrect data there. DHCP (Kea) has been restarted to clear errors. The following clears the (DNS Helper) Manager generated data in the ConfigMap. This is generally safe as Unbound runtime data is held elsewhere.\nncn-mw# kubectl -n services patch configmaps cray-dns-unbound --type merge -p \u0026#39;{\u0026#34;binaryData\u0026#34;:{\u0026#34;records.json.gz\u0026#34;:\u0026#34;H4sICLQ/Z2AAA3JlY29yZHMuanNvbgCLjuUCAETSaHADAAAA\u0026#34;}}\u0026#39; Change the site DNS server Use the following procedure to change the site DNS server that Unbound forwards queries to. This may be necessary if the site DNS server is moved to a different IP address.\nEdit the cray-dns-unbound ConfigMap.\nncn-mw# kubectl -n services edit configmap cray-dns-unbound Update the forward-zone value in unbound.conf.\nforward-zone: name: . forward-addr: 172.30.84.40 Multiple DNS servers can be defined if required.\nforward-zone: name: . forward-addr: 172.30.84.40 forward-addr: 192.168.0.1 Restart cray-dns-unbound for this change to take effect.\nncn-mw# kubectl -n services rollout restart deployment cray-dns-unbound Example output:\ndeployment.apps/cray-dns-unbound restarted Update customizations.yaml.\nIMPORTANT: If this step is not performed, then the Unbound configuration will be overwritten with the previous value the next time CSM or Unbound is upgraded.\nExtract customizations.yaml from the site-init secret in the loftsman namespace.\nncn-mw# kubectl -n loftsman get secret site-init -o json | jq -r \u0026#39;.data.\u0026#34;customizations.yaml\u0026#34;\u0026#39; | base64 -d \u0026gt; customizations.yaml Update system_to_site_lookups with the value of the new DNS server.\nspec: network: netstaticips: system_to_site_lookups: 172.30.84.40 If multiple DNS servers are required, add the additional servers into the cray-dns-unbound service configuration.\nspec: kubernetes: services: cray-dns-unbound: forwardZones: - name: \u0026#34;.\u0026#34; forwardIps: - \u0026#34;{{ network.netstaticips.system_to_site_lookups }}\u0026#34; - \u0026#34;192.168.0.1\u0026#34; domain_name: \u0026#39;{{ network.dns.external }}\u0026#39; Update the site-init secret in the loftsman namespace.\nncn-mw# kubectl delete secret -n loftsman site-init ncn-mw# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml "
},
{
	"uri": "/docs-csm/en-12/operations/network/customer_accessible_networks/dual_spine_configuration/",
	"title": "CAN/CMN with Dual-Spine Configuration",
	"tags": [],
	"description": "",
	"content": "CAN/CMN with Dual-Spine Configuration The Customer Access Network (CAN) and Customer Management Network (CMN) needs to be connected to both spines in a dual-spine configuration so that each spine can access the outside network. However, the NCNs should only have one default gateway. Therefore, the multi-active gateway protocol (MAGP) on the Mellanox spines can be used to create a virtual router gateway IP address that can direct to either of the spines, depending on the state of the spines. The Virtual Switching Extension (VSX) for Aruba spines serve the same purpose.\nFor more information:\nMellanox: https://community.mellanox.com/s/article/howto-configure-magp-on-mellanox-switches Aruba: https://www.arubanetworks.com/techdocs/AOS-CX/10.04/HTML/5200-6728/index.html#book.html The following is an example of the point-to-point configuration on the spine switches. The IP address should be replaced with the IP address chosen by the customer that matches the switch configuration.\nIn CSM 1.2 the Customer VRF is introduced, this requires the interface that is connected to the site to be configured to use this VRF.\nMellanox:\ninterface ethernet 1/11 speed auto force interface ethernet 1/11 description to-can interface ethernet 1/11 no switchport force interface ethernet 1/11 vrf forwarding Customer interface ethernet 1/11 ip address 10.101.15.150/30 primary Aruba:\ninterface 1/1/36 no shutdown vrf attach Customer description to-can ip address 10.101.15.150/30 exit There must then be two routes on the customer\u0026rsquo;s switch directing traffic for the customer_access_network and customer_management_network subnet to the endpoint on the spine switch. The following is an example of the route configuration on the customer switch.\nip route 10.103.9.0/25 10.101.15.150 ip route 10.103.9.0/25 10.101.15.152 The next hop IP address 10.101.15.150 would be the interface IP address on the Spine switch.\nThere must be a default route on each spine switch that will direct traffic that does not match other routes to the endpoint on the customer switch. The following examples are for the route configuration on sw-spine-001.\nMellanox:\nip route vrf Customer 0.0.0.0/0 10.101.15.149 Aruba:\nip route 0.0.0.0/0 10.101.15.149 vrf Customer Distribution/Site Spine Switch Connection The connection between the distribution/site switch and the spines require two separate uplinks from the spine switch to the distribution switch. Two static routes need to be created on the distribution switch to route the CAN subnet to each of the spine switches. These routes will have equal cost (ECMP) to split the load across the two spines and provide redundancy if one of the spines should go down.\nExample\ninterface 1/1/41 no shutdown description WASP spine-001 1/11 ip address 10.101.15.149/30 interface 1/1/42 no shutdown description WASP spine-002 1/11 ip address 10.101.15.153/30 ip route 10.102.5/26 10.101.15.150 ip route 10.102.5/26 10.101.15.154 "
},
{
	"uri": "/docs-csm/en-12/operations/network/create_a_csm_configuration_upgrade_plan/",
	"title": "Create a CSM Configuration Upgrade Plan",
	"tags": [],
	"description": "",
	"content": "Create a CSM Configuration Upgrade Plan Creating an upgrade plan is unique and dependent on the requirements of the upgrade path. Some release versions of the network configuration require coupled upgrade of software to enable new software functionality, or bug fixes that may add time required to do the full upgrade.\nFor example, in CSM release 1.2, Aruba and Mellanox switches are being upgraded to newer code.\nIn this case and cases where configuration changes are extensive, consider taking the generated configurations after review and uploading them to the switches startup config prior to booting to new code to upgrade both configuration and software simultaneously. This will prevent human error, especially from extensive changes such as modifying a high number of ports away and installing the generated configuration via the system without having to do the individual changes by hand.\nIn addition to firmware upgrade paths, the application of CANU-generated switch configurations should be carefully considered and detailed. The following are important considerations:\nCritically analyze proposed changes to ensure the customer does not have an unexpected outage. Provide a holistic upgrade plan, which includes switch-by-switch ordered changes and minimizes system outages. Typically, this should begin on the periphery of the network (leaf-bmcs) and move centrally towards spines and site uplinks. Where system outages or interruptions are expected to occur, provide details on the change order of operations, expected timing of interruptions, and guidance should the interruption be beyond expected timing. The resulting \u0026ldquo;plan\u0026rdquo; will provide a procedure to upgrade the system from the current state to a newer version.\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/about_etcd/",
	"title": "About etcd",
	"tags": [],
	"description": "",
	"content": "About etcd The system uses etcd for storing all of its cluster data. It is an open source database that is excellent for maintaining the state of Kubernetes. Failures in the etcd cluster at the heart of Kubernetes will cause a failure of Kubernetes. To mitigate this risk, the system is deployed with etcd on dedicated disks and with a specific configuration to optimize Kubernetes workloads. The system also provides additional etcd cluster(s) as necessary to help maintain an operational state of services. These additional clusters are managed by a Kubernetes operator and do not interact with the core Kubernetes etcd service.\nTo learn more about etcd, refer to the following links:\nGeneral documentation - https://github.com/etcd-io/etcd README - https://github.com/etcd-io/etcd/tree/master/etcdctl etcd upstream performance - https://etcd.io/docs/v3.5/benchmarks/ Usage of etcd on the System Communication between etcd machines is handled via the Raft consensus algorithm. Latency from the etcd leader is the most important metric to track because severe latency will introduce instability within the cluster. Raft is only as fast as the slowest machine in the majority. This problem can be mitigated by properly tuning the cluster.\netcd is a highly available key value store that runs on the three non-compute nodes (NCNs) that act as Kubernetes worker nodes. The three node cluster size deployment is used to meet the minimum requirements for resiliency. Scaling to more nodes will provide more resiliency, but it will not provide more speed. For example, one write to the cluster is actually three writes, so one to each instance. Scaling to five or more instances in a cluster would mean that one write will actually equal five writes to the cluster.\nThe system utilizes etcd in two major ways:\netcd running on bare-metal with a dedicated disk partition Supports only Kubernetes Includes a dedicated partition to provide the best throughput and scalability Enables the Kubernetes services to be scaled, as well as the physical nodes running those services Run on the Kubernetes master nodes and will not relocate Handles replication and instance re-election in the event of a node failure Backed up to a Ceph Rados Gateway (S3 compatible) bucket etcd running via a Kubernetes operator Services utilize this to deploy an etcd cluster on the worker nodes The etcd pods are mobile and will relocate in the event of a pod or node failure Each etcd cluster can be backed up to a Ceph Rados Gateway (S3 compatible) bucket This option is decided by the service owner or developer as some information has an extremely short lifespan, and by the time the restore could be performed, the data would be invalid "
},
{
	"uri": "/docs-csm/en-12/operations/image_management/configure_ims_to_validate_rpms/",
	"title": "Configure IMS to Validate RPMs",
	"tags": [],
	"description": "",
	"content": "Configure IMS to Validate RPMs Configuring the Image Management Service (IMS) to validate the GPG signatures of RPMs during IMS Build operations involves the following two steps:\nCreate and update IMS to use a new Kiwi-NG Image with the Signing Keys embedded.\nNOTE: The default IMS Kiwi-NG Image is already configured with the signing keys needed to validate HPE and SuSE RPMs and repositories.\nUpdate IMS Recipes to require GPG verification of RPMs, repositories, or both.\nCreate and Update IMS to Use a New Kiwi-NG Image with an Embedded Signing Key Create a temporary directory to perform the actions necessary to configure IMS to validate RPM signatures.\nncn# mkdir ims-validate ncn# cd ims-validate/ Determine the container version for the IMS Kiwi-NG container.\nncn# kubectl -n services get cm cray-configmap-ims-v2-image-create-kiwi-ng -o yaml | grep cray-ims-kiwi-ng-opensuse-x86_64-builder Example output:\n- image: cray/cray-ims-kiwi-ng-opensuse-x86_64-builder:0.4.7 If successful, make note of the version of the listed container. In this case, the version is 0.4.7.\nCreate a file containing the public portion of the Signing Key to be added to the IMS Kiwi-NG image.\nncn# cat my-signing-key.asc -----BEGIN PGP PUBLIC KEY BLOCK----- ... -----END PGP PUBLIC KEY BLOCK----- Obtain a copy of the entrypoint.sh script from cray-ims-kiwi-ng-opensuse-x86_64-builder.\nncn# podman run -it --entrypoint \u0026#34;\u0026#34; --rm cray/cray-ims-kiwi-ng-opensuse-x86_64-builder:0.4.7 cat /scripts/entrypoint.sh | tee entrypoint.sh Modify the entrypoint.sh script to pass the signing key to the kiwi-ng command.\nncn# cat entrypoint.sh Example output:\n[...] # Call kiwi to build the image recipe. Note that the command line --add-bootstrap-package # causes kiwi to install the cray-ca-cert RPM into the image root. kiwi-ng $DEBUG_FLAGS --logfile=$PARAMETER_FILE_KIWI_LOGFILE --type tbz system build --description $RECIPE_ROOT_PARENT \\ --target $IMAGE_ROOT_PARENT --add-bootstrap-package file:///mnt/ca-rpm/cray_ca_cert-1.0.1-1.x86_64.rpm \\ --signing-key /signing-keys/my-signing-key.asc # \u0026lt;--- ADD SIGNING-KEY FILE [...] Create a Dockerfile to create a new cray-ims-kiwi-ng-opensuse-x86_64-builder image.\nncn# cat Dockerfile FROM registry.local/cray/cray-ims-kiwi-ng-opensuse-x86_64-builder:0.4.7 RUN mkdir /signing-keys COPY my-signing-key.asc /signing-keys COPY entrypoint.sh /scripts/entrypoint.sh ENTRYPOINT [\u0026#34;/scripts/entrypoint.sh\u0026#34;] NOTE: Make sure that the version of the cray-ims-kiwi-ng-opensuse-x86_64-builder image in the FROM line matches the version of the image above.\nVerify that the following files are in the temporary directory.\nncn# ls Dockerfile entrypoint.sh my-signing-key.asc Using the podman command, build and tag a new cray-ims-kiwi-ng-opensuse-x86_64-builder image.\nncn# podman build -t registry.local/cray/cray-ims-kiwi-ng-opensuse-x86_64-builder:0.4.7-validate . STEP 1: FROM registry.local/cray/cray-ims-kiwi-ng-opensuse-x86_64-builder:0.4.7 STEP 2: RUN mkdir /signing-keys --\u0026gt; Using cache 5d64aadcffd3f9f8f112cca75b886cecfccbfe903d4b0d4176882f0e78ccd4d0 --\u0026gt; 5d64aadcffd STEP 3: COPY my-signing-key.asc /signing-keys --\u0026gt; Using cache c10ffb877529bdbe855522af93827503f76d415e2e129d171a7fc927f896095a --\u0026gt; c10ffb87752 STEP 4: COPY entrypoint.sh /scripts/entrypoint.sh --\u0026gt; Using cache 6e388b60f42b6cd26df65ec1798ad771bdb835267126f16aa86e90aec78b0f32 --\u0026gt; 6e388b60f42 STEP 5: ENTRYPOINT [\u0026#34;/scripts/entrypoint.sh\u0026#34;] --\u0026gt; Using cache 46c78827eb62c66c9f42aeba12333281b073dcc80212c4547c8cc806fe5519b3 STEP 6: COMMIT registry.local/cray/cray-ims-kiwi-ng-opensuse-x86_64-builder:0.4.7-validate --\u0026gt; 46c78827eb6 46c78827eb62c66c9f42aeba12333281b073dcc80212c4547c8cc806fe5519b3 Obtain Nexus credentials.\nncn# NEXUS_USERNAME=\u0026#34;$(kubectl -n nexus get secret nexus-admin-credential --template {{.data.username}} | base64 -d)\u0026#34; ncn# NEXUS_PASSWORD=\u0026#34;$(kubectl -n nexus get secret nexus-admin-credential --template {{.data.password}} | base64 -d)\u0026#34; Push the new image to the Nexus image registry.\nncn# podman push registry.local/cray/cray-ims-kiwi-ng-opensuse-x86_64-builder:0.4.7-validate --creds=\u0026#34;$NEXUS_USERNAME:$NEXUS_PASSWORD\u0026#34; Update the IMS cray-configmap-ims-v2-image-create-kiwi-ng ConfigMap to use this new image.\nncn# kubectl -n services edit cm cray-configmap-ims-v2-image-create-kiwi-ng Example output:\n[...] - image: cray/cray-ims-kiwi-ng-opensuse-x86_64-builder:0.4.7-validate [...] NOTE: It may take several minutes for this change to take effect. Restarting IMS is not necessary.\nCleanup and remove the temporary directory\nncn# cd .. ncn# rm -rfv ims-validate/ Update IMS Recipes to Require GPG Verification of RPMs/Repos List the IMS recipes and determine which recipes need to be updated.\nncn# cray ims recipes list --format json Example output:\n[ [...] { \u0026#34;created\u0026#34;: \u0026#34;2021-06-29T21:50:38.319526+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1aab3dbb-a654-4c84-b820-a293bd4ab2b4\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://ims/recipes/1aab3dbb-a654-4c84-b820-a293bd4ab2b4/my_recipe.tgz\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;linux_distribution\u0026#34;: \u0026#34;sles15\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cos-2.1.51-slingshot-1.2.1\u0026#34;, \u0026#34;recipe_type\u0026#34;: \u0026#34;kiwi-ng\u0026#34; }, [...] ] Download the recipe archive for any recipe that will be updated.\nncn# cray artifacts get ims recipes/1aab3dbb-a654-4c84-b820-a293bd4ab2b4/recipe.tar.gz recipe.tar.gz Uncompress the recipe archive into a temporary directory.\nncn# mkdir -v recipe ncn# tar xvfz recipe.tar.gz -C recipe/ ncn# cd recipe/ Modify the recipe\u0026rsquo;s config.xml file and enable GPG validation on any repos that should be validated. To validate each package\u0026rsquo;s GPG signature, add package_gpgcheck=\u0026quot;true\u0026quot;. To validate the repository signature, add repository_gpgcheck=\u0026quot;true\u0026quot;.\n\u0026lt;repository type=\u0026#34;rpm-md\u0026#34; alias=\u0026#34;...\u0026#34; priority=\u0026#34;2\u0026#34; imageinclude=\u0026#34;true\u0026#34; package_gpgcheck=\u0026#34;true\u0026#34;\u0026gt; ... \u0026lt;/repository\u0026gt; \u0026lt;repository type=\u0026#34;rpm-md\u0026#34; alias=\u0026#34;...\u0026#34; priority=\u0026#34;2\u0026#34; imageinclude=\u0026#34;true\u0026#34; repository_gpgcheck=\u0026#34;true\u0026#34;\u0026gt; ... \u0026lt;/repository\u0026gt; Create a new recipe tar file.\nncn# tar cvfz ../recipe-new.tgz . Move to the parent directory.\nncn# cd .. Create a new IMS recipe record.\nncn# cray ims recipes create --name \u0026#34;My Recipe\u0026#34; \\ --recipe-type kiwi-ng --linux-distribution sles15 Example output:\ncreated = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34; linux_distribution = \u0026#34;sles15\u0026#34; name = \u0026#34;my_recipe.tgz\u0026#34; recipe_type = \u0026#34;kiwi-ng\u0026#34; If successful, create a variable for the id value in the returned data.\nncn# IMS_RECIPE_ID=2233c82a-5081-4f67-bec4-4b59a60017a6 Upload the customized recipe to S3.\nIt is suggested as a best practice that the S3 object name start with recipes/ and contain the IMS recipe ID to remove ambiguity.\nncn# cray artifacts create ims recipes/$IMS_RECIPE_ID/recipe.tgz recipe-new.tgz Update the IMS recipe record with the S3 path to the recipe archive.\nncn-m001# cray ims recipes update $IMS_RECIPE_ID \\ --link-type s3 \\ --link-path s3://ims/recipes/$IMS_RECIPE_ID/recipe.tgz Example output:\nid = \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34; recipe_type = \u0026#34;kiwi-ng\u0026#34; linux_distribution = \u0026#34;sles15\u0026#34; name = \u0026#34;my_recipe.tgz\u0026#34; created = \u0026#34;2020-02-05T19:24:22.621448+00:00\u0026#34; [link] path = \u0026#34;s3://ims/recipes/2233c82a-5081-4f67-bec4-4b59a60017a6/my_recipe.tgz\u0026#34; etag = \u0026#34;\u0026#34; type = \u0026#34;s3\u0026#34; Cleanup and remove the temporary directory.\nncn# cd .. ncn# rm -rf recipe/ "
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/component_group_members/",
	"title": "Component Group Members",
	"tags": [],
	"description": "",
	"content": "Component Group Members The members object in the group definition has additional actions available for managing the members after the group has been created.\nThe following is an example of group members:\n{ \u0026#34;ids\u0026#34; : [ \u0026#34;x0c0s0b0n0\u0026#34;,\u0026#34;x0c0s0b0n1\u0026#34;,\u0026#34;x0c0s0b1n0\u0026#34; ] } Retrieve Group Members Retrieve just the members array for a group:\nncn-m# cray hsm groups members list GROUP_LABEL Retrieve only the members of a group that are also in a specific partition:\nncn-m# cray hsm groups members list --partition PARTITION_NAME GROUP_LABEL Retrieve only the members of a group that are not in any partition currently:\nncn-m# cray hsm groups members list --partition NULL GROUP_LABEL Add Group Members Add a single component to a group. The only time this is not permitted is if the component already exists, or the group has an exclusiveGroup label and the component is already a member of a group with that exclusive label.\nAdd a component to a group:\nncn-m# cray hsm groups members create --id MEMBER_ID GROUP_LABEL For example:\nncn-m# cray hsm groups members create --id x1c0s0b0n0 blue Remove Group Members Single members with the specified component name (xname) are removed from the given group.\nRemove a member from a group:\nncn-m# cray hsm groups members delete MEMBER_ID GROUP_LABEL For example:\nncn-m# cray hsm groups members delete x1c0s0b0n0 blue "
},
{
	"uri": "/docs-csm/en-12/operations/firmware/fas_filters/",
	"title": "FAS Filters",
	"tags": [],
	"description": "",
	"content": "FAS Filters FAS uses five primary filters for actions and snapshots to determine what operations to create. The filters are listed below:\nSelection Filters - Determine what operations will be created. The following selection filters are available: stateComponentFilter targetFilter inventoryHardwareFilter imageFilter Command Filters - Determine how the operations will be executed. The following command filters are available: command All filters are logically connected with AND logic. Only the stateComponentFilter, targetFilter, and inventoryHardwareFilter are used for snapshots.\nSelection Filters stateComponentFilter The state component filter allows users to select hardware to update. Hardware can be selected individually with component names (xnames), or in groups by leveraging the Hardware State Manager (HSM) groups and partitions features.\nParameters xnames - A list of component names (xnames) to target. partitions - A partition to target. groups- A group to target. deviceTypes Set to NodeBMC, RouterBMC, or ChassisBMC. These are the ONLY three allowed types and come from the Hardware State Manager (HSM). inventoryHardwareFilter The inventory hardware filter takes place after the state component filter has been applied. It will remove any devices that do not conform to the identified manufacturer or models determined by querying the Redfish endpoint.\nIMPORTANT: There can be a mismatch of hardware models. The model field is human-readable and is human-programmable. In some cases, there can be typos where the wrong model is programmed, which causes issues filtering. If this occurs, query the hardware, find the model name, and add it to the images repository on the desired image.\nParameters manufacturer - Set to Cray, HPE, or Gigabyte. model - The Redfish reported model, which can be specified. imageFilter FAS applies images to component name (xname)/targets. The image filter is a way to specify an explicit image that should be used. When included with other filters, the image filter reduces the devices considered to only those devices where the image can be applied.\nFor example, consider if a user specifies an image that only applies to Gigabyte nodeBMC BIOS targets. If all hardware in the system is targeted with an empty stateComponentFilter, FAS would find all devices in the system that can be updated via Redfish, and then the image filter would remove all component name (xname)/ targets that this image could not be applied to. In this example, FAS would remove any device that is not a Gigabyte nodeBMC, as well as any target that is not BIOS.\nParameters imageID - The ID of the image to force onto the system. overrideImage - If this is combined with imageID, then it will FORCE the selected image onto all hardware identified, even if it is not applicable. WARNING: This may cause undesirable outcomes, but most hardware will prevent a bad image from being loaded.\ntargetFilter The target filter selects targets that match against the list. For example, if the user specifies only the BIOS target, FAS will include only operations that explicitly have BIOS as a target. A Redfish device has potentially many targets (members). Targets for FAS are case sensitive and must match Redfish.\nParameters targets - The actual \u0026lsquo;members\u0026rsquo; that will be upgraded. Examples include, but are not limited to the following: BIOS BMC NIC Node0.BIOS Node1.BIOS Recovery Command Filters command The command group is the most important part of an action command and controls if the action is executed as dry-run or a live update.\nIt also determines whether or not to override an operation that would normally not be executed if there is no way to return the component name (xname)/target to the previous firmware version. This happens if an image does not exist in the image repository.\nThese filters are then applied; and then command parameter applies settings for the overall action. The swagger file is a great reference.\nParameters version - Usually latest because that is the most common use case. tag - Usually default because the default image is the most useful one to use. This parameter can usually be ignored. overrideDryrun - This determines if this is a LIVE UPDATE or a DRY-RUN. If doing an override; then it will provide a live update. restoreNotPossibleOverride - This determines if an update (live or dry-run) will be attempted if a restore cannot be performed. Typically there is not enough firmware to be able to do a rollback, which means if the system is an UPDATE away from a particular version, it cannot go back to a previous version. It is most likely that this value will ALWAYS need to be set true. overwriteSameImage - This will cause a firmware update to be performed EVEN if the device is already at the identified, selected version. timeLimit - This is the amount of time in seconds that any operation should be allowed to execute. Most cray hardware can be completed in approximately 1000 seconds or less; but the gigabyte hardware will commonly take 1500 seconds or greater. Setting the value to 4000 is recommended as a stop gap to prevent the operation from never ending, should something get stuck. description- A human-friendly description that should be set to give useful information about the firmware operation. "
},
{
	"uri": "/docs-csm/en-12/operations/conman/conman/",
	"title": "ConMan",
	"tags": [],
	"description": "",
	"content": "ConMan ConMan is a tool used for connecting to remote consoles and collecting console logs. These node logs can then be used for various administrative purposes, such as troubleshooting node boot issues.\nConMan runs on the system as a containerized service. It runs in a set of Docker containers within Kubernetes pods named cray-console-operator and cray-console-node. Node console logs are stored locally within the cray-console-node pods in the /var/log/conman/ directory, as well as being collected by the System Monitoring Framework (SMF).\nIn CSM versions 1.0 and later, the ConMan logs and interactive consoles are accessible through one of the cray-console-node pods. There are multiple cray-console-node pods, scaled to the size of the system.\nHow To Use See Log in to a Node Using ConMan for more information.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/automatic_session_deletion_with_sessionttl/",
	"title": "Automatic Session Deletion with sessionTTL",
	"tags": [],
	"description": "",
	"content": "Automatic Session Deletion with sessionTTL By default, the Configuration Framework Service (CFS) will delete completed CFS sessions whose start date was more than seven days prior. Kubernetes jobs associated with these sessions will also be deleted as part of this process. This is done to ensure that CFS sessions do not accumulate and eventually adversely affect the performance of the Kubernetes cluster.\nFor larger systems or systems that do frequent reboots of nodes that are configured with CFS sessions, this setting may need to be reduced.\nIMPORTANT: The sessionTTL option deletes all completed sessions that meet the TTL criteria, regardless of if they were successful.\nPrerequisites This requires that the Cray command line interface is configured. See Configure the Cray Command Line Interface.\nUpdate sessionTTL Update the sessionTTL using the following command:\nncn-mw# cray cfs options update --session-ttl 24h --format toml Example output will contain a line resembling the following:\nsessionTTL = \u0026#34;24h\u0026#34; Disable sessionTTL To disable the sessionTTL feature, use an empty string as the argument of the --session-ttl flag:\nncn-mw# cray cfs options update --session-ttl \u0026#34;\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_nodes_failing_to_upgrade_in_a_crus_session/",
	"title": "Troubleshoot Nodes Failing to Upgrade in a CRUS Session",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Nodes Failing to Upgrade in a CRUS Session Note: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality. See Deprecated features.\nTroubleshoot compute nodes failing to upgrade during a Compute Rolling Upgrade Service (CRUS) session and rerun the session on the failed nodes.\nWhen nodes are marked as failed they are added to the failed node group associated with the upgrade session, and the nodes are marked as down in the workload manager (WLM). If the WLM supports some kind of reason string, that string contains the cause of the down status.\nComplete a CRUS session that did not successfully upgrade all of the intended compute nodes.\nPrerequisites A CRUS upgrade session has completed with a group of nodes that failed to upgrade. The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray Command Line Interface. Procedure Determine which nodes failed the upgrade by listing the contents of the Hardware State Manager (HSM) group that was set up for failed nodes.\nncn# cray hsm groups describe FAILED_NODES_GROUP --format toml Example output:\nlabel = \u0026#34;failed-nodes\u0026#34; description = \u0026#34;\u0026#34; [members] ids = [ \u0026#34;x0c0s28b0n0\u0026#34;,] Determine the cause of the failed nodes and fix it.\nFailed nodes result from the following:\nFailure of the BOS upgrade session for a given step of the upgrade causes all of the nodes in that step to be marked as failed. Failure of any given node in a step to reach a ready state in the workload manager within 10 minutes of detecting that the BOS boot session has completed causes that node to be marked as failed. Deletion of a CRUS session while the current step is at or beyond the Booting stage causes all of the nodes in that step that have not reached a ready state in the workload manager to be marked as failed. Create a new CRUS session on the failed nodes.\nCreate a new failed node group with a different name.\nThis group should be empty.\nncn# cray hsm groups create --label NEW_FAILED_NODES_GROUP --description \u0026#39;Failed Node Group for my Compute Node upgrade\u0026#39; Create a new CRUS session.\nUse the label of the failed node group from the original upgrade session as the starting label, and use the new failed node group as the failed label. The rest of the parameters need to be the same ones that were used in the original upgrade.\nncn# cray crus session create \\ --starting-label OLD_FAILED_NODES_GROUP \\ --upgrading-label node-group \\ --failed-label NEW_FAILED_NODES_GROUP \\ --upgrade-step-size 50 \\ --workload-manager-type slurm \\ --upgrade-template-id boot-template --format toml Example output:\napi_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;NEW_FAILED_NODES_GROUP\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [] starting_label = \u0026#34;OLD_FAILED_NODES_GROUP\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;135f9667-6d33-45d4-87c8-9b09c203174e\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;node-group\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/boot_issue_symptom_node_hsn_interface_does_not_appear_or_shows_no_link_detected/",
	"title": "Boot Issue Symptom Node HSN Interface Does Not Appear or Show Detected Links Detected",
	"tags": [],
	"description": "",
	"content": "Boot Issue Symptom: Node HSN Interface Does Not Appear or Show Detected Links Detected A node may fail to boot if the HSN interface is experiencing issues, or if it is not able to detect any links.\nSymptom The node\u0026rsquo;s HSN interface does not appear in the output of the ip addr command or the output of the ethtool interface command shows no link detected.\nResolution Reseat the node\u0026rsquo;s PCIe card.\n"
},
{
	"uri": "/docs-csm/en-12/operations/artifact_management/manage_artifacts_with_the_cray_cli/",
	"title": "Manage Artifacts with the Cray CLI",
	"tags": [],
	"description": "",
	"content": "Manage Artifacts with the Cray CLI The artifacts (objects) available for use on the system are created and managed with the Cray CLI. The cray artifacts command provides the ability to manage any given artifact. The Cray CLI automatically authenticates users and provides Simple Storage Service (S3) credentials.\nAll operations with the cray artifacts command assume that the user has already been authenticated. If the user has not been authenticated with the Cray CLI, run the following command:\nncn# cray auth login Enter the appropriate credentials when prompted:\nUsername: adminuser Password: Success! will be returned if the user is successfully authenticated.\nAuthorization is Local to a Host: whenever you are using the CLI (cray command) on a host (e.g. a workstation or NCN) where it has not been used before, it is necessary to authenticate on that host using cray auth login. There is no mechanism to distribute CLI authorization amongst hosts.\nView S3 Buckets There are several S3 buckets available that can be used to upload and download files with the cray artifacts command. To see the list of available S3 buckets:\nncn# cray artifacts buckets list Example output:\nresults = [ \u0026#34;alc\u0026#34;, \u0026#34;badger\u0026#34;, \u0026#34;benji-backups\u0026#34;, \u0026#34;boot-images\u0026#34;, \u0026#34;etcd-backup\u0026#34;, \u0026#34;fw-update\u0026#34;, \u0026#34;ims\u0026#34;, \u0026#34;nmd\u0026#34;, \u0026#34;sds\u0026#34;, \u0026#34;ssm\u0026#34;, \u0026#34;vbis\u0026#34;, \u0026#34;wlm\u0026#34;,] Create and Upload Artifacts Use the cray artifacts create command to create an object and upload it to S3.\nIn the example below, S3_BUCKET is a placeholder for the bucket name, site/repos/repo.tgz is the object name, and /path/to/repo.tgz is the location of the file to be uploaded to S3 on the local file system.\nncn# cray artifacts create S3_BUCKET site/repos/repo.tgz /path/to/repo.tgz Example output:\nartifact = \u0026#34;5c5b6ae5-64da-4212-887a-301087a17099\u0026#34; Key = \u0026#34;site/repos/repo.tgz\u0026#34; In S3, the object name can be path-like and include slashes to resemble files in directories. This is useful for organizing objects within a bucket, but S3 treats it as a name only. No directory structure exists.\nWhen interacting with Cray services, use the artifact value returned by the cray artifacts create command. This will ensure that Cray services can access the uploaded object.\nDownload Artifacts Artifacts are downloaded with the cray artifacts get command. Provide the object name, the bucket, and a file path to download the artifact in order to use this command.\nncn# cray artifacts get S3_BUCKET S3_OBJECT_KEY DOWNLOAD_FILEPATH For example:\nncn# cray artifacts get boot-images 5c5b6ae5-64da-4212-887a-301087a17099 /path/to/downloads/dl-repo.tgz No output is shown unless an error occurs.\nDelete Artifacts Artifacts are removed from buckets with the cray artifacts delete command. Provide the object name and the bucket to delete it.\nncn# cray artifacts delete S3_BUCKET S3_OBJECT_KEY No output is shown unless an error occurs.\nList Artifacts Use the cray artifacts list command to list all artifacts in a bucket.\nncn# cray artifacts list S3_BUCKET Example output:\n[[artifacts]] LastModified = \u0026#34;2020-04-03T12:20:23.876000+00:00\u0026#34; ETag = \u0026#34;\\\u0026#34;e3f195c20a2399bf1b5a20df12416115\\\u0026#34;\u0026#34; StorageClass = \u0026#34;STANDARD\u0026#34; Key = \u0026#34;recipes/47411cbe-e249-40f2-8c13-0df7856b91a3/recipe.tar.gz\u0026#34; Size = 11234 [artifacts.Owner] DisplayName = \u0026#34;Image Management Service User\u0026#34; ID = \u0026#34;IMS\u0026#34; [...] Retrieve Artifact Details Details of an artifact object in a bucket are found with the cray artifacts describe command. The output of this command provides information about the size of the artifact and any metadata associated with the object.\nIMPORTANT: The Cray-specific metadata provided by this command is automatically generated. This metadata should be considered deprecated and should not be used for future development.\nncn# cray artifacts describe S3_BUCKET S3_OBJECT_KEY Example output:\n[artifact] AcceptRanges = \u0026#34;bytes\u0026#34; ContentType = \u0026#34;binary/octet-stream\u0026#34; LastModified = \u0026#34;2020-04-03T12:20:23+00:00\u0026#34; ContentLength = 11234 VersionId = \u0026#34;.2aoRPGDGRuRIFrjc9urQiHLADvwPCU\u0026#34; ETag = \u0026#34;\\\u0026#34;e3f195c20a2399bf1b5a20df12416115\\\u0026#34;\u0026#34; [artifact.Metadata] md5sum = \u0026#34;e3f195c20a2399bf1b5a20df12416115\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/choosing_uai_resource_settings/",
	"title": "Choosing UAI Resource Settings",
	"tags": [],
	"description": "",
	"content": "Choosing UAI Resource Settings The Resource Specifications and UAI Classes sections describe how to set up resource specifications to be used with UAIs. Refer to those sections for all procedures and specific data structures associated with resources. In this section the question of how and why to configure UAI resources is addressed. While Kubernetes resource requests and limits can be used for other things, this section focuses on memory requests and limits and CPU requests and limits. Other resource types are outside the scope of this discussion.\nBefore discussing why custom resource specifications might be used, it is worthwhile to understand how Kubernetes behaves with respect to resources.\nKubernetes uses resource requests and limits on all of its pods (a UAI is, at its most basic, a Kubernetes pod) to determine where and whether to schedule the pod. Kubernetes will not schedule work on a host node if the pod containing that work requests more CPU or memory (or any other resource) than the host node has available. Once a request for resources has been granted by the scheduler, and the pod (UAI in this case) has been scheduled there, the available resources on the host node are reduced by the requested amount. It is possible for a resource specification to set a high limit and a low request, in which case many pods (UAIS) may be scheduled on the available host nodes. In this case, there is a risk of oversubscription as these pods grow into their limits. When Kubernetes detects resource pressure on a host node, it starts evicting pods from that node. If Kubernetes can find another host node with available space, it will reschedule an evicted pod (UAI) on that node. If not, the pod will remain evicted until a host node with sufficient resources becomes available.\nThe most common reason to set custom resource limits on a class of UAIs is that the workflows within those UAIs are more computationally or memory intensive than the default namespace resources support. In this case, the site should determine what the bottleneck is (memory or CPU) and experiment with larger settings. Note that by increasing a resource request or limit on a UAI Class you decrease the capacity of the UAI host nodes for UAIs of that class. Also note that, while it may be tempting to set low request values and higher limit values, the resulting potential oversubscription of nodes can make UAIs unstable and difficult to use.\nAnother reason for setting custom resource limits on a class of UAIs is that the UAIs are very lightweight and do not need the default namespace resource requests / limits. This can increase the capacity of the pool of available UAI host nodes, for UAIs of that class. The caveat here is that Kubernetes will terminate any pod (UAI) that tries to grow past its resource limits. Making the resource limits on UAI Classes too small can lead to instability of UAIs of that class.\nTop: User Access Service (UAS)\nNext Topic: Setting End-User UAI Timeouts\n"
},
{
	"uri": "/docs-csm/en-12/operations/csm_product_management/configure_csm_packages_with_cfs/",
	"title": "Configure CSM packages with CFS",
	"tags": [],
	"description": "",
	"content": "Configure CSM packages with CFS Note: This section applies only to the 1.2.6 CSM release. Earlier versions of 1.2 do not include this playbook.\nCSM includes a playbook that should be applied to Compute and Application node images. The csm_packages.yml playbook installs the packages for both the CFS and BOS reporters. These packages are necessary for CFS and BOS to run, so a configuration layer containing the playbook must be included in the image customization for any nodes that are expected to be managed with CFS and BOS.\nSetting up the CSM configuration layer To setup the compute configuration layer, first gather the following information:\nHTTP clone URL for the configuration repository in VCS. Path to the Ansible play to run in the repository. Commit ID in the repository for CFS to pull and run on the nodes. Field Value Description cloneUrl https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git CSM configuration repository commit Example: 5081c1ecea56002df41218ee39f6030c3eebdf27 CSM configuration commit hash name Example: csm-\u0026lt;version\u0026gt; CSM configuration layer name playbook compute_nodes.yml Default Ansible playbook for CSM configuration of compute nodes Retrieve the commit in the repository to use for configuration.\nIf changes have been made to the default branch that was imported during a CSM installation or upgrade, use the commit containing the changes.\nIf no changes have been made, the latest commit on the default branch for this version of CSM should be used. Find the commit in the cray-product-catalog for the current version of CSM. For example:\nncn-mw# kubectl -n services get cm cray-product-catalog -o jsonpath=\u0026#39;{.data.csm}\u0026#39; Part of the output will be a section resembling the following:\n1.2.1: configuration: clone_url: https://vcs.cmn.SYSTEM_DOMAIN_NAME/vcs/cray/csm-config-management.git commit: 43ecfa8236bed625b54325ebb70916f55884b3a4 import_branch: cray/csm/1.9.24 import_date: 2021-07-28 03:26:01.869501 ssh_url: git@vcs.cmn.SYSTEM_DOMAIN_NAME:cray/csm-config-management.git The commit will be different for each system and version of CSM. In the above example it is 43ecfa8236bed625b54325ebb70916f55884b3a4.\nCraft a new configuration layer entry for CSM using the procedure in Update a CFS Configuration:\nThe following is an example entry for the JSON configuration file:\n{ \u0026#34;name\u0026#34;: \u0026#34;csm-\u0026lt;version\u0026gt;\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;csm_packages.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;retrieved git commit\u0026gt;\u0026#34; } "
},
{
	"uri": "/docs-csm/en-12/introduction/differences/",
	"title": "Differences from Previous Release",
	"tags": [],
	"description": "",
	"content": "Differences from Previous Release The most noteworthy changes since the previous release are described here.\nNew features Deprecated features Removed features Other changes New features The following features are new in this release:\nScaling improvements for larger systems to the following services: BOS CAPMC FAS New hardware supported in this release: Compute nodes Milan-Based Grizzly Peak with A100 40 GB GPU Milan-Based Windom Liquid Cooled System Rome-Based HPE Apollo 6500 XL675d Gen10+ with A100 40 GB GPU Rome-Based HPE Apollo 6500 XL645d Gen10+ with A100 40 GB GPU User Access Nodes (UANs) Milan-Based HPE DL 385(v2) Gen10+ Rome-Based HPE DL 385(v1) Gen10 Node consoles are now managed by cray-console-node which is based on ConMan HSM now has a v2 REST API NCN user password and SSH key management is available for both root and non-root users via NCN personalization. See Configure Non-Compute Nodes with CFS. Deprecated features The following features are no longer supported and are planned to be removed in a future release:\nHSM v1 REST API has been deprecated as of CSM version 0.9.3. The v1 HSM APIs will be removed in the CSM version 1.3 release. Many CAPMC v1 REST API and CLI features were deprecated as part of CSM version 1.0; Full removal of the deprecated CAPMC features will happen in CSM version 1.3. Further development of CAPMC service or CLI has stopped. CAPMC has entered end-of-life but will still be generally available. CAPMC is going to be replaced with the Power Control Service (PCS) in a future release. The current API/CLI portfolio for CAPMC is being pruned to better align with the future direction of PCS. More information about PCS and the CAPMC transition will be released as part of subsequent CSM releases. For more information on what features have been deprecated, see the CAPMC deprecation notice. HMNFD v1 REST API has been deprecated as of CSM version 1.2. The v1 HMNFD APIs will be removed in the CSM version 1.5 release. The Boot Orchestration Service (BOS) API is changing in the CSM V1.2.0 release: The --template-body option for the Cray CLI bos command is deprecated. Prior to CSM V1.2.0, performing a successful GET on the session status for a boot set (i.e. /v1/session/{session_id}/status/{boot_set_name}) incorrectly returned a status code of 201. It now correctly returns a status code of 200. The Compute Rolling Upgrade Service (CRUS) is deprecated in CSM 1.2.0 and will be removed in CSM 1.5.0. Enhanced BOS functionality will replace CRUS. This includes the ability to stage changes to nodes that can be acted upon later when the node reboots. It also includes the ability to reboot nodes without specifying any boot artifacts. This latter ability relies on the artifacts already having been staged. SLS support for downloading and uploading credentials in the dumpstate and loadstate REST APIs. Removed features The following features have been completely removed:\ncray-conman pod. This has been replaced by cray-console-node. CFS v1 API and CLI. The v2 API and CLI have been the default since CSM 0.9 (Shasta 1.4). Other changes "
},
{
	"uri": "/docs-csm/en-12/install/switch_pxe_boot_from_onboard_nic_to_pcie/",
	"title": "Switch PXE Boot from Onboard NIC to PCIe",
	"tags": [],
	"description": "",
	"content": "Switch PXE Boot from Onboard NIC to PCIe This section details how to migrate NCNs from using their onboard NICs for PXE booting to booting over the PCIe cards.\nSwitch PXE Boot from Onboard NIC to PCIe Enabling UEFI PXE Mode Mellanox Print Current UEFI and SR-IOV State Setting Expected Values High-Speed Network Obtaining Mellanox Tools QLogic FastLinq Kernel Modules Disabling or Removing On-Board Connections This applies to Newer systems (Spring 2020 or newer) where onboard NICs are still used.\nThis presents a need for migration for systems still using the legacy, preview topology. Specifically, systems with onboard connections to their leaf-bmc switches and NCNs need to disable/remove that connection.\nThis onboard NCN port came from before spine-switches were added to the shasta-network topology. The onboard connection was responsible for every network (MTL/NMN/HMN/CAN) and was the sole driver of PXE booting for. Now, NCNs use bond interfaces and spine switches for those networks; however, some older systems still have this legacy connection to their leaf-bmc switches and solely use it for PXE booting. This NIC is not used during runtime, and NCNs in this state should enable PXE within their PCIe devices\u0026rsquo; OpROMs and disable/remove this onboard connection.\nEnabling UEFI PXE Mode Mellanox The Mellanox CLI Tools are required to configure UEFI PXE from the Linux command line.\nOn any NCN (using 0.0.10 k8s, or 0.0.8 Ceph; anything built on ncn-0.0.21 or higher), run the following command to begin interacting with Mellanox cards:\nNOTE: If recovering NCNs with an earlier image without the Mellanox tools, refer to the Obtaining Mellanox Tools section.\nncn# mst start Now mst status and other commands like mlxfwmanager or mlxconfig will work, and devices required for these commands will be created in /dev/mst.\nPrint Current UEFI and SR-IOV State UEFI: All boots are UEFI; this needs to be enabled for access to the UEFI OpROM for configuration and for usage of UEFI firmwares. SR_IOV: This is currently DISABLED because it can attribute to longer POSTs on HPE blades (Gen10+, i.e. DL325 or DL385) with Mellanox ConnectX-5 PCIe cards. The technology is not yet enabled for virtualization usage, but may be in the future.\nUse the following snippet to display device name and current UEFI PXE state.\nncn# mst status for MST in $(ls /dev/mst/*); do mlxconfig -d ${MST} q | egrep \u0026#34;(Device|EXP_ROM|SRIOV_EN)\u0026#34; done Setting Expected Values Use the following snippet to enable and dump UEFI PXE state.\nfor MST in $(ls /dev/mst/*); do echo ${MST} mlxconfig -d ${MST} -y set EXP_ROM_UEFI_x86_ENABLE=1 mlxconfig -d ${MST} -y set EXP_ROM_PXE_ENABLE=1 mlxconfig -d ${MST} -y set SRIOV_EN=0 mlxconfig -d ${MST} q | egrep \u0026#34;EXP_ROM\u0026#34; done High-Speed Network For worker nodes with High-Speed network attachments, the PXE and SR-IOV features should be disabled.\nRun mlxfwmanager to probe and dump the Mellanox PCIe cards.\nncn# mlxfwmanager Find the device path for the HSN card, assuming it is a ConnectX-5 or other 100GB card, this should be easy to pick out.\nRun the following commands, swapping the MST variable for the actual card path.\n# Set UEFI to YES ncn# MST=/dev/mst/mt4119_pciconf1 ncn# mlxconfig -d ${MST} -y set EXP_ROM_UEFI_ARM_ENABLE=0 ncn# mlxconfig -d ${MST} -y set EXP_ROM_UEFI_x86_ENABLE=0 ncn# mlxconfig -d ${MST} -y set EXP_ROM_PXE_ENABLE=0 ncn# mlxconfig -d ${MST} -y set SRIOV_EN=0 ncn# mlxconfig -d ${MST} q | egrep \u0026#34;EXP_ROM\u0026#34; The Mellanox HSN card is now neutralized, and will only be usable in a booted system.\nObtaining Mellanox Tools For 1.4 or later systems, mft is installed in NCN images by default.\nFor 1.3 systems, obtain the Mellanox tools with the following commands:\nlinux# wget https://www.mellanox.com/downloads/MFT/mft-4.15.1-9-x86_64-rpm.tgz linux# tar -xzvf mft-4.15.1-9-x86_64-rpm.tgz linux# cd mft-4.15.1-9-x86_64-rpm/RPMS linux# rpm -ivh ./mft-4.15.1-9.x86_64.rpm linux# cd linux# mst start QLogic FastLinq These should already be configured for PXE booting.\nKernel Modules KMP modules for Qlogic are installed:\nqlgc-fastlinq-kmp-default qlgc-qla2xxx-kmp-default Disabling or Removing On-Board Connections The onboard connection can be disabled in a few ways; short of removing the physical connection, one may shutdown the switchport as well.\nIf the physical connection can be removed, this is preferred and can be done so after enabling PXE on the PCIe cards.\nIf the connection must be disabled, log in to the respective leaf-bmc switch.\nConnect to the leaf-bmc switch using serial or SSH connections.\nSelect one of the connection options below. The IP addresses and device names may vary in the commands below.\n# SSH over METAL MANAGEMENT pit# ssh admin@10.1.0.4 # SSH over NODE MANAGEMENT pit# ssh admin@10.252.0.4 # SSH over HARDWARE MANAGEMENT pit# ssh admin@10.254.0.4 # or.. serial (device name will vary). pit# minicom -b 115200 -D /dev/tty.USB1 Enter configuration mode.\nsw-leaf-bmc-001\u0026gt; configure terminal sw-leaf-bmc-001(config)#\u0026gt; Disable the NCN interfaces.\nCheck the SHCD for reference before continuing so that the interfaces connected to management NCNs are being changed. Ports 2 to 10 are commonly the master, worker, and storage nodes when there are 3 of each. Some systems may have more worker nodes or utility storage nodes, or may be racked and cabled differently.\nsw-leaf-bmc-001(config)#\u0026gt; interface range 1/1/2-1/1/10 sw-leaf-bmc-001(config)#\u0026gt; shutdown sw-leaf-bmc-001(config)#\u0026gt; write memory Enable the interfaces again at anytime by switching the shutdown command out for no shutdown.\n"
},
{
	"uri": "/docs-csm/en-12/install/bootstrap_livecd_usb/",
	"title": "Bootstrap PIT Node from LiveCD USB",
	"tags": [],
	"description": "",
	"content": "Bootstrap PIT Node from LiveCD USB The Pre-Install Toolkit (PIT) node needs to be bootstrapped from the LiveCD. There are two media available to bootstrap the PIT node: the RemoteISO or a bootable USB device. This procedure describes using the USB device. If not using the USB device, see Bootstrap PIT Node from LiveCD Remote ISO.\nThese steps provide a bootable USB with SSH enabled, capable of installing this CSM release.\nTopics Download and expand the CSM release Create the bootable media Configuration payload Generate installation files Subsequent installs (reinstalls) Initial installs (bare-metal) Verify and backup system_config.yaml Prepare Site Init Prepopulate LiveCD daemons configuration and NCN artifacts Boot the LiveCD First login Configure the running LiveCD Next topic 1. Download and expand the CSM release Fetch the base installation CSM tarball, extract it, and install the contained CSI tool.\nCreate a working area for this procedure:\nlinux# mkdir usb linux# cd usb Set up the initial typescript.\nlinux# SCRIPT_FILE=$(pwd)/csm-install-usb.$(date +%Y-%m-%d).txt linux# script -af ${SCRIPT_FILE} linux# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Set and export helper variables.\nImportant: All CSM install procedures for preparing the PIT node assume that these variables are set and exported.\nlinux# export CSM_RELEASE=csm-x.y.z linux# export SYSTEM_NAME=eniac linux# export PITDATA=/mnt/pitdata Download and expand the CSM software release.\nImportant: Ensure that you have the CSM release plus any patches or hotfixes by following the instructions in Update CSM Product Stream\nImportant: Download to a location that has sufficient space for both the tarball and the expanded tarball.\nImportant: All CSM install procedures for preparing the PIT node assume that the CSM_PATH variable has been set and exported.\nNote: Expansion of the tarball may take more than 45 minutes.\nlinux# tar -zxvf ${CSM_RELEASE}.tar.gz linux# ls -l ${CSM_RELEASE} linux# export CSM_PATH=$(pwd)/${CSM_RELEASE} The ISO and other files are now available in the directory from the extracted CSM tarball.\nInstall the latest version of CSI tool.\nlinux# rpm -Uvh --force $(find ${CSM_PATH}/rpm/cray/csm/ -name \u0026#34;cray-site-init-*.x86_64.rpm\u0026#34; | sort -V | tail -1) Install the latest documentation RPM.\nSee Check for Latest Documentation\nShow the version of CSI installed.\nlinux# csi version Expected output looks similar to the following:\nCRAY-Site-Init build signature... Build Commit : b3ed3046a460d804eb545d21a362b3a5c7d517a3-release-shasta-1.4 Build Time : 2021-02-04T21:05:32Z Go Version : go1.14.9 Git Version : b3ed3046a460d804eb545d21a362b3a5c7d517a3 Platform : linux/amd64 App. Version : 1.5.18 Configure zypper with the embedded repository from the CSM release.\nlinux# zypper ar -fG \u0026#34;${CSM_PATH}/rpm/embedded\u0026#34; \u0026#34;${CSM_RELEASE}-embedded\u0026#34; Install Podman or Docker to support container tools required to generate sealed secrets.\nPodman RPMs are included in the embedded repository in the CSM release and may be installed in your pre-LiveCD environment using zypper as follows:\nInstall podman and podman-cni-config packages:\nlinux# zypper in --repo ${CSM_RELEASE}-embedded -y podman podman-cni-config Alternatively, one may use rpm -Uvh to install RPMs (and their dependencies) manually from the ${CSM_PATH}/rpm/embedded directory.\nlinux# rpm -Uvh $(find ${CSM_PATH}/rpm/embedded -name \u0026#34;podman-*.x86_64.rpm\u0026#34; | sort -V | tail -1) \\ $(find ${CSM_PATH}/rpm/embedded -name \u0026#34;podman-cni-config-*.noarch.rpm\u0026#34; | sort -V | tail -1) Install lsscsi to view attached storage devices.\nlsscsi RPMs are included in the embedded repository in the CSM release and may be installed in your pre-LiveCD environment using zypper as follows:\nInstall lsscsi package:\nlinux# zypper in --repo ${CSM_RELEASE}-embedded -y lsscsi Alternatively, one may use rpm -Uvh to install RPMs (and their dependencies) manually from the ${CSM_PATH}/rpm/embedded directory.\nlinux# rpm -Uvh $(find ${CSM_PATH}/rpm/embedded -name \u0026#34;lsscsi-*.x86_64.rpm\u0026#34; | sort -V | tail -1) Remove CNI configuration from prior install\nIf reinstalling the system and using ncn-m001 to prepare the USB image, then remove the prior CNI configuration.\nRemove the configuration files.\nncn-m001# rm -rf /etc/cni/net.d/00-multus.conf /etc/cni/net.d/10-*.conflist /etc/cni/net.d/multus.d List the remaining contents of the /etc/cni/net.d directory.\nncn-m001# ls /etc/cni/net.d Example output:\n87-podman-bridge.conflist 99-loopback.conf.sample The exact directory contents may vary depending on the CSM version previously installed on ncn-m001.\n2. Create the bootable media Before creating the the bootable LiveCD, identify which device will be used for it.\nIdentify the USB device.\nThis example shows the USB device is /dev/sdd on the host.\nlinux# lsscsi Expected output looks similar to the following:\n[6:0:0:0] disk ATA SAMSUNG MZ7LH480 404Q /dev/sda [7:0:0:0] disk ATA SAMSUNG MZ7LH480 404Q /dev/sdb [8:0:0:0] disk ATA SAMSUNG MZ7LH480 404Q /dev/sdc [14:0:0:0] disk SanDisk Extreme SSD 1012 /dev/sdd [14:0:0:1] enclosu SanDisk SES Device 1012 - In the above example, internal disks are the ATA devices and USB drives are final two devices.\nSet a variable with your disk to avoid mistakes:\nlinux# USB=/dev/sd\u0026lt;disk_letter\u0026gt; Format the USB device.\nOn Linux, use the CSI application to do this:\nlinux# csi pit format ${USB} ${CSM_PATH}/cray-pre-install-toolkit-*.iso 50000 Note: If the previous command fails with the following error message, it indicates that this Linux computer does not have the checkmedia RPM installed. In that case, install the RPM and run csi pit format again.\nERROR: Unable to validate ISO. Please install checkmedia Install the missing RPMs linux# zypper in --repo ${CSM_RELEASE}-embedded -y libmediacheck5 checkmedia linux# csi pit format ${USB} ${CSM_PATH}/cray-pre-install-toolkit-*.iso 50000 On MacOS, use the write-livecd.sh script to do this.\nThis script is contained in the CSI tool RPM. See install latest version of the CSI tool step.\nmacos# write-livecd.sh ${USB} ${CSM_PATH}/cray-pre-install-toolkit-*.iso 50000 Note: At this point, the USB device is usable in any server with a CPU with x86_64 architecture. The remaining steps help add the installation data and enable SSH on boot.\nMount the configuration and persistent data partitions.\nlinux# mkdir -pv /mnt/cow ${PITDATA} \u0026amp;\u0026amp; mount -vL cow /mnt/cow \u0026amp;\u0026amp; mount -vL PITDATA ${PITDATA} \u0026amp;\u0026amp; mkdir -pv ${PITDATA}/configs ${PITDATA}/prep/{admin,logs} ${PITDATA}/data/{ceph,k8s} Copy and extract the tarball into the USB.\nlinux# cp -v ${CSM_PATH}.tar.gz ${PITDATA} \u0026amp;\u0026amp; tar -zxvf ${CSM_PATH}.tar.gz -C ${PITDATA}/ The USB device is now bootable and contains the CSM artifacts. This may be useful for internal or quick usage. Administrators seeking a Shasta installation must continue on to the configuration payload.\n3. Configuration payload The SHASTA-CFG structure and other configuration files will be prepared, then csi will generate a system-unique configuration payload. This payload will be used for the rest of the CSM installation on the USB device.\nGenerate Installation Files Verify and Backup system_config.yaml Prepare Site Init 3.1 Generate installation files Some files are needed for generating the configuration payload. See these topics in Prepare Configuration Payload if the information for this system has not yet been prepared.\nCommand line configuration payload Configuration payload files Note:: The USB device is usable at this time, but without SSH enabled as well as core services. This means the USB device could be used to boot the system now, and this step can be returned to at another time.\nAt this time see Create HMN Connections JSON for instructions about creating the hmn_connections.json.\nCreate the configuration input files if needed and copy them into the preparation directory.\nThe preparation directory is ${PITDATA}/prep.\nCopy these files into the preparation directory, or create them if this is an initial install of the system:\napplication_node_config.yaml (optional - see below) cabinets.yaml (optional - see below) hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml (only available after first-install generation of system files) The optional application_node_config.yaml file may be provided for further definition of settings relating to how application nodes will appear in HSM for roles and subroles. See Create Application Node YAML\nThe optional cabinets.yaml file allows cabinet naming and numbering as well as some VLAN overrides. See Create Cabinets YAML.\nThe system_config.yaml file is generated by the csi tool during the first install of a system, and can later be used for reinstalls of the system. For the initial install, the information in it must be provided as command line arguments to csi config init.\nProceed to the appropriate next step.\nIf this is the initial install of the system, then proceed to Initial Installs (bare-metal). If this is a reinstall of the system, then proceed to Subsequent Installs (Re-Installs). 3.1.a Subsequent installs (reinstalls) For subsequent fresh-installs (re-installs) where the system_config.yaml parameter file is available, generate the updated system configuration (see Cray Site Init Files).\nWarning: If the system_config.yaml file is unavailable, then skip this step and proceed to Initial Installs (bare-metal).\nCheck for the configuration files. The needed files should be in the preparation directory.\nlinux# ls -1 ${PITDATA}/prep Expected output looks similar to the following:\napplication_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml Generate the system configuration.\nNote: Ensure that you specify a reachable NTP pool or server using the ntp-pools or ntp-servers fields, respectively. Adding an unreachable server can cause clock skew as chrony tries to continually reach out to a server it can never reach.\nlinux# cd ${PITDATA}/prep \u0026amp;\u0026amp; csi config init A new directory matching the system-name field in system_config.yaml will now exist in the working directory.\nNote: These warnings from csi config init for issues in hmn_connections.json can be ignored.\nThe node with the external connection (ncn-m001) will have a warning similar to this because its BMC is connected to the site and not the HMN like the other management NCNs. It can be ignored.\n\u0026#34;Couldn\u0026#39;t find switch port for NCN: x3000c0s1b0\u0026#34; An unexpected component may have this message. If this component is an application node with an unusual prefix, it should be added to the application_node_config.yaml file. Then rerun csi config init. See the procedure to Create Application Node Config YAML.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1610405168.8705149,\u0026#34;msg\u0026#34;:\u0026#34;Found unknown source prefix! If this is expected to be an Application node, please update application_node_config.yaml\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u33\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3002\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u48\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;}} If a cooling door is found in hmn_connections.json, there may be a message like the following. It can be safely ignored.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1612552159.2962296,\u0026#34;msg\u0026#34;:\u0026#34;Cooling door found, but xname does not yet exist for cooling doors!\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;}} Skip the next step and continue to verify and backup system_config.yaml.\n3.1.b Initial installs (bare-metal) For first-time/initial installs (without a system_config.yamlfile), generate the system configuration. See below for an explanation of the command line parameters and some common settings.\nCheck for the configuration files. The needed files should be in the preparation directory.\nlinux# ls -1 ${PITDATA}/prep Expected output looks similar to the following:\napplication_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv Generate the system configuration.\nNotes:\nRun csi config init --help to print a full list of parameters that must be set. These will vary significantly depending on the system and site configuration. Ensure that you specify a reachable NTP pool or server using the --ntp-pools or --ntp-servers flags, respectively. Adding an unreachable server can cause clock skew as chrony tries to continually reach out to a server it can never reach. linux# cd ${PITDATA}/prep \u0026amp;\u0026amp; csi config init \u0026lt;options\u0026gt; A new directory matching the --system-name argument will now exist in the working directory.\nImportant: After generating a configuration, a visual audit of the generated files for network data should be performed.\nSpecial Notes: Certain parameters to csi config init may be hard to grasp on first-time configuration generations:\nThe optional application_node_config.yaml file is used to map prefixes in hmn_connections.csv to HSM subroles. A command line option is required in order for csi to use the file. See Create Application Node YAML. The bootstrap-ncn-bmc-user and bootstrap-ncn-bmc-pass must match what is used for the BMC account and its password for the management NCNs. Set site parameters (site-domain, site-ip, site-gw, site-nic, site-dns) for the network information which connects ncn-m001 (the PIT node) to the site. The site-nic is the interface on ncn-m001 that is connected to the site network. There are other interfaces possible, but the install-ncn-bond-members are typically: p1p1,p10p1 for HPE nodes p1p1,p1p2 for Gigabyte nodes p801p1,p801p2 for Intel nodes If not using a cabinets-yaml file, then set the three cabinet parameters (mountain-cabinets, hill-cabinets, and river-cabinets) to the quantity of each cabinet type included in this system. The starting cabinet number for each type of cabinet (for example, starting-mountain-cabinet) has a default that can be overridden. See the csi config init --help. For systems that use non-sequential cabinet ID numbers, use the cabinets-yaml argument to include the cabinets.yaml file. This file gives the ability to explicitly specify the ID of every cabinet in the system. When specifying a cabinets.yaml file with the cabinets-yaml argument, other command line arguments related to cabinets will be ignored by csi. See Create Cabinets YAML. An override to default cabinet IPv4 subnets can be made with the hmn-mtn-cidr and nmn-mtn-cidr parameters. Note: These warnings from csi config init for issues in hmn_connections.json can be ignored.\nThe node with the external connection (ncn-m001) will have a warning similar to this because its BMC is connected to the site and not the HMN like the other management NCNs. It can be ignored.\n\u0026#34;Couldn\u0026#39;t find switch port for NCN: x3000c0s1b0\u0026#34; An unexpected component may have this message. If this component is an application node with an unusual prefix, it should be added to the application_node_config.yaml file. Then rerun csi config init. See the procedure to Create Application Node Config YAML.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1610405168.8705149,\u0026#34;msg\u0026#34;:\u0026#34;Found unknown source prefix! If this is expected to be an Application node, please update application_node_config.yaml\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u33\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3002\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u48\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;}} If a cooling door is found in hmn_connections.json, there may be a message like the following. It can be safely ignored.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1612552159.2962296,\u0026#34;msg\u0026#34;:\u0026#34;Cooling door found, but xname does not yet exist for cooling doors!\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;}} Link the generated system_config.yaml file into the prep/ directory. This is needed for pit-init to find and resolve the file.\nNOTE This step is needed only for fresh installs where system_config.yaml is missing from the prep/ directory.\nlinux# cd ${PITDATA}/prep \u0026amp;\u0026amp; ln ${SYSTEM_NAME}/system_config.yaml Continue to the next step to verify and backup system_config.yaml.\n3.2 Verify and backup system_config.yaml Verify that the newly generated system_config.yaml matches the current version of CSI.\nView the new system_config.yaml file and note the CSI version reported near the end of the file.\nlinux# cat ${PITDATA}/prep/${SYSTEM_NAME}/system_config.yaml Note the version reported by the csi tool.\nNOTE The App. Version will report incorrectly in CSM 1.2.0 and CSM 1.2.1. Please obtain the version information by running the step below and by invoking rpm -q cray-site-init.\nlinux# csi version The two versions should match. If they do not, determine the cause and regenerate the file.\nCopy the new system_config.yaml file somewhere safe to facilitate re-installs.\nContinue to the next step to Prepare Site Init.\n3.3 Prepare Site Init Note:: It is assumed at this point that $PITDATA (that is, /mnt/pitdata) is still mounted on the Linux system. This is important because the following procedure depends on that mount existing.\nInstall Git if not already installed.\nPrepare the site-init directory.\nPerform the Prepare Site Init procedures.\n4. Prepopulate LiveCD daemons configuration and NCN artifacts Now that the configuration is generated, the LiveCD must be populated with the generated files.\nUse CSI to populate the LiveCD with networking files so SSH will work on the first boot.\nlinux# cd ${PITDATA}/prep \u0026amp;\u0026amp; csi pit populate cow /mnt/cow/ ${SYSTEM_NAME}/ Expected output looks similar to the following:\nconfig------------------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/config...OK ifcfg-bond0-------------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-bond0...OK ifcfg-lan0--------------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-lan0...OK ifcfg-bond0.nmn0--------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-bond0.nmn0...OK ifcfg-bond0.hmn0--------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-bond0.hmn0...OK ifcfg-bond0.can0--------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-bond0.can0...OK ifcfg-bond0.cmn0--------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-bond0.cmn0...OK ifroute-lan0------------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifroute-lan0...OK ifroute-bond0.nmn0------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifroute-bond0.nmn0...OK CAN.conf----------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/CAN.conf...OK CMN.conf----------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/CMN.conf...OK HMN.conf----------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/HMN.conf...OK NMN.conf----------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/NMN.conf...OK MTL.conf----------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/MTL.conf...OK statics.conf------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/statics.conf...OK conman.conf-------------------\u0026gt; /mnt/cow/rw/etc/conman.conf...OK Set the hostname and print it into the hostname file.\nNote: Do not confuse other administrators by naming the LiveCD ncn-m001. Append the -pit suffix, indicating that the node is booted from the LiveCD.\nlinux# echo \u0026#34;${SYSTEM_NAME}-ncn-m001-pit\u0026#34; | tee /mnt/cow/rw/etc/hostname Add some helpful variables to the PIT environment.\nBy adding these to the /etc/environment file of the PIT image, these variables will be automatically set and exported in shell sessions on the booted PIT node.\nImportant: All CSM install procedures on the booted PIT node assume that these variables are set and exported.\nThe echo prepends a newline to ensure that the variable assignment occurs on a unique line, and not at the end of another line.\nlinux# echo \u0026#34; CSM_RELEASE=${CSM_RELEASE} SYSTEM_NAME=${SYSTEM_NAME}\u0026#34; | tee -a /mnt/cow/rw/etc/environment Unmount the overlay.\nlinux# umount -v /mnt/cow Copy the NCN artifacts.\nCopy Kubernetes node artifacts:\nlinux# csi pit populate pitdata \u0026#34;${CSM_PATH}/images/kubernetes/\u0026#34; ${PITDATA}/data/k8s/ -kiK Expected output looks similar to the following:\n5.3.18-24.37-default-0.0.6.kernel-----------------\u0026gt; /mnt/pitdata/data/k8s/...OK initrd.img-0.0.6.xz-------------------------------\u0026gt; /mnt/pitdata/data/k8s/...OK kubernetes-0.0.6.squashfs-------------------------\u0026gt; /mnt/pitdata/data/k8s/...OK Copy Ceph/storage node artifacts:\nlinux# csi pit populate pitdata \u0026#34;${CSM_PATH}/images/storage-ceph/\u0026#34; ${PITDATA}/data/ceph/ -kiC Expected output looks similar to the following:\n5.3.18-24.37-default-0.0.5.kernel-----------------\u0026gt; /mnt/pitdata/data/ceph/...OK initrd.img-0.0.5.xz-------------------------------\u0026gt; /mnt/pitdata/data/ceph/...OK storage-ceph-0.0.5.squashfs-----------------------\u0026gt; /mnt/pitdata/data/ceph/...OK Quit the typescript session with the exit command and copy the typescript file to the data partition on the USB drive.\nlinux# exit linux# cp -v ${SCRIPT_FILE} /mnt/pitdata/prep/admin Unmount the data partition:\nlinux# cd ~ \u0026amp;\u0026amp; umount -v /mnt/pitdata Move the USB device to the system to be installed, if needed.\nIf the USB device was created somewhere other than ncn-m001 of the system to be installed, move it there from its current location.\nProceed to the next step to boot into the LiveCD image.\n5. Boot the LiveCD Some systems will boot the USB device automatically if no other OS exists (bare-metal). Otherwise the administrator may need to use the BIOS Boot Selection menu to choose the USB device.\nIf an administrator has the node booted with an operating system which will next be rebooting into the LiveCD, then use efibootmgr to set the boot order to be the USB device. See the set boot order page for more information about how to set the boot order to have the USB device first.\nNote: UEFI booting must be enabled in order for the system to find the USB device\u0026rsquo;s EFI bootloader.\nStart a typescript on an external system.\nThis will record this section of activities done on the console of ncn-m001 using IPMI.\nexternal# script -a boot.livecd.$(date +%Y-%m-%d).txt external# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Confirm that the IPMI credentials work for the BMC by checking the power status.\nSet the BMC variable to the hostname or IP address of the BMC of the PIT node.\nread -s is used in order to prevent the credentials from being displayed on the screen or recorded in the shell history.\nexternal# BMC=eniac-ncn-m001-mgmt external# read -s IPMI_PASSWORD external# export IPMI_PASSWORD ; ipmitool -I lanplus -U root -E -H ${BMC} chassis power status Connect to the IPMI console.\nexternal# ipmitool -I lanplus -U root -E -H ${BMC} sol activate Reboot ncn-m001.\nncn-m001# reboot Watch the shutdown and boot from the ipmitool console session.\nAn integrity check runs before Linux starts by default; it can be skipped by selecting OK in its prompt.\n5.1 First login On first log in (over SSH or at local console), the LiveCD will prompt the administrator to change the password.\nThe initial password is empty; enter the username of root and press return twice.\npit login: root Expected output looks similar to the following:\nPassword: \u0026lt;-------just press Enter here for a blank password You are required to change your password immediately (administrator enforced) Changing password for root. Current password: \u0026lt;------- press Enter here, again, for a blank password New password: \u0026lt;------- type new password Retype new password:\u0026lt;------- retype new password Welcome to the CRAY Pre-Install Toolkit (LiveOS) Note: If this password ever becomes lost or forgotten, one may reset it by mounting the USB device on another computer. See Reset root Password on LiveCD for information on clearing the password.\nDisconnect from IPMI console.\nOnce the network is up so that SSH to the node works, disconnect from the IPMI console.\nYou can disconnect from the IPMI console by entering ~.; That is, the tilde character followed by a period character.\nExit the typescript started on the external system and use scp to transfer it to the PIT node.\nSet PIT_NODE variable to the site IP address or hostname of the PIT node.\nexternal# exit external# PIT_NODE=eniac-ncn-m001 external# scp boot.livecd.*.txt root@${PIT_NODE}:/root Log in to the PIT node as root using ssh.\nexternal# ssh root@${PIT_NODE} Mount the data partition.\nThe data partition is set to fsopt=noauto to facilitate LiveCDs over virtual-ISO mount. Therefore, USB installations need to mount this manually by running the following command.\nNote: When creating the USB PIT image, this was mounted over /mnt/pitdata. Now that the USB PIT is booted, it will mount over /var/www/ephemeral. The FSLabel PITDATA is already in /etc/fstab, so the path is omitted in the following mount command.\npit# mount -vL PITDATA Set and export new environment variables.\nThe commands below save them to /etc/environment as well, which makes them available in all new shell sessions on the PIT node.\npit# export PITDATA=$(lsblk -o MOUNTPOINT -nr /dev/disk/by-label/PITDATA) pit# export CSM_PATH=${PITDATA}/${CSM_RELEASE} pit# echo \u0026#34; PITDATA=${PITDATA} CSM_PATH=${CSM_PATH}\u0026#34; | tee -a /etc/environment Start a typescript to record this section of activities done on ncn-m001 while booted from the LiveCD.\npit# script -af \u0026#34;${PITDATA}/prep/admin/booted-csm-livecd.$(date +%Y-%m-%d).txt\u0026#34; pit# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Verify that expected environment variables are set in the new login shell.\nThese were written into /etc/environment on the USB PIT image earlier in this procedure, before it was booted.\npit# echo -e \u0026#34;CSM_PATH=${CSM_PATH}\\nCSM_RELEASE=${CSM_RELEASE}\\nPITDATA=${PITDATA}\\nSYSTEM_NAME=${SYSTEM_NAME}\u0026#34; Copy the typescript made on the external system into the PITDATA mount.\npit# cp -v /root/boot.livecd.*.txt ${PITDATA}/prep/admin Check the hostname.\npit# hostnamectl Note:\nThe hostname should be similar to eniac-ncn-m001-pit when booted from the LiveCD, but it will be shown as pit# in the documentation command prompts from this point onward. If the hostname returned by the hostnamectl command is pit, then set the hostname manually with hostnamectl. In that case, do not confuse other administrators by using the hostname ncn-m001. Append the -pit suffix, indicating that the node is booted from the LiveCD. Install the latest documentation RPM.\nSee Check for Latest Documentation\nPrint information about the booted PIT image.\nThere is nothing in the output that needs to be verified. This is run in order to ensure the information is recorded in the typescript file, in case it is needed later. For example, this information is useful to include in any bug reports or service queries for issues encountered on the PIT node.\nNOTE The App. Version will report incorrectly in CSM 1.2. Please obtain the version information by running the step below and by invoking rpm -q cray-site-init.\npit# /root/bin/metalid.sh Expected output looks similar to the following:\n= PIT Identification = COPY/CUT START ======================================= VERSION=1.5.7 TIMESTAMP=20211028194247 HASH=ge4aceb1 CRAY-Site-Init build signature... Build Commit : a6c8dddf9df1a9fc7f8c4f17cb26568a8b41d433-main Build Time : 2021-12-01T16:16:41Z Go Version : go1.16.10 Git Version : a6c8dddf9df1a9fc7f8c4f17cb26568a8b41d433 Platform : linux/amd64 App. Version : 1.12.2 metal-net-scripts-0.0.2-1.noarch metal-basecamp-1.1.9-1.x86_64 metal-ipxe-2.0.10-1.noarch pit-init-1.2.12-1.noarch = PIT Identification = COPY/CUT END ========================================= 6. Configure the running LiveCD Set and export BMC credential variables.\nread -s is used in order to prevent the credentials from being displayed on the screen or recorded in the shell history.\npit# read -s IPMI_PASSWORD pit# USERNAME=root pit# export IPMI_PASSWORD USERNAME Initialize the PIT.\nThe pit-init.sh script will prepare the PIT server for deploying NCNs.\npit# /root/bin/pit-init.sh Install csm-testing and hpe-csm-goss-package.\nThe following assumes the CSM_PATH environment variable is set to the absolute path of the unpacked CSM release.\npit# rpm -Uvh --force $(find ${CSM_PATH}/rpm/ -name \u0026#34;csm-testing*.rpm\u0026#34; | sort -V | tail -1) $(find ${CSM_PATH}/rpm/ -name \u0026#34;hpe-csm-goss-package*.rpm\u0026#34; | sort -V | tail -1) Next topic After completing this procedure, proceed to configure the management network switches.\nSee Configure Management Network Switches.\n"
},
{
	"uri": "/docs-csm/en-12/background/ncn_bios/",
	"title": "NCN BIOS",
	"tags": [],
	"description": "",
	"content": "NCN BIOS This page specifies the BIOS settings that are desirable for non-compute nodes (NCNs).\nNOTES\nAny tunables on this page are in the interest of performance and stability. If either of those facets seem to be infringed by any of the content on this page, then contact HPE Cray for support. The table below declares desired settings; unlisted settings should remain at vendor default. This table may be expanded as new settings are adjusted. PCIe options can be found in PCIe : Setting Expected Values. Common Name Common Value Description Value Rationale Intel® Hyper-Threading (HT) Enabled Enables two threads per physical core Leverage the full performance of the CPU. The higher thread-count assists with parallel tasks within the processors. Intel® Virtualization Technology (VT-x or VT) and AMD Virtualization Technology (AMD-V) Enabled Enables Virtual Machine extensions Provides added CPU support for hypervisors and more for the virtualized plane within the system. PXE Retry Count 0 or 2 (see Rationale) Boot re-attempts per boot option For healthy networks, retries should not produce different results. For unhealthy or congested networks, 2 is recommended. PXE Timeout 5 Seconds or less PXE ROM maximum time for DHCP handshake to complete. DHCP handshake should not take longer than 5 seconds. This timeout could be increased for unhealthy networks, but ideally should be 2-3 seconds. Continuous Boot Disabled Whether the boot-group (e.g. all network devices, or all disk devices) should continuously retry If enabled, it prevents failed network boots to disk boots to proceed to attempting a disk boot. "
},
{
	"uri": "/docs-csm/en-12/upgrade/prepare_for_upgrade/",
	"title": "Prepare For Upgrade",
	"tags": [],
	"description": "",
	"content": "Prepare For Upgrade Before beginning an upgrade to a new version of CSM, there are a few things to do on the system first.\nReduced resiliency during upgrade [Preparation steps] Reduced resiliency during upgrade Warning: Management service resiliency is reduced during the upgrade.\nAlthough it is expected that compute nodes and application nodes will continue to provide their services without interruption, it is important to be aware that the degree of management services resiliency is reduced during the upgrade. While one node is being upgraded, if another node of the same type has an unplanned fault that removes it from service, then this may result in a degraded system. For example, if there are three Kubernetes master nodes and one is being upgraded, the quorum is maintained by the remaining two nodes. If one of those two nodes has a fault before the third node completes its upgrade, then quorum would be lost.\nPreparation steps Start a typescript.\nIf a typescript session is already running in the shell, then first stop it with the exit command.\nStart a typescript.\nncn-m001# script -af /root/csm_upgrade.$(date +%Y%m%d_%H%M%S).prepare_for_upgrade.txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; If additional shells are opened during this procedure, then record those with typescripts as well. When resuming a procedure after a break, always be sure that a typescript is running before proceeding.\nIf it is installed, use the System Diagnostic Utility (SDU) to capture current state of system before the upgrade.\nImportant: SDU takes about 15 minutes to run on a small system (longer for large systems).\nncn-m001# sdu --scenario triage --start_time \u0026#39;-4 hours\u0026#39; --reason \u0026#34;saving state before powerdown/up\u0026#34; Refer to the HPE Cray EX System Diagnostic Utility (SDU) Administration Guide for more information and troubleshooting steps.\nCheck for running sessions.\nBoot Orchestration Service (BOS), Configuration Framework Service (CFS), Compute Rolling Upgrade Service (CRUS), Firmware Action Service (FAS), and Node Memory Dump (NMD) sessions should not be started or underway during the CSM upgrade process.\nEnsure that these services do not have any sessions in progress.\nThis SAT command has shutdown as one of the command line options, but it will not start a shutdown process on the system.\nncn-m001# sat bootsys shutdown --stage session-checks Example output:\nChecking for active BOS sessions. Found no active BOS sessions. Checking for active CFS sessions. Found no active CFS sessions. Checking for active CRUS upgrades. Found no active CRUS upgrades. Checking for active FAS actions. Found no active FAS actions. Checking for active NMD dumps. Found no active NMD dumps. No active sessions exist. It is safe to proceed with the shutdown procedure. If active sessions are running, then either wait for them to complete, or shut down, cancel, or delete them.\nCoordinate with the site to prevent new sessions from starting in these services.\nThere is currently no method to prevent new sessions from being created as long as the service APIs are accessible on the API gateway.\nValidate CSM health.\nRun the CSM health checks to ensure that everything is working properly before the upgrade starts. After the upgrade is completed, another health check is performed, and it is important to know if any problems observed at that time existed prior to the upgrade.\nIMPORTANT: See the CSM Install Validation and Health Checks procedures in the documentation for the CURRENT CSM version on the system. The validation procedures in the CSM documentation are only intended to work with that specific version of CSM.\nValidate Lustre health.\nIf a Lustre file system is being used, then see the ClusterStor documentation for details on how to validate Lustre health.\nAfter completing the above steps, proceed to Upgrade Management Nodes and CSM Services.\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/stage_2/",
	"title": "Stage 2 - Kubernetes Upgrade from 1.19.9 to 1.20.13",
	"tags": [],
	"description": "",
	"content": "Stage 2 - Kubernetes Upgrade from 1.19.9 to 1.20.13 Reminder: If any problems are encountered and the procedure or command output does not provide relevant guidance, see Relevant troubleshooting links for upgrade-related issues.\nStage 2.1 Run ncn-upgrade-master-nodes.sh for ncn-m002.\nFollow output of the script carefully. The script will pause for manual interaction.\nncn-m001# /usr/share/doc/csm/upgrade/1.2/scripts/upgrade/ncn-upgrade-master-nodes.sh ncn-m002 NOTE: The root password for the node may need to be reset after it is rebooted.\nRepeat the previous step for each other master node excluding ncn-m001, one at a time.\nStage 2.2 Make sure that not all pods of ingressgateway-hmn or spire-server are running on the same worker node.\nSee where the pods are running.\nncn-m001# kubectl get pods -A -o wide | grep -E \u0026#39;ingressgateway-hmn|spire-server|^NAMESPACE\u0026#39; Example output:\nNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES istio-system istio-ingressgateway-hmn-555dbc8c6b-2b6rv 1/1 Running 0 5h2m 10.42.1.41 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; istio-system istio-ingressgateway-hmn-555dbc8c6b-ks75r 1/1 Running 0 5h3m 10.44.1.16 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; istio-system istio-ingressgateway-hmn-555dbc8c6b-npmhz 1/1 Running 0 5h2m 10.47.0.185 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; spire spire-server-0 2/2 Running 0 22d 10.47.0.190 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; spire spire-server-1 2/2 Running 0 22d 10.42.1.133 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; spire spire-server-2 2/2 Running 0 22d 10.44.0.184 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; In the example output, for each deployment, the pods are spread out across different worker nodes, so no action would be required.\nFor either of those two deployments, if all pods are running on a single worker node, then move at least one pod to a different worker node.\nUse the /opt/cray/platform-utils/move_pod.sh script to do this.\nncn-m001# /opt/cray/platform-utils/move_pod.sh \u0026lt;pod_name\u0026gt; \u0026lt;target_node\u0026gt; Run ncn-upgrade-worker-nodes.sh for ncn-w001.\nFollow output of the script carefully. The script will pause for manual interaction.\nncn-m001# /usr/share/doc/csm/upgrade/1.2/scripts/upgrade/ncn-upgrade-worker-nodes.sh ncn-w001 NOTE: The root password for the node may need to be reset after it is rebooted.\nAssign a new CFS configuration to the worker node.\nThe content of the new CFS configuration is described in HPE Cray EX System Software Getting Started Guide S-8000, section \u0026ldquo;HPE Cray EX Software Upgrade Workflow\u0026rdquo; subsection \u0026ldquo;Cray System Management (CSM)\u0026rdquo;. Replace ${NEW_NCN_CONFIGURATION} with the name of the new CFS configuration and ${XNAME} with the component name (xname) of the worker node that was upgraded.\nncn-m001# cray cfs components update --desired-config ${NEW_NCN_CONFIGURATION} ${XNAME} Repeat the previous steps for each other worker node, one at a time.\nStage 2.3 By this point, all NCNs have been upgraded, except for ncn-m001. In the upgrade process so far, ncn-m001 has been the \u0026ldquo;stable node\u0026rdquo; \u0026ndash; that is, the node from which the other nodes were upgraded. At this point, the upgrade procedure pivots to use ncn-m002 as the new \u0026ldquo;stable node\u0026rdquo;, in order to allow the upgrade of ncn-m001.\nIf the CSM tarball is located on an rbd device, then remap that device to ncn-m002.\nSee Move an rbd device to another node.\nLog in to ncn-m002 from outside the cluster.\nNOTE: Very rarely, a password hash for the root user that works properly on a SLES SP2 NCN is not recognized on a SLES SP3 NCN. If password login fails, then log in to ncn-m002 from ncn-m001 and use the passwd command to reset the password. Then log in using the CMN IP address as directed below. Once ncn-m001 has been upgraded, log in from ncn-m002 and use the passwd command to reset the password. The other NCNs will have their passwords updated when NCN personalization is run in a subsequent step.\nssh to the bond0.cmn0/CMN IP address of ncn-m002.\nAuthenticate with the Cray CLI on ncn-m002.\nSee Configure the Cray Command Line Interface for details on how to do this.\nSet the CSM_RELEASE variable to the target CSM version of this upgrade.\nThe following command is just an example. Be sure to set the appropriate CSM_RELEASE version for the upgrade being performed.\nncn-m002# CSM_RELEASE=csm-1.2.2 Copy artifacts from ncn-m001.\nA later stage of the upgrade expects the docs-csm RPM to be located at /root/docs-csm-latest.noarch.rpm on ncn-m002; that is why this command copies it there.\nncn-m002# mkdir -pv /etc/cray/upgrade/csm/${CSM_RELEASE} \u0026amp;\u0026amp; scp ncn-m001:/etc/cray/upgrade/csm/myenv /etc/cray/upgrade/csm/myenv \u0026amp;\u0026amp; scp ncn-m001:/root/output.log /root/pre-m001-reboot-upgrade.log \u0026amp;\u0026amp; cray artifacts create config-data pre-m001-reboot-upgrade.log /root/pre-m001-reboot-upgrade.log ncn-m002# csi_rpm=$(ssh ncn-m001 \u0026#34;find /etc/cray/upgrade/csm/${CSM_RELEASE}/tarball/${CSM_RELEASE}/rpm/cray/csm/ -name \u0026#39;cray-site-init*.rpm\u0026#39;\u0026#34;) \u0026amp;\u0026amp; scp ncn-m001:${csi_rpm} /tmp/cray-site-init.rpm \u0026amp;\u0026amp; scp ncn-m001:/root/docs-csm-*.noarch.rpm /root/docs-csm-latest.noarch.rpm \u0026amp;\u0026amp; rpm -Uvh --force /tmp/cray-site-init.rpm /root/docs-csm-latest.noarch.rpm Upgrade ncn-m001.\nncn-m002# /usr/share/doc/csm/upgrade/1.2/scripts/upgrade/ncn-upgrade-master-nodes.sh ncn-m001 Stage 2.4 Apply the workaround for kdump:\nncn-m002# /usr/share/doc/csm/scripts/workarounds/kdump/run.sh Example output:\nUploading hotfix files to ncn-m001:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-m002:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-m003:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s001:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s002:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s003:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s004:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w001:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w002:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w003:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w004:/srv/cray/scripts/common/ ... Done Running updated create-kdump-artifacts.sh script on [11] NCNs ... Done The following NCNs contain the kdump patch: ncn-m001 ncn-m002 ncn-m003 ncn-s001 ncn-s002 ncn-s003 ncn-s004 ncn-w001 ncn-w002 ncn-w003 ncn-w004 This hotfix has completed. Stage 2.5 Run the following command to complete the upgrade of the weave and multus manifest versions:\nncn-m002# /srv/cray/scripts/common/apply-networking-manifests.sh Stage 2.6 Run the following script to apply anti-affinity to coredns pods:\nncn-m002# /usr/share/doc/csm/upgrade/1.2/scripts/k8s/apply-coredns-pod-affinity.sh Stage 2.7 Complete the Kubernetes upgrade. This script will restart several pods on each master node to their new Docker containers.\nncn-m002# /usr/share/doc/csm/upgrade/1.2/scripts/k8s/upgrade_control_plane.sh NOTE: kubelet has been upgraded already, ignore the warning to upgrade it.\nStage completed All Kubernetes nodes have been rebooted into the new image.\nREMINDER: If password for ncn-m002 was reset during Stage 2.3, then also reset the password on ncn-m001 at this time.\nThis stage is completed. Continue to Stage 3.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/kubernetes/troubleshoot_kubernetes_pods_not_starting/",
	"title": "Troubleshoot Kubernetes Pods Not Starting",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Kubernetes Pods Not Starting Use this procedure to check if Kubernetes pods get scheduled on an NCN, but do not eventually reach the Running state.\nPrerequisites The kubectl get pod command returns pods that seem to be stuck in the Init or ContainerCreating state.\nIdentify the node in question Run the kubectl get pod -o wide command to identify the node where the pod is not starting.\nncn-w001# kubectl get pod -A -o wide | egrep \u0026#39;Init|ContainerCreating\u0026#39; services cray-sls-58cfdb7c46-b7dbj 0/2 Init:0/2 0 2d22h 10.39.0.165 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services gitea-vcs-65c98746b-jk5v7 0/2 ContainerCreating 0 2d3h 10.47.0.104 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; In the above example, ncn-w002 is the node that may need attention.\nRecovery Steps Execute the following steps on the node that was determined in the previous step.\nRestart the kubelet service.\nncn-w002# systemctl restart kubelet Ensure that kubelet is running.\nncn-w002# systemctl status kubelet Restart the containerd service.\nncn-w002# systemctl restart containerd Ensure that containerd is running.\nncn-w002# systemctl status containerd Try running the kubectl get pod command again; within a few minutes, the pods should transition to the Running state.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/",
	"title": "CSM Troubleshooting Information",
	"tags": [],
	"description": "",
	"content": "CSM Troubleshooting Information This document provides links to troubleshooting information for services and functionality provided by CSM.\nHelpful tips for navigating the CSM repository Known issues Kubernetes Grafana dashboards User Access Service (UAS) Booting UAN boot issues Compute node boot issues Compute rolling upgrades Configuration management Security and authentication ConMan Utility storage Node management Customer Management Network (CMN) Domain Name Service (DNS) MetalLB Spire Etcd Helpful tips for navigating the CSM repository In the main repository landing page, change the branch to the CSM version being used on the system (for example, release/1.0, release/1.2, release/1.3, etc.).\nUse the pre-populated GitHub \u0026ldquo;Search or jump to\u0026hellip;\u0026rdquo; function in the upper left hand side of the page and append keywords related to the exiting problem seen into the existing search. (The example searches for \u0026ldquo;ping\u0026rdquo; and \u0026ldquo;PXE\u0026rdquo; related troubleshooting resources on the \u0026ldquo;main\u0026rdquo; branch.)\nFollow any run-books, guides, or procedures which are directly related to the problem.\nChange the branch to main and search a second time to retrieve very recent or beta run-books and guides.\nUsers can also expand the search beyond the \u0026ldquo;troubleshooting\u0026rdquo; section (instead of doing \u0026ldquo;path troubleshooting\u0026rdquo;) and/or use more advanced GitHub searches such as \u0026ldquo;path configure\u0026rdquo; to find the right context.\nKnown issues SAT/HSM/CAPMC Component Power State Mismatch HMS Discovery job not creating RedfishEndpoints in Hardware State Manager initrd.img.xz not found Platform CA Issues Kafka Failure after CSM 1.2 Upgrade SLS Not Working During Node Rebuild Multiple Console Node Pods on the Same Worker HPE nodes not properly transitioning power state Nexus Fails Authentication with Keycloak Users Gigabyte BMC Missing Redfish Data Hang Listing BOS Sessions Kubernetes General Kubernetes Commands for Troubleshooting Kubernetes Log File Locations Liveliness or Readiness Probe Failures Unresponsive kubectl Commands Kubernetes Node NotReady Kubernetes Pods not Starting Postgres Database Recover from Postgres WAL Event Restore Postgres Disaster Recovery for Postgres Grafana dashboards Grafana Dashboards User Access Service (UAS) Viewing UAI Log Output Stale Brokered UAIs UAI Stuck in ContainerCreating Duplicate Mount Paths in a UAI Missing or Incorrect UAI Images Common Mistakes When Creating a Custom End-User UAI Image Booting UAN boot issues UAN Boot Issues Compute node boot issues Issues Related to Unified Extensible Firmware Interface (UEFI) Issues Related to Dynamic Host Configuration Protocol (DHCP) Issues Related to the Boot Script Service Issues Related to Trivial File Transfer Protocol (TFTP) Troubleshooting Using Kubernetes Log File Locations and Ports Used Issues Related to Slow Boot Times Compute rolling upgrades CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality. See Deprecated features for more information.\nNodes Failing to Upgrade in a CRUS Session Failed CRUS Session Because of Unmet Conditions Failed CRUS Session Because of Bad Parameters Configuration management Ansible Play Failures in CFS Sessions CFS Session Failing to Complete Security and authentication Common Vault Cluster Issues Keycloak User Localization ConMan ConMan Blocking Access to a Node BMC ConMan Failing to Connect to a Console ConMan Asking for Password on SSH Connection Utility storage Failure to Get Ceph Health Down OSDs Ceph OSDs Reporting Full System Clock Skew Unresponsive S3 Endpoint Ceph-Mon Processes Stopping and Exceeding Max Restarts Large Object Map Objects in Ceph Health Failure of RGW Health Check Troubleshoot S3FS Mounts Node management Issues with Redfish Endpoint DiscoveryCheck for Redfish Events from Nodes Interfaces with IP Address Issues Loss of Console Connections and Logs on Gigabyte Nodes Customer Management Network (CMN) CMN Issues Domain Name Service (DNS) Connectivity to Services with External IP addresses DNS Configuration Issues MetalLB Services Without an Allocated IP Address BGP not Accepting Routes from MetalLB Spire Restore Spire Postgres without a Backup Spire Database Cluster DNS Lookup Failure Spire Failing to Start on NCNs Etcd Etcd Cluster Backup Timeout "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/nexus_fail_authentication_with_keycloak_users/",
	"title": "Nexus Fails Authentication with Keycloak Users",
	"tags": [],
	"description": "",
	"content": "Nexus Fails Authentication with Keycloak Users There is a known issue where the Nexus chart gets created and setup before keycloak-setup has completed running. This causes an issue while attempting to log in to Nexus with a Keycloak user. This can also cause a Nexus test to fail during CSM health validation.\nFix To recover from this situation, perform the following procedure.\nGet the correct client secret for the Nexus Keycloak client.\nncn-mw# correct_secret=$(kubectl get secret -n nexus system-nexus-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d) Get the already setup Keycloak integration configuration secret from Nexus.\nncn-mw# old_config=$(kubectl get secret -n nexus nexus-keycloak-realm-config -o jsonpath=\u0026#39;{.data.keycloak\\.json}\u0026#39; | base64 -d) Update the Keycloak integration configuration secret.\nncn-mw# new_config=$(echo $old_config | jq -c --arg secret $correct_secret \u0026#39;.credentials.secret = $secret\u0026#39;) Update the Keycloak integration secret in Kubernetes.\nncn-mw# kubectl patch secret -n nexus nexus-keycloak-realm-config --patch=\u0026#34;{\\\u0026#34;data\\\u0026#34;: { \\\u0026#34;keycloak.json\\\u0026#34;: \\\u0026#34;$(echo $new_config | base64 -w0)\\\u0026#34; }}\u0026#34; Restart Nexus to update its configuration.\nncn-mw# kubectl rollout restart -n nexus deployment nexus "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/alternate_storage_pools/",
	"title": "Alternate Storage Pools",
	"tags": [],
	"description": "",
	"content": "Alternate Storage Pools Description Use cases Best practices Procedures Create a storage pool Create and map an rbd device Move an rbd device to another node Unmount, unmap, and delete an rbd device Remove a storage pool Description Creating, maintaining, and removing Ceph storage pools.\nUse cases A landing space for the CSM tarball used for upgrades. Temporary space needed for maintenance or pre/post upgrade activities. Best practices Apply a proper quota to any pools created. This will use storage from the default crush rule which is utilizing every OSD. Improper use of this procedure can have a negative impact on the cluster and space available to running services. Failure to set a quota and not policing space usage can result in the Ceph cluster going into read-only mode. This will cause running services to crash if the space issue is not resolved quickly. Cleanup after the criteria for the pool creation has been met. This can be as simple as removing volumes but leaving the pool for future use. Procedures This example shows the creation and mounting of an rbd device on ncn-m001.\nNOTE: The commands to create and delete pools or rbd devices must be run from a master node or one of the first three storage nodes (ncn-s001, ncn-s002, or ncn-s003).\nCreate a storage pool The below example will create a storage pool name csm-release. The pool name can be changed to better reflect any use cases outside of support for upgrades. The 3 3 arguments can be left unchanged. For more information on their meaning and possible alternative values, see the Ceph product documentation.\nncn-ms# ceph osd pool create csm-release 3 3 ncn-ms# ceph osd pool application enable csm-release rbd ncn-ms# ceph osd pool set-quota csm-release max_bytes 500G ncn-ms# ceph osd pool get-quota csm-release Output:\nncn-s001:~ # ceph osd pool create csm-release 3 3 pool \u0026#39;csm-release\u0026#39; created ncn-s001:~ # ceph osd pool application enable csm-release rbd enabled application \u0026#39;rbd\u0026#39; on pool \u0026#39;csm-release\u0026#39; ncn-s001:~ # ceph osd pool set-quota csm-release max_bytes 500G set-quota max_bytes = 536870912000 for pool csm-release ncn-s001:~ # ceph osd pool get-quota csm-release quotas for pool \u0026#39;csm-release\u0026#39;: max objects: N/A max bytes : 500 GiB (current num bytes: 0 bytes) NOTES:\nThe above example sets the quota to 500 GiB. If this pool is fully utilized it will be using 1.5 TiB of raw space. This space counts against the total space provided by the cluster; use cautiously. If this pool or any pool reaches 95-100% utilization, then all volumes for the fully utilized pool will go into read-only mode. Create and map an rbd device IMPORTANT:\nCreating an rbd device requires proper access and must be run from a master node or one of the first three storage nodes (ncn-s001, ncn-s002, or ncn-s003). Mounting a device will occur on the node where the storage needs to be present. ncn-ms# rbd create -p csm-release release_version --size 100G ncn-ms# rbd map -p csm-release release_version ncn-ms# rbd showmapped Output:\nncn-m001:~ # rbd create -p csm-release release_version --size 100G ncn-m001:~ # rbd map -p csm-release release_version /dev/rbd0 ncn-m001:~ # rbd showmapped id pool namespace image snap device 0 csm-release release_version - /dev/rbd0 IMPORTANT NOTE:\nMaster nodes normally do not have rbd devices mapped via Ceph provisioner. If mapping to a worker node where there are mapped PVCs, then ensure the proper rbd device is being captured for the following steps. Failure to do this most likely will result in data corruption or loss. Mount an rbd device ncn-ms# mkfs.ext4 /dev/rbd0 ncn-ms# mkdir -pv /etc/cray/csm/csm-release ncn-ms# mount /dev/rbd0 /etc/cray/csm/csm-release/ ncn-ms# mountpoint /etc/cray/csm/csm-release/ Output:\nncn-m001:~ # mkfs.ext4 /dev/rbd0 mke2fs 1.43.8 (1-Jan-2018) Discarding device blocks: done Creating filesystem with 26214400 4k blocks and 6553600 inodes Filesystem UUID: d5fe6df4-a0ab-49bc-8d49-9cc62700915d Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872 Allocating group tables: done Writing inode tables: done Creating journal (131072 blocks): done Writing superblocks and filesystem accounting information: mkdir done ncn-m001:~ # mkdir -pv /etc/cray/csm/csm-release mkdir: created directory \u0026#39;/etc/cray/csm\u0026#39; mkdir: created directory \u0026#39;/etc/cray/csm/csm-release\u0026#39; Note that the below command does not return output on success:\nncn-m001:~ # mount /dev/rbd0 /etc/cray/csm/csm-release/ ncn-m001:~ # mountpoint /etc/cray/csm/csm-release/ /etc/cray/csm/csm-release/ is a mountpoint Move an rbd device to another node On node where the rbd device is mapped:\nncn-ms# umount /etc/cray/csm/csm-release ncn-ms# rbd unmap -p csm-release release_version ncn-ms# rbd showmapped NOTE: There should be no output from the above unless other rbd devices are mapped on the node. In this case, it is a master node, which typically will not have mapped rbd devices.\nThen run the following commands on the destination node (that is, the node where the rbd device is being remapped to).\nncn-ms# rbd map -p csm-release release_version ncn-ms# rbd showmapped ncn-ms# mkdir -pv /etc/cray/csm/csm-release ncn-ms# mount /dev/rbd0 /etc/cray/csm/csm-release Output:\nncn-m002:~ # rbd map -p csm-release release_version /dev/rbd0 The following output will vary based on the existence of the directories.\nncn-m002:~ # mkdir -pv /etc/cray/csm/csm-release mkdir: created directory \u0026#39;/etc/cray\u0026#39; mkdir: created directory \u0026#39;/etc/cray/csm\u0026#39; mkdir: created directory \u0026#39;/etc/cray/csm/csm-release\u0026#39; ncn-m002:~ # rbd showmapped id pool namespace image snap device 0 csm-release release_version - /dev/rbd0 ncn-m002:~ # mount /dev/rbd0 /etc/cray/csm/csm-release/ Unmount, unmap, and delete an rbd device ncn-ms# umount /etc/cray/csm/csm-release ncn-ms# rbd unmap /dev/rbd0 ncn-ms# rbd showmapped ncn-ms# rbd remove csm-release/release_version Output:\nncn-m001:~ # umount /etc/cray/csm/csm-release ncn-m001:~ # rbd unmap /dev/rbd0 ncn-m001:~ # rbd showmapped ncn-m001:~ # rbd remove csm-release/release_version Removing image: 100% complete...done. Remove a storage pool CRITICAL NOTE: This will permanently delete data.\nCheck to see if the cluster is allowing pool deletion.\nncn-ms# ceph config get mon mon_allow_pool_delete Output:\nncn-s001:~ # ceph config get mon mon_allow_pool_delete true If the above command shows false, then enable it using the following command:\nncn-ms# ceph config set mon mon_allow_pool_delete true Remove the pool.\nncn-ms# ceph osd pool rm csm-release csm-release --yes-i-really-really-mean-it Output:\nncn-s001:~ # ceph osd pool rm csm-release csm-release --yes-i-really-really-mean-it pool \u0026#39;csm-release\u0026#39; removed "
},
{
	"uri": "/docs-csm/en-12/operations/system_layout_service/create_a_backup_of_the_sls_postgres_database/",
	"title": "Create a Backup of the SLS Postgres Database",
	"tags": [],
	"description": "",
	"content": "Create a Backup of the SLS Postgres Database Perform a manual backup of the contents of the SLS Postgres database. This backup can be used to restore the contents of the SLS Postgres database at a later point in time using the Restoring SLS Postgres cluster from backup procedure.\nPrerequisites Healthy SLS Postgres Cluster. Use patronictl list on the SLS Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:\nncn# kubectl exec cray-sls-postgres-0 -n services -c postgres -it -- patronictl list + Cluster: cray-sls-postgres (6975238790569058381) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +---------------------+------------+--------+---------+----+-----------+ | cray-sls-postgres-0 | 10.44.0.40 | Leader | running | 1 | | | cray-sls-postgres-1 | 10.36.0.37 | | running | 1 | 0 | | cray-sls-postgres-2 | 10.42.0.42 | | running | 1 | 0 | +---------------------+------------+--------+---------+----+-----------+ Healthy SLS Service. Verify all 3 SLS replicas are up and running:\nncn# kubectl -n services get pods -l cluster-name=cray-sls-postgres NAME READY STATUS RESTARTS AGE cray-sls-postgres-0 3/3 Running 0 18d cray-sls-postgres-1 3/3 Running 0 18d cray-sls-postgres-2 3/3 Running 0 18d Procedure Create a directory to store the SLS backup files in:\nncn# BACKUP_LOCATION=\u0026#34;/root\u0026#34; ncn# export BACKUP_NAME=\u0026#34;cray-sls-postgres-backup_`date \u0026#39;+%Y-%m-%d_%H-%M-%S\u0026#39;`\u0026#34; ncn# export BACKUP_FOLDER=\u0026#34;${BACKUP_LOCATION}/${BACKUP_NAME}\u0026#34; ncn# mkdir -p \u0026#34;$BACKUP_FOLDER\u0026#34; The SLS backup will be located at the following directory:\nncn# echo $BACKUP_FOLDER /root/cray-sls-postgres-backup_2021-07-07_16-39-44 Run the backup_sls_postgres.sh script to take a backup of the SLS Postgres:\nncn# /usr/share/doc/csm/scripts/operations/system_layout_service/backup_sls_postgres.sh Example output:\n~/cray-sls-postgres-backup_2021-07-07_16-39-44 ~ SLS postgres backup file will land in /root/cray-sls-postgres-backup_2021-07-07_16-39-44 Determining the postgres leader... The SLS postgres leader is cray-sls-postgres-0 Using pg_dumpall to dump the contents of the SLS database... PSQL dump is available at /root/cray-sls-postgres-backup_2021-07-07_16-39-44/cray-sls-postgres-backup_2021-07-07_16-39-44.psql Saving Kubernetes secret service-account.cray-sls-postgres.credentials Saving Kubernetes secret slsuser.cray-sls-postgres.credentials Saving Kubernetes secret postgres.cray-sls-postgres.credentials Saving Kubernetes secret standby.cray-sls-postgres.credentials Removing extra fields from service-account.cray-sls-postgres.credentials.yaml Removing extra fields from slsuser.cray-sls-postgres.credentials.yaml Removing extra fields from postgres.cray-sls-postgres.credentials.yaml Removing extra fields from standby.cray-sls-postgres.credentials.yaml Adding Kubernetes secret service-account.cray-sls-postgres.credentials to secret manifest Adding Kubernetes secret slsuser.cray-sls-postgres.credentials to secret manifest Adding Kubernetes secret postgres.cray-sls-postgres.credentials to secret manifest Adding Kubernetes secret standby.cray-sls-postgres.credentials to secret manifest Secret manifest is located at /root/cray-sls-postgres-backup_2021-07-07_16-39-44/cray-sls-postgres-backup_2021-07-07_16-39-44.manifest Performing SLS dumpstate... SLS dumpstate is available at /root/cray-sls-postgres-backup_2021-07-07_16-39-44/sls_dump.json SLS Postgres backup is available at: /root/cray-sls-postgres-backup_2021-07-07_16-39-44 Copy the backup folder off of the cluster, and store it in a secure location.\nThe BACKUP_FOLDER environment variable is the name of the folder to backup.\nncn# echo $BACKUP_FOLDER /root/cray-sls-postgres-backup_2021-07-07_16-39-44 is the returned value in this example.\nOptionally, create a tarball of the Postgres backup files:\nncn# cd $BACKUP_FOLDER \u0026amp;\u0026amp; cd .. ncn# tar -czvf $BACKUP_NAME.tar.gz $BACKUP_NAME "
},
{
	"uri": "/docs-csm/en-12/operations/system_management_health/grafterm/",
	"title": "Grafterm",
	"tags": [],
	"description": "",
	"content": "Grafterm Visualize metrics dashboards on the terminal, like a simplified and minimalist version of Grafana for terminal.\nThe utility(script) can be found in the /opt/cray/platform-utils directory in all NCNs.\nRunning options Exit with q or Esc.\nHelp ncn-m001# ./grafterm.sh --help List available terminal dashboards ncn-m001# ./grafterm.sh --list Default usage To view the dashboard, pass the value(dashboard json file) to the -c parameter to the script. The Grafterm will query for all data accessible in the datasource by default, and the dashboard refresh frequency is set to 10 seconds.\nncn-m001# ./grafterm.sh -c critical_services_dashboard.json Relative time ncn-m001# ./grafterm.sh -c critical_services_dashboard.json -d 3h Refresh interval ncn-m001# ./grafterm.sh -c critical_services_dashboard.json -r 10s ncn-m001# ./grafterm.sh -c critical_services_dashboard.json -d 3h -r 10s Fixed time Set a fixed time range to visualize the metrics using duration notation. In the following example, the start time is now-23h and end time is now-18h.\nncn-m001# ./grafterm.sh -c critical_services_dashboard.json -s 23h -e 18h Set a fixed time range to visualize the metrics using timestamp [ISO 8601] notation.\nncn-m001# ./grafterm.sh -c critical_services_dashboard.json -s 2021-10-30T11:25:10+05:00 -e 2021-10-30T11:55:10+05:00 "
},
{
	"uri": "/docs-csm/en-12/operations/system_configuration_service/system_configuration_service/",
	"title": "System Configuration Service",
	"tags": [],
	"description": "",
	"content": "System Configuration Service The System Configuration Service (SCSD) allows administrators to set various BMC and controller parameters. These parameters are typically set during discovery, but this tool enables parameters to be set before or after discovery. The operations to change these parameters are available in the Cray CLI under the scsd command.\nThe following are the parameters that most commonly must be set:\nSSH keys IMPORTANT: If the scsd tool is used to update the SSHConsoleKey value outside of ConMan, it will disrupt the ConMan connection to the console and collection of console logs. Refer to ConMan for more information. NTP server syslog server BMC/controller passwords The scsd tool includes a REST API to facilitate operations to set parameters. It contacts the Hardware State Manager (HSM) to verify that targets are correct and in a valid hardware state, unless the force flag is specified. Once it has a list of targets, scsd performs the needed Redfish operations in parallel using TRS. Any credentials needed are retrieved from Vault.\nIn all POST operation payloads, there is an optional force parameter. If this parameter is present and set to true, then HSM will not be contacted and the Redfish operations will be attempted without verifying that they are present or in a good state. If the force option is not present or it is set to false, then HSM will be used.\nThe specified targets can be BMCs, controller component names (xnames), or HSM group IDs. If BMCs and controllers are grouped in HSM, this service becomes much easier to use because single targets can be used rather than long lists.\nTo view the current build version of the scsd service:\nncn-mw# cray scsd version list "
},
{
	"uri": "/docs-csm/en-12/operations/spire/troubleshoot_spire_failing_to_start_on_ncns/",
	"title": "Troubleshoot Spire Failing to Start on NCNs",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Spire Failing to Start on NCNs The spire-agent service may fail to start on Kubernetes non-compute nodes (NCNs). A key indication of this failure is when logging errors occur with the journalctl command. The following are logging errors that will indicate if the spire-agent is failing to start:\nThe join token does not exist or has already been used message is returned The last lines of the logs contain multiple lines of systemd[1]: spire-agent.service: Start request repeated too quickly. Deleting the request-ncn-join-token daemonset pod running on the node may clear the issue.\nWhile the spire-agent systemctl service on the Kubernetes node should eventually restart cleanly, administrators may need to log in to the impacted nodes and restart the service. The easiest way to delete the appropriate pod is to create the following function and run it on the impacted node.\nfunction renewncnjoin() { if [ -z \u0026#34;$1\u0026#34; ]; then echo \u0026#34;usage: renewncnjoin NODE_HOSTNAME\u0026#34; else for pod in $(kubectl get pods -n spire | grep request-ncn-join-token | awk \u0026#39;{print $1}\u0026#39;); do if kubectl describe -n spire pods $pod | grep -q \u0026#34;Node:.*$1\u0026#34;; then echo \u0026#34;Restarting $pod running on $1\u0026#34;; kubectl delete -n spire pod \u0026#34;$pod\u0026#34;; fi done fi } Run the renewncnjoin function on the NCN where kubectl is running:\nrenewncnjoin NODE_HOSTNAME The spire-agent service may also fail if an NCN was powered off for too long and its tokens expired. If this happens, delete /root/spire/agent_svid.der, /root/spire/bundle.der, and /root/spire/data/svid.key off the NCN before deleting the request-ncn-join-token daemonset pod.\nrm /root/spire/agent_svid.der rm /root/spire/bundle.der rm /root/spire/data/svid.key "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/add_root_service_account_for_gigabyte_controllers/",
	"title": "Add Root Service Account for Gigabyte Controllers",
	"tags": [],
	"description": "",
	"content": "Add Root Service Account for Gigabyte Controllers By default, Gigabyte BMC and CMC controllers have the admin service account configured. In order to discover this type of hardware, the root service account needs to be configured.\nPrerequisites The root service account is not already configured for the controller.\nCheck which service accounts are currently configured.\nncn# curl -s -k -u \u0026lt;user\u0026gt;:\u0026lt;password\u0026gt; https://\u0026lt;xname\u0026gt;/redfish/v1/AccountService/Accounts | jq \u0026#34;.Members\u0026#34; [ { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/1\u0026#34; } ] Verify that none of the accounts listed are for root.\nncn# curl -s -k -u \u0026lt;user\u0026gt;:\u0026lt;password\u0026gt; https://\u0026lt;xname\u0026gt;/redfish/v1/AccountService/Accounts/1 | jq \u0026#39;. | { Name: .Name, UserName: .UserName, RoleId: .RoleId }\u0026#39; { \u0026#34;Name\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;UserName\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;RoleId\u0026#34;: \u0026#34;Administrator\u0026#34; } The root account is not configured in the example shown above. If root is already configured, do not proceed with the following steps.\nProcedure Run the following commands to configure the root service account for the controller:\nncn# ipmitool -U \u0026lt;user\u0026gt; -P \u0026lt;password\u0026gt; -I lanplus -H \u0026lt;ip\u0026gt; user set name 4 root ncn# ipmitool -U \u0026lt;user\u0026gt; -P \u0026lt;password\u0026gt; -I lanplus -H \u0026lt;ip\u0026gt; user set password 4 \u0026lt;root_password\u0026gt; Set User Password command successful (user 4) ncn# ipmitool -U \u0026lt;user\u0026gt; -P \u0026lt;password\u0026gt; -I lanplus -H \u0026lt;ip\u0026gt; user priv 4 4 1 Set Privilege Level command successful (user 4) ncn# ipmitool -U \u0026lt;user\u0026gt; -P \u0026lt;password\u0026gt; -I lanplus -H \u0026lt;ip\u0026gt; user enable 4 ncn# ipmitool -U \u0026lt;user\u0026gt; -P \u0026lt;password\u0026gt; -I lanplus -H \u0026lt;ip\u0026gt; channel setaccess 1 4 callin=on ipmi=on link=on Set User Access (channel 1 id 4) successful. Additionally, if the target controller is a BMC and not a CMC, run the following command:\nncn# ipmitool -U \u0026lt;user\u0026gt; -P \u0026lt;password\u0026gt; -I lanplus -H \u0026lt;ip\u0026gt; sol payload enable 1 4 Verify that the root service account is now configured.\nncn# curl -s -k -u \u0026lt;user\u0026gt;:\u0026lt;password\u0026gt; https://\u0026lt;xname\u0026gt;/redfish/v1/AccountService/Accounts | jq \u0026#34;.Members\u0026#34; [ { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/4\u0026#34; }, { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/1\u0026#34; } ] ncn# curl -s -k -u \u0026lt;user\u0026gt;:\u0026lt;password\u0026gt; https://\u0026lt;xname\u0026gt;/redfish/v1/AccountService/Accounts/4 | jq \u0026#39;. | { Name: .Name, UserName: .UserName, RoleId: .RoleId }\u0026#39; { \u0026#34;Name\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;UserName\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;RoleId\u0026#34;: \u0026#34;Administrator\u0026#34; } Now the root service account is configured.\n"
},
{
	"uri": "/docs-csm/en-12/operations/resiliency/resiliency/",
	"title": "Resiliency",
	"tags": [],
	"description": "",
	"content": "Resiliency HPE Cray EX systems are designed so that system management services (SMS) are fully resilient and that there is no single point of failure.\n"
},
{
	"uri": "/docs-csm/en-12/operations/power_management/power_off_compute_and_io_cabinets/",
	"title": "Power Off Compute and IO Cabinets",
	"tags": [],
	"description": "",
	"content": "Power Off Compute and IO Cabinets Power off HPE Cray EX liquid-cooled and standard racks.\nCabinet/rack types Liquid-cooled cabinets HPE Cray EX liquid-cooled cabinet CDU and PDU circuit breakers are controlled manually.\nWhen the PDU breakers are switched to OFF, the Chassis Management Modules (CMMs) and Cabinet Environmental Controllers (CECs) are also powered off.\nWarning: The cabinet 480VAC power bus bars remain energized. Facility power must be disconnected to completely remove power from the cabinet. Follow lockout-tagout procedures for the site before maintenance.\nStandard racks HPE Cray standard EIA racks typically include two redundant PDUs. Some PDU models may require a flat-blade screwdriver to open or close the PDU circuit breakers.\nWarning: The cabinet PDUs remain energized when circuit breakers are OFF. Facility power must be disconnected or the PDUs must be unplugged to completely remove power from the rack. Follow lockout-tagout procedures for the site before maintenance.\nPrerequisites An authentication token is required to access the API gateway and to use the sat command. See the \u0026ldquo;SAT Authentication\u0026rdquo; section of the HPE Cray EX System Admin Toolkit (SAT) product stream documentation (S-8031) for instructions on how to acquire a SAT authentication token. This procedure assumes all system software and user jobs were shut down. See Shut Down and Power Off Compute and User Access Nodes (UAN). Procedure If the system does not include HPE Cray EX liquid-cooled cabinets, then skip the next section and proceed to Power off standard rack PDU circuit breakers.\nPower off HPE Cray EX liquid-cooled cabinets Check CDU control panel for alerts or warnings and resolve any issues before continuing.\nCheck the power status before shutdown.\nThis example shows cabinets 1000 - 1003.\nncn-m# cray capmc get_xname_status create --xnames x[1000-1003]c[0-7] --format json Shut down services and power off liquid-cooled cabinets.\nncn-m# sat bootsys shutdown --stage cabinet-power This command suspends the hms-discovery cron job and recursively powers off the liquid-cooled cabinet chassis.\nThe sat bootsys shutdown command may fail to power off some cabinets and indicate that requests to CAPMC have timed out. In this case, the sat command may be run with an increased --api-timeout option:\nncn-m# sat --api-timeout 180 bootsys shutdown --stage cabinet-power Verify that the hms-discovery cron job has been suspended.\nIf that is the case, then the SUSPEND column should be True in the output of the following command:\nncn-m# kubectl get cronjobs -n services hms-discovery Example output:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE^M hms-discovery */3 * * * * True 0 117s 15d Check the power off status.\nThis example shows cabinets 1000 - 1003.\nncn-m# cray capmc get_xname_status create --xnames x[1000-1003]c[0-7] --format json Rectifiers (PSUs) should indicate that DC power is OFF (AC OK means the power is on).\nSet the cabinet PDU circuit breakers to OFF for each shelf.\nThe AC OK LED on each PSU will remain amber for about 30 seconds (AC lost) until the system de-energizes, then it will extinguish.\nNOTE: If the TDS cabinet rack-mounted coolant distribution unit (MCDU) is receiving power from the PDUs in the management cabinet, then the MCDU may stay on after the TDS cabinet PDU circuit breakers are set to OFF. This is expected.\nCAUTION: Do not power off the CDU if it is actively cooling other equipment.\nIf other systems are not being cooled by the floor-standing CDU, then open the CDU rear door to access the control panel and set the circuit breakers to OFF.\nPower off standard rack PDU circuit breakers Check the power status before shutdown.\nThis example shows nodes in cabinets 3001 - 3003.\nncn-m# cray capmc get_xname_status create --xnames x300[1-3]c0s[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35]b[1-4]n0 --format json The get_xname_status command requires that the list of components be explicitly listed. In this example, the system includes only 2U servers and there are no state manager entries for even-numbered U-positions (slots); those would return an error.\nThe command does not filter nonexistent component names (xnames) and displays an error when invalid component names are specified. Use --filter show_all option to filter all the output:\nncn-m# cray capmc get_xname_status create --filter show_all --format json Example output:\n{ \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;off\u0026#34;: [ \u0026#34;x3000c0s19b0n0\u0026#34;, \u0026#34;x3000c0s20b0n0\u0026#34;, \u0026#34;x3000c0s22b0n0\u0026#34;, \u0026#34;x3000c0s24b0n0\u0026#34;, \u0026#34;x3000c0s27b1n0\u0026#34;, \u0026#34;x3000c0s27b2n0\u0026#34;, \u0026#34;x3000c0s27b3n0\u0026#34;, \u0026#34;x3000c0s27b4n0\u0026#34; ], \u0026#34;on\u0026#34;: [ \u0026#34;x3000c0r15e0\u0026#34;, \u0026#34;x3000c0s2b0n0\u0026#34;, \u0026#34;x3000c0s3b0n0\u0026#34;, \u0026#34;x3000c0s4b0n0\u0026#34;, \u0026#34;x3000c0s5b0n0\u0026#34;, \u0026#34;x3000c0s6b0n0\u0026#34;, \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;x3000c0s8b0n0\u0026#34;, \u0026#34;x3000c0s9b0n0\u0026#34; ] } Use CAPMC to power off HPE Cray standard racks for non-management nodes.\nCAUTION: Do not power off the management cabinet. Verify that the components names (xnames) specified in the following command line do not accidentally power off management cabinets.\nThis example shuts down racks 3001 - 3003.\nncn-m# cray capmc xname_off create --xnames x300[1-3]c0s[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35]b[1-4]n0 Check the status of the CAPMC power off command.\nncn-m# cray capmc get_xname_status create --xnames x300[1-3]c0s[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35]b[1-4]n0 --format json Set each cabinet PDU circuit breaker to OFF.\nA slotted screwdriver may be required to open PDU circuit breakers.\nTo power off Motivair liquid-cooled chilled doors and CDUs, locate the power off switch on the CDU control panel and set it to OFF.\nRefer to vendor documentation for the chilled-door cooling system for power control procedures when chilled doors are installed on standard racks.\nNext step Return to System Power Off Procedures and continue with next step.\n"
},
{
	"uri": "/docs-csm/en-12/operations/package_repository_management/nexus_export_and_restore/",
	"title": "Nexus Export and Restore",
	"tags": [],
	"description": "",
	"content": "Nexus Export and Restore The current process for ensuring the safety of the nexus-data PVC is a one time, space intensive, manual process, and is only recommended to be done while Nexus is in a known good state. An export is recommended to be done before an upgrade, in order to enable the ability to roll back. Taking an export can also be used to improve Nexus resiliency by allowing easy fixes for data corruption.\nCAUTION: This process may be risky and is not recommended for all use cases.\nExport Note: Only one export should be taken. Each time the script is run, it will overwrite the old export.\nPrior to making an export, check the size of the exported tar file on the cluster (for example, three times the size of just the export) and the amount of storage that the cluster has left.\nRun the following command on a master node:\nncn-m# kubectl exec -n nexus deploy/nexus -c nexus -- df -P /nexus-data | grep \u0026#39;/nexus-data\u0026#39; | awk \u0026#39;{print \u0026#34;Amount of space the Nexus export will take up on cluster: \u0026#34;(($3 * 3)/1048576)\u0026#34; GiB\u0026#34;;}\u0026#39; \u0026amp;\u0026amp; ceph df | grep \u0026#39;zone1.rgw.buckets.data\u0026#39; | awk \u0026#39;{ print \u0026#34;Currently used: \u0026#34; $7 $8 \u0026#34;, Max Available \u0026#34; $10 $11;}\u0026#39; The above commands will return the following information:\nThe amount of space the Nexus export will take on the cluster. The amount of space currently used in the Ceph pool where the export will be stored. The maximum amount of space available in that Ceph pool. If the size of the Nexus export plus the size of the currently used space is larger than the maximum available space, then follow the steps on Nexus Space Cleanup.\nTaking the export can take multiple hours and Nexus will be unavailable for the entire time. For a fresh install of Nexus, the export takes around 1 hour for every 60 GiB stored in the nexus-data PVC. For example, if the nexus-data PVC is 120 GiB (meaning the first step showed the export will use 360 GiB on cluster), then Nexus would be unavailable for around 2 hours while the export was taking place. If the time required to backup is too long because of the size it will take follow the steps on Nexus Space Cleanup.\n(ncn-m#) If an export has been taken previously, then it should be deleted before a new export is taken.\nCheck for existing nexus-bak PVC. If found, it needs to be removed.\nkubectl get pvc -n nexus nexus-bak Example output:\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nexus-bak Bound pvc-7551d342-f976-48e1-bb91-1957b75dbc53 1000Gi RWO k8s-block-replicated 42d Check for existing nexus-backup job. If found, it needs to be removed.\nkubectl get jobs -n nexus nexus-backup Example output:\nNAME COMPLETIONS DURATION AGE nexus-backup 1/1 6h22m 42d See Cleanup previous export.\nTo take an export of nexus, run the export script on any master node where the latest CSM documentation is installed. See Check for latest documentation.\nncn-m# /usr/share/doc/csm/scripts/nexus-export.sh Example output:\nGibibytes available in cluster: 52418 Gibibytes used in nexus-data: 434 Gibibytes available in nexus-data: 566 Space to be used from backup: 1302 Creating PVC for Nexus backup, if needed Error from server (NotFound): persistentvolumeclaims \u0026#34;nexus-bak\u0026#34; not found persistentvolumeclaim/nexus-bak created Scaling Nexus deployment to 0 deployment.apps/nexus scaled Starting backup, do not exit this script. Should be done around Fri 22 Mar 2024 06:29:03 PM UTC (7:14 from now) job.batch/nexus-backup created Waiting for the backup to finish. .............................. A single \u0026ldquo;.\u0026rdquo; will be output every 30 seconds until the export reports \u0026ldquo;Done\u0026rdquo;.\nRestore The restore will delete any changes made to Nexus after the backup was taken. The restore takes around half the time that the export took (for example, if the export took two hours then the restore would take around one hour). While the restore is underway, Nexus is unavailable.\nTo restore Nexus to the state of the backup, run the restore script on any master node where the latest CSM documentation is installed. See Check for latest documentation.\nncn-m# /usr/share/doc/csm/scripts/nexus-restore.sh Cleanup To cleanup all the jobs and data that the export or restore creates, there are a few different commands that can be used.\nCleanup export job ncn-mw# kubectl delete job -n nexus nexus-backup Cleanup restore job ncn-mw# kubectl delete job -n nexus nexus-restore Cleanup previous export If a new export is being created, then it is recommended to first delete the old export, in order to ensure that everything exported correctly. If the old export is not deleted, then the new job will overwrite the old export. This is expected behavior, because only one export should be used at a time.\nTo delete an export:\nncn-mw# kubectl delete pvc -n nexus nexus-bak Cleanup failed or stopped export If an export is stopped prematurely or fails to complete, there are a few steps that need to be taken to bring Nexus back into a working state.\nDelete the failed or stopped job.\nSee Cleanup export job.\nDelete the partially filled export PVC.\nSee Cleanup previous export.\nRestart Nexus if it is still stopped.\nDepending on where the job failed the Nexus pods may still be down.\nCheck if the Nexus pods are down.\nncn-mw# kubectl get pods -n nexus | grep nexus If the Nexus pod is not found, then scale it back up.\nncn-mw# kubectl -n nexus scale deployment nexus --replicas=1 "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/rebuild_ncns/power_cycle_and_rebuild_nodes/",
	"title": "Power Cycle and Rebuild Nodes",
	"tags": [],
	"description": "",
	"content": "Power Cycle and Rebuild Nodes This section applies to all node types. The commands in this section assume the variables from the prerequisites section have been set.\nProcedure Open and watch the console for the node being rebuilt.\nLog in to a second session to use it to watch the console using the instructions at the link below:\nOpen this link in a new tab or page Log in to a Node Using ConMan\nThe first session will be needed to run the commands in the following Rebuild Node steps.\nSet the PXE boot option and power cycle the node.\nIMPORTANT: Run these commands from a node NOT being rebuilt.\nIMPORTANT: The commands in this section assume the variables from the prerequisites section have been set.\nSet the BMC variable to the hostname of the BMC of the node being rebuilt.\nlinux# BMC=\u0026#34;${NODE}-mgmt\u0026#34; Set and export the root password of the BMC.\nNOTE: read -s is used to prevent the password from echoing to the screen or being saved in the shell history.\nlinux# read -r -s -p \u0026#34;${BMC} root password: \u0026#34; IPMI_PASSWORD linux# export IPMI_PASSWORD Set the PXE/efiboot option.\nlinux# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC}\u0026#34; chassis bootdev pxe options=efiboot Power off the node.\nlinux# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC}\u0026#34; chassis power off Verify that the node is off.\nlinux# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC}\u0026#34; chassis power status Ensure the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nPower on the node.\nlinux# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC}\u0026#34; chassis power on Verify that the node is on.\nEnsure the power is reporting as on. This may take 5-10 seconds for this to update.\nlinux# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC}\u0026#34; chassis power status Observe the boot.\nAfter a bit, the node should begin to boot. This can be viewed from the ConMan console window. Eventually, there will be a NBP file... message in the console output which indicates that the PXE boot has begun the TFTP download of the ipxe program. Messages will appear as the Linux kernel loads, and later when the scripts in the initrd begin to run, including cloud-init.\nWait until cloud-init displays messages similar to these on the console, indicating that cloud-init has finished with the module called modules:final.\n[ 295.466827] cloud-init[9333]: Cloud-init v. 20.2-8.45.1 running \u0026#39;modules:final\u0026#39; at Thu, 26 Aug2021 15:23:20 +0000. Up 125.72 seconds. [ 295.467037] cloud-init[9333]: Cloud-init v. 20.2-8.45.1 finished at Thu, 26 Aug 2021 15:26:12+0000. Datasource DataSourceNoCloudNet [seed=cmdline,http://10.92.100.81:8888/][dsmode=net]. Up 29546 seconds Troubleshooting:\nIf the NBP file... output never appears, or something else goes wrong, then go back to the steps for modifying the XNAME.json file (see the step to inspect and modify the JSON file and make sure these instructions were completed correctly.\nMaster nodes only: If cloud-init did not complete, then the newly rebuilt node will need to have its etcd service definition manually updated. Reconfigure the etcd service and restart cloud-init on the newly rebuilt master:\nncn-m# systemctl stop etcd.service; sed -i \u0026#39;s/new/existing/\u0026#39; \\ /etc/systemd/system/etcd.service /srv/cray/resources/common/etcd/etcd.service; \\ systemctl daemon-reload ; rm -rf /var/lib/etcd/member; \\ systemctl start etcd.service; /srv/cray/scripts/common/kubernetes-cloudinit.sh Rebuilt node with modified SSH keys: The cloud-init process can fail when accessing other nodes if SSH keys have been modified in the cluster. If this occurs, the following steps can be used to repair the desired SSH keys on the newly rebuilt node:\nAllow cloud-init to fail because of the non-matching keys.\nCopy the correct SSH keys to the newly rebuilt node.\nRe-run cloud-init on the newly rebuilt node.\nncn-m# cloud-init clean; cloud-init init --local; cloud-init init Press enter on the console to ensure that the the login prompt is displayed including the correct hostname of this node.\nExit the ConMan console.\nType \u0026amp; and then ..\nSet the wipe flag back so it will not wipe the disk when the node is rebooted.\nRun the following commands from a node that has cray CLI initialized.\nSee Configure the Cray CLI.\nncn# cray bss bootparameters list --name \u0026#34;${XNAME}\u0026#34; --format=json | jq .[] \u0026gt; \u0026#34;${XNAME}.json\u0026#34; Edit the XNAME.json file and set the metal.no-wipe=1 value.\nGet a token to interact with BSS using the REST API.\nncn# TOKEN=$(curl -s -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Do a PUT action for the edited JSON file.\nThis command can be run from any node.\nncn# curl -i -s -k -H \u0026#34;Content-Type: application/json\u0026#34; \\ -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ \u0026#34;https://api-gw-service-nmn.local/apis/bss/boot/v1/bootparameters\u0026#34; \\ -X PUT -d @\u0026#34;./${XNAME}.json\u0026#34; Verify that the bss bootparameters list command returns the expected information.\nExport the list from BSS to a file with a different name.\nncn# cray bss bootparameters list --name \u0026#34;${XNAME}\u0026#34; --format=json |jq .[]\u0026gt; \u0026#34;${XNAME}.check.json\u0026#34; Compare the new JSON file with what was put into BSS.\nThis command should give no output, because the files should be identical.\nncn# diff \u0026#34;${XNAME}.json\u0026#34; \u0026#34;${XNAME}.check.json\u0026#34; Next step Proceed to the next step to Validate Boot Loader.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_remove_replace_ncns/allocate_ncn_ip_addresses/",
	"title": "Allocate NCN IP Addresses",
	"tags": [],
	"description": "",
	"content": "Allocate NCN IP Addresses Description This procedure allocates IP addresses for an NCN being added to a system. The addresses are allocated on the applicable networks (HMN, NMN, MTL, CMN, etc.), and added to both the System Layout Service (SLS) and the Boot Script Service (BSS).\nThis procedure will perform and verify the following:\nIf the NCN being added is one of the first three master, storage, or worker NCNs, then its IP address is expected to already be present and consistent between SLS and BSS. Otherwise, new IP addresses for the NCN will be allocated and verified to be within the static IP address pool in the bootstrap_dhcp subnet for the various networks in system. Prerequisites The latest CSM documentation is installed on the system. See Check for latest documentation. Procedure Retrieve an API token.\nncn-mw# export TOKEN=$(curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth \\ -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Determine the component name (xname) of the NCN by referring to the HMN of the systems SHCD, if it has not been determined yet.\nSample row from the HMN tab of an SHCD:\nSource (J20) Source Rack (K20) Source Location (L20) (M20) Parent (N20) (O20) Source Port (P20) Destination (Q20) Destination Rack (R20) Destination Location (S20) (T20) Destination Port (U20) wn01 x3000 u04 - j3 sw-smn01 x3000 u14 - j48 The Source name for the a worker NCN would be in the format of wn01; master NCNs are mn01, and storage NCNs are sn01.\nNode xname format: xXcCsSbBnN\nSHCD Column to Reference Description X Cabinet number Source Rack (K20) The Cabinet or rack number containing the Management NCN. C Chassis number For air-cooled nodes within a standard rack, the chassis is 0. S Slot/Rack U Source Location (L20) The Slot of the node is determined by the bottom most rack U that node occupies. B BMC number For Management NCNs the BMC number is 0. N Node number For Management NCNs the Node number is 0. ncn-mw# export XNAME=x3000c0s4b0n0 Perform a dry-run of allocating IP addresses for the NCN.\nncn-mw# cd /usr/share/doc/csm/scripts/operations/node_management/Add_Remove_Replace_NCNs/ ncn-mw# ./add_management_ncn.py allocate-ips --xname \u0026#34;$XNAME\u0026#34; --alias \u0026#34;$NODE\u0026#34; Example output:\n... IP Addressees have been allocated for x3000c0s36b0n0 (ncn-s004) and been added to SLS and BSS WARNING A Dryrun was performed, and no changes were performed to the system ================================= Management NCN IP Allocation ================================= Network | IP Address --------|----------- CMN | 10.103.11.42 CAN | 10.102.4.10 HMN | 10.254.1.14 MTL | 10.1.1.7 NMN | 10.252.1.9 ================================= Management NCN BMC IP Allocation ================================= Network | IP Address --------|----------- HMN | 10.254.1.21 Depending on the networking configuration of the system, the CMN or CAN networks may not be present in SLS network data. No IP addresses will be allocated for networks that do not exist in SLS.\nAllocate IP addresses for the NCN in SLS and HSM by adding the --perform-changes argument to the command in the previous step.\nncn-mw# ./add_management_ncn.py allocate-ips --xname \u0026#34;$XNAME\u0026#34; --alias \u0026#34;$NODE\u0026#34; --perform-changes Example output:\nIP Addressees have been allocated for x3000c0s36b0n0 (ncn-s004) and been added to SLS and BSS ================================= Management NCN IP Allocation ================================= Network | IP Address --------|----------- CMN | 10.103.11.42 CAN | 10.102.4.10 HMN | 10.254.1.14 MTL | 10.1.1.7 NMN | 10.252.1.9 ================================= Management NCN BMC IP Allocation ================================= Network | IP Address --------|----------- HMN | 10.254.1.21 Next step Proceed to the next step to Add Switch Configuration or return to the main Add, Remove, Replace, or Move NCNs page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/metallb_bgp/troubleshoot_bgp_not_accepting_routes_from_metallb/",
	"title": "Troubleshoot BGP not Accepting Routes from MetalLB",
	"tags": [],
	"description": "",
	"content": "Troubleshoot BGP not Accepting Routes from MetalLB Check the number of routes that the Border Gateway Protocol (BGP) Router is accepting in the peering session. This procedure is useful if Kubernetes LoadBalancer services in the NMNLB, HMNLB, CMN, CHN or CAN address pools are not accessible from outside the cluster.\nRegain access to Kubernetes LoadBalancer services from outside the cluster.\nPrerequisites This procedure requires administrative privileges.\nProcedure Log into the spine or aggregate switch.\nIn this example, the Aruba or Mellanox spine or aggregate switch is accessed from ncn-m001. In this case, sw-spine-001.hmn is being accessed: You should check BOTH spine switches during this process.\nncn-m001# ssh admin@sw-spine-001.hmn Check the number of routes that the BGP Router is accepting in the peering session.\nMellanox:\nLook at the number under the State/Pfx column in the output. There should be a number that matches the number of unique LoadBalancer IP addresses configured in the cluster.\nsw-spine-001 [standalone: master] # show ip bgp vrf all summary Example output:\nVRF name : Customer BGP router identifier : 10.2.0.2 local AS number : 65533 BGP table version : 1634 Main routing table version: 1634 IPV4 Prefixes : 46 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.101.8.8 4 65536 1267504 1278132 1634 0 0 13:20:11:58 ESTABLISHED/14 10.101.8.9 4 65536 1267296 1278315 1634 0 0 13:20:12:03 ESTABLISHED/18 10.101.8.10 4 65536 1267478 1278327 1634 0 0 13:20:12:15 ESTABLISHED/14 VRF name : default BGP router identifier : 10.2.0.2 local AS number : 65533 BGP table version : 40 Main routing table version: 40 IPV4 Prefixes : 40 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.1.7 4 65533 1195933 1195910 40 0 0 13:20:11:51 ESTABLISHED/12 10.252.1.8 4 65533 1195946 1195921 40 0 0 13:20:12:02 ESTABLISHED/16 10.252.1.9 4 65533 1195961 1195934 40 0 0 13:20:12:15 ESTABLISHED/12 If there is a number smaller than expected, check the routes that have been accepted with the following command:\nsw-spine-001 [standalone: master] # show ip route vrf all bgp Example output:\nFlags: F: Failed to install in H/W B: BFD protected (static route) i: BFD session initializing (static route) x: protecting BFD session failed (static route) c: consistent hashing p: partial programming in H/W VRF Name default: ------------------------------------------------------------------------------------------------------ Destination Mask Flag Gateway Interface Source AD/M ------------------------------------------------------------------------------------------------------ 10.92.100.60 255.255.255.255 c 10.252.1.7 vlan2 bgp 200/0 c 10.252.1.8 vlan2 bgp 200/0 c 10.252.1.9 vlan2 bgp 200/0 10.92.100.71 255.255.255.255 c 10.252.1.7 vlan2 bgp 200/0 c 10.252.1.8 vlan2 bgp 200/0 c 10.252.1.9 vlan2 bgp 200/0 10.92.100.81 255.255.255.255 c 10.252.1.8 vlan2 bgp 200/0 10.92.100.82 255.255.255.255 c 10.252.1.8 vlan2 bgp 200/0 10.92.100.85 255.255.255.255 c 10.252.1.7 vlan2 bgp 200/0 c 10.252.1.8 vlan2 bgp 200/0 c 10.252.1.9 vlan2 bgp 200/0 10.92.100.222 255.255.255.255 c 10.252.1.8 vlan2 bgp 200/0 10.92.100.225 255.255.255.255 c 10.252.1.7 vlan2 bgp 200/0 c 10.252.1.8 vlan2 bgp 200/0 c 10.252.1.9 vlan2 bgp 200/0 10.94.100.60 255.255.255.255 c 10.254.1.10 vlan4 bgp 200/0 c 10.254.1.12 vlan4 bgp 200/0 c 10.254.1.14 vlan4 bgp 200/0 10.94.100.71 255.255.255.255 c 10.254.1.10 vlan4 bgp 200/0 c 10.254.1.12 vlan4 bgp 200/0 c 10.254.1.14 vlan4 bgp 200/0 10.94.100.85 255.255.255.255 c 10.254.1.10 vlan4 bgp 200/0 c 10.254.1.12 vlan4 bgp 200/0 c 10.254.1.14 vlan4 bgp 200/0 10.94.100.222 255.255.255.255 c 10.254.1.12 vlan4 bgp 200/0 10.94.100.225 255.255.255.255 c 10.254.1.10 vlan4 bgp 200/0 c 10.254.1.12 vlan4 bgp 200/0 c 10.254.1.14 vlan4 bgp 200/0 VRF Name Customer: ------------------------------------------------------------------------------------------------------ Destination Mask Flag Gateway Interface Source AD/M ------------------------------------------------------------------------------------------------------ 10.92.100.60 255.255.255.255 c 10.101.8.8 vlan7 bgp 20/0 c 10.101.8.9 vlan7 bgp 20/0 c 10.101.8.10 vlan7 bgp 20/0 10.92.100.71 255.255.255.255 c 10.101.8.8 vlan7 bgp 20/0 c 10.101.8.9 vlan7 bgp 20/0 c 10.101.8.10 vlan7 bgp 20/0 10.92.100.81 255.255.255.255 c 10.101.8.9 vlan7 bgp 20/0 10.92.100.82 255.255.255.255 c 10.101.8.9 vlan7 bgp 20/0 10.92.100.85 255.255.255.255 c 10.101.8.8 vlan7 bgp 20/0 c 10.101.8.9 vlan7 bgp 20/0 c 10.101.8.10 vlan7 bgp 20/0 10.92.100.222 255.255.255.255 c 10.101.8.9 vlan7 bgp 20/0 10.92.100.225 255.255.255.255 c 10.101.8.8 vlan7 bgp 20/0 c 10.101.8.9 vlan7 bgp 20/0 c 10.101.8.10 vlan7 bgp 20/0 10.94.100.60 255.255.255.255 c 10.101.8.8 vlan7 bgp 20/0 c 10.101.8.9 vlan7 bgp 20/0 c 10.101.8.10 vlan7 bgp 20/0 10.94.100.71 255.255.255.255 c 10.101.8.8 vlan7 bgp 20/0 c 10.101.8.9 vlan7 bgp 20/0 c 10.101.8.10 vlan7 bgp 20/0 10.94.100.85 255.255.255.255 c 10.101.8.8 vlan7 bgp 20/0 c 10.101.8.9 vlan7 bgp 20/0 c 10.101.8.10 vlan7 bgp 20/0 10.94.100.222 255.255.255.255 c 10.101.8.9 vlan7 bgp 20/0 10.94.100.225 255.255.255.255 c 10.101.8.8 vlan7 bgp 20/0 c 10.101.8.9 vlan7 bgp 20/0 c 10.101.8.10 vlan7 bgp 20/0 10.101.8.113 255.255.255.255 c 10.101.8.8 vlan7 bgp 20/0 c 10.101.8.9 vlan7 bgp 20/0 c 10.101.8.10 vlan7 bgp 20/0 10.101.8.128 255.255.255.255 c 10.101.8.8 vlan7 bgp 20/0 c 10.101.8.9 vlan7 bgp 20/0 c 10.101.8.10 vlan7 bgp 20/0 10.101.8.129 255.255.255.255 c 10.101.8.8 vlan7 bgp 20/0 c 10.101.8.9 vlan7 bgp 20/0 c 10.101.8.10 vlan7 bgp 20/0 10.101.8.130 255.255.255.255 c 10.101.8.8 vlan7 bgp 20/0 c 10.101.8.9 vlan7 bgp 20/0 c 10.101.8.10 vlan7 bgp 20/0 10.101.10.128 255.255.255.255 c 10.101.8.8 vlan7 bgp 20/0 c 10.101.8.9 vlan7 bgp 20/0 c 10.101.8.10 vlan7 bgp 20/0 10.101.11.128 255.255.255.255 c 10.101.8.8 vlan7 bgp 20/0 c 10.101.8.9 vlan7 bgp 20/0 c 10.101.8.10 vlan7 bgp 20/0 If the expected routes are not present, check the route-map or prefix-list configuration on the spine switch.\nAruba:\nTo check the status for Aruba:\nsw-spine-001# show bgp all-vrf all summary Example output:\nVRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.2.0.2 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 3 Cfg. Keep Alive : 1 Confederation Id : 0 Address-family : IPv4 Unicast ----------------------------- Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.3 65533 571006 571002 06d:14h:38m Established Up 10.252.1.7 65533 451712 451502 03d:09h:34m Established Up 10.252.1.8 65533 450943 450712 03d:09h:36m Established Up 10.252.1.9 65533 451463 451267 03d:09h:35m Established Up Address-family : IPv6 Unicast ----------------------------- Address-family : L2VPN EVPN ----------------------------- VRF : Customer BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.103.15.186 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 3 Cfg. Keep Alive : 1 Confederation Id : 0 Address-family : IPv4 Unicast ----------------------------- Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.103.11.3 65533 500874 500891 00h:00m:11s Established Up 10.103.11.8 65536 374118 374039 03d:09h:35m Established Up 10.103.11.9 65536 373454 373290 03d:09h:35m Established Up 10.103.11.10 65536 374169 374087 03d:09h:34m Established Up Address-family : IPv6 Unicast ----------------------------- To check the routes for Aruba:\nsw-spine-001# show ip route bgp all-vrfs Example output:\nDisplaying ipv4 routes selected for forwarding Origin Codes: C - connected, S - static, L - local R - RIP, B - BGP, O - OSPF Type Codes: E - External BGP, I - Internal BGP, V - VPN, EV - EVPN IA - OSPF internal area, E1 - OSPF external type 1 E2 - OSPF external type 2 VRF: Customer Prefix Nexthop Interface VRF(egress) Origin/ Distance/ Age Type Metric ---------------------------------------------------------------------------------------------- 10.92.100.60/32 10.103.11.9 vlan7 - B/E [20/0] 03h:54m:13s 10.103.11.8 vlan7 - [20/0] 03h:54m:13s 10.103.11.10 vlan7 - [20/0] 03h:54m:13s 10.92.100.71/32 10.103.11.9 vlan7 - B/E [20/0] 03d:09h:38m 10.103.11.8 vlan7 - [20/0] 03d:09h:38m 10.103.11.10 vlan7 - [20/0] 03d:09h:38m 10.92.100.81/32 10.103.11.8 vlan7 - B/E [20/0] 03d:09h:39m 10.92.100.85/32 10.103.11.9 vlan7 - B/E [20/0] 03d:09h:33m 10.103.11.8 vlan7 - [20/0] 03d:09h:33m 10.103.11.10 vlan7 - [20/0] 03d:09h:33m 10.92.100.225/32 10.103.11.9 vlan7 - B/E [20/0] 04h:06m:56s 10.103.11.8 vlan7 - [20/0] 04h:06m:56s 10.103.11.10 vlan7 - [20/0] 04h:06m:56s 10.94.100.60/32 10.103.11.9 vlan7 - B/E [20/0] 03h:54m:13s 10.103.11.8 vlan7 - [20/0] 03h:54m:13s 10.103.11.10 vlan7 - [20/0] 03h:54m:13s 10.94.100.71/32 10.103.11.9 vlan7 - B/E [20/0] 03d:09h:38m 10.103.11.8 vlan7 - [20/0] 03d:09h:38m 10.103.11.10 vlan7 - [20/0] 03d:09h:38m 10.94.100.85/32 10.103.11.9 vlan7 - B/E [20/0] 03d:09h:33m 10.103.11.8 vlan7 - [20/0] 03d:09h:33m 10.103.11.10 vlan7 - [20/0] 03d:09h:33m 10.94.100.225/32 10.103.11.9 vlan7 - B/E [20/0] 04h:06m:56s 10.103.11.8 vlan7 - [20/0] 04h:06m:56s 10.103.11.10 vlan7 - [20/0] 04h:06m:56s 10.103.11.61/32 10.103.11.9 vlan7 - B/E [20/0] 03d:09h:33m 10.103.11.8 vlan7 - [20/0] 03d:09h:33m 10.103.11.10 vlan7 - [20/0] 03d:09h:33m 10.103.11.64/32 10.103.11.9 vlan7 - B/E [20/0] 03d:09h:38m 10.103.11.8 vlan7 - [20/0] 03d:09h:38m 10.103.11.10 vlan7 - [20/0] 03d:09h:38m 10.103.11.65/32 10.103.11.9 vlan7 - B/E [20/0] 03d:09h:38m 10.103.11.8 vlan7 - [20/0] 03d:09h:38m 10.103.11.10 vlan7 - [20/0] 03d:09h:38m 10.103.11.66/32 10.103.11.9 vlan7 - B/E [20/0] 03d:09h:33m 10.103.11.8 vlan7 - [20/0] 03d:09h:33m 10.103.11.10 vlan7 - [20/0] 03d:09h:33m 10.103.11.160/32 10.103.11.9 vlan7 - B/E [20/0] 03d:09h:38m 10.103.11.8 vlan7 - [20/0] 03d:09h:38m 10.103.11.10 vlan7 - [20/0] 03d:09h:38m 10.103.11.161/32 10.103.11.9 vlan7 - B/E [20/0] 03d:09h:33m 10.103.11.8 vlan7 - [20/0] 03d:09h:33m 10.103.11.10 vlan7 - [20/0] 03d:09h:33m 10.103.11.224/32 10.103.11.9 vlan7 - B/E [20/0] 03d:09h:38m 10.103.11.8 vlan7 - [20/0] 03d:09h:38m 10.103.11.10 vlan7 - [20/0] 03d:09h:38m 10.103.11.225/32 10.103.11.9 vlan7 - B/E [20/0] 03d:09h:33m 10.103.11.8 vlan7 - [20/0] 03d:09h:33m 10.103.11.10 vlan7 - [20/0] 03d:09h:33m VRF: default Prefix Nexthop Interface VRF(egress) Origin/ Distance/ Age Type Metric ---------------------------------------------------------------------------------------------- 10.92.100.60/32 10.252.1.9 vlan2 - B/I [70/0] 03h:54m:14s 10.92.100.71/32 10.252.1.8 vlan2 - B/I [70/0] 03d:09h:39m 10.252.1.7 vlan2 - [70/0] 03d:09h:39m 10.252.1.9 vlan2 - [70/0] 03d:09h:39m 10.92.100.81/32 10.252.1.9 vlan2 - B/I [70/0] 03d:09h:39m 10.92.100.82/32 10.252.1.8 vlan2 - B/I [70/0] 03h:51m:26s 10.252.1.7 vlan2 - [70/0] 03h:51m:26s 10.92.100.85/32 10.252.1.8 vlan2 - B/I [70/0] 03d:09h:33m 10.252.1.7 vlan2 - [70/0] 03d:09h:33m 10.252.1.9 vlan2 - [70/0] 03d:09h:33m 10.92.100.222/32 10.252.1.7 vlan2 - B/I [70/0] 03d:09h:39m 10.92.100.225/32 10.252.1.8 vlan2 - B/I [70/0] 04h:06m:57s 10.252.1.7 vlan2 - [70/0] 04h:06m:57s 10.252.1.9 vlan2 - [70/0] 04h:06m:57s 10.94.100.60/32 10.252.1.9 vlan2 - B/I [70/0] 03h:54m:14s 10.94.100.71/32 10.254.1.14 vlan4 - B/I [70/0] 03d:09h:39m 10.254.1.12 vlan4 - [70/0] 03d:09h:39m 10.254.1.10 vlan4 - [70/0] 03d:09h:39m 10.94.100.85/32 10.254.1.14 vlan4 - B/I [70/0] 03d:09h:33m 10.254.1.12 vlan4 - [70/0] 03d:09h:33m 10.254.1.10 vlan4 - [70/0] 03d:09h:33m 10.94.100.222/32 10.254.1.10 vlan4 - B/I [70/0] 03d:09h:39m 10.94.100.225/32 10.254.1.14 vlan4 - B/I [70/0] 04h:06m:57s 10.254.1.12 vlan4 - [70/0] 04h:06m:57s 10.254.1.10 vlan4 - [70/0] 04h:06m:57s Total Route Count : 29 There should be a route for each unique LoadBalancer IP addresses configured in the cluster.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/bgp_basic/",
	"title": "BGP basics",
	"tags": [],
	"description": "",
	"content": "BGP basics \u0026ldquo;The primary function of a Border Gateway Protocol (BGP) speaking system is to exchange network reachability information with other BGP systems. This network reachability information includes information on the list of Autonomous Systems (ASes) that reachability information traverses. This information is sufficient for constructing a graph of AS connectivity for this reachability, from which routing loops may be pruned and, at the AS level, some policy decisions may be enforced.\u0026rdquo; –rfc4271A You can configure BGP to run in either internal (iBGP) or external (eBGP) mode.\nRelevant Configuration\nEnable BGP\nswitch(config)# protocol bgp Configure a BGP Instance\nswitch(config)# router bgp 100 Apply IP address to the VLAN interface on Router 1. Run:\nswitch (config interface vlan 10)# ip address 10.10.10.1 /24 Apply IP address to the VLAN interface on Router 2. Run:\nswitch (config interface vlan 10)# ip address 10.10.10.2 /24 On BGP router 1\nswitch(config router bgp 100)# neighbor 10.10.10.2 remote-as 100 On BGP router 2\nswitch(config router bgp 100)# neighbor 10.10.10.1 remote-as 100 Show Commands to Validate Functionality\nswitch# show ip bgp summary Expected Results\nStep 1: You can configure BGP on the switch Step 2: You can create the network statements and the routes are in the routing table Step 3: You can configure a BGP neighbor that uses an MD5 encrypted password Step 4: You can validate the BGP relationship is established and that the network statement is advertised to the peer □ Step 5: Soft reconfiguration is enabled Use the space below for notes as needed. Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/dns-client/",
	"title": "Configure Domain Name System (DNS) Client",
	"tags": [],
	"description": "",
	"content": "Configure Domain Name System (DNS) Client The Domain Name System (DNS) translates domain and host names to and from IP addresses. A DNS client resolves hostnames to IP addresses by querying assigned DNS servers for the appropriate IP address.\nConfiguration Commands Enter a domain name in CONFIGURATION mode (up to 64 alphanumeric characters):\n# ip domain-name NAME Add names to complete unqualified host names in CONFIGURATION mode:\n# ip domain-list NAME Expected Results Administrators can configure the DNS client The output is correct Administrators can ping the device Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/canu/",
	"title": "CSM Automatic Network Utility",
	"tags": [],
	"description": "",
	"content": "CSM Automatic Network Utility CSM Automatic Network Utility (CANU) is a tool used to generate/validate/test the Shasta management network.\nIntroduction to CANU Official Documentation Quick Start Guide Install CANU Update CANU From CSM Tarball Initialize CANU Verify, generate, or compare switch configurations Generate full network configuration Uninstall CANU CANU Validation Error "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/apply_custom_config_1.2/",
	"title": "Apply Custom Switch Configuration CSM 1.2",
	"tags": [],
	"description": "",
	"content": "Apply Custom Switch Configuration CSM 1.2 Apply the backed up site connection configuration with a couple modifications. Since virtual routing and forwarding (VRF) is now used to separate customer traffic, the site ports and default routes must be added to that VRF.\nPrerequisites Access to the switches\nCustom switch configurations\nBackup Custom Configuration Generated switch configurations already applied\nApply Switch Configurations Aruba apply configurations vrf attach Customer will be added to the port configuration that connects to the site. This has to be applied before the ip address configuration.\nsw-spine-001# conf t interface 1/1/36 no shutdown vrf attach Customer description to:CANswitch_cfcanb6s1-31:from:sw-25g01_x3000u39-j36 ip address 10.101.15.142/30 exit sw-spine-001# conf t sw-spine-001(config)# system interface-group 3 speed 10g sw-spine-002# conf t interface 1/1/36 no shutdown vrf attach Customer description to:CANswitch_cfcanb6s1-46:from:sw-25g02_x3000u40-j36 ip address 10.101.15.190/30 exit If the switch had system interface-group commands those would be added here.\nsw-spine-001(config)# system interface-group 3 speed 10g vrf Customer will be appended to the default route configuration.\nsw-spine-001# conf t sw-spine-001(config)# ip route 0.0.0.0/0 10.101.15.141 vrf Customer sw-spine-002# conf t sw-spine-002(config)# ip route 0.0.0.0/0 10.101.15.189 vrf Customer Mellanox apply configurations vrf forwarding Customer will be added to the port configuration. This has to be applied before the ip address configuration.\nsw-spine-001 [mlag-domain: master] # conf t interface ethernet 1/16 speed 10G force interface ethernet 1/16 mtu 1500 force interface ethernet 1/16 no switchport force interface ethernet 1/16 vrf forwarding Customer interface ethernet 1/16 ip address 10.102.255.10/30 primary sw-spine-002 [mlag-domain: master] # conf t interface ethernet 1/16 speed 10G force interface ethernet 1/16 mtu 1500 force interface ethernet 1/16 no switchport force interface ethernet 1/16 vrf forwarding Customer interface ethernet 1/16 ip address 10.102.255.86/30 primary vrf Customer will replace vrf default\nsw-spine-001 [mlag-domain: master] # conf t ip route vrf Customer 0.0.0.0/0 10.102.255.9 sw-spine-002 [mlag-domain: master] # conf t ip route vrf Customer 0.0.0.0/0 10.102.255.85 Apply users/password All that is required to re-apply the users is to get into the global configuration mode using conf t and paste in the configuration that was copied from the previous step.\nAruba credentials sw-leaf-bmc-001# conf t user admin group administrators password ciphertext xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Dell credentials sw-leaf-001# conf t system-user linuxadmin password xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx username admin password xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx role sysadmin priv-lvl 15 Mellanox credentials sw-spine-001 [standalone: master] # conf t username admin password 7 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx username monitor password 7 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Apply SNMP credentials Dell SNMP sw-leaf-bmc-001# conf t snmp-server group cray-reds-group 3 noauth read cray-reds-view snmp-server user testuser cray-reds-group 3 auth md5 xxxxxxxx priv des xxxxxxx snmp-server view cray-reds-view 1.3.6.1.2 included Aruba SNMP sw-leaf-bmc-001# conf t snmp-server vrf default snmpv3 user testuser auth md5 auth-pass plaintext xxxxxx priv des priv-pass plaintext xxxxx For more information on SNMP credentials, see Change SNMP Credentials on Leaf-BMC Switches and Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials.\nWrite memory Save the configuration once the configuration is applied. See Saving Configuration.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/backup/",
	"title": "Backup a Switch Configuration",
	"tags": [],
	"description": "",
	"content": "Backup a Switch Configuration Copies the running configuration or the startup configuration to a remote location as a file. The configuration can be exported to a file of either type CLI or type JSON format. The \u0026lt;VRF-NAME\u0026gt; is used for the configuration of interfaces on a particular VRF.\nProcedure Create a copy of a running configuration or the startup configuration using the following command:\ncopy {running-config | startup-config} \u0026lt;REMOTE-URL\u0026gt; {cli | json} [vrf \u0026lt;VRF-NAME\u0026gt;] The parameters/syntax of the copy command are described below:\n{running-config | startup-config}\nSelects whether the running configuration or the startup configuration will be copied to a remote location as a file.\n\u0026lt;REMOTE-URL\u0026gt;\nSpecifies the remote target for copying the file.\n{tftp | sftp}://\u0026lt;IP-ADDRESS\u0026gt;[:\u0026lt;PORT-NUMBER\u0026gt;][;blocksize=\u0026lt;BLOCKSIZE-VALUE\u0026gt;]/\u0026lt;FILE-NAME\u0026gt;{cli | json}\nSelects whether the export file is in CLI or JSON format.\nvrf \u0026lt;VRF-NAME\u0026gt;\nSpecifies the VRF to receive the interface configuration. If a VRF is not specified, the default VRF is used.\nThe following is an example of copying a running configuration to remote file in the CLI format:\nswitch# copy running-config tftp://192.168.1.10/runcli cli vrf default ######################################################################### 100.0%Success Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/external_dns/ingress_routing/",
	"title": "Ingress Routing",
	"tags": [],
	"description": "",
	"content": "Ingress Routing Ingress routing to services via Istio\u0026rsquo;s ingress gateway is configured by VirtualService custom resource definitions (CRD). When using external hostnames, there needs to be a VirtualService CRD that matches the external hostname to the desired destination.\nFor example, the configuration below controls the ingress routing for prometheus.cmn.SYSTEM_DOMAIN_NAME:\nncn-w001# kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus Example output:\nNAME GATEWAYS HOSTS AGE cray-sysmgmt-health-prometheus [services/services-gateway] [prometheus.cmn.SYSTEM_DOMAIN_NAME] 22h ncn-w001# kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus -o yaml Example output:\napiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: creationTimestamp: \u0026#34;2020-07-09T17:49:07Z\u0026#34; generation: 1 labels: app: cray-sysmgmt-health-prometheus app.kubernetes.io/instance: cray-sysmgmt-health app.kubernetes.io/managed-by: Tiller app.kubernetes.io/name: cray-sysmgmt-health app.kubernetes.io/version: 8.15.4 helm.sh/chart: cray-sysmgmt-health-0.3.1 name: cray-sysmgmt-health-prometheus namespace: sysmgmt-health resourceVersion: \u0026#34;41620\u0026#34; selfLink: /apis/networking.istio.io/v1beta1/namespaces/sysmgmt-health/virtualservices/cray-sysmgmt-health-prometheus uid: d239dfcc-a827-4a51-9b73-6eccfb937088 spec: gateways: - services/services-gateway hosts: - prometheus.cmn.SYSTEM_DOMAIN_NAME http: - match: - authority: exact: prometheus.cmn.SYSTEM_DOMAIN_NAME route: - destination: host: cray-sysmgmt-health-promet-prometheus port: number: 9090 By matching the external hostname in the authority field, Istio\u0026rsquo;s ingress gateway is able to route incoming traffic from OAuth2 Proxy to the cray-sysmgmt-health-prometheus service in the sysmgmt-health namespace. Also, notice that the VirtualService for prometheus.cmn.SYSTEM_DOMAIN_NAME uses the existing services/services-gateway Gateway CRD and does not create a new one.\nSecure Ingress via OAuth2 Proxy Web apps intended to be accessed via the browser, such as Prometheus, Alertmanager, Grafana, Kiali, Jaeger, Kibana, Elasticsearch, should go through the OAuth2 Proxy reverse proxy. Browser sessions are automatically configured to use a JSON Web Token (JWT) for authorization to Istio\u0026rsquo;s ingress gateway, enabling a central enforcement point of Open Policy Agent (OPA) policies for system management traffic.\nThe OAuth2 Proxy will inject HTTP headers so that an upstream endpoint can identify the user and customize access as needed. To enable ingress via OAuth2 Proxy external hostnames, web apps need to be added to the proxiedWebAppExternalHostnames for the appropriate ingress (customerManagement, customerAccess, or customerHighSpeed) in customizations.yaml (i.e sma-grafana.cmn.{{ network.dns.external }}).\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/dns/powerdns_configuration/",
	"title": "PowerDNS Configuration",
	"tags": [],
	"description": "",
	"content": "PowerDNS Configuration External DNS PowerDNS replaces the CoreDNS server that earlier versions of CSM used to provide External DNS services.\nThe cray-dns-powerdns-can-tcp and cray-dns-powerdns-can-udp LoadBalancer resources are configured to service external DNS requests using the IP address specified by the CSI --cmn-external-dns command line argument.\nThe CSI --system-name and --site-domain command line arguments are combined to form the subdomain used for External DNS.\nSite setup In the following example, the IP address 10.101.8.113 is used for External DNS and the system has the subdomain system.dev.cray.com\nncn-m001# kubectl -n services get service -l app.kubernetes.io/name=cray-dns-powerdns Example output:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cray-dns-powerdns-api ClusterIP 10.24.24.29 \u0026lt;none\u0026gt; 8081/TCP 21d cray-dns-powerdns-can-tcp LoadBalancer 10.27.91.157 10.101.8.113 53:30726/TCP 21d cray-dns-powerdns-can-udp LoadBalancer 10.17.232.118 10.101.8.113 53:30810/UDP 21d cray-dns-powerdns-hmn-tcp LoadBalancer 10.31.228.190 10.94.100.85 53:31080/TCP 21d cray-dns-powerdns-hmn-udp LoadBalancer 10.24.134.53 10.94.100.85 53:31338/UDP 21d cray-dns-powerdns-nmn-tcp LoadBalancer 10.22.159.196 10.92.100.85 53:31996/TCP 21d cray-dns-powerdns-nmn-udp LoadBalancer 10.17.203.241 10.92.100.85 53:31898/UDP 21d A system administrator would typically setup the subdomain system.dev.cray.com in their site DNS and create a record which points to the IP address 10.101.8.113, for example ins1.system.dev.cray.com.\nThe administrator would then delegate queries to system.dev.cray.com to ins1.system.dev.cray.com making it authoritative for that subdomain allowing CSM to respond to queries for services like prometheus.system.dev.cray.com\nThe specifics of how to configure to configuring DNS forwarding is dependent on the DNS server in use, please consult the documentation provided by the DNS server vendor for more information.\nAuthoritative Zone Transfer In addition to responding to external DNS queries, PowerDNS can support replication of domain information to secondary servers via AXFR (Authoritative Zone Transfer) queries.\nConfiguration parameters Zone transfer is configured via customizations.yaml parameters and can also be configured at install time via CSI command line arguments.\nParameter: spec.network.dns.primary_server_name CSI command line argument: --primary-server-name Default value: primary Description: The name of the PowerDNS server, this is combined with the system domain information to create the NS record for zones, for example.\nsystem.dev.cray.com.\t1890\tIN\tNS\tprimary.system.dev.cray.com. This record will also point to the External DNS IP address\n$ dig +short primary.system.dev.cray.com 10.101.8.113 Parameter: spec.network.dns.secondary_servers CSI command line argument: --secondary-servers Default value: \u0026quot;\u0026quot; Description: A comma-separated list of DNS servers to notify in the format server name/ip address.\nexternaldns1.my.domain/1.1.1.1,externaldns2.my.domain/2.2.2.2 If the default value is used no servers to notify on zone update will be configured.\nParameter: spec.network.dns.notify_zones CSI command line argument: --notify-zones Default value: \u0026quot;\u0026quot; Description: A comma-separated list of zones to transfer.\nsystem.dev.cray.com,8.101.10.in-addr.arpa If the default value is used then PowerDNS will attempt to transfer all zones.\nExample configuration for BIND An example configuration demonstrating how to configure BIND as a secondary server for zone transfer.\nFor other DNS servers please consult the documentation provided by the DNS server vendor.\n// This is the primary configuration file for the BIND DNS server named. // // Please read /usr/share/doc/bind9/README.Debian.gz for information on the // structure of BIND configuration files in Debian, *BEFORE* you customize // this configuration file. // // If you are just adding zones, please do that in /etc/bind/named.conf.local include \u0026#34;/etc/bind/named.conf.options\u0026#34;; include \u0026#34;/etc/bind/named.conf.local\u0026#34;; include \u0026#34;/etc/bind/named.conf.default-zones\u0026#34;; include \u0026#34;/etc/bind/named.conf.log\u0026#34;; zone \u0026#34;system.dev.cray.com\u0026#34; { type slave; masters { 10.101.8.113; }; allow-notify { 10.101.8.8; 10.101.8.9; 10.101.8.10; }; file \u0026#34;/var/lib/bind/db.system.dev.cray.com\u0026#34;; }; zone \u0026#34;can.system.dev.cray.com\u0026#34; { type slave; masters { 10.101.8.113; }; allow-notify { 10.101.8.8; 10.101.8.9; 10.101.8.10; }; file \u0026#34;/var/lib/bind/db.can.system.dev.cray.com\u0026#34;; }; zone \u0026#34;8.101.10.in-addr.arpa\u0026#34; { type slave; masters { 10.101.8.113; }; allow-notify { 10.101.8.8; 10.101.8.9; 10.101.8.10; }; file \u0026#34;/var/lib/bind/db.8.101.10.in-addr.arpa\u0026#34;; }; masters should be set to the CMN IP address of the PowerDNS service. This is typically defined at install time by the --cmn-external-dns CSI option.\nallow-notify should contain the CAN IP addresses of all Kubernetes worker nodes.\nDNS Security Extensions and zone transfer Zone signing The CSM implementation of PowerDNS supports the DNS Security Extensions (DNSSEC) and the signing of zones with a user-supplied zone signing key.\nIf DNSSEC is to be used for zone transfer then the dnssec SealedSecret in customizations.yaml should be updated to include a base64 encoded version of the private key portion of the desired zone signing key.\nHere is an example of a zone signing key.\nncn-m001# cat Ksystem.dev.cray.com.+013+63812.private Example output:\nPrivate-key-format: v1.3 Algorithm: 13 (ECDSAP256SHA256) PrivateKey: +WFrfooCjTtoRU5UfhrpuTL0IEm6hYc4YJ6u8CcYquo= Created: 20210817081902 Publish: 20210817081902 Activate: 20210817081902 Encode the key using the base64 utility.\nncn-m001# base64 Ksystem.dev.cray.com.+013+63812.private Example output:\nUHJpdmF0ZS1rZXktZm9ybWF0OiB2MS4zCkFsZ29yaXRobTogMTMgKEVDRFNBUDI1NlNIQTI1NikK UHJpdmF0ZUtleTogK1dGcmZvb0NqVHRvUlU1VWZocnB1VEwwSUVtNmhZYzRZSjZ1OENjWXF1bz0K Q3JlYXRlZDogMjAyMTA4MTcwODE5MDIKUHVibGlzaDogMjAyMTA4MTcwODE5MDIKQWN0aXZhdGU6 IDIwMjEwODE3MDgxOTAyCg== Populate the generate block in customizations.yaml with the encoded key.\nIMPORTANT the name of the key in SealedSecret must match the name of the zone being secured, in the below example the zone name is system.dev.cray.com. If multiple zones are to be secured each zone should have its own entry even if the same key is used.\nspec: kubernetes: sealed_secrets: dnssec: generate: name: dnssec-keys data: - type: static_b64 args: name: system.dev.cray.com value: | UHJpdmF0ZS1rZXktZm9ybWF0OiB2MS4zCkFsZ29yaXRobTogMTMgKEVDRFNBUDI1NlNIQTI1NikK UHJpdmF0ZUtleTogK1dGcmZvb0NqVHRvUlU1VWZocnB1VEwwSUVtNmhZYzRZSjZ1OENjWXF1bz0K Q3JlYXRlZDogMjAyMTA4MTcwODE5MDIKUHVibGlzaDogMjAyMTA4MTcwODE5MDIKQWN0aXZhdGU6 IDIwMjEwODE3MDgxOTAyCg== Transaction signatures Transaction signatures (TSIG) provide a secure communication channel between a primary and secondary DNS server\nTo configure TSIG add the desired key to the dnssec generate block in customizations.yaml. At this time only a single transaction signing key is supported and that key is applied to all zones.\nspec: kubernetes: sealed_secrets: dnssec: generate: name: dnssec-keys data: - type: static_b64 args: name: system.dev.cray.com value: | UHJpdmF0ZS1rZXktZm9ybWF0OiB2MS4zCkFsZ29yaXRobTogMTMgKEVDRFNBUDI1NlNIQTI1NikK UHJpdmF0ZUtleTogK1dGcmZvb0NqVHRvUlU1VWZocnB1VEwwSUVtNmhZYzRZSjZ1OENjWXF1bz0K Q3JlYXRlZDogMjAyMTA4MTcwODE5MDIKUHVibGlzaDogMjAyMTA4MTcwODE5MDIKQWN0aXZhdGU6 IDIwMjEwODE3MDgxOTAyCg== - type: static args: name: system-key.tsig value: name: system-key algorithm: hmac-sha256 key: dnFC5euKixIKXAr6sZhI7kVQbQCXoDG5R5eHSYZiBxY= IMPORTANT The key used for TSIG must have .tsig in the name and unlike the zone signing key it should not be base64 encoded.\nExample configuration for BIND An example configuration demonstrating how to extend the previous BIND configuration example and add the TSIG key.\nkey \u0026#34;system-key\u0026#34; { algorithm hmac-sha256; secret \u0026#34;dnFC5euKixIKXAr6sZhI7kVQbQCXoDG5R5eHSYZiBxY=\u0026#34;; }; # Primary server IP address (i.e., PowerDNS CAN ip) server 10.101.8.113 { keys { system-key; }; }; For other DNS servers please consult the documentation provided by the DNS server vendor.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/default_ip_address_ranges/",
	"title": "Default IP Address Ranges",
	"tags": [],
	"description": "",
	"content": "Default IP Address Ranges The initial installation of the system creates default networks with default settings and with no external exposure. These IP address default ranges ensure that no nodes in the system attempt to use the same IP address as a Kubernetes service or pod, which would result in undefined behavior that is extremely difficult to reproduce or debug.\nThe following table shows the default IP address ranges:\nNetwork IP Address Range Kubernetes service network 10.16.0.0/12 Kubernetes pod network 10.32.0.0/12 Install Network (MTL) 10.1.0.0/16 Node Management Network (NMN) 10.252.0.0/17 High Speed Network (HSN) 10.253.0.0/16 Hardware Management Network (HMN) 10.254.0.0/17 Mountain NMN (see note below table) 10.100.0.0/17 Mountain HMN (see note below table) 10.104.0.0/17 River NMN 10.106.0.0/17 River HMN 10.107.0.0/17 Load Balanced NMN 10.92.100.0/24 Load Balanced HMN 10.94.100.0/24 For the Mountain NMN:\nAllocate a /22 from this range per liquid-cooled cabinet. For example, the following cabinets would be given the following IP addresses in the allocated ranges:\ncabinet 1 = 10.100.0.0/22 cabinet 2 = 10.100.4.0/22 cabinet 3 = 10.100.8.0/22 \u0026hellip; For the Mountain HMN:\nAllocate a /22 from this range per liquid-cooled cabinet. For example, the following cabinets would be given the following IP addresses in the allocated ranges:\ncabinet 1 = 10.104.0.0/22 cabinet 2 = 10.104.4.0/22 cabinet 3 = 10.104.8.0/22 \u0026hellip; The values in the table could be modified prior to install if there is a need to ensure that there are no conflicts with customer resources, such as LDAP or license servers. If a customer has more than one HPE Cray EX system, these values can be safely reused across them all.\nContact customer support for this site if it is required to change the IP address range for Kubernetes services or pods; for example, if the IP addresses within those ranges must be used for something else. The cluster must be fully reinstalled if either of those ranges are changed.\nCustomizable network values There are several network values and other pieces of system information that must be unique to the customer system.\nIP address values and the network for ncn-m001 and the BMC on ncn-m001.\nThe main Customer Management Network (CMN) subnet. The two address pools mentioned below need to be part of this subnet.\nFor more information on the CMN, see Customer Accessible Networks.\nSubnet for the MetalLB static address pool (cmn-static-pool), which is used for services that need to be pinned to the same IP address, such as the system DNS service. Subnet for the MetalLB dynamic address pool (cmn-dynamic-pool), which is used for services such as Prometheus and Nexus that can be reached by DNS. HPE Cray EX Domain: The value of the subdomain that is used to access externally exposed services.\nFor example, if the system is named TestSystem, and the site is example.com, the HPE Cray EX domain would be testsystem.example.com. Central DNS would need to be configured to delegate requests for addresses in this domain to the HPE Cray EX DNS IP address for resolution.\nHPE Cray EX DNS IP: The IP address used for the HPE Cray EX DNS service. Central DNS delegates the resolution for addresses in the HPE Cray EX Domain to this server. The IP address must be in the cmn-static-pool subnet.\nCMN gateway IP address: The IP address assigned to a specific port on the spine switch, which will act as the gateway between the CMN and the rest of the customer\u0026rsquo;s internal networks. This address would be the last-hop route to the CMN network. This will default to the first IP address in the main CMN subnet if it is not specified otherwise.\nThe User Network subnet which will be either the Customer Access Network (CAN) or Customer High-speed Network (CHN). The address pool mentioned below needs to be part of this subnet.\nFor more information on the CAN and CHN, see Customer Accessible Networks.\nSubnet for the MetalLB dynamic address pool (can-dynamic-pool) or (chn-dynamic-pool), which is used for services such as User Access Instances (UAIs) that can be reached by DNS. "
},
{
	"uri": "/docs-csm/en-12/operations/network/customer_accessible_networks/externally_exposed_services/",
	"title": "Externally Exposed Services",
	"tags": [],
	"description": "",
	"content": "Externally Exposed Services The following services are exposed on one or more of the external networks (CMN, CAN, and CHN). Each of these services requires an IP address in the relevant subnets so they are reachable on that network. This IP address is allocated by the MetalLB component.\nServices under Istio Ingress Gateway and OAuth2 Proxy Ingress share an ingress, so they all use the IP allocated to the Ingress.\nEach service is given a DNS name that is served by the PowerDNS service to make them resolvable from the site network. This makes it possible to access each of these services by name rather than finding the allocated IP address. The DNS name and network are prepended to the system-name.site-domain specified during csi config init. For example, if the system is named TestSystem, and the site is example.com, the HPE Cray EX domain would be testsystem.example.com.\nSee External DNS for more information.\nService DNS Name Address Pool Requires CMN/CAN/CHN IP External Port Notes Istio Ingress Gateway - CMN customer-management Yes 80/443, 8081, 8888 Istio Ingress Gateway - CAN customer-access Yes 80/443, 8081, 8888 Istio Ingress Gateway - CHN customer-high-speed Yes 80/443, 8081, 8888 HPE Cray EX REST API api No Uses the IP address of Istio Ingress Gateway (CMN/CAN/CHN) Authentication auth No Uses the IP address of Istio Ingress Gateway (CMN/CAN/CHN) S3 s3 customer-management Yes 8080 PowerDNS customer-management Yes 53 OAuth2 Proxy Ingress - CMN customer-management Yes 443 OAuth2 Proxy Ingress - CAN customer-access Yes 443 OAuth2 Proxy Ingress - CHN customer-high-speed Yes 443 System Management Health Prometheus prometheus No Uses the IP address of OAuth2 Proxy Ingress (CMN) System Management Health Alert Manager alertmanager No Uses the IP address of OAuth2 Proxy Ingress (CMN) System Management Health Grafana grafana No Uses the IP address of OAuth2 Proxy Ingress (CMN) Istio Kiali kiali-istio No Uses the IP address of OAuth2 Proxy Ingress (CMN) VCS vcs No Uses the IP address of OAuth2 Proxy Ingress (CMN) SMA Kibana sma-kibana No Uses the IP address of OAuth2 Proxy Ingress (CMN) SMA Grafana sma-grafana No Uses the IP address of OAuth2 Proxy Ingress (CMN) OPA GPM opa-gpm No Uses the IP address of OAuth2 Proxy Ingress (CMN) CSMS csms No Uses the IP address of OAuth2 Proxy Ingress (CMN) Nexus nexus No Uses the IP address of Istio Ingress Gateway (CMN) Rsyslog Aggregator rsyslog customer-management Yes 514/8514 UAI customer-access or customer-high-speed Yes (multiple) 22 Can be several of these each with a unique ID IMS \u0026lt;uid\\\u0026gt;.ims customer-management Yes (multiple) 22 Can be several of these each with a unique ID "
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/about_kubectl/",
	"title": "About kubectl",
	"tags": [],
	"description": "",
	"content": "About kubectl kubectl is a CLI that can be used to run commands against a Kubernetes cluster. The format of the kubectl command is shown below:\nncn# kubectl COMMAND RESOURCE_TYPE RESOURCE_NAME FLAGS An example of using kubectl to retrieve information about a pod is shown below:\nncn# kubectl get pod POD_NAME1 POD_NAME2 kubectl is installed by default on the non-compute node (NCN) image. To learn more about kubectl, refer to https://kubernetes.io/docs\n"
},
{
	"uri": "/docs-csm/en-12/operations/image_management/convert_tgz_archives_to_squashfs_images/",
	"title": "Convert TGZ Archives to SquashFS Images",
	"tags": [],
	"description": "",
	"content": "Convert TGZ Archives to SquashFS Images If customizing a pre-built image root archive compressed as a .txz or other non-SquashFS format, convert the image root to SquashFS and upload the SquashFS archive to S3.\nThe steps in this section only apply if the image root is not in SquashFS format.\nPrerequisites There is a pre-built image that is not currently in SquashFS format.\nProcedure Locate the image root to be converted to SquashFS.\nImages and recipes are uploaded to IMS and S3 via containers.\nUncompress the desired file to a temporary directory.\nReplace the TXZ_COMPRESSED_IMAGE value with the name of the image root being used that was located in the previous step.\nncn-mw# mkdir -p ~/tmp/image-root ncn-mw# cd ~/tmp/ ncn-mw# tar xvf TXZ_COMPRESSED_IMAGE -C image-root Recompress the image root with SquashFS.\nncn-mw# mksquashfs image-root IMAGE_NAME.squashfs "
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/component_groups_and_partitions/",
	"title": "Component Groups and Partitions",
	"tags": [],
	"description": "",
	"content": "Component Groups and Partitions The Hardware State Manager (HSM) provides the group and partition services. Both are means of grouping (also known as labeling) system components that are tracked by HSM. Components include the nodes, blades, controllers, and more on a system.\nThere is no limit to the number of members a group or partition contains. The only limitation is that all members must be actual members of the system. The HSM needs to know that the components exist.\nGroups Groups are collections of components (primarily nodes) in /hsm/v2/State/Components. Components can be members of any number of groups. Groups can be created freely, and HSM does not assign them any predetermined meaning.\nIf a group has exclusiveGroup=EXCLUSIVE_LABEL_NAME set, then a component may only be a member of one group that matches that exclusive label. For example, if the exclusive group label colors is associated with groups blue, red, and green, then a node that is part of the green group could not also be placed in the red group.\nPartitions Partitions are isolated, non-overlapping groups. Each component can be a member of only one partition at a time, and partitions are used as an access control mechanism. Partitions have a specific predefined meaning, intended to provide logical divisions of a single physical system.\n"
},
{
	"uri": "/docs-csm/en-12/operations/firmware/fas_recipes/",
	"title": "FAS Recipes",
	"tags": [],
	"description": "",
	"content": "FAS Recipes NOTE This is a collection of various FAS recipes for performing updates. For step by step directions and commands, see FAS Use Cases.\nThe following example JSON files are useful to reference when updating specific hardware components. In all of these examples, the overrideDryrun field will be set to false; set them to true to perform a live update.\nWhen updating an entire system, walk down the device hierarchy component type by component type, starting first with routers (switches), proceeding to chassis, and then finally to nodes. While this is not strictly necessary, it does help eliminate confusion.\nNOTE: Any node that is locked remains in the state inProgress with the stateHelper message of \u0026quot;failed to lock\u0026quot; until the action times out, or the lock is released. If the action is timed out, these nodes report as failed with the stateHelper message of \u0026quot;time expired; could not complete update\u0026quot;. This includes NCNs which are manually locked to prevent accidental rebooting and firmware updates.\nRefer to FAS Filters for more information on the content used in the example JSON files.\nManufacturer : Cray (Cray) Device Type: ChassisBMC | Target: BMC IMPORTANT: Before updating a CMM:\nThe hms-discovery job must be stopped before updates are done and restarted after updates are complete.\nStop the hms-discovery job.\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;suspend\u0026#34;:true}}\u0026#39; Start the hms-discovery job.\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;suspend\u0026#34;:false}}\u0026#39; Use CAPMC to make sure all chassis rectifier power is off.\nCheck power status of the chassis. If the chassis power is off, then everything else is off, and it is safe to proceed.\nncn-mw# cray capmc get_xname_status create --xnames x[1000-1008]c[0-7] If the chassis are still powered on, then use CAPMC to make sure everything is off.\nIssue power off command.\nThis command may produce a large list of errors when talking to BMCs. This is expected if the hardware has been partially powered down.\nncn-mw# cray capmc xname_off create --xnames x[1000-1008]c[0-7] --force true --continue true --recursive true Verify that chassis power is off.\nRepeat the earlier check of the chassis power.\n{ \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;chassisBMC\u0026#34; ] }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Cray Chassis Controllers\u0026#34; } } (Cray) Device Type: NodeBMC | Target: BMC { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Olympus node BMCs\u0026#34; } } (Cray) Device Type: NodeBMC | Target: NodeBIOS IMPORTANT:\nThe nodes themselves must be powered off in order to update their BIOS. The BMC will still have power and will perform the update. When the BMC is updated or rebooted after updating the Node0.BIOS and/or Node1.BIOS on liquid-cooled nodes, the node BIOS version will not report the new version string until the nodes are powered back on. It is recommended that the Node0.BIOS / Node1.BIOS be updated in a separate action, either before or after a BMC update, and that the nodes are powered back on after a BIOS update. The liquid-cooled nodes must be powered off for the BIOS to be updated. { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;Node0.BIOS\u0026#34;, \u0026#34;Node1.BIOS\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Node BIOS\u0026#34; } } NOTE If this update does not work as expected, follow the Compute Node BIOS Workaround for HPE CRAY EX425 procedure.\n(Cray) Device Type: NodeBMC | Target: Redstone FPGA IMPORTANT:\nThe nodes themselves must be powered on in order to update the firmware of the Redstone FPGA on the nodes. If updating FPGAs fails because of No Image available, update using the \u0026ldquo;Override an Image for an Update\u0026rdquo; procedure in FAS Admin Procedures: Find the imageID using the following command\nncn-mw# cray fas images list --format json | jq \u0026#39;.[] | .[] | select(.target==\u0026#34;Node0.AccFPGA0\u0026#34;)\u0026#39; { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;Node0.AccFPGA0\u0026#34;, \u0026#34;Node1.AccFPGA0\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Node Redstone FPGA\u0026#34; } } Manufacturer: HPE (HPE) Device Type: NodeBMC | Target: iLO 5 (BMC) { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;hpe\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;iLO 5\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of HPE node iLO 5\u0026#34; } } (HPE) Device Type: NodeBMC | Target: System ROM (BIOS) IMPORTANT:\nIf updating the System ROM of an NCN, the NTP and DNS server values will be lost and must be restored. For NCNs other than ncn-m001, this can be done using the /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh script. Use the -h option to get a list of command line options required to restore the NTP and DNS values. See Configure DNS and NTP on Each BMC. Node should be powered on for System ROM update and will need to be rebooted to use the updated BIOS. { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;NodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;hpe\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;System ROM\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of HPE node system rom\u0026#34; } } NOTE Update of System ROM may report as an error when it actually succeeded because of an incorrect string in the image metadata in FAS. Manually check the update version to get around this error.\nManufacturer: Gigabyte (Gigabyte) Device Type: NodeBMC | Target: BMC { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;gigabyte\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 4000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Gigabyte node BMCs\u0026#34; } } NOTE The timeLimit is 4000 because the Gigabytes can take a lot longer to update.\nTroubleshooting A node may fail to update with the output:\nstateHelper = \u0026#34;Firmware Update Information Returned Downloading – See /redfish/v1/UpdateService\u0026#34; FAS has incorrectly marked this node as failed. It most likely will complete the update successfully.\nTo resolve this issue, do either of the following actions:\nCheck the update status by looking at the Redfish FirmwareInventory (/redfish/v1/UpdateService/FirmwareInventory/BMC). Rerun FAS to verify that the BMC firmware was updated. Make sure to wait for the current firmware to be updated before starting a new FAS action on the same node.\n(Gigabyte) Device Type: NodeBMC | Target: BIOS { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;gigabyte\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BIOS\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 4000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Gigabyte node BIOS\u0026#34; } } Update Non-Compute Nodes (NCNs) See Uploading BIOS and BMC Firmware for NCNs in FAS Use Cases.\n"
},
{
	"uri": "/docs-csm/en-12/operations/conman/disable_conman_after_system_software_installation/",
	"title": "Disable ConMan After the System Software Installation",
	"tags": [],
	"description": "",
	"content": "Disable ConMan After the System Software Installation The ConMan utility is enabled by default. The first procedure provides instructions for disabling it after the system software has been installed, and the second procedure provides instructions on how to later re-enable it.\nPrerequisites This procedure requires administrative privileges.\nDisable Procedure Note: this procedure has changed since the CSM 0.9 release.\nLog on to a Kubernetes master or worker node.\nScale the cray-console-operator pods to 0 replicas.\nncn# kubectl -n services scale --replicas=0 deployment/cray-console-operator Example output:\ndeployment.apps/cray-console-operator scaled Verify the cray-console-operator service is no longer running.\nThe following command will give no output when the service is no longer running.\nncn# kubectl -n services get pods | grep console-operator Scale the cray-console-node pods to 0 replicas.\nncn# kubectl -n services scale --replicas=0 statefulset/cray-console-node Example output:\nstatefulset.apps/cray-console-node scaled Verify the cray-console-node service is no longer running.\nThe following command will give no output when the service is no longer running.\nncn# kubectl -n services get pods | grep console-node Re-enable Procedure Scale the cray-console-operator service back to 1 replica. It will scale the cray-console-node pods after it starts operation.\nncn# kubectl -n services scale --replicas=1 deployment/cray-console-operator Example output:\ndeployment.apps/cray-console-operator scaled Verify services are running again.\nncn# kubectl -n services get pods | grep -e console-operator -e console-node Example output:\ncray-console-node-0 3/3 Running 0 8m44s cray-console-node-1 3/3 Running 0 8m18s cray-console-operator-79bf95964-lngpz 2/2 Running 0 9m29s "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/cfs_flow_diagrams/",
	"title": "CFS Flow",
	"tags": [],
	"description": "",
	"content": "CFS Flow Single session flow Automated session flow Single session flow This section covers the components and actions taken when a user or service creates a session using the CFS sessions endpoint.\nA user creates a CFS configuration. A user creates a CFS session, causing a session record to be created. When a session record is created, the CFS-API also posts an event to a Kafka queue. The CFS-Operator is always monitoring the Kafka queue, and handles events as they come in. The CFS-Operator creates a Kubernetes job for the session in response to a session creation event. The CFS-Operator monitors the Kubernetes job (not the pod) for completion. When the Kubernetes job is complete, the CFS-Operator updates the session record in the CFS-API. Automated session flow This section covers the components and actions taken when a user or service sets a desired configuration for a component, causing automatic configuration to occur.\nA user creates a CFS configuration. A user sets the desired configuration for some number of components. This may immediately trigger configuration if the component is enabled in CFS and the desired configuration has not yet been applied. If that is the case, then skip to step 4. When a node reboots, the CFS-State-Reporter runs on the node, and contacts the CFS-API to enable and clear the state for the current node. This will always trigger the next steps for configuration as long as a desired configuration is set for the node. The CFS-Batcher monitors the CFS-API, periodically querying for enabled components with a pending configuration status. (The status is determined by the API at query time based on the desired and current state of the component). Any components found are placed into batches. The CFS-Batcher calls the CFS-API to create sessions for each batch, using the ansible-limit parameter to limit each session to the components in a batch. The CFS-API creates a session record and session creation event. See Single session flow for details. The CFS-Operator monitors for session creation events. See Single session flow for details. The CFS-Operator creates a Kubernetes job. See Single session flow for details. The Ansible Execution Environment contains a custom Ansible plugin that calls CFS to update the status of all components affected after each play. If the component does not have a configured status due to failure or other reasons, return to step 4. "
},
{
	"uri": "/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_bad_parameters/",
	"title": "Troubleshoot a Failed CRUS Session Because of Bad Parameters",
	"tags": [],
	"description": "",
	"content": "Troubleshoot a Failed CRUS Session Because of Bad Parameters Note: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality. See Deprecated features.\nA CRUS session must be deleted and recreated if it does not start or complete because of parameters having incorrect values.\nThe following are examples of incorrect parameters:\nChoosing the wrong Boot Orchestration Service (BOS) session template. Choosing the wrong group labels. Improperly defined BOS session template. For example, specifying nodes in the template instead of using the label of the upgrading group. Prerequisites A Compute Rolling Upgrade Service (CRUS) session was run and failed to complete. The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray Command Line Interface. Procedure Delete the failed session.\nDeleting a CRUS session that is in progress will terminate the session and move all of the unfinished nodes into the group said up for failed nodes. The time frame for recognizing a delete request, cleaning up, and deleting the session is roughly a minute. A session being deleted will move to a DELETING status immediately upon receiving a delete request, which will prevent further processing of the upgrade in that session.\nncn# cray crus session delete CRUS_UPGRADE_ID --format toml Example output:\napi_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-node-group\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [ \u0026#34;Processing step 0 in stage STARTING failed - failed to obtain Node Group named \u0026#39;slurm-node-group\u0026#39; - {\u0026#34;type\u0026#34;:\u0026#34;about:blank\u0026#34;,\u0026#34;title\u0026#34;:\u0026#34;Not Found\u0026#34;,\u0026#34;detail\u0026#34;:\u0026#34;No such group: slurm-node-group\u0026#34;,\u0026#34;status\u0026#34;:404}\\n[404]\u0026#34;,] starting_label = \u0026#34;slurm-node-group\u0026#34; state = \u0026#34;DELETING\u0026#34; upgrade_id = \u0026#34;d388c6f5-be67-4a31-87a9-819bb4fa804c\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;node-group\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; Recreate the session that failed.\nEnsure that the correct parameters are used when restarting the session.\nncn# cray crus session create \\ --starting-label slurm-nodes \\ --upgrading-label node-group \\ --failed-label failed-node-group \\ --upgrade-step-size 50 \\ --workload-manager-type slurm \\ --upgrade-template-id boot-template \\ --format toml Example output:\napi_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-node-group\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;135f9667-6d33-45d4-87c8-9b09c203174e\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;node-group\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/boot_orchestration/",
	"title": "Boot Orchestration",
	"tags": [],
	"description": "",
	"content": "Boot Orchestration The Boot Orchestration Service (BOS) is responsible for booting, configuring, and shutting down collections of nodes. This is accomplished using BOS components, such as boot orchestration session templates and sessions, as well as launching a Boot Orchestration Agent (BOA) that fulfills boot requests.\nBOS users create a BOS session template via the REST API. A session template is a collection of metadata for a group of nodes and their desired boot artifacts and configuration. A BOS session can then be created by applying an action to a session template. The available actions are boot, reboot, shutdown, and configure. BOS will create a Kubernetes BOA job to apply an action. BOA coordinates with the underlying subsystems to complete the action requested. The session can be monitored to determine the status of the request.\nBOS depends on each of the following services to complete its tasks:\nBOA - Handles any action type submitted to the BOS API. BOA jobs are created and launched by BOS. Boot Script Service (BSS) - Stores the configuration information that is used to boot each hardware component. Nodes consult BSS for their boot artifacts and boot parameters when nodes boot or reboot. Configuration Framework Service (CFS) - BOA launches CFS to apply configuration to the nodes in its boot sets (node personalization). Cray Advanced Platform Monitoring and Control (CAPMC) - Used to power on and off the nodes. Hardware State Manager (HSM) - Tracks the state of each node and what groups and roles nodes are included in. Use the BOS Cray CLI Commands BOS utilizes the Cray CLI commands. The latest API information can be found with the following command:\nncn-m001# cray bos list Example output:\n[[results]] major = \u0026#34;1\u0026#34; minor = \u0026#34;0\u0026#34; patch = \u0026#34;0\u0026#34; [[results.links]] href = \u0026#34;https://api-gw-service-nmn.local/apis/bos/v1\u0026#34; rel = \u0026#34;self\u0026#34; BOS API Changes in Upcoming CSM-1.2.0 Release This is a forewarning of changes that will be made to the BOS API in the upcoming CSM-1.2.0 release. The following changes will be made:\nThe --template-body option for the Cray CLI bos command will be deprecated. Performing a GET on the session status for a boot set (i.e. /v1/session/{session_id}/status/{boot_set_name}) currently returns a status code of 201, but instead it should return a status code of 200. This will be corrected to return 200. "
},
{
	"uri": "/docs-csm/en-12/operations/artifact_management/use_s3_libraries_and_clients/",
	"title": "Use S3 Libraries and Clients",
	"tags": [],
	"description": "",
	"content": "Use S3 Libraries and Clients Several command line clients and language-specific libraries are available in addition to the Simple Storage Service (S3) RESTful API. Developers and system administrators can interact with artifacts in the S3 object store with these tools.\nTo learn more, refer to the following links:\nS3 Python client S3 Go client Amazon Web Services (AWS) S3 CLI documentation "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/common_uai_config/",
	"title": "Common UAI Configuration",
	"tags": [],
	"description": "",
	"content": "Common UAI Configuration This section provides guidance on common UAI configuration activities. Specific procedures and settings are covered elsewhere, but each topic provides links to the appropriate information as well as guidance on using that particular configuration.\nRefer to the following configuration topics:\nChoosing UAI Resource Settings Setting End-User UAI Timeouts Broker UAI Resiliency and Load Balancing Top: User Access Service (UAS)\nNext Topic: Choosing UAI Resource Settings\n"
},
{
	"uri": "/docs-csm/en-12/operations/csm_product_management/configure_keycloak_account/",
	"title": "Configure Keycloak Account",
	"tags": [],
	"description": "",
	"content": "Configure Keycloak Account Installation of CSM software includes a default account for administrative access to keycloak.\nDepending on choices made during the installation, there may be a federated connection to an external Identity Provider (IdP), such as an LDAP or AD server, which enables the use of external accounts in keycloak.\nHowever, if the external accounts are not available, then an \u0026ldquo;internal user account\u0026rdquo; could be created in keycloak. Having a usable account in keycloak with administrative authorization enables the use of the cray CLI for many administrative commands, such as those used to Validate CSM Health and general operation of the management services via the API gateway.\nIn Security and Authentication see the \u0026ldquo;Default Keycloak Realms, Accounts, and Clients\u0026rdquo; section for more information about these topics:\nCertificate Types Change the Keycloak Admin Password Create a Service Account in Keycloak Retrieve the Client Secret for Service Accounts Get a Long-Lived Token for a Service Account Access the Keycloak user Management UI Create Internal User Accounts in the Keycloak Shasta Realm Delete Internal User Accounts in the Keycloak Shasta Realm Remove the Email Mapper from the LDAP User Federation Re-Sync Keycloak Users to Compute Nodes Configure Keycloak for LDAP/AD Authentication Configure the RSA Plugin in Keycloak Preserve Username Capitalization for Users Exported from Keycloak Change the LDAP Server IP Address for Existing LDAP Server Content Change the LDAP Server IP Address for New LDAP Server Content Remove the LDAP User Federation from Keycloak Add LDAP User Federation "
},
{
	"uri": "/docs-csm/en-12/introduction/documentation_conventions/",
	"title": "Documentation Conventions",
	"tags": [],
	"description": "",
	"content": "Documentation Conventions Several conventions have been used in the preparation of this documentation.\nMarkdown Format File Formats Typographic Conventions Command Prompt Conventions which describe the context for user, host, directory, chroot environment, or container environment Markdown Format This documentation is in Markdown format. Although much of it can be viewed with any text editor, a richer experience will come from using a tool which can render the Markdown to show different font sizes, the use of bold and italics formatting, inclusion of diagrams and screen shots as image files, and to follow navigational links within a topic file and to other files.\nThere are many tools which render the Markdown format and provide these advantages. Any Internet search for Markdown tools will provide a long list of these tools. Some of the tools are better than others at displaying the images and allowing you to follow the navigational links.\nFile Formats Some of the installation instructions require updating files in JSON, YAML, or TOML format. These files should be updated with care because some file formats do not accept tab characters for indentation of lines. Only space characters are supported. Refer to online documentation to learn more about the syntax of JSON, YAML, and TOML files. YAML does not support tab characters. The JSON convention is to use four spaces rather than a tab character.\nTypographic Conventions This style indicates program code, reserved words, library functions, command-line prompts, screen output, file/path names, and other software constructs.\n\\ (backslash) At the end of a command line, indicates the Linux shell line continuation character (lines joined by a backslash are parsed as a single line).\nCommand Prompt Conventions Host name and account in command prompts The host name in a command prompt indicates where the command must be run. The account that must run the command is also indicated in the prompt.\nThe root or super-user account always has the # character at the end of the prompt Any non-root account is indicated with account@hostname. A non-privileged account is referred to as user. Node abbreviations The following list contains abbreviations for nodes used below\nCN - compute Node NCN - Non-Compute Node AN - Application Node (special type of NCN) UAN - User Access Node (special type of AN) PIT - Pre-Install Toolkit (initial node used as the inception node during software installation booted from the LiveCD) Prompt Description ncn# Run the command as root on any NCN, except an NCN which is functioning as an Application Node (AN), such as a UAN. ncn-m# Run the command as root on any NCN-M (NCN which is a Kubernetes master node). ncn-m002# Run the command as root on the specific NCN-M (NCN which is a Kubernetes master node) which has this hostname (ncn-m002). ncn-w# Run the command as root on any NCN-W (NCN which is a Kubernetes worker node). ncn-w001# Run the command as root on the specific NCN-W (NCN which is a Kubernetes master node) which has this hostname (ncn-w001). ncn-s# Run the command as root on any NCN-S (NCN which is a Utility Storage node). ncn-s003# Run the command as root on the specific NCN-S (NCN which is a Utility Storage node) which has this hostname (ncn-s003). pit# Run the command as root on the PIT node. linux# Run the command as root on a Linux host. uan# Run the command as root on any UAN. uan01# Run the command as root on hostname uan01. user@uan\u0026gt; Run the command as any non-root user on any UAN. cn# Run the command as root on any CN. Note that a CN will have a hostname of the form nid124356, that is \u0026ldquo;nid\u0026rdquo; and a six digit, zero padded number. hostname# Run the command as root on the specified hostname. user@hostname\u0026gt; Run the command as any non-root user on the specified hostname. Command prompt inside chroot If the chroot command is used, the prompt changes to indicate that it is inside a chroot environment on the system.\nhostname# chroot /path/to/chroot chroot-hostname# Command prompt inside Kubernetes pod If executing a shell inside a container of a Kubernetes pod where the pod name is $podName, the prompt changes to indicate that it is inside the pod. Not all shells are available within every pod, this is an example using a commonly available shell.\nncn# kubectl exec -it $podName /bin/sh pod# Command prompt inside image customization session If using SSH to access the image customization environment (pod) during an image customization session, the prompt changes to indicate that it is inside this environment. This example uses $PORT and $HOST as environment variables with specific settings. When using chroot in this context, the prompt will be different than the above chroot example.\nhostname# ssh -p $PORT root@$HOST root@POD# chroot /mnt/image/image-root :/# Directory path in command prompt Example prompts do not include the directory path, because long paths can reduce the clarity of examples. Most of the time, the command can be executed from any directory. When it matters which directory the command is invoked within, the cd command is used to change into the directory, and the directory is referenced with a period (.) to indicate the current directory\nExamples of prompts as they appear on the system:\nhostname:~ # cd /etc hostname:/etc# cd /var/tmp hostname:/var/tmp# ls ./file hostname:/var/tmp# su - user user@hostname:~\u0026gt; cd /usr/bin user hostname:/usr/bin\u0026gt; ./command Examples of prompts as they appear in this publication:\nhostname # cd /etc hostname # cd /var/tmp hostname # ls ./file hostname # su - user user@hostname \u0026gt; cd /usr/bin user@hostname \u0026gt; ./command Command prompts for network switch configuration The prompts when doing network switch configuration can vary widely depending on which vendor switch is being configured and the context of the item being configured on that switch. There may be two levels of user privilege which have different commands available and a special command to enter configuration mode.\nExample of prompts as they appear in this publication:\nEnter \u0026ldquo;setup\u0026rdquo; mode for the switch make and model, for example:\nremote# ssh admin@sw-leaf-001 sw-leaf-001\u0026gt; enable sw-leaf-001# configure terminal sw-leaf-001(conf)# Refer to the switch vendor OEM documentation for more information about configuring a specific switch.\n"
},
{
	"uri": "/docs-csm/en-12/install/troubleshooting_installation/",
	"title": "Troubleshooting Installation Problems",
	"tags": [],
	"description": "",
	"content": "Troubleshooting Installation Problems The installation of the Cray System Management (CSM) product requires knowledge of the various nodes and switches for the HPE Cray EX system. The procedures in this section should be referenced during the CSM install for additional information on system hardware, troubleshooting, and administrative tasks related to CSM.\nTopics Reset root Password on LiveCD Reinstall LiveCD PXE Boot Troubleshooting Wipe NCN Disks for Reinstallation Restart Network Services and Interfaces on NCNs Utility Storage Node Installation Troubleshooting Ceph CSI Troubleshooting Safeguards for CSM NCN Upgrades Postgres Troubleshooting CSM Services Install Fails Because of Missing Secret Details Reset root Password on LiveCD\nIf the root password on the LiveCD needs to be changed, then this procedure does the reset.\nSee Reset root Password on LiveCD Reinstall LiveCD\nIf a reinstall of the PIT node is needed, the data from the PIT node can be saved to the LiveCD USB and the LiveCD USB can be rebuilt.\nSee Reinstall LiveCD PXE Boot Troubleshooting\nIf a reinstall of the PIT node is needed, the data from the PIT node can be saved to the LiveCD USB and the LiveCD USB can be rebuilt.\nSee PXE Boot Troubleshooting Wipe NCN Disks for Reinstallation\nIf it has been determined an NCN did not properly configure its storage while trying to Deploy Management Nodes during the install, then the storage should be wiped so the node can be redeployed.\nSee Wipe NCN Disks for Reinstallation Restart Network Services and Interfaces on NCNs\nIf an NCN shows any of these problems, the network services and interfaces on that node might need to be restarted.\nInterfaces not showing up IP Addresses not applying Member/children interfaces not being included See Restart Network Services and Interfaces on NCNs Utility Storage Node Installation Troubleshooting\nIf there is a failure in the creation of Ceph storage on the utility storage nodes for one of these scenarios, the Ceph storage might need to be reinitialized.\nSometimes a large OSD can be created which is a concatenation of multiple devices, instead of one OSD per device See Utility Storage Node Installation Troubleshooting Ceph CSI Troubleshooting\nIf there has been a failure to initialize all Ceph CSI components on ncn-s001, then the storage node cloud-init may need to be rerun.\nVerify Ceph CSI Rerun Storage Node cloud-init See Ceph CSI Troubleshooting Safeguards for CSM NCN Upgrades\nIf a reinstall or upgrade is being done, there might be a reason to use one of these safeguards.\nPreserve Ceph on Utility Storage Nodes Protect RAID Configuration on Management Nodes See Safeguards for CSM NCN Upgrades\nPostgres Troubleshooting\nTimeout on cray-sls-init-load during Install CSM Services due to Postgres cluster in SyncFailed state See Troubleshoot Postgres Database CSM Services Install Fails Because of Missing Secret\nIf a new installation is failing with a missing admin-client-auth secret, then see CSM Services Install Fails Because of Missing Secret.\n"
},
{
	"uri": "/docs-csm/en-12/install/cable_management_network_servers/",
	"title": "Cable Management Network Servers",
	"tags": [],
	"description": "",
	"content": "Cable Management Network Servers This topic describes nodes in the air-cooled cabinet with diagrams and pictures showing where to find the ports on the nodes and how to cable the nodes to the management network switches.\nHPE Hardware HPE DL385 HPE DL325 HPE Worker Node Cabling HPE Master Node Cabling HPE Storage Node Cabling HPE UAN Cabling HPE Apollo 6500 XL645D HPE Apollo 6500 XL675D Gigabyte/Intel Hardware Worker Node Cabling Master Node Cabling Storage Node Cabling UAN Cabling HPE Hardware HPE DL385 The OCP Slot is noted (number 7) in the image above. This is the bottom middle slot to the left of the VGA port. Ports are numbered left-to-right: the far left port is port 1. The PCIe Slot 1 is on the top left side of the image above (under number 1). Ports are numbered left-to-right: the far left port is port 1. HPE DL325 The OCP Slot is noted (number 9) in the image above. This is the slot on the bottom left of the node. Ports are numbered left-to-right: the far left port is port 1. The PCIE Slot 1 is on the top left side of the image above (under number 1). Ports are numbered left-to-right: the far left port is port 1. HPE Worker Node Cabling Device Port Linux Device Destination Name VLAN LAG OCP 1 mgmt0 primary N/A HMN, NMN, CAN MLAG-LACP OCP 2 mgmt1 secondary N/A HMN, NMN, CAN MLAG-LACP ILO 1 None HMN leaf-bmc N/A HMN N/A NOTES: A single OCP card is the default worker configuration. SHCD Example hostname Source Destination Destination wn01 x3000u04ocp-j1 x3000u12-j7 sw-25g01 wn01 x3000u04ocp-j2 x3000u13-j7 sw-25g02 HPE Master Node Cabling Dual Card Installations The table below describes the cabling of dual card configurations. Also read notes in this section to see other possible customer-based configurations.\nDevice Port Linux Device Destination Name VLAN LAG OCP 1 mgmt0 primary N/A HMN, NMN, CAN MLAG-LACP OCP 2 sun0 N/A N/A N/A N/A PCIE-SLOT1 1 mgmt1 secondary N/A HMN, NMN, CAN MLAG-LACP PCIE-SLOT1 2 sun1 N/A N/A N/A N/A ILO 1 None HMN leaf-bmc N/A HMN N/A NOTES: REQUIRED: Master 001 (ncn-m001) is required to have a site connection on OCP Port 2 for installation and maintenance. RECOMMENDED: Masters 002 and 003 may optionally have a site connection on OCP Port 2 for emergency system access. REQUIRED: Master 001 (ncn-m001) is required to have its BMC/iLO connected to the site. SHCD Example hostname Source Destination Destination mn01 x3000u01ocp-j1 x3000u12-j1 sw-25g01 mn01 x3000u01s1-j1 x3000u13-j1 sw-25g02 NOTE: Master 1 (ncn-m001) is required to have a site connection for installation and non-CAN system access. This can have several configurations depending on customer requirements/equipment:\nDual 10/25Gb card configurations as described in the table above should use PCIe Slot 1, Port 2 as a site connection if the customer supports 10/25Gb. If the customer does not support 10/25Gb speeds (or connection type) and requires RJ45 copper or 1Gb, then a new and separate card will be installed on ncn-m001 and that card will provide site connectivity. Another possibility (non-HPE hardware mainly) is that a built-in 1Gb port will be used if available (similar to Shasta v1.3 PoR on Gigabyte hardware). HPE Storage Node Cabling Device Port Linux Device Destination Name VLAN LAG OCP 1 mgmt0 primary N/A HMN, NMN, CAN MLAG-LACP OCP 2 sun0 primary N/A SUN MLAG-LACP PCIE-SLOT1 1 mgmt1 secondary N/A HMN, NMN, CAN MLAG-LACP PCIE-SLOT1 2 sun1 secondary N/A SUN MLAG-LACP ILO 1 None HMN leaf-bmc N/A HMN N/A NOTES: All ports are cabled. OCP Port 1 and PCIE Slot 1 Port 1 (first ports) are bonded for the NMN, HMN and CAN. OCP Port 2 and PCIE Slot 1 Port 2 (second ports) cabled but not configured in this release. SHCD Example hostname Source Destination Destination sn01 x3000u17s1-j2 x3000u34-j14 sw-25g02 sn01 x3000u17s1-j1 x3000u34-j8 sw-25g02 sn01 x3000u17ocp-j2 x3000u33-j14 sw-25g01 sn01 x3000u17ocp-j1 x3000u33-j8 sw-25g01 The OCP ports go to the First switch and the PCIe ports go to the Second switch. OCP port 1 and PCIe port 1 form a Bond. OCP port 2 and PCIe port 2 form a Bond. For systems that include 4 leaf switches the cabling will look like the following.\nSHCD Example with four leaf switches. hostname Source Destination Destination sn01 x3000u10ocp-j2 x3000u36-j5 sw-25g04 sn01 x3000u10s1-j2 x3000u35-j5 sw-25g03 sn01 x3000u10ocp-j1 x3000u34-j6 sw-25g02 sn01 x3000u10s1-j1 x3000u33-j6 sw-25g01 HPE UAN Cabling Device Port Linux Device Destination Name VLAN LAG OCP 1 mgmt0 primary N/A NMN N/A OCP 2 mgmt1 primary N/A CAN MLAG-LACP PCIE-SLOT1 1 mgmt2 secondary N/A N/A N/A PCIE-SLOT1 2 mgmt3 secondary N/A CAN MLAG-LACP ILO 1 None HMN leaf-bmc N/A HMN N/A NOTES: All ports are cabled. The OCP Port 1 connects to the NMN in a non-bonded configuration. The PCIE Slot 1 Port 1 is cabled but not configured/used in this release. OCP Port 2 and PCIE Slot 1 Port 2 (second ports) are bonded for the CAN. SHCD Example hostname Source Destination Destination uan01 x3000u17s1-j2 x3000u34-j14 sw-25g02 uan01 x3000u17s1-j1 x3000u34-j8 sw-25g02 uan01 x3000u17ocp-j2 x3000u33-j14 sw-25g01 uan01 x3000u17ocp-j1 x3000u33-j8 sw-25g01 HPE Apollo 6500 XL645D The XL645D has two servers in the same chassis. The iLO BMC RJ45 port is a shared network port. Both iLO/BMC traffic and compute node traffic could transit this link. Isolating this port to iLO/BMC only traffic is not possible within firmware configuration alone. iLO configuration settings must be paired with management switch port settings to ensure only BMC traffic exits the port. The iLO firmware must be set to tag traffic to VLAN 4. The switch port must be set to trunk VLAN 4. Ports on the OCP card are numbered left-to-right: the far left port is port 1. Apollo XL645D Cabling (per server) Server Port Management Network Port Speed Use / Configuration OCP port 1 1G leaf-bmc switch 1Gb Management Network NMN OCP port 2 None None None OCP port 3 None None None OCP port 4 None None None iLO 1G leaf-bmc switch 1Gb Management Network HMN HPE Apollo 6500 XL675D Two PCIe slots (chassis slots 21 and 22) are highlighted. One will contain the 1Gb management network card and one will be for the HSN. The iLO BMC RJ45 port is a shared network port. Both iLO/BMC traffic and compute node traffic could transit this link. Isolating this port to iLO/BMC only traffic is not possible within firmware configuration alone. iLO configuration settings must be paired with management switch port settings to ensure only BMC traffic exits the port. The iLO firmware must be set to tag traffic to VLAN 4. The switch port must be set to trunk VLAN 4. Ports on the PCIe card are numbered left-to-right: the far left port is port 1. Apollo XL675D Cabling Server Port Management Network Port Speed Use / Configuration PCIe port 1 1G leaf-bmc switch 1Gb Management Network NMN PCIe port 2 None None None PCIe port 3 None None None PCIe port 4 None None None iLO 1G leaf-bmc switch 1Gb Management Network HMN Gigabyte/Intel Hardware Worker Node Cabling Server Port Management Network Port Speed Use / Configuration PCIe Slot 1 port 1 spine or leaf pair, switch 1/2 40Gb Management Network NMN/HMN/CAN PCIe Slot 1 port 2 spine or leaf pair, switch 2/2 40Gb Management Network NMN/HMN/CAN SHCD Example hostname Source Destination Destination wn01 x3000u07s1-j1 x3000u24L-j4 sw-smn02 wn01 x3000u07s1-j2 x3000u24R-j4 sw-smn03 NOTE: Cabling of ncn-w001 has changed in Shasta v1.4. Please see ncn-m001 note below.\nMaster Node Cabling Server Port Management Network Port Speed Use / Configuration PCIe Slot 1 port 1 spine or leaf pair, switch 1/2 40Gb Management Network NMN/HMN/CAN PCIe Slot 1 port 2 spine or leaf pair, switch 2/2 40Gb Management Network NMN/HMN/CAN LAN0 port 1 NONE (See note below for ncn-m001) NONE Site (See note below for ncn-m001) SHCD Example hostname Source Destination Destination mn01 x3000u01s1-j1 x3000u24L-j1 sw-smn02 mn01 x3000u01s1-j2 x3000u24R-j1 sw-smn03 NOTE: Master 1 (ncn-m001) is required to have a site connection for installation and non-CAN system access. In Shasta versions \u0026lt;=1.3 this connection was on ncn-w001. This can have several configurations depending on customer requirements/equipment:\nThe default configuration for Gigabyte systems uses the built-in 1Gb lan0 port for site connection on ncn-m001. If the customer requires connectivity greater than 1Gb (or a different connection type), then a new and separate card will be installed on ncn-m001 and that card will provide site connectivity. Storage Node Cabling Server Port Management Network Port Speed Use / Configuration PCIe Slot 1 port 1 spine or leaf pair, switch 1/2 40Gb Management Network NMN/HMN/CAN PCIe Slot 1 port 2 spine or leaf pair, switch 2/2 40Gb Management Network NMN/HMN/CAN SHCD Example hostname Source Destination Destination sn01 x3000u13s1-j1 x3000u24L-j7 sw-smn02 sn01 x3000u13s1-j2 x3000u24R-j7 sw-smn03 UAN Cabling Server Port Management Network Port Speed Use / Configuration LAN0 port 1 leaf-bmc (see note) 1Gb Management Network NMN PCIe Slot 1 port 1 spine or leaf pair, switch 1/2 40Gb Management Network CAN bond PCIe Slot 1 port 2 spine or leaf pair, switch 2/2 40Gb Management Network CAN bond SHCD Example hostname Source Destination Destination uan01 x3000u27s1-j1 x3000u24L-j10 sw-smn02 uan01 x3000u27s1-j2 x3000u24R-j10 sw-smn03 NOTE that there are a couple configurations possible for LAN0:\nExisting Gigabyte systems on Dell and Mellanox network hardware will use the (existing) Dell leaf-bmc port. Any Gigabyte system on Aruba network hardware will use an Aruba 6300 (for the 1Gb port). Optionally a 10/25Gb card could be added in an Aruba hardware system to match the HPE UANs. "
},
{
	"uri": "/docs-csm/en-12/background/ncn_boot_workflow/",
	"title": "NCN Boot Workflow",
	"tags": [],
	"description": "",
	"content": "NCN Boot Workflow This document provides information on non-compute node (NCN) boot devices and boot ordering.\nBoot sources Determine the current boot order Reasons to change the boot order after CSM install Determine if NCNs booted via disk or PXE Set BMCs to DHCP Boot order overview Setting boot order Trimming boot order Example boot orders Reverting changes Locating USB device Boot sources Non-compute nodes (NCNs) can boot from two sources:\nNetwork/PXE Disk Determine the current boot order Under normal operations, the NCNs use the following boot order:\nPXE (to ensure that the NCN is booting with desired images and configuration) Disk (fallback in the event that PXE services are unavailable) Reasons to change the boot order after CSM install After the CSM install is complete, it is usually not necessary to change the boot order. Having PXE first and disk as a fallback works in the majority of situations.\nIt may be desirable to change the boot order under these circumstances:\nTesting disk-backed booting Booting from a USB or remote ISO Testing or deploying other customizations Determine if NCNs booted via disk or PXE There are two different methods for determining whether a management node is booted using disk or PXE. The method to use will vary depending on the system environment.\nCheck kernel parameters.\nncn/pit# cat /proc/cmdline If it starts with kernel, then the node network booted. If it starts with BOOT_IMAGE=(, then it disk booted.\nCheck output from efibootmgr.\nncn/pit# efibootmgr The BootCurrent value should be matched to the list beneath to see if it lines up with a networking option or a cray sd*) option for disk boots.\nncn/pit# efibootmgr Example output:\nBootCurrent: 0016 Timeout: 2 seconds BootOrder: 0000,0011,0013,0014,0015,0016,0017,0005,0007,0018,0019,001A,001B,001C,001D,001E,001F,0020,0021,0012 Boot0000* cray (sda1) Boot0001* UEFI: Built-in EFI Shell Boot0005* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:1D:D8:4E Boot0007* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:1D:D8:4F Boot0010* UEFI: AMI Virtual CDROM0 1.00 Boot0011* cray (sdb1) Boot0012* UEFI: Built-in EFI Shell Boot0013* UEFI OS Boot0014* UEFI OS Boot0015* UEFI: AMI Virtual CDROM0 1.00 Boot0016* UEFI: SanDisk \u0026lt;--- Matches here Boot0017* UEFI: SanDisk, Partition 2 Boot0018* UEFI: HTTP IP4 Intel(R) I350 Gigabit Network Connection Boot0019* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot001A* UEFI: HTTP IP4 Mellanox Network Adapter - B8:59:9F:1D:D8:4E Boot001B* UEFI: HTTP IP4 Mellanox Network Adapter - B8:59:9F:1D:D8:4F Boot001C* UEFI: HTTP IP4 Intel(R) I350 Gigabit Network Connection Boot001D* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot001E* UEFI: PXE IP6 Intel(R) I350 Gigabit Network Connection Boot001F* UEFI: PXE IP6 Intel(R) I350 Gigabit Network Connection Boot0020* UEFI: PXE IP6 Mellanox Network Adapter - B8:59:9F:1D:D8:4E Boot0021* UEFI: PXE IP6 Mellanox Network Adapter - B8:59:9F:1D:D8:4F Set BMCs to DHCP When reinstalling a system, the BMCs for the NCNs may be set to static IP addressing. The /var/lib/misc/dnsmasq.leases file is checked when setting up the symlinks for the artifacts each node needs to boot. So if the BMCs are set to static, those artifacts will not get set up correctly. Set the BMCs back to DHCP by using a command such as:\nread -s is used to prevent the password from being written to the screen or the shell history.\nncn# USERNAME=root ncn# read -r -s -p \u0026#34;NCN BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# for h in $( grep mgmt /etc/hosts | grep -v m001 | awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; ); do ipmitool -U \u0026#34;${USERNAME}\u0026#34; -I lanplus -H \u0026#34;${h}\u0026#34; -E lan set 1 ipsrc dhcp done Some BMCs need a cold reset in order to pick up this change fully:\nncn# for h in $( grep mgmt /etc/hosts | grep -v m001 | awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; ); do ipmitool -U \u0026#34;${USERNAME}\u0026#34; -I lanplus -H \u0026#34;${h}\u0026#34; -E mc reset cold done Boot order overview ipmitool can set and edit boot order; it works better for some vendors based on their BMC implementation efibootmgr speaks directly to the node\u0026rsquo;s UEFI; it can only be ignored by new BIOS activity NOTE: cloud-init will set boot order when it runs, but this does not always work with certain hardware vendors. An administrator can invoke the cloud-init script at /srv/cray/scripts/metal/set-efi-bbs.sh on any NCN.\nSetting boot order This section gives the procedure for setting the boot order on NCNs and the PIT node.\nSetting the boot order with efibootmgr will ensure that the desired network interfaces and disks are in the proper order for booting.\nThe commands are the same for all hardware vendors, except where noted.\nCreate a list of the desired IPv4 boot devices.\nFollow the section corresponding to the hardware manufacturer of the system:\nGigabyte Technology\nncn/pit# efibootmgr | grep -iP \u0026#39;(pxe ipv?4.*adapter)\u0026#39; | tee /tmp/bbs1 Hewlett-Packard Enterprise\nncn/pit# efibootmgr | grep -i \u0026#39;port 1\u0026#39; | grep -i \u0026#39;pxe ipv4\u0026#39; | tee /tmp/bbs1 Intel Corporation\nncn/pit# efibootmgr | grep -i \u0026#39;ipv4\u0026#39; | grep -iv \u0026#39;baseboard\u0026#39; | tee /tmp/bbs1 Create a list of the Cray disk boot devices.\nncn/pit# efibootmgr | grep -i cray | tee /tmp/bbs2 Set the boot order to first PXE boot, with disk boot as the fallback option.\nncn/pit# efibootmgr -o $(cat /tmp/bbs* | awk \u0026#39;!x[$0]++\u0026#39; | sed \u0026#39;s/^Boot//g\u0026#39; | tr -d \u0026#39;*\u0026#39; | awk \u0026#39;{print $1}\u0026#39; | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) | grep -i bootorder Set next boot entry.\nncn/pit# efibootmgr -n \u0026lt;desired_next_boot_device\u0026gt; Set all of the desired boot options to be active.\nncn/pit# cat /tmp/bbs* | awk \u0026#39;!x[$0]++\u0026#39; | sed \u0026#39;s/^Boot//g\u0026#39; | tr -d \u0026#39;*\u0026#39; | awk \u0026#39;{print $1}\u0026#39; | xargs -r -t -i efibootmgr -b {} -a Set next boot entry.\nefibootmgr -n \u0026lt;desired_next_boot_device\u0026gt; After following the steps above on a given NCN, that NCN will use the desired Shasta boot order.\nThis is the end of the Setting boot order procedure.\nTrimming boot order This section gives the procedure for removing unwanted entries from the boot order on NCNs and the PIT node.\nThis section will only advise on removing other PXE entries. There are too many vendor-specific entries beyond disks and NICs to cover in this section (e.g. BIOS entries, iLO entries, etc.).\nIn this case, the instructions are the same regardless of node type (management, storage, or worker):\nMake lists of the unwanted boot entries.\nGigabyte Technology\nncn/pit# efibootmgr | grep -ivP \u0026#39;(pxe ipv?4.*)\u0026#39; | grep -iP \u0026#39;(adapter|connection|nvme|sata)\u0026#39; | tee /tmp/rbbs1 ncn/pit# efibootmgr | grep -iP \u0026#39;(pxe ipv?4.*)\u0026#39; | grep -i connection | tee /tmp/rbbs2 Hewlett-Packard Enterprise\nNOTE: This does not trim HSN Mellanox cards; these should disable their OpROMs using the high speed network snippets.\nncn/pit# efibootmgr | grep -vi \u0026#39;pxe ipv4\u0026#39; | grep -i adapter |tee /tmp/rbbs1 ncn/pit# efibootmgr | grep -iP \u0026#39;(sata|nvme)\u0026#39; | tee /tmp/rbbs2 Intel Corporation\nncn/pit# efibootmgr | grep -vi \u0026#39;ipv4\u0026#39; | grep -iP \u0026#39;(sata|nvme|uefi)\u0026#39; | tee /tmp/rbbs1 ncn/pit# efibootmgr | grep -i baseboard | tee /tmp/rbbs2 Remove them.\nncn/pit# cat /tmp/rbbs* | awk \u0026#39;!x[$0]++\u0026#39; | sed \u0026#39;s/^Boot//g\u0026#39; | awk \u0026#39;{print $1}\u0026#39; | tr -d \u0026#39;*\u0026#39; | xargs -r -t -i efibootmgr -b {} -B The boot menu should be trimmed down to contain only relevant entries.\nThis is the end of the Trimming boot order procedure.\nExample boot orders Each section shows example output of the efibootmgr command.\nMaster node (with onboard NICs enabled)\nBootCurrent: 0009 Timeout: 2 seconds BootOrder: 0004,0000,0007,0009,000B,000D,0012,0013,0002,0003,0001 Boot0000* cray (sda1) Boot0001* UEFI: Built-in EFI Shell Boot0002* UEFI OS Boot0003* UEFI OS Boot0004* cray (sdb1) Boot0007* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot0009* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:89:62 Boot000B* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:89:63 Boot000D* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot0012* UEFI: PNY USB 3.1 FD PMAP Boot0013* UEFI: PNY USB 3.1 FD PMAP, Partition 2 Storage node (with onboard NICs enabled)\nBootNext: 0005 BootCurrent: 0006 Timeout: 2 seconds BootOrder: 0007,0009,0000,0002 Boot0000* cray (sda1) Boot0001* UEFI: Built-in EFI Shell Boot0002* cray (sdb1) Boot0005* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot0007* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:88:76 Boot0009* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:88:77 Boot000B* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Worker node (with onboard NICs enabled)\nBootNext: 0005 BootCurrent: 0008 Timeout: 2 seconds BootOrder: 0007,0009,000B,0000,0002 Boot0000* cray (sda1) Boot0001* UEFI: Built-in EFI Shell Boot0002* cray (sdb1) Boot0005* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot0007* UEFI: PXE IP4 Mellanox Network Adapter - 98:03:9B:AA:88:30 Boot0009* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:89:2A Boot000B* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:89:2B Boot000D* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Reverting changes This procedure is only needed if wishing to revert boot order changes.\nReset the BIOS. Refer to vendor documentation for resetting the BIOS or attempt to reset the BIOS with ipmitool\nNOTE: When using ipmitool against a machine remotely, it requires more arguments:\nread -s is used to prevent the password from being written to the screen or the shell history.\nlinux# USERNAME=root linux# read -r -s -p \u0026#34;NCN BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD linux# export IPMI_PASSWORD linux# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;bmc-hostname\u0026gt; Reset BIOS with ipmitool.\nncn/pit# ipmitool chassis bootdev none options=clear-cmos Set next boot with ipmitool.\nncn/pit# ipmitool chassis bootdev pxe options=persistent ncn/pit# ipmitool chassis bootdev pxe options=efiboot Boot to BIOS for checkout of boot devices.\nncn/pit# ipmitool chassis bootdev bios options=efiboot This is the end of the Reverting changes procedure.\nLocating USB device This procedure explains how to identify USB devices on NCNs.\nSome nodes very obviously display which device is the USB, whereas other nodes (such as Gigabyte) do not.\nParsing the output of efibootmgr can be helpful in determining which device is a USB device. Tools such as lsblk, blkid, or kernel (/proc) may also be of use. As an example, one can sometimes match up ls -l /dev/disk/by-partuuid with efibootmgr -v.\nDisplay the current UEFI boot selections.\nncn/pit# efibootmgr Example output:\nBootCurrent: 0015 Timeout: 1 seconds BootOrder: 000E,000D,0011,0012,0007,0005,0006,0008,0009,0000,0001,0002,000A,000B,000C,0003,0004,000F,0010,0013,0014 Boot0000* Enter Setup Boot0001 Boot Device List Boot0002 Network Boot Boot0003* Launch EFI Shell Boot0004* UEFI HTTPv6: Network 00 at Riser 02 Slot 01 Boot0005* UEFI HTTPv6: Intel Network 00 at Baseboard Boot0006* UEFI HTTPv4: Intel Network 00 at Baseboard Boot0007* UEFI IPv4: Intel Network 00 at Baseboard Boot0008* UEFI IPv6: Intel Network 00 at Baseboard Boot0009* UEFI HTTPv6: Intel Network 01 at Baseboard Boot000A* UEFI HTTPv4: Intel Network 01 at Baseboard Boot000B* UEFI IPv4: Intel Network 01 at Baseboard Boot000C* UEFI IPv6: Intel Network 01 at Baseboard Boot000D* UEFI HTTPv4: Network 00 at Riser 02 Slot 01 Boot000E* UEFI IPv4: Network 00 at Riser 02 Slot 01 Boot000F* UEFI IPv6: Network 00 at Riser 02 Slot 01 Boot0010* UEFI HTTPv6: Network 01 at Riser 02 Slot 01 Boot0011* UEFI HTTPv4: Network 01 at Riser 02 Slot 01 Boot0012* UEFI IPv4: Network 01 at Riser 02 Slot 01 Boot0013* UEFI IPv6: Network 01 at Riser 02 Slot 01 Boot0014* UEFI Samsung Flash Drive 1100 Boot0015* UEFI Samsung Flash Drive 1100 Boot0018* UEFI SAMSUNG MZ7LH480HAHQ-00005 S45PNA0M838871 Boot1001* Enter Setup Set next boot entry.\nIn the example above, the device is 0014 or 0015. An option is to guess it is the first one, and can correct this on-the-fly in POST. Notice the lack of Boot in the ID number given; If wanting to choose Boot0014 in the output above, pass 0014 to efibootmgr:\nncn/pit# efibootmgr -n 0014 Verify that the BootNext device is what was selected.\nncn/pit# efibootmgr | grep -i bootnext Example output:\nBootNext: 0014 Now the UEFI Samsung Flash Drive will boot next.\nNOTE: There are duplicates in the list. During boot, the EFI boot manager will select the first one. If the first one is false, then it can be deleted with efibootmgr -b 0014 -d.\nThis is the end of the Locating USB device procedure.\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/stage_3/",
	"title": "Stage 3 - CSM Service Upgrades",
	"tags": [],
	"description": "",
	"content": "Stage 3 - CSM Service Upgrades Reminder: If any problems are encountered and the procedure or command output does not provide relevant guidance, see Relevant troubleshooting links for upgrade-related issues.\nPrepare assets on ncn-m002 Set the CSM_RELEASE variable to the target CSM version of this upgrade.\nncn-m002# CSM_RELEASE=csm-1.2.0 Follow either the Direct download or Manual copy procedure.\nIf there is a URL for the CSM tar file that is accessible from ncn-m002, then the Direct download procedure may be used. Alternatively, the Manual copy procedure may be used, which includes manually copying the CSM tar file to ncn-m002. Direct download Set the ENDPOINT variable to the URL of the directory containing the CSM release tar file.\nIn other words, the full URL to the CSM release tar file must be ${ENDPOINT}${CSM_RELEASE}.tar.gz\nNOTE This step is optional for Cray/HPE internal installs, if ncn-m002 can reach the internet.\nncn-m002# ENDPOINT=https://put.the/url/here/ Run the script.\nNOTE For Cray/HPE internal installs, if ncn-m002 can reach the internet, then the --endpoint argument may be omitted.\nncn-m002# /usr/share/doc/csm/upgrade/1.2/scripts/upgrade/prepare-assets.sh --csm-version ${CSM_RELEASE} --endpoint \u0026#34;${ENDPOINT}\u0026#34; Skip the Manual copy subsection and proceed to Perform upgrade.\nManual copy Copy the CSM release tar file to ncn-m002.\nSee Update Product Stream.\nSet the CSM_TAR_PATH variable to the full path to the CSM tar file on ncn-m002.\nThe prepare-assets.sh script will delete the CSM tarball in order to free space on the node. If not wanting the tarball file to be deleted for other reasons, then copy the tarball file to a different location, and set CSM_TAR_PATH to point to this new location.\nncn-m002# CSM_TAR_PATH=/path/to/${CSM_RELEASE}.tar.gz Run the script.\nncn-m002# /usr/share/doc/csm/upgrade/1.2/scripts/upgrade/prepare-assets.sh --csm-version ${CSM_RELEASE} --tarball-file \u0026#34;${CSM_TAR_PATH}\u0026#34; Fix the disk boot entries and boot order Apply a workaround for the boot order and to ensure that disk boot entries appear in the BIOS boot selection menu.\nncn-m002# /usr/share/doc/csm/scripts/workarounds/boot-order/run.sh Perform upgrade During this stage there will be a brief (approximately five minutes) window where pods with Persistent Volumes (PVs) will not be able to migrate between nodes. This is due to a redeployment of the Ceph csi provisioners into namespaces, in order to accommodate the newer charts and a better upgrade strategy.\nSet the SW_ADMIN_PASSWORD environment variable.\nSet it to the admin user password for the switches. This is required for post-upgrade tests.\nread -s is used to prevent the password from being written to the screen or the shell history.\nncn-m002# read -s SW_ADMIN_PASSWORD ncn-m002# export SW_ADMIN_PASSWORD Perform the upgrade.\nRun csm-upgrade.sh to deploy upgraded CSM applications and services.\nncn-m002# /usr/share/doc/csm/upgrade/1.2/scripts/upgrade/csm-upgrade.sh Validate cray-shared-kafka was updated properly Occasionally the cray-shared-kafka-kafka pods will be restarted before the cray-shared-kafka-zookeeper pods are ready. Check to make sure that all cray-shared-kafka-kafka and cray-shared-kafka-zookeeper pods have a READY status of 1/1. If any of them have a 2/2, then run the kafka-restart.sh script.\nncn-m002# kubectl get pods -n services -l app.kubernetes.io/instance=cray-shared-kafka Expected output:\nNAME READY STATUS RESTARTS AGE cray-shared-kafka-entity-operator-7f9895897d-zjgkm 3/3 Running 0 12m cray-shared-kafka-kafka-0 2/2 Running 0 10m cray-shared-kafka-kafka-1 2/2 Running 0 10m cray-shared-kafka-kafka-2 2/2 Running 0 10m cray-shared-kafka-zookeeper-0 1/1 Running 0 8m cray-shared-kafka-zookeeper-1 1/1 Running 0 8m cray-shared-kafka-zookeeper-2 1/1 Running 0 8m In this example, because some of the pods are 2/2, the kafka-restart.sh script must be run.\nncn-m002# /usr/share/doc/csm/upgrade/1.2/scripts/strimzi/kafka-restart.sh Verify Keycloak users Verify that the Keycloak users localize job has completed as expected.\nThis section can be skipped if user localization is not required.\nAfter an upgrade, it is possible that all expected Keycloak users were not localized. See Verification procedure to confirm that Keycloak localization has completed as expected.\nStage completed This stage is completed. Continue to Stage 4.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/kubernetes/troubleshoot_liveliness_readiness_probe_failures/",
	"title": "Troubleshoot Liveliness or Readiness Probe Failures",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Liveliness or Readiness Probe Failures Identify and troubleshoot Readiness or Liveliness probes that report services as unhealthy intermittently.\nThis is a known issue and can be classified into two categories, connection refused and client timeout. The commands in this procedure assume the user is logged into either a master or worker non-compute node (NCN).\nTroubleshoot a refused connection Refused connection - symptom Refused connection - procedure Troubleshoot a client timeout Client timeout - symptom Client timeout - procedure Next steps Troubleshoot a refused connection Refused connection - symptom The symptom of this problem is a connection refused message in the event log.\nncn-mw# kubectl get events -A | grep -i unhealthy | grep \u0026#34;connection refused\u0026#34; Example output:\nistio-system 5m24s Warning Unhealthy pod/istio-pilot-68477d98d-5bsmk Readiness probe failed: Get http://10.45.0.100:8080/ready: dial tcp 10.45.0.100:8080: connect: connection refused Refused connection - procedure This may occur if the health check ran when the pod was being terminated.\nTo confirm that this is the case, check that the pod no longer exists. If that is true, then disregard this unhealthy event.\nncn-mw# kubectl get pod/istio-pilot-68477d98d-5bsmk -n istio-system Example output indicating that the pod no longer exists:\nError from server (NotFound): pods \u0026#34;istio-pilot-68477d98d-5bsmk\u0026#34; not found Troubleshoot a client timeout Client timeout - symptom The symptom of this problem is a Client.Timeout or DeadlineExceeded message in the event log.\nncn-mw# kubectl get events -A | grep -i unhealthy | grep -E \u0026#34;Client[.]Timeout|DeadlineExceeded\u0026#34; Example output indicating this issue:\nservices 40m Warning Unhealthy pod/cray-bos-69f85bcd89-vdq52 Liveness probe failed: Get http://10.45.0.20:15020/app-health/cray-bos/livez: net/http: request canceled (Client.Timeout exceeded while awaiting headers) Client timeout - procedure This may occur if the health check did not respond within the specified timeout.\nTo confirm that the service is healthy, check the health using the curl command.\nncn-mw# curl -i http://10.45.0.20:15020/app-health/cray-bos/livez Example output of a healthy service:\nHTTP/1.1 200 OK Date: Tue, 07 Jul 2020 19:37:32 GMT Content-Length: 0 An HTTP response code in the 200\u0026rsquo;s or 300\u0026rsquo;s is considered success. For example, a response of 200 OK.\nNext steps If there is an unhealthy event where the above procedures do not clarify the issue, then contact support.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/interpreting_hms_health_check_results/",
	"title": "Interpreting HMS Health Check Results",
	"tags": [],
	"description": "",
	"content": "Interpreting HMS Health Check Results Table of contents Introduction HMS smoke tests HMS functional tests Additional troubleshooting run_hms_ct_tests.sh smd_tavern_api_test_ncn-functional.sh smd_discovery_status_test_ncn-smoke.sh HTTPsGetFailed ChildVerificationFailed DiscoveryStarted Install blocking vs. Non-blocking failures Known issues Warning flags incorrectly set in HSM for Mountain BMCs BMCs set to On state in HSM ComponentEndpoints of Redfish subtype AuxiliaryController in HSM ComponentEndpoints with EthernetInterfaces named DE* in HSM Custom Roles and SubRoles for Components in HSM Resolved backup pod issues causing smoke test failures Introduction This document describes how to interpret the results of the HMS health check scripts and techniques for troubleshooting when failures occur.\nHMS smoke tests The HMS smoke tests consist of bash scripts that check the status of HMS service pods and jobs in Kubernetes and verify HTTP status codes returned by the HMS service APIs. Additionally, there is one test called smd_discovery_status_test_ncn-smoke.sh which verifies that the system hardware has been discovered successfully. The hms_run_ct_smoke_tests_ncn-resources.sh wrapper script checks for executable files in the HMS smoke test directory on the NCN and runs all tests found in succession.\nncn-mw# /opt/cray/tests/ncn-resources/hms/hms-test/hms_run_ct_smoke_tests_ncn-resources.sh Example output:\nsearching for HMS CT smoke tests... found 11 HMS CT smoke tests... running HMS CT smoke tests... A summary of the test results is printed at the bottom of the output.\nHMS smoke tests ran with 2/11 failures exiting with status code: 1 The tests print the commands being executed while running. They also print the command output and status code if failures occur in order to help with debugging.\nThe following is an example of a pod status failure:\nrunning \u0026#39;/opt/cray/tests/ncn-smoke/hms/hms-reds/reds_smoke_test_ncn-smoke.sh\u0026#39;... Running reds_smoke_test... (11:40:33) Running \u0026#39;/opt/cray/tests/ncn-resources/hms/hms-test/hms_check_pod_status_ncn-resources_remote-resources.sh cray-reds\u0026#39;... services cray-reds-867c65879d-cr4mg 1/2 CrashLoopBackOff 266 24h Pod status: CrashLoopBackOff ERROR: \u0026#39;/opt/cray/tests/ncn-resources/hms/hms-test/hms_check_pod_status_ncn-resources_remote-resources.sh cray-reds\u0026#39; failed with error code: 1 FAIL: reds_smoke_test ran with failures cleaning up... \u0026#39;/opt/cray/tests/ncn-smoke/hms/hms-reds/reds_smoke_test_ncn-smoke.sh\u0026#39; exited with status code: 1 The following is an example of an API call failure:\nrunning \u0026#39;/opt/cray/tests/ncn-smoke/hms/hms-capmc/capmc_smoke_test_ncn-smoke.sh\u0026#39;... Running capmc_smoke_test... (11:40:27) Running \u0026#39;/opt/cray/tests/ncn-resources/hms/hms-test/hms_check_pod_status_ncn-resources_remote-resources.sh cray-capmc\u0026#39;... (11:40:27) Running \u0026#39;kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39;\u0026#39;... (11:40:27) Running \u0026#39;curl -k -i -s -S -d grant_type=client_credentials -d client_id=admin-client -d client_secret=${CLIENT_SECRET} https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token\u0026#39;... (11:40:28) Testing \u0026#39;curl -k -i -s -S -o /tmp/capmc_smoke_test_out-${DATETIME}.${RAND}.curl${NUM}.tmp -X POST -d \u0026#39;{}\u0026#39; -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api-gw-service-nmn.local/apis/capmc/capmc/v1/get_power_cap_capabilities\u0026#39;... HTTP/2 503 ERROR: \u0026#39;curl -k -i -s -S -o /tmp/capmc_smoke_test_out-${DATETIME}.${RAND}.curl${NUM}.tmp -X POST -d \u0026#39;{}\u0026#39; -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api-gw-service-nmn.local/apis/capmc/capmc/v1/get_power_cap_capabilities\u0026#39; did not return \u0026#34;200\u0026#34; or \u0026#34;204\u0026#34; status code as expected MAIN_ERRORS=1 FAIL: capmc_smoke_test ran with failures cleaning up... \u0026#39;/opt/cray/tests/ncn-smoke/hms/hms-capmc/capmc_smoke_test_ncn-smoke.sh\u0026#39; exited with status code: 1 HMS functional tests The HMS functional tests consist of Tavern-based API tests for HMS services that are written in YAML and execute within hms-pytest containers on the NCNs that are spun up using podman. The functional tests are more rigorous than the smoke tests and verify the behavior of HMS service APIs in greater detail. The hms_run_ct_functional_tests_ncn-resources.sh wrapper script checks for executable files in the HMS functional test directory on the NCN and runs all tests found in succession.\nncn-mw# /opt/cray/tests/ncn-resources/hms/hms-test/hms_run_ct_functional_tests_ncn-resources.sh Initial output will resemble the following:\nsearching for HMS CT functional tests... found 4 HMS CT functional tests... running HMS CT functional tests... A summary of the test results is printed at the bottom of the output.\nHMS functional tests ran with 1/4 failures exiting with status code: 1 The tests print the commands being executed while running. They also print the command output and status code if failures occur in order to help with debugging.\nThe following is an example of an hms-pytest container spin-up failure, which may occur if the hms-pytest image is unavailable or missing from the local image registry on the NCN:\n(20:06:04) Running \u0026#39;/usr/bin/hms-pytest --tavern-global-cfg=/opt/cray/tests/ncn-functional/hms/hms-bss/common.yaml /opt/cray/tests/ncn-functional/hms/hms-bss\u0026#39;... Trying to pull registry.local/cray/hms-pytest:2.0.0... manifest unknown: manifest unknown Error: unable to pull registry.local/cray/hms-pytest:2.0.0: Error initializing source docker://registry.local/cray/hms-pytest:2.0.0: Error reading manifest 2.0.0 in registry.local/cray/hms-pytest: manifest unknown: manifest unknown FAIL: bss_tavern_api_test ran with failures cleaning up... A summary of the test suites executed and their results is printed for each HMS service tested. Period \u0026lsquo;.\u0026rsquo; characters represent test cases that passed and letter \u0026lsquo;F\u0026rsquo; characters represent test cases that failed within each test suite.\nThe following is an example of a pytest summary table for Tavern test suites executed against a service:\n============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-6.1.2, py-1.10.0, pluggy-0.13.1 rootdir: /opt/cray/tests/ncn-functional/hms/hms-smd, configfile: pytest.ini plugins: tap-3.2, tavern-1.12.2 collected 38 items test_smd_component_endpoints_ncn-functional_remote-functional.tavern.yaml . [ 2%] ...... [ 18%] test_smd_components_ncn-functional_remote-functional.tavern.yaml F.F.... [ 36%] [ 36%] test_smd_discovery_status_ncn-functional_remote-functional.tavern.yaml . [ 39%] [ 39%] test_smd_groups_ncn-functional_remote-functional.tavern.yaml . [ 42%] test_smd_hardware_ncn-functional_remote-functional.tavern.yaml ....F [ 55%] test_smd_memberships_ncn-functional_remote-functional.tavern.yaml . [ 57%] test_smd_partitions_ncn-functional_remote-functional.tavern.yaml . [ 60%] test_smd_redfish_endpoints_ncn-functional_remote-functional.tavern.yaml . [ 63%] [ 63%] test_smd_service_endpoints_ncn-functional_remote-functional.tavern.yaml . [ 65%] ...F........ [ 97%] test_smd_state_change_notifications_ncn-functional_remote-functional.tavern.yaml . [100%] When API test failures occur, output from Tavern is printed by pytest indicating the following:\nThe Source test stage that was executing when the failure occurred which is a portion of the source code for the failed test case. The Formatted stage that was executing when the failure occurred which is a portion of the source code for the failed test case with its variables filled in with the values that were set at the time of the failure. This includes the request header, method, URL, and other options of the failed test case which is useful for attempting to reproduce the failure using the curl command. The specific Errors encountered when processing the API response that caused the failure. This is the first place to look when debugging API test failures. The following is an example Source test stage:\nSource test stage (line 179): - name: Ensure the boot script service can provide the bootscript for a given node request: url: \u0026#34;{base_url}/bss/boot/v1/bootscript?nid={nid}\u0026#34; method: GET headers: Authorization: \u0026#34;Bearer {access_token}\u0026#34; verify: !bool \u0026#34;{verify}\u0026#34; response: status_code: 200 The following is an example Formatted stage:\nFormatted stage: name: Ensure the boot script service can provide the bootscript for a given node request: headers: Authorization: Bearer \u0026lt;REDACTED\u0026gt; method: GET url: \u0026#39;https://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript?nid=None\u0026#39; verify: !bool \u0026#39;False\u0026#39; response: status_code: 200 The following is an example Errors section:\nErrors: E tavern.util.exceptions.TestFailError: Test \u0026#39;Ensure the boot script service can provide the bootscript for a given node\u0026#39; failed: - Status code was 400, expected 200: {\u0026#34;type\u0026#34;: \u0026#34;about:blank\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Bad Request\u0026#34;, \u0026#34;detail\u0026#34;: \u0026#34;Need a mac=, name=, or nid= parameter\u0026#34;, \u0026#34;status\u0026#34;: 400} Additional troubleshooting This section provides guidance for handling specific HMS health check failures that may occur.\nrun_hms_ct_tests.sh This script runs the suite of HMS CT tests.\nsmd_tavern_api_test_ncn-functional.sh This script executes the tests for Hardware State Manager (HSM).\ntest_smd_components_ncn-functional_remote-functional.tavern.yaml and test_smd_hardware_ncn-functional_remote-functional.tavern.yaml These tests require compute nodes to be discovered in HSM.\nThe following is an example of a failed test execution due to no discovered compute nodes in HSM:\nrunning \u0026#39;/opt/cray/tests/ncn-functional/hms/hms-smd/smd_tavern_api_test_ncn-functional.sh\u0026#39;... ... ============================= test session starts ============================== platform linux -- Python 3.8.10, pytest-6.1.2, py-1.11.0, pluggy-0.13.1 cachedir: /var/tmp/.pytest_cache rootdir: /opt/cray/tests/ncn-functional/hms/hms-smd, configfile: pytest.ini plugins: tavern-1.12.2, tap-3.3 collected 37 items ... opt/cray/tests/ncn-functional/hms/hms-smd/test_smd_components_ncn-functional_remote-functional.tavern.yaml F [ 31%] ... opt/cray/tests/ncn-functional/hms/hms-smd/test_smd_hardware_ncn-functional_remote-functional.tavern.yaml . [ 50%] ... =================================== FAILURES =================================== _ opt/cray/tests/ncn-functional/hms/hms-smd/test_smd_components_ncn-functional_remote-functional.tavern.yaml::Ensure that we can conduct a variety of queries on the Components collection _ ... ------------------------------ Captured log call ------------------------------- WARNING tavern.util.dict_util:dict_util.py:46 Formatting \u0026#39;xname\u0026#39; will result in it being coerced to a string (it is a \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt;) ... _ opt/cray/tests/ncn-functional/hms/hms-smd/test_smd_hardware_ncn-functional_remote-functional.tavern.yaml::Query the Hardware collection for Node information _ ... Errors: E tavern.util.exceptions.TestFailError: Test \u0026#39;Retrieve the hardware information for a given node xname from the Hardware collection\u0026#39; failed: - Status code was 404, expected 200: {\u0026#34;type\u0026#34;: \u0026#34;about:blank\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Not Found\u0026#34;, \u0026#34;detail\u0026#34;: \u0026#34;no such xname.\u0026#34;, \u0026#34;status\u0026#34;: 404} ... ------------------------------ Captured log call ------------------------------- WARNING tavern.util.dict_util:dict_util.py:46 Formatting \u0026#39;node_xname\u0026#39; will result in it being coerced to a string (it is a \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt;) ... =========================== short test summary info ============================ FAILED opt/cray/tests/ncn-functional/hms/hms-smd/test_smd_components_ncn-functional_remote-functional.tavern.yaml::Ensure that we can conduct a variety of queries on the Components collection FAILED opt/cray/tests/ncn-functional/hms/hms-smd/test_smd_hardware_ncn-functional_remote-functional.tavern.yaml::Query the Hardware collection for Node information If these failures occur, then confirm that there are no discovered compute nodes in HSM.\nncn-mw# cray hsm state components list --type=Node --role=compute --format=json Example output:\n{ \u0026#34;Components\u0026#34;: [] } There are several reasons why there may be no discovered compute nodes in HSM.\nThe following situations do not warrant additional troubleshooting and the test failures can be safely ignored if:\nThere is no compute hardware physically connected to the system All compute hardware in the system is powered off If none of the above cases are applicable, then the failures warrant additional troubleshooting:\nRun the smd_discovery_status_test_ncn-smoke.sh script.\nncn-mw# /opt/cray/tests/ncn-smoke/hms/hms-smd/smd_discovery_status_test_ncn-smoke.sh If the script fails, this indicates a discovery issue and further troubleshooting steps to take are printed.\nOtherwise, missing compute nodes in HSM with no discovery failures may indicate a problem with a leaf-bmc switch.\nCheck to see if the leaf-bmc switch resolves using the nslookup command.\nncn-mw# nslookup \u0026lt;leaf-bmc-switch\u0026gt; Example output:\nServer: 10.92.100.225 Address: 10.92.100.225#53 Name: sw-leaf-bmc-001.nmn Address: 10.252.0.4 Verify connectivity to the leaf-bmc switch.\nncn-mw# ssh admin@\u0026lt;leaf-bmc-switch\u0026gt; Example output:\nssh: connect to host sw-leaf-bmc-001 port 22: Connection timed out Restoring connectivity, resolving configuration issues, or restarting the relevant ports on the leaf-bmc switch should allow the compute hardware to issue DHCP requests and be discovered successfully.\ntest_components.tavern.yaml These tests include checks for healthy node states and flags in HSM.\nHardware problems may cause Warning flags to be set for nodes in HSM.\nThe following is an example of a failed test execution due to an unexpected flag set for a node in HSM:\nRunning tavern tests... ============================= test session starts ============================== platform linux -- Python 3.9.16, pytest-7.1.2, pluggy-1.0.0 -- /usr/bin/python3 cachedir: .pytest_cache rootdir: /src/app, configfile: pytest.ini plugins: allure-pytest-2.12.0, tavern-1.23.1 collecting ... collected 37 items ... test_components.tavern.yaml::Ensure that we can conduct a query for all Nodes in the Component collection FAILED [ 21%] ... Errors: E tavern.util.exceptions.TestFailError: Test \u0026#39;Verify the expected response fields for all Nodes\u0026#39; failed: - Error calling validate function \u0026#39;\u0026lt;function validate_pykwalify at 0x7f26a6e13820\u0026gt;\u0026#39;: Traceback (most recent call last): File \u0026#34;/usr/lib/python3.9/site-packages/tavern/schemas/files.py\u0026#34;, line 106, in verify_generic verifier.validate() File \u0026#34;/usr/lib/python3.9/site-packages/pykwalify/core.py\u0026#34;, line 194, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Enum \u0026#39;Warning\u0026#39; does not exist. Path: \u0026#39;/Components/7/Flag\u0026#39; Enum: [\u0026#39;OK\u0026#39;].: Path: \u0026#39;/\u0026#39;\u0026gt; ... =========================== short test summary info ============================ FAILED api/1-non-disruptive/test_components.tavern.yaml::Ensure that we can conduct a query for all Nodes in the Component collection ======================== 1 failed, 36 passed in 47.99s ========================= Test failures due to flags other than OK set for nodes in HSM do not prevent CSM installations or upgrades from proceeding. It is safe to postpone the investigation and resolution of these failures until after the CSM installation or upgrade has completed.\nsmd_discovery_status_test_ncn-smoke.sh This test verifies that the system hardware has been discovered successfully.\nThe following is an example of a failed test execution:\nRunning smd_discovery_status_test... (22:19:34) Running \u0026#39;kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39;\u0026#39;... (22:19:34) Running \u0026#39;curl -k -i -s -S -d grant_type=client_credentials -d client_id=admin-client -d client_secret=\u0026lt;REDACTED\u0026gt; https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token\u0026#39;... (22:19:35) Testing \u0026#39;curl -s -k -H \u0026#34;Authorization: Bearer \u0026lt;REDACTED\u0026gt;\u0026#34; https://api-gw-service-nmn.local/apis/smd/hsm/v1/Inventory/RedfishEndpoints\u0026#39;... (22:19:35) Processing response with: \u0026#39;jq \u0026#39;.RedfishEndpoints[] | { ID: .ID, LastDiscoveryStatus: .DiscoveryInfo.LastDiscoveryStatus}\u0026#39; -c | sort -V | jq -c\u0026#39;... (19:06:02) Verifying endpoint discovery statuses... {\u0026#34;ID\u0026#34;:\u0026#34;x3000c0s1b0\u0026#34;,\u0026#34;LastDiscoveryStatus\u0026#34;:\u0026#34;HTTPsGetFailed\u0026#34;} {\u0026#34;ID\u0026#34;:\u0026#34;x3000c0s9b0\u0026#34;,\u0026#34;LastDiscoveryStatus\u0026#34;:\u0026#34;ChildVerificationFailed\u0026#34;} {\u0026#34;ID\u0026#34;:\u0026#34;x3000c0s19b999\u0026#34;,\u0026#34;LastDiscoveryStatus\u0026#34;:\u0026#34;HTTPsGetFailed\u0026#34;} {\u0026#34;ID\u0026#34;:\u0026#34;x3000c0s27b0\u0026#34;,\u0026#34;LastDiscoveryStatus\u0026#34;:\u0026#34;ChildVerificationFailed\u0026#34;} FAIL: smd_discovery_status_test found 4 endpoints that failed discovery, maximum allowable is 1 \u0026#39;/opt/cray/tests/ncn-smoke/hms/hms-smd/smd_discovery_status_test_ncn-smoke.sh\u0026#39; exited with status code: 1 The expected state of LastDiscoveryStatus is DiscoverOK for all endpoints with the exception of the BMC for ncn-m001, which is not normally connected to the site network and expected to be HTTPsGetFailed. If the test fails because of two or more endpoints not having been discovered successfully, then take the following additional steps in order to determine the cause of the failure:\nHTTPsGetFailed Check to see if the failed component name (xname) resolves using the nslookup command.\nIf not, then the problem may be a DNS issue.\nncn-mw# nslookup \u0026lt;xname\u0026gt; Check to see if the failed component name (xname) responds to the ping command.\nIf not, then the problem may be a network or hardware issue.\nncn-mw# ping -c 1 \u0026lt;xname\u0026gt; Check to see if the failed component name (xname) responds to a Redfish query.\nIf not, then the problem may be a credentials issue. Use the password set in the REDS sealed secret.\nncn-mw# curl -s -k -u root:\u0026lt;password\u0026gt; https://\u0026lt;xname\u0026gt;/redfish/v1/Managers | jq If discovery failures for Gigabyte CMCs with component names (xnames) of the form xXc0sSb999 occur, then verify that the root service account is configured for the CMC and add it if needed by following the steps outlined in Add Root Service Account for Gigabyte Controllers.\nIf discovery failures for HPE PDUs with component names (xnames) of the form xXmM occur, this may indicate that configuration steps have not yet been executed which are required for the PDUs to be discovered. Refer to HPE PDU Admin Procedures for additional configuration for this type of PDU. The steps to run will depend on if the PDU has been set up yet, and whether or not an upgrade or fresh install of CSM is being performed.\nChildVerificationFailed Check the SMD logs to determine the cause of the bad Redfish path encountered during discovery.\nGet the SMD pod names.\nncn-mw# kubectl -n services get pods -l app.kubernetes.io/name=cray-smd Example output:\nNAME READY STATUS RESTARTS AGE cray-smd-5b9d574756-9b2lj 2/2 Running 0 24d cray-smd-5b9d574756-bnztf 2/2 Running 0 24d cray-smd-5b9d574756-hhc5p 2/2 Running 0 24d Get the logs from each of the SMD pods.\nncn-mw# kubectl -n services logs \u0026lt;cray-smd-pod1\u0026gt; cray-smd \u0026gt; smd_pod1_logs ncn-mw# kubectl -n services logs \u0026lt;cray-smd-pod2\u0026gt; cray-smd \u0026gt; smd_pod2_logs ncn-mw# kubectl -n services logs \u0026lt;cray-smd-pod3\u0026gt; cray-smd \u0026gt; smd_pod3_logs DiscoveryStarted The endpoint is in the process of being inventoried by Hardware State Manager (HSM). Wait for the current discovery operation to end which should result in a new LastDiscoveryStatus state being set for the endpoint.\nUse the following command to check the current discovery status of the endpoint:\nncn-mw# cray hsm inventory redfishEndpoints describe \u0026lt;xname\u0026gt; Install blocking vs. Non-blocking failures The HMS health checks include tests for multiple types of system components, some of which are critical for the installation of the system, while others are not.\nThe following types of HMS test failures should be considered blocking for system installations:\nHMS service pods not running HMS service APIs unreachable through the API Gateway or Cray CLI Failures related to HMS discovery (for example: unreachable BMCs, unresponsive controller hardware, or no Redfish connectivity) The following types of HMS test failures should not be considered blocking for system installations:\nFailures because of hardware issues on individual nodes (alerts or warning flags set in HSM) It is safe to postpone the investigation and resolution of non-blocking failures until after the CSM installation or upgrade has completed.\nKnown issues This section outlines known issues that cause HMS health check failures. Some of these issues have been fixed in CSM-1.2 but may still be encountered on CSM-1.2 systems that have been upgraded from a previous release.\nWarning flags incorrectly set in HSM for Mountain BMCs BMCs set to On state in HSM ComponentEndpoints of Redfish subtype AuxiliaryController in HSM ComponentEndpoints with EthernetInterfaces named DE* in HSM Custom Roles and SubRoles for Components in HSM Resolved backup pod issues causing smoke test failures Warning flags incorrectly set in HSM for Mountain BMCs The HMS functional tests include a check for unexpected flags that may be set in Hardware State Manager (HSM) for the BMCs on the system. There is a known issue that can cause Warning flags to be incorrectly set in HSM for Mountain BMCs and result in test failures.\nThe following HMS functional test may fail due to this issue:\ntest_smd_components_ncn-functional_remote-functional.tavern.yaml The symptom of this issue is the test fails with error messages about Warning flags being set on one or more BMCs. It may look similar to the following in the test output:\n=================================== FAILURES =================================== _ /opt/cray/tests/ncn-functional/hms/hms-smd/test_smd_components_ncn-functional_remote-functional.tavern.yaml::Ensure that we can conduct a query for all Node BMCs in the Component collection _ Errors: E tavern.util.exceptions.TestFailError: Test \u0026#39;Verify the expected response fields for all NodeBMCs\u0026#39; failed: - Error calling validate function \u0026#39;\u0026lt;function validate_pykwalify at 0x7f44666179d0\u0026gt;\u0026#39;: Traceback (most recent call last): File \u0026#34;/usr/lib/python3.8/site-packages/tavern/schemas/files.py\u0026#34;, line 106, in verify_generic verifier.validate() File \u0026#34;/usr/lib/python3.8/site-packages/pykwalify/core.py\u0026#34;, line 166, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Enum \u0026#39;Warning\u0026#39; does not exist. Path: \u0026#39;/Components/9/Flag\u0026#39;. - Enum \u0026#39;Warning\u0026#39; does not exist. Path: \u0026#39;/Components/10/Flag\u0026#39;. - Enum \u0026#39;Warning\u0026#39; does not exist. Path: \u0026#39;/Components/11/Flag\u0026#39;. - Enum \u0026#39;Warning\u0026#39; does not exist. Path: \u0026#39;/Components/12/Flag\u0026#39;. - Enum \u0026#39;Warning\u0026#39; does not exist. Path: \u0026#39;/Components/13/Flag\u0026#39;. - Enum \u0026#39;Warning\u0026#39; does not exist. Path: \u0026#39;/Components/14/Flag\u0026#39;.: Path: \u0026#39;/\u0026#39;\u0026gt; If this failure is encountered, then perform the following steps:\nRetrieve the component names (xnames) of all Mountain BMCs with Warning flags set in HSM.\nncn-mw# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api-gw-service-nmn.local/apis/smd/hsm/v1/State/Components?Type=NodeBMC\\\u0026amp;Class=Mountain\\\u0026amp;Flag=Warning | jq \u0026#39;.Components[] | { ID: .ID, Flag: .Flag, Class: .Class }\u0026#39; -c | sort -V | jq -c Example output:\n{\u0026#34;ID\u0026#34;:\u0026#34;x5000c1s0b0\u0026#34;,\u0026#34;Flag\u0026#34;:\u0026#34;Warning\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;Mountain\u0026#34;} {\u0026#34;ID\u0026#34;:\u0026#34;x5000c1s0b1\u0026#34;,\u0026#34;Flag\u0026#34;:\u0026#34;Warning\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;Mountain\u0026#34;} {\u0026#34;ID\u0026#34;:\u0026#34;x5000c1s1b0\u0026#34;,\u0026#34;Flag\u0026#34;:\u0026#34;Warning\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;Mountain\u0026#34;} {\u0026#34;ID\u0026#34;:\u0026#34;x5000c1s1b1\u0026#34;,\u0026#34;Flag\u0026#34;:\u0026#34;Warning\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;Mountain\u0026#34;} {\u0026#34;ID\u0026#34;:\u0026#34;x5000c1s2b0\u0026#34;,\u0026#34;Flag\u0026#34;:\u0026#34;Warning\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;Mountain\u0026#34;} {\u0026#34;ID\u0026#34;:\u0026#34;x5000c1s2b1\u0026#34;,\u0026#34;Flag\u0026#34;:\u0026#34;Warning\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;Mountain\u0026#34;} For each Mountain BMC xname, check its Redfish BMC Manager status:\nncn-mw# curl -s -k -u root:${BMC_PASSWORD} https://x5000c1s0b0/redfish/v1/Managers/BMC | jq \u0026#39;.Status\u0026#39; Example output:\n{ \u0026#34;Health\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;State\u0026#34;: \u0026#34;Online\u0026#34; } Test failures and HSM Warning flags for Mountain BMCs with the Redfish BMC Manager status shown above can be safely ignored.\nBMCs set to On state in HSM The following HMS functional test may fail due to a known issue because of CMMs setting BMC states to On instead of Ready in HSM:\ntest_smd_components_ncn-functional_remote-functional.tavern.yaml This issue looks similar to the following in the test output:\nTraceback (most recent call last): verifier.validate() File \u0026#34;/usr/lib/python3.8/site-packages/pykwalify/core.py\u0026#34;, line 166, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Enum \u0026#39;On\u0026#39; does not exist. Path: \u0026#39;/Components/9/State\u0026#39;. - Enum \u0026#39;On\u0026#39; does not exist. Path: \u0026#39;/Components/10/State\u0026#39;.: Path: \u0026#39;/\u0026#39;\u0026gt; Failures of this test caused by BMCs in the On state can be safely ignored.\nComponentEndpoints of Redfish subtype AuxiliaryController in HSM The following HMS functional test may fail due to a known issue because of ComponentEndpoints of Redfish subtype AuxiliaryController in HSM:\ntest_smd_component_endpoints_ncn-functional_remote-functional.tavern.yaml This issue looks similar to the following in the test output:\nTraceback (most recent call last): File \u0026#34;/usr/lib/python3.8/site-packages/tavern/schemas/files.py\u0026#34;, line 106, in verify_generic verifier.validate() File \u0026#34;/usr/lib/python3.8/site-packages/pykwalify/core.py\u0026#34;, line 166, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Enum \u0026#39;AuxiliaryController\u0026#39; does not exist. Path: \u0026#39;/ComponentEndpoints/32/RedfishSubtype\u0026#39;. - Enum \u0026#39;AuxiliaryController\u0026#39; does not exist. Path: \u0026#39;/ComponentEndpoints/83/RedfishSubtype\u0026#39;. - Enum \u0026#39;AuxiliaryController\u0026#39; does not exist. Path: \u0026#39;/ComponentEndpoints/92/RedfishSubtype\u0026#39;. - Enum \u0026#39;AuxiliaryController\u0026#39; does not exist. Path: \u0026#39;/ComponentEndpoints/106/RedfishSubtype\u0026#39;. - Enum \u0026#39;AuxiliaryController\u0026#39; does not exist. Path: \u0026#39;/ComponentEndpoints/126/RedfishSubtype\u0026#39;.: Path: \u0026#39;/\u0026#39;\u0026gt; Failures of this test caused by AuxiliaryController endpoints for Cassini mezzanine cards can be safely ignored.\nComponentEndpoints with EthernetInterfaces named DE* in HSM There is a known issue that causes the HMS functional test to fail when HSM ComponentEndpoints have EthernetInterfaces with names of the format DE*.\ntest_smd_component_endpoints_ncn-functional_remote-functional.tavern.yaml This issue looks similar to the following in the test output:\nTraceback (most recent call last): File \u0026#34;/usr/lib/python3.8/site-packages/tavern/schemas/files.py\u0026#34;, line 106, in verify_generic verifier.validate() File \u0026#34;/usr/lib/python3.8/site-packages/pykwalify/core.py\u0026#34;, line 166, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Value \u0026#39;DE07A000\u0026#39; does not match pattern \u0026#39;^$|[0-9]+|HPCNet[0-9]+|ManagementEthernet\u0026#39;. Path: \u0026#39;/ComponentEndpoints/34/RedfishSystemInfo/EthernetNICInfo/0/RedfishId\u0026#39;. - Value \u0026#39;DE07A001\u0026#39; does not match pattern \u0026#39;^$|[0-9]+|HPCNet[0-9]+|ManagementEthernet\u0026#39;. Path: \u0026#39;/ComponentEndpoints/34/RedfishSystemInfo/EthernetNICInfo/1/RedfishId\u0026#39;.: Path: \u0026#39;/\u0026#39;\u0026gt; Failures of this test caused by EthernetInterface IDs of the form DE* can be safely ignored.\nThis issue may be remediated by rediscovering the BMCs associated with these EthernetInterfaces.\nncn-mw# cray hsm inventory discover create --xnames \u0026lt;xname\u0026gt; Custom Roles and SubRoles for Components in HSM The following HMS functional test may fail due to a known issue because of Components with custom Roles or SubRoles set in HSM:\ntest_smd_components_ncn-functional_remote-functional.tavern.yaml This issue looks similar to the following in the test output:\nTraceback (most recent call last): File \u0026#34;/usr/lib/python3.8/site-packages/tavern/schemas/files.py\u0026#34;, line 106, in verify_generic verifier.validate() File \u0026#34;/usr/lib/python3.8/site-packages/pykwalify/core.py\u0026#34;, line 166, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Enum \u0026#39;DVS\u0026#39; does not exist. Path: \u0026#39;/Components/7/SubRole\u0026#39;. - Enum \u0026#39;VizNode\u0026#39; does not exist. Path: \u0026#39;/Components/20/SubRole\u0026#39;. - Enum \u0026#39;CrayDataServices\u0026#39; does not exist. Path: \u0026#39;/Components/147/SubRole\u0026#39;. - Enum \u0026#39;MI\u0026#39; does not exist. Path: \u0026#39;/Components/165/SubRole\u0026#39;. - Enum \u0026#39;NearNodeTier0\u0026#39; does not exist. Path: \u0026#39;/Components/198/SubRole\u0026#39;. - Enum \u0026#39;DataMovers\u0026#39; does not exist. Path: \u0026#39;/Components/1499/SubRole\u0026#39;.: Path: \u0026#39;/\u0026#39;\u0026gt; Failures of this test caused by custom Component Roles or SubRoles can be safely ignored.\nResolved backup pod issues causing smoke test failures The following HMS smoke test may fail due to backup pods in bad states in Kubernetes:\nsls_smoke_test_ncn-smoke.sh This issue looks similar to the following in the test output:\nrunning \u0026#39;/opt/cray/tests/ncn-smoke/hms/hms-sls/sls_smoke_test_ncn-smoke.sh\u0026#39;... Running sls_smoke_test... (07:17:52) Running \u0026#39;/opt/cray/tests/ncn-resources/hms/hms-test/hms_check_pod_status_ncn-resources_remote-resources.sh cray-sls\u0026#39;... services cray-sls-58cfdb7c46-r7bgz 2/2 Running 0 8h services cray-sls-58cfdb7c46-rb9gb 2/2 Running 0 8h services cray-sls-58cfdb7c46-xrlfl 2/2 Running 0 8h services cray-sls-init-load-2mbq5 0/2 Completed 0 8h services cray-sls-postgres-0 3/3 Running 0 8h services cray-sls-postgres-1 3/3 Running 0 8h services cray-sls-postgres-2 3/3 Running 0 8h services cray-sls-postgresql-db-backup-1665702600-b54xt 0/1 Error 0 8h services cray-sls-postgresql-db-backup-1665702600-l8pnn 0/1 Error 0 8h services cray-sls-postgresql-db-backup-1665702600-rcmzz 0/1 Error 0 8h services cray-sls-postgresql-db-backup-1665702600-rfgfh 0/1 Error 0 8h services cray-sls-postgresql-db-backup-1665702600-vzts9 0/1 Error 0 8h services cray-sls-postgresql-db-backup-1665702600-wfxjs 0/1 Completed 0 7h57m services cray-sls-wait-for-postgres-1-pc62h 0/3 Completed 0 8h cray-sls-58cfdb7c46-r7bgz pod status: Running cray-sls-58cfdb7c46-rb9gb pod status: Running cray-sls-58cfdb7c46-xrlfl pod status: Running cray-sls-init-load-2mbq5 pod status: Completed cray-sls-postgres-0 pod status: Running cray-sls-postgres-1 pod status: Running cray-sls-postgres-2 pod status: Running cray-sls-postgresql-db-backup-1665702600-b54xt pod status: Error Unexpected pod status \u0026#39;Error\u0026#39;, expected one of: Running Completed Run \u0026#39;kubectl -n services describe pod cray-sls-postgresql-db-backup-1665702600-b54xt\u0026#39; to continue troubleshooting ERROR: \u0026#39;/opt/cray/tests/ncn-resources/hms/hms-test/hms_check_pod_status_ncn-resources_remote-resources.sh cray-sls\u0026#39; failed with error code: 1 FAIL: sls_smoke_test ran with failures Failures of this test caused by backup pods in bad states can be safely ignored if a more recent backup of the same type has completed successfully.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/sls_not_working_during_node_rebuild/",
	"title": "SLS Not Working During Node Rebuild",
	"tags": [],
	"description": "",
	"content": "SLS Not Working During Node Rebuild During some node rebuilds (including those that happen during Stage 1 and Stage 2 of the CSM upgrade process), the SLS Postgres database gets into a bad state, causing SLS to become unhealthy. This page outlines how to detect if this has happened and provides a remediation procedure.\nNote: If encountering this during a CSM upgrade, then at this point of the upgrade process, the system has not yet upgraded the CSM services themselves. Because of that, the documentation for the source CSM version still applies, and this page includes links for both the current CSM version (1.2) and for the previous CSM version (1.0).\nDetection This procedure can be run on any master or worker NCN (unless it is the node being rebuilt).\nGet a token to use for API requests to SLS.\nncn-mw# TOKEN=$(\\ set -o pipefail secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \u0026amp;\u0026amp; curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=\u0026#34;$secret\u0026#34; \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) ; [[ -n $TOKEN ]] \u0026amp;\u0026amp; echo \u0026#34;Token obtained\u0026#34; || echo \u0026#34;Error getting token\u0026#34; Expected output:\nToken obtained Perform basic SLS health check.\nncn-mw# curl -iskH \u0026#34;Authorization: Bearer $TOKEN\u0026#34; https://api-gw-service-nmn.local/apis/sls/v1/health ; echo Example output if SLS is healthy:\nHTTP/2 200 date: Fri, 17 Jun 2022 16:23:22 GMT content-length: 58 content-type: text/plain; charset=utf-8 x-envoy-upstream-service-time: 4 server: istio-envoy {\u0026#34;Vault\u0026#34;:\u0026#34;Enabled and initialized\u0026#34;,\u0026#34;DBConnection\u0026#34;:\u0026#34;Ready\u0026#34;} Note that the first line of expected output includes 200 as the status code of the response. If that is not the case, or if other errors are seen, proceed to Remediation.\nPerform a basic SLS liveness check.\nncn-mw# curl -iskH \u0026#34;Authorization: Bearer $TOKEN\u0026#34; https://api-gw-service-nmn.local/apis/sls/v1/liveness ; echo Example output if SLS is functioning:\nHTTP/2 204 date: Fri, 17 Jun 2022 16:25:26 GMT x-envoy-upstream-service-time: 3 server: istio-envoy As with the previous command, validate that the status code on the first line matches the expected output (204 in this case). If a different status code is returned, or other errors are seen, proceed to Remediation.\nPerform a basic SLS query.\nThis query lists all nodes in the system with the Management role.\nncn-mw# curl -skH \u0026#34;Authorization: Bearer $TOKEN\u0026#34; https://api-gw-service-nmn.local/apis/sls/v1/search/hardware?extra_properties.Role=Management | jq Example output if SLS is working:\n[ { \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s1b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s1b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;LastUpdated\u0026#34;: 1654191069, \u0026#34;LastUpdatedTime\u0026#34;: \u0026#34;2022-06-02 17:31:09.155802 +0000 +0000\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Aliases\u0026#34;: [ \u0026#34;ncn-m001\u0026#34; ], \u0026#34;NID\u0026#34;: 100010, \u0026#34;Role\u0026#34;: \u0026#34;Management\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;Master\u0026#34; } }, [\u0026#34;...omitting many lines for readability...\u0026#34;], { \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s7b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;LastUpdated\u0026#34;: 1654191069, \u0026#34;LastUpdatedTime\u0026#34;: \u0026#34;2022-06-02 17:31:09.155802 +0000 +0000\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Aliases\u0026#34;: [ \u0026#34;ncn-w004\u0026#34; ], \u0026#34;NID\u0026#34;: 100004, \u0026#34;Role\u0026#34;: \u0026#34;Management\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;Worker\u0026#34; } } ] If the query fails, proceed to Remediation.\nIf all of the API calls provide expected output, then SLS appears to be working properly. In that case, the rest of this page should be skipped.\nRemediation If a check in the previous section indicates that SLS is not working properly, then check the status of the SLS Postgres database.\nncn-mw# kubectl get postgresql cray-sls-postgres -n services Expected output if the database is healthy:\nNAME TEAM VERSION PODS VOLUME CPU-REQUEST MEMORY-REQUEST AGE STATUS cray-sls-postgres cray-sls 11 3 1Gi 157d Running If the STATUS is SyncFailed: If currently doing a CSM upgrade, then see (CSM 1.0) Postgres status SyncFailed. Otherwise, see Postgres status SyncFailed. Otherwise, see the standard Postgres troubleshooting procedures for further avenues of investigation. If currently doing a CSM upgrade, then see (CSM 1.0) Troubleshoot Postgres Database. Otherwise, see Troubleshoot Postgres Database. "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/ceph_daemon_memory_profiling/",
	"title": "Ceph Daemon Memory Profiling",
	"tags": [],
	"description": "",
	"content": "Ceph Daemon Memory Profiling This procedure is meant as an instructional guide to provide information back to HPE Cray to assist in tuning and troubleshooting exercises.\nProcedure NOTE: For this example, a ceph-mon process on ncn-s001 is used.\nIdentify the process and location of the daemon to profile.\nncn-s00(1/2/3)# ceph orch ps --daemon_type mon Example output:\nNAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID mon.ncn-s001 ncn-s001 running (1h) 60s ago 1h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c bcca26f69191 mon.ncn-s002 ncn-s002 running (1h) 61s ago 1h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 43c8472465b2 mon.ncn-s003 ncn-s003 running (1h) 61s ago 1h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 7aa1b1f19a00 SSH to the node where the process is running if it is different from the current node.\nStart the profiler.\nncn-s001# ceph tell mon.ncn-s001 heap start_profiler A message stating \u0026ldquo;mon.ncn-s001 started profiler\u0026rdquo; will be returned.\nDump stats. This does NOT require the profiler to be running.\nncn-s001# ceph tell mon.ncn-s001 heap stats Example output:\nmon.ncn-s001 tcmalloc heap stats:------------------------------------------------ MALLOC: 972461744 ( 927.4 MiB) Bytes in use by application MALLOC: + 0 ( 0.0 MiB) Bytes in page heap freelist MALLOC: + 8804424 ( 8.4 MiB) Bytes in central cache freelist MALLOC: + 3706880 ( 3.5 MiB) Bytes in transfer cache freelist MALLOC: + 25649416 ( 24.5 MiB) Bytes in thread cache freelists MALLOC: + 5636096 ( 5.4 MiB) Bytes in malloc metadata MALLOC: ------------ MALLOC: = 1016258560 ( 969.2 MiB) Actual memory used (physical + swap) MALLOC: + 189841408 ( 181.0 MiB) Bytes released to OS (aka unmapped) MALLOC: ------------ MALLOC: = 1206099968 ( 1150.2 MiB) Virtual address space used MALLOC: MALLOC: 14833 Spans in use MALLOC: 25 Thread heaps in use MALLOC: 8192 Tcmalloc page size ------------------------------------------------ Call ReleaseFreeMemory() to release freelist memory to the OS (via madvise()). Bytes released to the OS take up virtual address space but no physical memory. Dump heap. This requires the profiler to be running.\n# ceph tell mon.ncn-s001 heap dump Example output:\nmon.ncn-s001 dumping heap profile now. ------------------------------------------------ MALLOC: 976849264 ( 931.6 MiB) Bytes in use by application MALLOC: + 0 ( 0.0 MiB) Bytes in page heap freelist MALLOC: + 8819048 ( 8.4 MiB) Bytes in central cache freelist MALLOC: + 3617280 ( 3.4 MiB) Bytes in transfer cache freelist MALLOC: + 25531176 ( 24.3 MiB) Bytes in thread cache freelists MALLOC: + 5636096 ( 5.4 MiB) Bytes in malloc metadata MALLOC: ------------ MALLOC: = 1020452864 ( 973.2 MiB) Actual memory used (physical + swap) MALLOC: + 185647104 ( 177.0 MiB) Bytes released to OS (aka unmapped) MALLOC: ------------ MALLOC: = 1206099968 ( 1150.2 MiB) Virtual address space used MALLOC: MALLOC: 14834 Spans in use MALLOC: 25 Thread heaps in use MALLOC: 8192 Tcmalloc page size ------------------------------------------------ Call ReleaseFreeMemory() to release freelist memory to the OS (via madvise()). Bytes released to the OS take up virtual address space but no physical memory. Release memory.\nncn-s001# ceph tell mon.ncn-s001 heap release A message stating \u0026ldquo;mon.ncn-s001 releasing free RAM back to system\u0026rdquo; will be returned.\nStop the profiler.\nncn-s001# ceph tell mon.ncn-s001 heap stop_profiler A message stating \u0026quot; mon.ncn-s001 stopped profiler\u0026quot; will be returned.\n"
},
{
	"uri": "/docs-csm/en-12/operations/system_layout_service/dump_sls_information/",
	"title": "Dump SLS Information",
	"tags": [],
	"description": "",
	"content": "Dump SLS Information Perform a dump of the System Layout Service (SLS) database.\nThis procedure preserves the information stored in SLS when backing up or reinstalling the system. It will create the file, sls_dump.json, in the current directory.\nPrerequisites This procedure requires administrative privileges.\nProcedure Use the get_token function to retrieve a token to validate requests to the API gateway.\nncn-m001# function get_token () { curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39; } Perform the SLS dump.\nThe SLS dump will be stored in the sls_dump.json file.\nncn-m001# curl -X GET \\ https://api-gw-service-nmn.local/apis/sls/v1/dumpstate \\ -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; \u0026gt; sls_dump.json "
},
{
	"uri": "/docs-csm/en-12/operations/system_management_health/remove_kiali/",
	"title": "Remove Kiali",
	"tags": [],
	"description": "",
	"content": "Remove Kiali If not planning to use Kiali, then Kiali may be removed for CVE (Common Vulnerabilities and Exposures) remediation. NOTE: The removal will not persist after a CSM upgrade, so the removal procedure must be rerun after CSM upgrades.\nProcedure Delete kiali deployment and cray-kiali chart.\nncn-m# kubectl delete deployment kiali -n istio-system ncn-m# helm uninstall cray-kiali -n operators --keep-history Remove cray-kiali chart from loftsman-platform ConfigMap.\nncn-m# kubectl get configmap -n loftsman loftsman-platform -o json | jq -r \u0026#39;.data.\u0026#34;manifest.yaml\u0026#34;\u0026#39; \u0026gt; platform.yaml ncn-m# cp platform.yaml platform.yaml.saved ncn-m# yq d -i platform.yaml \u0026#39;spec.charts(name==\u0026#39;\u0026#34;cray-kiali\u0026#34;\u0026#39;)\u0026#39; ncn-m# kubectl create configmap -n loftsman loftsman-platform --from-file=manifest.yaml=platform.yaml \\ --dry-run=client -o yaml | kubectl apply -f - "
},
{
	"uri": "/docs-csm/en-12/operations/spire/update_spire_intermediate_ca_certificate/",
	"title": "Update Spire Intermediate CA Certificate",
	"tags": [],
	"description": "",
	"content": "Update Spire Intermediate CA Certificate Prior to CSM 1.2.5 there is no mechanism to automatically update the spire intermediate CA certificate before it expires. This certificate expires after one year. Administrators will want to keep track of when this certificate expires and manually update the certificate before it expires.\nObtain the Expiration Date for the Spire Intermediate CA To obtain the expiration date of the Spire intermediate CA certificate, run the following command on a node that has access to kubectl (such as ncn-m001):\nncn# kubectl get secret -n spire spire.spire.ca-tls -o json | jq -r \u0026#39;.data.\u0026#34;tls.crt\u0026#34; | @base64d\u0026#39; | openssl x509 -noout -enddate Replace the Spire Intermediate CA Certificate Delete the secret that stores the certificate.\nncn# SPIRE_INTERMEDIATE_JOB=$(kubectl get job -n vault -o name| grep \u0026#39;spire-intermediate\u0026#39; | tail -n1) ncn# kubectl get secrets -n spire spire.spire.ca-tls -o yaml \u0026gt; spire.spire.ca-tls.yaml.bak ncn# kubectl delete secret -n spire spire.spire.ca-tls Re-run the job that obtains the secret and creates the certificate.\nncn# kubectl get -n vault \u0026#34;$SPIRE_INTERMEDIATE_JOB\u0026#34; -o json | jq \u0026#39;del(.spec.selector,.spec.template.metadata.labels)\u0026#39; | kubectl replace --force -f - After the spire.spire.ca-tls secret in the spire namespace has been repopulated, roll the spire-server to make sure all of them pick up the new CA.\nncn# kubectl rollout restart -n spire statefulset spire-server Any spire-agent in the CrashLoopBackOff state should come back into a Running state the next time they are started. If you do not wish to wait for them to be restarted automatically, then you can delete the spire-agent pod, which will cause a new one to start up in its place.\nEnable the NCNs to rejoin Spire.\nncn# kubectl rollout restart -n spire daemonset request-ncn-join-token Re-run the command to get the certificate\u0026rsquo;s expiration date to verify that it has been updated.\nncn# kubectl get secret -n spire spire.spire.ca-tls -o json | jq -r \u0026#39;.data.\u0026#34;tls.crt\u0026#34; | @base64d\u0026#39; | openssl x509 -noout -enddate "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/audit_logs/",
	"title": "Audit Logs",
	"tags": [],
	"description": "",
	"content": "Audit Logs Overview Audit logs are used to monitor the system and search for suspicious behavior. Host and Kubernetes API audit logging can be enabled to produce extra audit logs for analysis. Enabling audit logging is optional. If enabled it generates some load and data on the non-compute nodes (NCNs).\nBy default, host and Kubernetes API audit logging are not enabled. It is not required for both to be enabled or disabled at the same time.\nHost audit logs are stored in the /var/log/audit/HostOS directory on each NCN. Host audit logging uses a maximum of 60GB on each NCN when using log rotation settings. The log rotation settings are enabled after editing the CSI settings and rebooting the NCNs.\nThe Kubernetes API audit logs are stored in the /var/log/audit/kl8s/apiserver directory on each master NCN. Kubernetes API audit logging uses a maximum of 1GB on each master NCN when using log rotation settings.\nEnable or disable audit logging for host and Kubernetes APIs During CSM install, from the PIT node Use csi tool Edit system_config.yaml After CSM install Use csi tool from ncn-m001 Modify BSS from a Kubernetes NCN Verify that audit logging is enabled Restart NCNs to make settings take effect Enable or disable audit logging for host and Kubernetes APIs The method for updating the audit log settings varies depending on the state of the system.\nSelect one of the following options to enable audit logging based on the installation status of the system. For each of the following options, only enable the desired level of audit logging. It is not required to enable both.\nDuring CSM install, from the PIT node After CSM install Enable audit logging during CSM install, from the PIT node NOTE: This step needs to happen at the same time that csi config init is normally run during the install.\nTo update the audit log settings during the installation, use one of the following options:\nUse csi tool Edit system_config.yaml Use csi tool During the installation, audit logging is enabled or disabled by modifying the CSI settings. To enable or disable audit logging, use the following flags with the csi config init command. For more information on using flags, see csi config init -h.\nHost audit logging\nSet to true to enable host logging or to false to disable host logging.\npit# csi config init --ncn-mgmt-node-auditing-enabled=true [other config init options] Kubernetes API audit logging\nSet to true to enable Kubernetes API logging or to false to disable Kubernetes API logging.\npit# csi config init --k8s-api-auditing-enabled=true [other config init options] Edit system_config.yaml Adjust the audit log settings by editing the system_config.yaml file.\nView the current settings with the following command:\npit# cd /var/www/ephemeral/prep pit# grep audit system_config.yaml Example output:\nk8s-api-auditing-enabled: false ncn-mgmt-node-auditing-enabled: false Enable audit logging after CSM install Choose either of the following options:\nUse csi tool from ncn-m001 Modify BSS from a Kubernetes NCN Use csi tool from ncn-m001 Enable audit logging using the csi tool on ncn-m001.\nInstall the csi tool on ncn-m001, if it is not already installed.\nIf the csi command is not installed on ncn-m001, then locate the cray-site-init RPM on ncn-m001 and install it.\nncn-m001# find /mnt/pitdata -name cray-site-init* ncn-m001# rpm -Uvh --force \u0026lt;rpm file path\u0026gt; It is also possible to enable audit logging without csi. See Modify BSS from a Kubernetes NCN.\nEnable audit logging.\nHost audit logging\nncn-m001# TOKEN=$(curl -k -s -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) \\ csi handoff bss-update-param --limit \u0026lt;mgmt-node-xname\u0026gt; --set ncn-mgmt-node-auditing-enabled=true Kubernetes API audit logging\nncn-m001# TOKEN=$(curl -k -s -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) \\ csi handoff bss-update-param --limit \u0026lt;mgmt-node-xname\u0026gt; --set k8s-api-auditing-enabled=true Modify BSS from a Kubernetes NCN Enable audit logging with Boot Script Service (BSS) parameters.\nConfigure the Cray CLI, if it is not already.\nSee Configure the Cray CLI.\nEnable audit logging.\nHost audit logging\nncn-mw# XNAME=\u0026lt;node_xname\u0026gt; ncn-mw# PARAMS=$(cray bss bootparameters list --hosts \u0026#34;${XNAME}\u0026#34; --format json | jq \u0026#39;.[] |.\u0026#34;params\u0026#34;\u0026#39; | tr -d \\\u0026#34;) ncn-mw# PARAMS=\u0026#34;$PARAMS ncn-mgmt-node-auditing-enabled=true\u0026#34; ncn-mw# cray bss bootparameters update --hosts \u0026#34;${XNAME}\u0026#34; --params \u0026#34;${PARAMS}\u0026#34; Kubernetes API audit logging\nncn-mw# XNAME=\u0026lt;node_xname\u0026gt; ncn-mw# PARAMS=$(cray bss bootparameters list --hosts \u0026#34;${XNAME}\u0026#34; --format json | jq \u0026#39;.[] |.\u0026#34;params\u0026#34;\u0026#39; | tr -d \\\u0026#34;) ncn-mw# PARAMS=\u0026#34;$PARAMS k8s-api-auditing-enabled=true\u0026#34; ncn-mw# cray bss bootparameters update --hosts \u0026#34;${XNAME}\u0026#34; --params \u0026#34;${PARAMS}\u0026#34; Verify that audit logging is enabled Changes made post-install will not be reflected until after the NCN is rebooted.\nHost audit logging\nncn# craysys metadata get ncn-mgmt-node-auditing-enabled Kubernetes API audit logging\nncn# craysys metadata get k8s-api-auditing-enabled Restart NCNs in order to make settings take effect This section is only necessary if the audit logging settings were changed after the CSM install. If the desired audit logging settings were made as part of the CSM install, then skip this section.\nRestart each NCN to apply the new settings after the CSI setting is changed.\nFollow the Reboot NCNs procedure.\n"
},
{
	"uri": "/docs-csm/en-12/operations/resiliency/resiliency_testing_procedure/",
	"title": "Resiliency Testing Procedure",
	"tags": [],
	"description": "",
	"content": "Resiliency Testing Procedure This document and the procedures contained within it are for the purposes of communicating the kind of testing done by the internal Cray System Management (CSM) team to ensure a basic level of system resiliency in the event of the loss of a single non-compute node (NCN).\nIt is assumed that some procedures are already known by admins and thus does not go into great detail or attempt to encompass every command necessary for execution. It is intended to be higher level guidance (with some command examples) to inform internal users and customers about our process.\nPrepare for resiliency testing Establish system health before beginning Monitor for changes Launch a non-interactive batch job Shut down an NCN Conduct testing Power on the downed NCN Execute post-boot health checks Prepare for resiliency testing Confirm the component name (xname) mapping for each node on the system by running the /opt/cray/platform-utils/ncnGetXnames.sh script on each node.\nVerify that metal.no-wipe=1 is set for each of the NCNs using output from running the ncnGetXnames.sh script.\nEnsure the user account in use is an authorized user on the Cray CLI.\nLog in as a user account where the credentials are known:\nncn-mw# export CRAY_CONFIG_DIR=$(mktemp -d); echo $CRAY_CONFIG_DIR; cray init --configuration default --hostname https://api-gw-service-nmn.local Then, validate the authorization by executing the cray uas list command, for example. For more information, see the Validate UAI Creation section of Validate CSM Health.\nVerify that kubectl get nodes reports all master and worker nodes are Ready.\nncn-mw# kubectl get nodes -o wide Get a current list of pods that have a status of anything other than Running or Completed. Investigate any of concern. Save the list of pods for comparison once resiliency testing is completed and the system has been restored.\nncn-mw# kubectl get pods -o wide -A | grep -Ev \u0026#39;Running|Completed\u0026#39; Note which pods are running on an NCN that will be taken down (as well as the total number of pods running). The following is an example that shows the listing of pods running on ncn-w001:\nncn-mw# kubectl get pods -o wide -A | grep ncn-w001 | awk \u0026#39;{print $2}\u0026#39; Note that the above would only apply to Kubernetes nodes, such as master and worker nodes.\nVerify ipmitool can report power status for the NCN to be shut down.\nlinux# ipmitool -I lanplus -U root -P \u0026lt;password\u0026gt; -H \u0026lt;ncn-node-name\u0026gt; chassis power status If ncn-m001 is the node to be brought down, then note that it has the external network connection. Therefore it is important to establish that ipmitool commands are able to be run from a node external to the system, in order to get the power status of ncn-m001.\nIf ncn-m001 is the node to be brought down, then establish Customer Access Network (CAN) links to bypass ncn-m001 (because it will be down) in order to enable an external connection to one of the other master NCNs before, during, and after ncn-m001 is brought down.\nVerify Boot Orchestration Service (BOS) templates and create a new one if needed (to be set-up for booting a specific compute nodes after the targeted NCN has been shutdown).\nBefore shutting down the NCN and beginning resiliency testing, verify that compute nodes identified for reboot validation can be successfully rebooted and configured.\nTo see a list of BOS templates that exist on the system:\nncn-mw# cray bos sessiontemplate list For more information regarding management of BOS session templates, refer to Manage a Session Template.\nIf a UAN is present on the system, log onto it and verify that the workload manager (WLM) is configured by running a command.\nThe following is an example for Slurm:\nuan# srun -N 4 hostname | sort Establish system health before beginning In order to ensure that the system is healthy before taking an NCN node down, run the Platform Health Checks section of Validate CSM Health.\nIf health issues are noted, it is best to address those before proceeding with the resiliency testing procedure. If it is believed (in the case of an internal Cray-HPE testing environment) that the issue is known/understood and will not impact the testing to be performed, then those health issues just need to be noted (so that it does not appear that they were caused by inducing the fault, in this case, powering off the NCN). There is an optional section of the platform health validation that deals with using the System Management monitoring tools to survey system health. If that optional validation is included, note that the Prometheus alert manager may show various alerts that would not prevent or block moving forward with this testing. For more information about Prometheus alerts (and some that can be safely ignored), see Troubleshooting Prometheus Alerts.\nPart of the data being returned via execution of the Platform Health Checks includes patronictl information for each Postgres cluster. Each of the Postgres clusters has a leader pod, and in the case of a resiliency test that involves bringing an NCN worker node down, it may be useful to take note of the Postgres clusters that have their leader pods running on the NCN worker targeted for shutdown. The postgres-operator should handle re-establishment of a leader on another pod running in the cluster, but it is worth taking note of where leader re-elections are expected to occur so special attention can be given to those Postgres clusters.\nncn-mw# /opt/cray/platform-utils/ncnPostgresHealthChecks.sh Monitor for changes In order to keep watch on various items during and after the fault has been introduced (in this case, the shutdown of a single NCN), the steps listed below can help give insight into changing health conditions.\nSet up a watch command to repeatedly run with the Cray CLI (that will hit the service API) to ensure that critical services can ride through a fault. Note that there is not more than a window of 5-10 minutes where a service would intermittently fail to respond.\nIn the examples below, the CLI commands are checking the BOS and CPS APIs. It may be desired to choose additional Cray CLI commands to run in this manner. The ultimate proof of system resiliency lies in the ability to perform system level use cases and to, further, prove that can be done at scale. If there are errors being returned consistently (and without recovery) with respect to these commands, then it is likely that business critical use cases (that utilize the same APIs) will also fail.\nIt may be useful to reference instructions for Configuring the Cray CLI.\nncn-mw# watch -n 5 \u0026#34;date; cray cps contents\u0026#34; ncn-mw# watch -n 5 \u0026#34;date; cray bos session list\u0026#34; Monitor Ceph health, in a window, during and after a single NCN is taken down.\nncn-mw# watch -n 5 \u0026#34;date; ceph -s\u0026#34; Identify when pods on a downed master or worker NCN are no longer responding.\nThis takes around 5-6 minutes, and Kubernetes will begin terminating pods so that new pods to replace them can start-up on another NCN. Pods that had been running on the downed NCN will remain in Terminated state until the NCN is back up. Pods that need to start-up on other nodes will be Pending until they start-up. Some pods that have anti-affinity configurations or that run as daemonsets will not be able to start up on another NCN. Those pods will remain in Pending state until the NCN is back up.\nFinally, it is helpful to have a window tracking the list of pods that are not in Completed or Running state to be able to determine how that list is changing once the NCN is downed and pods begin shifting around. This step offers a view of what is going on at the time that the NCN is brought down and once Kubernetes detects an issue and begins remediation. It is not so important to capture everything that is happening during this step. It may be helpful for debugging. The output of these windows/commands becomes more interesting once the NCN is down for a period of time and then it is brought back up. At that point, the expectation is that everything can recover.\nRun the following commands in separate windows:\nncn-mw# watch -n 5 \u0026#34;date; kubectl get pods -o wide -A | grep Termin\u0026#34; ncn-mw# watch -n 10 \u0026#34;date; kubectl get pods -o wide -A | grep Pending\u0026#34; ncn-mw# watch -n 5 \u0026#34;date; kubectl get pods -o wide -A | grep -v Completed | grep -v Running\u0026#34; Detect the change in state of the various Postgres instances running.\nRun the following in a separate window:\nncn-mw# watch -n 30 \u0026#34;date; kubectl get postgresql -A\u0026#34; If Postgres reports a status that deviates from Running, that would require further investigation and possibly remediation via Troubleshooting the Postgres Database.\nLaunch a non-interactive batch job The purpose of this procedure is to launch a non-interactive, long-running batch job across computes via a UAI (or the UAN, if present) in order to ensure that even though the UAI pod used to launch the job is running on the worker NCN being taken down, it will start up on another worker NCN (once Kubernetes begins terminating pods).\nAdditionally, it is important to verify that the batch job continued to run, uninterrupted through that process. If the target NCN for shutdown is not a worker node (where a UAI would be running), then there is no need to pay attention to the steps, below, that discuss ensuring the UAI can only be created on the NCN worker node that is targeted for shutdown. If executing a shut down of a master or storage NCN, the procedure can begin at creating a UAI. It is still good to ensure that non-interactive batch jobs are uninterrupted, even with the UAI they are launched from is not being disrupted.\nLaunch on a UAI Create a UAI.\nTo create a UAI, see the Validate UAI Creation section of Validate CSM Health and/or Create a UAI.\nIf the target node for shutdown is a worker NCN, force the UAI to be created on target worker NCN with the following steps:\nCreate labels to ensure that UAI pods will only be able to start-up on the target worker NCN.\nIn this example, ncn-w002 is the target node for shutdown on a system with only three NCN worker nodes.\nncn-mw# kubectl label node ncn-w001 uas=False --overwrite ncn-mw# kubectl label node ncn-w003 uas=False --overwrite If successful, output similar to node/ncn-w00X labeled will be returned.\nVerify the labels were applied successfully.\nncn-mw# kubectl get nodes -l uas=False Example output:\nNAME STATUS ROLES AGE VERSION ncn-w001 Ready \u0026lt;none\u0026gt; 4d19h v1.18.2 ncn-w003 Ready \u0026lt;none\u0026gt; 4d19h v1.18.2 Remove the labels set above after the UAI has been created.\nIMPORTANT: After a UAI has been created, labels must be cleared or else when Kubernetes terminates the running UAI on the NCN being shut down, it will not be able to reschedule the UAI on another pod.\nncn-mw# kubectl label node ncn-w001 uas- ncn-mw# kubectl label node ncn-w003 uas- Verify that WLM is configured with the appropriate workload manager within the created UAI.\nVerify the connection string of the created UAI.\nncn-mw# cray uas list --format toml Example output:\n[[results]] uai_age = \u0026#34;2m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.103.8.170\u0026#34; uai_host = \u0026#34;ncn-w002\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-cos-2.1.70:latest\u0026#34; uai_ip = \u0026#34;10.103.8.170\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-f8fa541f\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; Log in to the created UAI.\nFor example:\nncn-mw# ssh vers@10.103.8.170 Verify the configuration of Slurm, for example, within the UAI:\nuai# srun -N 4 hostname | sort Example output:\nnid000001 nid000002 nid000003 nid000004 Copy an MPI application source and WLM batch job files to the UAI.\nCompile an MPI application with the UAI. Launch application as batch job (not interactive) on compute node(s) that have not been designated, already, for reboots once an NCN is shut down.\nVerify that batch job is running and that application output is streaming to a file. Streaming output will be used to verify that the batch job is still running during resiliency testing. A batch job, when submitted, will designate a log file location. This log file can be accessed to be able to verify that the batch job is continuing to run after an NCN is brought down and once it is back online. Additionally, the squeue command can be used to verify that the job continues to run (for Slurm).\nDelete the UAI session when all of the testing is complete.\nncn-mw# cray uas delete --uai-list uai-vers-f8fa541f Launch on a UAN Log in to the UAN and verify that a WLM has been properly configured.\nIn this example, Slurm will be used.\nuan# srun -N 4 hostname | sort Example output:\nnid000001 nid000002 nid000003 nid000004 Copy an MPI application source and WLM batch job files to UAN.\nCompile an MPI application within the UAN. Launch the application as interactive on compute node(s)that have not been designated, already, for either reboots (once an NCN is shut down) or that are not already running an MPI job via a UAI.\nVerify that the job launched on the UAN is running and that application output is streaming to a file. Streaming output will be used to verify that the batch job is still running during resiliency testing. A batch job, when submitted, will designate a log file location. This log file can be accessed to be able to verify that the batch job is continuing to run after an NCN is brought down and once it is back online. Additionally, the squeue command can be used to verify that the job continues to run (for Slurm).\nShut down an NCN Establish a console session to the NCN targeted for shutdown by executing the steps in Establish a Serial Connection to NCNs.\nLog onto the target node and execute /sbin/shutdown -h 0.\nTake note of the timestamp of the power off in the target node\u0026rsquo;s console output.\nOnce the target node is reported as being powered off, verify that the node\u0026rsquo;s power status with the ipmitool is reported as off.\nlinux# ipmitool -I lanplus -U root -P \u0026lt;password\u0026gt; -H \u0026lt;ncn-node-name\u0026gt; chassis power status NOTE In previous releases, an ipmitool command has been used to simply yank the power to an NCN. There have been times where this resulted in a longer recovery procedure under Shasta 1.5 (mostly due to issues with getting nodes physically booted up again), so the preference has been to simply use the shutdown command.\nIf the NCN shutdown is a master or worker node, within 5-6 minutes of the node being shut down, Kubernetes will begin reporting Terminating pods on the target node and start rescheduling pods to other NCN nodes. New pending pods will be created for pods that can not be relocated off of the NCN shut down. Pods reported as Terminating will remain in that state until the NCN has been powered back up.\nTake note of changes in the data being reported out of the many monitoring windows that were set-up in a previous step.\nConduct testing After the target NCN was shut down, assuming the command line windows that were set up for ensuring API responsiveness are not encountering persistent failures, the next step will be to use a BOS template to boot a pre-designated set of compute nodes. The timing of this test is recommended to be around 10 minutes after the NCN has gone down. That should give ample time for Kubernetes to have terminated pods on the downed node (in the case of a master or worker NCN) and for them to have been rescheduled and in a healthy state on another NCN. Going too much earlier than 10 minutes runs the risk that there are still some critical pods that are settling out to reach a healthy state.\nReboot a pre-designated set of compute nodes and watch the reboot.\nUse BOS to reboot the designated compute nodes.\nncn-mw# cray bos session create --template-uuid boot-nids-1-4 --operation reboot Issuing this reboot command will output a Boot Orchestration Agent (BOA) jobId, which can be used to find the new BOA pod that has been created for the boot. Then, the logs can be tailed to watch the compute boot proceed.\nFind the BOA job name using the returned BOA jobID.\nncn-mw# kubectl get pods -o wide -A | grep \u0026lt;boa-job-id\u0026gt; Watch the progress of the reboot of the compute nodes.\nncn-mw# kubectl logs -n services \u0026lt;boa-job-pod-name\u0026gt; -c boa -f Failures or a timeout being reached in either the boot or CFS (post-boot configuration) phase will need investigation. For more information around accessing logs for the BOS operations, see Check the Progress of BOS Session Operations.\nIf the target node for shutdown was a worker NCN, verify that the UAI launched on that node still exists. It should be running on another worker NCN.\nAny prior SSH session established with the UAI while it was running on the downed NCN worker node will be unresponsive. A new SSH session will need to be established once the UAI pods has been successfully relocated to another worker NCN. Log back into the UAI and verify that the WLM batch job is still running and streaming output. The log file created with the kick-off of the batch job should still be accessible and the squeue command can be used to verify that the job continues to run (for Slurm). If the WLM batch job was launched on a UAN, log back into it and verify that the batch job is still running and streaming output via the log file created with the batch job and/or the squeue command (if Slurm is used as the WLM).\nVerify that new WLM jobs can be started on a compute node after the NCN is down (either via a UAI or the UAN node).\nLook for any pods that are in a state other than Running, Completed, Pending, or Terminating:\nncn-mw# kubectl get pods -o wide -A | grep -Ev \u0026#34;Running|Completed|Pending|Termin\u0026#34; Compare what comes up in this list to the pod list that was collected before. If there are new pods that are in status ImagePullBackOff or CrashLoopBackOff, a kubectl describe as well as kubectl logs command should be run against them to collect additional data about what happened. Obviously, if there were pods in a bad state before the procedure started, then it should not be expected that bringing one of the NCNs down is going to fix that.\nIgnore anything that was already in a bad state before (that was deemed to be okay). It is also worth taking note of any pods in a bad state at this stage as this should be checked again after bringing the NCN back up - to see if those pods remain in a bad state or if they are cleared. Noting behaviors, collecting logs, and opening tickets throughout this process is recommended when behavior occurs that is not expected. When we see an issue that has not been encountered before, it may not be immediately clear if code changes/regressions are at fault or if it is simply an intermittent/timing kind of issue that has not previously surfaced. The recommendation at that point, given time/resources is to repeat the test to gain a sense of the repeatability of the behavior (in the case that the issue is not directly tied to a code change).\nAdditionally, it is as important to understand (and document) any work-around procedures needed to fix issues encountered. In addition to filing a bug for a permanent fix, workaround documentation can be very useful when written up - for both internal and external customers to access.\nPower on the downed NCN Use the ipmitool command to power up the NCN.\nIt will take several minutes for the NCN to reboot. Progress can be monitored over the connected serial console session. Wait to begin execution of the next steps until after it can be determined that the NCN has booted up and is back at the login prompt (when viewing the serial console log).\nlinux# ipmitool -I lanplus -U root -P \u0026lt;password\u0026gt; -H \u0026lt;hostname\u0026gt; chassis power on #example hostname is ncn-w003-mgmt Check the following depending on the NCN type powered on:\nIf the NCN being powered on is a master or worker, verify that Terminating pods on that NCN clear up. It may take several minutes. Watch the command prompt, previously set-up, that is displaying the Terminating pod list. If the NCN being powered on is a storage node, wait for Ceph to recover and again report a HEALTH_OK status. It may take several minutes for Ceph to resolve clock skew. This can be noted in the previously set-up window to watch Ceph status. Check that pod statuses have returned to the state that they were in at the beginning of this procedure, paying particular attention to any pods that were previously noted to be in a bad state while the NCN was down. Additionally, there is no concern if pods that were in a bad state at the beginning of the procedure, are still in a bad state. What is important to note is anything that is different from either the beginning of the test or from the time that the NCN was down.\nExecute post-boot health checks Re-run the Platform Health Checks section of Validate CSM Health noting any output that indicates output is not as expected.\nEnsure that after a downed NCN worker node (can ignore if not a worker node) has been powered up, a new UAI can be created on that NCN. It may be necessary to label the nodes again, to ensure the UAI gets created on the worker node that was just powered on. Refer to the section above for Launch a Non-Interactive Batch Job for the procedure.\nIMPORTANT: Do not forget to remove the labels after the UAI has been created. Once the UAI has been created, log into it and ensure a new workload manager job can be launched.\nEnsure tickets have been opened for any unexpected behavior along with associated logs and notes on workarounds, if any were executed.\n"
},
{
	"uri": "/docs-csm/en-12/operations/power_management/power_off_the_external_lustre_file_system/",
	"title": "Power Off the External Lustre File System",
	"tags": [],
	"description": "",
	"content": "Power Off the External Lustre File System General procedure for powering off an external ClusterStor system.\nUse this procedure as a general guide to power off an external ClusterStor system. Refer to the detailed procedures in the appropriate ClusterStor administration guide:\nTitle Model ClusterStor E1000 Administration Guide 4.2 - S-2758 ClusterStor E1000 ClusterStor Administration Guide 3.4 - S-2756 ClusterStor L300/L300N ClusterStor Administration Guide - S-2755 Legacy ClusterStor Procedure SSH to the primary MGMT node as admin.\nncn# remote$ ssh -l admin cls01234n00.us.cray.com Change to root user.\nadmin@n000$ sudo su – Collect status information for the system before shutdown.\nn000# cscli csinfo n000# cscli show_nodes n000# cscli fs_info n000# crm_mon -1r Check resources before unmounting the file system.\nn000# ssh cls01234n002 crm_mon -r1 | grep fsys n000# ssh cls01234n004 crm_mon -r1 | grep fsys n000# ssh cls01234n006 crm_mon -r1 | grep fsys n000# ssh cls01234n008 crm_mon -r1 | grep fsys n000# ssh cls01234n010 crm_mon -r1 | grep fsys n000# ssh cls01234n012 crm_mon -r1 | grep fsys . . . Stop the Lustre file system.\n[n000]# cscli unmount -f FILESYSTEM_NAME Verify that resources have been stopped by running the following on all even-numbered nodes:\n[n000]# ssh NODENAME crm_mon -r1 | grep fsys Example output:\ncls01234n006_md0-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n006_md1-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n006_md2-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n006_md3-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n006_md4-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n006_md5-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n006_md6-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n006_md7-fsys (ocf::heartbeat:XYMNTR): Stopped SSH to the MGS node.\nMGS# ssh MGS_NODE To determine if Resource Group md65-group is stopped, use the crm_mon utility to monitor the status of the MGS and MDS nodes.\nShows MGS and MDS nodes in a partial stopped state.\n[MGS]# crm_mon -1r | grep fsys Example output:\ncls01234n003_md66-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n003_md65-fsys (ocf::heartbeat:XYMNTR): Started If the output above shows a partial stopped state (Stopped and Started), issue the stop_xyraid command and verify that the node is stopped.\n[MGS]# stop_xyraid nodename_md65-group [MGS]# crm\\_mon -1r | grep fsys Example output:\ncls01234n003_md66-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n003_md65-fsys (ocf::heartbeat:XYMNTR): Stopped Exit the MGS node.\nMGS# exit Power off the non-MGMT diskless nodes.\nCheck power state of all non-MGMT nodes and list the node hostnames (in this example cls01234n[02-15]) before power off.\nn000# pm –q Example output:\non: cls01234n[000-001] on: cls01234n[002-015] unknown: Power off all non-MGMT nodes.\n[n00]$ cscli power_manage -n cls01234n[02-15] --power-off Check the power status of the nodes.\nn000# pm –q Example output:\non: cls01234n[000-001] off: cls01234n[002-015] unknown: Repeat the previous step until all non-MGMT nodes are powered off.\nFrom the primary MGMT node, power off the MGMT nodes.\nn000# cscli power_manage -n cls01234n[000-001] --power-off Shut down the primary management node.\nn000# shutdown -h now Next step Return to System Power Off Procedures and continue with next step.\n"
},
{
	"uri": "/docs-csm/en-12/operations/package_repository_management/nexus_space_cleanup/",
	"title": "Nexus Space Cleanup",
	"tags": [],
	"description": "",
	"content": "Nexus Space Cleanup Nexus stores all data in a Kubernetes Persistent Volume Claim (PVC) called nexus-data. If this PVC fills up, then it will enter a read-only state. This read-only state causes issues for Nexus as well as all the services that rely on Nexus. There is no automatic cleanup of old data from Nexus.\nDuring the install of any CSM version, a large amount data is added to Nexus. If there has not been a manual cleanup of old files in Nexus, then there is likely to be insufficient space for the next version of CSM to be installed.\nNOTE: The HPE Cray EX System Software release has around 130 Gigabytes of space needed in Nexus.\nThis page outlines the procedure to manually cleanup Nexus, in order to ensure that there is sufficient free space for a CSM upgrade.\nCleanup of data not being used Cleanup of old installs Remove data marked for deletion Check PVC size Increase PVC size Cleanup of data not being used Any data in Nexus that is not currently being used can be deleted to make space for the upgrade. Data includes repositories or artifacts. To delete a blob store, refer to Remove Data Marked For Deletion. This can include data added after the previous install, or data that was added during an install that is no longer needed. If there is anything in Nexus that is no longer needed, then it is recommended to delete that first, before taking any further steps.\nMarking a repository for deletion A repository can be deleted from the web UI or from the CLI.\nTo delete a repository from the CLI, reference Manage Repositories with Nexus.\nTo delete a repository out of the web UI:\nRefer to Access Nexus with the Web UI.\nAuthenticate to the web UI.\nBrowse to the Nexus admin section.\nClick on Repository.\nClick on Repositories.\nSelect any repository to delete, where the Type is hosted and a new screen with a button that says Delete repository should appear at the top. Clicking that button will mark the repository for deletion.\nMarking a artifact for deletion An artifact can be easily deleted from the web UI. To delete an artifact from a repository:\nAuthenticate in the web UI. Click on Browse. Click on the repository you want to remove an asset from. Click on the asset itself (either the folder, component, or asset). Select delete on the right side menu. Cleanup of old installs There is no documented list of files that are known to be older versions. If the system has been installed and around for an extended period of time, or been put through multiple upgrades, then submit a help request in order to determine what can be safely deleted from Nexus. Before submitting the help request, run a script to gather data about the system. This script outputs all of the blob stores and repositories in Nexus, and a list of what blob store each repository point to.\nThe script can be run on any Kubernetes master management node where the latest CSM documentation is installed. See Check for latest documentation.\nncn-m# /usr/share/doc/csm/scripts/nexus-space-usage.sh Remove data marked for deletion After something is deleted in Nexus, the data is marked for deletion but not removed from the disk. This can lead to a situation where there is no space left on the disk but no way to see what is taking up space. After removing data from Nexus a clean up task needs to be created to clear data from the disk. After data (a repository or just some artifacts) are deleted from Nexus create a task in Nexus by going to the Nexus admin section then clicking on System, then Tasks. The task that should be created is called \u0026ldquo;Admin - Compact blob store\u0026rdquo;.\nThe task has three required fields. A name, a blob store, and task frequency. The name can be anything as long as it is not the same as another task. The blob store should be a blob store that has some data marked for deletion. The task frequency should be manual.\nOnce the task is created, then the task must be run. Click on the task name from the task list, then click on run. It should take anywhere from 30 seconds to 5 minutes depending on how much is marked for deletion. Check in the blob store section under repositories to make sure the total size decreases. If the task takes zero seconds to run, then either no data is marked for deletion or the task needs to be run again. After the blob store has zero total size the blob store can be deleted.\nNOTE: If the task fails to run and never cleans any data, and the nexus-data PVC is completely full the PVC will need to be grown to run the task.\nCheck PVC size To check the amount of storage used, available, and total in the PVC, run the following command:\nncn-m# kubectl exec -n nexus deploy/nexus -c nexus -- df -Ph /nexus-data | grep \u0026#39;/nexus-data\u0026#39; | awk \u0026#39;{print (\u0026#34;Used:\u0026#34;, $3, \u0026#34;Available:\u0026#34;, $4, \u0026#34;Total Size:\u0026#34;, $2)}\u0026#39; Increase PVC size If no other methods of cleaning work, then submit a help request to expand the nexus-data PVC. Expanding the PVC will allow the upgrade to proceed. Expanding the PVC will also require future work to allow for further upgrades.\nCAUTION: This is an irreversible step and is not recommended.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/rebuild_ncns/prepare_storage_nodes/",
	"title": "Prepare Storage Nodes",
	"tags": [],
	"description": "",
	"content": "Prepare Storage Nodes Prepare a storage node before rebuilding it.\nIMPORTANT: All of the output examples may not reflect the cluster status where this operation is being performed. For example, if this is a rebuild in place, then Ceph components will not be reporting down, in contrast to a failed node rebuild.\nPrerequisites Procedure Next step Prerequisites Ensure that the latest CSM documentation RPM is installed on ncn-m001.\nSee Check for Latest Documentation.\nWhen rebuilding a node, make sure that /srv/cray/scripts/common/storage-ceph-cloudinit.sh and /srv/cray/scripts/common/pre-load-images.sh have been removed from the runcmd in BSS.\nSet node name and xname if not already set.\nncn-m001# NODE=ncn-s00n ncn-m001# XNAME=$(ssh $NODE cat /etc/cray/xname) Get the runcmd in BSS.\nncn-m001# cray bss bootparameters list --name ${XNAME} --format=json|jq -r \u0026#39;.[]|.[\u0026#34;cloud-init\u0026#34;]|.[\u0026#34;user-data\u0026#34;].runcmd\u0026#39; Expected output:\n[ \u0026#34;/srv/cray/scripts/metal/net-init.sh\u0026#34;, \u0026#34;/srv/cray/scripts/common/update_ca_certs.py\u0026#34;, \u0026#34;/srv/cray/scripts/metal/install.sh\u0026#34;, \u0026#34;/srv/cray/scripts/common/ceph-enable-services.sh\u0026#34;, \u0026#34;touch /etc/cloud/cloud-init.disabled\u0026#34; ] If /srv/cray/scripts/common/storage-ceph-cloudinit.sh or /srv/cray/scripts/common/pre-load-images.sh is in the runcmd, then it will need to be fixed using the following procedure:\nObtain an API authentication token.\nA token will need to be generated and made available as an environment variable. Refer to the Retrieve an Authentication Token procedure for more information.\nRun the following command to patch BSS.\nncn-m001# python3 /usr/share/doc/csm/scripts/patch-ceph-runcmd.py Repeat the original Cray CLI command and verify that the expected output is obtained.\nProcedure Upload Ceph container images into Nexus.\nLog into one of the first three storage NCNs.\nThis procedure must be performed on a ceph-monnode. By default these will be any of the first three storage NCNs: ncn-s001, ncn-s002, or ncn-s003\nCopy upload_ceph_images_to_nexus.sh from ncn-m001 and execute it.\nncn-s# scp ncn-m001:/usr/share/doc/csm/scripts/upload_ceph_images_to_nexus.sh /srv/cray/scripts/common/upload_ceph_images_to_nexus.sh \u0026amp;\u0026amp; \\ /srv/cray/scripts/common/upload_ceph_images_to_nexus.sh Check the status of Ceph.\nCheck the OSD status, weight, and location:\nncn-s# ceph osd tree Example output:\nID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 62.87558 root default -5 20.95853 host ncn-s001 2 ssd 3.49309 osd.2 up 1.00000 1.00000 5 ssd 3.49309 osd.5 up 1.00000 1.00000 6 ssd 3.49309 osd.6 up 1.00000 1.00000 9 ssd 3.49309 osd.9 up 1.00000 1.00000 12 ssd 3.49309 osd.12 up 1.00000 1.00000 16 ssd 3.49309 osd.16 up 1.00000 1.00000 -3 20.95853 host ncn-s002 0 ssd 3.49309 osd.0 up 1.00000 1.00000 3 ssd 3.49309 osd.3 up 1.00000 1.00000 7 ssd 3.49309 osd.7 up 1.00000 1.00000 10 ssd 3.49309 osd.10 up 1.00000 1.00000 13 ssd 3.49309 osd.13 up 1.00000 1.00000 15 ssd 3.49309 osd.15 up 1.00000 1.00000 -7 20.95853 host ncn-s003 1 ssd 3.49309 osd.1 up 1.00000 1.00000 4 ssd 3.49309 osd.4 up 1.00000 1.00000 8 ssd 3.49309 osd.8 up 1.00000 1.00000 11 ssd 3.49309 osd.11 up 1.00000 1.00000 14 ssd 3.49309 osd.14 up 1.00000 1.00000 17 ssd 3.49309 osd.17 up 1.00000 1.00000 If the node is up, then stop and disable all the Ceph services on the node being rebuilt.\nncn-s# ceph orch maintenance enter \u0026lt;storage node hostname being rebuilt\u0026gt; Example output:\nDaemons for Ceph cluster 5f79a490-c281-11ed-b6ec-fa163e741e89 stopped on host ncn-s003. Host ncn-s003 moved to maintenance mode IMPORTANT: The \u0026ndash;force flag is used to bypass warnings. These pertain to Ceph services which can handle failures, like rgw.\nIF the command returns any lines with an ALERT status then please follow the output to remedy. Typically this will be something like the active MGR process is on that node and it must be failed over first. Example:\nWARNING: Stopping 1 out of 1 daemons in Alertmanager service. Service will not be operational with no daemons left. At least 1 daemon must be running to guarantee service. ALERT: Cannot stop active Mgr daemon, Please switch active Mgrs with \u0026#39;ceph mgr fail ncn-s003.ydycwn\u0026#39; WARNING: Removing RGW daemons can cause clients to lose connectivity. In this example, the warnings for RGW and Alertmanager would be ignored by passing the --force flag. The alert for active Mgr will need to be addressed with the provided command (ceph mgr fail ncn-s003.ydycwn).\nRe-check the OSD status, weight, and location:\nncn-s# ceph osd tree Example output:\nID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 62.87558 root default -5 20.95853 host ncn-s001 2 ssd 3.49309 osd.2 up 1.00000 1.00000 5 ssd 3.49309 osd.5 up 1.00000 1.00000 6 ssd 3.49309 osd.6 up 1.00000 1.00000 9 ssd 3.49309 osd.9 up 1.00000 1.00000 12 ssd 3.49309 osd.12 up 1.00000 1.00000 16 ssd 3.49309 osd.16 up 1.00000 1.00000 -3 20.95853 host ncn-s002 0 ssd 3.49309 osd.0 up 1.00000 1.00000 3 ssd 3.49309 osd.3 up 1.00000 1.00000 7 ssd 3.49309 osd.7 up 1.00000 1.00000 10 ssd 3.49309 osd.10 up 1.00000 1.00000 13 ssd 3.49309 osd.13 up 1.00000 1.00000 15 ssd 3.49309 osd.15 up 1.00000 1.00000 -7 20.95853 host ncn-s003 1 ssd 3.49309 osd.1 down 1.00000 1.00000 4 ssd 3.49309 osd.4 down 1.00000 1.00000 8 ssd 3.49309 osd.8 down 1.00000 1.00000 11 ssd 3.49309 osd.11 down 1.00000 1.00000 14 ssd 3.49309 osd.14 down 1.00000 1.00000 17 ssd 3.49309 osd.17 down 1.00000 1.00000 Check the status of the Ceph cluster:\nncn-s# ceph -s Example output:\ncluster: id: 4c9e9d74-a208-11ed-b008-98039bb427f6 health: HEALTH_WARN 1 host is in maintenance mode \u0026lt;-------- Expect this line. 1/3 mons down, quorum ncn-s001,ncn-s002 6 osds down 1 host (6 osds) down Degraded data redundancy: 34257/102773 objects degraded (33.333%), 370 pgs degraded, 352 pgs undersized services: mon: 3 daemons, quorum ncn-s001,ncn-s002 (age 56s), out of quorum: ncn-s003 mgr: ncn-s002.amfitm(active, since 43m), standbys: ncn-s001.rytusj mds: 1/1 daemons up, 1 hot standby osd: 18 osds: 12 up (since 55s), 18 in (since 13h) rgw: 2 daemons active (2 hosts, 1 zones) data: volumes: 1/1 healthy pools: 13 pools, 553 pgs objects: 34.26k objects, 58 GiB usage: 173 GiB used, 63 TiB / 63 TiB avail pgs: 34257/102773 objects degraded (33.333%) 370 active+undersized+degraded 159 active+undersized 24 active+clean io: client: 8.7 KiB/s rd, 353 KiB/s wr, 3 op/s rd, 53 op/s wr List down Ceph OSDs.\nIMPORTANT: Before proceeding, ensure that this rebuild requires OSD wipes. Storage node rebuilds that are done on an active node do not require the OSD removal. Some examples are rebuilds to get some a custom patched image.\nThe ceph osd tree capture indicated that there are down OSDs on ncn-s003.\nncn-s# ceph osd tree down Example output:\nID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 62.87758 root default -7 20.95853 host ncn-s003 1 ssd 3.49309 osd.1 down 1.00000 1.00000 4 ssd 3.49309 osd.4 down 1.00000 1.00000 8 ssd 3.49309 osd.8 down 1.00000 1.00000 11 ssd 3.49309 osd.11 down 1.00000 1.00000 14 ssd 3.49309 osd.14 down 1.00000 1.00000 17 ssd 3.49309 osd.17 down 1.00000 1.00000 Remove the OSD references to allow the rebuild to re-use the original OSD references on the drives.\nBy default, if the OSD reference is not removed, then there will still a reference to them in the CRUSH map. This will result in OSDs that no longer exist appearing to be down.\nThe following command assumes the variables from the prerequisites section are set.\nncn-s# for osd in $(ceph osd ls-tree $NODE); do ceph osd destroy osd.$osd --force; ceph osd purge osd.$osd --force; done Example output:\ndestroyed osd.1 purged osd.1 destroyed osd.4 purged osd.4 destroyed osd.6 purged osd.6 destroyed osd.11 purged osd.11 destroyed osd.14 purged osd.14 destroyed osd.17 purged osd.17 Next step Proceed to Identify Nodes and Update Metadata or return to the main Rebuild NCNs page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_remove_replace_ncns/boot_ncn/",
	"title": "Boot NCN",
	"tags": [],
	"description": "",
	"content": "Boot NCN Description Boot a master, worker, or storage non-compute node (NCN) that is to be added to the cluster.\nProcedure Open and watch the console for the node being rebuilt Log in to a second session in order to watch the console.\nOpen this link in a new tab or page: Log in to a Node Using ConMan\nThe first session will be needed to run the commands in the following Rebuild Node steps.\nSet the PXE boot option and power on the node IMPORTANT: These commands assume that the variables from the prerequisites section have been set.\nSet the BMC variable to the hostname of the BMC of the node being rebuilt.\nIf booting ncn-m001, set this to the FQDN or IP address.\nlinux# BMC=\u0026#34;${NODE}-mgmt\u0026#34; Export the root user password of the BMC.\nread -s is used in order to prevent the password from being echoed to the screen or saved in the shell history.\nlinux# read -r -s -p \u0026#34;${BMC} root password: \u0026#34; IPMI_PASSWORD linux# export IPMI_PASSWORD Check the power status. Power the BMC off if Chassis Power is on.\nlinux# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC}\u0026#34; chassis power status linux# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC}\u0026#34; chassis power off Set the pxe efiboot option.\nlinux# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC}\u0026#34; chassis bootdev pxe options=efiboot Power on the node.\nlinux# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC}\u0026#34; chassis power on Verify that the node is on.\nEnsure that the power is reporting as on. It may take 5-10 seconds for this to update.\nlinux# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC}\u0026#34; chassis power status Observe the boot Within several minutes, the node should begin to boot. This can be viewed from the ConMan console window. Eventually, there will be a NBP file... message in the console output. This indicates that the PXE boot has started the TFTP download of the ipxe program. Later messages will appear as the Linux kernel loads and the scripts in the initrd begin to run, including cloud-init.\nWait until cloud-init displays messages similar to these on the console. This indicates that cloud-init has finished with the module called modules-final.\n[ 300.390000] cloud-init[7110]: 2022-03-16 18:30:59,449 - util.py[DEBUG]: cloud-init mode \u0026#39;modules\u0026#39; took 244.143 seconds (198.87) [ 300.390106] cloud-init[7110]: 2022-03-16 18:30:59,449 - handlers.py[DEBUG]: finish: modules-final: SUCCESS: running modules for final [ OK ] Started Execute cloud user/final scripts. [ OK ] Reached target Cloud-init target. Press enter on the console and ensure that the the login prompt includes the correct hostname of this node.\nExit the ConMan console (\u0026amp; then .).\nUse ssh to log in to the node in order to complete any remaining steps based on the node type.\nVerify that the added master or worker node has joined the cluster Skip this section if the node being added is a storage node.\nList the nodes in the Kubernetes cluster.\nncn-mw# kubectl get nodes Example output:\nNAME STATUS ROLES AGE VERSION ncn-m001 Ready master 2d7h v1.19.9 ncn-m002 Ready master 20d v1.19.9 ncn-m003 Ready master 20d v1.19.9 ncn-w001 Ready \u0026lt;none\u0026gt; 27h v1.19.9 ncn-w002 Ready \u0026lt;none\u0026gt; 20d v1.19.9 ncn-w003 Ready \u0026lt;none\u0026gt; 20d v1.19.9 ncn-w004 Ready \u0026lt;none\u0026gt; 1h v1.19.9 Set the no-wipe flag Setting the no-wipe flag safeguards against the disks being wiped when the node is rebooted.\nRun the following commands from a node that has cray CLI initialized. See Configure the Cray CLI.\nSave the current BSS boot parameters for the node.\nncn-mw# cray bss bootparameters list --name \u0026#34;${XNAME}\u0026#34; --format=json | jq .[] \u0026gt; \u0026#34;${XNAME}.json\u0026#34; Edit the XNAME.json file and set the metal.no-wipe=1 value.\nGet a token to interact with BSS using the REST API.\nncn-mw# TOKEN=$(curl -s -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Do a PUT action for the edited JSON file.\nThis command can be run from any node.\nncn-mw# curl -i -s -k -H \u0026#34;Content-Type: application/json\u0026#34; \\ -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ \u0026#34;https://api-gw-service-nmn.local/apis/bss/boot/v1/bootparameters\u0026#34; \\ -X PUT -d @\u0026#34;./${XNAME}.json\u0026#34; Verify that the cray bss bootparameters list command returns the expected information.\nExport the list from BSS to a file with a different name.\nncn-mw# cray bss bootparameters list --name \u0026#34;${XNAME}\u0026#34; --format=json |jq .[]\u0026gt; \u0026#34;${XNAME}.check.json\u0026#34; Compare the new JSON file with what was put into BSS.\nncn-mw# diff \u0026#34;${XNAME}.json\u0026#34; \u0026#34;${XNAME}.check.json\u0026#34; The command should return no output because the files should be identical.\nRun NCN personalizations using CFS as desired Run the following commands from a node that has cray CLI initialized. See Configure the Cray CLI.\nDetermine which configuration to apply to the node.\nThere are multiple ways to do this. Choose the one which best fits the situation.\nRun the following commands to list the available configurations.\nncn-mw# cray cfs configurations list --format toml Example output:\n[[results]] lastUpdated = \u0026#34;2022-03-14T20:59:44Z\u0026#34; name = \u0026#34;ncn-personalization\u0026#34; [[results.layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34; commit = \u0026#34;1dc4038615cebcfad3e8230caecc885d987e8148\u0026#34; name = \u0026#34;csm-ncn-1.6.28\u0026#34; playbook = \u0026#34;site.yml\u0026#34; Determine the configuration applied another NCN of the same type. This example checks the configuration on ncn-w002.\nncn-mw# cray cfs components describe \u0026#34;$(ssh ncn-w002 cat /etc/cray/xname)\u0026#34; --format toml Example output:\nconfigurationStatus = \u0026#34;configured\u0026#34; desiredConfig = \u0026#34;ncn-personalization\u0026#34; enabled = true errorCount = 0 id = \u0026#34;x3000c0s9b0n0\u0026#34; [[state]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34; commit = \u0026#34;1dc4038615cebcfad3e8230caecc885d987e8148\u0026#34; lastUpdated = \u0026#34;2022-03-15T15:29:20Z\u0026#34; playbook = \u0026#34;site.yml\u0026#34; sessionName = \u0026#34;batcher-5e431205-a4b4-4a2e-8be3-21cf058774cc\u0026#34; Select the appropriate configuration based on the previous step to personalize the added NCN.\nIn this example, the ncn-personalization configuration is used.\nncn-mw# cray cfs components update \u0026#34;${XNAME}\u0026#34; --desired-config ncn-personalization Wait for configurationStatus to transition from pending to configured.\nncn-mw# watch \u0026#34;cray cfs components describe \u0026#39;${XNAME}\u0026#39; --format toml\u0026#34; Example output excerpt:\nconfigurationStatus = \u0026#34;configured\u0026#34; desiredConfig = \u0026#34;ncn-personalization\u0026#34; Set BMC management roles Follow the Set BMC Management Roles procedure. This will mark the added NCN\u0026rsquo;s BMC with the Management role, making it easier to identify as a BMC that is associated with a management node. This step is needed before locking the BCM of the added NCN.\nLock the management nodes Follow the How to Lock Management Single Node procedure. The management nodes may be unlocked at this point. Locking the management nodes and their BMCs will prevent actions from FAS to update their firmware, or from CAPMC to power off or do a power reset. Doing any of these by accident will take down a management node. If the management node is a Kubernetes master or worker node, then this can have serious negative effects on system operation.\nConfigure the Cray Command Line Interface (Cray CLI) See Configure the Cray CLI for details on how to configure the Cray CLI on the added node.\nAdd storage node to the Ceph cluster Skip this section if the node being added is NOT a storage node.\nFollow Add Ceph Node to join the added storage node to the Ceph cluster.\nRestore the site link for ncn-m001 Skip this section if the node being added is NOT ncn-m001.\nRestore and verify the site link for ncn-m001.\nAccess ncn-m002 using its CMN IP address, which was recorded prior to powering down ncn-m001.\nIMPORTANT: If the vendor of the replaced master node has changed, then before the configuration is reloaded, verify that the BRIDGE_PORTS setting in /etc/sysconfig/network/ifcfg-lan0 is based on the actual NIC names for the external site interface.\nremote# ssh root@CMN_IP ncn-m002# rsync /tmp/ifcfg-lan0-m001 ncn-m001:/etc/sysconfig/network/ifcfg-lan0 ncn-m002# ssh ncn-m001 ncn-m001# wicked ifreload lan0 ncn-m001# wicked ifstatus lan0 Example output:\nlan0 up link: #30, state up, mtu 1500 type: bridge, hwaddr a4:bf:01:5a:a9:ff config: compat:suse:/etc/sysconfig/network/ifcfg-lan0 leases: ipv4 static granted addr: ipv4 172.30.52.72/20 [static] Verify that the correct information is displayed for the site link.\nRun ip a to show the lan0 IP address.\nncn-m001# ip a show lan0 Next step Proceed to Redeploy Services or return to the main Add, Remove, Replace, or Move NCNs page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/metallb_bgp/troubleshoot_services_without_an_allocated_ip_address/",
	"title": "Troubleshoot Services without an Allocated IP Address",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Services without an Allocated IP Address Check if a given service has an IP address allocated for it if the Kubernetes LoadBalancer services in the NMNLB, HMNLB, CMN, CHN, or CAN address pools are not accessible from outside the cluster.\nRegain access to Kubernetes LoadBalancer services from outside the cluster.\nPrerequisites This procedure requires administrative privileges.\nProcedure Check the status of the services with the kubectl command to see the External-IP of the service.\nIf \u0026lt;pending\u0026gt; appears in this column, the service is having a problem getting an IP address assigned from MetalLB.\nncn-w001# kubectl get service -A | grep Load Example output:\nims cray-ims-b9cdea70-223f-4968-a0f4-589518c89a80-service LoadBalancer 10.17.97.66 \u0026lt;pending\u0026gt; 22:32678/TCP 2d9h ims cray-ims-eca49ecd-5434-46b2-9a3c-f4f0467f8ecb-service LoadBalancer 10.18.171.14 \u0026lt;pending\u0026gt; 22:30821/TCP 2d5h istio-system istio-ingressgateway LoadBalancer 10.26.49.253 10.92.100.50 80:30517/TCP,443:30754/TCP 3d5h istio-system istio-ingressgateway-cmn LoadBalancer 10.28.192.172 \u0026lt;pending\u0026gt; 80:30708/TCP,443:31430/TCP 3d5h istio-system istio-ingressgateway-hmn LoadBalancer 10.17.46.139 10.94.100.1 80:32444/TCP 3d5h Check which user network is configured.\nGet a token from the NMNLB API gateway using the instructions at Retrieve an Authentication Token.\nQuery SLS for the configured user network.\ncurl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api.nmnlb.SYSTEM_DOMAIN_NAME/apis/sls/v1/dumpstate | jq -r \u0026#39;.Networks.BICAN.ExtraProperties.SystemDefaultRoute\u0026#39; If the user network is CHN, then the CAN network is not configured and you can expect to see \u0026lt;pending\u0026gt; for any of the CAN services (e.g. istio-ingressgateway-can) and you can skip the rest of the checks.\nIf the user network is CAN, then the CHN network is not configured and you can expect to see \u0026lt;pending\u0026gt; for any of the CHN services (e.g. istio-ingressgateway-chn) and you can skip the rest of the checks.\nCheck that the address pool in the annotation for the service matches one of the address pools in the MetalLB ConfigMap.\nTo view information on the service:\nncn-w001# kubectl -n istio-system describe service istio-ingressgateway-cmn Example output:\nName: istio-ingressgateway-cmn Namespace: istio-system Labels: app=istio-ingressgateway chart=cray-istio heritage=Tiller istio=ingressgateway release=cray-istio Annotations: external-dns.alpha.kubernetes.io/hostname: api.cmn.SYSTEM_DOMAIN_NAME,auth.cmn.SYSTEM_DOMAIN_NAME,nexus.cmn.SYSTEM_DOMAIN_NAME ** metallb.universe.tf/address-pool: customer-management** Selector: app=istio-ingressgateway,istio=ingressgateway,release=cray-istio Type: LoadBalancer IP: 10.28.192.172 Port: http2 80/TCP TargetPort: 80/TCP NodePort: http2 30708/TCP Endpoints: 10.39.0.5:80 Port: https 443/TCP TargetPort: 443/TCP NodePort: https 31430/TCP Endpoints: 10.39.0.5:443 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; Run the following command to view the ConfigMap. There is no customer-management address pool in the example below, indicating it has not been added yet. This is why the external IP address value is \u0026lt;pending\u0026gt;.\nncn-w001# kubectl -n metallb-system get cm metallb -o yaml Example output:\napiVersion: v1 data: config: | **address-pools:** - name: node-management protocol: layer2 addresses: - 10.92.100.0/24 - name: hardware-management protocol: layer2 addresses: - 10.94.100.0/24 - name: customer-high-speed protocol: layer2 addresses: - 169.0.100.16/28 kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;config\u0026#34;:\u0026#34;address-pools:\\n- name: node-management\\n protocol: layer2\\n addresses:\\n - 10.92.100.0/24\\n- name: hardware-management\\n protocol: layer2\\n addresses:\\n - 10.94.100.0/24\\n- name: customer-high-speed\\n protocol: layer2\\n addresses:\\n - 169.0.100.16/28\\n\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;config\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;metallb-system\u0026#34;}} creationTimestamp: \u0026#34;2020-01-09T20:33:25Z\u0026#34; name: config namespace: metallb-system resourceVersion: \u0026#34;1645\u0026#34; selfLink: /api/v1/namespaces/metallb-system/configmaps/config uid: 49967541-331f-11ea-9421-b42e993a2608 "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/cable_diagnostics/",
	"title": "Cable diagnostics",
	"tags": [],
	"description": "",
	"content": "Cable diagnostics Cable plugin collects various information from the cables attached to the fabric ports.\n\u0026ndash;get_cable_info\tGets cable info from the fabric ports. \u0026ndash;cable_info_disconnected\tGets cable info on disconnected ports (the cable is attached only to the switch port). This option is applicable with the \u0026ldquo;get-cable-info\u0026rdquo; flag.\nRelevant Configuration\nExample:\nibdiagnet --get-cable-info --cable_info_disconnected The data is dumped to the ibdiagnet2.cables file in the following format: ------------------------------------------------------- Port=1 Lid=0x00a4 GUID=0xf45214030046a0a1 Port Name=coral-ufm-001/U1/P1 ------------------------------------------------------- Vendor: Mellanox OUI: 0x2c9 PN: MCP1600-E002 SN: MT1739VS02126 Rev: A3 Length: 2 m Type: Copper cable- unequalized SupportedSpeed: SDR/DDR/QDR/FDR/EDR Temperature: N/A PowerClass: 1 NominalBitrate: 0 Gb/s CDREnableTxRx: N/A N/A InputEq: N/A OutputAmp: N/A OutputEmp: N/A FW Version: N/A Attenuation(5,7,12): 7 8 13 RX power type: OMA RX1 Power: 0.000 mW, -999.999 dBm RX2 Power: 0.000 mW, -999.999 dBm RX3 Power: 0.000 mW, -999.999 dBm RX4 Power: 0.000 mW, -999.999 dBm TX1 Bias: 0.000 mA TX2 Bias: 0.000 mA TX3 Bias: 0.000 mA TX4 Bias: 0.000 mA TX1 Power: 0.000 mW, -999.999 dBm TX2 Power: 0.000 mW, -999.999 dBm TX3 Power: 0.000 mW, -999.999 dBm TX4 Power: 0.000 mW, -999.999 dBm Expected Results\nStep 1: You can enter diagnostics mode successfully Step 2: You can test the cable and see the results in the CLI output Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/domain_name/",
	"title": "Configure Domain Name",
	"tags": [],
	"description": "",
	"content": "Configure Domain Name A domain name is a name to identify the person, group, or organization that controls the devices within an area. An example of a domain name could be us.cray.com.\nConfiguration commands Create a domain name:\nswitch(config)# domain-name NAME Show commands to validate functionality:\nswitch# show domain-name Expected results Administrators can configure the domain name The output of all show commands is correct Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/canu/initializing_canu/",
	"title": "Initializing CANU",
	"tags": [],
	"description": "",
	"content": "Initializing CANU Initialize the CSM Automatic Network Utility (CANU) in order to help create the switch configurations. CANU can automatically parse CSI output or the Shasta System Layout Service (SLS) API for switch IPv4 addresses. Using the SLS API is only possible after the CSM install has been completed at least to the point where CSM Services have been deployed. Prior to that, parsing CSI output is the only option.\nCANU output file The output file for the canu init command is set with the --out FILENAME argument.\nCSI input In order to parse CSI output, use the --csi-folder FOLDER flag to pass in the folder where the sls_input_file.json file is located.\nThe sls_input_file.json file is generally stored in one of two places, depending on how far the system is in the install process.\nWhen running off of the LiveCD, the sls_input_file.json file is normally found in the the /var/www/ephemeral/prep/SYSTEMNAME/ directory on the PIT node.\nAfter the PIT node has been redeployed, the sls_input_file.json file may be found in the /metal/bootstrap/prep/SYSTEMNAME/ directory on ncn-m001 or ncn-m003.\nTo get the switch IP addresses from CSI output, run the following command:\nlinux# canu -s 1.4 init --csi-folder /CSI/OUTPUT/FOLDER/ADDRESS --out output.txt Eight IP addresses are saved to output.txt.\nSLS API input Parsing the SLS API for IP addresses requires a valid API token. Either the token file can be passed in with the --auth-token TOKEN_FILE flag, or the token can be read from the SLS_TOKEN environment variable, if it is exported.\nThe SLS address is by default set to api-gw-service-nmn.local. If needed, a different address can be specified using the --sls-address SLS_ADDRESS flag.\nTo get the switch IP addresses from the Shasta SLS API, run the following command:\nlinux# canu -s 1.4 init --auth-token ~./config/cray/tokens/ --sls-address 1.2.3.4 --out output.txt Back to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/apply_switch_configurations/",
	"title": "Apply Switch Configurations",
	"tags": [],
	"description": "",
	"content": "Apply Switch Configurations This process is generally straightforward and requires the user to copy and paste the generated switch configuration into the terminal.\nAll ports will be shutdown before applying switch configuration. If the port is in the SHCD and being used, it will be enabled when the configuration is applied.\nThere are some caveats that are mentioned below.\nPrerequisites Switch without any configuration Wipe Management Switches Generated switch configurations Generate Switch Configuration Aruba Shutdown all ports. Use show int physical to see the range of ports.\nswitch(config)# int 1/1/1-1/1/52 switch(config-if-\u0026lt;1/1/1-1/1/52# shut Enter auto-confirm before pasting in the configuration. This will automatically accept prompts.\nswitch(config)# auto-confirm Paste in the generated configuration.\nDell Shut down all ports.\nsw-leaf-bmc-001(config)# interface range ethernet 1/1/1-1/1/52 sw-leaf-bmc-001(conf-range-eth1/1/1-1/1/52)# shut Paste in the generated configuration.\nWhen pasting in the configuration be sure that all the commands were accepted. In some cases you will need to back out of the current configuration context and back to global configuration for the commands to work as intended.\nbanner exec will need to be manually applied.\nFor example:\nsw-leaf-bmc-001(config)# router ospf 1 sw-leaf-bmc-001(config-router-ospf-1)# router-id 10.2.0.4 sw-leaf-bmc-001(config-router-ospf-1)# router-id ospf 2 vrf Customer % Error: Illegal parameter. sw-leaf-bmc-001(config-router-ospf-1)# router-id 10.2.0.4 To fix:\nsw-leaf-bmc-001(config)# router ospf 1 sw-leaf-bmc-001(config-router-ospf-1)# router-id 10.2.0.4 sw-leaf-bmc-001(config-router-ospf-1)# exit sw-leaf-bmc-001(config)# router ospf 2 vrf Customer sw-leaf-bmc-001(config-router-ospf-2)# router-id 10.2.0.4 Mellanox Verify that no cli default prefix-modes enable is configured on the switch before applying any configuration.\nsw-spine-001 [mlag-domain: standby] (config) # no cli default prefix-modes enable Write memory Save the configuration once the configuration is applied.\nRefer to the Saving Configuration procedure.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/bgp_basic/",
	"title": "Border Gateway Protocol (BGP) Basics",
	"tags": [],
	"description": "",
	"content": "Border Gateway Protocol (BGP) Basics \u0026ldquo;The primary function of a Border Gateway Protocol (BGP) speaking system is to exchange network reachability information with other BGP systems. This network reachability information includes information on the list of Autonomous Systems (ASes) that reachability information traverses. This information is sufficient for constructing a graph of AS connectivity for this reachability, from which routing loops may be pruned and, at the AS level, some policy decisions may be enforced.\u0026rdquo; –rfc4271A\nBGP is configurable to run in either internal (iBGP) or external (eBGP) mode.\nConfiguration Commands Create a static route towards a blackhole interface:\nswitch(config)# ip route IP-ADDR/SUBNET blackhole Configure a BGP instance:\nswitch(config)# router bgp AS-NUM [vrf VRF] Create network statements for each subnet to advertise:\nswitch(config-router)# network IP-ADDR/SUBNET Configure a neighbor relationship with another BGP speaker:\nswitch(config-router)# neighbor IP-ADDR remote-as AS-NUM Configure an MD5 encrypted password to secure the neighbor relationship:\nswitch(config-router)# neighbor IP-ADDR password \u0026lt;cipher|plain\u0026gt;text PSWD Configure soft reconfiguration:\nswitch(config-router)# neighbor IP-ADDR soft-reconfiguration inbound Show commands to validate functionality: :\nswitch# show bgp all [summary|neighbors] Expected Results Administrators can configure BGP on the switch Administrators can create the network statements and the routes are in the routing table Administrators can configure a BGP neighbor that uses an MD5 encrypted password Administrators can validate the BGP relationship is established and that the network statement is advertised to the peer Soft reconfiguration is enabled Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/dns/powerdns_migration/",
	"title": "PowerDNS Migration Guide",
	"tags": [],
	"description": "",
	"content": "PowerDNS Migration Guide The migration to PowerDNS as the authoritative DNS source and the introduction of Bifurcated CAN (Customer Access Network) will result in some changes to the node and service naming conventions.\nDNS Record Naming Changes Fully qualified domain names will be introduced for all DNS records.\nCanonical name: hostname.network-path.system-name.site-domain\nhostname - The hostname of the node or service network-path - The network path used to access the node .nmn - Node Management Network .nmnlb - Node Management Network LoadBalancers .hmn - Hardware Management Network .hmnlb - Hardware management Network LoadBalancers .hsn - High Speed Network (Slingshot) .can - Customer Access Network .chn - Customer High Speed network .cmn - Customer Management Network system-name - The customer defined name of the system site-domain - The top-level domain It will be possible to refer to a hostname via a short name consisting of hostname.network-path, for example ncn-w001.nmn.\nUnderscores (_) will be removed from all names in favor of hyphens (-) to ensure compliance with RFC 1035.\nNetwork paths such as -nmn and -hmn in the hostname will be removed. The fully qualified domain name will be used to define the network path.\nKubernetes services that were accessed via the .local domain will now be accessed via a fully qualified domain name.\nBackwards Compatibility The old service and node names will not be migrated to PowerDNS however they will be maintained in Unbound as local records for the purpose of backwards compatibility. These records will be removed entirely in a future release when the cray-dns-unbound-manager job is deprecated. Unbound will remain as the front-end cache.\nThe user and administrative traffic segregation introduced by Bifurcated CAN has changed the URLs for certain services as it is now necessary to include the network path in the fully qualified domain name.\nIn the following table of examples the Backwards Compatible column indicates whether the old DNS name can be expected to function in CSM 1.2.\nExamples The following table of examples assumes that the system was configured with a system-name of shasta and a site-domain of dev.cray.com\nOld Name New Name Short name Backwards Compatible api-gw-service-nmn.local api.nmnlb.shasta.dev.cray.com api.nmnlb Yes registry.local registry.nmnlb.shasta.dev.cray.com registry.nmnlb Yes packages.local packages.nmnlb.shasta.dev.cray.com packages.nmnlb Yes spire.local spire.nmnlb.shasta.dev.cray.com spire.nmnlb Yes rgw-vip.nmn rgw-vip.nmn.local rgw-vip.nmn.shasta.dev.cray.com rgw-vip.nmn Yes rgw-vip.hmn rgw-vip.hmn.local rgw-vip.hmn.shasta.dev.cray.com rgw-vip.hmn Yes ncn-w001 ncn-w001.nmn.shasta.dev.cray.com ncn-w001.nmn Yes ncn-w001-mgmt ncn-w001-mgmt.hmn.shasta.dev.cray.com ncn-w001-mgmt.hmn Yes nid000001-nmn nid000001.nmn.shasta.dev.cray.com nid000001.nmn Yes x3000c0s2b0 x3000c0s2b0.hmn.shasta.dev.cray.com x3000c0s2b0.hmn Yes x3000c0s2b0n0 x3000c0s2b0n0.nmn.shasta.dev.cray.com x3000c0s2b0n0.nmn Yes x1000c5s0b0n0h0 x1000c5s0b0n0h0.hsn.shasta.dev.cray.com x1000c5s0b0n0h0.hsn Yes x1000c5s0b0n0h1 x1000c5s0b0n0h1.hsn.shasta.dev.cray.com x1000c5s0b0n0h1.hsn Yes auth.shasta.dev.cray.com auth.cmn.shasta.dev.cray.com No nexus.shasta.dev.cray.com nexus.cmn.shasta.dev.cray.com No grafana.shasta.dev.cray.com grafana.cmn.shasta.dev.cray.com No prometheus.shasta.dev.cray.com prometheus.cmn.shasta.dev.cray.com No alertmanager.shasta.dev.cray.com alertmanager.cmn.shasta.dev.cray.com No vcs.shasta.dev.cray.com vcs.cmn.shasta.dev.cray.com No kiali-istio.shasta.dev.cray.com kiali-istio.cmn.shasta.dev.cray.com No s3.shasta.dev.cray.com s3.cmn.shasta.dev.cray.com No sma-grafana.shasta.dev.cray.com sma-grafana.cmn.shasta.dev.cray.com No sma-kibana.shasta.dev.cray.com sma-kibana.cmn.shasta.dev.cray.com No api.shasta.dev.cray.com api.cmn.shasta.dev.cray.com\napi.chn.shasta.dev.cray.com\napi.can.shasta.dev.cray.com No "
},
{
	"uri": "/docs-csm/en-12/operations/network/external_dns/troubleshoot_dns_configuration_issues/",
	"title": "Troubleshoot DNS Configuration Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot DNS Configuration Issues Troubleshoot issues when DNS is not properly configured to delegate name resolution to the core DNS instance on a specific cluster. Although the CMN/CAN/CHN IP address may still be routable using the IP address directly, it may not work because Istio\u0026rsquo;s ingress gateway depends on the hostname (or SNI) to route traffic. For command line tools like cURL, using the --resolve option to force correct resolution can be used to work around this issue.\nTo get names to resolve correctly in a browser, modifying /etc/hosts to map the external hostname to the appropriate CMN/CAN/CHN IP address may be necessary. In either case, knowing the correct CMN/CAN/CHN IP address is required to use the cURL --resolve option or to update /etc/hosts.\nAssuming that the CMN/CAN/CHN, BGP, MetalLB, and external DNS are properly configured on a system, name resolution requests can be sent directly to the desired DNS server.\nThis document also covers how to gain access to system services when external DNS is not configured properly.\nPrerequisites The Domain Name Service (DNS) is not configured properly.\nProcedure View the DNS configuration on the system.\nncn-mw# kubectl -n services get svc cray-dns-powerdns-cmn-udp Example output:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cray-dns-powerdns-cmn-udp LoadBalancer 10.25.156.88 10.101.5.61 53:32674/UDP 45h Confirm that DNS is configured properly.\nRun the following command from a laptop or workstation.\nexternal# dig SERVICE.NETWORK.SYSTEM_DOMAIN_NAME +short If an IP address is returned, then DNS is configured properly and the remaining steps in this procedure can be skipped. If an IP address is not returned, then proceed to the next step.\nUse the IP address to direct DNS requests directly to the cray-dns-powerdns-cmn-udp service.\nReplace the example IP address (10.101.5.61) with the EXTERNAL-IP value returned in step 1. If an IP address is returned, then it means upstream DNS is not configured correctly.\nexternal# dig SERVICE.NETWORK.SYSTEM_DOMAIN_NAME +short @10.101.5.61 Direct DNS requests to the cluster IP address from an NCN.\nReplace the example cluster IP address (10.25.156.88) with the CLUSTER-IP value returned in step 1. If an IP address is returned, then external DNS is configured on the cluster and something is likely wrong with the CMN or BGP.\nncn-mw# dig SERVICE.NETWORK.SYSTEM_DOMAIN_NAME +short @10.25.156.88 Access services in the event that external DNS is down or something is configured incorrectly.\nSearch through Kubernetes service objects for external-dns.alpha.kubernetes.io/hostname annotations to find the corresponding external IP address. The kubectl command makes it easy to generate an /etc/hosts compatible listing of IP addresses to hostnames using the go-template output format shown below.\nncn-mw# kubectl get svc --all-namespaces -o go-template --template \\ \u0026#39;{{ range .items }}{{ $lb := .status.loadBalancer }}{{ with .metadata.annotations }} {{ with (index . \u0026#34;external-dns.alpha.kubernetes.io/hostname\u0026#34;) }} {{ $hostnames := . }}{{ with $lb }}{{ range .ingress }} {{ printf \u0026#34;%s\\t%s\\n\u0026#34; .ip $hostnames }}{{ end }}{{ end }} {{ end }}{{ end }}{{ end }}\u0026#39; | sort -u | tr , \u0026#39; \u0026#39; Example output:\n10.101.5.128 opa-gpm.cmn.SYSTEM_DOMAIN_NAME jaeger-istio.cmn.SYSTEM_DOMAIN_NAME kiali-istio.cmn.SYSTEM_DOMAIN_NAME prometheus.cmn.SYSTEM_DOMAIN_NAME alertmanager.cmn.SYSTEM_DOMAIN_NAME grafana.cmn.SYSTEM_DOMAIN_NAME vcs.cmn.SYSTEM_DOMAIN_NAME sma-grafana.cmn.SYSTEM_DOMAIN_NAME sma-kibana.cmn.SYSTEM_DOMAIN_NAME csms.cmn.SYSTEM_DOMAIN_NAME 10.101.5.129 api.cmn.SYSTEM_DOMAIN_NAME auth.cmn.SYSTEM_DOMAIN_NAME nexus.cmn.SYSTEM_DOMAIN_NAME 10.101.5.130 s3.cmn.SYSTEM_DOMAIN_NAME 10.92.100.71 api.nmn.SYSTEM_DOMAIN_NAME auth.nmn.SYSTEM_DOMAIN_NAME 10.92.100.222 cray-dhcp-kea 10.92.100.225 cray-dns-unbound 10.94.100.222 cray-dhcp-kea 10.94.100.225 cray-dns-unbound "
},
{
	"uri": "/docs-csm/en-12/operations/network/network/",
	"title": "Network",
	"tags": [],
	"description": "",
	"content": "Network There are several different networks supported by the HPE Cray EX system. This page outlines the available internal and external networks, as well as the devices that connect to each network.\nExternal networks Customer network (data center) System networks Hardware Management Network (HMN) Node Management Network (NMN) ClusterStor Management Network High Speed Network (HSN) IP address ranges Access Control Lists (ACLs) External networks Customer network (data center) The following devices are connected to this network:\nThe ncn-m001 BMC is connected by the customer network switch to the customer management network All management nodes (worker, master, and storage) ClusterStor System Management Unit (SMU) interfaces User Access Nodes (UANs) System networks Hardware Management Network (HMN) The following devices are connected to the HMN:\nBMCs for administrative tasks Power Distribution Units (PDUs) Keyboard/video/mouse (KVM) Node Management Network (NMN) The following devices are connected to the NMN:\nAll NCNs and compute nodes. UANs ClusterStor Management Network The following devices are connected to this network:\nClusterStor controller management interfaces of all ClusterStor components (SMU, Metadata Management Unit (MMU), and Scalable Storage Unit (SSU)) High Speed Network (HSN) The following devices are connected to the HSN:\nKubernetes worker nodes UANs ClusterStor controller data interfaces of all ClusterStor components (SMU, MMU, and SSU) At least two NCNs whose BMCs are on the HMN. If these are not present, there cannot be multiple DVS servers that function correctly, which will have an adverse effect on compute node root file system and CPE scaling, performance, and reliability. IP address ranges During initial installation, several of those networks are created with default IP address ranges. See Default IP Address Ranges.\nAccess Control Lists (ACLs) A default configuration of ACLs is also set when the system is installed. The default configuration of ACLs between the NMN and HMN are described below:\nThe network management system (NMS) data model and REST API enable customer sites to construct their own \u0026ldquo;networks\u0026rdquo; of nodes within the high-speed fabric, where a \u0026ldquo;network\u0026rdquo; is a collection of nodes that share a VLAN and an IP subnet.\nThe low-level network management components (switch, DHCP service, ARP service) of the NCNs and ClusterStor interfaces are configured to serve one particular network (the \u0026ldquo;supported network\u0026rdquo;) on the high-speed fabric. The supported network includes all of the compute nodes, thereby enabling those compute nodes to access the gateway, user access services, and ClusterStor devices. A site may create other networks as well, but it is only the supported network that is served by those devices.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/customer_accessible_networks/troubleshoot_cmn_issues/",
	"title": "Troubleshoot CMN issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot CMN issues Various connection points to check when using the CMN and how to fix any issues that arise.\nThe most frequent issue with the Customer Management Network (CMN) is trouble accessing IP addresses outside of the HPE Cray EX system from a node or pod inside the system.\nThe best way to resolve this issue is to try to ping an outside IP address from one of the NCNs other than ncn-m001, which has a direct connection that it can use instead of the Customer Management Network (CMN). The following are some things to check to make sure CMN is configured correctly:\nDoes the NCN have an IP Address Configured on the bond0.cmn0 Interface? Check the status of the bond0.cmn0 interface. Make sure it has an address specified.\nip addr show bond0.cmn0 Example output:\n534: bond0.cmn0@bond0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 98:03:9b:b4:27:62 brd ff:ff:ff:ff:ff:ff inet 10.102.5.5/26 brd 10.101.8.255 scope global bond0.cmn0 valid_lft forever preferred_lft forever inet6 fe80::9a03:9bff:feb4:2762/64 scope link valid_lft forever preferred_lft forever If there is not an address specified, make sure the cmn- values have been defined in csi config init input.\nDoes the NCN have a Default Gateway Configured? Check the default route on an NCN other than ncn-m001. There should be a default route with a gateway matching the cmn-gateway value.\nip route | grep default Example output:\ndefault via 10.102.5.1 dev bond0.cmn0 If there is not an address specified, make sure the can- values have been defined in csi config init input.\nCan the Node Reach the Default CMN Gateway? Check that the node can ping the default gateway shown in the default route.\nping 10.102.5.1 Example output:\nPING 10.102.5.1 (10.102.5.1) 56(84) bytes of data. 64 bytes from 10.102.5.1: icmp_seq=1 ttl=64 time=0.148 ms 64 bytes from 10.102.5.1: icmp_seq=2 ttl=64 time=0.107 ms 64 bytes from 10.102.5.1: icmp_seq=3 ttl=64 time=0.133 ms 64 bytes from 10.102.5.1: icmp_seq=4 ttl=64 time=0.122 ms ^C --- 10.102.5.1 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3053ms rtt min/avg/max/mdev = 0.107/0.127/0.148/0.018 ms If the default gateway cannot be accessed, check the spine switch configuration.\nCan the Spines Reach Outside of the System? Check that each of the spines can ping an IP address outside of the HPE Cray EX system. This must be an IP address that is reachable from the network to which the CMN is connected. If there is only one spine being used on the system, only spine-001 needs to be checked.\nsw-spine-001 [standalone: master] # ping 8.8.8.8 Example output:\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=112 time=12.6 ms 64 bytes from 8.8.8.8: icmp_seq=2 ttl=112 time=12.5 ms 64 bytes from 8.8.8.8: icmp_seq=3 ttl=112 time=22.4 ms 64 bytes from 8.8.8.8: icmp_seq=4 ttl=112 time=12.5 ms ^C --- 8.8.8.8 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3004ms rtt min/avg/max/mdev = 12.501/15.022/22.440/4.285 ms If the outside IP address cannot be reached, check the spine switch configuration and the connection to the customer network.\nCan the Spines Reach the NCN? Check that each of the spines can ping one or more of the NCNs at its bond0.cmn0 IP address. If there is only one spine being used on the system, only spine-001 needs to be checked.\nsw-spine-001 [standalone: master] # ping 10.102.5.5 Example output:\nPING 10.102.5.5 (10.102.5.5) 56(84) bytes of data. 64 bytes from 10.102.5.5: icmp_seq=1 ttl=64 time=0.140 ms 64 bytes from 10.102.5.5: icmp_seq=2 ttl=64 time=0.134 ms 64 bytes from 10.102.5.5: icmp_seq=3 ttl=64 time=0.126 ms 64 bytes from 10.102.5.5: icmp_seq=4 ttl=64 time=0.178 ms ^C --- 10.102.5.5 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3058ms rtt min/avg/max/mdev = 0.126/0.144/0.178/0.023 ms If the NCN cannot be reached, check the spine switch configuration.\nCan a Device Outside the System Reach the CMN Gateway? Check that a device outside the HPE Cray EX system that is expected to have access to nodes and services on the CMN can ping the CMN gateway.\n# ping 10.102.5.1 Example output:\nPING 10.102.5.1 (10.102.5.1): 56 data bytes 64 bytes from 10.102.5.1: icmp_seq=0 ttl=58 time=54.724 ms 64 bytes from 10.102.5.1: icmp_seq=1 ttl=58 time=65.902 ms 64 bytes from 10.102.5.1: icmp_seq=2 ttl=58 time=51.960 ms 64 bytes from 10.102.5.1: icmp_seq=3 ttl=58 time=55.032 ms 64 bytes from 10.102.5.1: icmp_seq=4 ttl=58 time=57.606 ms ^C --- 10.102.5.1 ping statistics --- 5 packets transmitted, 5 packets received, 0.0% packet loss round-trip min/avg/max/stddev = 51.960/57.045/65.902/4.776 ms If the CMN gateway cannot be reached from outside, check the spine switch configuration and the connection to the customer network.\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/backups_for_etcd-operator_clusters/",
	"title": "Backups for etcd-operator Clusters",
	"tags": [],
	"description": "",
	"content": "Backups for etcd-operator Clusters Backups are periodically created for etcd clusters. These backups are stored in the Ceph Rados Gateway (S3). Not all services are backed up automatically. Services that are not backed up automatically will need to be manually rediscovered if the cluster is unhealthy.\nClusters with automated backups Clusters without automated backups Clusters with automated backups The following services are backed up daily (one week of backups retained) as part of the automated solution:\nBoot Orchestration Service (BOS) Boot Script Service (BSS) Compute Rolling Upgrade Service (CRUS) Firmware Action Service (FAS) User Access Service (UAS) Run the following command on any Kubernetes master or worker node in order to list the backups for a specific project. In the example below, the backups for BSS are listed.\nncn-mw# kubectl exec -it -n operators \\ $(kubectl get pod -n operators | grep etcd-backup-restore | head -1 | awk \u0026#39;{print $1}\u0026#39;) \\ -c boto3 -- list_backups cray-bss Example output:\ncray-bss/etcd.backup_v1450_2020-01-30-20:44:41 cray-bss/etcd.backup_v4183_2020-02-01-20:45:48 cray-bss/etcd.backup_v5771_2020-02-02-20:45:48 cray-bss/etcd.backup_v7210_2020-02-03-20:45:48 To view all available backups across all projects:\nncn-mw# kubectl exec -it -n operators \\ $(kubectl get pod -n operators | grep etcd-backup-restore | head -1 | awk \u0026#39;{print $1}\u0026#39;) \\ -c boto3 -- list_backups \u0026#34;\u0026#34; Example output:\nbare-metal/etcd-backup-2020-02-03-14-40-07.tar.gz bare-metal/etcd-backup-2020-02-03-14-50-03.tar.gz bare-metal/etcd-backup-2020-02-03-15-00-10.tar.gz bare-metal/etcd-backup-2020-02-03-15-10-06.tar.gz bare-metal/etcd-backup-2020-02-03-15-30-05.tar.gz bare-metal/etcd-backup-2020-02-03-15-40-01.tar.gz bare-metal/etcd-backup-2020-02-03-15-50-08.tar.gz cray-bos/etcd.backup_v1200_2020-02-03-20:45:48 cray-bos/etcd.backup_v240_2020-01-30-20:44:34 cray-bos/etcd.backup_v480_2020-01-31-20:44:34 cray-bos/etcd.backup_v720_2020-02-01-20:45:48 cray-bos/etcd.backup_v960_2020-02-02-20:45:48 cray-bss/etcd.backup_v1450_2020-01-30-20:44:41 cray-bss/etcd.backup_v4183_2020-02-01-20:45:48 [...] The returned output includes the date and time of the latest backup for each service. If a recent backup for any service is not included, it is an indication that the service is not backed up automatically. Create a manual backup for that service by following the Create a Manual Backup of a Healthy etcd Cluster procedure.\nClusters without automated backups The following projects are not backed up as part of the automated solution:\nContent Projection Service (CPS) Heartbeat Tracking Daemon (HBTD) HMS Notification Fanout Daemon (HMNFD) River Endpoint Discovery Service (REDS) If these clusters become unhealthy, the process for rediscovering their data should be followed. See Repopulate Data in etcd Clusters When Rebuilding Them.\n"
},
{
	"uri": "/docs-csm/en-12/operations/image_management/create_uan_boot_images/",
	"title": "Create UAN Boot Images",
	"tags": [],
	"description": "",
	"content": "Create UAN Boot Images Update configuration management Git repository to match the installed version of the User Access Node (UAN) product. Then use that updated configuration to create UAN boot images and a Boot Orchestration Service (BOS) session template.\nThis is the overall workflow for preparing UAN images for booting UANs:\nClone the UAN configuration Git repository and create a branch based on the branch imported by the UAN installation. Update the configuration content and push the changes to the newly created branch. Create a Configuration Framework Service (CFS) configuration for the UANs, specifying the Git configuration and the UAN image to apply the configuration to. More Cray products can also be added to the CFS configuration so that the UANs can install multiple Cray products into the UAN image at the same time. Configure the UAN image using CFS and generate a newly configured version of the UAN image. Create a BOS boot session template for the UANs. This template maps the configured image, the CFS configuration to be applied post-boot, and the nodes which will receive the image and configuration. Once the UAN BOS session template is created, then the UANs will be ready to be booted by a BOS session.\nReplace PRODUCT_VERSION and CRAY_EX_HOSTNAME in the example commands in this procedure with the current UAN product version installed (see 1. Get UAN information) and the hostname of the HPE Cray EX system, respectively.\nPrerequisites Limitations Procedure Get UAN information UAN image pre-boot configuration Configure UAN images Customize UAN image using CFS Identify UAN xnames Prepare UAN boot session templates Prerequisites The UAN product stream must be installed.\nLimitations This guide only details how to apply UAN-specific configuration to the UAN image and nodes. Consult the manuals for the individual HPE products (for example, workload managers and the HPE Cray Programming Environment) that must be configured on the UANs.\nProcedure 1. Get UAN information Obtain UAN artifact IDs and other information.\nUpon successful installation of the UAN product, the UAN configuration, image recipes, and prebuilt boot images are cataloged in the cray-product-catalog Kubernetes ConfigMap. This information is required for this procedure.\nTake note of the Gitea, Git, and Image Management Service (IMS) values (highlighted in the example output below).\nncn-mw# kubectl get cm -n services cray-product-catalog -o json | jq -r .data.uan Example output:\nPRODUCT_VERSION: configuration: clone_url: https://vcs.CRAY_EX_HOSTNAME/vcs/cray/uan-config-management.git # \u0026lt;--- Gitea clone url commit: 6658ea9e75f5f0f73f78941202664e9631a63726 # \u0026lt;--- Git commit id import_branch: cray/uan/PRODUCT_VERSION # \u0026lt;--- Git branch with configuration import_date: 2021-02-02 19:14:18.399670 ssh_url: git@vcs.CRAY_EX_HOSTNAME:cray/uan-config-management.git images: cray-shasta-uan-cos-sles15sp1.x86_64-0.1.17: # \u0026lt;--- IMS image name id: c880251d-b275-463f-8279-e6033f61578b # \u0026lt;--- IMS image id recipes: cray-shasta-uan-cos-sles15sp1.x86_64-0.1.17: # \u0026lt;--- IMS recipe name id: cbd5cdf6-eac3-47e6-ace4-aa1aecb1359a # \u0026lt;--- IMS recipe id 2. UAN image pre-boot configuration Generate the password hash for the root user.\nReplace PASSWORD with the desired root password. Do not omit the -n from the echo command. It is necessary to generate a valid hash.\nncn-mw# echo -n PASSWORD | openssl passwd -6 -salt $(\u0026lt; /dev/urandom tr -dc ./A-Za-z0-9 | head -c4) --stdin Obtain the HashiCorp Vault root token.\nncn-mw# kubectl get secrets -n vault cray-vault-unseal-keys -o jsonpath=\u0026#39;{.data.vault-root}\u0026#39; | base64 -d; echo Write the password hash obtained from the openssl command to the HashiCorp Vault.\nOpen a shell in the Vault pod.\nncn-mw# kubectl exec -itn vault cray-vault-0 -- sh Authenticate with Vault.\nThe vault login command will request the root token found in the earlier step.\ncray-vault-0# export VAULT_ADDR=http://cray-vault:8200 cray-vault-0# vault login Write the password hash to Vault.\nThis password hash will be written to the UAN for the root user by CFS.\nReplace HASH in the example below with the password hash obtained from the earlier step.\ncray-vault-0# vault write secret/uan root_password=\u0026#39;HASH\u0026#39; Read back the password hash to verify that it was stored correctly.\ncray-vault-0# vault read secret/uan Exit from the Vault pod.\ncray-vault-0# exit Obtain the password for the crayvcs user from the Kubernetes secret for use in the next command.\nncn-mw# kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode Clone the UAN configuration management repository.\nThe repository is in the Version Control Service (VCS)/Gitea service, and its location is reported in the cray-product-catalog Kubernetes ConfigMap in the configuration.clone_url key. The CRAY_EX_HOSTNAME from the clone_url is replaced with api-gw-service-nmn.local in the command that clones the repository.\nncn-mw# git clone https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git ncn-mw# cd uan-config-management \u0026amp;\u0026amp; git checkout cray/uan/PRODUCT_VERSION \u0026amp;\u0026amp; git pull Create a branch using the imported branch from the installation to customize the UAN image.\nThis imported branch will be reported in the cray-product-catalog Kubernetes ConfigMap in the configuration.import_branch key under the UAN section. The format is cray/uan/PRODUCT_VERSION. In this guide, an integration branch is used for examples, but the name can be any valid Git branch name.\nModifying the cray/uan/PRODUCT_VERSION branch that was created by the UAN product installation is not allowed by default.\nncn-mw# git checkout -b integration \u0026amp;\u0026amp; git merge cray/uan/PRODUCT_VERSION Configure a root user in the UAN image.\nAdd the encrypted password of the root user from /etc/shadow on an Non-Compute Node (NCN) worker to the file group_vars/Application/passwd.yml.\nSkip this step if the root user is already configured in the image.\nHewlett Packard Enterprise recommends configuring a root user in the UAN image for troubleshooting purposes. The entry for root user password will resemble the following example:\nroot_passwd: $6$LmQ/PlWlKixK$VL4ueaZ8YoKOV6yYMA9iH0gCl8F4C/3yC.jMIGfOK6F61h6d.iZ6/QB0NLyex1J7AtOsYvqeycmLj2fQcLjfE1 Apply any site-specific customizations and modifications to the Ansible configuration for the UANs and commit the changes.\nThe default Ansible play to configure UANs is site.yml in the base of the uan-config-management repository. The roles that are executed in this play allow for nondefault configuration as required for the system.\nConsult the individual Ansible role README.md files in the uan-config-management repository roles directory to configure individual role variables. Roles prefixed with uan_ are specific to UAN configuration and include network interfaces, disk, LDAP, software packages, and message of the day roles.\nVariables should be defined and overridden in the Ansible inventory locations of the repository as shown in the following example and not in the Ansible plays and roles defaults. See Ansible Directory Layout.\nWARNING: Never place sensitive information such as passwords in the Git repository.\nThe following example shows how to add a vars.yml file containing site-specific configuration values to the Application group variable location.\nThese and other Ansible files do not necessarily need to be modified for UAN image creation.\nncn-mw# vim group_vars/Application/vars.yml ncn-mw# git add group_vars/Application/vars.yml ncn-mw# git commit -m \u0026#34;Add vars.yml customizations\u0026#34; Verify that the System Layout Service (SLS) and the uan_interfaces configuration role refer to the Mountain Node Management Network by the same name.\nSkip this step if there are no Mountain cabinets in the HPE Cray EX system.\nEdit the roles/uan_interfaces/tasks/main.yml file.\nChange the line that reads url: http://cray-sls/v1/search/networks?name=MNMN to read url: http://cray-sls/v1/search/networks?name=NMN_MTN.\nThe following excerpt of the relevant section of the file shows the result of the change.\n- name: Get Mountain NMN Services Network info from SLS local_action: module: uri url: http://cray-sls/v1/search/networks?name=NMN_MTN method: GET register: sls_mnmn_svcs ignore_errors: yes Stage and commit the network name change.\nncn-mw# git add roles/uan_interfaces/tasks/main.yml ncn-mw# git commit -m \u0026#34;Add Mountain cabinet support\u0026#34; Push the changes to the repository using the proper credentials, including the password obtained previously.\nncn-mw# git push --set-upstream origin integration Enter the appropriate credentials when prompted:\nUsername for \u0026#39;https://api-gw-service-nmn.local\u0026#39;: crayvcs Password for \u0026#39;https://crayvcs@api-gw-service-nmn.local\u0026#39;: Capture the most recent commit for reference in setting up a CFS configuration and navigate to the parent directory.\nncn-mw# git rev-parse --verify HEAD ecece54b1eb65d484444c4a5ca0b244b329f4667 is an example commit that could be returned.\nNavigate back to the parent directory.\nncn-mw# cd .. The configuration parameters have been stored in a branch in the UAN Git repository. The next phase of the process is initiating the Configuration Framework Service (CFS) to customize the image.\n3. Configure UAN images Create a JSON input file for generating a CFS configuration for the UAN.\nGather the Git repository clone URL, commit, and top-level play for each configuration layer (that is, Cray product). Add them to the CFS configuration for the UAN, if wanted.\nFor the commit value for the UAN layer, use the Git commit value obtained in the previous step.\nSee the product manuals for further information on configuring other Cray products, as this procedure documents only the configuration of the UAN. More layers can be added to be configured in a single CFS session.\nThe following configuration example can be used for preboot image customization as well as post-boot node configuration. This example contains only a single layer. However, configuration layers for other products may be specified in the list after this layer, if desired.\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;uan-integration-PRODUCT_VERSION\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;ecece54b1eb65d484444c4a5ca0b244b329f4667\u0026#34; } ] } Add the configuration to CFS using the JSON input file.\nIn the following example, the JSON file created in the previous step is named uan-config-PRODUCT_VERSION.json. Only the details for the UAN layer are shown.\nncn-mw# cray cfs configurations update uan-config-PRODUCT_VERSION \\ --file ./uan-config-PRODUCT_VERSION.json \\ --format json Example output:\nThis output uses the example single-layer configuration from earlier. If layers were added for additional products, then they will also appear in the output.\n{ \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:00:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;ecece54b1eb65d484444c4a5ca0b244b329f4667\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;uan-integration-PRODUCT_VERSION\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;uan-config-PRODUCT_VERSION\u0026#34; } Modify the UAN image to include the 1.4.0 Day Zero RPMs.\nExpand the 1.4.0 Day Zero Patch tarball if it has not been done already.\nncn-mw# tar -xvf shasta-1.4.0-p2.tar Example output:\n1.4.0-p2/ 1.4.0-p2/csm/ 1.4.0-p2/csm/csm-0.8.22-0.9.0.patch.gz 1.4.0-p2/csm/csm-0.8.22-0.9.0.patch.gz.md5sum 1.4.0-p2/uan/ 1.4.0-p2/uan/uan-2.0.0-uan-2.0.0.patch.gz 1.4.0-p2/uan/uan-2.0.0-uan-2.0.0.patch.gz.md5sum 1.4.0-p2/rpms/ 1.4.0-p2/rpms/cray-dvs-compute-2.12_4.0.102-7.0.1.0_8.1__g30d29e7a.x86_64.rpm 1.4.0-p2/rpms/cray-dvs-devel-2.12_4.0.102-7.0.1.0_8.1__g30d29e7a.x86_64.rpm 1.4.0-p2/rpms/cray-dvs-kmp-cray_shasta_c-2.12_4.0.102_k4.12.14_197.78_9.1.58-7.0.1.0_8.1__g30d29e7a.x86_64.rpm 1.4.0-p2/rpms/cray-network-config-1.1.7-20210318094806_b409053-sles15sp1.x86_64.rpm 1.4.0-p2/rpms/slingshot-network-config-1.1.7-20210318093253_83fab52-sles15sp1.x86_64.rpm 1.4.0-p2/rpms/slingshot-network-config-full-1.1.7-20210318093253_83fab52-sles15sp1.x86_64.rpm 1.4.0-p2/rpms/cray-dvs-compute-2.12_4.0.102-7.0.1.0_8.1__g30d29e7a.x86_64.rpm.md5sum 1.4.0-p2/rpms/cray-dvs-devel-2.12_4.0.102-7.0.1.0_8.1__g30d29e7a.x86_64.rpm.md5sum 1.4.0-p2/rpms/cray-dvs-kmp-cray_shasta_c-2.12_4.0.102_k4.12.14_197.78_9.1.58-7.0.1.0_8.1__g30d29e7a.x86_64.rpm.md5sum 1.4.0-p2/rpms/cray-network-config-1.1.7-20210318094806_b409053-sles15sp1.x86_64.rpm.md5sum 1.4.0-p2/rpms/slingshot-network-config-1.1.7-20210318093253_83fab52-sles15sp1.x86_64.rpm.md5sum 1.4.0-p2/rpms/slingshot-network-config-full-1.1.7-20210318093253_83fab52-sles15sp1.x86_64.rpm.md5sum Download the rootfs image specified in the UAN product catalog.\nReplace IMAGE_ID in the following export command with the IMS image ID recorded earlier in 1. Get UAN information.\nncn-mw# UAN_IMAGE_ID=IMAGE_ID ncn-mw# cray artifacts get boot-images ${UAN_IMAGE_ID}/rootfs ${UAN_IMAGE_ID}.squashfs ncn-mw# ls -A ${UAN_IMAGE_ID}.squashfs Example output:\n-rw-r--r-- 1 root root 1.5G Mar 17 19:35 f3ba09d7-e3c2-4b80-9d86-0ee2c48c2214.squashfs Mount the SquashFS file and copy its contents to a different directory.\nncn-mw# mkdir mnt ncn-mw# mkdir UAN-1.4.0-day-zero ncn-mw# mount -t squashfs ${UAN_IMAGE_ID}.squashfs mnt -o ro,loop ncn-mw# cp -a mnt UAN-1.4.0-day-zero ncn-mw# umount mnt ncn-mw# rmdir mnt Copy the new RPMs into the new image directory.\nncn-mw# cp 1.4.0-p2/rpms/* UAN-1.4.0-day-zero/ ncn-mw# cd UAN-1.4.0-day-zero/ chroot into the new image directory.\nncn-mw# chroot . bash Update, erase, and install RPMs in the new image directory.\nchroot-ncn-mw# rpm -Uv cray-dvs-*.rpm chroot-ncn-mw# rpm -e cray-network-config chroot-ncn-mw# rpm -e slingshot-network-config-full chroot-ncn-mw# rpm -e slingshot-network-config chroot-ncn-mw# rpm -iv slingshot-network-config-full-1.1.7-20210318093253_83fab52-sles15sp1.x86_64.rpm \\ slingshot-network-config-1.1.7-20210318093253_83fab52-sles15sp1.x86_64.rpm \\ cray-network-config-1.1.7-20210318094806_b409053-sles15sp1.x86_64.rpm Generate a new initrd to match the updated image.\nRun the /tmp/images.sh script. Then wait for this script to complete before continuing.\nchroot-ncn-mw# /tmp/images.sh The output of this script will contain error messages. These error messages can be ignored as long as the following message appears at the end: dracut: *** Creating initramfs image file\nCopy the /boot/initrd and /boot/vmlinuz files out of the chroot environment and into a temporary location on the file system of the node.\nExit the chroot environment and delete the packages.\nchroot-ncn-mw# exit ncn-mw# rm *.rpm ncn-mw# cd .. Verify that there is only one subdirectory in the lib/modules directory of the image.\nThe existence of more than one subdirectory indicates a mismatch between the kernel of the image and the DVS RPMs that were installed in the previous step.\nncn-mw# la UAN-1.4.0-day-zero/lib/modules/ Example output:\ntotal 8.0K drwxr-xr-x 3 root root 49 Feb 25 17:50 ./ drwxr-xr-x 8 root root 4.0K Feb 25 17:52 ../ drwxr-xr-x 6 root root 4.0K Mar 17 19:49 4.12.14-197.78_9.1.58-cray_shasta_c/ Squash the new image directory.\nncn-mw# mksquashfs UAN-1.4.0-day-zero UAN-1.4.0-day-zero.squashfs Example output:\nParallel mksquashfs: Using 64 processors Creating 4.0 filesystem on UAN-1.4.0-day-zero.squashfs, block size 131072. [...] Create a new IMS image registration and save the id field in an environment variable.\nncn-mw# cray ims images create --name UAN-1.4.0-day-zero --format toml Example output:\nname = \u0026#34;UAN-1.4.0-day-zero\u0026#34; created = \u0026#34;2021-03-17T20:23:05.576754+00:00\u0026#34; id = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e\u0026#34; ncn-mw# export NEW_IMAGE_ID=ac31e971-f990-4b5f-821d-c0c18daefb6e Upload the new image, initrd, and kernel to S3 using the ID from the previous step.\nUpload the image.\nncn-mw# cray artifacts create boot-images ${NEW_IMAGE_ID}/rootfs UAN-1.4.0-day-zero.squashfs --format toml Example output:\nartifact = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e/UAN-1.4.0-day-zero.rootfs\u0026#34; Key = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e/UAN-1.4.0-day-zero.rootfs\u0026#34; Upload the initrd.\nncn-mw# cray artifacts create boot-images ${NEW_IMAGE_ID}/initrd initrd --format toml Example output:\nartifact = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e/UAN-1.4.0-day-zero.initrd\u0026#34; Key = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e/UAN-1.4.0-day-zero.initrd\u0026#34; Upload the kernel.\nncn-mw# cray artifacts create boot-images ${NEW_IMAGE_ID}/kernel vmlinuz --format toml Example output:\nartifact = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e/UAN-1.4.0-day-zero.kernel\u0026#34; Key = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e/UAN-1.4.0-day-zero.kernel\u0026#34; Get the S3 generated etag value for each uploaded artifact.\nDisplay S3 values for uploaded image.\nncn-mw# cray artifacts describe boot-images ${NEW_IMAGE_ID}/rootfs --format toml Example output:\n[artifact] AcceptRanges = \u0026#34;bytes\u0026#34; LastModified = \u0026#34;2021-05-05T00:25:21+00:00\u0026#34; ContentLength = 1647050752 ETag = \u0026#34;\\\u0026#34;db5582fd817c8a8dc084e1b8b4f0ea3b-197\\\u0026#34;\u0026#34; \u0026lt;--- ContentType = \u0026#34;binary/octet-stream\u0026#34; [artifact.Metadata] md5sum = \u0026#34;cb6a8934ad3c483e740c648238800e93\u0026#34; Note that when adding the etag to the IMS manifest below, remove the quotation marks from the etag value. So, for the above artifact, the etag would be db5582fd817c8a8dc084e1b8b4f0ea3b-197.\nDisplay S3 values for uploaded initrd.\nncn-mw# cray artifacts describe boot-images ${NEW_IMAGE_ID}/initrd Display S3 values for uploaded kernel.\nncn-mw# cray artifacts describe boot-images ${NEW_IMAGE_ID}/kernel Obtain the md5sum of the SquashFS image, initrd, and kernel.\nncn-mw# md5sum UAN-1.4.0-day-zero.squashfs initrd vmlinuz Example output:\ncb6a8934ad3c483e740c648238800e93 UAN-1.4.0-day-zero.squashfs 3fd8a72a49a409f70140fabe11bdac25 initrd 5edcf3fd42ab1eccfbf1e52008dac5b9 vmlinuz Print out all the IMS details about the current UAN image.\nUse the IMS image ID from 1. Get UAN information.\nncn-mw# cray ims images describe c880251d-b275-463f-8279-e6033f61578b --format toml Example output:\ncreated = \u0026#34;2021-03-24T18:00:24.464755+00:00\u0026#34; id = \u0026#34;c880251d-b275-463f-8279-e6033f61578b\u0026#34; name = \u0026#34;cray-shasta-uan-cos-sles15sp1.x86_64-0.1.32\u0026#34; [link] etag = \u0026#34;d4e09fb028d5d99e4a0d4d9b9d930e13\u0026#34; path = \u0026#34;s3://boot-images/c880251d-b275-463f-8279-e6033f61578b/manifest.json\u0026#34; type = \u0026#34;s3\u0026#34; Use the path of the manifest.json file to download that JSON to a local file.\nncn-mw# cray artifacts get boot-images c880251d-b275-463f-8279-e6033f61578b/manifest.json uan-manifest.json ncn-mw# cat uan-manifest.json Example output:\n{ \u0026#34;artifacts\u0026#34;: [ { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;6d04c3a4546888ee740d7149eaecea68\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/c880251d-b275-463f-8279-e6033f61578b/rootfs\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;a159b94238fc5bfe80045889226b33a3\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.rootfs.squashfs\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;6d04c3a4546888ee740d7149eaecea68\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/c880251d-b275-463f-8279-e6033f61578b/kernel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;175f0c1363c9e3a4840b08570a923bc5\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.kernel\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;6d04c3a4546888ee740d7149eaecea68\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/c880251d-b275-463f-8279-e6033f61578b/initrd\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;0094629e4da25226c75b113760eeabf7\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.initrd\u0026#34; } ], \u0026#34;created\u0026#34; : \u0026#34;20210317153136\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; } Alternatively, a manifest.json can be created from scratch.\nReplace the path, md5, and etag values of the initrd, kernel, and rootfs with the values obtained in substeps above.\nUpdate the value for the created field in the manifest with the output of the following command:\nncn-mw# date \u0026#39;+%Y%m%d%H%M%S\u0026#39; Verify that the modified JSON file is still valid.\nncn-mw# cat manifest.json | jq Upload the updated manifest.json file.\nncn-mw# cray artifacts create boot-images ${NEW_IMAGE_ID}/manifest.json uan-manifest.json Update the IMS image to use the new uan-manifest.json file.\nncn-mw# cray ims images update ${NEW_IMAGE_ID} \\ --link-type s3 --link-path s3://boot-images/${NEW_IMAGE_ID}/manifest.json \\ --link-etag 6d04c3a4546888ee740d7149eaecea68 --format toml Example output:\ncreated = \u0026#34;2021-03-17T20:23:05.576754+00:00\u0026#34; id = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e\u0026#34; name = \u0026#34;UAN-1.4.0-day-zero\u0026#34; [link] etag = \u0026#34;6d04c3a4546888ee740d7149eaecea68\u0026#34; path = \u0026#34;s3://boot-images/ac31e971-f990-4b5f-821d-c0c18daefb6e/manifest.json\u0026#34; type = \u0026#34;s3\u0026#34; 4. Customize UAN image using CFS Create a CFS session to perform preboot image customization of the UAN image.\nncn-mw# cray cfs sessions create --name uan-config-PRODUCT_VERSION \\ --configuration-name uan-config-PRODUCT_VERSION \\ --target-definition image \\ --target-group Application $NEW_IMAGE_ID \\ --format json Wait for the CFS configuration session for the image customization to complete.\nThen record the ID of the IMS image created by CFS.\nThe following command will produce output while the process is running. If the CFS session completes successfully, an IMS image ID will appear in the output.\nncn-mw# cray cfs sessions describe uan-config-PRODUCT_VERSION --format json | jq 5. Identify UAN xnames Retrieve the component names (xnames) of the UANs from the Hardware State Manager (HSM).\nncn-mw# cray hsm state components list --role Application --subrole UAN --format json | jq -r .Components[].ID Example output:\nx3000c0s19b0n0 x3000c0s24b0n0 x3000c0s20b0n0 x3000c0s22b0n0 6. Prepare UAN boot session templates Determine the correct value for the ifmap option in the kernel_parameters string for the type of UAN.\nUse ifmap=net0:nmn0,lan0:hsn0,lan1:hsn1 if the UANs are: Either HPE DL325 or DL385 nodes that have a single OCP PCIe card installed. Gigabyte nodes that do not have additional PCIe network cards installed other than the built-in LOM ports. Use ifmap=net2:nmn0,lan0:hsn0,lan1:hsn1 if the UANs are: Either HPE DL325 or DL385 nodes which have a second OCP PCIe card installed, regardless of if it is being used or not. Gigabyte nodes that have a PCIe network card installed in addition to the built-in LOM ports, regardless of if it is being used or not. Construct a JSON BOS boot session template for the UAN.\nPopulate the template with the following information:\nThe value of the ifmap option for the kernel_parameters string that was determined in the previous step. For the node_list in the boot set, use the xnames of the UANs found in 5. Identify UAN xnames. The path field in the boot set should contain the customized image IMS ID (result_id) from 4. Customize UAN image using CFS. The cfs.configuration field should be set to the name of the CFS configuration used in 4. Customize UAN image using CFS. Verify that the session template matches the format and structure in the following example:\n{ \u0026#34;boot_sets\u0026#34;: { \u0026#34;uan\u0026#34;: { \u0026#34;boot_ordinal\u0026#34;: 2, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=nmn0:dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y quiet rd.neednet=1 rd.retry=10 rd.shell turbo_boost_limit=999 ifmap=net2:nmn0,lan0:hsn0,lan1:hsn1 spire_join_token=${SPIRE_JOIN_TOKEN}\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;#... List of Application Nodes from cray hsm state command ...\u0026#34; ], \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/IMS_IMAGE_ID/manifest.json\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:nmn0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;cfs\u0026#34;: { \u0026#34;configuration\u0026#34;: \u0026#34;uan-config-PRODUCT\\_VERSION\u0026#34; }, \u0026#34;enable_cfs\u0026#34;: true } Save the template with a descriptive name, such as uan-sessiontemplate-PRODUCT_VERSION.json.\nRegister the session template with BOS.\nThe following command uses the JSON session template file to save a session template in BOS. This step allows administrators to boot UANs by referring to the session template name.\nncn-mw# cray bos sessiontemplate create \\ --name uan-sessiontemplate-PRODUCT_VERSION \\ --file uan-sessiontemplate-PRODUCT_VERSION.json Example output:\n/sessionTemplate/uan-sessiontemplate-PRODUCT_VERSION Perform Boot UANs to boot the UANs with the new image and BOS session template.\n"
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/component_memberships/",
	"title": "Component Memberships",
	"tags": [],
	"description": "",
	"content": "Component Memberships Memberships are a read-only resource that is generated automatically by changes to groups and partitions. Each component in /hsm/v2/State/Components is represented. Filter options are available to prune the list, or a specific component name (xname) can be given. All groups and the partition (if any) of each component are listed.\nAt this point in time, only information about node components is needed. The --type node filter option is used in the commands below to retrieve information about node memberships only.\nThe following is an example membership:\n{ \u0026#34;id\u0026#34; : \u0026#34;x2c3s0b0n0\u0026#34;, \u0026#34;groupLabels\u0026#34; : [ \u0026#34;grp1\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;my_nodes\u0026#34; ], \u0026#34;partitionName\u0026#34; : \u0026#34;partition2\u0026#34; } Troubleshooting: If the Cray CLI has not been initialized, the CLI commands will not work.\nRetrieve Group and Partition Memberships By default, the memberships collection contains all components, regardless of if they are in a group. However, a filtered subset is desired more frequently. Querying the memberships collection supports the same query options as /hsm/v2/State/Components.\nRetrieve all node memberships:\nncn-m# cray hsm memberships list --type node Retrieve only nodes not in a partition:\nncn-m# cray hsm memberships list --type node --partition NULL Retrieve Membership Data for a Given Component Any components in /hsm/v2/State/Components can have its group and memberships looked up with its individual component component name (xname).\nncn-m# cray hsm memberships describe MEMBER_ID "
},
{
	"uri": "/docs-csm/en-12/operations/firmware/fas_use_cases/",
	"title": "FAS Use Cases",
	"tags": [],
	"description": "",
	"content": "FAS Use Cases Use the Firmware Action Service (FAS) to update the firmware on supported hardware devices. Each procedure includes the prerequisites and example recipes required to update the firmware.\nWhen updating an entire system, walk down the device hierarchy component type by component type, starting first with routers (switches), proceeding to chassis, and then finally to nodes. While this is not strictly necessary, it does help eliminate confusion.\nNOTE: Any node that is locked remains in the state inProgress with the stateHelper message of \u0026quot;failed to lock\u0026quot; until the action times out, or the lock is released. If the action is timed out, these nodes report as failed with the stateHelper message of \u0026quot;time expired; could not complete update\u0026quot;. This includes NCNs which are manually locked to prevent accidental rebooting and firmware updates.\nRefer to FAS Filters for more information on the content used in the example JSON files.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. The following procedures are included in this section:\nUpdate liquid-cooled nodes BMC, FPGA, and node BIOS Update air-cooled compute node BMC, BIOS, iLO 5, and system ROM Update Chassis Management Module (CMM) firmware Update NCN BIOS and BMC firmware with FAS Compute node BIOS workaround for HPE CRAY EX425 NOTE To update switch Controllers (sC) or RouterBMC, refer to the Rosetta documentation.\nUpdate liquid-cooled nodes BMC, FPGA, and node BIOS Update firmware for a liquid-cooled node controller (nC) using FAS. This section includes templates for JSON files that can be used and the procedure for running the update.\nAll of the example JSON files below are set to run a dry-run. Update the overrideDryrun value to true to update the firmware.\nThis procedure updates node controller (nC) firmware.\nLiquid-cooled nodes update procedures Manufacturer: Cray | Device Type: NodeBMC | Target: BMC BMC firmware with FPGA updates require the nodes to be off. If the nodes are not off when the update command is issued, the update will get deferred until the next power cycle of the BMC, which may be a long period of time.\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Olympus node BMCs\u0026#34; } } Manufacturer: Cray | Device Type: NodeBMC | Target: Redstone FPGA IMPORTANT: The nodes themselves must be powered on in order to update the firmware of the Redstone FPGA on the nodes.\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;Node0.AccFPGA0\u0026#34;, \u0026#34;Node1.AccFPGA0\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Node Redstone FPGA\u0026#34; } } Manufacturer: Cray | Device Type : NodeBMC | Target : NodeBIOS There are two nodes that must be updated on each BMC; these have the targets Node0.BIOS and Node1.BIOS. The targets can be run in the same action (as shown in the example) or run separately by only including one target in the action. On larger systems, it is recommended to run as two actions one after each other as the output will be shorter.\nIMPORTANT: The Cray nodeBMC device needs to be updated before the nodeBIOS because the nodeBMC adds a new Redfish field (softwareId) that the NodeX.BIOS update will require. See Update liquid-cooled node firmware for more information. IMPORTANT: The nodes themselves must be powered off in order to update the BIOS on the nodes. The BMC will still have power and will perform the update. If nodes are not off when the update command is issued, it will report as a failed update. IMPORTANT: When the BMC is updated or rebooted after updating the Node0.BIOS and/or Node1.BIOS liquid-cooled nodes, the node BIOS version will not report the new version string until the nodes are powered back on. It is recommended that the Node0/1 BIOS be updated in a separate action, after a BMC update. It is also recommended that the nodes be powered back on after the updates are completed.\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;Node0.BIOS\u0026#34;, \u0026#34;Node1.BIOS\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Node BIOS\u0026#34; } } Cray node BIOS update procedure Create a JSON file using one of the example recipes with the command parameters required for updating the firmware or node BIOS.\nInitiate a dry-run to verify that the firmware can be updated.\nCreate the dry-run session.\nThe overrideDryrun = false value indicates that the command will do a dry run.\nncn# cray fas actions create nodeBMC.json --format toml Example output:\noverrideDryrun = false actionID = \u0026#34;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026#34; Describe the actionID for firmware update dry-run job.\nReplace the actionID value with the string returned in the previous step. In this example, \u0026quot;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026quot; is used.\nncn# cray fas actions describe {actionID} --format toml Example output:\nblockedBy = [] state = \u0026#34;completed\u0026#34; actionID = \u0026#34;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026#34; startTime = \u0026#34;2020-08-31 15:49:44.568271843 +0000 UTC\u0026#34; snapshotID = \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; endTime = \u0026#34;2020-08-31 15:51:35.426714612 +0000 UTC\u0026#34; [command] description = \u0026#34;Update Cray Node BMCs Dryrun\u0026#34; tag = \u0026#34;default\u0026#34; restoreNotPossibleOverride = true timeLimit = 10000 version = \u0026#34;latest\u0026#34; overrideDryrun = false If state = \u0026quot;completed\u0026quot;, the dry-run has found and checked all the nodes. Check the following sections for more information:\nLists the nodes that have a valid image for updating:\n[operationSummary.succeeded] Lists the nodes that will not be updated because they are already at the correct version:\n[operationSummary.noOperation] Lists the nodes that had an error when attempting to update:\n[operationSummary.failed] Lists the nodes that do not have a valid image for updating:\n[operationSummary.noSolution] Update the firmware after verifying that the dry-run worked as expected.\nEdit the JSON file and update the values so an actual firmware update can be run.\nThe following example is for the nodeBMC.json file. Update the following values:\n\u0026#34;overrideDryrun\u0026#34;:true, \u0026#34;description\u0026#34;:\u0026#34;Update Cray Node BMCs\u0026#34; Run the firmware update.\nThe output overrideDryrun = true indicates that an actual firmware update job was created. A new actionID will also be displayed.\nncn# cray fas actions create nodeBMC.json --format toml Example output:\noverrideDryrun = true actionID = \u0026#34;bc40f10a-e50c-4178-9288-8234b336077b\u0026#34; The time it takes for a firmware action to finish varies. It can be a few minutes or over 20 minutes depending on response time.\nThe liquid-cooled node BMC automatically reboots after the BMC firmware has been loaded.\nRetrieve the operationID and verify that the update is complete.\nncn# cray fas actions describe {actionID} --format toml Example output:\n[operationSummary.failed] [[operationSummary.failed.operationKeys]] stateHelper = \u0026#34;unexpected change detected in firmware version. Expected nc.1.3.10-shasta-release.arm.2020-07-21T23:58:22+00:00.d479f59 got: nc.cronomatic-dev.arm.2019-09-24T13:20:24+00:00.9d0f8280\u0026#34; fromFirmwareVersion = \u0026#34;nc.cronomatic-dev.arm.2019-09-24T13:20:24+00:00.9d0f8280\u0026#34; xname = \u0026#34;x1005c6s4b0\u0026#34; target = \u0026#34;BMC\u0026#34; operationID = \u0026#34;e910c6ad-db98-44fc-bdc5-90477b23386f\u0026#34; View more details for an operation using the operationID from the previous step.\nCheck the list of nodes for the failed or completed state.\nncn# cray fas operations describe {operationID} For example:\nncn# cray fas operations describe \u0026#34;e910c6ad-db98-44fc-bdc5-90477b23386f\u0026#34; --format toml Example output:\nfromFirmwareVersion = \u0026#34;nc.cronomatic-dev.arm.2019-09-24T13:20:24+00:00.9d0f8280\u0026#34; fromTag = \u0026#34;\u0026#34; fromImageURL = \u0026#34;\u0026#34; endTime = \u0026#34;2020-08-31 16:40:13.464321212 +0000 UTC\u0026#34; actionID = \u0026#34;bc40f10a-e50c-4178-9288-8234b336077b\u0026#34; startTime = \u0026#34;2020-08-31 16:28:01.228524446 +0000 UTC\u0026#34; fromSemanticFirmwareVersion = \u0026#34;\u0026#34; toImageURL = \u0026#34;\u0026#34; model = \u0026#34;WNC_REV_B\u0026#34; operationID = \u0026#34;e910c6ad-db98-44fc-bdc5-90477b23386f\u0026#34; fromImageID = \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; target = \u0026#34;BMC\u0026#34; toImageID = \u0026#34;39c0e553-281d-4776-b68e-c46a2993485e\u0026#34; toSemanticFirmwareVersion = \u0026#34;1.3.10\u0026#34; refreshTime = \u0026#34;2020-08-31 16:40:13.464325422 +0000 UTC\u0026#34; blockedBy = [] toTag = \u0026#34;\u0026#34; state = \u0026#34;failed\u0026#34; stateHelper = \u0026#34;unexpected change detected in firmware version. Expected nc.1.3.10-shasta-release.arm.2020-07-21T23:58:22+00:00.d479f59 got: nc.cronomatic-dev.arm.2019-09-24T13:20:24+00:00.9d0f8280\u0026#34; deviceType = \u0026#34;NodeBMC\u0026#34; Once firmware and BIOS are updated, the compute nodes can be turned back on.\nUpdate Chassis Management Module firmware Update the Chassis Management Module (CMM) controller (cC) firmware using FAS. This procedure uses the dry-run feature to verify that the update will be successful.\nThe CMM firmware update process also checks and updates the Cabinet Environmental Controller (CEC) firmware.\nExample recipes Manufacturer: Cray | Device Type: ChassisBMC | Target: BMC\nIMPORTANT: Before updating a CMM, make sure all slot and rectifier power is off and the discovery job is stopped (see procedure below).\n{ \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;chassisBMC\u0026#34; ] }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Cray Chassis Controllers\u0026#34; } } Cray chassis BMC update procedure Power off the liquid-cooled chassis slots and chassis rectifiers.\nDisable the hms-discovery Kubernetes cronjob:\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : true }}\u0026#39; Power off all the components. For example, in chassis 0-7, cabinets 1000-1003:\nncn# cray capmc xname_off create --xnames x[1000-1003]c[0-7] --recursive true --continue true This command powers off all the node cards, then all the compute blades, then all the Slingshot switch ASICS, then all the Slingshot switch enclosures, and finally all the chassis enclosures in cabinets 1000-1003.\nWhen power is removed from a chassis, the high-voltage DC rectifiers that support the chassis are powered off. If a component is not populated, the --continue option enables the command to continue instead of returning error messages.\nCreate a JSON file using the example recipe above with the command parameters required for updating the CMM firmware.\nInitiate a dry-run to verify that the firmware can be updated.\nCreate the dry-run session.\nThe overrideDryrun = false value indicates that the command will do a dry-run.\nncn# cray fas actions create chassisBMC.json --format toml Example output:\noverrideDryrun = false actionID = \u0026#34;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026#34; Describe the actionID to see the firmware update dry-run job status.\nReplace the actionID value with the string returned in the previous step. In this example, \u0026quot;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026quot; is used.\nncn# cray fas actions describe {actionID} --format toml Example output:\nblockedBy = [] state = \u0026#34;completed\u0026#34; actionID = \u0026#34;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026#34; startTime = \u0026#34;2020-08-31 15:49:44.568271843 +0000 UTC\u0026#34; snapshotID = \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; endTime = \u0026#34;2020-08-31 15:51:35.426714612 +0000 UTC\u0026#34; [command] description = \u0026#34;Update Cray Chassis Management Module controllers Dryrun\u0026#34; tag = \u0026#34;default\u0026#34; restoreNotPossibleOverride = true timeLimit = 10000 version = \u0026#34;latest\u0026#34; overrideDryrun = false If state = \u0026quot;completed\u0026quot;, the dry-run has found and checked all the nodes. Check the following sections for more information:\nLists the nodes that have a valid image for updating:\n[operationSummary.succeeded] Lists the nodes that will not be updated because they are already at the correct version:\n[operationSummary.noOperation] Lists the nodes that had an error when attempting to update:\n[operationSummary.failed] Lists the nodes that do not have a valid image for updating:\n[operationSummary.noSolution] Update the firmware after verifying that the dry-run worked as expected.\nEdit the JSON file and update the values so an actual firmware update can be run.\nThe following example is for the chassisBMC.json file. Update the following values:\n\u0026#34;overrideDryrun\u0026#34;:true, \u0026#34;description\u0026#34;:\u0026#34;Update Cray Chassis Management Module controllers\u0026#34; Run the firmware update.\nThe output overrideDryrun = true indicates that an actual firmware update job was created. A new actionID will also be displayed.\nncn# cray fas actions create chassisBMC.json --format toml Example output:\noverrideDryrun = true actionID = \u0026#34;bc40f10a-e50c-4178-9288-8234b336077b\u0026#34; The time it takes for a firmware update varies. It can be a few minutes or over 20 minutes depending on response time.\nRestart the hms-discovery cronjob.\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : false }}\u0026#39; The hms-discovery cronjob will run within 5 minutes of being unsuspended and start powering on the chassis enclosures, switches, and compute blades. If components are not being powered back on, then power them on manually:\nncn# cray capmc xname_on create --xnames x[1000-1003]c[0-7]r[0-7],x[1000-1003]c[0-7]s[0-7] --prereq true --continue true The --prereq option ensures all required components are powered on first. The --continue option allows the command to complete in systems without fully populated hardware.\nBring up the Slingshot Fabric.\nRefer to the following documentation on the HPE Customer Support Center for more information on how to bring up the Slingshot Fabric:\nThe HPE Slingshot Operations Guide PDF for HPE Cray EX systems. The HPE Slingshot Troubleshooting Guide PDF. After the components have powered on, boot the nodes using the Boot Orchestration Services (BOS).\nUpdate air-cooled compute node BMC, BIOS, iLO 5, and system ROM Firmware and BIOS for Gigabyte and HPE compute nodes can be updated with FAS. This section includes templates for JSON files that can be used for updates, and the procedure for running the updates.\nAll of the example JSON files below are set to run a dry-run. Update the overrideDryrun value to true to update the firmware.\nAfter updating the BIOS or System ROM, the compute node will need to be rebooted before the new version will be displayed in the Redfish output.\nThis procedure updates node controller (nC) firmware.\nGigabyte Device Type: NodeBMC | Target: BMC\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ], \u0026#34;xnames\u0026#34;: [ \u0026#34;x3000c0s1b0\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;gigabyte\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 4000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Gigabyte node BMCs\u0026#34; } } IMPORTANT: The timeLimit is 4000 because the Gigabytes can take a lot longer to update.\nTroubleshooting:\nA node may fail to update with the output:\nstateHelper = \u0026#34;Firmware Update Information Returned Downloading – See /redfish/v1/UpdateService\u0026#34; FAS has incorrectly marked this node as failed. It most likely will complete the update successfully.\nTo resolve this issue, do either of the following actions:\nCheck the update status by looking at the Redfish FirmwareInventory (/redfish/v1/UpdateService/FirmwareInventory/BMC). Rerun FAS to verify that the BMC firmware was updated. Make sure to wait for the current firmware to be updated before starting a new FAS action on the same node.\nDevice Type: NodeBMC | Target: BIOS\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ], \u0026#34;xnames\u0026#34;: [ \u0026#34;x3000c0s1b0\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;gigabyte\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BIOS\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 4000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Gigabyte node BIOS\u0026#34; } } IMPORTANT: The timeLimit is 4000 because the Gigabytes can take a lot longer to update.\nHPE Device Type: NodeBMC | Target: iLO 5 aka BMC\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ], \u0026#34;xnames\u0026#34;: [ \u0026#34;x3000c0s1b0\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;hpe\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;iLO 5\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of HPE node iLO 5\u0026#34; } } Device Type: NodeBMC | Target: System ROM aka BIOS\nIMPORTANT: If updating the System ROM of an NCN, the NTP and DNS server values will be lost and must be restored. For NCNs other than ncn-m001 this can be done using the /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh script. Use the -h option to get a list of command line options required to restore the NTP and DNS values. See Configure DNS and NTP on Each BMC.\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;NodeBMC\u0026#34; ], \u0026#34;xnames\u0026#34;: [ \u0026#34;x3000c0s1b0\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;hpe\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;System ROM\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of HPE node system rom\u0026#34; } } HPE node system ROM (BIOS) update procedure Create a JSON file using one of the example recipes with the command parameters required for updating the firmware or node BIOS.\nInitiate a dry-run to verify that the firmware can be updated.\nCreate the dry-run session.\nThe overrideDryrun = false value indicates that the command will do a dry run.\nncn# cray fas actions create nodeBMC.json --format toml Example output:\noverrideDryrun = false actionID = \u0026#34;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026#34; Describe the actionID for firmware update dry-run job.\nReplace the actionID value with the string returned in the previous step. In this example, \u0026quot;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026quot; is used.\nncn# cray fas actions describe {actionID} --format toml Example output:\nblockedBy = [] state = \u0026#34;completed\u0026#34; actionID = \u0026#34;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026#34; startTime = \u0026#34;2020-08-31 15:49:44.568271843 +0000 UTC\u0026#34; snapshotID = \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; endTime = \u0026#34;2020-08-31 15:51:35.426714612 +0000 UTC\u0026#34; [command] description = \u0026#34;Update of HPE Node iLO5\u0026#34; tag = \u0026#34;default\u0026#34; restoreNotPossibleOverride = true timeLimit = 10000 version = \u0026#34;latest\u0026#34; overrideDryrun = false If state = \u0026quot;completed\u0026quot;, the dry-run has found and checked all the nodes. Check the following sections for more information:\nLists the nodes that have a valid image for updating:\n[operationSummary.succeeded] Lists the nodes that will not be updated because they are already at the correct version:\n[operationSummary.noOperation] Lists the nodes that had an error when attempting to update:\n[operationSummary.failed] Lists the nodes that do not have a valid image for updating:\n[operationSummary.noSolution] Update the firmware after verifying that the dry-run worked as expected.\nEdit the JSON file and update the values so an actual firmware update can be run.\nThe following example is for the nodeBMC.json file. Update the following values:\n\u0026#34;overrideDryrun\u0026#34;:true, \u0026#34;description\u0026#34;:\u0026#34;Update of HPE node iLO 5\u0026#34; Run the firmware update.\nThe returned overrideDryrun = true indicates that an actual firmware update job was created. A new actionID will also be returned.\nncn# cray fas actions create nodeBMC.json --format toml Example output:\noverrideDryrun = true actionID = \u0026#34;bc40f10a-e50c-4178-9288-8234b336077b\u0026#34; The time it takes for a firmware action to finish varies. It can be a few minutes or over 20 minutes depending on response time.\nThe air-cooled node BMC automatically reboots after the BMC or iLO 5 firmware has been loaded.\nRetrieve the operationID and verify that the update is complete.\nncn# cray fas actions describe {actionID} --format toml Example output:\n[operationSummary.failed] [[operationSummary.failed.operationKeys]] stateHelper = \u0026#34;unexpected change detected in firmware version. Expected 2.46 May 11 2021 got: 2.32 Apr 27 2020\u0026#34; fromFirmwareVersion = \u0026#34;2.32 Apr 27 2020\u0026#34; xname = \u0026#34;x1005c6s4b0\u0026#34; target = \u0026#34;iLO 5\u0026#34; operationID = \u0026#34;e910c6ad-db98-44fc-bdc5-90477b23386f\u0026#34; View more details for an operation using the operationID from the previous step.\nCheck the list of nodes for the failed or completed state.\nncn# cray fas operations describe {operationID} For example:\nncn# cray fas operations describe \u0026#34;e910c6ad-db98-44fc-bdc5-90477b23386f\u0026#34; --format toml Example output:\nfromFirmwareVersion = \u0026#34;2.32 Apr 27 2020\u0026#34; fromTag = \u0026#34;\u0026#34; fromImageURL = \u0026#34;\u0026#34; endTime = \u0026#34;2020-08-31 16:40:13.464321212 +0000 UTC\u0026#34; actionID = \u0026#34;bc40f10a-e50c-4178-9288-8234b336077b\u0026#34; startTime = \u0026#34;2020-08-31 16:28:01.228524446 +0000 UTC\u0026#34; fromSemanticFirmwareVersion = \u0026#34;\u0026#34; toImageURL = \u0026#34;\u0026#34; model = \u0026#34;ProLiant DL325 Gen10 Plus\u0026#34; operationID = \u0026#34;e910c6ad-db98-44fc-bdc5-90477b23386f\u0026#34; fromImageID = \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; target = \u0026#34;iLO 5\u0026#34; toImageID = \u0026#34;39c0e553-281d-4776-b68e-c46a2993485e\u0026#34; toSemanticFirmwareVersion = \u0026#34;2.46.0\u0026#34; refreshTime = \u0026#34;2020-08-31 16:40:13.464325422 +0000 UTC\u0026#34; blockedBy = [] toTag = \u0026#34;\u0026#34; state = \u0026#34;failed\u0026#34; stateHelper = \u0026#34;unexpected change detected in firmware version. Expected 2.46 May 11 2021 got: 2.32 Apr 27 2020\u0026#34; deviceType = \u0026#34;NodeBMC\u0026#34; Update Non-Compute Node (NCN) BIOS and BMC firmware Gigabyte and HPE non-compute nodes (NCNs) firmware can be updated with FAS. This section includes templates for JSON files that can be used to update firmware with the cray fas actions create command.\nAfter creating the JSON file for the device being upgraded, use the following command to run the FAS job:\nncn# cray fas actions create CUSTOM_DEVICE_PARAMETERS.json All of the example JSON files below are set to run a dry-run. Update the overrideDryrun value to True to update the firmware.\nWARNING: Rebooting more than one NCN at a time MAY cause system instability. Be sure to follow the correct process for updating NCNs. Firmware updates have the capacity to harm the system.\nAfter updating the BIOS, the NCN will need to be rebooted. Follow the Reboot NCNs procedure.\nDue to networking, FAS cannot update ncn-m001. See Updating Firmware on ncn-m001\nGigabyte NCNs Device Type: NodeBMC | Target: BMC\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ], \u0026#34;xnames\u0026#34;: [ \u0026#34;x3000c0s1b0\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;gigabyte\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 4000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Gigabyte node BMCs\u0026#34; } } IMPORTANT: The timeLimit is 4000 because the Gigabytes can take a lot longer to update.\nTroubleshooting: It may report that a node failed to update with the output: stateHelper = \u0026quot;Firmware Update Information Returned Downloading – See /redfish/v1/UpdateService\u0026quot; FAS has incorrectly marked this node as failed. It most likely will complete the update successfully. To resolve this issue, do either of the following actions:\nCheck the update status by looking at the Redfish FirmwareInventory (/redfish/v1/UpdateService/FirmwareInventory/BMC) Rerun FAS to verify that the BMC firmware was updated. Make sure you have waited for the current firmware to be updated before starting a new FAS action on the same node.\nDevice Type: NodeBMC | Target: BIOS\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ], \u0026#34;xnames\u0026#34;: [ \u0026#34;x3000c0s1b0\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;gigabyte\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BIOS\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 4000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Gigabyte node BIOS\u0026#34; } } IMPORTANT: The timeLimit is 4000 because the Gigabytes can take a lot longer to update.\nHPE NCNs Device Type: NodeBMC | Target: iLO 5 aka BMC\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ], \u0026#34;xnames\u0026#34;: [ \u0026#34;x3000c0s1b0\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;hpe\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;iLO 5\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of HPE node iLO 5\u0026#34; } } Device Type: NodeBMC | Target: System ROM aka BIOS\nIMPORTANT: If updating the System ROM of an NCN, the NTP and DNS server values will be lost and must be restored. For NCNs other than ncn-m001 this can be done using the /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh script. Use the -h option to get a list of command line options required to restore the NTP and DNS values. See Configure DNS and NTP on Each BMC.\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;NodeBMC\u0026#34; ], \u0026#34;xnames\u0026#34;: [ \u0026#34;x3000c0s1b0\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;hpe\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;System ROM\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of HPE node system rom\u0026#34; } } The NCN must be rebooted after updating the BIOS firmware. Follow the Reboot NCNs procedure.\nHPE node system ROM (BIOS) procedure for NCN For HPE NCNs, check the DNS servers by running the script /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh ilo -H XNAME -s. Replace XNAME with the xname of the NCN BMC. See Configure DNS and NTP on Each BMC for more information. Run a dryrun for all NCNs first to determine which NCNs and targets need updating. For each NCN requiring updates to target BMC or iLO 5: NOTE Update of BMC and iLO 5 will not affect the nodes.\nUnlock the NCN BMC. See Lock and Unlock Management Nodes. Run the FAS action on the NCN. Relock the NCN BMC. See Lock and Unlock Management Nodes. For each NCN requiring updates to target BIOS or System ROM: Unlock the NCN BMC. See Lock and Unlock Management Nodes. Run the FAS action on the NCN. Reboot the node. See Reboot NCNs. For HPE NCNs, run the script /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh. See Configure DNS and NTP on Each BMC. Relock the NCN BMC. See Lock and Unlock Management Nodes. Compute node BIOS workaround for HPE CRAY EX425 Correct an issue where the model of the liquid-cooled compute node BIOS is the incorrect name. The name has changed from WNC-ROME to HPE CRAY EX425 or HPE CRAY EX425 (ROME).\nPrerequisites:\nThe system is running HPE Cray EX release v1.4 or higher.\nA firmware upgrade has been done following Update liquid-cooled compute node BIOS firmware.\nThe result of the upgrade is that the NodeX.BIOS has failed as noSolution and the stateHelper field for the operation states is \u0026quot;No Image Available\u0026quot;. The BIOS in question is running a version less than or equal to 1.2.5 as reported by Redfish or described by the noSolution operation in FAS. The hardware model reported by Redfish is wnc-rome, which is now designated as HPE CRAY EX425.\nIf the Redfish model is different (ignoring casing) and the blades in question are not Windom, contact customer support. To find the model reported by Redfish, run the following:\nncn# cray fas operations describe {operationID} --format json Example output:\n{ \u0026#34;operationID\u0026#34;:\u0026#34;102c949f-e662-4019-bc04-9e4b433ab45e\u0026#34;, \u0026#34;actionID\u0026#34;:\u0026#34;9088f9a2-953a-498d-8266-e2013ba2d15d\u0026#34;, \u0026#34;state\u0026#34;:\u0026#34;noSolution\u0026#34;, \u0026#34;stateHelper\u0026#34;:\u0026#34;No Image available\u0026#34;, \u0026#34;startTime\u0026#34;:\u0026#34;2021-03-08 13:13:14.688500503 +0000 UTC\u0026#34;, \u0026#34;endTime\u0026#34;:\u0026#34;2021-03-08 13:13:14.688508333 +0000 UTC\u0026#34;, \u0026#34;refreshTime\u0026#34;:\u0026#34;2021-03-08 13:13:14.722345901 +0000 UTC\u0026#34;, \u0026#34;expirationTime\u0026#34;:\u0026#34;2021-03-08 15:59:54.688500753 +0000 UTC\u0026#34;, \u0026#34;xname\u0026#34;:\u0026#34;x9000c1s0b0\u0026#34;, \u0026#34;deviceType\u0026#34;:\u0026#34;NodeBMC\u0026#34;, \u0026#34;target\u0026#34;:\u0026#34;Node1.BIOS\u0026#34;, \u0026#34;targetName\u0026#34;:\u0026#34;Node1.BIOS\u0026#34;, \u0026#34;manufacturer\u0026#34;:\u0026#34;cray\u0026#34;, \u0026#34;model\u0026#34;:\u0026#34;WNC-Rome\u0026#34;, \u0026#34;softwareId\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;fromImageID\u0026#34;:\u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;fromSemanticFirmwareVersion\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;:\u0026#34;wnc.bios-1.2.5\u0026#34;, \u0026#34;fromImageURL\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;fromTag\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;toImageID\u0026#34;:\u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;toSemanticFirmwareVersion\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;toFirmwareVersion\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;toImageURL\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;toTag\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;blockedBy\u0026#34;:[ ] } The model in this example is WNC-Rome and the firmware version currently running is wnc.bios-1.2.5.\nCompute node BIOS workaround for HPE CRAY EX425 procedure Search for a FAS image record with cray as the manufacturer, Node1.BIOS as the target, and HPE CRAY EX425 as the model.\nncn# cray fas images list --format json | jq \u0026#39;.images[] | select(.manufacturer==\u0026#34;cray\u0026#34;) \\ | select(.target==\u0026#34;Node1.BIOS\u0026#34;) | select(any(.models[]; contains(\u0026#34;EX425\u0026#34;)))\u0026#39; Example output:\n{ \u0026#34;imageID\u0026#34;: \u0026#34;e23f5465-ed29-4b18-9389-f8cf0580ca60\u0026#34;, \u0026#34;createTime\u0026#34;: \u0026#34;2021-03-04T00:04:05Z\u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;nodeBMC\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34;, \u0026#34;models\u0026#34;: [ \u0026#34;HPE CRAY EX425\u0026#34; ], \u0026#34;softwareIds\u0026#34;: [ \u0026#34;bios.ex425..\u0026#34; ], \u0026#34;target\u0026#34;: \u0026#34;Node1.BIOS\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;default\u0026#34; ], \u0026#34;firmwareVersion\u0026#34;: \u0026#34;ex425.bios-1.4.3\u0026#34;, \u0026#34;semanticFirmwareVersion\u0026#34;: \u0026#34;1.4.3\u0026#34;, \u0026#34;pollingSpeedSeconds\u0026#34;: 30, \u0026#34;s3URL\u0026#34;: \u0026#34;s3:/fw-update/2227040f7c7d11eb9fa00e2f2e08fd5d/ex425.bios-1.4.3.tar.gz\u0026#34; } Take note of the returned imageID value to use in the next step.\nCreate a JSON file to override the existing image with the corrected values.\nIMPORTANT: The imageID must be changed to match the identified imageID in the previous step.\n{ \u0026#34;stateComponentFilter\u0026#34;:{ \u0026#34;deviceTypes\u0026#34;:[ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;:{ \u0026#34;manufacturer\u0026#34;:\u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;:{ \u0026#34;targets\u0026#34;:[ \u0026#34;Node0.BIOS\u0026#34;, \u0026#34;Node1.BIOS\u0026#34; ] }, \u0026#34;imageFilter\u0026#34;:{ \u0026#34;imageID\u0026#34;:\u0026#34;e23f5465-ed29-4b18-9389-f8cf0580ca60\u0026#34;, \u0026#34;overrideImage\u0026#34;:true }, \u0026#34;command\u0026#34;:{ \u0026#34;version\u0026#34;:\u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;:\u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;:true, \u0026#34;restoreNotPossibleOverride\u0026#34;:true, \u0026#34;timeLimit\u0026#34;:1000, \u0026#34;description\u0026#34;:\u0026#34; upgrade of Node BIOS\u0026#34; } } Run a firmware upgrade using the updated parameters defined in the new JSON file.\nncn# cray fas actions create UPDATED_COMMAND.json Get a high-level summary of the job to verify the changes corrected the issue.\nUse the returned actionID from the cray fas actions create command.\nncn# cray fas actions create UPDATED_COMMAND.json "
},
{
	"uri": "/docs-csm/en-12/operations/conman/establish_a_serial_connection_to_ncns/",
	"title": "Establish a Serial Connection to NCNs",
	"tags": [],
	"description": "",
	"content": "Establish a Serial Connection to NCNs The ConMan pod can be used to establish a serial console connection with each non-compute node (NCN) in the system.\nIn the scenario of a power down or reboot of an NCN worker, one must first determine if any cray-console pods are running on that NCN. It is important to move cray-console pods to other worker nodes before rebooting or powering off a worker node to minimize disruption in console logging. If a brief interruption in console logging and interactive access is acceptable while the NCN worker is being drained, then the evacuation may be skipped.\nIf a cray-console-node pod is running on a worker node when it is powered off or rebooted, then access to its associated consoles will be unavailable until one of the following things happens:\nthe worker node comes back up and the cray-console-node pod begins running on it. the cray-console-node pod is terminated and comes up on another worker node. the cray-console-operator pod assigns the associated consoles to a different cray-console-node pod. Prerequisites The user performing these procedures needs to have access permission to the cray-console-operator and cray-console-node pods.\nConnection procedure Find the cray-console-operator pod.\nncn-mw# OP_POD=$(kubectl get pods -n services \\ -o wide|grep cray-console-operator|awk \u0026#39;{print $1}\u0026#39;) ncn-mw# echo $OP_POD Example output:\ncray-console-operator-6cf89ff566-kfnjr Find the cray-console-node pod that is connecting with the console.\nncn-mw# NODE_POD=$(kubectl -n services exec $OP_POD -c cray-console-operator -- sh -c \\ \u0026#34;/app/get-node $XNAME\u0026#34; | jq .podname | sed \u0026#39;s/\u0026#34;//g\u0026#39;) ncn-mw# echo $NODE_POD Example output:\ncray-console-node-1 Check which NCN worker node the cray-console-node pod is running on.\nncn-mw# kubectl -n services get pods -o wide | grep $NODE_POD Example output:\ncray-console-node-1 3/3 Running 0 3h55m 10.42.0.12 ncn-w010 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; If the pod is running on the node that is going to be rebooted, then the interactive session and logging will be interrupted while the NCN worker is drained and the pods are all migrated to different NCN workers. To maintain an interactive console session, the cray-console-node pod must be moved:\nCordon the NCN worker node to suspend scheduling, then delete the pod.\nncn-mw# WNODE=ncn-wxxx ncn-mw# kubectl cordon $WNODE ncn-mw# kubectl -n services delete pod $NODE_POD Wait for the pod to restart on another NCN worker.\nRepeat the previous step to find if this node is now being monitored by a different cray-console-node pod.\nNOTE: If desiring to minimize the disruption to console logging and interaction, then follow the Evacuation procedure to remove all console logging services prior to draining this node.\nEstablish a serial console session with the desired NCN.\nncn-mw# kubectl -n services exec -it $NODE_POD -- conman -j $XNAME The console session log files for each NCN are located in a shared volume in the cray-console-node pods. In those pods, the log files are in the /var/log/conman/ directory and are named console.\u0026lt;xname\u0026gt;.\nExit the connection to the console by entering \u0026amp;..\nEvacuation procedure In order to avoid losing data while monitoring a reboot or power down of a worker node, first follow this procedure to evacuate the target worker node of its pods.\nNOTE: If multiple cray-console-node pods are present on the worker being evacuated, the deployment may be edited to add anti-affinity to the Kubernetes scheduling. Follow the directions in Multiple Console Node Pods on the Same Worker.\nSet the WNODE variable to the name of the worker node being evacuated.\nModify the following example to reflect the actual worker node number.\nncn-mw# WNODE=ncn-wxxx Cordon the node so that rescheduled pods do not end up back on the same node.\nncn-mw# kubectl cordon $WNODE Find all cray-console pods that need to be migrated.\nThis includes cray-console-node, cray-console-data (but not its Postgres pods), and cray-console-operator.\nncn-mw# kubectl get pods -n services -l \u0026#39;app.kubernetes.io/name in (cray-console-node, cray-console-data, cray-console-operator)\u0026#39; \\ --field-selector spec.nodeName=$WNODE | awk \u0026#39;{print $1}\u0026#39; Example output:\ncray-console-operator-6cf89ff566-kfnjr Delete the cray-console-operator and cray-console-data pods listed in the previous step.\nIf none were listed, then skip this step.\nDelete the pods.\nncn-mw# for POD in $(kubectl get pods -n services -l \u0026#39;app.kubernetes.io/name in (cray-console-data, cray-console-operator)\u0026#39; \\ --field-selector spec.nodeName=$WNODE | awk \u0026#39;{print $1}\u0026#39;); do kubectl -n services delete pod $POD done Wait for the console-operator and console-data pods to be re-scheduled on other nodes.\nRun the following command until both deployments show 2/2 pods are ready.\nncn-mw# kubectl -n services get deployment | grep cray-console Example output:\ncray-console-data 2/2 1 1 1m cray-console-operator 2/2 1 1 1m Delete any cray-console-node pods listed in the earlier step.\nIf none were listed, then skip this step.\nDelete the pods.\nncn-mw# for POD in $(kubectl get pods -n services -l \u0026#39;app.kubernetes.io/name=cray-console-node\u0026#39; --field-selector spec.nodeName=$WNODE | awk \u0026#39;{print $1}\u0026#39;); do kubectl -n services delete pod $POD done Wait for the console-node pods to be re-scheduled on other nodes.\nRun the following command until all pods show ready.\nncn-mw# kubectl -n services get statefulset cray-console-node Example output:\nNAME READY AGE cray-console-node 3/3 1m After the node has been rebooted and can accept cray-console pods again, remove the node cordon.\nncn-mw# kubectl uncordon $WNODE "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/cfs_global_options/",
	"title": "CFS Global Options",
	"tags": [],
	"description": "",
	"content": "CFS Global Options The Configuration Framework Service (CFS) provides a global service options endpoint for modifying the base configuration of the service itself.\nView the options with the following command:\nncn# cray cfs options list --format json Example output:\n{ \u0026#34;additionalInventoryUrl\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;batchSize\u0026#34;: 25, \u0026#34;batchWindow\u0026#34;: 60, \u0026#34;batcherCheckInterval\u0026#34;: 10, \u0026#34;defaultAnsibleConfig\u0026#34;: \u0026#34;cfs-default-ansible-cfg\u0026#34;, \u0026#34;defaultBatcherRetryPolicy\u0026#34;: 1, \u0026#34;defaultPlaybook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;hardwareSyncInterval\u0026#34;: 10, \u0026#34;sessionTTL\u0026#34;: \u0026#34;7d\u0026#34; } The following are the CFS global options:\nadditionalInventoryUrl\nA Git clone URL to supply additional inventory content to all CFS sessions.\nSee Manage Multiple Inventories in a Single Location for more information.\nbatchSize\nThis option determines the maximum number of components that will be included in each session created by CFS Batcher.\nSee Configuration Management with the CFS Batcher for more information.\nbatchWindow\nThis option sets the number of seconds that CFS batcher will wait before scheduling a CFS session when the number of components needing configuration has not reached the batchSize limit.\nSee Configuration Management with the CFS Batcher for more information.\nbatcherCheckInterval\nThis option sets how often CFS batcher checks for components waiting to be configured. This value must be lower than batchWindow.\nSee Configuration Management with the CFS Batcher for more information.\ndefaultBatcherRetryPolicy\nWhen a component (node) requiring configuration fails to configure from a previous configuration session launched by CFS Batcher, the error is logged. defaultBatcherRetryPolicy is the maximum number of failed configurations allowed per component before CFS Batcher will stop attempts to configure the component.\nSee Configuration Management with the CFS Batcher for more information.\ndefaultAnsibleConfig\nSee Set the ansible.cfg for a Session for more information.\ndefaultPlaybook\nUse this value when no playbook is specified in a configuration layer.\nhardwareSyncInterval\nThe number of seconds between checks to the Hardware State Manager (HSM) for new hardware additions to the system. When new hardware is registered with HSM, CFS will add it as a component.\nSee Configuration Management of System Components for more information.\nThe default values for all CFS global options can be modified with the cray cfs options update command.\n"
},
{
	"uri": "/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_unmet_conditions/",
	"title": "Troubleshoot a Failed CRUS Session Because of Unmet Conditions",
	"tags": [],
	"description": "",
	"content": "Troubleshoot a Failed CRUS Session Because of Unmet Conditions Note: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality. See Deprecated features.\nIf a CRUS session has any unmet conditions, adding or fixing them will cause the session to continue from wherever it got stuck. Updating other parts of the system to meet the required conditions of a CRUS session will unblock the upgrade session.\nThe following are examples of unmet conditions:\nUndefined groups in the Hardware State Manager (HSM). No predefined Boot Orchestration Service (BOS) session template exists that describes the desired states of the nodes being upgraded. Prerequisites A Compute Rolling Upgrade Service (CRUS) session was run and failed to complete. The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray Command Line Interface. Procedure View the details for the CRUS session that failed.\nncn# cray crus session describe CRUS_UPGRADE_ID --format toml Example output:\napi_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-node-group\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [ \u0026#34;Processing step 0 in stage STARTING failed - failed to obtain Node Group named \u0026#39;failed-node-group\u0026#39; - {\u0026#34;type\u0026#34;:\u0026#34;about:blank\u0026#34;,\u0026#34;title\u0026#34;:\u0026#34;Not Found\u0026#34;,\u0026#34;detail\u0026#34;:\u0026#34;No such group: failed-node-group\u0026#34;,\u0026#34;status\u0026#34;:404}\\n[404]\u0026#34;,] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;2c7fdce6-0047-4421-9676-4301d411d14e\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;dummy-boot-template\u0026#34; upgrading_label = \u0026#34;dummy-node-group\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; The messages value returned in the output will provide details explaining where the job failed. In this example, there is a note stating the failed node group could not be obtained. This implies that the user forgot to create the failed node group before starting the job.\nCreate a new node group for the missing group.\nFollowing the example in the previous step, the failed node group needs to be created.\nncn# cray hsm groups create --label failed-node-group --format toml Example output:\n[[results]] URI = \u0026#34;/hsm/v2/groups/failed-node-group\u0026#34; View the details for the CRUS session again to see if the job started.\nncn# cray crus session describe CRUS_UPGRADE_ID --format toml Example output:\napi_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-node-group\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [ \u0026#34;Processing step 0 in stage STARTING failed - failed to obtain Node Group named \u0026#39;failed-node-group\u0026#39; - {\u0026#34;type\u0026#34;:\u0026#34;about:blank\u0026#34;,\u0026#34;title\u0026#34;:\u0026#34;Not Found\u0026#34;,\u0026#34;detail\u0026#34;:\u0026#34;No such group: failed-node-group\u0026#34;,\u0026#34;status\u0026#34;:404}\\n[404]\u0026#34;, \u0026#34;Quiesce requested in step 0: moving to QUIESCING\u0026#34;, \u0026#34;All nodes quiesced in step 0: moving to QUIESCED\u0026#34;, \u0026#34;Began the boot session for step 0: moving to BOOTING\u0026#34;,] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;2c7fdce6-0047-4421-9676-4301d411d14e\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;dummy-boot-template\u0026#34; upgrading_label = \u0026#34;dummy-node-group\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; The messages value states that the job has resumed now that the error has been fixed.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/boot_uans/",
	"title": "Boot UANs",
	"tags": [],
	"description": "",
	"content": "Boot UANs Boot UANs with an image so that they are ready for user logins.\nPrerequisites UAN boot images and a BOS session template have been created. See Create UAN Boot Images.\nProcedure Create a BOS session to boot the UAN nodes.\nncn-mw# cray bos session create --template-uuid uan-sessiontemplate-PRODUCT_VERSION \\ --operation reboot --format json | tee session.json Example output:\n{ \u0026#34;links\u0026#34;: [ { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d\u0026#34;, \u0026#34;jobId\u0026#34;: \u0026#34;boa-89680d0a-3a6b-4569-a1a1-e275b71fce7d\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;session\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d/status\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;status\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; } ], \u0026#34;operation\u0026#34;: \u0026#34;reboot\u0026#34;, \u0026#34;templateUuid\u0026#34;: \u0026#34;uan-sessiontemplate-PRODUCT_VERSION\u0026#34; } The first attempt to reboot the UANs will most likely fail. The UAN boot may hang and the UAN console will look similar to the following:\n2021-03-19 01:32:41 dracut-initqueue[420]: DVS: node map generated. 2021-03-19 01:32:41 katlas: init_module: katlas loaded, currently disabled 2021-03-19 01:32:41 2021-03-19 01:32:41 DVS: Revision: kbuild Built: Mar 17 2021 @ 15:14:05 against LNet 2.12.4 2021-03-19 01:32:41 DVS debugfs: Revision: kbuild Built: Mar 17 2021 @ 15:14:05 against LNet 2.12.4 2021-03-19 01:32:41 dracut-initqueue[420]: DVS: loadDVS: successfully added 10 new nodes into map. 2021-03-19 01:32:41 ed dvsproc module. 2021-03-19 01:32:41 DVS: message size checks complete. 2021-03-19 01:32:41 dracut-initqueuedvs_thread_generator: Watching pool DVS-IPC_msg (id 0) 2021-03-19 01:32:41 [420]: DVS: loaded dvs module. 2021-03-19 01:32:41 dracut-initqueue[420]: mount is: /opt/cray/cps-utils/bin/cpsmount.sh -a api-gw-service-nmn.local -t dvs -T 300 -i nmn0 -e 3116cf653e84d265cf8da94956f34d9e-181 s3://boot-images/763213c7-3d5f-4f2f-9d8a-ac6086583f43/rootfs /tmp/cps 2021-03-19 01:32:41 dracut-initqueue[420]: 2021/03/19 01:31:01 cpsmount_helper Version: 1.0.0 2021-03-19 01:32:47 dracut-initqueue[420]: 2021/03/19 01:31:07 Adding content: s3://boot-images/763213c7-3d5f-4f2f-9d8a-ac6086583f43/rootfs 3116cf653e84d265cf8da94956f34d9e-181 dvs 2021-03-19 01:33:02 dracut-initqueue[420]: 2021/03/19 01:31:22 WARN: readyForMount=false type=dvs ready=0 total=2 2021-03-19 01:33:18 dracut-initqueue[420]: 2021/03/19 01:31:38 WARN: readyForMount=false type=dvs ready=0 total=2 2021-03-19 01:33:28 dracut-initqueue[420]: 2021/03/19 01:31:48 2 dvs servers [10.252.1.7 10.252.1.8] If this occurs, repeat the BOS command.\nRetrieve the BOS session ID from the output of the cray bos session create command in the previous step.\nncn-mw# BOS_SESSION=$(jq -r \u0026#39;.links[] | select(.rel==\u0026#34;session\u0026#34;) | .href\u0026#39; session.json | cut -d \u0026#39;/\u0026#39; -f4) ; echo $BOS_SESSION Example output:\n89680d0a-3a6b-4569-a1a1-e275b71fce7d Retrieve the Boot Orchestration Agent (BOA) Kubernetes job name for the BOS session.\nncn-mw# BOA_JOB_NAME=$(cray bos session describe $BOS_SESSION --format json | jq -r .boa_job_name) Retrieve the Kubernetes pod name for the BOA assigned to run this session.\nncn-mw# BOA_POD=$(kubectl get pods -n services -l job-name=$BOA_JOB_NAME --no-headers -o custom-columns=\u0026#34;:metadata.name\u0026#34;) View the logs for the BOA to track session progress.\nncn-mw# kubectl logs -f -n services $BOA_POD -c boa List the CFS sessions started by the BOA.\nSkip this step if CFS was not enabled in the boot session template used to boot the UANs.\nIf CFS was enabled in the boot session template, the BOA will initiate a CFS session.\nIn the following command, pending and complete are also valid statuses to filter on.\nncn-mw# cray cfs sessions list --tags bos_session=$BOS_SESSION --status running --format json Verify that the Day Zero patch was applied correctly during Create UAN Boot Images.\nSkip this step if the patch has already been verified.\nSSH into a newly booted UAN.\nncn-mw# ssh uan01-nmn Verify that the DVS RPM versions match what exists in the 1.4.0-p2/rpms directory.\nuan01# rpm -qa | grep \u0026#39;cray-dvs.*2.12\u0026#39; | sort Example output:\ncray-dvs-compute-2.12_4.0.102-7.0.1.0_8.1__g30d29e7a.x86_64 cray-dvs-devel-2.12_4.0.102-7.0.1.0_8.1__g30d29e7a.x86_64 cray-dvs-kmp-cray_shasta_c-2.12_4.0.102_k4.12.14_197.78_9.1.58-7.0.1.0_8.1__g30d29e7a.x86_64 Log out of the UAN.\nuan01# exit "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/configure_end-user_uai_classes_for_broker_mode/",
	"title": "Configure End-User UAI Classes for Broker Mode",
	"tags": [],
	"description": "",
	"content": "Configure End-User UAI Classes for Broker Mode Each UAI broker will create and manage a single class of End-User UAIs. A site using the Broker Mode of UAI management must create a Brokered End-User UAI Class for each distinct type of End-User UAI it wants served by a Broker UAI. Information on what should be configured for a Brokered End-User UAI Class can be found in UAI Classes.\nTop: User Access Service (UAS)\nNext Topic: Configure a Broker UAI class\n"
},
{
	"uri": "/docs-csm/en-12/operations/csm_product_management/configure_non-compute_nodes_with_cfs/",
	"title": "Configure Non-Compute Nodes with CFS",
	"tags": [],
	"description": "",
	"content": "Configure Non-Compute Nodes with CFS Non-compute node (NCN) personalization applies post-boot configuration to the HPE Cray EX management nodes. Several HPE Cray EX product environments outside of CSM require NCN personalization to function. Consult the manual for each product to configure them on NCNs by referring to the HPE Cray EX System Software Getting Started Guide (S-8000) 22.07 on the HPE Customer Support Center.\nThis procedure defines the NCN personalization process for the CSM product using the Configuration Framework Service (CFS).\nDuring a fresh install, carry out these procedures in order. Later, individual procedures may be re-run as needed.\nSet up passwordless SSH Option 1: Use the CSM-provided SSH keys Option 2: Provide custom SSH keys Option 3: Disable CSM-provided passwordless SSH Restore CSM-provided SSH keys Configure the root password and SSH keys in Vault Option 1: Automated default Option 2: Manual Perform management NCN personalization Option 1: Automatically apply CSM configuration Automatic CSM configuration steps Automatic CSM configuration overrides Option 2: Manually apply CSM configuration 1. Set up passwordless SSH This procedure should be run during CSM installation and any later time when the SSH keys need to be changed per site requirements.\nThe goal of passwordless SSH is to enable an easy way for interactive passwordless SSH from and between CSM product environments (management nodes) to downstream managed product environments (COS, UAN, etc), without requiring each downstream environment to create and apply individual changes to NCNs, and as a primary way to manage passwordless SSH configuration between management nodes. Passwordless SSH from downstream nodes into CSM management nodes is not intended or supported.\nPasswordless SSH keypairs for the Cray System Management (CSM) are created automatically and maintained with a Kubernetes deployment and staged into Kubernetes secrets (csm-private-key) and ConfigMaps (csm-public-key) in the services namespace. Administrators can use these provided keys, provide their own keys, or use their own solution for authentication.\nThe management of keys on NCNs is achieved by the trust-csm-ssh-keys and passwordless-ssh Ansible roles in the CSM configuration management repository. The SSH keypair is applied to management nodes using NCN personalization.\nNOTE: CFS itself does not use the CSM-provided (or user-supplied) SSH keys to make connections between nodes. CFS will continue to function if passwordless SSH is disabled between CSM and other product environments.\nOption 1: Use the CSM-provided SSH keys The default CSM Ansible plays are already configured to enable Passwordless SSH by default. No further action is necessary before proceeding to Configure the root password and SSH keys in Vault.\nOption 2: Provide custom SSH keys Administrators may elect to replace the CSM-provided keys with their own custom keys.\nSet variables to the locations of the public and private SSH key files.\nReplace the values in the examples below with the paths to the desired key files on the system.\nncn-mw# PUBLIC_KEY_FILE=/path/to/id_rsa-csm.pub ncn-mw# PRIVATE_KEY_FILE=/path/to/id_rsa-csm Provide the custom keys by script or manually.\nThere are two options for providing the keys.\nProvide custom SSH keys by script.\nThe replace_ssh_keys.sh script can be used to replace the keys from files.\nThe docs-csm RPM must be installed in order to use this script. See Check for Latest Documentation\nncn-mw# /usr/share/doc/csm/scripts/operations/configuration/replace_ssh_keys.sh \\ --public-key-file \u0026#34;${PUBLIC_KEY_FILE}\u0026#34; --private-key-file \u0026#34;${PRIVATE_KEY_FILE}\u0026#34; Manually provide custom SSH keys\nThe keys stored in Kubernetes can be updated directly.\nReplace the private key half:\nncn-mw# KEY64=$(cat \u0026#34;${PRIVATE_KEY_FILE}\u0026#34; | base64) \u0026amp;\u0026amp; kubectl get secret -n services csm-private-key -o json | \\ jq --arg value \u0026#34;$KEY64\u0026#34; \u0026#39;.data[\u0026#34;value\u0026#34;]=$value\u0026#39; | kubectl apply -f - \u0026amp;\u0026amp; unset KEY64 Replace the public key half:\nncn-mw# kubectl delete configmap -n services csm-public-key \u0026amp;\u0026amp; cat \u0026#34;${PUBLIC_KEY_FILE}\u0026#34; | base64 \u0026gt; ./value \u0026amp;\u0026amp; kubectl create configmap --from-file value csm-public-key --namespace services \u0026amp;\u0026amp; rm ./value Passwordless SSH with the provided keys will be set up once NCN personalization runs on the NCNs.\nNOTE: This keypair may be the same keypair used for the NCN root user, but it is not required to be the same. Either option is valid.\nProceed Configure the root password and SSH keys in Vault.\nOption 3: Disable CSM-provided passwordless SSH Local site security requirements may preclude use of passwordless SSH access between management nodes. A variable has been added to the associated Ansible roles that allows disabling of passwordless SSH setup to any or all nodes. From the cloned csm-config-management repository directory:\nncn-mw# grep csm_passwordless_ssh_enabled roles/trust-csm-ssh-keys/defaults/main.yaml Example output:\ncsm_passwordless_ssh_enabled: \u0026#39;false\u0026#39; This variable can be overwritten using either a host-specific setting or global to affect all nodes where the playbook is run. See Customize Configuration Values for more detailed information. Do not modify the value in the roles/trust-csm-ssh-keys/defaults/main.yaml file.\nPublished roles within product configuration repositories can contain more comprehensive information regarding these role-specific flags. Reference any role-specific associated Readme.md documents for additional information, because role documentation is updated more frequently as changes are introduced.\nConsult the manual for each product in order to change the default configuration by referring to the HPE Cray EX System Software Getting Started Guide (S-8000) 22.07 on the HPE Customer Support Center. Similar configuration values for disabling the role will be required in these product-specific configuration repositories.\nModifying Ansible plays in a configuration repository will require a new commit and subsequent update of the configuration layer associated with the product.\nProceed Configure the root password and SSH keys in Vault.\nRestore CSM-provided SSH keys Use this procedure if switching from custom keys to the default CSM SSH keys only; otherwise it should be skipped.\nIn order to restore the default CSM keys, there are two options:\nRestore by script.\nThe docs-csm RPM must be installed in order to use this script. See Check for Latest Documentation\nncn-mw# /usr/share/doc/csm/scripts/operations/configuration/restore_ssh_keys.sh Restore manually.\nThe keys can be deleted from Kubernetes directly. The csm-ssh-keys Kubernetes deployment provided by CSM periodically checks the ConfigMap and secret containing the key information. If these entries do not exist, it will recreate them from the default CSM keys. Therefore, in order to manually restore the keys, delete the associated ConfigMap and secret. The default CSM-provided keys will be republished.\nDelete the csm-private-key Kubernetes secret.\nncn-mw# kubectl delete secret -n services csm-private-key Delete the csm-public-key Kubernetes ConfigMap.\nncn-mw# kubectl delete configmap -n services csm-public-key 2. Configure the root password and SSH keys in Vault The root user password and SSH keys are managed on NCNs by using the csm.password and csm.ssh_keys Ansible roles, respectively, located in the CSM configuration management repository. root user passwords and SSH keys are set and managed in Vault.\nThere are two options for setting the root password and SSH keys in Vault: automated default or manual.\nAfter these have been set in Vault, they will automatically be applied to NCNs during NCN personalization. For more information on how to configure and run NCN personalization, see 3. Perform management NCN personalization.\nOption 1: Automated default The automated default method uses the write_root_secrets_to_vault.py script to read in the current root user password and SSH keys from the NCN where it is run, and write those to Vault. All of the NCNs are booted from images which already had their root passwords and SSH keys customized during the Deploy Management Nodes procedure of the CSM install. In most cases, these are the same password and keys that should be written to Vault, and this script provides an easy way to do that.\nSpecifically, the write_root_secrets_to_vault.py script reads the following from the NCN where it is run:\nThe root user password hash from the /etc/shadow file. The private SSH key from /root/.ssh/id_rsa. The public SSH key from /root/.ssh/id_rsa.pub. This script can be run on any Kubernetes management NCN (master or worker). It only needs to be run once for the cluster, because the same Vault credentials are used for all management NCNs.\nThe docs-csm RPM must be installed in order to use this script. See Check for Latest Documentation\nRun the script with the following command:\nncn-mw# /usr/share/doc/csm/scripts/operations/configuration/write_root_secrets_to_vault.py A successful execution will exit with return code 0 and will have output similar to the following:\nReading in file \u0026#39;/root/.ssh/id_rsa\u0026#39; Reading in file \u0026#39;/root/.ssh/id_rsa.pub\u0026#39; Reading in file \u0026#39;/etc/shadow\u0026#39; Found root user line in /etc/shadow Initializing Kubernetes client Getting Vault token from vault/cray-vault-unseal-keys Kubernetes secret Examining Kubernetes cray-vault service to determine URL for Vault API endpoint of secret/csm/users/root Writing SSH keys and root password hash to secret/csm/users/root in Vault Making POST request to http://10.18.232.40:8200/v1/secret/csm/users/root Response status code = 204 Read back secrets from Vault to verify that the values were correctly saved Making GET request to http://10.18.232.40:8200/v1/secret/csm/users/root Response status code = 200 Validating that Vault contents match what was written to it All secrets successfully written to Vault SUCCESS Proceed to 3. Perform management NCN personalization.\nOption 2: Manual NOTE: Information on writing the root user password and the SSH keys to Vault is documented in two separate procedures. However, if both the password and the SSH keys are to be stored in Vault (the standard case), then the two procedures must be combined. Specifically, only a single write command must be made to Vault, containing both the password and the SSH keys. If multiple write commands are performed, only the information from the final command will persist.\nSet the root user password and SSH keys in Vault by combining the following two procedures:\nThe Configure Root Password in Vault procedure in Update NCN User Passwords. The Configure Root SSH Keys in Vault procedure in Update NCN User SSH Keys. Proceed to 3. Perform management NCN personalization.\n3. Perform management NCN personalization The previous procedures on this page did not make any changes to the NCNs on the system. They merely specified how the NCNs should be configured. In order for these configurations to be applied to the NCNs, a process called NCN personalization takes place using CFS.\nThere are two steps:\nCreate or update the NCN personalization configuration in CFS, if needed. Update CFS to set the NCN personalization configuration as the desired configuration for the NCNs. These can be accomplished by following Option 1: Automatically apply CSM configuration or Option 2: Manually apply CSM configuration.\nOption 1: Automatically apply CSM configuration The docs-csm RPM must be installed in order to use this script. See Check for Latest Documentation\nBy default the script will select the latest available CSM release. However, it is possible to specify the CSM release version using the --csm-release parameter.\nncn-mw# /usr/share/doc/csm/scripts/operations/configuration/apply_csm_configuration.sh --csm-release \u0026lt;version e.g. 1.0.11\u0026gt; Automatic CSM configuration steps By default, the script will perform the following steps:\nFinds the latest installed release version of the CSM product stream. Finds the CSM configuration version associated with the given release. Finds the latest commit on the release branch of the csm-config-management repository. Creates or updates the ncn-personalization.json configuration file. Finds all nodes in HSM with the Management role. Disables configuration for all NCNs. Updates the ncn-personalization configuration in CFS from the ncn-personalization.json file. Enables configuration for all NCN nodes, and sets their desired configuration to ncn-personalization. Monitors CFS until all NCN nodes have successfully completed or failed configuration. Automatic CSM configuration overrides The script also supports several flags to override these behaviors:\n--csm-release: Overrides the version of the CSM release that is used. Defaults to the latest version.\nAvailable versions can be found in the cray-product-catalog ConfigMap.\nncn-mw# kubectl -n services get cm cray-product-catalog --csm-config-version: Overrides the version of the CSM configuration. This corresponds to the version of a branch starting with cray/csm/ in the csm repository in VCS.\n--git-commit: Overrides the Git commit cloned for the configuration content. Defaults to the latest commit on the csm-release branch.\n--git-clone-url: Overrides the source of the configuration content. Defaults to https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git.\n--ncn-config-file: Sets a file other than ncn-personalization.json to be used for the configuration.\n--xnames: A comma-separated list of component names (xnames) to deploy to. Defaults to all Management nodes in HSM.\n--clear-state: Clears existing state from components to ensure that CFS runs. This can be used if configuration needs to be re-run on successful nodes with no change to the Git content since the previous run; for example, if the SSH keys have changed.\nOption 2: Manually apply CSM configuration In order to manually run management NCN personalization, first gather the following information:\nHTTP clone URL for the configuration repository in VCS. Path to the Ansible play to run in the repository. Git commit ID in the repository for CFS to pull and run on the nodes. Field Value Description cloneUrl https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git CSM configuration repository commit Example: 5081c1ecea56002df41218ee39f6030c3eebdf27 CSM configuration commit hash name Example: csm-1.9.21 CSM configuration layer name playbook site.yml Default site-wide Ansible playbook for CSM Retrieve the commit in the repository to use for configuration. If changes have been made to the default branch that was imported during a CSM installation or upgrade, use the commit containing the changes.\nIf no changes have been made, then the latest commit on the default branch for this version of CSM should be used.\nFind the commit in the cray-product-catalog ConfigMap for the current version of CSM. For example:\nncn-mw# kubectl -n services get cm cray-product-catalog -o jsonpath=\u0026#39;{.data.csm}\u0026#39; Look for something similar to the following in the output:\n1.2.1: configuration: clone_url: https://vcs.cmn.SYSTEM_DOMAIN_NAME/vcs/cray/csm-config-management.git commit: 43ecfa8236bed625b54325ebb70916f55884b3a4 import_branch: cray/csm/1.9.24 import_date: 2021-07-28 03:26:01.869501 ssh_url: git@vcs.cmn.SYSTEM_DOMAIN_NAME:cray/csm-config-management.git The commit will be different for each system and version of CSM. For this example, it is 43ecfa8236bed625b54325ebb70916f55884b3a4.\nCraft a new configuration layer entry for the new CSM.\n{ \u0026#34;name\u0026#34;: \u0026#34;csm-\u0026lt;version\u0026gt;\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;retrieved git commit ID\u0026gt;\u0026#34; } Create or update the NCN personalization configuration in CFS.\nNOTE The CSM configuration layer MUST be the first layer in the NCN personalization CFS configuration.\n"
},
{
	"uri": "/docs-csm/en-12/introduction/",
	"title": "Introduction to CSM Installation",
	"tags": [],
	"description": "",
	"content": "Introduction to CSM Installation This document provides an introduction to the Cray System Management (CSM) installation documentation for an HPE Cray EX system.\nTopics: CSM Overview Scenarios for Shasta v1.5 CSM Product Stream Updates CSM Operational Activities Differences from Previous Release Documentation Conventions Details CSM Overview The CSM installation prepares and deploys a distributed system across a group of management nodes organized into a Kubernetes cluster which uses Ceph for utility storage. These nodes perform their function as Kubernetes master nodes, Kubernetes worker nodes, or utility storage nodes with the Ceph storage.\nSystem services on these nodes are provided as containerized micro-services packaged for deployment via Helm charts. Kubernetes orchestrates these services and schedules them on Kubernetes worker nodes with horizontal scaling. Horizontal scaling increases or decreases the number of services\u0026rsquo; instances as demand for them varies, such as when booting many compute nodes or application nodes.\nThere is much more information available in the CSM Overview about the hardware, software, network, and access to these services and components.\nSee CSM Overview\nScenarios for Shasta v1.5 These scenarios for how to get CSM software onto a system are described in Scenarios for Shasta v1.5.\nInstallation of CSM software First time installation of CSM software Reinstall of CSM software Upgrade from a previous version of CSM software Note: A migration from Shasta v1.3.x software to Shasta v1.5 software is not supported as a direct action, but is a two step process of first migrating from Shasta v1.3.x to Shasta v1.4 and then following the Upgrade procedure from v1.4 to v1.5.\nSee Scenarios for Shasta v1.5\nCSM Product Stream Updates The software included in the CSM product stream is released in more than one way. The initial product release may be augmented with late-breaking documentation updates or hotfixes after the release.\nSee CSM Product Stream Updates\nCSM Operational Activities Procedures which are used during either installation, upgrading or general operation of the system reside here. They are referenced in the context of the specific workflow. For example, updating firmware with FAS or running the CSM health checks.\nSee CSM Operational Activities\nDifferences from Previous Release Significant changes from the previous release of CSM are described.\nNew Features Deprecating Features Deprecated Features Other Changes See Differences from Previous Release\nDocumentation Conventions Several conventions have been used in the preparation of this documentation.\nFile Formats Typographic Conventions Command Prompt Conventions which indicate the context for user, host, directory, chroot environment, or container environment See Documentation Conventions\n"
},
{
	"uri": "/docs-csm/en-12/install/utility_storage_node_installation_troubleshooting/",
	"title": "Utility Storage Installation Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Utility Storage Installation Troubleshooting If there is a failure in the creation of Ceph storage on the utility storage nodes for one of these scenarios, the Ceph storage might need to be reinitialized.\nTopics Scenario 1 (Shasta v1.4 only) Scenario 2 (Shasta v1.5 only) Details Scenario 1 (Shasta 1.4 only) IMPORTANT (FOR NODE INSTALLS/REINSTALLS ONLY): If the Ceph install failed, check the following:\nncn-s# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 83.83459 root default -5 27.94470 host ncn-s001 0 ssd 3.49309 osd.0 up 1.00000 1.00000 4 ssd 3.49309 osd.4 up 1.00000 1.00000 6 ssd 3.49309 osd.6 up 1.00000 1.00000 8 ssd 3.49309 osd.8 up 1.00000 1.00000 10 ssd 3.49309 osd.10 up 1.00000 1.00000 12 ssd 3.49309 osd.12 up 1.00000 1.00000 14 ssd 3.49309 osd.14 up 1.00000 1.00000 16 ssd 3.49309 osd.16 up 1.00000 1.00000 -3 27.94470 host ncn-s002 1 ssd 3.49309 osd.1 down 1.00000 1.00000 3 ssd 3.49309 osd.3 down 1.00000 1.00000 5 ssd 3.49309 osd.5 down 1.00000 1.00000 7 ssd 3.49309 osd.7 down 1.00000 1.00000 9 ssd 3.49309 osd.9 down 1.00000 1.00000 11 ssd 3.49309 osd.11 down 1.00000 1.00000 13 ssd 3.49309 osd.13 down 1.00000 1.00000 15 ssd 3.49309 osd.15 down 1.00000 1.00000 -7 27.94519 host ncn-s003 \u0026lt;--- node where the issue exists 2 ssd 27.94519 osd.2 down 1.00000 1.00000 \u0026lt;--- the problematic VG SSH to the node(s) where the issue exists.\nRun the following commands on the nodes:\nncn-s# systemctl stop ceph-osd.target ncn-s# vgremove -f --select \u0026#39;vg_name=~ceph*\u0026#39; # This will take a little bit of time, so do not panic ncn-s# for i in {g..n}; do sgdisk --zap-all /dev/sd$i; done This will vary node to node. Use lsblk to identify all drives available to Ceph.\nManually create OSDs on the problematic nodes.\nncn-s# for i in {g..n}; do ceph-volume lvm create --data /dev/sd$i --bluestore; done NOTE: The remaining steps must be run from ncn-s001.\nVerify the /etc/cray/ceph directory is empty. If there are any files there, then delete them.\nPut in safeguard.\nEdit /srv/cray/scripts/metal/lib.sh Comment out the below lines 22 if [ $wipe == \u0026#39;yes\u0026#39; ]; then 23 ansible osds -m shell -a \u0026#34;vgremove -f --select \u0026#39;vg_name=~ceph*\u0026#39;\u0026#34; 24 fi Run the cloud init script.\nncn-s001# /srv/cray/scripts/common/storage-ceph-cloudinit.sh Scenario 2 (Shasta 1.5 only) IMPORTANT (FOR NODE INSTALLS/REINSTALLS ONLY): If the Ceph install failed, check the following:\nncn-s# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 31.43875 root default -3 10.47958 host ncn-s001 2 ssd 1.74660 osd.2 up 1.00000 1.00000 3 ssd 1.74660 osd.3 up 1.00000 1.00000 6 ssd 1.74660 osd.6 up 1.00000 1.00000 9 ssd 1.74660 osd.9 up 1.00000 1.00000 12 ssd 1.74660 osd.12 up 1.00000 1.00000 15 ssd 1.74660 osd.15 up 1.00000 1.00000 -5 10.47958 host ncn-s002 0 ssd 1.74660 osd.0 down 1.00000 1.00000 \u0026lt;-- the bad OSD 4 ssd 1.74660 osd.4 up 1.00000 1.00000 7 ssd 1.74660 osd.7 up 1.00000 1.00000 10 ssd 1.74660 osd.10 up 1.00000 1.00000 13 ssd 1.74660 osd.13 up 1.00000 1.00000 16 ssd 1.74660 osd.16 up 1.00000 1.00000 -7 10.47958 host ncn-s003 1 ssd 1.74660 osd.1 up 1.00000 1.00000 5 ssd 1.74660 osd.5 up 1.00000 1.00000 8 ssd 1.74660 osd.8 up 1.00000 1.00000 11 ssd 1.74660 osd.11 up 1.00000 1.00000 14 ssd 1.74660 osd.14 up 1.00000 1.00000 17 ssd 1.74660 osd.17 up 1.00000 1.00000 Get more information using the host and OSD.\nncn-s# ceph orch ps --daemon-type osd ncn-s002 NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID osd.0 ncn-s002 running (23h) 7m ago 2d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 98859a09a946 osd.10 ncn-s002 running (23h) 7m ago 2d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 808162b421b8 osd.13 ncn-s002 running (23h) 7m ago 2d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 594d6fd03361 osd.16 ncn-s002 running (23h) 7m ago 2d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 726295e3625f osd.4 ncn-s002 running (23h) 7m ago 2d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c ee1987d99e5a osd.7 ncn-s002 running (23h) 7m ago 2d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 78a89eaef92a Optionally, leave off the host name and it will return all the OSD processing the cluster.\nIn order to zap a single OSD, it is necessary to gather some information.\nList the devices on that host with ceph orch device ls \u0026lt;hostname\u0026gt;.\nncn-s# ceph orch device ls ncn-s002 --wide Hostname Path Type Transport RPM Vendor Model Serial Size Health Ident Fault Available Reject Reasons ncn-s002 /dev/sdc ssd Unknown Unknown ATA SAMSUNG MZ7LH1T9 S455NY0M811867 1920G Unknown N/A N/A No locked, LVM detected, Insufficient space (\u0026lt;10 extents) on vgs ncn-s002 /dev/sdd ssd Unknown Unknown ATA SAMSUNG MZ7LH1T9 S455NY0M812407 1920G Unknown N/A N/A No locked, LVM detected, Insufficient space (\u0026lt;10 extents) on vgs ncn-s002 /dev/sde ssd Unknown Unknown ATA SAMSUNG MZ7LH1T9 S455NY0M812406 1920G Unknown N/A N/A No locked, LVM detected, Insufficient space (\u0026lt;10 extents) on vgs ncn-s002 /dev/sdf ssd Unknown Unknown ATA SAMSUNG MZ7LH1T9 S455NY0M812405 1920G Unknown N/A N/A No locked, LVM detected, Insufficient space (\u0026lt;10 extents) on vgs ncn-s002 /dev/sdg ssd Unknown Unknown ATA SAMSUNG MZ7LH1T9 S455NY0M811921 1920G Unknown N/A N/A No locked, LVM detected, Insufficient space (\u0026lt;10 extents) on vgs ncn-s002 /dev/sdh ssd Unknown Unknown ATA SAMSUNG MZ7LH1T9 S455NY0M811873 1920G Unknown N/A N/A No locked, LVM detected, Insufficient space (\u0026lt;10 extents) on vgs The locked status in the Reject column is likely the result of a wipe failure.\nFind the drive path.\nncn-s# cephadm ceph-volume lvm list Inferring fsid 8f4dd38b-ee84-4d29-8305-1ef24e61a5d8 Using recent Ceph image docker.io/ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d /usr/bin/podman: stdout /usr/bin/podman: stdout /usr/bin/podman: stdout ====== osd.0 ======= /usr/bin/podman: stdout /usr/bin/podman: stdout [block] /dev/ceph-380453cf-4581-4616-b95e-30a8743bece0/osd-data-59bcf0c9-5867-41c3-8e40-2e99232cf8e9 /usr/bin/podman: stdout /usr/bin/podman: stdout block device /dev/ceph-380453cf-4581-4616-b95e-30a8743bece0/osd-data-59bcf0c9-5867-41c3-8e40-2e99232cf8e9 /usr/bin/podman: stdout block uuid 54CjSj-kxEs-df0N-13Vs-miIF-g2KH-sX2UMQ /usr/bin/podman: stdout cephx lockbox secret /usr/bin/podman: stdout cluster fsid 8f4dd38b-ee84-4d29-8305-1ef24e61a5d8 /usr/bin/podman: stdout cluster name ceph /usr/bin/podman: stdout crush device class None /usr/bin/podman: stdout encrypted 0 /usr/bin/podman: stdout osd fsid b2eb119c-4f45-430b-96b0-bad9e8b9aca6 /usr/bin/podman: stdout osd id 0 \u0026lt;-- the OSD number /usr/bin/podman: stdout osdspec affinity /usr/bin/podman: stdout type block /usr/bin/podman: stdout vdo 0 /usr/bin/podman: stdout devices /dev/sdf \u0026lt;--the path /usr/bin/podman: stdout Above output truncated for the purposes of this example.\nZap a single device with ceph orch device zap (hostname) (device path).\nncn-s# ceph orch device zap ncn-s002 /dev/sdf "
},
{
	"uri": "/docs-csm/en-12/install/ceph_csi_troubleshooting/",
	"title": "Ceph CSI Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Ceph CSI Troubleshooting If there has been a failure to initialize all Ceph CSI components on ncn-s001, then the storage node cloud-init may need to be rerun.\nTopics: Verify Ceph CSI Rerun Storage Node cloud-init Details 1. Verify Ceph CSI Verify that the ceph-csi requirements are in place.\nLog in to ncn-s001 and run the following command.\nncn-s001# ceph -s If it returns a connection error, then assume Ceph is not installed. See Rerun Storage Node cloud-init.\nVerify all post-Ceph-install tasks have run.\nLog in to ncn-s001 and check /etc/cray/ceph for completed task files ceph_k8s_initialized and csi_initialized.\nncn-s001# ls /etc/cray/ceph/ ceph_k8s_initialized csi_initialized installed kubernetes_nodes.txt tuned Check your results against this example.\nIf any components are missing, see Rerun Storage Node cloud-init.\nCheck to see if ceph-csi prerequisites have been created in Kubernetes.\nThese commands can be run from any master node, any worker node, or ncn-s001.\nncn# kubectl get cm NAME DATA AGE ceph-csi-config 1 3h50m cephfs-csi-sc 1 3h50m kube-csi-sc 1 3h50m sma-csi-sc 1 3h50m sts-rados-config 1 4h ncn# kubectl get secrets | grep csi csi-cephfs-secret Opaque 4 3h51m csi-kube-secret Opaque 2 3h51m csi-sma-secret Opaque 2 3h51m Check your results against the above examples.\nIf any components are missing, see Rerun Storage Node cloud-init.\n1. Rerun Storage Node cloud-init This procedure will restart the storage node cloud-init process to prepare Ceph for use by the utility storage nodes.\nRun the following on ncn-s001:\nncn-s001# ls /etc/cray/ceph If any files are there they will represent completed stages.\nIf you have a running cluster, edit storage-ceph-cloudinit.sh on ncn-s001:\nncn-s001# vi /srv/cray/scripts/common/storage-ceph-cloudinit.sh Comment out this section:\n#if [ -f \u0026#34;$ceph_installed_file\u0026#34; ]; then # echo \u0026#34;This ceph cluster has been initialized\u0026#34; #else # echo \u0026#34;Installing ceph\u0026#34; # init # mark_initialized $ceph_installed_file #fi Run the storage-ceph-cloudinit.sh script on ncn-s001:\nncn-s001# /srv/cray/scripts/common/storage-ceph-cloudinit.sh Configuring node auditing software Using generic auditing configuration This ceph cluster has been initialized This ceph cluster has already been tuned This ceph radosgw config and initial k8s integration already complete ceph-csi configuration has been already been completed If your output is like above, then that means that all the steps ran. You can also locate the files in /etc/cray/ceph that are created as each step completes. If the script failed, then examine the output for indications of what may be causing the problem. "
},
{
	"uri": "/docs-csm/en-12/background/ncn_images/",
	"title": "NCN Images",
	"tags": [],
	"description": "",
	"content": "NCN Images Overview of NCN images LiveCD server Overview of NCN images The management non-compute nodes (NCNs) boot from images which are created from layers on top of a common base image. The common image is customized with a kubernetes layer for the master nodes and worker nodes. The common image is customized with a storage-ceph layer for the utility storage nodes.\nWhen booting NCNs, an administrator will need to choose between stable (Release) and unstable (pre-release/development) images.\nIn short, each image (i.e. Kubernetes and storage-ceph) inherit from the non-compute-common layer. Operationally these are all that matter; the common layer, Kubernetes layer, Ceph layer, and any other new images.\nTo boot an NCN, there are three required artifacts for each node-type (kubernetes-master/worker, storage-ceph):\nThe Kubernetes SquashFS (stable or unstable)\ninitrd.img-[RELEASE].xz $version-[RELEASE].kernel kubernetes-[RELEASE].squashfs The CEPH SquashFS (stable or unstable)\ninitrd.img-[RELEASE].xz $version-[RELEASE].kernel storage-ceph-[RELEASE].squashfs LiveCD Server View the current ephemeral data payload:\npit# ls -l /var/www Example output:\ntotal 8 drwxr-xr-x 1 dnsmasq tftp 4096 Dec 17 21:20 boot drwxr-xr-x 7 root root 4096 Dec 2 04:45 ephemeral pit# ls -l /var/www/ephemeral/data/* Example output:\n/var/www/ephemeral/data/ceph: total 4 drwxr-xr-x 2 root root 4096 Dec 17 21:42 0.0.7 /var/www/ephemeral/data/k8s: total 4 drwxr-xr-x 2 root root 4096 Dec 17 21:26 0.0.8 Setup the \u0026ldquo;booting repositories\u0026rdquo;:\npit# set-sqfs-links.sh Example output:\nMismatching kernels! The discovered artifacts will deploy an undesirable stack. mkdir: created directory \u0026#39;ncn-m001\u0026#39; /var/www/ncn-m001 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-m002\u0026#39; /var/www/ncn-m002 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-m003\u0026#39; /var/www/ncn-m003 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-w002\u0026#39; /var/www/ncn-w002 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-w003\u0026#39; /var/www/ncn-w003 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-s001\u0026#39; /var/www/ncn-s001 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-s002\u0026#39; /var/www/ncn-s002 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-s003\u0026#39; /var/www/ncn-s003 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs\u0026#39; /var/www View the currently set links.\npit# ls -l /var/www/ncn-* Example output:\nboot: total 1552 -rw-r--r-- 1 root root 166634 Dec 17 13:21 graffiti.png -rw-r--r-- 1 dnsmasq tftp 700480 Dec 17 13:25 ipxe.efi -rw-r--r-- 1 dnsmasq tftp 700352 Dec 15 09:35 ipxe.efi.stable -rw-r--r-- 1 root root 6157 Dec 15 05:12 script.ipxe -rw-r--r-- 1 root root 6284 Dec 17 13:21 script.ipxe.rpmnew ephemeral: total 32 drwxr-xr-x 2 root root 4096 Dec 6 22:18 configs drwxr-xr-x 4 root root 4096 Dec 7 04:29 data drwx------ 2 root root 16384 Dec 2 04:25 lost+found drwxr-xr-x 4 root root 4096 Dec 3 02:31 prep drwxr-xr-x 2 root root 4096 Dec 2 04:45 static ncn-m001: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel ncn-m002: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel ncn-m003: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel ncn-s001: total 4 lrwxrwxrwx 1 root root 56 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs lrwxrwxrwx 1 root root 48 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz lrwxrwxrwx 1 root root 62 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel ncn-s002: total 4 lrwxrwxrwx 1 root root 56 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs lrwxrwxrwx 1 root root 48 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz lrwxrwxrwx 1 root root 62 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel ncn-s003: total 4 lrwxrwxrwx 1 root root 56 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs lrwxrwxrwx 1 root root 48 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz lrwxrwxrwx 1 root root 62 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel ncn-w002: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel ncn-w003: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel "
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/stage_4/",
	"title": "Stage 4 - Ceph Upgrade",
	"tags": [],
	"description": "",
	"content": "Stage 4 - Ceph Upgrade Reminder: If any problems are encountered and the procedure or command output does not provide relevant guidance, see Relevant troubleshooting links for upgrade-related issues.\nAddresses CVEs CVE-2021-3531: Swift API denial of service. CVE-2021-3524: HTTP header injects via CORS in RGW.. CVE-2021-3509: Dashboard XSS via token cookie. CVE-2021-20288: Unauthorized global_id reuse in cephx. The upgrade includes all fixes from v15.2.9 through v15.2.15. See the Ceph version index for details.\nProcedure This upgrade is performed using the ceph orchestrator. Unless otherwise noted, all ceph commands in this stage may be run on any master node or any of the first three storage nodes (ncn-s001, ncn-s002, or ncn-s003).\nInitiate upgrade Check to ensure that the upgrade is possible.\nncn-ms# ceph orch upgrade check --image registry.local/artifactory.algol60.net/csm-docker/stable/quay.io/ceph/ceph:v15.2.15 Example output:\n{ \u0026#34;needs_update\u0026#34;: { \u0026#34;alertmanager.ncn-s001\u0026#34;: { \u0026#34;current_id\u0026#34;: \u0026#34;6ec9fa439af31102c9e6581cbb3d12ee2ab258dada41d40d0f8ad987e8ff266f\u0026#34;, \u0026#34;current_name\u0026#34;: \u0026#34;registry.local/quay.io/prometheus/alertmanager:v0.21.0\u0026#34;, \u0026#34;current_version\u0026#34;: \u0026#34;0.21.0\u0026#34; }, \u0026#34;crash.ncn-s001\u0026#34;: { \u0026#34;current_id\u0026#34;: \u0026#34;6a777b4f888c24feec6e12eeeff4ab485f2c043b415bc2213815d5fb791f2597\u0026#34;, \u0026#34;current_name\u0026#34;: \u0026#34;registry.local/ceph/ceph:v15.2.8\u0026#34;, \u0026#34;current_version\u0026#34;: \u0026#34;15.2.8\u0026#34; }, \u0026#34;crash.ncn-s002\u0026#34;: { \u0026#34;current_id\u0026#34;: \u0026#34;6a777b4f888c24feec6e12eeeff4ab485f2c043b415bc2213815d5fb791f2597\u0026#34;, \u0026#34;current_name\u0026#34;: \u0026#34;registry.local/ceph/ceph:v15.2.8\u0026#34;, \u0026#34;current_version\u0026#34;: \u0026#34;15.2.8\u0026#34; }, \u0026#34;[ ... lines omitted for readability ... ]\u0026#34;, \u0026#34;rgw.site1.zone1.ncn-s003.adrubu\u0026#34;: { \u0026#34;current_id\u0026#34;: \u0026#34;6a777b4f888c24feec6e12eeeff4ab485f2c043b415bc2213815d5fb791f2597\u0026#34;, \u0026#34;current_name\u0026#34;: \u0026#34;registry.local/ceph/ceph:v15.2.8\u0026#34;, \u0026#34;current_version\u0026#34;: \u0026#34;15.2.8\u0026#34; } }, \u0026#34;target_id\u0026#34;: \u0026#34;cba763a65a95e8849d578e05b111123f55a78ab096e67e8ecf7fdc98e67aea71\u0026#34;, \u0026#34;target_name\u0026#34;: \u0026#34;registry.local/artifactory.algol60.net/csm-docker/stable/quay.io/ceph/ceph:v15.2.15\u0026#34;, \u0026#34;target_version\u0026#34;: \u0026#34;ceph version 15.2.15 (2dfb18841cfecc2f7eb7eb2afd65986ca4d95985) octopus (stable)\u0026#34;, \u0026#34;up_to_date\u0026#34;: [] } Notes:\nThis upgrade is targeting the Ceph processes running 15.2.8 only. The monitoring services may be listed but those are patched internally and will not be upgraded with this upgrade. This includes alertmanager, prometheus, node-exporter, and grafana. The main goals of this check are to see the listed 15.2.8 services and to see the output at the bottom that confirms the presence of the 15.2.15 target image. If the output does not match what is expected, then this can indicate that a previous step has failed. Review output from Stage 1 for errors or contact support. Set the container image.\nncn-ms# ceph config set global container_image registry.local/artifactory.algol60.net/csm-docker/stable/quay.io/ceph/ceph:v15.2.15 Verify that the change has occurred.\nncn-ms# ceph config dump -f json-pretty|jq \u0026#39;.[]|select(.name==\u0026#34;container_image\u0026#34;)|.value\u0026#39; Expected result:\n\u0026#34;registry.local/artifactory.algol60.net/csm-docker/stable/quay.io/ceph/ceph:v15.2.15\u0026#34; Start the upgrade.\nncn-ms# ceph orch upgrade start --image registry.local/artifactory.algol60.net/csm-docker/stable/quay.io/ceph/ceph:v15.2.15 Monitor the upgrade.\nIf upgrading a larger cluster, consider splitting this into two different watch commands in separate windows.\nncn-ms# watch \u0026#34;ceph -s; ceph orch ps\u0026#34; Monitor upgrade The processes running the Ceph container image will go through the upgrade process. This involves stopping the old process running the version 15.2.8 container and restarting the process with the new version 15.2.15 container image.\nIMPORTANT: Only processes running the 15.2.8 image will be upgraded. This includes crash, mds, mgr, mon, osd, and rgw processes only.\nUPGRADE_FAILED_PULL: Upgrade: failed to pull target image If ceph -s shows a warning with UPGRADE_FAILED_PULL: Upgrade: failed to pull target image as the description, then perform the following procedure on any of the first three storage nodes (ncn-s001, ncn-s002, or ncn-s003).\nCheck the upgrade status.\nncn-s# ceph orch upgrade status Example output:\n{ \u0026#34;target_image\u0026#34;: \u0026#34;registry.local/artifactory.algol60.net/csm-docker/stable/quay.io/ceph/ceph:v15.2.15\u0026#34;, \u0026#34;in_progress\u0026#34;: true, \u0026#34;services_complete\u0026#34;: [], \u0026#34;message\u0026#34;: \u0026#34;Error: UPGRADE_FAILED_PULL: Upgrade: failed to pull target image\u0026#34; } Pause and resume the upgrade.\nncn-s# ceph orch upgrade pause ncn-s# ceph orch upgrade resume Watch cephadm.\nThis command watches the cephadm logs. If the issue occurs again, it will give more details about which node may be having an issue.\nncn-s# ceph -W cephadm If the issue occurs again, then log into each of the storage nodes and perform a podman pull of the image.\nncn-s# podman pull registry.local/artifactory.algol60.net/csm-docker/stable/quay.io/ceph/ceph:v15.2.15 If these steps do not resolve the issue, then contact support for further assistance.\nExpected warnings From ceph -s:\nhealth: HEALTH_WARN clients are using insecure global_id reclaim mons are allowing insecure global_id reclaim From ceph health detail:\nHEALTH_WARN clients are using insecure global_id reclaim; mons are allowing insecure global_id reclaim; 1 osds down [WRN] AUTH_INSECURE_GLOBAL_ID_RECLAIM: clients are using insecure global_id reclaim osd.4 at [REDACTED] is using insecure global_id reclaim mds.cephfs.ncn-s001.qcalye at [REDACTED] is using insecure global_id reclaim client.rgw.site1.zone1.ncn-s001.lgfngf at [REDACTED] is using insecure global_id reclaim client.rgw.site1.zone1.ncn-s001.lgfngf at [REDACTED] is using insecure global_id reclaim osd.0 at [REDACTED] is using insecure global_id reclaim client.rgw.site1.zone1.ncn-s003.wllbbx at [REDACTED] is using insecure global_id reclaim osd.5 at [REDACTED] is using insecure global_id reclaim osd.7 at [REDACTED] is using insecure global_id reclaim client.rgw.site1.zone1.ncn-s002.aanqmw at [REDACTED] is using insecure global_id reclaim client.rgw.site1.zone1.ncn-s002.aanqmw at [REDACTED] is using insecure global_id reclaim client.rgw.site1.zone1.ncn-s002.aanqmw a [REDACTED] is using insecure global_id reclaim osd.3 at [REDACTED]] is using insecure global_id reclaim mds.cephfs.ncn-s002.tdrohq at [REDACTED]] is using insecure global_id reclaim client.rgw.site1.zone1.ncn-s003.wllbbx a [REDACTED] is using insecure global_id reclaim client.rgw.site1.zone1.ncn-s001.lgfngf a [REDACTED] is using insecure global_id reclaim client.rgw.site1.zone1.ncn-s003.wllbbx a [REDACTED] is using insecure global_id reclaim mds.cephfs.ncn-s003.ddbgzt at [REDACTED]] is using insecure global_id reclaim osd.8 at [REDACTED]] is using insecure global_id reclaim osd.1 at [REDACTED]] is using insecure global_id reclaim [WRN] AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED: mons are allowing insecure global_id reclaim mon.ncn-s001 has auth_allow_insecure_global_id_reclaim set to true mon.ncn-s002 has auth_allow_insecure_global_id_reclaim set to true mon.ncn-s003 has auth_allow_insecure_global_id_reclaim set to true Verify completed upgrade Verify that the upgrade has completed using the following procedure.\nVerify that the upgrade is no longer in progress.\nncn-ms# ceph orch upgrade status In the output of this command, validate that the in_progress field is false. If it is true, then the upgrade is still underway. In that case, retry this step after waiting for a few minutes.\nIf the upgrade encountered any problems, there may be indications of this in the messages field of the command output.\nIn the case of a completed upgrade without errors, the output will resemble the following:\n{ \u0026#34;target_image\u0026#34;: null, \u0026#34;in_progress\u0026#34;: false, \u0026#34;services_complete\u0026#34;: [], \u0026#34;message\u0026#34;: \u0026#34;\u0026#34; } Verify that ceph health detail only shows the following:\nHEALTH_WARN mons are allowing insecure global_id reclaim [WRN] AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED: mons are allowing insecure global_id reclaim mon.ncn-s001 has auth_allow_insecure_global_id_reclaim set to true mon.ncn-s002 has auth_allow_insecure_global_id_reclaim set to true mon.ncn-s003 has auth_allow_insecure_global_id_reclaim set to true Verify that ceph -s shows the following for its health:\nhealth: HEALTH_WARN mons are allowing insecure global_id reclaim Verify that all Ceph crash, mds, mgr, mon, osd, and rgw processes are running version 15.2.15.\nThe following command will count the number of crash, mds, mgr, mon, osd, and rgw processes which are not running version 15.2.15.\nncn-ms# ceph orch ps -f json-pretty|jq -r \u0026#39;[.[]|select(.version!=\u0026#34;15.2.15\u0026#34;)|select(.daemon_type as $d | [ \u0026#34;crash\u0026#34;, \u0026#34;mds\u0026#34;, \u0026#34;mgr\u0026#34;, \u0026#34;mon\u0026#34;, \u0026#34;osd\u0026#34;, \u0026#34;rgw\u0026#34; ] | index($d))] | length\u0026#39; If the command outputs any number other than zero, then this means that not all expected processes are running 15.2.15. In that case, do the following:\nList the processes which are not at the expected version.\nncn-ms# ceph orch ps -f json-pretty|jq -r \u0026#39;[.[]|select(.version!=\u0026#34;15.2.15\u0026#34;)|select(.daemon_type as $d | [ \u0026#34;crash\u0026#34;, \u0026#34;mds\u0026#34;, \u0026#34;mgr\u0026#34;, \u0026#34;mon\u0026#34;, \u0026#34;osd\u0026#34;, \u0026#34;rgw\u0026#34; ] | index($d))]\u0026#39; Make sure the upgrade has stopped.\nncn-ms# ceph orch upgrade stop Troubleshoot the failed upgrade.\nThe upgrade is not complete. See Ceph_Orchestrator_Usage.md for additional usage and troubleshooting.\nVerify that no processes are running version 15.2.8.\nThe following command will count the number of processes which are running version 15.2.8.\nncn-ms# ceph orch ps -f json-pretty|jq -r \u0026#39;[.[]|select(.version==\u0026#34;15.2.8\u0026#34;)] | length\u0026#39; If the command outputs any number other than zero, then this means there are processes still running 15.2.8. In that case, do the following:\nList the processes which are not at the expected version.\nncn-ms# ceph orch ps -f json-pretty|jq -r \u0026#39;[.[]|select(.version==\u0026#34;15.2.8\u0026#34;)]\u0026#39; Make sure the upgrade has stopped.\nncn-ms# ceph orch upgrade stop Troubleshoot the failed upgrade.\nThe upgrade is not complete. See Ceph_Orchestrator_Usage.md for additional usage and troubleshooting.\nDO NOT proceed past this point if the upgrade has not completed and been verified. Contact support for in-depth troubleshooting.\nPost-upgrade Disable auth_allow_insecure_global_id_reclaim:\nncn-ms# ceph config set mon auth_allow_insecure_global_id_reclaim false Wait until the Ceph cluster health is HEALTH_OK.\nIt may take up to 30 seconds for the health to return to HEALTH_OK.\nncn-ms# ceph health detail Successful output is:\nHEALTH_OK Stage completed This stage is completed. Continue to Stage 5.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/kubernetes/troubleshoot_unresponsive_kubectl_commands/",
	"title": "Troubleshoot Unresponsive kubectl Commands",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Unresponsive kubectl Commands Use this procedure to check if any kworkers are in an error state because of a high load. Once the error has been identified, workaround the issue by returning the high load to a normal level.\nSymptoms One or more of the following issues are possible symptoms of this issue.\nThe kubectl command can become unresponsive because of a high load. ps aux cannot return or complete because of aspects of the /proc file system being locked. If kubectl is non-responsive on any particular node, then commands can be run from any other master or worker non-compute node (NCN).\nProcedure In the following procedures, unless otherwise directed, run the commands on the node experiencing the issue. However, if kubectl is non-responsive on that node, run the kubectl commands from any other master or worker NCN.\nIdentify the kworker issue Check to see if kubectl is not responding because of a kworker issue.\nList the process identification (PID) numbers of the kworkers in the D state.\nProcesses in the D state are blocked on I/O and are not an issue unless they remain blocked indefinitely. Use the command below to see which PIDs remain stuck in this state.\nncn-mw# ps aux |grep [k]worker|grep -e \u0026#34; D\u0026#34;| awk \u0026#39;{ print $2 }\u0026#39; Show the stack for all kworkers in the D state.\nNote which kworkers clear and which ones remain stuck in this state over a period of time.\nncn-mw# for i in `ps aux | grep [k]worker | grep -e \u0026#34; D\u0026#34; | awk \u0026#39;{print $2}\u0026#39;` ; do cat \u0026#34;/proc/${i}/stack\u0026#34;; echo done Check the load on the node and gather data for any PIDs consuming a lot of CPU.\nMonitor the processes and system resource usage.\nncn-mw# top Example output (some trailing lines omitted):\ntop - 10:12:03 up 34 days, 17:31, 10 users, load average: 7.39, 9.16, 10.99 Tasks: 2155 total, 4 running, 2141 sleeping, 1 stopped, 9 zombie %Cpu(s): 4.3 us, 2.5 sy, 0.0 ni, 93.0 id, 0.0 wa, 0.0 hi, 0.3 si, 0.0 st MiB Mem : 257510.5+total, 69119.86+free, 89578.68+used, 98812.04+buff/cache MiB Swap: 0.000 total, 0.000 free, 0.000 used. 173468.1+avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 6105 root 20 0 193436 182772 2300 S 60.00 0.069 13485:54 lldpad 49574 root 20 0 14.299g 495212 60896 S 47.54 0.188 31582:58 kubelet 1 root 20 0 231236 19436 6572 S 38.69 0.007 16904:47 systemd 43098 root 20 0 16.148g 652640 78748 S 38.69 0.248 18721:18 containerd 20229 root 20 0 78980 14648 6448 S 35.08 0.006 15421:51 systemd 1515295 1001 20 0 16.079g 5.439g 96312 S 11.48 2.163 12480:39 java 4706 message+ 20 0 41060 5620 3724 S 8.852 0.002 3352:38 dbus-daemon 1282935 101 20 0 685476 38556 13748 S 6.557 0.015 262:09.88 patroni 81539 root 20 0 300276 161372 26036 S 5.902 0.061 4145:40 mixs 89619 root 20 0 4731796 498600 24144 S 5.902 0.189 2898:54 envoy 85600 root 20 0 2292564 123596 23248 S 4.590 0.047 2211:58 envoy Generate a performance counter profile for the PIDs consuming a lot of CPU.\nIn the following command, replace the PID value with the actual PID number.\nncn-mw# perf top -g -p PID Example output (some trailing lines omitted):\nSamples: 18 of event \u0026#39;cycles\u0026#39;, Event count (approx.): 4065227 Children Self Shared Object Symbol + 29.31% 9.77% [kernel] [k] load_balance + 19.54% 19.54% [kernel] [k] find_busiest_group + 11.17% 11.17% kubelet [.] 0x0000000000038d3c + 9.77% 9.77% [kernel] [k] select_task_rq_fair + 9.77% 9.77% [kernel] [k] cpuacct_charge Verify that ps -ef completes.\nncn-mw# ps -ef Check the /var/log/messages file on the node to see if there are any errors.\nncn-mw# grep -i error /var/log/messages Example output (some trailing lines omitted):\n\u0026lt;nil\u0026gt;\u0026#34; 2020-07-19T07:19:34.485659+00:00 ncn-w001 containerd[43098]: time=\u0026#34;2020-07-19T07:19:34.485540765Z\u0026#34; level=info msg=\u0026#34;Exec process \\\u0026#34;9946991ef8108d21c163a04c9085fd15a60e3991b8e9d7b2250a071df9b6cbb8\\\u0026#34; exits with exit code 0 and error \u0026lt;nil\u0026gt;\u0026#34; 2020-07-19T07:19:38.468970+00:00 ncn-w001 containerd[43098]: time=\u0026#34;2020-07-19T07:19:38.468818388Z\u0026#34; level=info msg=\u0026#34;Exec process \\\u0026#34;e6fe9ccbb1127a77f8c9db84b339dafe068f9e08579962f790ebf882ee35e071\\\u0026#34; exits with exit code 0 and error \u0026lt;nil\u0026gt;\u0026#34; 2020-07-19T07:19:44.440413+00:00 ncn-w001 containerd[43098]: time=\u0026#34;2020-07-19T07:19:44.440243465Z\u0026#34; level=info msg=\u0026#34;Exec process \\\u0026#34;7a3cf826f008c37bd0fe89382561af42afe37ac4d52f37ce9312cc950248f4da\\\u0026#34; exits with exit code 0 and error \u0026lt;nil\u0026gt;\u0026#34; 2020-07-19T07:20:02.442421+00:00 ncn-w001 containerd[43098]: time=\u0026#34;2020-07-19T07:20:02.442266943Z\u0026#34; level=error msg=\u0026#34;StopPodSandbox for \\\u0026#34;d449618d075b918fd6397572c79bd758087b31788dd8bf40f4dc10bb1a013a68\\\u0026#34; failed\u0026#34; error=\u0026#34;failed to destroy network for sandbox \\\u0026#34;d449618d075b918fd6397572c79bd758087b31788dd8bf40f4dc10bb1a013a68\\\u0026#34;: Multus: Err in getting k8s network from pod: getPodNetworkAnnotation: failed to query the pod sma-monasca-agent-xkxnj in out of cluster comm: pods \\\u0026#34;sma-monasca-agent-xkxnj\\\u0026#34; not found\u0026#34; 2020-07-19T07:20:04.440834+00:00 ncn-w001 containerd[43098]: time=\u0026#34;2020-07-19T07:20:04.440742542Z\u0026#34; level=info msg=\u0026#34;Exec process \\\u0026#34;2a751ca1453d7888be88ab4010becbb0e75b7419d82e45ca63e55e4155110208\\\u0026#34; exits with exit code 0 and error \u0026lt;nil\u0026gt;\u0026#34; 2020-07-19T07:20:06.587325+00:00 ncn-w001 containerd[43098]: time=\u0026#34;2020-07-19T07:20:06.587133372Z\u0026#34; level=error msg=\u0026#34;collecting metrics for bf1d562e060ba56254f5f5ea4634ef4ae189abb462c875e322c3973b83c4c85d\u0026#34; error=\u0026#34;ttrpc: closed: unknown\u0026#34; 2020-07-19T07:20:14.450624+00:00 ncn-w001 containerd[43098]: time=\u0026#34;2020-07-19T07:20:14.450547541Z\u0026#34; level=info msg=\u0026#34;Exec process \\\u0026#34;ceb384f1897d742134e7d2c9da5a62650ed1274f0ee4c5a17fa9cac1a24b6dc4\\\u0026#34; exits with exit code 0 and error Recovery steps Restart the kubelet on the node with the issue.\nncn-mw# systemctl restart kubelet If restarting the kubelet did not resolve the issue, then proceed to the next step.\nRestart the container runtime environment on the node with the issue.\nThis will likely hang or fail to complete without a timeout. If that is the case, then cancel the command with control-C and proceed to the next step.\nncn-mw# systemctl restart containerd Reboot the node with the issue.\nThe node must be rebooted if the remediation of restarting kubelet and containerd did not resolve the kworker and high load average issue.\nIMPORTANT: Do NOT run the commands in this step on the node experiencing the problem. Access to that node will be cut off when it is powered off.\nReplace NCN_NAME in the commands below with the node experiencing the issue. In this example, it is ncn-w999.\nread -s is used to prevent the password from being written to the screen or the shell history.\nncn# NCN_NAME=ncn-w999 ncn# USERNAME=root ncn# read -r -s -p \u0026#34;${NCN_NAME} BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -I lanplus -H \u0026#34;${NCN_NAME}-mgmt\u0026#34; power off; sleep 5; ncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -I lanplus -H \u0026#34;${NCN_NAME}-mgmt\u0026#34; power show; echo ncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -I lanplus -H \u0026#34;${NCN_NAME}-mgmt\u0026#34; power on; sleep 5; ncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -I lanplus -H \u0026#34;${NCN_NAME}-mgmt\u0026#34; power show; echo Watch the console of the node being rebooted.\nThis can be done using the Cray console service or with ipmitool.\nThe recommended method is to use the Cray console service. See Log in to a Node Using ConMan.\nAlternatively, the console can be accessed by using ipmitool.\nncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -I lanplus -H \u0026#34;${NCN_NAME}-mgmt\u0026#34; sol activate This command will not return anything, but will show the ttyS0 console of the node. Use ~. to disconnect. NOTE: The same ~. keystroke can also break an SSH session. After doing this, the connection to the SSH session may need to be reestablished.\nTry running a kubectl command on the node where it was previously unresponsive.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/admin_client_auth_not_found/",
	"title": "Known Issue admin-client-auth Not Found",
	"tags": [],
	"description": "",
	"content": "Known Issue: admin-client-auth Not Found Running the Install CSM Services script, the following error may occur:\nERROR Step: Set Management NCNs to use Unbound --- Checking Precondition + Getting admin-client-auth secret Error from server (NotFound): secrets \u0026#34;admin-client-auth\u0026#34; not found + Obtaining access token Fix This can occur if the keycloak-users-localize pod has not completed, and that can be caused by an intermittent Istio issue. Remediate the issue with the following procedure:\nFollow Troubleshoot Intermittent HTTP 503 Code Failures to verify that Istio is healthy.\nEnsure that the keycloak-wait-for-postgres-* pod is in a Completed state.\nncn-mw# kubectl get po -n services | grep keycloak-wait-for-postgres Example output:\nkeycloak-wait-for-postgres-1-pv85m 0/2 Completed 0 15d If the keycloak-wait-for-postgres-* pod is not in a Completed state, then resubmit the job.\nncn-mw# kubectl get job -n services -l app.kubernetes.io/name=keycloak-wait-for-postgres -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels)\u0026#39; | kubectl replace --force -f - Example output:\njob.batch \u0026#34;keycloak-wait-for-postgres-1\u0026#34; deleted job.batch/keycloak-wait-for-postgres-1 replaced Once the keycloak-wait-for-postgres-* pod has completed, the keycloak-users-localize job should create the admin-client-auth secret and complete. At that point, resume the CSM install by re-running the failed command.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/ceph_deep_scrubs/",
	"title": "Ceph Deep Scrubs",
	"tags": [],
	"description": "",
	"content": "Ceph Deep Scrubs During normal operation, the Ceph cluster performs deep scrubs of the placement groups (PGs) during intervals of low I/O activity on the cluster. By default, these deep scrubs occur on a weekly interval. Scheduling of deep scrubs is staggered across the PGs in the Ceph cluster, so that all PGs are not deep-scrubbed at the same time.\nCeph Deep Scrub Behavior During Outages When one or more OSDs are down, the deep scrubbing of the PGs on those OSDs cannot be performed. If a deep scrub of a PG is scheduled to occur while the OSD is down, the deep scrubbing will be delayed until the OSDs are available. This commonly occurs when the storage nodes are powered down as part of the System Power Off Procedures.\nAfter a prolonged power outage, for example after weekend power maintenance activities, some number of PGs may begin a deep scrub after the system is powered on. An alert will be displayed in the Ceph status while the deep scrub is occurring. Ceph is fully operational while that alert is present, and the alert should clear when scrubbing is completed. The time to complete deep scrubbing depends on the size of the cluster and the length of the outage. If the alert remains for more than a day, contact support.\nThe following example output from ceph -s shows Ceph in a HEALTH_WARN state due to some deep scrubs missed after the system was brought up after power down:\ncluster: id: e67366fb-7d13-4219-bdb4-44a5f7e06bf9 health: HEALTH_WARN 7 pgs not deep-scrubbed in time ... Note the message accompanying the HEALTH_WARN state indicating 7 pgs not deep-scrubbed in time. This alert will clear when deep scrubbing completes.\nViewing Ceph Deep Scrub Schedule The ceph pg dump command shows information about the PGs in the Ceph cluster. This command can be used to see the last time PGs were scrubbed and thus infer the next time they will be scrubbed. For example, the following command will get the last deep scrub time for each PG, convert it to the day of the week, and then count the number of PGs scheduled for deep scrub each day of the week:\nceph pg dump -f json | jq -r \u0026#39;.pg_map.pg_stats | .[].last_deep_scrub_stamp\u0026#39; | xargs -n 1 date +%A -d | sort | uniq -c The output of this command will look something like the following:\ndumped all 52 Friday 89 Monday 61 Saturday 57 Sunday 54 Thursday 156 Tuesday 116 Wednesday "
},
{
	"uri": "/docs-csm/en-12/operations/system_layout_service/load_sls_database_with_dump_file/",
	"title": "Load SLS Database with Dump File",
	"tags": [],
	"description": "",
	"content": "Load SLS Database with Dump File Load the contents of the SLS dump file to restore SLS to the state of the system at the time of the dump. This will upload and overwrite the current SLS database with the contents of the SLS dump file.\nUse this procedure to restore SLS data after a system re-install.\nPrerequisites The System Layout Service (SLS) database has been dumped. See Dump SLS Information for more information.\nProcedure Use the get_token function to retrieve a token to validate requests to the API gateway.\nncn-m001# function get_token () { curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39; } Load the dump file into SLS.\nThis will upload and overwrite the current SLS database with the contents of the posted file.\nncn-m001# curl -X POST \\ https://api-gw-service-nmn.local/apis/sls/v1/loadstate \\ -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; \\ -F sls_dump=@sls_dump.json "
},
{
	"uri": "/docs-csm/en-12/operations/system_management_health/system_management_health/",
	"title": "System Management Health",
	"tags": [],
	"description": "",
	"content": "System Management Health The primary goal of the System Management Health service is to enable system administrators to assess the health of their system. Operators need to quickly and efficiently troubleshoot system issues as they occur and be confident that a lack of issues indicates the system is operating normally. This service currently runs as a Helm chart on the system\u0026rsquo;s management Kubernetes cluster and monitors the health status of core system components, triggering alerts as potential issues are observed. It uses Prometheus to aggregate metrics from etcd, Kubernetes, Istio, and Ceph, all of which include support for the Prometheus API. The System Management Health service relies on the following tools:\nPrometheus is the standard cloud-native metrics and monitoring tool, which includes Alertmanager, a tool that handles alert duplication, silences, and notifications The Prometheus operator provides custom resource definitions (CRDs) that make it easy to operate Prometheus and Alertmanager instances, scrape metrics from service endpoints, and trigger alerts Grafana supports pulling data from Prometheus, and dashboards for system components are readily available from the open source community The stable/prometheus-operator Helm chart integrates the Prometheus operator, Prometheus, Alertmanager, Grafana, node exporters (daemon set), and kube-state-metrics to provide a monitoring solution for Kubernetes clusters Istio supports service mesh tracing and observability using Jaeger and Kiali, respectively The System Management Health service is intended to complement the System Monitoring Application (SMA) Framework, but the two are currently not integrated. The System Management Health metrics are not available using the Telemetry API. This service scrapes metrics from system components like Ceph, Kubernetes, and the hosts using node exporter, kube-state-metrics, and cadvisor. The design is flexible and supports:\nFiltering metrics such that only those necessary to determine system health are aggregated to the top level; all metrics are currently aggregated—no filtering is implemented Independent retention and persistence settings based on needs for specific services; the current default configuration retains metrics for ten days at the top level and four hours at intermediate levels Component-specific tooling for more detailed visibility: Grafana dashboards for Kubernetes Grafana dashboards, Kiali, and Jaeger for Istio Grafana dashboards for Ceph "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/authenticate_an_account_with_the_command_line/",
	"title": "Authenticate an Account with the Command Line",
	"tags": [],
	"description": "",
	"content": "Authenticate an Account with the Command Line Retrieve a token to authenticate to the Cray CLI using the command line. If the Cray CLI is needed before localization occurs and Keycloak is setup, an administrator can use this procedure to authenticate to the Cray CLI.\nProcedure Retrieve the Kubernetes secret to be used for authentication.\nncn-mw# ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d) Create the setup-token.json file and modify it to be readable only by root.\nncn-mw# touch /tmp/setup-token.json \u0026amp;\u0026amp; chmod 600 /tmp/setup-token.json Retrieve a token for the new Keycloak account.\nncn-mw# curl -s -d grant_type=client_credentials -d client_id=admin-client -d client_secret=\u0026#34;${ADMIN_SECRET}\u0026#34; \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \u0026gt; /tmp/setup-token.json Set up the new account with the authenticated token.\nncn-mw# export CRAY_CREDENTIALS=/tmp/setup-token.json "
},
{
	"uri": "/docs-csm/en-12/operations/resiliency/restore_system_functionality_if_a_kubernetes_worker_node_is_down/",
	"title": "Restore System Functionality if a Kubernetes Worker Node is Down",
	"tags": [],
	"description": "",
	"content": "Restore System Functionality if a Kubernetes Worker Node is Down Services running on Kubernetes worker nodes can be properly restored if downtime occurs. Use this procedure to ensure that if a Kubernetes worker node is lost or restored after being down, then certain features the node was providing can also be restored or recovered on another node.\nCapture the metadata for the unhealthy node before bringing down the node. The pods will successfully terminate when the node goes down, which should resolve most pods in an error state. Once any remaining testing or validation work is complete, these pods can be restored with the file used to capture the metadata.\nCollect information before powering down the node Check the Persistent Volume Claims (PVC) that have been created on the system.\nView the PVCs in all namespaces.\nncn-mw# kubectl get pvc –A Truncated example output (some trailing lines omitted):\nNAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE jhub claim-user Bound pvc-3cf34569-4db4-11ea-b8e1-a4bf01581d70 10Gi RWO ceph-rbd-external 14d jhub claim-users Bound pvc-18b7155a-4dba-11ea-bf78-a4bf01684f9e 10Gi RWO ceph-rbd-external 14d jhub claim-user01 Bound pvc-c5df3ba1-4db3-11ea-b8e1-a4bf01581d70 10Gi RWO ceph-rbd-external 14d jhub hub-db-dir Bound pvc-b41675c6-4d4e-11ea-b8e1-a4bf01581d70 1Gi RWO ceph-rbd-external 15d loftsman loftsman-chartmuseum-data-pvc Bound pvc-7d45b88b-4575-11ea-bf78-a4bf01684f9e 1Gi RWO ceph-rbd-external 25d Get a list of PVCs for a particular pod.\nncn-mw# kubectl get pod POD_NAME -o jsonpath=\u0026#39;{.spec.volumes[*].persistentVolumeClaim.claimName}{\u0026#34;\\n\u0026#34;}\u0026#39; Verify that the time is synced across all management nodes.\nncn-mw# pdsh -w $(grep -oP \u0026#39;ncn-\\w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) date Example output:\nncn-m001: Thu Feb 27 08:41:11 CST 2020 ncn-s002: Thu Feb 27 08:41:11 CST 2020 ncn-s003: Thu Feb 27 08:41:11 CST 2020 ncn-m002: Thu Feb 27 08:41:11 CST 2020 ncn-s001: Thu Feb 27 08:41:11 CST 2020 ncn-m003: Thu Feb 27 08:41:11 CST 2020 ncn-w001: Thu Feb 27 08:41:11 CST 2020 ncn-w003: Thu Feb 27 08:41:11 CST 2020 ncn-w002: Thu Feb 27 08:41:11 CST 2020 Generate a list of pods that are running on the node that will be taken down.\nThe example below is displaying all of the pods running on ncn-w001.\nncn-mw# kubectl get pods -A -o wide | grep NODE_NAME Example output (truncated):\ndefault cray-dhcp-7b5c6496c6-76rst 1/1 Running 0 5d14h 10.252.1.1 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; default kube-keepalived-vip-mgmt-plane-nmn-local-vxldv 1/1 Running 1 25d 10.252.1.1 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ims cray-ims-57b4f98b-bc0d-422e-8891-808ab69bf158-create-nbd5c 0/2 Init:Error 0 6d21h 10.40.1.36 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ims cray-ims-60ed661b-acf2-45fd-af44-300baabfc299-customize-skswx 0/2 Completed 0 40h 10.40.1.38 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ims cray-ims-bc748ef4-49ad-4211-92db-993d097ac80e-create-6j6xv 0/2 Completed 0 6d21h 10.40.1.40 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ims cray-ims-c31232bc-d4e0-4491-90fb-e3d2fd3ca0ce-create-gzbvk 0/2 Init:Error 0 6d21h 10.40.1.45 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ims cray-ims-ccacd186-854e-4057-acfa-192ccea16ad0-customize-6qkkg 0/2 Completed 0 46h 10.40.1.52 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; istio-system istio-pilot-9d769b86c-mzshz 2/2 Running 0 4m54s 10.40.1.38 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; istio-system istio-pilot-9d769b86c-t8mtg 2/2 Running 0 5m10s 10.40.1.51 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; istio-system istio-sidecar-injector-b887db765-td7db 1/1 Running 0 12d 10.40.0.173 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Take note of any pods that are not in a Running or Completed state, or have another state that is not considered to be healthy. This will help identify after the node is brought back up what new issues may have occurred.\nTo view the pods in an unhealthy state:\nncn-mw# kubectl get pods -A -o wide | grep -v -e Completed -e Running Example output (truncated):\nNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES backups benji-k8s-backup-backups-namespace-1594161300-gk72h 0/1 Error 0 5d20h 10.45.0.109 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; backups benji-k8s-backup-backups-namespace-1594161600-kqprj 0/1 Error 0 5d20h 10.45.0.126 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; backups benji-k8s-backup-backups-namespace-1594161900-6rqcx 0/1 Error 0 5d20h 10.45.0.125 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman helm-1594233719 0/1 Error 0 5d 10.36.0.192 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman helm-1594312289 0/1 Error 0 4d2h 10.36.0.186 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman helm-1594329768 0/1 Error 0 3d21h 10.36.0.164 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman helm-1594400189 0/1 Error 0 3d1h 10.36.0.191 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman helm-1594400232 0/1 Error 0 3d1h 10.36.0.191 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594216843-v5hh4 0/1 Error 0 5d4h 10.36.0.179 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594216923-z5clh 0/1 Error 0 5d4h 10.36.0.179 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594217083-w5mzk 0/1 Error 0 5d4h 10.36.0.179 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594312344-87555 0/1 Error 0 4d2h 10.36.0.194 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594317770-z8z7q 0/1 Error 0 4d 10.36.0.186 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594328454-khsfc 0/1 Error 0 3d21h 10.36.0.190 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594329426-75shp 0/1 Error 0 3d21h 10.36.0.195 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594329510-q8bsj 0/1 Error 0 3d21h 10.36.0.164 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services boa-8b1687b2-2c5b-4c92-8cd7-a44965fef41a-mbfsj 0/2 Error 0 6d 10.42.0.152 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services boa-8b1687b2-2c5b-4c92-8cd7-a44965fef41a-q7cr4 0/2 Error 0 6d 10.42.0.154 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services boa-cf973765-92e7-4c5e-b52a-e904088976b8-cplj6 0/2 Error 0 5d23h 10.42.0.158 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services boa-f6f86426-58bf-4c6f-b3cd-e25010aa9ff6-s7zph 0/2 Error 0 4d2h 10.36.0.191 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services cfs-0336105c-e697-4d9d-a129-badde6da3218-vn6n4 0/3 Error 0 6d20h 10.42.0.98 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; View the status of the node before taking it down.\nncn-mw# kubectl get nodes -o wide Example output:\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ncn-m001 Ready control-plane,master 27h v1.20.13 10.252.1.4 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-m002 Ready control-plane,master 8d v1.20.13 10.252.1.5 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-m003 Ready control-plane,master 8d v1.20.13 10.252.1.6 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-w001 Ready \u0026lt;none\u0026gt; 8d v1.20.13 10.252.1.7 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-w002 Ready \u0026lt;none\u0026gt; 8d v1.20.13 10.252.1.8 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-w003 Ready \u0026lt;none\u0026gt; 8d v1.20.13 10.252.1.9 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 Collect information after powering down the node Shut down the node.\nread -s is used to prevent the password from being written to the screen or the shell history.\nncn# USERNAME=root ncn# read -r -s -p \u0026#34;BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -H BMC_IP_ADDRESS -v -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E chassis power off View the node status after the node is taken down.\nncn-mw# kubectl get nodes View the pods on the system to see if their states have changed.\nView all of the pods on the system.\nThe following are important things to look for when viewing the pods:\nCheck for any pods that are still running on the node that was brought down; if there are still some, make sure those are expected. View the status for all pods before looking for any new error states. ncn-mw# kubectl get pods -A -o wide Take note of any pods that are in a Pending state.\nncn-mw# kubectl get pods -A -o wide | grep Pending Capture the details for any pod that is in an unexpected state.\nncn-mw# kubectl describe pod POD_NAME Collect information after the node is powered on Power the node back on.\nread -s is used to prevent the password from being written to the screen or the shell history.\nncn# USERNAME=root ncn# read -r -s -p \u0026#34;BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -H BMC_IP_ADDRESS -v -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E chassis power on Record the status of the pods again.\nThe pods will go to an Unknown state as the node is coming up and taking inventory of its assigned pods. The number of pods in Unknown states will increase as this inventory takes place, and then will decrease as their actual states are restored to Running, Terminated, or another state.\nView all the pods on the system.\nncn-mw# kubectl get pods --all-namespaces -o wide Take note of any pods that are in a Pending or Error state.\nncn-mw# kubectl get pods -A -o wide | grep -e \u0026#39;Pending|Error\u0026#39; Capture the details for any pod that is in an unexpected state.\nncn-mw# kubectl describe pod POD_NAME The node that encountered issues should now be returned to a healthy state.\n"
},
{
	"uri": "/docs-csm/en-12/operations/power_management/power_on_compute_and_io_cabinets/",
	"title": "Power On Compute and IO Cabinets",
	"tags": [],
	"description": "",
	"content": "Power On Compute and IO Cabinets Power on liquid-cooled and standard rack cabinet PDUs.\nLiquid-cooled Cabinets - HPE Cray EX liquid-cooled cabinet CDU and PDU circuit breakers are controlled manually.\nAfter the CDU is switched on and healthy, the liquid-cooled PDU circuit breakers can be switched ON. With PDU breakers ON, the Chassis Management Modules (CMM) and Cabinet Environmental Controllers (CEC) power on and boot. These devices can then communicate with the management cluster and larger system management network. HVDC power remains OFF on liquid-cooled chassis until environmental conditions are normal and the CMMs receive a chassis power-on command from Cray System Management (CSM) software.\nStandard Racks - HPE Cray standard EIA racks include redundant PDUs. Some PDU models may require a flat-blade screw driver to open or close the PDU circuit breakers.\nPrerequisites The cabinet PDUs and coolant distribution units are connected to facility power and are healthy. An authentication token is required to access the API gateway and to use the sat command. See the \u0026ldquo;SAT Authentication\u0026rdquo; section of the HPE Cray EX System Admin Toolkit (SAT) product stream documentation (S-8031) for instructions on how to acquire a SAT authentication token. Procedure Verify with site management that it is safe to power on the system.\nIf the system does not have Cray EX liquid-cooled cabinets, proceed to Power On Standard Rack PDU Circuit Breakers.\nPower On Cray Ex Liquid-Cooled Cabinet Circuit Breakers Power on the CDU for the cabinet cooling group.\nOpen the rear door of the CDU.\nSet the control panel circuit breakers to ON.\nSet the PDU circuit breakers to on in each Cray EX cabinet.\nVerify the status LEDs on the PSU are OK.\nUse the System Admin Toolkit (sat) to power on liquid-cooled cabinets, chassis, and slots.\nncn-m001# sat bootsys boot --stage cabinet-power This command first resumes the hms-discovery Kubernetes cronjob and waits for it to be scheduled. Then, the hms-discovery job initiates power-on of the liquid-cooled cabinets. Finally, the sat bootsys command waits for the components in the liquid-cooled cabinets to be powered on. The sat bootsys command controls power only to liquid-cooled cabinets.\nThe sat bootsys command may time out while waiting for the hms-discovery cronjob to be scheduled and display the following message:\nERROR: The cronjob hms-discovery in namespace services was not scheduled within expected window after being resumed. If this occurs, first check if the cronjob needs to be re-created. To do this, follow the instructions in the Check cronjobs section of the Power On and Start the Management Kubernetes Cluster procedure.\nIf the cronjob does not need to be re-created and has been scheduled within the time expected (based on its cron schedule), execute the sat bootsys boot --stage cabinet-power command again.\nIf sat bootsys fails to power on the cabinets through hms-discovery, then use CAPMC to manually power on the cabinet chassis, compute blade slots, and all populated switch blade slots (1, 3, 5, and 7). This example shows cabinets 1000-1003.\nncn-m001# cray capmc xname_on create --xnames x[1000-1003]c[0-7] --format json ncn-m001# cray capmc xname_on create --xnames x[1000-1003]c[0-7]s[0-7] --format json ncn-m001# cray capmc xname_on create --xnames x[1000-1003]c[0-7]r[1,3,5,7] --format json Power On Standard Rack PDU Circuit Breakers Switch the standard rack compute and I/O cabinet PDU circuit breakers to ON.\nThis applies power to the server BMCs and connects them to the management network. Compute and I/O nodes do not power on and boot automatically. The Boot Orchestration Service (BOS) brings up compute nodes and User Access Nodes (UANs).\nIf necessary, use IPMI commands to power on individual servers as needed.\nVerify that all system management network switches and Slingshot network switches are powered on in each rack, and that there are no error LEDS or hardware failures.\nBring up the Slingshot Fabric. Refer to the following documentation for more information on how to bring up the Slingshot Fabric:\nThe HPE Slingshot Operations Guide PDF for HPE Cray EX systems. The HPE Slingshot Troubleshooting PDF. Next Step Return to System Power On Procedures and continue with next step.\n"
},
{
	"uri": "/docs-csm/en-12/operations/package_repository_management/package_repository_management/",
	"title": "Package Repository Management",
	"tags": [],
	"description": "",
	"content": "Package Repository Management Repositories are added to systems to extend the system functionality beyond what is initially delivered. The Sonatype Nexus Repository Manager is the primary method for repository management. Nexus hosts the Yum, Docker, raw, and Helm repositories for software and firmware content.\nRefer to the following for more information about Nexus:\nThe official Sonatype documentation Manage Repositories with Nexus "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/rebuild_ncns/re-add_storage_node_to_ceph/",
	"title": "Re-Add a Storage Node to Ceph",
	"tags": [],
	"description": "",
	"content": "Re-Add a Storage Node to Ceph Use the following procedure to re-add a Ceph node to the Ceph cluster.\nNOTE: This operation can be done to add more than one node at the same time.\nAdd Join Script Copy and paste the below script into /srv/cray/scripts/common/join_ceph_cluster.sh.\nNOTE: This script may also available in the /usr/share/doc/csm/scripts directory where the latest docs-csm RPM is installed. If so, it can be copied from that node to the new storage node being rebuilt and skip to step 2.\n#!/bin/bash (( counter=0 )) host=$(hostname) \u0026gt; ~/.ssh/known_hosts for node in ncn-s001 ncn-s002 ncn-s003; do ssh-keyscan -H \u0026#34;$node\u0026#34; \u0026gt;\u0026gt; ~/.ssh/known_hosts pdsh -w $node \u0026gt; ~/.ssh/known_hosts if [[ \u0026#34;$host\u0026#34; == \u0026#34;$node\u0026#34; ]]; then continue fi if [[ $(nc -z -w 10 $node 22) ]] || [[ $counter -lt 3 ]] then if [[ \u0026#34;$host\u0026#34; =~ ^(\u0026#34;ncn-s001\u0026#34;|\u0026#34;ncn-s002\u0026#34;|\u0026#34;ncn-s003\u0026#34;)$ ]] then scp $node:/etc/ceph/* /etc/ceph else scp $node:/etc/ceph/rgw.pem /etc/ceph/rgw.pem fi if [[ ! $(pdsh -w $node \u0026#34;/srv/cray/scripts/common/pre-load-images.sh; ceph orch host rm $host; ceph cephadm generate-key; ceph cephadm get-pub-key \u0026gt; ~/ceph.pub; ssh-keyscan -H $host \u0026gt;\u0026gt; ~/.ssh/known_hosts ;ssh-copy-id -f -i ~/ceph.pub root@$host; ceph orch host add $host\u0026#34;) ]] then (( counter+1 )) if [[ $counter -ge 3 ]] then echo \u0026#34;Unable to access ceph monitor nodes\u0026#34; exit 1 fi else break fi fi done sleep 30 (( ceph_mgr_failed_restarts=0 )) (( ceph_mgr_successful_restarts=0 )) until [[ $(cephadm shell -- ceph-volume inventory --format json-pretty|jq \u0026#39;.[] | select(.available == true) | .path\u0026#39; | wc -l) == 0 ]] do for node in ncn-s001 ncn-s002 ncn-s003; do if [[ $ceph_mgr_successful_restarts \u0026gt; 10 ]] then echo \u0026#34;Failed to bring in OSDs, manual troubleshooting required.\u0026#34; exit 1 fi if pdsh -w $node ceph mgr fail then (( ceph_mgr_successful_restarts+1 )) sleep 120 break else (( ceph_mgr_failed_restarts+1 )) if [[ $ceph_mgr_failed_restarts -ge 3 ]] then echo \u0026#34;Unable to access ceph monitor nodes.\u0026#34; exit 1 fi fi done done for service in $(cephadm ls | jq -r \u0026#39;.[].systemd_unit\u0026#39;) do systemctl enable $service done Change the mode of the script.\nncn-s# chmod u+x /srv/cray/scripts/common/join_ceph_cluster.sh In a separate window, log into one of the first three storage nodes (ncn-s001, ncn-s002, or ncn-s003) and execute the following:\nncn-ms# watch ceph -s Execute the script.\n/srv/cray/scripts/common/join_ceph_cluster.sh IMPORTANT: While watching the window running watch ceph -s, the health will go to a HEALTH_WARN state. This is expected. Most commonly, there will be an alert about \u0026ldquo;failed to probe daemons or devices\u0026rdquo; and this will clear.\nZap OSDs IMPORTANT: Only do this if unable to wipe the node prior to rebuild.\nNOTE: The commands in the Zapping OSDs section must be run on a node running ceph-mon. Typically these are ncn-s001, ncn-s002, and ncn-s003.\nFind the devices on the node being rebuilt.\nceph orch device ls $NODE Example Output:\nHostname Path Type Serial Size Health Ident Fault Available ncn-s003 /dev/sdc ssd S455NY0MB42493 1920G Unknown N/A N/A No ncn-s003 /dev/sdd ssd S455NY0MB42482 1920G Unknown N/A N/A No ncn-s003 /dev/sde ssd S455NY0MB42486 1920G Unknown N/A N/A No ncn-s003 /dev/sdf ssd S455NY0MB51808 1920G Unknown N/A N/A No ncn-s003 /dev/sdg ssd S455NY0MB42473 1920G Unknown N/A N/A No ncn-s003 /dev/sdh ssd S455NY0MB42468 1920G Unknown N/A N/A No IMPORTANT: In the above example the drives on our rebuilt node are showing \u0026ldquo;Available = no\u0026rdquo;. This is expected because the check is based on the presence of an LVM on the volume.\nNOTE: The ceph orch device ls $NODE command excludes the drives being used for the OS. Please double check that there are no OS drives. These will have a size of 480G.\nZap the drives.\nfor drive in $(ceph orch device ls $NODE --format json-pretty |jq -r \u0026#39;.[].devices[].path\u0026#39;) do ceph orch device zap $NODE $drive --force done Validate the drives are being added to the cluster.\nncn-ms# watch ceph -s The returned output will have the OSD count UP and IN counts increase. If the IN count increases but does not reflect the amount of drives being added back in, an administrator must fail over the ceph-mgr daemon. This is a known bug and is addressed in newer releases.\nIf necessary, fail over the ceph-mgr daemon with the following command:\nncn-s# ceph mgr fail Regenerate Rados-GW Load Balancer Configuration for the Rebuilt Nodes IMPORTANT: Rados-GW by default is deployed to the first 3 storage nodes. This includes HAproxy and Keepalived. This is automated as part of the install, but administrators may have to regenerate the configuration if they are not running on the first 3 storage nodes or all nodes.\nDeploy Rados Gateway containers to the new nodes.\nConfigure Rados Gateway containers with the complete list of nodes it should be running on:\nncn-s# ceph orch apply rgw site1 zone1 --placement=\u0026#34;\u0026lt;node1 node2 node3 node4 ... \u0026gt;\u0026#34; Verify Rados Gateway is running on the desired nodes.\nncn-s00(1/2/3)# ceph orch ps --daemon_type rgw Example output:\nNAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE D CONTAINER ID rgw.site1.zone1.ncn-s001.kvskqt ncn-s001 running (41m) 6m ago 41m 15.2.8 registry.local/ceph/ceph:v15.2.8 553b0cb212c 6e323878db46 rgw.site1.zone1.ncn-s002.tisuez ncn-s002 running (41m) 6m ago 41m 15.2.8 registry.local/ceph/ceph:v15.2.8 553b0cb212c 278830a273d3 rgw.site1.zone1.ncn-s003.nnwuqy ncn-s003 running (41m) 6m ago 41m 15.2.8 registry.local/ceph/ceph:v15.2.8 553b0cb212c a9706e6d7a69 Add nodes into HAproxy and KeepAlived.\nncn-s# pdsh -w ncn-s00[1-(end node number)] -f 2 \\ \u0026#39;source /srv/cray/scripts/metal/update_apparmor.sh reconfigure-apparmor; /srv/cray/scripts/metal/generate_haproxy_cfg.sh \u0026gt; /etc/haproxy/haproxy.cfg systemctl enable haproxy.service systemctl restart haproxy.service /srv/cray/scripts/metal/generate_keepalived_conf.sh \u0026gt; /etc/keepalived/keepalived.conf systemctl enable keepalived.service systemctl restart keepalived.service\u0026#39; Next Step Proceed to the next step: Storage Node Validation\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_remove_replace_ncns/collect_ncn_mac_addresses/",
	"title": "Collect NCN MAC Addresses",
	"tags": [],
	"description": "",
	"content": "Collect NCN MAC Addresses This procedure can be used to to collect MAC addresses from the NCNs along with their assigned interface names for use with the Add NCN Data procedure. A temporary MAC address collection iPXE bootscript is put into place on the system to discover the MAC addresses of the NCNs, along with their associated interface names (such as mgmt0).\nWARNING This procedure will temporarily break the system\u0026rsquo;s ability to properly boot nodes in the system.\nPrerequisites This procedure must be performed on a master or worker NCN that has the latest CSM documentation installed. See Check for latest documentation.\nProcedure Verify that the BMC_IP environment variable is set.\nncn-mw# echo ${BMC_IP} Put the MAC address collection iPXE script in place.\nSave a backup of the current iPXE BSS bootscript.\nncn-mw# kubectl -n services get cm cray-ipxe-bss-ipxe -o yaml \u0026gt; cray-ipxe-bss-ipxe.backup.yaml Delete the cray-ipxe-bss-ipxe Kubernetes ConfigMap.\nncn-mw# kubectl -n services delete cm cray-ipxe-bss-ipxe Put the MAC address collection iPXE boot script into place.\nncn-mw# kubectl -n services create cm cray-ipxe-bss-ipxe --from-file=bss.ipxe=/usr/share/doc/csm/scripts/operations/node_management/Add_Remove_Replace_NCNs/mac_collection_script.ipxe Take note of the last timestamp in the cray-ipxe log.\nncn-mw# kubectl -n services logs -l app.kubernetes.io/name=cray-ipxe -c cray-ipxe Wait for the updated iPXE binary to be built.\nncn-mw# sleep 30 ncn-mw# kubectl -n services logs -l app.kubernetes.io/name=cray-ipxe -c cray-ipxe -f The following output means that a new iPXE binary has been built:\n2022-03-17 22:16:14,648 - INFO - __main__ - Build completed. 2022-03-17 22:16:14,653 - INFO - __main__ - Newly created ipxe binary created: \u0026#39;/shared_tftp/ipxe.efi\u0026#39; Wait for a build notification message with a timestamp that is more recent than the timestamp recorded in the previous step.\nPower on node and collect MAC addresses from the NCN.\nVerify that the NCN is off.\nread -s is used in order to prevent the password from being echoed to the screen or saved in the shell history.\nncn-mw# read -r -s -p \u0026#34;${BMC_IP} root password: \u0026#34; IPMI_PASSWORD ncn-mw# export IPMI_PASSWORD ncn-mw# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC_IP}\u0026#34; chassis power status In another terminal, capture the NCN\u0026rsquo;s Serial Over Lan (SOL) console.\nread -s is used in order to prevent the password from being echoed to the screen or saved in the shell history.\nncn-mw# BMC_IP=BMC_IP_ADDRESS ncn-mw# read -r -s -p \u0026#34;${BMC_IP} root password: \u0026#34; IPMI_PASSWORD ncn-mw# export IPMI_PASSWORD ncn-mw# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC_IP}\u0026#34; sol activate When disconnecting from the IPMI SOL console, use the key sequence ~~. to exit ipmitool without exiting the SSH session.\nSet the PXE efiboot option.\nncn-mw# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC_IP}\u0026#34; chassis bootdev pxe options=efiboot Power on the NCN.\nncn-mw# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC_IP}\u0026#34; chassis power on Watch the NCN SOL console and wait for the following output to appear.\nThe output below shows the mapping of MAC addresses to interface names (mgmt0, mgmt1, hsn0, lan0, etc.).\n====DEVICE NAMING======================================================= net0 MAC ec:0d:9a:d4:2b:d8 net0 is hsn0 net1 MAC 98:03:9b:bb:a9:94 net1 is mgmt0 net2 MAC 98:03:9b:bb:a9:95 net2 is mgmt1 MAC Address collection completed. Please power the node off now via ipmitool. Using the above output from the MAC collection iPXE script, derive the following add_management_ncn.py script arguments:\nInterface MAC Address CLI Flag mgmt0 98:03:9b:bb:a9:94 --mac-mgmt0=98:03:9b:bb:a9:94 mgmt1 98:03:9b:bb:a9:95 --mac-mgmt1=98:03:9b:bb:a9:95 hsn0 ec:0d:9a:d4:2b:d8 --mac-hsn0=ec:0d:9a:d4:2b:d8 Power off the NCN.\nncn-mw# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC_IP}\u0026#34; chassis power off Restore the original iPXE bootscript.\nDelete the cray-ipxe-bss-ipxe Kubernetes ConfigMaps.\nncn-m# kubectl -n services delete cm cray-ipxe-bss-ipxe Put the original iPXE bootscript into place.\nncn-mw# cp -v cray-ipxe-bss-ipxe.backup.yaml cray-ipxe-bss-ipxe.yaml ncn-mw# yq d -i cray-ipxe-bss-ipxe.yaml metadata.resourceVersion ncn-mw# yq d -i cray-ipxe-bss-ipxe.yaml metadata.uid ncn-mw# kubectl -n services apply -f cray-ipxe-bss-ipxe.yaml Take note of the last timestamp in the cray-ipxe log.\nncn-mw# kubectl -n services logs -l app.kubernetes.io/name=cray-ipxe -c cray-ipxe Wait for the updated iPXE binary to be built.\nncn-mw# sleep 30 ncn-mw# kubectl -n services logs -l app.kubernetes.io/name=cray-ipxe -c cray-ipxe -f The following output means that a new iPXE binary has been built.\n2022-03-17 22:16:14,648 - INFO - __main__ - Build completed. 2022-03-17 22:16:14,653 - INFO - __main__ - Newly created ipxe binary created: \u0026#39;/shared_tftp/ipxe.efi\u0026#39; Wait for a build notification message with a timestamp that is more recent than the timestamp recorded in the previous step.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/check_bgp_and_metallb/",
	"title": "Check BGP and MetalLB",
	"tags": [],
	"description": "",
	"content": "Check BGP and MetalLB Log in to the spine switches if you have access and check that MetalLB is peering to the spines via BGP.\nCheck both spines if they are available (powered up):\nshow ip bgp summary Example working state:\nAll the neighbors should be in the Established state.\nsw-spine01 [standalone: master] # show ip bgp summary VRF name : default BGP router identifier : 10.252.0.1 local AS number : 65533 BGP table version : 6 Main routing table version: 6 IPV4 Prefixes : 84 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.0.4 4 65533 465 501 6 0 0 0:03:37:43 ESTABLISHED/28 10.252.0.5 4 65533 463 501 6 0 0 0:03:36:51 ESTABLISHED/28 10.252.0.6 4 65533 463 500 6 0 0 0:03:36:39 ESTABLISHED/28 If the State/pfxrcd is \u0026ldquo;IDLE\u0026rdquo; you need to restart the BGP process with the following command:\nsw-spine01 [standalone: master] # clear ip bgp all Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/hostname/",
	"title": "Configure Hostnames",
	"tags": [],
	"description": "",
	"content": "Configure Hostnames A hostname is a human-friendly name used to identify a device. An example of a hostname could be the name \u0026ldquo;Test.\u0026rdquo;\nConfiguration Commands Create a hostname:\nswitch(config)# hostname NAME Show commands to validate functionality:\nswitch# show hostname Example Output switch(config)# hostname switch-test switch-test# show hostname switch-test Expected Results Administrators can configure the hostname The output of all show commands is correct Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/canu/introduction_to_canu/",
	"title": "Introduction to CANU",
	"tags": [],
	"description": "",
	"content": "Introduction to CANU The CSM Automatic Network Utility (CANU) guides administrators through the installation of new Shasta networks. CANU helps ensure that the installation follows best practices and helps administrators set up a supported configuration.\nThe following are some of the tasks that CANU can perform:\nCheck if the management switches on a Shasta network meet the firmware version requirements Check the cabling status of the management switches on a Shasta network using LLDP. Use a CANU-generated configuration to compare an existing network configuration against the best practice configuration. CANU reads switch version information from the canu.yaml file in the root directory.\nAdditional information can be found in the CANU documentation.\nIf doing a CSM install or upgrade, a CANU RPM is located in the release tarball. For more information, see this procedure: Update CANU From CSM Tarball\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/bluetooth/",
	"title": "Bluetooth Capabilities",
	"tags": [],
	"description": "",
	"content": "Bluetooth Capabilities The Bluetooth feature allows Bluetooth enabled devices to connect to and manage the switch on a wireless Bluetooth Personal Area Network (PAN). The user needs a supported USB Bluetooth dongle and to enable both the USB port and Bluetooth on the switch to use this feature. Bluetooth and REST write permissions for Bluetooth clients are both enabled by default.\nConfiguration Commands Turn on the USB port:\nswitch# usb mount Enable Bluetooth:\nswitch# bluetooth enable Show Commands to Validate Functionality:\nswitch# show bluetooth Example Output Bluetooth enabled:\nSwitch(config)# bluetooth enable Switch(config)# show bluetooth Enabled Device name Adapter State Adapter IP address : 192.168.0.1 Adapter MAC address : e0x34-60126 : Yes : 8320-TJ12690890 : Ready Connected Clients ----------------- Name MAC Address ---------------------- -------------- ---------------- ------------------------ Bluetooth not enabled:\nSwitch# no Bluetooth enable Switch# show bluetooth Enabled : No Expected Results The USB mounts properly Administrators can see and connect to the Bluetooth PAN Administrators can edit the configuration via the Bluetooth connection The output of the show commands looks correct [Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/dns/troubleshoot_common_dns_issues/",
	"title": "Troubleshoot Common DNS Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Common DNS Issues The Domain Name Service (DNS) is part of an integrated infrastructure set designed to provide dynamic host discovery, addressing, and naming. There are several different place to look for troubleshooting as DNS interacts with Dynamic Host Configuration Protocol (DHCP), the Hardware Management Service (HMS), the System Layout Service (SLS), and the State Manager Daemon (SMD).\nThe information below describes what to check when experiencing issues with DNS.\nTroubleshoot an invalid hostname It is important to verify if a hostname is correct. The values in the networks.yml or networks_derived.yml files are sometimes inaccurate.\nThe formats show below are valid hostnames:\nComponent names (xnames) Node Management Network (NMN): \u0026lt;xname\\\u0026gt; \u0026lt;xname\\\u0026gt;.local Hardware Management Network (HMN): \u0026lt;xname\\\u0026gt;-mgmt \u0026lt;xname\\\u0026gt;-mgmt.local NID Node Management Network (NMN): nid\u0026lt;nid\\_number\\\u0026gt;-nmn nid\u0026lt;nid\\_number\\\u0026gt;-nmn.local Additional steps are needed if a hostname or component name (xname) is either listed incorrectly or not listed at all in the networks.yml or networks_derived.yml files. In that case, the following actions must be taken:\nUpdate the hostname in the Hardware State Manager (HSM). Re-run any Ansible plays that require the data in these files. Check if a host is in DNS Use the dig or nslookup commands directly against the Unbound resolver. A host is correctly in DNS if the response from the dig command includes the following:\nThe ANSWER SECTION value exists with a valid hostname and IP address. A QUERY value exists that has the status: NOERROR message. ncn# dig HOSTNAME @10.92.100.225 Example output:\n; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.2 \u0026lt;\u0026lt;\u0026gt;\u0026gt; x3000c0r41b0 @10.92.100.225 ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 57196 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;x3000c0r41b0. IN A ;; ANSWER SECTION: x3000c0r41b0. 3600 IN A 10.254.127.200 ;; Query time: 0 msec ;; SERVER: 10.92.100.225#53(10.92.100.225) ;; WHEN: Fri Jul 17 18:49:48 UTC 2020 ;; MSG SIZE rcvd: 57 If either of the commands fail to meet the two conditions mentioned above, then collect the logs to troubleshoot.\nIf there no record in the Unbound pod, that is also an indication that the host is not in DNS.\nncn-mw# kubectl -n services get cm cray-dns-unbound -o jsonpath=\u0026#39;{.binaryData.records\\.json\\.gz}\u0026#39; | base64 --decode | gzip -dc | jq -c \u0026#39;.[]\u0026#39; | grep XNAME Example output excerpt:\n{\u0026#34;hostname\u0026#34;: \u0026#34;x1003c7s7b0\u0026#34;, \u0026#34;ip-address\u0026#34;: \u0026#34;10.104.12.191\u0026#34;} Check the cray-dns-unbound logs for errors Use the following command to check the logs. Any logs with a message saying ERROR or Exception are an indication that the Unbound service is not healthy.\nncn-mw# kubectl logs -n services -l app.kubernetes.io/instance=cray-dns-unbound -c cray-dns-unbound Example output excerpt:\n[1596224129] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224129] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224135] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224135] unbound[8:0] debug: using localzone health.check.unbound. transparent To view the DNS Helper logs:\nncn-mw# kubectl logs -n services pod/$(kubectl get -n services pods | grep unbound | tail -n 1 | cut -f 1 -d \u0026#39; \u0026#39;) -c manager | tail -n4 Example output:\nuid: bc1e8b7f-39e2-49e5-b586-2028953d2940 Comparing new and existing DNS records. No differences found. Skipping DNS update Verify that MetalLB/BGP peering and routes are correct Log in to the spine switches and verify that MetalLB is peering to the spines via BGP.\nCheck both spines if they are available and powered up. All worker nodes should be peered with the spine BGP.\nsw-spine# show ip bgp neighbors Example output:\nBGP neighbor: 10.252.0.4, remote AS: 65533, link: internal: Route-map (in/out) : rm-ncn-w001 BGP version : 4 Configured hold time in seconds : 180 keepalive interval in seconds (configured) : 60 keepalive interval in seconds (established with peer): 30 Minimum holdtime from neighbor in seconds : 90BGP neighbor: 10.252.0.5, remote AS: 65533, link: internal: Route-map (in/out) : rm-ncn-w002 BGP version : 4 Configured hold time in seconds : 180 keepalive interval in seconds (configured) : 60 keepalive interval in seconds (established with peer): 30 Minimum holdtime from neighbor in seconds : 90BGP neighbor: 10.252.0.6, remote AS: 65533, link: internal: Route-map (in/out) : rm-ncn-w003 BGP version : 4 Configured hold time in seconds : 180 keepalive interval in seconds (configured) : 60 keepalive interval in seconds (established with peer): 30 Minimum holdtime from neighbor in seconds : 90 Confirm that routes to Kea (10.92.100.222) via all the NCN worker nodes are available:\nsw-spine# show ip route 10.92.100.222 Example output:\nFlags: F: Failed to install in H/W B: BFD protected (static route) i: BFD session initializing (static route) x: protecting BFD session failed (static route) c: consistent hashing p: partial programming in H/W VRF Name default: ------------------------------------------------------------------------------------------------------ Destination Mask Flag Gateway Interface Source AD/M ------------------------------------------------------------------------------------------------------ default 0.0.0.0 c 10.102.255.9 eth1/16 static 1/1 10.92.100.222 255.255.255.255 c 10.252.0.4 vlan2 bgp 200/0 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 tcpdump Verify that the NCN is receiving DNS queries. On an NCN worker or manager with kubectl installed, run the following command:\nncn-mw# tcpdump -envli bond0.nmn0 port 53 The ping and ssh commands fail for hosts in DNS If the IP address returned by the ping command is different than the IP address returned by the dig command, then restart nscd on the impacted node. This is done with the following command:\nncn# systemctl restart nscd.service Attempt to ping or ssh to the IP address that was experiencing issues after restarting nscd.\nCheck for missing DHCP leases Search for a DHCP lease by checking active leases for the service:\nncn# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \\ \u0026#34;Content-Type: application/json\u0026#34; \\-d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: \\ [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api-gw-service-nmn.local/apis/dhcp-kea | jq For example:\nncn# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X \\ POST -H \u0026#34;Content-Type: application/json\u0026#34; \\-d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: \\ [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api-gw-service-nmn.local/apis/dhcp-kea \\ | jq | grep x3000c0s19b4 -A 6 -B 4 Example output:\n{ \u0026#34;cltt\u0026#34;: 1597777241, \u0026#34;fqdn-fwd\u0026#34;: true, \u0026#34;fqdn-rev\u0026#34;: true, \u0026#34;hostname\u0026#34;: \u0026#34;x3000c0s19b4\u0026#34;, \u0026#34;hw-address\u0026#34;: \u0026#34;a4:bf:01:3e:d2:94\u0026#34;, \u0026#34;ip-address\u0026#34;: \u0026#34;10.254.127.205\u0026#34;, \u0026#34;state\u0026#34;: 0, \u0026#34;subnet-id\u0026#34;: 1, \u0026#34;valid-lft\u0026#34;: 300 } If there is not a DHCP lease found, then:\nEnsure the system is running and that its DHCP client is still sending requests. Reboot the system via Redfish/IPMI if required. See Troubleshoot DHCP Issues for more information. "
},
{
	"uri": "/docs-csm/en-12/operations/network/external_dns/troubleshoot_systems_not_provisioned_with_external_ip_addresses/",
	"title": "Troubleshoot Connectivity to Services with External IP addresses",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Connectivity to Services with External IP addresses Systems that do not support CMN/CAN/CHN will not have services provisioned with external IP addresses on CMN/CAN/CHN. Kubernetes will report a \u0026lt;pending\u0026gt; status for the external IP address of the service experiencing connectivity issues.\nIf SSH access to a non-compute node (NCN) is available, it is possible to override resolution of external hostnames and forward local ports into the cluster for the cluster IP address of the corresponding service.\nWARNING: This will bypass the OAuth2 Proxy and Istio ingress gateway, which handle authentication and authorization.\nEnable systems without CMN to provision services with external hostnames.\nPrerequisites The Customer Management Network (CMN) is not supported on the system.\nProcedure Search for the VirtualService object that corresponds to the desired service.\nThe command below will list all external hostnames.\nncn-w001# kubectl get vs -A | grep -v \u0026#39;[*]\u0026#39; Example output:\nNAMESPACE NAME GATEWAYS HOSTS AGE istio-system kiali [services/services-gateway] [kiali-istio.cmn.SYSTEM_DOMAIN_NAME] 2d16h istio-system tracing [services/services-gateway] [jaeger-istio.cmn.SYSTEM_DOMAIN_NAME] 2d16h nexus nexus [services/services-gateway] [packages.local registry.local nexus.cmn.SYSTEM_DOMAIN_NAME] 2d16h services gitea-vcs-external [services/services-gateway] [vcs.cmn.SYSTEM_DOMAIN_NAME] 2d16h services sma-grafana [services-gateway] [sma-grafana.cmn.SYSTEM_DOMAIN_NAME] 2d16h services sma-kibana [services-gateway] [sma-kibana.cmn.SYSTEM_DOMAIN_NAME] 2d16h sysmgmt-health cray-sysmgmt-health-alertmanager [services/services-gateway] [alertmanager.cmn.SYSTEM_DOMAIN_NAME] 2d16h sysmgmt-health cray-sysmgmt-health-grafana [services/services-gateway] [grafana.cmn.SYSTEM_DOMAIN_NAME] 2d16h sysmgmt-health cray-sysmgmt-health-prometheus [services/services-gateway] [prometheus.cmn.SYSTEM_DOMAIN_NAME] 2d16h Lookup the cluster IP and port for service.\nThe example below is for the cray-sysmgmt-health-promet-prometheus service.\nncn-w001# kubectl -n sysmgmt-health get service cray-sysmgmt-health-promet-prometheus Example output:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cray-sysmgmt-health-promet-prometheus ClusterIP 10.25.124.159 \u0026lt;none\u0026gt; 9090/TCP 23h Setup port forwarding from a laptop or workstation to access the service.\nUse the cluster IP and port for the service obtained in the previous step. If the port is unprivileged, use the same port number on the local side.\nReplace the cluster IP, port, and system name values in the example below.\n# ssh -L 9090:10.25.124.159:9090 root@SYSTEM_NCN_DOMAIN_NAME Visit http://localhost:9090/ in a laptop or workstation browser.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/customer_accessible_networks/bi-can_arista_aruba_config/",
	"title": "BI-CAN Aruba/Arista Configuration",
	"tags": [],
	"description": "",
	"content": "BI-CAN Aruba/Arista Configuration This is an example configuration of how to connect two Aruba spine switches to two Arista switches. This example is from a running system utilizing the bifurcated CAN feature offered in CSM 1.2.\nSummary:\nTwo Aruba 8325 switches running in a VSX cluster. Two Arista 7060CX2-32S switches running MLAG. The Arista switches are connected to the Slingshot/HSN network via static MLAG. The Aruba Spine switches are connected to the Arista switches with point-to-point OSPF links. The Arista Edge switches are redistributing the default route to the Aruba switches via OSPF. This allows the Shasta cluster to have external access. The Arista switches are utilizing a static default route. This is an example only, it should only be used as a reference only. DO NOT copy any configuration from the examples in this section.\nTopology Configurations sw-edge-001 ! device: sw-edge-001 (DCS-7060CX2-32S, EOS-4.23.0F) ! ! boot system flash:EOS-4.23.0F.swi ! transceiver qsfp default-mode 4x10G ! hostname sw-edge-001 ! spanning-tree mode none no spanning-tree vlan-id 4094 ! port-channel load-balance trident fields mac src-mac dst-mac port-channel load-balance trident fields ip source-ip destination-ip source-port destination-port ! service unsupported-transceiver PGSPermanentUnlock \u0026lt;removed\u0026gt; ! no aaa root ! username admin privilege 15 role network-admin secret sha512 \u0026lt;removed\u0026gt; ! vlan 5 name CHN ! vlan 4094 trunk group mlagpeer ! interface Port-Channel10 switchport mode trunk switchport trunk group mlagpeer ! interface Port-Channel64 mtu 9214 switchport access vlan 5 mlag 64 ! interface Port-Channel251 mtu 9214 switchport access vlan 5 ! interface Ethernet1/1 mtu 9198 speed auto 100gfull no switchport ip address 192.168.80.0/31 ip ospf network point-to-point ip ospf area 0.0.0.0 ! interface Ethernet2/1 mtu 9198 speed auto 100gfull no switchport ip address 192.168.80.2/31 ip ospf network point-to-point ip ospf area 0.0.0.0 ! interface Ethernet3/1 channel-group 10 mode active ! interface Ethernet4/1 channel-group 10 mode active ! interface Ethernet17/1 speed forced 10000full no switchport ip address 10.103.15.234/30 ! interface Ethernet29/1 mtu 9214 flowcontrol send on flowcontrol receive on speed forced 100gfull error-correction encoding reed-solomon channel-group 64 mode on ! interface Ethernet30/1 mtu 9214 flowcontrol send on flowcontrol receive on speed forced 100gfull error-correction encoding reed-solomon channel-group 64 mode on ! interface Ethernet31/1 mtu 9214 flowcontrol send on flowcontrol receive on speed forced 100gfull error-correction encoding reed-solomon channel-group 64 mode on ! interface Ethernet32/1 mtu 9214 flowcontrol send on flowcontrol receive on speed forced 100gfull error-correction encoding reed-solomon channel-group 64 mode on ! interface Management1 ! interface Vlan5 mtu 9214 ip address 10.103.0.194/26 ip virtual-router address 10.103.0.193 mac address virtual-router ! interface Vlan4094 ip address 10.0.0.1/30 ! ip virtual-router mac-address 00:1c:73:00:0a:13 ! ip route 0.0.0.0/0 10.103.15.233 ! ip routing ! mlag configuration domain-id mlag1 local-interface Vlan4094 peer-address 10.0.0.2 peer-link Port-Channel10 ! router ospf 1 router-id 10.103.15.234 max-lsa 12000 default-information originate ! end sw-edge-002 ! device: sw-edge-002 (DCS-7060CX2-32S, EOS-4.23.0F) ! ! boot system flash:EOS-4.23.0F.swi ! transceiver qsfp default-mode 4x10G ! hostname sw-edge-002 ! spanning-tree mode none no spanning-tree vlan-id 4094 ! port-channel load-balance trident fields mac src-mac dst-mac port-channel load-balance trident fields ip source-ip destination-ip source-port destination-port ! service unsupported-transceiver PGSPermanentUnlock \u0026lt;removed\u0026gt; ! no aaa root ! username admin privilege 15 role network-admin secret sha512 \u0026lt;removed\u0026gt; ! vlan 5 name CHN ! vlan 4094 trunk group mlagpeer ! interface Port-Channel10 switchport mode trunk switchport trunk group mlagpeer ! interface Port-Channel64 mtu 9214 switchport access vlan 5 mlag 64 ! interface Port-Channel252 mtu 9214 switchport access vlan 5 ! interface Ethernet1/1 mtu 9198 speed auto 100gfull no switchport ip address 192.168.80.4/31 ip ospf network point-to-point ip ospf area 0.0.0.0 ! interface Ethernet2/1 mtu 9198 speed auto 100gfull no switchport ip address 192.168.80.6/31 ip ospf network point-to-point ip ospf area 0.0.0.0 ! interface Ethernet3/1 channel-group 10 mode active ! interface Ethernet4/1 channel-group 10 mode active ! interface Ethernet18/1 speed forced 10000full no switchport ip address 10.103.15.238/30 ! interface Ethernet29/1 mtu 9214 flowcontrol send on flowcontrol receive on speed forced 100gfull error-correction encoding reed-solomon channel-group 64 mode on ! interface Ethernet30/1 mtu 9214 flowcontrol send on flowcontrol receive on speed forced 100gfull error-correction encoding reed-solomon channel-group 64 mode on ! interface Ethernet31/1 mtu 9214 flowcontrol send on flowcontrol receive on speed forced 100gfull error-correction encoding reed-solomon channel-group 64 mode on ! interface Ethernet32/1 mtu 9214 flowcontrol send on flowcontrol receive on speed forced 100gfull error-correction encoding reed-solomon channel-group 64 mode on ! interface Management1 ! interface Vlan5 description HSN mtu 9214 ip address 10.103.0.195/26 ip virtual-router address 10.103.0.193 mac address virtual-router ! interface Vlan4094 ip address 10.0.0.2/30 ! ip virtual-router mac-address 00:1c:73:00:0a:13 ! ip route 0.0.0.0/0 10.103.15.237 ! ip routing ! mlag configuration domain-id mlag1 local-interface Vlan4094 peer-address 10.0.0.1 peer-link Port-Channel10 ! router ospf 1 router-id 10.103.15.238 max-lsa 12000 default-information originate ! end sw-spine-001 Current configuration: ! !Version ArubaOS-CX GL.10.09.0010 !export-password: default hostname sw-spine-001 no ip icmp redirect profile leaf vrf Customer vrf keepalive ntp server 10.252.1.10 ntp server 10.252.1.8 ntp server 10.252.1.9 ntp enable ! ! ! ! ! ssh server vrf Customer ssh server vrf default ssh server vrf keepalive ssh server vrf mgmt access-list ip cmn-can 10 deny any 10.103.0.0/255.255.255.128 10.103.0.128/255.255.255.192 20 deny any 10.103.0.128/255.255.255.192 10.103.0.0/255.255.255.128 30 permit any any any access-list ip mgmt 10 comment ALLOW SSH, HTTPS, AND SNMP ON HMN SUBNET and CMN 20 permit tcp 10.254.0.0/255.255.128.0 any eq ssh 30 permit tcp 10.254.0.0/255.255.128.0 any eq https 40 permit udp 10.254.0.0/255.255.128.0 any eq snmp 50 permit udp 10.254.0.0/255.255.128.0 any eq snmp-trap 60 permit tcp 10.103.0.0/255.255.255.128 any eq ssh 70 permit tcp 10.103.0.0/255.255.255.128 any eq https 80 permit udp 10.103.0.0/255.255.255.128 any eq snmp 90 permit udp 10.103.0.0/255.255.255.128 any eq snmp-trap 100 comment ALLOW SNMP FROM HMN METALLB SUBNET 110 permit udp 10.94.100.0/255.255.255.0 any eq snmp 120 permit udp 10.94.100.0/255.255.255.0 any eq snmp-trap 130 comment BLOCK SSH, HTTPS, AND SNMP FROM EVERYWHERE ELSE 140 deny tcp any any eq ssh 150 deny tcp any any eq https 160 deny udp any any eq snmp 170 deny udp any any eq snmp-trap 180 comment ALLOW ANYTHING ELSE 190 permit any any any access-list ip nmn-hmn 10 deny any 10.252.0.0/255.255.128.0 10.254.0.0/255.255.128.0 20 deny any 10.254.0.0/255.255.128.0 10.252.0.0/255.255.128.0 30 deny any 10.252.0.0/255.255.128.0 10.104.0.0/255.255.128.0 40 deny any 10.254.0.0/255.255.128.0 10.100.0.0/255.255.128.0 50 deny any 10.100.0.0/255.255.128.0 10.254.0.0/255.255.128.0 60 deny any 10.100.0.0/255.255.128.0 10.104.0.0/255.255.128.0 70 deny any 10.104.0.0/255.255.128.0 10.252.0.0/255.255.128.0 80 deny any 10.104.0.0/255.255.128.0 10.100.0.0/255.255.128.0 90 deny any 10.92.100.0/255.255.255.0 10.254.0.0/255.255.128.0 100 deny any 10.94.100.0/255.255.255.0 10.252.0.0/255.255.128.0 110 deny any 10.254.0.0/255.255.128.0 10.92.100.0/255.255.255.0 120 deny any 10.252.0.0/255.255.128.0 10.94.100.0/255.255.255.0 130 permit any any any apply access-list ip mgmt control-plane vrf default vlan 1 vlan 2 name NMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 4 name HMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 6 name CAN apply access-list ip cmn-can in apply access-list ip cmn-can out vlan 7 name CMN apply access-list ip cmn-can in apply access-list ip cmn-can out vlan 10 name SUN spanning-tree spanning-tree priority 0 spanning-tree config-name MST0 spanning-tree config-revision 1 interface mgmt shutdown ip dhcp interface lag 101 multi-chassis no shutdown description spine_to_leaf_lag no routing vlan trunk native 1 vlan trunk allowed 1-2,4,6-7 lacp mode active spanning-tree root-guard interface lag 103 multi-chassis no shutdown description spine_to_leaf_lag no routing vlan trunk native 1 vlan trunk allowed 1-2,4,6-7 lacp mode active spanning-tree root-guard interface lag 201 multi-chassis no shutdown description sw-spine-001:15==\u0026gt;sw-cdu-001:49 no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7 lacp mode active spanning-tree root-guard interface lag 256 no shutdown description ISL link no routing vlan trunk native 1 tag vlan trunk allowed all lacp mode active interface 1/1/1 no shutdown mtu 9198 description sw-spine-001:1==\u0026gt;sw-leaf-002:53 lag 101 interface 1/1/2 no shutdown mtu 9198 description sw-spine-001:2==\u0026gt;sw-leaf-001:53 lag 101 interface 1/1/3 no shutdown mtu 9198 description sw-spine-001:3==\u0026gt;sw-leaf-004:53 lag 103 interface 1/1/4 no shutdown mtu 9198 description sw-spine-001:4==\u0026gt;sw-leaf-003:53 lag 103 interface 1/1/15 no shutdown mtu 9198 description sw-spine-001:15==\u0026gt;sw-cdu-001:49 lag 201 interface 1/1/16 no shutdown mtu 9198 description sw-spine-001:16==\u0026gt;sw-cdu-002:49 lag 201 interface 1/1/18 description UNUSED interface 1/1/28 no shutdown vrf attach Customer ip mtu 9198 ip address 192.168.80.5/31 ip ospf 2 area 0.0.0.0 ip ospf network point-to-point interface 1/1/29 no shutdown vrf attach Customer ip mtu 9198 ip address 192.168.80.1/31 ip ospf 2 area 0.0.0.0 ip ospf network point-to-point interface 1/1/30 no shutdown vrf attach keepalive description VSX keepalive ip address 192.168.255.0/31 interface 1/1/31 no shutdown mtu 9198 description vsx isl lag 256 interface 1/1/32 no shutdown mtu 9198 description vsx isl lag 256 interface loopback 0 ip address 10.2.0.2/32 ip ospf 1 area 0.0.0.0 interface vlan 1 description MGMT ip mtu 9198 ip address 10.1.0.2/16 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 10.1.0.1 ip helper-address 10.92.100.222 interface vlan 2 description NMN ip mtu 9198 ip address 10.252.0.2/17 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 10.252.0.1 ip helper-address 10.92.100.222 ip ospf 1 area 0.0.0.0 interface vlan 4 description HMN ip mtu 9198 ip address 10.254.0.2/17 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 10.254.0.1 ip helper-address 10.94.100.222 ip ospf 1 area 0.0.0.0 interface vlan 6 vrf attach Customer description CAN ip mtu 9198 ip address 10.103.0.130/26 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 10.103.0.129 interface vlan 7 vrf attach Customer description CMN ip mtu 9198 ip address 10.103.0.2/25 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 10.103.0.1 ip ospf 2 area 0.0.0.0 vsx system-mac 02:00:00:00:6b:00 inter-switch-link lag 256 role primary keepalive peer 192.168.255.1 source 192.168.255.0 vrf keepalive linkup-delay-timer 600 vsx-sync vsx-global ip dns server-address 10.92.100.225 ip prefix-list pl-cmn seq 10 permit 10.103.0.0/25 ge 25 ip prefix-list pl-hmn seq 20 permit 10.94.100.0/24 ge 24 ip prefix-list pl-nmn seq 30 permit 10.92.100.0/24 ge 24 ip prefix-list tftp seq 10 permit 10.92.100.60/32 ge 32 le 32 ip prefix-list tftp seq 20 permit 10.94.100.60/32 ge 32 le 32 ! ! ! ! route-map CMN permit seq 10 match interface vlan7 route-map ncn-w001 permit seq 10 match ip address prefix-list tftp match ip next-hop 10.252.1.8 set local-preference 1000 route-map ncn-w001 permit seq 20 match ip address prefix-list tftp match ip next-hop 10.252.1.9 set local-preference 1100 route-map ncn-w001 permit seq 30 match ip address prefix-list tftp match ip next-hop 10.252.1.10 set local-preference 1200 route-map ncn-w001 permit seq 40 match ip address prefix-list pl-cmn set ip next-hop 10.103.0.25 route-map ncn-w001 permit seq 50 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.16 route-map ncn-w001 permit seq 60 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.10 route-map ncn-w002 permit seq 10 match ip address prefix-list tftp match ip next-hop 10.252.1.8 set local-preference 1000 route-map ncn-w002 permit seq 20 match ip address prefix-list tftp match ip next-hop 10.252.1.9 set local-preference 1100 route-map ncn-w002 permit seq 30 match ip address prefix-list tftp match ip next-hop 10.252.1.10 set local-preference 1200 route-map ncn-w002 permit seq 40 match ip address prefix-list pl-cmn set ip next-hop 10.103.0.24 route-map ncn-w002 permit seq 50 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.14 route-map ncn-w002 permit seq 60 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.9 route-map ncn-w003 permit seq 10 match ip address prefix-list tftp match ip next-hop 10.252.1.8 set local-preference 1000 route-map ncn-w003 permit seq 20 match ip address prefix-list tftp match ip next-hop 10.252.1.9 set local-preference 1100 route-map ncn-w003 permit seq 30 match ip address prefix-list tftp match ip next-hop 10.252.1.10 set local-preference 1200 route-map ncn-w003 permit seq 40 match ip address prefix-list pl-cmn set ip next-hop 10.103.0.23 route-map ncn-w003 permit seq 50 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.12 route-map ncn-w003 permit seq 60 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.8 route-map ncn-w004 permit seq 10 match ip address prefix-list tftp match ip next-hop 10.252.1.8 set local-preference 1000 route-map ncn-w004 permit seq 20 match ip address prefix-list tftp match ip next-hop 10.252.1.9 set local-preference 1100 route-map ncn-w004 permit seq 30 match ip address prefix-list tftp match ip next-hop 10.252.1.10 set local-preference 1200 route-map ncn-w004 permit seq 40 match ip address prefix-list pl-cmn set ip next-hop 10.103.0.22 route-map ncn-w004 permit seq 50 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.10 route-map ncn-w004 permit seq 60 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.7 ! router ospf 2 vrf Customer router-id 10.2.0.2 redistribute connected route-map CMN area 0.0.0.0 router ospf 1 router-id 10.2.0.2 redistribute bgp area 0.0.0.0 router ospfv3 1 router-id 10.2.0.2 area 0.0.0.0 router bgp 65533 bgp router-id 10.2.0.2 maximum-paths 8 timers bgp 1 3 distance bgp 20 70 neighbor 10.252.0.3 remote-as 65533 neighbor 10.252.1.7 remote-as 65533 neighbor 10.252.1.7 passive neighbor 10.252.1.8 remote-as 65533 neighbor 10.252.1.8 passive neighbor 10.252.1.9 remote-as 65533 neighbor 10.252.1.9 passive neighbor 10.252.1.10 remote-as 65533 neighbor 10.252.1.10 passive address-family ipv4 unicast neighbor 10.252.0.3 activate neighbor 10.252.1.7 activate neighbor 10.252.1.7 route-map ncn-w004 in neighbor 10.252.1.8 activate neighbor 10.252.1.8 route-map ncn-w003 in neighbor 10.252.1.9 activate neighbor 10.252.1.9 route-map ncn-w002 in neighbor 10.252.1.10 activate neighbor 10.252.1.10 route-map ncn-w001 in exit-address-family ! vrf Customer bgp router-id 10.2.0.2 maximum-paths 8 timers bgp 1 3 distance bgp 20 70 neighbor 10.103.0.3 remote-as 65533 neighbor 10.103.0.22 remote-as 65536 neighbor 10.103.0.22 passive neighbor 10.103.0.23 remote-as 65536 neighbor 10.103.0.23 passive neighbor 10.103.0.24 remote-as 65536 neighbor 10.103.0.24 passive neighbor 10.103.0.25 remote-as 65536 neighbor 10.103.0.25 passive address-family ipv4 unicast neighbor 10.103.0.3 activate neighbor 10.103.0.22 activate neighbor 10.103.0.23 activate neighbor 10.103.0.24 activate neighbor 10.103.0.25 activate exit-address-family ! https-server vrf Customer https-server vrf default https-server vrf mgmt sw-spine-002 Current configuration: ! !Version ArubaOS-CX GL.10.09.0010 !export-password: default hostname sw-spine-002 no ip icmp redirect profile leaf vrf Customer vrf keepalive ntp server 10.252.1.10 ntp server 10.252.1.8 ntp server 10.252.1.9 ntp enable ! ! ! ! ! ssh server vrf Customer ssh server vrf default ssh server vrf keepalive ssh server vrf mgmt access-list ip cmn-can 10 deny any 10.103.0.0/255.255.255.128 10.103.0.128/255.255.255.192 20 deny any 10.103.0.128/255.255.255.192 10.103.0.0/255.255.255.128 30 permit any any any access-list ip mgmt 10 comment ALLOW SSH, HTTPS, AND SNMP ON HMN SUBNET and CMN 20 permit tcp 10.254.0.0/255.255.128.0 any eq ssh 30 permit tcp 10.254.0.0/255.255.128.0 any eq https 40 permit udp 10.254.0.0/255.255.128.0 any eq snmp 50 permit udp 10.254.0.0/255.255.128.0 any eq snmp-trap 60 permit tcp 10.103.0.0/255.255.255.128 any eq ssh 70 permit tcp 10.103.0.0/255.255.255.128 any eq https 80 permit udp 10.103.0.0/255.255.255.128 any eq snmp 90 permit udp 10.103.0.0/255.255.255.128 any eq snmp-trap 100 comment ALLOW SNMP FROM HMN METALLB SUBNET 110 permit udp 10.94.100.0/255.255.255.0 any eq snmp 120 permit udp 10.94.100.0/255.255.255.0 any eq snmp-trap 130 comment BLOCK SSH, HTTPS, AND SNMP FROM EVERYWHERE ELSE 140 deny tcp any any eq ssh 150 deny tcp any any eq https 160 deny udp any any eq snmp 170 deny udp any any eq snmp-trap 180 comment ALLOW ANYTHING ELSE 190 permit any any any access-list ip nmn-hmn 10 deny any 10.252.0.0/255.255.128.0 10.254.0.0/255.255.128.0 20 deny any 10.254.0.0/255.255.128.0 10.252.0.0/255.255.128.0 30 deny any 10.252.0.0/255.255.128.0 10.104.0.0/255.255.128.0 40 deny any 10.254.0.0/255.255.128.0 10.100.0.0/255.255.128.0 50 deny any 10.100.0.0/255.255.128.0 10.254.0.0/255.255.128.0 60 deny any 10.100.0.0/255.255.128.0 10.104.0.0/255.255.128.0 70 deny any 10.104.0.0/255.255.128.0 10.252.0.0/255.255.128.0 80 deny any 10.104.0.0/255.255.128.0 10.100.0.0/255.255.128.0 90 deny any 10.92.100.0/255.255.255.0 10.254.0.0/255.255.128.0 100 deny any 10.94.100.0/255.255.255.0 10.252.0.0/255.255.128.0 110 deny any 10.254.0.0/255.255.128.0 10.92.100.0/255.255.255.0 120 deny any 10.252.0.0/255.255.128.0 10.94.100.0/255.255.255.0 130 permit any any any apply access-list ip mgmt control-plane vrf default vlan 1 vlan 2 name NMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 4 name HMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 6 name CAN apply access-list ip cmn-can in apply access-list ip cmn-can out vlan 7 name CMN apply access-list ip cmn-can in apply access-list ip cmn-can out vlan 10 name SUN spanning-tree spanning-tree priority 0 spanning-tree config-name MST0 spanning-tree config-revision 1 interface mgmt shutdown ip dhcp interface lag 101 multi-chassis no shutdown description spine_to_leaf_lag no routing vlan trunk native 1 vlan trunk allowed 1-2,4,6-7 lacp mode active spanning-tree root-guard interface lag 103 multi-chassis no shutdown description spine_to_leaf_lag no routing vlan trunk native 1 vlan trunk allowed 1-2,4,6-7 lacp mode active spanning-tree root-guard interface lag 201 multi-chassis no shutdown description sw-spine-002:15==\u0026gt;sw-cdu-001:50 no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7 lacp mode active spanning-tree root-guard interface lag 256 no shutdown description ISL link no routing vlan trunk native 1 tag vlan trunk allowed all lacp mode active interface 1/1/1 no shutdown mtu 9198 description sw-spine-002:1==\u0026gt;sw-leaf-002:54 lag 101 interface 1/1/2 no shutdown mtu 9198 description sw-spine-002:2==\u0026gt;sw-leaf-001:54 lag 101 interface 1/1/3 no shutdown mtu 9198 description sw-spine-002:3==\u0026gt;sw-leaf-004:54 lag 103 interface 1/1/4 no shutdown mtu 9198 description sw-spine-002:4==\u0026gt;sw-leaf-003:54 lag 103 interface 1/1/15 no shutdown mtu 9198 description sw-spine-002:15==\u0026gt;sw-cdu-001:50 lag 201 interface 1/1/16 no shutdown mtu 9198 description sw-spine-002:16==\u0026gt;sw-cdu-002:50 lag 201 interface 1/1/28 no shutdown mtu 9198 vrf attach Customer ip mtu 9198 ip address 192.168.80.7/31 ip ospf 2 area 0.0.0.0 ip ospf network point-to-point interface 1/1/29 no shutdown mtu 9198 vrf attach Customer ip mtu 9198 ip address 192.168.80.3/31 ip ospf 2 area 0.0.0.0 ip ospf network point-to-point interface 1/1/30 no shutdown vrf attach keepalive description VSX keepalive ip address 192.168.255.1/31 interface 1/1/31 no shutdown mtu 9198 description vsx isl lag 256 interface 1/1/32 no shutdown mtu 9198 description vsx isl lag 256 interface loopback 0 ip address 10.2.0.3/32 ip ospf 1 area 0.0.0.0 interface vlan 1 description MGMT ip mtu 9198 ip address 10.1.0.3/16 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 10.1.0.1 ip helper-address 10.92.100.222 interface vlan 2 description NMN ip mtu 9198 ip address 10.252.0.3/17 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 10.252.0.1 ip helper-address 10.92.100.222 ip ospf 1 area 0.0.0.0 interface vlan 4 description HMN ip mtu 9198 ip address 10.254.0.3/17 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 10.254.0.1 ip helper-address 10.94.100.222 ip ospf 1 area 0.0.0.0 interface vlan 6 vrf attach Customer description CAN ip mtu 9198 ip address 10.103.0.131/26 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 10.103.0.129 interface vlan 7 vrf attach Customer description CMN ip mtu 9198 ip address 10.103.0.3/25 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 10.103.0.1 ip ospf 2 area 0.0.0.0 vsx system-mac 02:00:00:00:6b:00 inter-switch-link lag 256 role secondary keepalive peer 192.168.255.0 source 192.168.255.1 vrf keepalive linkup-delay-timer 600 vsx-sync vsx-global ip dns server-address 10.92.100.225 ip prefix-list pl-cmn seq 10 permit 10.103.0.0/25 ge 25 ip prefix-list pl-hmn seq 20 permit 10.94.100.0/24 ge 24 ip prefix-list pl-nmn seq 30 permit 10.92.100.0/24 ge 24 ip prefix-list tftp seq 10 permit 10.92.100.60/32 ge 32 le 32 ip prefix-list tftp seq 20 permit 10.94.100.60/32 ge 32 le 32 ! ! ! ! route-map CMN permit seq 10 match interface vlan7 route-map ncn-w001 permit seq 10 match ip address prefix-list tftp match ip next-hop 10.252.1.8 set local-preference 1000 route-map ncn-w001 permit seq 20 match ip address prefix-list tftp match ip next-hop 10.252.1.9 set local-preference 1100 route-map ncn-w001 permit seq 30 match ip address prefix-list tftp match ip next-hop 10.252.1.10 set local-preference 1200 route-map ncn-w001 permit seq 40 match ip address prefix-list pl-cmn set ip next-hop 10.103.0.25 route-map ncn-w001 permit seq 50 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.16 route-map ncn-w001 permit seq 60 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.10 route-map ncn-w002 permit seq 10 match ip address prefix-list tftp match ip next-hop 10.252.1.8 set local-preference 1000 route-map ncn-w002 permit seq 20 match ip address prefix-list tftp match ip next-hop 10.252.1.9 set local-preference 1100 route-map ncn-w002 permit seq 30 match ip address prefix-list tftp match ip next-hop 10.252.1.10 set local-preference 1200 route-map ncn-w002 permit seq 40 match ip address prefix-list pl-cmn set ip next-hop 10.103.0.24 route-map ncn-w002 permit seq 50 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.14 route-map ncn-w002 permit seq 60 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.9 route-map ncn-w003 permit seq 10 match ip address prefix-list tftp match ip next-hop 10.252.1.8 set local-preference 1000 route-map ncn-w003 permit seq 20 match ip address prefix-list tftp match ip next-hop 10.252.1.9 set local-preference 1100 route-map ncn-w003 permit seq 30 match ip address prefix-list tftp match ip next-hop 10.252.1.10 set local-preference 1200 route-map ncn-w003 permit seq 40 match ip address prefix-list pl-cmn set ip next-hop 10.103.0.23 route-map ncn-w003 permit seq 50 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.12 route-map ncn-w003 permit seq 60 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.8 route-map ncn-w004 permit seq 10 match ip address prefix-list tftp match ip next-hop 10.252.1.8 set local-preference 1000 route-map ncn-w004 permit seq 20 match ip address prefix-list tftp match ip next-hop 10.252.1.9 set local-preference 1100 route-map ncn-w004 permit seq 30 match ip address prefix-list tftp match ip next-hop 10.252.1.10 set local-preference 1200 route-map ncn-w004 permit seq 40 match ip address prefix-list pl-cmn set ip next-hop 10.103.0.22 route-map ncn-w004 permit seq 50 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.10 route-map ncn-w004 permit seq 60 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.7 ! router ospf 2 vrf Customer router-id 10.2.0.3 redistribute connected route-map CMN area 0.0.0.0 router ospf 1 router-id 10.2.0.3 redistribute bgp area 0.0.0.0 router ospfv3 1 router-id 10.2.0.3 area 0.0.0.0 router bgp 65533 bgp router-id 10.2.0.3 maximum-paths 8 timers bgp 1 3 distance bgp 20 70 neighbor 10.252.0.2 remote-as 65533 neighbor 10.252.1.7 remote-as 65533 neighbor 10.252.1.7 passive neighbor 10.252.1.8 remote-as 65533 neighbor 10.252.1.8 passive neighbor 10.252.1.9 remote-as 65533 neighbor 10.252.1.9 passive neighbor 10.252.1.10 remote-as 65533 neighbor 10.252.1.10 passive address-family ipv4 unicast neighbor 10.252.0.2 activate neighbor 10.252.1.7 activate neighbor 10.252.1.7 route-map ncn-w004 in neighbor 10.252.1.8 activate neighbor 10.252.1.8 route-map ncn-w003 in neighbor 10.252.1.9 activate neighbor 10.252.1.9 route-map ncn-w002 in neighbor 10.252.1.10 activate neighbor 10.252.1.10 route-map ncn-w001 in exit-address-family ! vrf Customer bgp router-id 10.2.0.3 maximum-paths 8 timers bgp 1 3 distance bgp 20 70 neighbor 10.103.0.2 remote-as 65533 neighbor 10.103.0.22 remote-as 65536 neighbor 10.103.0.22 passive neighbor 10.103.0.23 remote-as 65536 neighbor 10.103.0.23 passive neighbor 10.103.0.24 remote-as 65536 neighbor 10.103.0.24 passive neighbor 10.103.0.25 remote-as 65536 neighbor 10.103.0.25 passive address-family ipv4 unicast neighbor 10.103.0.2 activate neighbor 10.103.0.22 activate neighbor 10.103.0.23 activate neighbor 10.103.0.24 activate neighbor 10.103.0.25 activate exit-address-family ! https-server vrf Customer https-server vrf default https-server vrf mgmt Below is the desired state of a system running this configuration two OSPF neighbors on the Customer VRF. two default routes. sw-spine-001# show ip ospf neighbors vrf Customer VRF : Customer Process : 2 =================================================== Total Number of Neighbors : 11 Neighbor ID Priority State Nbr Address Interface ------------------------------------------------------------------------- 10.103.15.238 n/a FULL 192.168.80.4 1/1/28 10.103.15.234 n/a FULL 192.168.80.0 1/1/29 sw-spine-001# show ip route vrf Customer Displaying ipv4 routes selected for forwarding Origin Codes: C - connected, S - static, L - local R - RIP, B - BGP, O - OSPF Type Codes: E - External BGP, I - Internal BGP, V - VPN, EV - EVPN IA - OSPF internal area, E1 - OSPF external type 1 E2 - OSPF external type 2 VRF: Customer Prefix Nexthop Interface VRF(egress) Origin/ Distance/ Age Type Metric -------------------------------------------------------------------------------------------------------- 0.0.0.0/0 192.168.80.0 1/1/29 - O/E2 [110/1] 00m:01w:01d 192.168.80.4 1/1/28 - [110/1] 00m:01w:01d sw-spine-002# show ip ospf neighbors vrf Customer VRF : Customer Process : 2 =================================================== Total Number of Neighbors : 11 Neighbor ID Priority State Nbr Address Interface ------------------------------------------------------------------------- 10.103.15.238 n/a FULL 192.168.80.6 1/1/28 10.103.15.234 n/a FULL 192.168.80.2 1/1/29 sw-spine-002# show ip route vrf Customer Displaying ipv4 routes selected for forwarding Origin Codes: C - connected, S - static, L - local R - RIP, B - BGP, O - OSPF Type Codes: E - External BGP, I - Internal BGP, V - VPN, EV - EVPN IA - OSPF internal area, E1 - OSPF external type 1 E2 - OSPF external type 2 VRF: Customer Prefix Nexthop Interface VRF(egress) Origin/ Distance/ Age Type Metric -------------------------------------------------------------------------------------------------------- 0.0.0.0/0 192.168.80.6 1/1/28 - O/E2 [110/1] 00m:01w:01d 192.168.80.2 1/1/29 - [110/1] 00m:01w:01d "
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/cert_renewal_for_kubernetes_and_bare_metal_etcd/",
	"title": "Kubernetes and Bare Metal EtcD Certificate Renewal",
	"tags": [],
	"description": "",
	"content": "Kubernetes and Bare Metal EtcD Certificate Renewal As part of the installation, Kubernetes generates certificates for the required subcomponents. This document will help walk through the process of renewing the certificates.\nIMPORTANT:\nDepending on the version of Kubernetes, the command may or may not reside under the alpha category. Use kubectl certs --help and kubectl alpha certs --help to determine this. The overall command syntax is the same; the only difference is whether or not the command structure includes alpha. The node referenced in this document as ncn-m is the master node selected to renew the certificates on. This document is based off a base hardware configuration of three master nodes and three worker nodes. Utility storage nodes are not mentioned because they are not running Kubernetes. Make sure to update any commands that run on multiple nodes accordingly. If updating the Kubernetes certificates, then the Spire Intermediate CA is most likely expired or near its expiration date. Follow the instructions found in Update Spire Intermediate CA Certificate. Procedures for Certificate Renewal:\nFile Locations Check Certificates Backup Existing Certificates Renew All Certificates Renew Etcd Certificate Update Client Secrets File Locations IMPORTANT: Master nodes will have certificates for both Kubernetes services and the Kubernetes client. Workers will only have the certificates for the Kubernetes client.\nServices (master nodes):\n/etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver-etcd-client.crt /etc/kubernetes/pki/apiserver-etcd-client.key /etc/kubernetes/pki/apiserver.key /etc/kubernetes/pki/apiserver-kubelet-client.crt /etc/kubernetes/pki/apiserver-kubelet-client.key /etc/kubernetes/pki/ca.crt /etc/kubernetes/pki/ca.key /etc/kubernetes/pki/front-proxy-ca.crt /etc/kubernetes/pki/front-proxy-ca.key /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/pki/front-proxy-client.key /etc/kubernetes/pki/sa.key /etc/kubernetes/pki/sa.pub /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/etcd/ca.key /etc/kubernetes/pki/etcd/healthcheck-client.crt /etc/kubernetes/pki/etcd/healthcheck-client.key /etc/kubernetes/pki/etcd/peer.crt /etc/kubernetes/pki/etcd/peer.key /etc/kubernetes/pki/etcd/server.crt /etc/kubernetes/pki/etcd/server.key Client (master and worker nodes):\n/var/lib/kubelet/pki/kubelet-client-2021-09-07-17-06-36.pem /var/lib/kubelet/pki/kubelet-client-current.pem /var/lib/kubelet/pki/kubelet.crt /var/lib/kubelet/pki/kubelet.key Check Certificates Log into a master node.\nCheck the expiration of the certificates.\nncn-m# kubeadm alpha certs check-expiration --config /etc/kubernetes/kubeadmcfg.yaml Example output:\nWARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Sep 24, 2021 15:21 UTC 14d no apiserver Sep 24, 2021 15:21 UTC 14d ca no apiserver-etcd-client Sep 24, 2021 15:20 UTC 14d ca no apiserver-kubelet-client Sep 24, 2021 15:21 UTC 14d ca no controller-manager.conf Sep 24, 2021 15:21 UTC 14d no etcd-healthcheck-client Sep 24, 2021 15:19 UTC 14d etcd-ca no etcd-peer Sep 24, 2021 15:19 UTC 14d etcd-ca no etcd-server Sep 24, 2021 15:19 UTC 14d etcd-ca no front-proxy-client Sep 24, 2021 15:21 UTC 14d front-proxy-ca no scheduler.conf Sep 24, 2021 15:21 UTC 14d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Sep 02, 2030 15:21 UTC 8y no etcd-ca Sep 02, 2030 15:19 UTC 8y no front-proxy-ca Sep 02, 2030 15:21 UTC 8y no Backup Existing Certificates Backup existing certificates on master nodes:\nncn-m# pdsh -w $(grep -oP \u0026#39;ncn-m\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) \\ tar cvf /root/cert_backup.tar /etc/kubernetes/pki/ /var/lib/kubelet/pki/ Example output:\nncn-m001: tar: Removing leading / from member names ncn-m001: /etc/kubernetes/pki/ ncn-m001: /etc/kubernetes/pki/front-proxy-client.key ncn-m001: tar: Removing leading / from hard link targets ncn-m001: /etc/kubernetes/pki/apiserver-etcd-client.key ncn-m001: /etc/kubernetes/pki/sa.key [...] Backup existing certificates on worker nodes:\nncn-m# pdsh -w $(grep -oP \u0026#39;ncn-w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) \\ tar cvf /root/cert_backup.tar /var/lib/kubelet/pki/ Example output:\nncn-w003: tar: Removing leading / from member names ncn-w003: /var/lib/kubelet/pki/ ncn-w003: /var/lib/kubelet/pki/kubelet.key ncn-w003: /var/lib/kubelet/pki/kubelet-client-2021-09-07-17-06-36.pem ncn-w003: /var/lib/kubelet/pki/kubelet.crt [...] Renew All Certificates Run the following steps on each master node.\nRenew the certificates.\nncn-m# kubeadm alpha certs renew all --config /etc/kubernetes/kubeadmcfg.yaml Example output:\nWARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed certificate for serving the Kubernetes API renewed certificate the apiserver uses to access etcd renewed certificate for the API server to connect to kubelet renewed certificate embedded in the kubeconfig file for the controller manager to use renewed certificate for liveness probes to healthcheck etcd renewed certificate for etcd nodes to communicate with each other renewed certificate for serving etcd renewed certificate for the front proxy client renewed certificate embedded in the kubeconfig file for the scheduler manager to use renewed Check the new expiration.\nncn-m# kubeadm alpha certs check-expiration --config /etc/kubernetes/kubeadmcfg.yaml Example output:\nWARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Sep 22, 2022 17:13 UTC 364d no apiserver Sep 22, 2022 17:13 UTC 364d ca no apiserver-etcd-client Sep 22, 2022 17:13 UTC 364d etcd-ca no apiserver-kubelet-client Sep 22, 2022 17:13 UTC 364d ca no controller-manager.conf Sep 22, 2022 17:13 UTC 364d no etcd-healthcheck-client Sep 22, 2022 17:13 UTC 364d etcd-ca no etcd-peer Sep 22, 2022 17:13 UTC 364d etcd-ca no etcd-server Sep 22, 2022 17:13 UTC 364d etcd-ca no front-proxy-client Sep 22, 2022 17:13 UTC 364d front-proxy-ca no scheduler.conf Sep 22, 2022 17:13 UTC 364d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Sep 02, 2030 15:21 UTC 8y no etcd-ca Sep 02, 2030 15:19 UTC 8y no front-proxy-ca Sep 02, 2030 15:21 UTC 8y no Check to see if only some of the certificates were updated.\nncn-m# ncn-m001# ls -l /etc/kubernetes/pki Example output:\n-rw-r--r-- 1 root root 1249 Sep 22 17:13 apiserver.crt -rw-r--r-- 1 root root 1090 Sep 22 17:13 apiserver-etcd-client.crt -rw------- 1 root root 1675 Sep 22 17:13 apiserver-etcd-client.key -rw------- 1 root root 1679 Sep 22 17:13 apiserver.key -rw-r--r-- 1 root root 1099 Sep 22 17:13 apiserver-kubelet-client.crt -rw------- 1 root root 1679 Sep 22 17:13 apiserver-kubelet-client.key -rw------- 1 root root 1025 Sep 21 20:50 ca.crt -rw------- 1 root root 1679 Sep 21 20:50 ca.key drwxr-xr-x 2 root root 162 Sep 21 20:50 etcd -rw------- 1 root root 1038 Sep 21 20:50 front-proxy-ca.crt -rw------- 1 root root 1679 Sep 21 20:50 front-proxy-ca.key -rw-r--r-- 1 root root 1058 Sep 22 17:13 front-proxy-client.crt -rw------- 1 root root 1675 Sep 22 17:13 front-proxy-client.key -rw------- 1 root root 1675 Sep 21 20:50 sa.key -rw------- 1 root root 451 Sep 21 20:50 sa.pub ncn-m# ls -l /etc/kubernetes/pki/etcd Example output:\n-rw-r--r-- 1 root root 1017 Sep 21 20:50 ca.crt -rw-r--r-- 1 root root 1675 Sep 21 20:50 ca.key -rw-r--r-- 1 root root 1094 Sep 22 17:13 healthcheck-client.crt -rw------- 1 root root 1679 Sep 22 17:13 healthcheck-client.key -rw-r--r-- 1 root root 1139 Sep 22 17:13 peer.crt -rw------- 1 root root 1679 Sep 22 17:13 peer.key -rw-r--r-- 1 root root 1139 Sep 22 17:13 server.crt -rw------- 1 root root 1675 Sep 22 17:13 server.key Not all the certificate files were updated in this example.\nIMPORTANT: Some certificates were not updated because they have a distant expiration time and did not need to be updated. This is expected.\nCertificates most likely to not be updated due to a distant expiration:\nCERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Sep 02, 2030 15:21 UTC 8y no etcd-ca Sep 02, 2030 15:19 UTC 8y no front-proxy-ca Sep 02, 2030 15:21 UTC 8y no This means we can ignore the fact that our ca.crt/key, front-proxy-ca.crt/key, and etcd ca.crt/key were not updated.\nCheck the expiration of the certificates files that do not have a current date and are of the .crt or .pem format. See File Locations for the list of files.\nThis task is for each master node and below example checks each certificate in File Locations.\nfor i in $(ls /etc/kubernetes/pki/*.crt;ls /etc/kubernetes/pki/etcd/*.crt;ls /var/lib/kubelet/pki/*.crt;ls /var/lib/kubelet/pki/*.pem);do echo ${i}; openssl x509 -enddate -noout -in ${i};done Example output:\n/etc/kubernetes/pki/apiserver.crt notAfter=Sep 22 17:13:28 2022 GMT /etc/kubernetes/pki/apiserver-etcd-client.crt notAfter=Sep 22 17:13:28 2022 GMT /etc/kubernetes/pki/apiserver-kubelet-client.crt notAfter=Sep 22 17:13:28 2022 GMT /etc/kubernetes/pki/ca.crt notAfter=Sep 4 09:31:10 2031 GMT /etc/kubernetes/pki/front-proxy-ca.crt notAfter=Sep 4 09:31:11 2031 GMT /etc/kubernetes/pki/front-proxy-client.crt notAfter=Sep 22 17:13:29 2022 GMT /etc/kubernetes/pki/etcd/ca.crt notAfter=Sep 4 09:30:28 2031 GMT /etc/kubernetes/pki/etcd/healthcheck-client.crt notAfter=Sep 22 17:13:29 2022 GMT /etc/kubernetes/pki/etcd/peer.crt notAfter=Sep 22 17:13:29 2022 GMT /etc/kubernetes/pki/etcd/server.crt notAfter=Sep 22 17:13:29 2022 GMT /var/lib/kubelet/pki/kubelet.crt notAfter=Sep 21 19:50:16 2022 GMT /var/lib/kubelet/pki/kubelet-client-2021-09-07-17-06-36.pem notAfter=Sep 4 17:01:38 2022 GMT /var/lib/kubelet/pki/kubelet-client-current.pem notAfter=Sep 4 17:01:38 2022 GMT IMPORTANT: Do NOT forget to verify certificates in /etc/kubernetes/pki/etcd.\nAs noted in the above output, all certificates including those for Etcd were updated. Note that apiserver-etcd-client.crt is critical as it is the cert that allows the Kubernetes API server to talk to the bare-metal etcd cluster. Also, the /var/lib/kubelet/pki/ certificates will be updated in the Kubernetes client section that follows. Restart etcd.\nOnce the steps to renew the needed certificates have been completed on all the master nodes, log into each master node one at a time and run the following:\nncn-m# systemctl restart etcd.service Run the remaining steps on both master and worker nodes.\nRestart kubelet.\nRun the following command on each Kubernetes node.\nncn-m# pdsh -w $(grep -oP \u0026#39;ncn-m\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) \\ -w $(grep -oP \u0026#39;ncn-w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) \\ systemctl restart kubelet.service Fix kubectl command access.\nNOTE: The following command will only respond with Unauthorized if certificates have expired. In any case, the new client certificates will need to be distributed in the following steps.\nView the status of the nodes.\nncn-m# kubectl get nodes The following is returned if certificates have expired:\nerror: You must be logged in to the server (Unauthorized) Copy /etc/kubernetes/admin.conf to /root/.kube/config.\nncn-m# cp /etc/kubernetes/admin.conf /root/.kube/config Check the status of the nodes again.\nncn-m# kubectl get nodes Example output:\nNAME STATUS ROLES AGE VERSION ncn-m001 Ready master 370d v1.18.6 ncn-m002 Ready master 370d v1.18.6 ncn-m003 Ready master 370d v1.18.6 ncn-w001 Ready \u0026lt;none\u0026gt; 370d v1.18.6 ncn-w002 Ready \u0026lt;none\u0026gt; 370d v1.18.6 ncn-w003 Ready \u0026lt;none\u0026gt; 370d v1.18.6 Distribute the client certificate to the rest of the cluster.\nNOTE: There may be errors when copying files. The target may or may not exist depending on the version of CSM.\nDO NOT copy this to the master node where this work is being performed. Copy /etc/kubernetes/admin.conf to all master and worker nodes. Client access:\nNOTE: Update the following command with the appropriate range of worker nodes.\nncn-m# pdcp -w ncn-m00[2-3] -w ncn-w00[1-3] /etc/kubernetes/admin.conf /etc/kubernetes/ Regenerate kubelet .pem Certificates Backup certificates for kubelet on each master and worker node:\nncn-m# pdsh -w $(grep -oP \u0026#39;ncn-m\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) \\ -w $(grep -oP \u0026#39;ncn-w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) tar cvf \\ /root/kubelet_certs.tar /etc/kubernetes/kubelet.conf /var/lib/kubelet/pki/ Log into the master node that has the kubeadm configuration file to generate new kubelet.conf files.\nFind the master node with the /etc/cray/kubernetes/kubeadmin.yaml file.\nncn# MASTERNODE=$(for node in ncn-m00{1..3}; do ssh root@$node test -f /etc/cray/kubernetes/kubeadm.yaml \u0026amp;\u0026amp; echo $node \u0026amp;\u0026amp; break; done) ncn# echo $MASTERNODE Example output:\nncn-m002 Log into the master\nncn# ssh $MASTERNODE Generate a new kubelet.conf file in the /root/ directory.\nncn-m# for node in $(kubectl get nodes -o json|jq -r \u0026#39;.items[].metadata.name\u0026#39;); do kubeadm alpha kubeconfig user --org system:nodes \\ --client-name system:node:$node --config /etc/cray/kubernetes/kubeadm.yaml | sed \u0026#34;/WARNING/d\u0026#34; \u0026gt; /root/$node.kubelet.conf; done There should be a new kubelet.conf file per node running Kubernetes.\nCopy each file to the corresponding node shown in the filename.\nNOTE: Update the below command with the appropriate number of master and worker nodes.\nncn-m# for node in ncn-m00{1..3} ncn-w00{1..3}; do scp /root/$node.kubelet.conf $node:/etc/kubernetes/; done Log into each node one at a time and run the following commands:\nncn# systemctl stop kubelet.service \u0026amp;\u0026amp; rm -v /etc/kubernetes/kubelet.conf /var/lib/kubelet/pki/* \u0026amp;\u0026amp; cp -v /etc/kubernetes/$(hostname -s).kubelet.conf /etc/kubernetes/kubelet.conf \u0026amp;\u0026amp; systemctl start kubelet.service \u0026amp;\u0026amp; kubeadm init phase kubelet-finalize all --cert-dir /var/lib/kubelet/pki/ \u0026amp;\u0026amp; echo OK Check the expiration of the kubectl certificate files. See File Locations for the list of files.\nThis task is for each master and worker node. The example checks each kubelet certificate in File Locations.\nncn# for i in $(ls /var/lib/kubelet/pki/*.crt;ls /var/lib/kubelet/pki/*.pem);do echo ${i}; openssl x509 -enddate -noout -in ${i};done Example output:\n/var/lib/kubelet/pki/kubelet.crt notAfter=Sep 22 17:37:30 2022 GMT /var/lib/kubelet/pki/kubelet-client-2021-09-22-18-37-30.pem notAfter=Sep 22 18:32:30 2022 GMT /var/lib/kubelet/pki/kubelet-client-current.pem notAfter=Sep 22 18:32:30 2022 GMT Perform a rolling reboot of master nodes.\nFollow the Reboot NCNs process.\nIMPORTANT: Verify pods are running on the master node that was rebooted before proceeding to the next node.\nPerform a rolling reboot of worker nodes.\nFollow the Reboot NCNs process.\nRenew Etcd Certificate If Check Certificates indicates that only the apiserver-etcd-client need to be renewed, then the following can be used to renew just that one certificate. The full Renew All Certificates procedure will also renew this certificate.\nRun the following steps on each master node.\nRenew the Etcd certificate.\nkubeadm alpha certs renew apiserver-etcd-client --config /etc/kubernetes/kubeadmcfg.yaml systemctl restart etcd.service systemctl restart kubelet.service Update Client Secrets The client secrets can be updated independently from the Kubernetes certs.\nRun the following steps from a master node.\nUpdate the client certificate for kube-etcdbackup.\nUpdate the kube-etcdbackup-etcd secret.\nncn-m# kubectl --namespace=kube-system create secret generic kube-etcdbackup-etcd \\ --from-file=/etc/kubernetes/pki/etcd/ca.crt \\ --from-file=tls.crt=/etc/kubernetes/pki/etcd/server.crt \\ --from-file=tls.key=/etc/kubernetes/pki/etcd/server.key \\ --save-config --dry-run=client -o yaml | kubectl apply -f - Check the certificate\u0026rsquo;s expiration date to verify that the certificate is not expired.\nncn-m# kubectl get secret -n kube-system kube-etcdbackup-etcd -o json | jq -r \u0026#39;.data.\u0026#34;tls.crt\u0026#34; | @base64d\u0026#39; | openssl x509 -noout -enddate Example output:\nnotAfter=May 4 22:37:16 2023 GMT Check that the next kube-etcdbackup cronjob Completed. This cronjob runs every 10 minutes.\nncn-m# kubectl get pod -l app.kubernetes.io/instance=cray-baremetal-etcd-backup -n kube-system Example output:\nNAME READY STATUS RESTARTS AGE kube-etcdbackup-1652201400-czh5p 0/1 Completed 0 107s Update the client certificate for etcd-client.\nUpdate the etcd-client-cert secret.\nncn-m# kubectl --namespace=sysmgmt-health create secret generic etcd-client-cert \\ --from-file=etcd-client=/etc/kubernetes/pki/apiserver-etcd-client.crt \\ --from-file=etcd-client-key=/etc/kubernetes/pki/apiserver-etcd-client.key \\ --from-file=etcd-ca=/etc/kubernetes/pki/etcd/ca.crt \\ --save-config --dry-run=client -o yaml | kubectl apply -f - Check the certificates\u0026rsquo; expiration dates to verify that none of the certificate are expired.\nCheck the etcd-ca expiration date.\nncn-m# kubectl get secret -n sysmgmt-health etcd-client-cert -o json | jq -r \u0026#39;.data.\u0026#34;etcd-ca\u0026#34; | @base64d\u0026#39; | openssl x509 -noout -enddate Example output:\nnotAfter=May 1 18:20:23 2032 GMT Check the etcd-client expiration date.\nncn-m# kubectl get secret -n sysmgmt-health etcd-client-cert -o json | jq -r \u0026#39;.data.\u0026#34;etcd-client\u0026#34; | @base64d\u0026#39; | openssl x509 -noout -enddate Example output:\nnotAfter=May 4 18:20:24 2023 GMT Restart Prometheus.\nncn-m# kubectl rollout restart -n sysmgmt-health statefulSet/prometheus-cray-sysmgmt-health-promet-prometheus ncn-m# kubectl rollout status -n sysmgmt-health statefulSet/prometheus-cray-sysmgmt-health-promet-prometheus Example output:\nWaiting for 1 pods to be ready... statefulset rolling update complete ... Check for any tls errors from the active Prometheus targets. No errors are expected.\nncn-m# PROM_IP=$(kubectl get services -n sysmgmt-health cray-sysmgmt-health-promet-prometheus -o json | jq -r \u0026#39;.spec.clusterIP\u0026#39;) ncn-m# curl -s http://${PROM_IP}:9090/api/v1/targets | jq -r \u0026#39;.data.activeTargets[] | select(.\u0026#34;scrapePool\u0026#34; == \u0026#34;sysmgmt-health/cray-sysmgmt-health-promet-kube-etcd/0\u0026#34;)\u0026#39; | grep lastError | sort -u Example output:\n\u0026#34;lastError\u0026#34;: \u0026#34;\u0026#34;, "
},
{
	"uri": "/docs-csm/en-12/operations/image_management/customize_an_image_root_using_ims/",
	"title": "Customize an Image Root Using IMS",
	"tags": [],
	"description": "",
	"content": "Customize an Image Root Using IMS The Image Management Service (IMS) customization workflow sets up a temporary image customization environment within a Kubernetes pod and mounts the image to be customized in that environment. A system administrator then makes the desired changes to the image root within the customization environment.\nAfterwards, the IMS customization workflow automatically copies the NCN CA public key to /etc/cray/ca/certificate_authority.crt within the image root being customized, in order to enable secure communications between NCNs and client nodes. IMS then compresses the customized image root and uploads it and its associated initrd image and kernel image (needed to boot a node) to the artifact repository.\nPrerequisites System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCN) and include the following deployments: cray-ims, the Image Management Service (IMS) cray-nexus, the Nexus repository manager service kubectl is installed locally and configured to point at the SMS Kubernetes cluster. An IMS registered image root archive or a pre-built image root SquashFS archive is available to customize. The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system. A token providing Simple Storage Service (S3) credentials has been generated. When customizing an image using IMS Image Customization, once inside the image root using chroot (if using a `jailed` environment), the image will only have access to whatever configuration the image already contains. In order to talk to services, including Nexus RPM repositories, the image root must first be configured with DNS and other settings. A base level of customization is provided by the default Ansible plays used by the Configuration Framework Service (CFS) to enable DNS resolution. Limitations The commands in this procedure must be run as the root user. Currently, the initrd image and kernel image are not regenerated automatically when the image root is changed. The admin must manually regenerate them while in the customization environment, if needed. Images in the .txz compressed format need to be converted to SquashFS in order to use IMS image customization. Procedure Check for an existing IMS public key.\nIf it is known that a public key associated with the user account being used was not previously uploaded to the IMS service, then skip this step and proceed to upload the SSH public key to the IMS service.\nThe following query may return multiple public key records. The correct one will have a name value including the current username in use.\nncn-mw# cray ims public-keys list --format toml Example output excerpt:\n[[results]] public_key = \u0026#34;ssh-rsa AAAAB3NzaC1yc2EA ... AsVruw1Zeiec2IWt\u0026#34; id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; name = \u0026#34;username public key\u0026#34; created = \u0026#34;2018-11-21T17:19:07.830000+00:00\u0026#34; If a public key associated with the username in use is not returned, then proceed to upload the SSH public key to the IMS service. Otherwise, if a public key associated with the username does exist, then note the value of its id field and proceed to record the IMS public key ID. Upload the SSH public key to the IMS service.\nIf an IMS public key record has already been created for the account being used, then note the value of its id field and proceed to record the IMS public key ID.\nThe IMS debug/configuration shell relies on passwordless SSH. This SSH public key needs to be uploaded to IMS to enable interaction with the image customization environment later in this procedure.\nReplace the username value with the actual username being used on the system when setting the public key name.\nncn-mw# cray ims public-keys create --name \u0026#34;username public key\u0026#34; --public-key ~/.ssh/id_rsa.pub --format toml Example output:\npublic_key = \u0026#34;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCl50gK4l9uupxC2KHxMpTNxPTJbnwEdWy1jst5W5LqJx9fdTrc9uNJ33HAq+WIOhPVGbLm2N4GX1WTUQ4+wVOSmmBBJnlu/l5rmO9lEGT6U8lKG8dA9c7qhguGHy7M7WBgdW/gWA16gwE/u8Qc2fycFERRKmFucL/Er9wA0/Qvz7/U59yO+HOtk5hvEz/AUkvaaoY0IVBfdNBCl59CIdZHxDzgXlXzd9PAlrXZNO8jDD3jyFAOvMMRG7py78zj2NUngvsWYoBcV3FcREZJU529uJ0Au8Vn9DRADyB4QQS2o+fa6hG9i2SzfY8L6vAVvSE7A2ILAsVruw1Zeiec2IWt\u0026#34; id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; name = \u0026#34;username public key\u0026#34; created = \u0026#34;2018-11-21T17:19:07.830000+00:00\u0026#34; Note the value of the id field and proceed to record the IMS public key ID.\nRecord the IMS public key ID.\nCreate a variable for the IMS public key id value noted previously.\nncn-mw# IMS_PUBLIC_KEY_ID=a252ff6f-c087-4093-a305-122b41824a3e Determine if the image root being used is in IMS and ready to be customized.\nIf the image to be customized is already known to IMS, then proceed to Locate an IMS Image to Customize. If the image to be customized was created outside of IMS and is not yet known to IMS, then proceed to Import External Image to IMS. To build an image from an existing IMS recipe, proceed to Build an Image Using IMS REST Service. Locate the IMS image record for the image that is being customized.\nncn-mw# cray ims images list --format toml Example output excerpt:\n[[results]] created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; [results.link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/4e78488d-4d92-4675-9d83-97adfc17cb19/sles_15_image.squashfs\u0026#34; etag = \u0026#34;\u0026#34; Record the IMS image ID.\nCreate a variable for the id field value for the image that is being customized.\nncn-mw# IMS_IMAGE_ID=4e78488d-4d92-4675-9d83-97adfc17cb19 Create an IMS job record in order to start the image customization job.\nAfter customizing the image, IMS will automatically upload any build artifacts (root file system, kernel, and initrd) to S3, and associate the S3 artifacts with IMS. Unfortunately, IMS is not able to dynamically determine the names of the Linux kernel and initrd to look for, because the file name for these vary depending upon Linux distribution, Linux version, dracut configuration, and more. Therefore the user must pass into IMS the name of the kernel and initrd in the resultant image root\u0026rsquo;s /boot directory.\nUse the following table to help determine the default kernel and initrd file names to specify when submitting the job to customize an image. These are just default names. Consult with the site administrator to determine if these names have been changed for a given image or recipe.\nRecipe Recipe Name Kernel File Name initrd File Name SLES 15 SP3 Barebones cray-sles15sp3-barebones vmlinuz initrd COS cray-shasta-compute-sles15sp3.x86_64-1.4.66 vmlinuz initrd Under normal circumstances, IMS customization jobs will download and mount the rootfs for the specified IMS image under the /mnt/image/image-root directory within the SSH shell. After SSHing into the job container, cd or chroot into the /mnt/image/image-root directory in order to interact with the image root being customized.\nOptionally, IMS can be told to create a jailed SSH environment by specifying the --ssh-containers-jail True parameter.\nA jailed environment lets users SSH into the SSH container and be immediately within the image root for the image being customized. Users do not need to cd or chroot into the image root. Using a jailed environment has some advantages, such as making the IMS SSH job shell look more like a compute node. This allows applications like the CFS to perform actions on both IMS job pods (pre-boot) and compute nodes (post-boot).\nBefore running the following command, replace the MY_CUSTOMIZED_IMAGE value with the name of the image root being used.\nncn-mw# cray ims jobs create \\ --job-type customize \\ --kernel-file-name vmlinuz \\ --initrd-file-name initrd \\ --artifact-id $IMS_IMAGE_ID \\ --public-key-id $IMS_PUBLIC_KEY_ID \\ --enable-debug False \\ --image-root-archive-name MY_CUSTOMIZED_IMAGE \\ --format toml Example output:\nstatus = \u0026#34;creating\u0026#34; enable_debug = false kernel_file_name = \u0026#34;vmlinuz\u0026#34; artifact_id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; build_env_size = 10 job_type = \u0026#34;customize\u0026#34; kubernetes_service = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34; kubernetes_job = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-customize\u0026#34; id = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34; image_root_archive_name = \u0026#34;MY_CUSTOMIZED_IMAGE\u0026#34; initrd_file_name = \u0026#34;initrd\u0026#34; created = \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34; kubernetes_namespace = \u0026#34;ims\u0026#34; public_key_id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; kubernetes_configmap = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34; [[ssh_containers]] status = \u0026#34;pending\u0026#34; jail = false name = \u0026#34;customize\u0026#34; [ssh_containers.connection_info.\u0026#34;cluster.local\u0026#34;] host = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service.ims.svc.cluster.local\u0026#34; port = 22 [ssh_containers.connection_info.customer_access] host = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7.ims.cmn.shasta.cray.com\u0026#34; port = 22 Create variables for the IMS job ID and Kubernetes job ID.\nThe values for these variables are taken from the output of the command in the previous step. Set IMS_JOB_ID to the value of the id field. Set the IMS_KUBERNETES_JOB to the value of the kubernetes_job field.\nncn-mw# IMS_JOB_ID=ad5163d2-398d-4e93-94f0-2f439f114fe7 ncn-mw# IMS_KUBERNETES_JOB=cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-customize Create variables for the SSH connection values in the returned data.\nThe IMS customization job enables customization of the image root via an SSH shell accessible by one or more dynamic host names. The user needs to know if they will SSH from inside or outside the Kubernetes cluster to determine which host name to use. Typically, customers access the system from outside the Kubernetes cluster using the Customer Access Network (CAN).\nBefore setting the SSH values, determine the appropriate method to SSH into the customization pod:\n[ssh_containers.connection_info.customer_access] values (preferred): The customer_access address is a dynamic hostname that is made available for use by the customer to access the IMS job from outside the Kubernetes cluster. [ssh_containers.connection_info.\u0026quot;cluster.local\u0026quot;] values: The cluster.local address is used when trying to access an IMS job from a pod that is running within the HPE Cray EX Kubernetes cluster. For example, this is the address that CFS uses to talk to the IMS job during a pre-boot customization session. The external IP address should only be used if the dynamic customer_access hostname does not resolve properly. In the following example, the administrator could then SSH to the 10.103.2.160 IP address.\nncn-mw# kubectl get services -n ims | grep $IMS_JOB_ID Example output:\ncray-ims-06c3dd57-f347-4229-85b3-1d024a947b3f-service LoadBalancer 10.29.129.204 10.103.2.160 22:31627/TCP 21h To create the variables:\nncn-mw# IMS_SSH_HOST=ad5163d2-398d-4e93-94f0-2f439f114fe7.ims.cmn.shasta.cray.com ncn-mw# IMS_SSH_PORT=22 Describe the image create job.\nncn-mw# kubectl -n ims describe job $IMS_KUBERNETES_JOB Example output:\nName: cray-ims-cfa864b3-4e08-49b1-9c57-04573228fd3f-customize Namespace: default [...] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 4m job-controller Created pod: cray-ims-cfa864b3-4e08-49b1-9c57-04573228fd3f-customize-xh2jf Record the name of the Kubernetes pod from the previous command.\nThe name is located in the Events section of the output.\nncn-mw# POD=cray-ims-cfa864b3-4e08-49b1-9c57-04573228fd3f-customize-xh2jf Verify that the status of the IMS job is waiting_on_user.\nncn-mw# cray ims jobs describe $IMS_JOB_ID --format toml Example output:\nstatus = \u0026#34;waiting_on_user\u0026#34; enable_debug = false kernel_file_name = \u0026#34;vmlinuz\u0026#34; artifact_id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; build_env_size = 10 job_type = \u0026#34;customize\u0026#34; kubernetes_service = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34; kubernetes_job = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-customize\u0026#34; id = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34; image_root_archive_name = \u0026#34;my_customized_image\u0026#34; initrd_file_name = \u0026#34;initrd\u0026#34; created = \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34; kubernetes_namespace = \u0026#34;ims\u0026#34; public_key_id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; kubernetes_configmap = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34; [[ssh_containers]] status = \u0026#34;pending\u0026#34; jail = false name = \u0026#34;customize\u0026#34; [ssh_containers.connection_info.\u0026#34;cluster.local\u0026#34;] host = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service.ims.svc.cluster.local\u0026#34; port = 22 [ssh_containers.connection_info.customer_access] host = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7.ims.cmn.shasta.cray.com\u0026#34; port = 22 Customize the image in the image customization environment.\nOnce in the image root using chroot (or if using a `jailed` environment) during image customization, the image will only have access to whatever configuration the image already contains. In order to talk to services, including Nexus RPM repositories, the image root must first be configured with DNS and other settings. A base level of customization is provided by the default Ansible plays used by the CFS to enable DNS resolution.\nOption 1: SSH to the image customization environment.\nIn order for passwordless SSH to work, ensure that the correct public/private key pair is used. The private key will need to match the public key that was uploaded to the IMS service and specified in the IMS job.\nIMPORTANT: The following command will work when run on any of the master nodes and worker nodes, except for ncn-w001.\nncn-mw# ssh -p $IMS_SSH_PORT root@$IMS_SSH_HOST Once connected to the IMS image customization shell, perform any customizations required. If the SSH shell was created without using the --ssh-containers-jail True parameter, cd or chroot into the image root. The image root is available under /mnt/image/image-root.\nAfter changes have been made, run the touch command on the complete file. The location of the complete file depends on whether or not the SSH job shell was created using the --ssh-containers-jail True parameter. See the table below for more information.\n--ssh-containers-jail Command used to create the complete file False (default) touch /mnt/image/complete True touch /tmp/complete [root@POD image]# cd /mnt/image/ [root@POD image]# chroot image-root/ :/ # (do touch complete flag) :/ # exit [root@POD image]# When the complete file has been created, the following actions will occur:\nThe job SSH container will close any active SSH connections. The buildenv-sidecar container will compresses the image root. The customized artifacts will be uploaded to S3 and associated with a new IMS image record. Option 2: Use Ansible to run playbooks against the image root.\nncn-mw# ansible all -i $IMS_SSH_HOST, -m ping --ssh-extra-args \\ \u0026#34; -p $IMS_SSH_PORT -i ./pod_rsa_key -o StrictHostKeyChecking=no\u0026#34; -u root Example output:\nad5163d2-398d-4e93-94f0-2f439f114fe7.ims.cmn.shasta.cray.com | SUCCESS =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } The Ansible inventory file below can also be used. The private key (./pod_rsa_key) corresponds to the public key file the container has in its authorized_keys file.\nmyimage-customize ansible_user=root ansible_host=ad5163d2-398d-4e93-94f0-2f439f114fe7.ims.cmn.shasta.cray.com ansible_port=22 \\ ansible_ssh_private_key_file=./pod_rsa_key ansible_ssh_common_args=\u0026#39;-o \\ StrictHostKeyChecking=no\u0026#39; A sample playbook can be run on the image root:\n--- # The playbook creates a new database test and populates data in the database to test the sharding. - hosts: all remote_user: root tasks: - name: Look at the image root command: \u0026#34;ls -l /mnt/image/image-root\u0026#34; - name: chroot and run dracut command: \u0026#34;chroot /mnt/image/image-root dracut --force --kver 4.4.143-94.47-default\u0026#34; - name: example copying file with owner and permissions copy: src: sample_playbook.yml dest: /mnt/image/image-root/tmp - name: Exit the build container copy: src: nothing_file dest: /mnt/image/complete The sample playbook can be run with the following command:\nncn-mw# ansible-playbook -i ./inventory.ini sample_playbook.yml Follow the buildenv-sidecar to ensure that any artifacts are properly uploaded to S3 and associated with IMS.\nncn-mw# kubectl -n ims logs -f $POD -c buildenv-sidecar Example output:\n+ python -m ims_python_helper image upload_artifacts sles15_barebones_image 7de80ccc-1e7d-43a9-a6e4-02cad10bb60b -v -r /mnt/image/sles15_barebones_image.sqsh -k /mnt/image/image-root/boot/vmlinuz -i /mnt/image/image-root/boot/initrd ... { \u0026#34;ims_image_artifacts\u0026#34;: [ { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;4add976679c7e955c4b16d7e2cfa114e-32\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/rootfs\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;94165af4373e5ace3e817eb4baba2284\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.rootfs.squashfs\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;f836412241aae79d160556ed6a4eb4d4\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/kernel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;f836412241aae79d160556ed6a4eb4d4\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.kernel\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;ec8793c07f94e59a2a30abdb1bd3d35a-4\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/initrd\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;86832ee3977ca0515592e5d00271d2fe\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.initrd\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;13af343f3e76b0f8c7fbef7ee3588ac1\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;13af343f3e76b0f8c7fbef7ee3588ac1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/json\u0026#34; } ], \u0026#34;ims_image_record\u0026#34;: { \u0026#34;created\u0026#34;: \u0026#34;2018-12-17T22:59:43.264129+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;d88521c3-b339-43bc-afda-afdfda126388\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;sles15_barebones_image\u0026#34; \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;13af343f3e76b0f8c7fbef7ee3588ac1\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, }, \u0026#34;ims_job_record\u0026#34;: { \u0026#34;artifact_id\u0026#34;: \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34;, \u0026#34;build_env_size\u0026#34;: 10, \u0026#34;created\u0026#34;: \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34;, \u0026#34;enable_debug\u0026#34;: false, \u0026#34;id\u0026#34;: \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34;, \u0026#34;image_root_archive_name\u0026#34;: \u0026#34;sles15_barebones_image\u0026#34;, \u0026#34;initrd_file_name\u0026#34;: \u0026#34;initrd\u0026#34;, \u0026#34;job_type\u0026#34;: \u0026#34;create\u0026#34;, \u0026#34;kernel_file_name\u0026#34;: \u0026#34;vmlinuz\u0026#34;, \u0026#34;kubernetes_configmap\u0026#34;: \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34;, \u0026#34;kubernetes_job\u0026#34;: \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-create\u0026#34;, \u0026#34;kubernetes_service\u0026#34;: \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34;, \u0026#34;public_key_id\u0026#34;: \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34;, \u0026#34;resultant_image_id\u0026#34;: \u0026#34;d88521c3-b339-43bc-afda-afdfda126388\u0026#34;, \u0026#34;ssh_port\u0026#34;: 0, \u0026#34;status\u0026#34;: \u0026#34;packaging_artifacts\u0026#34; }, \u0026#34;result\u0026#34;: \u0026#34;success\u0026#34; } The IMS customization workflow automatically copies the NCN Certificate Authority\u0026rsquo;s public certificate to /etc/cray/ca/certificate_authority.crt within the image root being customized. This can be used to enable secure communications between the NCN and the client node.\nLook up the ID of the newly created image.\nncn-mw# cray ims jobs describe $IMS_JOB_ID --format toml Example output:\nstatus = \u0026#34;success\u0026#34; enable_debug = false kernel_file_name = \u0026#34;vmlinuz\u0026#34; artifact_id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; build_env_size = 10 job_type = \u0026#34;customize\u0026#34; kubernetes_service = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34; kubernetes_job = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-customize\u0026#34; id = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34; image_root_archive_name = \u0026#34;my_customized_image\u0026#34; initrd_file_name = \u0026#34;initrd\u0026#34; resultant_image_id = \u0026#34;d88521c3-b339-43bc-afda-afdfda126388\u0026#34; created = \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34; kubernetes_namespace = \u0026#34;ims\u0026#34; public_key_id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; kubernetes_configmap = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34; Record the IMS image ID of the created image.\nSet IMS_RESULTANT_IMAGE_ID to the value of the resultant_image_id field in the output of the previous command.\nncn-mw# IMS_RESULTANT_IMAGE_ID=d88521c3-b339-43bc-afda-afdfda126388 Verify that the new IMS image record exists.\nncn-mw# cray ims images describe $IMS_RESULTANT_IMAGE_ID --format toml Example output:\ncreated = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;d88521c3-b339-43bc-afda-afdfda126388\u0026#34; name = \u0026#34;my_customized_image.squashfs\u0026#34; [link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/d88521c3-b339-43bc-afda-afdfda126388/my_customized_image.squashfs\u0026#34; etag = \u0026#34;28f3d78c8cceca2083d7d3090d96bbb7\u0026#34; Clean up the image customization environment.\nDelete the IMS job record.\nncn-mw# cray ims jobs delete $IMS_JOB_ID Deleting the job record also deletes the underlying Kubernetes job, service, and ConfigMap that were created when the job record was submitted.\nJobs left in a \u0026lsquo;Running\u0026rsquo; state continue to consume Kubernetes resources until the job is completed, or deleted. If there are enough \u0026lsquo;Running\u0026rsquo; IMS jobs on the system it may not be possible to schedule more pods on worker nodes due to insufficient resouces errors.\nThe image root has been modified, compressed, and uploaded to S3, along with its associated initrd and kernel files. The image customization environment has also been cleaned up.\n"
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/component_partition_members/",
	"title": "Component Partition Members",
	"tags": [],
	"description": "",
	"content": "Component Partition Members The members object in the partition definition has additional actions available for managing the members after the partition has been created.\nThe following is an example of partition members:\n{ \u0026#34;ids\u0026#34; : [ \u0026#34;x0c0s0b0n0\u0026#34;,\u0026#34;x0c0s0b0n1\u0026#34;,\u0026#34;x0c0s0b1n0\u0026#34;,\u0026#34;x0c0s0b1n1\u0026#34; ] } Retrieve Partition Members Retrieving members of a partition is very similar to how group members are retrieved and modified. No filtering options are available in partitions. However, there are partition and group filtering parameters for the /hsm/v2/State/Components and /hsm/v2/memberships collections, with both essentially working the same way.\nRetrieve only the members array for a single partition:\nncn-m# cray hsm partitions members list PARTITION_NAME Add a Component to Partition Components can be added to a partition\u0026rsquo;s member list, assuming it is not already a member or in another partition. This can be verified by looking at the membership information.\nAdd a component to a partition:\nncn-m# cray hsm partitions members create --id COMPONENT_ID PARTITION_NAME For example:\nncn-m# cray hsm partitions members create --id x1c0s0b0n0 partition1 Remove a Partition Member Remove a single component from a partition, assuming it is a current member. It will no longer be in any partition and is free to be assigned to a new one.\nncn-m# cray hsm partitions members delete MEMBER_ID PARTITION_NAME For example:\nncn-m# cray hsm partitions members delete x1c0s0b0n0 partition1 "
},
{
	"uri": "/docs-csm/en-12/operations/firmware/update_firmware_with_fas/",
	"title": "Update Firmware with FAS",
	"tags": [],
	"description": "",
	"content": "Update Firmware with FAS If FAS has not yet been installed, firmware for NCNs can be updated manually without FAS. See Updating Firmware without FAS.\nThe Firmware Action Service (FAS) provides an interface for managing firmware versions of Redfish-enabled hardware in the system. FAS interacts with the Hardware State Managers (HSM), device data, and image data in order to update firmware.\nReset Gigabyte node BMC to factory defaults if having problems with ipmitool, problems using Redfish, or when flashing procedures fail. See Set Gigabyte Node BMC to Factory Defaults.\nFAS images contain the following information that is needed for a hardware device to update firmware versions:\nHardware-specific information: Contains the allowed device states and how to reboot a device if necessary. Selection criteria: How to link a firmware image to a specific hardware type. Image data: Where the firmware image resides in Simple Storage Service (S3) and what firmwareVersion it will report after it is successfully applied. See Artifact Management for more information. Topics Prerequisites Warning Current capabilities Order of operations Hardware precedence order FAS administrative procedures Firmware actions Firmware operations Firmware images Prerequisites CSM software has been installed, firmware has been loaded into FAS as part of the HPC Firmware Pack (HFP) install, HSM is running, and nodes have been discovered. All management nodes have been locked. Identify the type and manufacturers of hardware in the system. If Gigabyte nodes are not in use on the system, do not update them! Warning Non-compute nodes (NCNs) and their BMCs should be locked with the HSM locking API to ensure they are not unintentionally updated by FAS. See Lock and Unlock Management Nodes for more information. Failure to lock the NCNs could result in unintentional update of the NCNs if FAS is not used correctly; this will lead to system instability problems.\nNOTE: Any node that is locked remains in the state inProgress with the stateHelper message of \u0026quot;failed to lock\u0026quot; until the action times out, or the lock is released. If the action is timed out, these nodes report as failed with the stateHelper message of \u0026quot;time expired; could not complete update\u0026quot;. This includes NCNs which are manually locked to prevent accidental rebooting and firmware updates.\nFollow the process outlined in FAS CLI to update the system. Use the recipes listed in FAS Recipes to update each supported type.\nNOTE Each system is different and may not have all hardware options.\nCurrent capabilities The following table describes the hardware items that can have their firmware updated via FAS. For more information about the upgradable targets, refer to the Firmware product stream repository.\nTable 1. Upgradable Firmware Items\nManufacturer Type Target Cray nodeBMC BMC, Node0.BIOS, Node1.BIOS, Recovery, Node1.AccFPGA0, Node0.AccFPGA0 Cray chassisBMC BMC, Recovery Cray routerBMC BMC, Recovery Gigabyte nodeBMC BMC, BIOS HPE nodeBMC iLO 5 (BMC or 1 ), System ROM ,Redundant System ROM (BIOS or 2) Order of operations For each item in the Hardware Precedence Order:\nComplete a dry-run:\ncray fas actions create {jsonfile} Note the ActionID. Poll the status of the action until the action state is completed: cray fas actions describe {actionID} --format json Interpret the outcome of the dry-run.\nLook at the counts and determine if the dry-run identified any hardware to update.\nFor the steps below, the following returned messages will help determine if a firmware update is needed. The following are end states for operations. The firmware action itself should be in completed once all operations have finished.\nNoOp: Nothing to do; already at the requested version. NoSol: No viable image is available; this will not be updated. succeeded: IF dryrun: The operation should succeed if performed as a live update. succeeded means that FAS identified that it COULD update a component name (xname) and target with the declared strategy. IF live update: The operation succeeded and has updated the component name (xname) and target to the identified version. failed: IF dryrun: There is something that FAS could do, but it likely would fail (most likely because the file is missing). IF live update: The operation failed. The identified version could not be put on the component name (xname) and target. If succeeded count is greater than zero, then perform a live update.\nUpdate the JSON file to set the overrideDryrun field to true. cray fas actions create {jsonfile} Note the ActionID! Poll the status of the action until the action state is completed: cray fas actions describe {actionID} --format json Interpret the outcome of the live update; proceed to next type of hardware.\nHardware precedence order After identifying which hardware is in the system, start with the top item on this list to update. If any of the following hardware is not in the system, then skip it.\nIMPORTANT: This process does not communicate the SAFE way to update NCNs. If the NCNs and their BMCs have not been locked, or if FAS is blindly used to update NCNs without following the correct process, then THE STABILITY OF THE SYSTEM WILL BE JEOPARDIZED. Read the corresponding recipes before updating. There are sometimes ancillary actions that must be completed in order to ensure update integrity. NOTE To update Switch Controllers (sC) or RouterBMC, refer to the Rosetta Documentation.\nCray ChassisBMC NodeBMC BMC NodeBIOS Redstone FPGA Gigabyte BMC BIOS HPE BMC (iLO5) BIOS (System ROM) FAS administrative procedures There are several use cases for using the FAS to update firmware on the system. These use cases are intended to be run by system administrators with a good understanding of firmware. Under no circumstances should non-administrator users attempt to use FAS or perform a firmware update.\nPerform a firmware update: Update the firmware of a component name (xname)\u0026rsquo;s target to the latest, earliest, or an explicit version. Determine what hardware can be updated by performing a dry-run: This is the easiest way to determine what can be updated. Take a snapshot of the system: Record the firmware versions present on each target for the identified component names (xnames). If the firmware version corresponds to an image available in the images repository, link the imageID to the record. Restore the snapshot of the system: Take the previously recorded snapshot and use the related imageIDs to put the component name (xname)/targets back to the firmware version they were at, at the time of the snapshot. Provide firmware for updating: FAS can only update a component name (xname)/target if it has an image record that is applicable. Most administrators will not encounter this use case. Firmware actions An action is collection of operations, which are individual firmware update tasks. Only one FAS action can be run at a time. Any other attempted action will be queued. Additionally, only one operation can be run on a component name (xname) at a time. For example, if there are 1000 xnames with 5 targets each to be updated, all 1000 xnames can be updating a target, but only 1 target on each xname will be updated at a time.\nThe life cycle of any action can be divided into the static and dynamic portions of the life cycle.\nThe static portion of the life cycle is where the action is created and configured. It begins with a request to create an action through either of the following requests:\nDirect: Request to /actions API. Indirect: Request to restore a snapshot via the /snapshots API. The dynamic portion of the life cycle is where the action is executed to completion. It begins when the actions is transitioned from the new to configured state. The action will then be ultimately transitioned to an end state of aborted or completed.\nFirmware operations Operations are individual tasks in a FAS action. FAS will create operations based on the configuration sent through the actions create command. FAS operations will have one of the following states:\ninitial - Operation just created. configured - The operation is configured, but nothing has been started. blocked - Only one operation can be performed on a node at a time. If more than one update is required for a component name (xname), then operations will be blocked. This will have a message of blocked by sibling. inProgress - Update is in progress, but not completed. verifying - Waiting for update to complete. failed - An update was attempted, but FAS is unable to tell that the update succeeded in the allotted time. noOperation - Firmware is at the correct version according to the images loaded into FAS. noSolution - FAS does not have a suitable image for an update. aborted - The operation was aborted before it could determine if it was successful. If aborted after the update command was sent to the node, then the node may still have updated. Firmware images FAS requires images in order to update firmware for any device on the system. An image contains the data that allows FAS to establish a link between an administrative command, available devices (xname/targets), and available firmware.\nThe following is an example of an image:\n{ \u0026#34;imageID\u0026#34;: \u0026#34;3fa85f64-5717-4562-b3fc-2c963f66afa6\u0026#34;, \u0026#34;createTime\u0026#34;: \u0026#34;2020-05-11T17:11:07.017Z\u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;nodeBMC\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;intel\u0026#34;, \u0026#34;model\u0026#34;: [\u0026#34;s2600\u0026#34;,\u0026#34;s2600_REV_a\u0026#34;], \u0026#34;target\u0026#34;: \u0026#34;BIOS\u0026#34;, \u0026#34;tag\u0026#34;: [\u0026#34;recovery\u0026#34;, \u0026#34;default\u0026#34;], \u0026#34;firmwareVersion\u0026#34;: \u0026#34;f1.123.24xz\u0026#34;, \u0026#34;semanticFirmwareVersion\u0026#34;: \u0026#34;v1.2.252\u0026#34;, \u0026#34;updateURI\u0026#34;: \u0026#34;/redfish/v1/Systems/UpdateService/BIOS\u0026#34;, \u0026#34;needManualReboot\u0026#34;: true, \u0026#34;waitTimeBeforeManualRebootSeconds\u0026#34;: 600, \u0026#34;waitTimeAfterRebootSeconds\u0026#34;: 180, \u0026#34;pollingSpeedSeconds\u0026#34;: 30, \u0026#34;forceResetType\u0026#34;: \u0026#34;ForceRestart\u0026#34;, \u0026#34;s3URL\u0026#34;: \u0026#34;s3://firmware/f1.1123.24.xz.iso\u0026#34;, \u0026#34;allowableDeviceStates\u0026#34;: [ \u0026#34;On\u0026#34;, \u0026#34;Off\u0026#34; ] } The main components of an image are described in the following sections.\nFirmware image: key This includes the deviceType, manufacturer, model, target, tag, semanticFirmwareVersion (firmware version) fields.\nThese fields are how administrators assess what firmware is on a device, and if an image is applicable to that device.\nFirmware image: process guides This includes the forceResetType, pollingSpeedSeconds, waitTime(s), allowableDeviceStates fields.\nFAS gets information about how to update the firmware from these fields. These values determine if FAS is responsible for rebooting the device, and what communication pattern to use.\nFirmware image: s3URL The URL that FAS uses to get the firmware binary and the download link that is supplied to Redfish devices. Redfish devices are not able to directly communicate with S3.\n"
},
{
	"uri": "/docs-csm/en-12/operations/conman/log_in_to_a_node_using_conman/",
	"title": "Log in to a Node Using ConMan",
	"tags": [],
	"description": "",
	"content": "Log in to a Node Using ConMan This procedure shows how to connect to the node\u0026rsquo;s Serial Over LAN (SOL) via ConMan.\nPrerequisites The user performing this procedure needs to have access permission to the cray-console-operator and cray-console-node pods.\nProcedure Note: this procedure has changed since the CSM 0.9 release.\nLog on to a Kubernetes master or worker node.\nFind the cray-console-operator pod.\nncn-mw# OP_POD=$(kubectl get pods -n services -o wide|grep cray-console-operator|awk \u0026#39;{print $1}\u0026#39;); echo $OP_POD Example output:\ncray-console-operator-6cf89ff566-kfnjr Set the XNAME variable to the component name (xname) of the node whose console is to be opened.\nncn-mw# XNAME=x123456789s0c0n0 Find the cray-console-node pod that is connected to that node.\nncn-mw# NODEPOD=$(kubectl -n services exec $OP_POD -c cray-console-operator -- sh -c \u0026#34;/app/get-node $XNAME\u0026#34; | jq .podname | sed \u0026#39;s/\u0026#34;//g\u0026#39;); echo $NODEPOD Example output:\ncray-console-node-1 Connect to the node\u0026rsquo;s console using ConMan on the cray-console-node pod that was found.\nncn-mw# kubectl exec -it -n services $NODEPOD -- conman -j $XNAME Example output:\n\u0026lt;ConMan\u0026gt; Connection to console [x3000c0s25b1] opened. nid000009 login: Using the command above, a user can also attach to an already active SOL session that is being used by another user, so both can access the node\u0026rsquo;s SOL simultaneously.\nExit the connection to the console with the \u0026amp;. command.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/cfs_key_management/",
	"title": "CFS Key Management and Permission Denied Errors",
	"tags": [],
	"description": "",
	"content": "CFS Key Management and Permission Denied Errors Configuration Framework Service (CFS) manages its own keys separate from keys for communication between CFS and the components or images that it is configuring. These are separate from the keys used by users and should not need to be managed.\nPermission Denied Errors If Ansible is unable to connect with its target and fails with an Unreachable - Permission denied error, the first place to check is the cfs-state-reporter on the target node.\nEvery booted node should be running a copy of cfs-state-reporter. This service is responsible for pulling down the public key. To check the status of this service, ssh to the node that CFS cannot communicate with, and run systemctl status cfs-state-reporter.\nncn-m001# systemctl status cfs-state-reporter cfs-state-reporter should be complete, but report success. Any other state can be an indication of a problem.\n● cfs-state-reporter.service - cfs-state-reporter reports configuration level of the system Loaded: loaded (/usr/lib/systemd/system/cfs-state-reporter.service; enabled; vendor preset: disabled) Active: inactive (dead) since Wed 2022-01-19 18:53:45 UTC; 1s ago Process: 678311 ExecStart=/usr/bin/python3 ${MODULEFLAG} ${MODULENAME} (code=exited, status=0/SUCCESS) Main PID: 678311 (code=exited, status=0/SUCCESS) cfs-state-reporter Failed If cfs-state-reporter is complete but has failed, it can safely be restarted with systemctl restart cfs-state-reporter, although any log messages in the status should first be noted in case they are needed for later debugging.\n● cfs-state-reporter.service - cfs-state-reporter reports configuration level of the system Loaded: loaded (/usr/lib/systemd/system/cfs-state-reporter.service; enabled; vendor preset: disabled) Active: failed (Result: exit-code) since Thu 2022-01-13 14:41:57 UTC; 6 days ago Process: 14849 ExecStart=/usr/bin/python3 ${MODULEFLAG} ${MODULENAME} (code=exited, status=1/FAILURE) Main PID: 14849 (code=exited, status=1/FAILURE) cfs-state-reporter Still Running cfs-state-reporter may also still be in a running state. In this case it is likely waiting either to authenticate or to pull down the SSH key. The service can safely be restarted as with the failure case, but this is less likely to be successful.\nIf the log messages indicate problems communicating with Spire, checking the health of the Spire service on the node is the next step. See Troubleshoot Spire Failing to Start on NCNs for more information on troubleshooting Spire.\nIf there are errors indicating failure to communicate with the Boot Script Service (BSS) or the metadata service, check the health of BSS with kubectl -n services logs deployment/cray-bss -c cray-bss and the health of cfs-trust with kubectl -n services logs deployment/cfs-trust -c cfs-trust.\n"
},
{
	"uri": "/docs-csm/en-12/operations/compute_rolling_upgrades/upgrade_compute_nodes_with_crus/",
	"title": "Upgrade Compute Nodes with CRUS",
	"tags": [],
	"description": "",
	"content": "Upgrade Compute Nodes with CRUS Note: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality. See Deprecated features.\nUpgrade a set of compute nodes with the Compute Rolling Upgrade Service (CRUS). Manage the workload management status of nodes and quiesce each node before taking the node out of service and upgrading it. Then reboot it into the upgraded state and return it to service within the workload manager (WLM).\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray Command Line Interface. A Boot Orchestration Service (BOS) session template describing the desired states of the nodes being upgraded must exist. Procedure This procedure can be run on any master or worker NCN.\nCreate and populate the starting node group.\nThis is the group of nodes that will be upgraded.\nCreate a starting node group (starting label).\nLabel names are defined by the user. The names used in this procedure are only examples. The label name used in this example is slurm-nodes.\nncn-mw# cray hsm groups create --label slurm-nodes --description \u0026#39;Starting Node Group for my Compute Node upgrade\u0026#39; Add members to the group.\nAdd compute nodes to the group by using the component name (xname) for each node being added.\nncn-mw# cray hsm groups members create slurm-nodes --id XNAME --format toml Example output:\n[[results]] URI = \u0026#34;/hsm/v2/groups/slurm-nodes/members/x0c0s28b0n0\u0026#34; Create a group for upgrading nodes (upgrading label).\nThe label name used in this example is upgrading-nodes.\nncn-mw# cray hsm groups create --label upgrading-nodes --description \u0026#39;Upgrading Node Group for my Compute Node upgrade\u0026#39; Do not add members to this group; it should be empty when the compute rolling upgrade process begins.\nCreate a group for failed nodes (failed label).\nThe label name used in this example is failed-nodes.\nncn-mw# cray hsm groups create --label failed-nodes --description \u0026#39;Failed Node Group for my Compute Node upgrade\u0026#39; Do not add members to this group; it should be empty when the compute rolling upgrade process begins.\nCreate an upgrade session with CRUS.\nThe following example is upgrading 50 nodes per step. The --upgrade-template-id value should be the name of the Boot Orchestration Service (BOS) session template being used to reboot the nodes.\nncn-mw# cray crus session create \\ --starting-label slurm-nodes \\ --upgrading-label upgrading-nodes \\ --failed-label failed-nodes \\ --upgrade-step-size 50 \\ --workload-manager-type slurm \\ --upgrade-template-id=BOS_SESSION_TEMPLATE_NAME \\ --format toml Example output:\napi_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-nodes\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;e0131663-dbee-47c2-aa5c-13fe9b110242\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;upgrading-nodes\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; Note the upgrade_id in the returned data of the previous command.\nncn-mw# UPGRADE_ID=e0131663-dbee-47c2-aa5c-13fe9b110242 Monitor the status of the upgrade session.\nThe progress of the session through the upgrade process is described in the messages field of the session. This is a list of messages, in chronological order, containing information about stage transitions, step transitions, and other conditions of interest encountered by the session as it progresses. It is cleared once the session completes.\nncn-mw# cray crus session describe $UPGRADE_ID --format toml Example output:\napi_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-nodes\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [ \u0026#34;Quiesce requested in step 0: moving to QUIESCING\u0026#34;, \u0026#34;All nodes quiesced in step 0: moving to QUIESCED\u0026#34;, \u0026#34;Began the boot session for step 0: moving to BOOTING\u0026#34;,] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;e0131663-dbee-47c2-aa5c-13fe9b110242\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;upgrading-nodes\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; A CRUS session goes through a number of steps (approximately the number of nodes to be upgraded divided by the requested step size) to complete an upgrade. Each step moves through the following stages, unless the boot session is interrupted by being deleted.\nStarting - Preparation for the step; CRUS initiates WLM quiescing of nodes. Quiescing - Waits for all WLM nodes in the step to reach a quiesced (not busy) state. Quiesced - The nodes in the step are all quiesced and CRUS initiates booting the nodes into the upgraded environment. Booting - Waits for the boot session to complete. Booted - The boot session has completed. Check the success or failure of the boot session. Mark all nodes in the step as failed if the boot session failed. WLM Waiting - The boot session succeeded. Wait for nodes to reach a ready state in the WLM. All nodes in the step that fail to reach a ready state within 10 minutes of entering this stage are marked as failed. Cleanup - The upgrade step has finished. Clean up resources to prepare for the next step. When a step moves from one stage to the next, CRUS adds a message to the messages field of the upgrade session to mark the progress.\nDelete the CRUS upgrade session. (Optional)\nOnce a CRUS upgrade session has completed, it can no longer be used. It can be kept for historical purposes if desired, or it can be deleted.\nncn-mw# cray crus session delete $UPGRADE_ID --format toml Example output:\napi_version = \u0026#34;1.0.0\u0026#34; completed = true failed_label = \u0026#34;failed-nodes\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [ \u0026#34;Upgrade Session Completed\u0026#34;,] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;DELETING\u0026#34; upgrade_id = \u0026#34;e0131663-dbee-47c2-aa5c-13fe9b110242\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;upgrading-nodes\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; The session may be visible briefly after it is deleted. This allows for orderly cleanup of the session.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/check_the_progress_of_bos_session_operations/",
	"title": "Check the Progress of BOS Session Operations",
	"tags": [],
	"description": "",
	"content": "Check the Progress of BOS Session Operations Describes how to view the logs of BOS operations with Kubernetes.\nWhen a Boot Orchestration Service (BOS) session is created, it will return a job ID. This ID can be used to locate the Boot Orchestration Agent (BOA) Kubernetes job that executes the session. For example:\nncn-mw# cray bos session create --template-uuid SESSIONTEMPLATE_NAME --operation boot --format toml Example output:\noperation = \u0026#34;boot\u0026#34; templateName = \u0026#34;SESSIONTEMPLATE_NAME\u0026#34; [[links]] href = \u0026#34;foo-c7faa704-3f98-4c91-bdfb-e377a184ab4f\u0026#34; jobId = \u0026#34;boa-a939bd32-9d27-433f-afc2-735e77ec8e58\u0026#34; rel = \u0026#34;session\u0026#34; type = \u0026#34;GET\u0026#34; All BOS Kubernetes pods operate in the services namespace.\nFind the BOA Kubernetes job Use the following command to locate the Kubernetes BOA pod.\nncn-mw# kubectl get pods -n services | grep -E \u0026#34;NAME | BOS_SESSION_JOB_ID\u0026#34; For example:\nncn-mw# kubectl get pods -n services | grep -E \u0026#34;NAME | boa-a939bd32-9d27-433f-afc2-735e77ec8e58\u0026#34; Example output:\nNAME READY STATUS RESTARTS AGE boa-a939bd32-9d27-433f-afc2-735e77ec8e58-ztscd 0/2 Completed 0 16m Use the following command to locate the Kubernetes BOA job.\nncn-mw# kubectl get jobs -n services BOS_SESSION_JOB_ID For example:\nncn-mw# kubectl get jobs -n services boa-a939bd32-9d27-433f-afc2-735e77ec8e58 Example output:\nNAME COMPLETIONS DURATION AGE boa-a939bd32-9d27-433f-afc2-735e77ec8e58 1/1 13m 15m The Kubernetes BOA pod name is not a one-to-one match with the BOA job name. The pod name has -XXXX appended to it, where \u0026lsquo;X\u0026rsquo; is a hexadecimal digit.\nView the BOA log Use the following command to look at the BOA pod\u0026rsquo;s logs.\nncn-mw# kubectl logs -n services KUBERNETES_BOA_POD_ID -c boa For example:\nncn-mw# kubectl logs -n services boa-a939bd32-9d27-433f-afc2-735e77ec8e58 -c boa View the Configuration Framework Service (CFS) log If a session template has CFS enabled, then BOA will attempt to configure the nodes during a boot, reboot, or configure operation. Use the BOA job ID to find the CFS job that BOA launched to configure the nodes.\nncn-mw# cray cfs sessions describe BOA_JOB_ID For example:\nncn-mw# cray cfs sessions describe boa-86b78489-1d76-4957-9c0e-a7b1d6665c35 --format json Example output:\n{ \u0026#34;ansible\u0026#34;: { \u0026#34;limit\u0026#34;: \u0026#34;x3000c0s19b4n0,x3000c0s19b3n0,x3000c0s19b2n0,x3000c0s19b1n0\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; }, \u0026#34;id\u0026#34;: \u0026#34;ffdda2c6-2277-11ea-8db8-b42e993b706a\u0026#34;, \u0026#34;links\u0026#34;: [ { \u0026#34;href\u0026#34;: \u0026#34;/apis/cfs/sessions/boa-86b78489-1d76-4957-9c0e-a7b1d6665c35\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;self\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/apis/cms.cray.com/v1/namespaces/services/cfsessions/boa-86b78489-1d76-4957-9c0e-a7b1d6665c35\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;k8s\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;boa-86b78489-1d76-4957-9c0e-a7b1d6665c35\u0026#34;, \u0026#34;repo\u0026#34;: { \u0026#34;branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;artifacts\u0026#34;: [], \u0026#34;session\u0026#34;: { \u0026#34;completionTime\u0026#34;: \u0026#34;2019-12-19T16:05:11+00:00\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;cfs-85e3e48f-6795-4570-b379-347b05b39dbe\u0026#34;, \u0026lt;\u0026lt;-- Kubernetes CFS job ID \u0026#34;startTime\u0026#34;: \u0026#34;2019-12-19T15:55:37+00:00\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;complete\u0026#34;, \u0026#34;succeeded\u0026#34;: \u0026#34;true\u0026#34; }, \u0026#34;targets\u0026#34;: { \u0026#34;failed\u0026#34;: 0, \u0026#34;running\u0026#34;: 0, \u0026#34;success\u0026#34;: 0 } }, \u0026#34;target\u0026#34;: { \u0026#34;definition\u0026#34;: \u0026#34;dynamic\u0026#34;, \u0026#34;groups\u0026#34;: [] } } Use the Kubernetes CFS job ID in the returned output above to find the CFS pod ID. It is the pod with three containers listed, not two.\nncn-mw# kubectl -n services get pods|grep KUBERNETES_CFS_JOB_ID Example output:\ncfs-85e3e48f-6795-4570-b379-347b05b39dbe-59645667b-ffznt 2/2 Running 0 3h57m cfs-85e3e48f-6795-4570-b379-347b05b39dbe-cvr54 0/3 Completed 0 3h57m View the pod\u0026rsquo;s logs for the Ansible container:\nncn-mw# kubectl -n services logs -f -c ansible KUBERNETES_CFS_POD_ID View the BOS log The BOS log shows when a session was launched. It also logs any errors encountered while attempting to launch a session.\nThe BOS Kubernetes pod ID can be found with the following command:\nncn-mw# kubectl get pods -n services | grep bos | grep -v etcd Example output:\ncray-bos-d97cf465c-klcrw 2/2 Running 0 90s Examine the logs:\nncn-mw# kubectl logs BOS_POD_ID BOS uses an etcd database. Looking at the etcd logs is typically not necessary.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/configure_uais_in_uas/",
	"title": "Configure UAIs in UAS",
	"tags": [],
	"description": "",
	"content": "Configure UAIs in UAS The sub-topics in this section cover the four main elements of UAI configuration in UAS, and provide Links to procedures for listing, adding, examining, updating, and deleting each kind of element.\nOptions for the elements of a UAI are maintained in the UAS configuration. The following can be configured in UAS:\nUAI images Volumes Resource Specifications UAI Classes Only users who are defined as administrators in an HPE Cray EX system and are logged in using the administrative CLI (cray command) can configure UAS. Configure UAS from a LiveCD node or from any system with the administrative CLI installed that can reach the HPE Cray EX API Gateway as an administrator.\nThe following procedures provide instructions for creating, updating, examining, and removing configuration items from UAS:\nUAI Images: List Registered UAI Images Register a UAI Image Retrieve UAI Image Registration Information Update a UAI Image Registration Delete a UAI Image Registration UAS Volumes: List Volumes Registered in UAS Add a Volume to UAS Obtain the Configuration of a UAS Volume Update a UAS Volume Delete a Volume Configuration UAI Resource Specifications: List UAI Resource Specifications Retrieve Resource Specification Details Create a UAI Resource Specification Update a Resource Specification Delete a UAI Resource Specification UAI Classes: List Available UAI Classes View a UAI Class Create a UAI Class Modify a UAI Class Delete a UAI Class Top: User Access Service (UAS)\nNext Topic: UAI Images\n"
},
{
	"uri": "/docs-csm/en-12/operations/csm_product_management/perform_ncn_personalization/",
	"title": "Perform NCN Personalization",
	"tags": [],
	"description": "",
	"content": "Perform NCN Personalization NCN personalization is the process of applying product-specific configuration to NCNs post-boot.\nPrerequisites Prior to running this procedure, gather the following information required by CFS to create a configuration layer:\nHTTP clone URL for the configuration repository in VCS Path to the Ansible play to run in the repository Commit ID in the repository for CFS to pull and run on the nodes Products may supply multiple plays to run, in which case multiple configuration layers must be created. Consult the manual for each product to configure them on NCNs by referring to the HPE Cray EX System Software Getting Started Guide (S-8000) 22.07 on the HPE Customer Support Center.\nProcedure: Perform NCN Personalization Determine if NCN Personalization CFS Configuration Exists If upgrading a product to a new version, an NCN personalization configuration in CFS should already exist. By default, the configuration is named ncn-personalization. If the default name is not used, substitute that name in the steps below.\nDetermine if a configuration already exists.\nncn# cray cfs configurations describe ncn-personalization --format json \u0026gt; ncn-personalization.json If the configuration exists, the ncn-personalization.json file will be created and populated with previously defined configuration layers. If it does not exist, the file will be created empty and the command will respond with an error. This error can be ignored.\nAdd Layer(s) to the CFS Configuration CFS executes configuration layers in order. Refer to the HPE Cray EX System Software Getting Started Guide (S-8000) 22.07 on the HPE Customer Support Center to determine if the configuration layer requires special placement in the layer list.\nNOTE: The CSM configuration layer MUST be the first layer in the NCN personalization CFS configuration.\nAdd a configuration layer to the ncn-personalization.json file. Follow the appropriate step based on if an NCN personalization CFS configuration exists: If the ncn-personalization.json file is empty, overwrite the file with the configuration layer(s) information gathered from the product that is configuring the NCNs. Use the sample file with a single layer as a template.\nIf a CFS configuration exists with one or more layers, add (or replace) the corresponding layer entry/entries with the configuration layer information gathered for this specific product. For example:\nncn# cat ncn-personalization.json Example configuration:\n{ \u0026#34;layers\u0026#34;: [ # ... { \u0026#34;name\u0026#34;: \u0026#34;\u0026lt;product-release-etc\u0026gt;\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/\u0026lt;product\u0026gt;-config-management.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit\u0026gt;\u0026#34; }, # ... ] } Create/Update the NCN Personalization CFS Configuration Layer Upload the configuration file to CFS to update or create the ncn-personalization CFS configuration.\nncn# cray cfs configurations update ncn-personalization --file ncn-personalization.json --format json Example output:\n{ \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:01Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { ... layer information here ... }, ], \u0026#34;name\u0026#34;: \u0026#34;ncn-personalization\u0026#34; } Set the Desired Configuration on NCNs Update the desired configuration for all NCNs.\nncn# for xname in $(cray hsm state components list --role Management --type node --format json | jq -r .Components[].ID) do cray cfs components update --desired-config ncn-personalization --enabled true --format json $xname done After this command is issued, the CFS Batcher service will dispatch a CFS session to configure the NCNs. Since the NCN is now managed by CFS by setting a desired configuration, the same will happen every time the NCN boots.\nQuery the status of the NCN Personalization process. The status will be pending while the node is being configured by CFS, and will change to configured when the configuration has completed.\nncn# for xname in $(cray hsm state components list \\ --role Management --type node \\ --format json | jq -r .Components[].ID) do cray cfs components describe --format json $xname | jq -r \u0026#39; .id+\u0026#34; status=\u0026#34;+.configurationStatus\u0026#39; done Example output:\nx3000c0s17b0n0 status=configured x3000c0s19b0n0 status=pending x3000c0s21b0n0 status=configured [...] The NCN personalization step is complete and the NCNs are now configured as specified in the ncn-personalization configuration layers when each node\u0026rsquo;s status is configured.\nSee Configuration Management of System Components for more information on setting desired configuration on specific nodes using CFS.\nProcedure: Re-Run NCN Personalization If no changes have been made to the configuration layers (such as a new layer, different playbook, or new commit made), but NCN personalization needs to be run again, CFS can re-run NCN personalization on specific nodes.\nRe-run the configuration for an NCN by clearing the state of the node. Clearing the node state will cause CFS to reconfigure the node, so long as the desired configuration was set previously.\nClear the state and error count of the node using CFS.\nReplace the \u0026lt;XNAME\u0026gt; string in the following command with the xname of the node being reconfigured.\nncn# cray cfs components update --error-count 0 --state \u0026#39;[]\u0026#39; --format json \u0026lt;XNAME\u0026gt; (Optional) To re-run NCN personalization on all NCNs at once, use the following loop:\nncn# for xname in $(cray hsm state components list \\ --role Management --type node \\ --format json | jq -r .Components[].ID) do cray cfs components update --error-count 0 --state \u0026#39;[]\u0026#39; --format json $xname done "
},
{
	"uri": "/docs-csm/en-12/introduction/scenarios/",
	"title": "Scenarios for Shasta v1.5",
	"tags": [],
	"description": "",
	"content": "Scenarios for Shasta v1.5 There are multiple scenarios for installing CSM software which are described in this documentation with many supporting procedures.\nScenarios for Shasta v1.5 Installation Upgrade Migration Installation There are two ways to install the CSM software. There are some differences between a first time install which must create the initial configuration payload and configure the management network switches, whereas a reinstall can reuse a previous configuration payload and skip the configuration of management network switches. The first time install will check and then may update firmware for various components whereas the reinstall will check and indicate that no firmware update is required. There are two different ways to use the LiveCD, either from a RemoteISO or a USB device, which are described in Bootstrap PIT Node. There are a few places where a comment will be made in a procedure for how one of the scenarios needs to do something differently.\nFirst time Install\nPrepare Configuration Payload creates the initial configuration payload. Bootstrap PIT Node Configure Management Network Switches Reinstall\nPrepare Configuration Payload can reuse a previous configuration payload. There may be additional steps to manually wipe disks on the management nodes and do other actions to prepare the management node hardware for the reinstall. Bootstrap PIT Node Can skip the procedure to Configure Management Network Switches The two paths merge together after configuration of the management network switches to do later actions the same regardless of the starting point in the workflow.\nDeploy Management Nodes Install CSM Services Validate CSM Health Before Final NCN Deployment Deploy Final NCN Configure Administrative Access Validate CSM Health Update Firmware with FAS Prepare Compute Nodes After completion of the firmware update with FAS and the preparation of compute nodes, the CSM product stream has been fully installed and configured. Refer to the HPE Cray EX System Software Getting Started Guide (S-8000) 22.07 for more information on other product streams to be installed and configured after CSM.\nSee Install CSM for the details on the installation process for either a first time install or a reinstall.\nUpgrade The upgrade from Shasta v1.4.2 (including CSM 0.9.3) to Shasta v1.5 (including CSM 1.0) is supported. This process will upgrade the Ceph storage software, then the storage nodes, then the Kubernetes master nodes and worker nodes, and finally the CSM services. The management nodes are upgraded using a rolling upgrade approach which enables management services to continue to function even as one or a few nodes are being upgraded.\nSee Upgrade CSM.\nMigration There is no direct migration from Shasta v1.3.x releases to Shasta v1.5. However, there is a supported path.\nMigration from v1.3.x to v1.4.0\nThe migration from v1.3.x to v1.4.0 is described in the Shasta v1.4 documentation. Refer to \u0026ldquo;1.3 to 1.4 Install Prerequisites\u0026rdquo; and \u0026ldquo;Collect Data From Healthy Shasta 1.3 System for EX 1.4 Installation\u0026rdquo; in the HPE Cray EX System Installation and Configuration Guide (S-8000) 1.4.\nUpgrade v1.4.x to v1.5\nAn upgrade from the previous release (Shasta v1.4.x) is supported with this release.\nSee Upgrade from v1.4.x to v1.5.\n"
},
{
	"uri": "/docs-csm/en-12/install/wipe_ncn_disks_for_reinstallation/",
	"title": "Wipe NCN Disks for Reinstallation",
	"tags": [],
	"description": "",
	"content": "Wipe NCN Disks for Reinstallation This page details how to wipe NCN disks. This page uses the same wipe commands that the automatic wipe uses. However, this page accounts for running services that are present on a running CSM NCN.\nEverything in this section should be considered DESTRUCTIVE.\nAfter following these procedures, an NCN can be rebooted and redeployed.\nNOTE All types of disk wipe can be run from Linux or from an emergency shell.\nThe following are potential use cases for wiping disks:\nAdding a node that is not bare. Adopting new disks that are not bare. Doing a fresh install. For wiping Linux on an NCN with a previously installed OS, the basic wipe is sufficient.\nTopics Basic wipe Advanced wipe Full wipe Basic wipe This wipe erases the magic bits on the disk to prevent them from being recognized and making them ready for deployment, as well as removing the common volume groups.\nList the disks for verification.\nncn# disks_to_wipe=$(lsblk -l -o NAME,TYPE,TRAN | grep -E \u0026#39;[[:space:]].*(sata|nvme|sas|raid)\u0026#39; | awk \u0026#39;{ print \u0026#34;/dev/\u0026#34;$1 }\u0026#39; | sort -u | tr \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39;) ncn# echo \u0026#34;${disks_to_wipe}\u0026#34; Wipe the disks and the RAIDs.\nncn# for disk in $disks_to_wipe; do wipefs --all --force ${disk}* 2\u0026gt; /dev/null done If any disks had labels present, then the output looks similar to the following:\n/dev/sda: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa /dev/sdb: 6 bytes were erased at offset 0x00000000 (crypto_LUKS): 4c 55 4b 53 ba be /dev/sdb: 6 bytes were erased at offset 0x00004000 (crypto_LUKS): 53 4b 55 4c ba be /dev/sdc: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sdc: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sdc: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa Verify that there are no error messages in the output.\nThe wipefs command may fail if no labeled disks are found, which is an indication of a larger problem.\nRemove the volume groups on all NCNs.\nNOTE The Ceph volume group will only exist on storage nodes, but this code snippet will work on all NCNs.\nncn# ceph_vgs=\u0026#39;vg_name=~ceph*\u0026#39; ncn# metal_vgs=\u0026#39;vg_name=~metal*\u0026#39; ncn# for volume_group in ${ceph_vgs} ${metal_vgs}; do vgremove -f -v --select \u0026#34;${volume_group}\u0026#34; -y \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 done Advanced wipe An advanced wipe includes handling storage node specific items before running the Basic wipe.\nStop Ceph on all of the storage nodes.\nCSM 0.9 or earlier\nncn-s# systemctl stop ceph-osd.target CSM 1.0 or later\nncn-s# cephadm rm-cluster --fsid $(cephadm ls|jq -r \u0026#39;.[0].fsid\u0026#39;) --force Make sure the OSDs (if any) are not running on the storage nodes.\nCSM 0.9 or earlier\nncn-s# ps -ef|grep ceph-osd CSM 1.0 or later\nncn-s# podman ps Examine the output. There should be no running ceph-osd processes or containers.\nPerform the Basic wipe procedure.\nFull wipe This section walks a user through cleanly stopping all running services that require partitions, as well removing the node from the Ceph or Kubernetes cluster (as appropriate for the node type).\nThis does not zero disks; this will ensure that all disks look raw on the next reboot.\nIMPORTANT For each step, pay attention to whether the command is to be run on a master node, storage node, or worker node. If wiping a different type of node than what a step specifies, then skip that step.\nReset Kubernetes on worker nodes ONLY.\nThis will stop kubelet, stop underlying containers, and remove the contents of /var/lib/kubelet.\nReset Kubernetes.\nncn-w# kubeadm reset --force List any containers running in containerd.\nncn-w# crictl ps Example output:\nCONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 66a78adf6b4c2 18b6035f5a9ce About a minute ago Running spire-bundle 1212 6d89f7dee8ab6 7680e4050386d c8344c866fa55 24 hours ago Running speaker 0 5460d2bffb4d7 b6467c907f063 8e6730a2b718c 3 days ago Running request-ncn-join-token 0 a3a9ca9e1ca78 e8ce2d1a8379f 64d4c06dc3fb4 3 days ago Running istio-proxy 0 6d89f7dee8ab6 c3d4811fc3cd0 0215a709bdd9b 3 days ago Running weave-npc 0 f5e25c12e617e If there are any running containers from the output of the crictl ps command, then stop them.\nncn-w# crictl stop \u0026lt;container id from the CONTAINER column\u0026gt; Reset Kubernetes on master nodes ONLY.\nThis will stop kubelet, stop underlying containers, and remove the contents of /var/lib/kubelet.\nReset Kubernetes.\nncn-m# kubeadm reset --force List any containers running in containerd.\nncn-m# crictl ps Example output:\nCONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 66a78adf6b4c2 18b6035f5a9ce About a minute ago Running spire-bundle 1212 6d89f7dee8ab6 7680e4050386d c8344c866fa55 24 hours ago Running speaker 0 5460d2bffb4d7 b6467c907f063 8e6730a2b718c 3 days ago Running request-ncn-join-token 0 a3a9ca9e1ca78 e8ce2d1a8379f 64d4c06dc3fb4 3 days ago Running istio-proxy 0 6d89f7dee8ab6 c3d4811fc3cd0 0215a709bdd9b 3 days ago Running weave-npc 0 f5e25c12e617e If there are any running containers from the output of the crictl ps command, then stop them.\nncn-m# crictl stop \u0026lt;container id from the CONTAINER column\u0026gt; Run the Advanced wipe, but stop when it mentions the \u0026ldquo;basic wipe\u0026rdquo;. Then return here.\nUnmount volumes.\nNOTE: Some of the following umount commands may fail or have warnings depending on the state of the NCN. Failures in this section can be ignored and will not inhibit the wipe process.\nNOTE: There is an edge case where the overlay may keep the drive from being unmounted. If this is a rebuild, ignore this.\nThe exact commands used depends on the node type:\nMaster nodes\nStop the etcd service on the master node before unmounting /var/lib/etcd and other mounts.\nncn-m# systemctl stop etcd.service ncn-m# umount -v /run/lib-etcd /var/lib/etcd /var/lib/sdu /var/opt/cray/sdu/collection-mount /var/lib/admin-tools /var/lib/s3fs_cache /var/lib/containerd Storage nodes\nncn-s# umount -vf /var/lib/ceph /var/lib/containers /etc/ceph /var/opt/cray/sdu/collection-mount /var/lib/admin-tools /var/lib/s3fs_cache /var/lib/containerd If the umount command outputs target is busy on the storage node, then try the following:\nLook for containers mounts:\nncn-s# mount | grep containers Example output:\n/dev/mapper/metalvg0-CONTAIN on /var/lib/containers type xfs (rw,noatime,swalloc,attr2,largeio,inode64,allocsize=131072k,logbufs=8,logbsize=32k,noquota) /dev/mapper/metalvg0-CONTAIN on /var/lib/containers/storage/overlay type xfs (rw,noatime,swalloc,attr2,largeio,inode64,allocsize=131072k,logbufs=8,logbsize=32k,noquota) Unmount /var/lib/containers/storage/overlay.\nncn-s# umount -v /var/lib/containers/storage/overlay Example output:\numount: /var/lib/containers/storage/overlay unmounted Unmount /var/lib/containers.\nncn-s# umount -v /var/lib/containers Example output:\numount: /var/lib/containers unmounted Worker nodes\nncn-w# umount -v /var/lib/kubelet /var/lib/sdu /run/containerd /var/lib/containerd /run/lib-containerd /var/opt/cray/sdu/collection-mount /var/lib/admin-tools /var/lib/s3fs_cache /var/lib/containerd Stop cray-sdu-rda on all node types (master, storage, or worker).\nSee if any cray-sdu-rda containers are running.\nncn# podman ps Example output:\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7741d5096625 registry.local/sdu-docker-stable-local/cray-sdu-rda:1.1.1 /bin/sh -c /usr/s... 6 weeks ago Up 6 weeks ago cray-sdu-rda If there is a running cray-sdu-rda container in the above output, then stop it using the container ID:\nncn# podman stop 7741d5096625 Example output:\n7741d50966259410298bb4c3210e6665cdbd57a82e34e467d239f519ae3f17d4 Remove etcd device on master nodes ONLY.\nDetermine whether or not an etcd volume is present.\nncn-m# dmsetup ls Expected output when the etcd volume is present will show ETCDLVM, but the numbers might be different.\nETCDLVM (254:1) Remove the etcd device mapper.\nncn-m# dmsetup remove $(dmsetup ls | grep -i etcd | awk \u0026#39;{print $1}\u0026#39;) NOTE: The following output means that the etcd volume mapper is not present. This is okay.\nNo device specified. Command failed. Remove etcd volumes on master nodes ONLY.\nncn-m# vgremove etcdvg0 Perform the Basic wipe procedure.\n"
},
{
	"uri": "/docs-csm/en-12/install/clear_gigabyte_cmos/",
	"title": "Clear Gigabyte CMOS",
	"tags": [],
	"description": "",
	"content": "Clear Gigabyte CMOS Because of a bug in the Gigabyte firmware, the Shasta 1.5 install may negatively impact Gigabyte motherboards when attempting to boot using bonded Mellanox network cards. The result is a board that is unusable until a CMOS clear is physically done via a jumper on the board itself.\nA patched firmware release (newer than C20 BIOS) is expected to be available for a future release of Shasta. It is recommended that Gigabyte users wait for this new firmware before attempting an installation of Shasta 1.5. The procedure to recover the boards is included below.\nAll firmware can be found with HFP package provided with the Shasta release.\nClear BIOS settings by jumper Pull the power cables or blade server from the chassis, and open the system top cover. Move the Clear CMOS Jumper to 2-3, and wait 2 to 3 seconds. Move the Clear CMOS Jumper to 1-2. Motherboard MZ62-HD0-00/-YF for Gigabyte H262 chassis Motherboard MZ32-AR0-00/-YF for Gigabyte R272 chassis Motherboard MZ92-FS0-00/-YF for Gigabyte R282 chassis "
},
{
	"uri": "/docs-csm/en-12/background/ncn_mounts_and_file_systems/",
	"title": "NCN Mounts and File Systems",
	"tags": [],
	"description": "",
	"content": "NCN Mounts and File Systems The management non-compute nodes (NCNs) use drive storage for persistence and block storage. This page outlines reference information for these disks, their partition tables, and their management.\nWhat controls partitioning Plan of record / baseline Problems when above or below baseline Disk layout quick-reference tables OverlayFS and persistence SQFSRAID and ROOTRAID overlays Helpful commands OverlayFS example mount command losetup command lsblk command Persistent directories Layering: Upper and lower directory Layering: Real world example OverlayFS control Reset toggles Reset on next boot Reset on every boot Re-sizing the persistent overlay Thin overlay feature metalfs Old/retired FS labels What controls partitioning Partitioning is controlled by two aspects:\ndracut: this selects disks and builds their partition tables and/or LVM storage. cloud-init: this manages standalone partitions or volumes, as well as high-level object storage. Plan of record / baseline Node Type # \u0026ldquo;small\u0026rdquo; disks (0.5 TiB) # \u0026ldquo;large\u0026rdquo; disks (1.9 TiB) master NCNs 3 0 storage NCNs 2 3+ worker NCNs 2 1 Disks are chosen by dracut. Kubernetes and storage nodes use different dracut modules.\nTwo disks for the OS are chosen from the pool of \u0026ldquo;small\u0026rdquo; disks. One disk is selected for the ephemeral data. Problems when above or below baseline The master and worker NCNs use the same artifacts, and therefore have the same dracut modules.\nStorage NCNs do not run the same dracut modules because they have different disk demands.\nBe cautious of master NCNs with one or more extra \u0026ldquo;large\u0026rdquo; disks; these disks help but are unnecessary.\nOn the other hand, there can be problems with worker NCNs with one or more extra \u0026ldquo;small\u0026rdquo; disks, because they can end up being used by the metal ETCD module. For worker NCNs with extra \u0026ldquo;small\u0026rdquo; disks, expand the RAID to consume the extra disks, leaving none behind for the metal ETCD module to find.\nSet metal.disks equal to the number of \u0026ldquo;small\u0026rdquo; disks in the node.\nThis will reserve them for the RAID and prevent any other partitioning from happening on them.\nFrom the pit node:\npit# sed -i \u0026#39;s/disk-opts /disk-opts metal.disks=3 /g\u0026#39; /var/www/ncn-w*/script.ipxe During runtime with csi:\nncn# csi handoff bss-update-param metal.disks=3 (Optional) Change the RAID type.\nSkip this step to leave it at its default type (mirror).\nFrom the pit node:\npit# sed -i \u0026#39;s/disk-opts /disk-opts metal.md-level=stripe /g\u0026#39; /var/www/ncn-w*/script.ipxe During runtime with csi:\nncn# csi handoff bss-update-param metal.md-level=stripe Rebuild the node.\nRun the basic disk wipe if the node was already booted.\nReboot the node.\nDisk layout quick-reference tables The table below represents all recognizable FS labels on any given management node, varying slightly by node role (Kubernetes master or Kubernetes worker).\nMaster Worker Storage FS Label Partitions Devices Partition Size OverlayFS Notes Yes Yes Yes BOOTRAID /metal/recovery RAID1: 2 small disks 500 MiB No Yes Yes Yes SQFSRAID /run/initramfs/live RAID1: 2 small disks 25 GiB Yes Yes Yes Yes ROOTRAID /run/initramfs/overlayfs RAID1: 2 small disks 150 GiB Yes The persistent image file is loaded from this partition1. Yes Yes Yes AUX /dev/md/AUX (Not Mounted) RAID0: 2 small disks 250 GiB No Auxiliary RAID array for cloud-init to use. No No Yes CEPHETC /etc/ceph LVM 10 GiB No No No Yes CEPHVAR /var/lib/ceph LVM 60 GiB No No No Yes CONTAIN /run/containers LVM 60 GiB No Yes Yes No CRAYS3FSCACHE /var/lib/s3fs_cache LVM 100 GiB No No Yes No CONRUN /run/containerd Ephemeral 75 GiB No No Yes No CONLIB /run/lib-containerd Ephemeral 25% Yes Yes No No ETCDLVM /run/lib-etcd Ephemeral 32 GiB Yes Yes No No K8SLET /var/lib/kubelet Ephemeral 25% No The above table\u0026rsquo;s rows with OverlayFS map their Mount Paths to the Upper Directory in the table below:\nThe \u0026ldquo;OverlayFS Name\u0026rdquo; is the name used in /etc/fstab and seen in the output of mount.\nOverlayFS Name Upper Directory Lower Directory etcd_overlayfs /run/lib-etcd /var/lib/etcd containerd_overlayfs /run/lib-containerd /var/lib/containerd For notes on previous/old labels, see Old/retired FS labels.\nOverlayFS and persistence The overlays used on NCNs enable two critical functions:\nChanges to data and new data will persist between reboots. RAM (memory) is freed because the data is stored on block devices (SATA/PCIe). There are a few overlays used for NCN image boots:\nROOTRAID is the persistent root OverlayFS. It commits and saves all changes made to the running OS. CONLIB is a persistent OverlayFS for containerd. It commits and saves all new changes while allowing read-through to pre-existing data from the SquashFS. ETCDK8S is a persistent OverlayFS for etcd. It works like the CONLIB OverlayFS, but it exists in an encrypted LUKS2 partition. SQFSRAID and ROOTRAID overlays /run/rootfsbase is the SquashFS image itself. /run/initramfs/live is the SquashFS\u0026rsquo;s storage array, where one or more SquashFS can be stored. /run/initramfs/overlayfs is the OverlayFS storage array, where the persistent directories are stored. /run/overlayfs and /run/ovlwork are symbolic links to /run/initramfs/overlayfs/overlayfs-SQFSRAID-$(blkid -s UUID -o value /dev/disk/by-label/SQFSRAID) and the neighboring \u0026ldquo;work\u0026rdquo; directory2. Helpful commands Commands Details lsblk, lsblk -f Shows how the RAIDs and disks are mounted losetup -a Shows where the SquashFS is mounted from `mount grep \u0026rsquo; / \u0026lsquo;` OverlayFS examples mount command ncn-m# mount | grep \u0026#39; / \u0026#39; Example output:\nLiveOS_rootfs on / type overlay (rw,relatime,lowerdir=/run/rootfsbase,upperdir=/run/overlayfs,workdir=/run/ovlwork) ^^^R/O^SQUASHFS IMAGE^^^|^^^ R/W PERSISTENCE ^^^|^^^^^^INTERIM^^^^^^ losetup command ncn-m# losetup -a Example output:\n/dev/loop1: [0025]:74858 (/run/initramfs/thin-overlay/meta) /dev/loop2: [0025]:74859 (/run/initramfs/thin-overlay/data) /dev/loop0: [2430]:100 (/run/initramfs/live/LiveOS/filesystem.squashfs) The \u0026ldquo;thin overlay\u0026rdquo; is the transient space the system uses behind the scenes to allow data to be in RAM as it is written to disk. The \u0026ldquo;thin\u0026rdquo; part of the overlay is the magic; using thin overlays means that the kernel will automatically clear free blocks. For more details, see Thin overlay feature.\nlsblk command Below is the layout of what a persistent system looks like.\nncn-m# lsblk Example output:\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 3.8G 1 loop /run/rootfsbase loop1 7:1 0 30G 0 loop └─live-overlay-pool 254:2 0 300G 0 dm loop2 7:2 0 300G 0 loop └─live-overlay-pool 254:2 0 300G 0 dm sda 8:0 1 447.1G 0 disk ├─sda1 8:1 1 476M 0 part │ └─md127 9:127 0 476M 0 raid1 ├─sda2 8:2 1 92.7G 0 part │ └─md126 9:126 0 92.6G 0 raid1 /run/initramfs/live └─sda3 8:3 1 279.4G 0 part └─md125 9:125 0 279.3G 0 raid1 /run/initramfs/overlayfs sdb 8:16 1 447.1G 0 disk ├─sdb1 8:17 1 476M 0 part │ └─md127 9:127 0 476M 0 raid1 ├─sdb2 8:18 1 92.7G 0 part │ └─md126 9:126 0 92.6G 0 raid1 /run/initramfs/live └─sdb3 8:19 1 279.4G 0 part └─md125 9:125 0 279.3G 0 raid1 /run/initramfs/overlayfs sdc 8:32 1 447.1G 0 disk └─ETCDLVM 254:0 0 447.1G 0 crypt └─etcdvg0-ETCDK8S 254:1 0 32G 0 lvm /run/lib-etcd Note that the above output means that persistent capacity is there, but administrators should beware of reset toggles on unfamiliar systems. There are toggles to reset overlays that are, by default, toggled off (so that data persistence by default is safe, but one should not assume). For more information, see OverlayFS control.\nPersistent directories Not all directories are persistent!\nOnly the following directories are persistent by default:\n/etc /home /root /run/containerd /run/lib-containerd /run/lib-etcd /run/lib/kubelet /srv /tmp /var This initial set is managed by dracut. When using a reset toggle, the above list is reset to the above default value. While more directories can be added to the list, they will be eradicated when enabling a reset toggle. For more information, see OverlayFS control.\nThese are all provided through the overlay from /run/overlayfs:\nncn-m# cd /run/overlayfs \u0026amp;\u0026amp; ls -l Example output:\ntotal 0 drwxr-xr-x 8 root root 290 Oct 15 22:41 etc drwxr-xr-x 3 root root 18 Oct 15 22:41 home drwx------ 3 root root 39 Oct 13 16:53 root drwxr-xr-x 3 root root 18 Oct 5 19:16 srv drwxrwxrwt 2 root root 85 Oct 16 14:50 tmp drwxr-xr-x 8 root root 76 Oct 13 16:52 var Remember: /run/overlayfs is a symbolic link to the real disk /run/initramfs/overlayfs/*.\nLayering: Upper and lower directory The file system the user is working on is really two layered file systems (overlays).\nThe lower layer (also called the lower directory) is the SquashFS image itself. It is read-only and provides all that is needed to run. The upper layer (also called the upper directory) is the OverlayFS. It is read-write, and does a bit-wise xor with the lower layer. Anything in the upper layer takes precedence by default. There are fancier options for overlays, such as multiple lower layers, copy-up (lower layer precedence), and opaque (removing a directory in the upper layer hides it in the lower layer). For details, see Overlay Filesystem: Inode properties.\nLayering: Real world example Take /root for example.\nThe upper directory (the overlay) has these files:\nncn# ls -l /run/overlayfs/root/ Example output:\ntotal 4 -rw------- 1 root root 252 Nov 4 18:23 .bash_history drwxr-x--- 4 root root 37 Nov 4 04:35 .kube drwx------ 2 root root 29 Oct 21 21:57 .ssh The lower directory (the SquashFS image) has these files:\nncn# ls -l /run/rootfsbase/root/ Example output:\ntotal 1 -rw------- 1 root root 0 Oct 19 15:31 .bash_history drwxr-xr-x 2 root root 3 May 25 2018 bin drwx------ 3 root root 26 Oct 21 22:07 .cache drwx------ 2 root root 3 May 25 2018 .gnupg drwxr-xr-x 4 root root 57 Oct 19 15:23 inst-sys drwxr-xr-x 2 root root 33 Oct 19 15:33 .kbd drwxr-xr-x 5 root root 53 Oct 19 15:34 spire drwx------ 2 root root 70 Oct 21 21:57 .ssh -rw-r--r-- 1 root root 172 Oct 26 15:25 .wget-hsts Notice the following:\nThe .bash_history file in the lower directory is 0 bytes, but it is 252 bytes in the upper directory. The .kube directory exists in the upper directory, but not the lower directory. Keeping the above in mind, look at the contents of /root itself:\nncn# ls -l /root Example output:\ntotal 5 -rw------- 1 root root 252 Nov 4 18:23 .bash_history drwxr-xr-x 2 root root 3 May 25 2018 bin drwx------ 3 root root 26 Oct 21 22:07 .cache drwx------ 2 root root 3 May 25 2018 .gnupg drwxr-xr-x 4 root root 57 Oct 19 15:23 inst-sys drwxr-xr-x 2 root root 33 Oct 19 15:33 .kbd drwxr-x--- 4 root root 37 Nov 4 04:35 .kube drwxr-xr-x 5 root root 53 Oct 19 15:34 spire drwx------ 1 root root 29 Oct 21 21:57 .ssh -rw-r--r-- 1 root root 172 Oct 26 15:25 .wget-hsts Notice the following:\n.bash_history matches the upper directory. The .kube directory exists here. The take-away here is that any change done to /root/ will persist through /run/overlayfs/root and will take precedence to the SquashFS image root.\nOverlayFS control These features or toggles are passable on the kernel command line, and change the behavior of the OverlayFS.\nReset toggles The overlay FS provides a few reset toggles to clear out the persistence directories without reinstall.\nThe toggles require rebooting.\nReset on next boot The preferred way to reset persistent storage is to use the OverlayFS reset toggle.\nModify the boot command line on the PXE server, adding this\n# Reset the overlay on boot rd.live.overlay.reset=1 Once reset, if wanting to enable persistence again, then simply revert the change; the next reboot will persist.\n# Cease resetting the OverlayFS rd.live.overlay.reset=0 Reset on every boot There are two options one can leave enabled to accomplish this:\nrd.live.overlay.reset=1 will eradicate/recreate the overlay every reboot. rd.live.overlay.readonly=1 will clear the overlay on every reboot. For long-term usage, rd.live.overlay.readonly=1 should be added to the command line.\nThe reset=1 toggle is usually used to fix a problematic overlay. For example, if one wants to refresh and purge the overlay completely.\n# Authorize METAL to purge metal.no-wipe=0 rd.live.overlay.reset=1 Note: metal.no-wipe=1 does not protect against rd.live.overlay.reset. metal.no-wipe is not a feature of dmsquash-live.\nRe-sizing the persistent overlay Default size: 300 GiB File system: XFS The overlay can be resized to fit a variety of needs or use cases. The size is provided directly on the command line. Any value can be provided, but it must be in megabytes.\nIf resetting the overlay on a deployed node, rd.live.overlay.reset=1 must also be set.\nIt is recommended to set the size before deployment. There is a linkage between the metal-dracut module and the live-module that makes this inflexible.\n# Use a 300 GiB OverlayFS (default) rd.live.overlay.size=307200 # Use a 1 TiB OverlayFS rd.live.overlay.size=1000000 Thin overlay feature The persistent OverlayFS leverages newer \u0026ldquo;thin\u0026rdquo; overlays. These \u0026ldquo;thin\u0026rdquo; overlays support discards, and that will free blocks that are not claimed by the file system. This means that memory is freed/released when the file system no longer claims it.\nThin overlays can be disabled, and instead classic DM Snapshots can be used to manage the overlay. This will use more RAM. It is not recommended, because dmraid is not included in the initrd.\n# Enable (default) rd.live.overlay.thin=1 # Disable (not recommended; undesirable RAM waste) rd.live.overlay.thin=0 metalfs The metalfs systemd service will try to mount any metal-created partitions.\nThis runs against the /run/initramfs/overlayfs/fstab.metal when it exists. This file is dynamically created by most metal dracut modules.\nThe service will continuously attempt to mount the partitions. If problems arise, then stop the service:\nncn# systemctl stop metalfs Old/retired FS labels This is a table of deprecated FS labels/partitions from Shasta 1.3 (no longer in Shasta 1.4 / CSM 0.9 and onwards).\nFS Label Partitions Nodes Device Size on Disk K8SKUBE /var/lib/kubelet ncn-w001, ncn-w002 Ephemeral Max/Remainder K8SEPH /var/lib/cray/k8s_ephemeral ncn-w001, ncn-w002 Ephemeral Max/Remainder CRAYINSTALL /var/cray/vfat ncn-w001, ncn-w002 Ephemeral 12 GiB CRAYVBIS /var/cray/vbis ncn-w001, ncn-w002 Ephemeral 900 GiB CRAYNFS /var/lib/nfsroot/nmd ncn-w001, ncn-w002 Ephemeral 12 GiB CRAYSDU /var/lib/sdu All masters and workers LVM 100 GiB When the image is loaded, the underlying drive is lazily unmounted (umount -l), so that it will close once the overlay closes.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe \u0026ldquo;work\u0026rdquo; directory is where the operating system processes data. It is the interim where data passes between RAM and persistent storage.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/stage_5/",
	"title": "Stage 5 - Perform NCN Personalization",
	"tags": [],
	"description": "",
	"content": "Stage 5 - Perform NCN Personalization Reminder: If any problems are encountered and the procedure or command output does not provide relevant guidance, see Relevant troubleshooting links for upgrade-related issues.\nProcedure Set the root user password and SSH keys before running NCN personalization. The location where the password is stored in Vault has changed since previous CSM versions. See Configure the Root Password and Root SSH Keys in Vault.\nIf custom configuration content was merged with content from a previous CSM installation, then merge the new CSM configuration in with it in the csm-config-management Git repository. This is not necessary if the NCN personalization configuration was using a commit on a cray/csm/VERSION branch (that is, using the default configuration).\nThe new CSM configuration content is found in the cray-product-catalog Kubernetes ConfigMap. If using the default CSM configuration, simply note the value in the commit field.\nncn-m002# kubectl -n services get cm cray-product-catalog -o jsonpath=\u0026#39;{.data.csm}\u0026#39; The output will contain a section resembling the following:\n1.2.1: configuration: clone_url: https://vcs.cmn.SYSTEM_DOMAIN_NAME/vcs/cray/csm-config-management.git commit: 43ecfa8236bed625b54325ebb70916f55884b3a4 import_branch: cray/csm/1.9.24 import_date: 2022-07-28 03:26:01.869501 ssh_url: git@vcs.cmn.SYSTEM_DOMAIN_NAME:cray/csm-config-management.git The specific dates, commits, and other values may not be the same as the output above.\nView the current ncn-personalization configuration and write it to a JSON file.\nncn-m002# cray cfs configurations describe ncn-personalization --format json | tee ncn-personalization.json Run the apply_csm_configuration.sh script. This script will update the CSM layer in the ncn-personalization configuration, enable configuration of the NCNs, and monitor the progress of the NCN personalization process.\nIMPORTANT:\nIf using a different branch than the default to include custom changes, use the --git-commit argument to specify the desired commit on the branch including the customizations. Otherwise this argument is not needed. By default the latest available CSM release will be applied. Otherwise, the release may be specified explicitly using the --csm-release argument. This argument is not needed if using the default CSM configuration found in the product catalog in the earlier step. If the existing ncn-personalization configuration contains layers other than the CSM layer from the csm-config-management repository, then the arguments to the script should include --ncn-config-file. If this argument is not specified, then any existing non-csm layers will not be preserved in the new ncn-personalization configuration. ncn-m002# /usr/share/doc/csm/scripts/operations/configuration/apply_csm_configuration.sh \\ [--csm-release 1.2.1] [--git-commit COMMIT] [--ncn-config-file /path/to/ncn-personalization.json] For more information on this script, see Automatically Apply CSM Configuration to NCNs.\nReview the new ncn-personalization configuration and write it to a JSON file.\nncn-m002# cray cfs configurations describe ncn-personalization --format json | tee ncn-personalization.json.new Stage completed This stage is completed. Continue to Validate CSM Health on the main upgrade page.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/pxe_runbook/",
	"title": "PXE Booting Runbook",
	"tags": [],
	"description": "",
	"content": "PXE Booting Runbook PXE booting is a key component of a working Shasta system. There are a lot of different components involved, which increases the complexity. This guide runs through the most common issues and shows what is needed in order to have a successful PXE boot.\nNCNs on install ncn-m001 on reboot or NCN boot 2.1. Verify DHCP packets can be forwarded from the workers to the MTL network (VLAN1) 2.2. Verify BGP 2.3. Verify route to TFTP 2.4. Test TFTP traffic (Aruba Only) 2.5. Check DHCP lease is getting allocated 2.6. Verify the DHCP traffic on the Workers 2.7. Verify the switches are forwarding DHCP traffic. Computes/UANs/Application Nodes 1. NCNs on install Verify the DNSMASQ configuration file matches what is configured on the switches. Here is a DNSMASQ configuration file for the Metal network (VLAN1). As you can see, the router IP address is 10.1.0.1. This has to match what the IP address is on the switches doing the routing for the MTL network. This is most commonly on the spines. This configuration is commonly missed on the CSI input file. MTL DNSMASQ file\n# MTL: server=/mtl/ address=/mtl/ domain=mtl,10.1.1.0,10.1.1.233,local dhcp-option=interface:bond0,option:domain-search,mtl interface=bond interface-name=pit.mtl,bond # This needs to point to the LiveCD IP address for provisioning in bare-metal environments. dhcp-option=interface:bond0,option:dns-server,10.1.1. dhcp-option=interface:bond0,option:ntp-server,10.1.1. # This must point at the router for the network; the L3/IP address for the VLAN. dhcp-option=interface:bond0,option:router,10.1.0. dhcp-range=interface:bond0,10.1.1.33,10.1.1.233,10m Here is an example of what the spine switch configuration should be. Mellanox Configuration\nsw-spine-001 [standalone: master] # show run int vlan 1 interface vlan 1 interface vlan 1 ip address 10.1.0.2/16 primary interface vlan 1 ip dhcp relay instance 2 downstream interface vlan 1 magp 1 interface vlan 1 magp 1 ip virtual-router address 10.1.0. interface vlan 1 magp 1 ip virtual-router mac-address 00:00:5E:00:01: sw-spine-002 [standalone: master] # show run int vlan 1 interface vlan 1 interface vlan 1 ip address 10.1.0.3/16 primary interface vlan 1 ip dhcp relay instance 2 downstream interface vlan 1 magp 1 interface vlan 1 magp 1 ip virtual-router address 10.1.0. interface vlan 1 magp 1 ip virtual-router mac-address 00:00:5E:00:01: Aruba Configuration\nsw-spine-001# show run int vlan 1 interface vlan vsx-sync active-gateways ip address 10.1.0.2/ active-gateway ip mac 12:01:00:00:01: active-gateway ip 10.1.0. ip mtu 9198 ip bootp-gateway 10.1.0. ip helper-address 10.92.100. exit sw-spine-002# show run int vlan 1 interface vlan vsx-sync active-gateways ip address 10.1.0.3/ active-gateway ip mac 12:01:00:00:01: active-gateway ip 10.1.0. ip mtu 9198 ip helper-address 10.92.100. exit You should be able to ping the MTL router from ncn-m001. 2. ncn-m001 on reboot or NCN boot Common Error messages.\n2021-04-19 23:27:09 PXE-E18: Server response timeout. 2021-02-02 17:06:13 PXE-E99: Unexpected network error. Verify the ip helper-address on VLAN 1 on the switches. This is the same configuration as above for the \u0026ldquo;Mellanox Configuration\u0026rdquo; and \u0026ldquo;Aruba Configuration\u0026rdquo;.\n2.1. Verify DHCP packets can be forwarded from the workers to the MTL network (VLAN1) If the Worker nodes cannot reach the metal network DHCP will fail. ALL WORKERS need to be able to reach the MTL network! This can normally be achieved by having a default route TEST ncn-w# ping 10.1.0. PING 10.1.0.1 (10.1.0.1) 56(84) bytes of data. 64 bytes from 10.1.0.1: icmp_seq=1 ttl=64 time=0.361 ms 64 bytes from 10.1.0.1: icmp_seq=2 ttl=64 time=0.145 ms If this fails you may have a misconfigured CAN or need to add a route to the MTL network. ncn-w# ip route add 10.1.0.0/16 via 10.252.0.1 dev vlan 2.2. Verify BGP Verify the BGP neighbors are in the established state on BOTH the switches. Aruba BGP\nsw-spine-002# show bgp ipv4 u s VRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.3 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.2 65533 45052 45044 02m:02w:02d Established Up 10.252.1.7 65533 78389 90090 02m:02w:02d Established Up 10.252.1.8 65533 78384 90059 02m:02w:02d Established Up 10.252.1.9 65533 78389 90108 02m:02w:02d Established Up Mellanox BGP\nsw-spine-001 [standalone: master] # show ip bgp summary VRF name : default BGP router identifier : 10.252.0.2 local AS number : 65533 BGP table version : 39 Main routing table version: 39 IPV4 Prefixes : 18 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.1.7 4 65533 18018 20690 39 0 0 6:05:54:02 ESTABLISHED/6 10.252.1.8 4 65533 18014 20694 39 0 0 6:05:54:03 ESTABLISHED/6 10.252.1.9 4 65533 18010 20671 39 0 0 6:05:52:03 ESTABLISHED/6 2.3. Verify route to TFTP On BOTH Aruba switches we need a single route to the TFTP server 10.92.100.60. This is needed because there are issues with Aruba ECMP hashing and TFTP traffic. sw-spine-002# show ip route 10.92.100.60 Displaying ipv4 routes selected for forwarding \u0026#39;[x/y]\u0026#39; denotes [distance/metric] 10.92.100.60/32, vrf default, tag 0 via 10.252.1.9, [70/0], bgp This route can be a static route or a BGP route that is pinned to a single worker. (1.4.2 patch introduces the BGP pinned route) Verify that you can ping the next hop of this route. For the example above we would ping 10.252.1.9. If this is not reachable this is your problem. 2.4. Test TFTP traffic (Aruba Only) You can test the TFTP traffic by trying to download the ipxe.efi binary. Log into the leaf switch and try to download the iPXE binary. This requires that the leaf switch can talk to the TFTP server \u0026ldquo;10.92.100.60\u0026rdquo; sw-leaf-001# start-shell sw-leaf-001:~$ sudo su sw-leaf-001:/home/admin# tftp 10.92.100. tftp\u0026gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds tftp\u0026gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds tftp\u0026gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds You can see here that the ipxe.efi binary is downloaded three times in a row. When we have seen issues with ECMP hashing this would fail intermittently. 2.5. Check DHCP lease is getting allocated Check the KEA logs and verify that the lease is getting allocated.\nncn# kubectl logs -n services pod/$(kubectl get -n services pods | grep kea | head -n1 | cut -f 1 -d \u0026#39; \u0026#39;) -c cray-dhcp-kea 2021-04-21 00:13:05.416 INFO [kea-dhcp4.leases/24.139710796402304] DHCP4_LEASE_ALLOC [hwtype=1 02:23:28:01:30:10], cid=[00:78:39:30:30:30:63:31:73:30:62:31], tid=0x21f2433a: lease 10.104.0.23 has been allocated for 300 seconds Here we can see that KEA is allocating a lease to 10.104.0.23.\nThe lease MUST say DHCP4_LEASE_ALLOC. If it says DHCP4_LEASE_ADVERT, there is likely a problem. Restarting KEA will fix this issue most of the time.\n2021-06-21 16:44:31.124 INFO [kea-dhcp4.leases/18.139837089017472] DHCP4_LEASE_ADVERT [hwtype=1 14:02:ec:d9:79:88], cid=[no info], tid=0xe87fad10: lease 10.252.1.16 will be advertised 2.6. Verify the DHCP traffic on the Workers We have ran into issues on HPE servers and Aruba switches where the source address of the DHCP Offer is the MetalLB address of KEA 10.92.100.222. The source address of the DHCP Reply/Offer needs to be the address of the VLAN interface on the Worker.\nHere is how to look at DHCP traffic on the workers.\nncn-w# tcpdump -envli bond0 port 67 or 68 You are looking for the source IP address of the DHCP Reply/Offer.\n10.252.1.9.67 \u0026gt; 255.255.255.255.68: BOOTP/DHCP, Reply, length 309, hops 1, xid 0x98b0982e, Flags [Broadcast] Your-IP 10.252.1.17 Server-IP 10.92.100.60 Gateway-IP 10.252.0.1 Client-Ethernet-Address 14:02:ec:d9:79:88 file \u0026#34;ipxe.efi\u0026#34;[|bootp] If the Source IP address of the DHCP Reply/Offer is the MetalLB IP address, the DHCP packet will never make it out of the NCN. An example of this is below.\n10.92.100.222.116 \u0026gt; 255.255.255.255.68: BOOTP/DHCP, Reply, length 309, hops 1, xid 0x260ea655, Flags [Broadcast] Your-IP 10.252.1.14 Server-IP 10.92.100.60 Gateway-IP 10.252.0.4 Client-Ethernet-Address 14:02:ec:d9:79:88 file \u0026#34;ipxe.efi\u0026#34;[|bootp] If you run into this, the only solution that we have found so far is restarting KEA and making sure that it gets moved to a different worker. We believe this has something to do with conntrack.\n2.7. Verify the switches are forwarding DHCP traffic If you still cannot PXE boot, the IP-Helper may be breaking on the switch. On Aruba, Dell, and Mellanox switches we have seen the IP-Helpers get stuck and stop forwarding DHCP traffic to the client. The solutions vary from vendor to vendor. On an Aruba or Mellanox switch, delete the entire VLAN configuration and re-apply it, in order for the DHCP traffic to come back. On a Dell switch, do a reboot in order to restore DHCP traffic. The underlying cause of IP-Helper breaking is not yet known. 3. Compute Nodes/UANs/Application Nodes The following are required for compute node PXE booting. Verify BGP Verify route to TFTP Test TFTP traffic Check DHCP lease is getting allocated Verify the DHCP traffic on the Workers Verify the switches are forwarding DHCP traffic Verify the IP-Helpers on the VLAN the computes nodes are booting over. This is typically VLAN 2 or VLAN 2xxx (MTN Computes). If the compute nodes make it past PXE and go into the PXE shell you can verify DNS and connectivity. iPXE\u0026gt; dhcp Configuring (net0 98:03:9b:a8:60:88).................. No configuration methods succeeded (http://ipxe.org/040ee186) Configuring (net1 b4:2e:99:be:1a:37)...... ok iPXE\u0026gt; show dns net1.dhcp/dns:ipv4 = 10.92.100.225 iPXE\u0026gt; nslookup address api-gw-service-nmn.local iPXE\u0026gt; echo ${address} 10.92.100.71 "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/component_power_state_mismatch/",
	"title": "SAT/HSM/CAPMC Component Power State Mismatch",
	"tags": [],
	"description": "",
	"content": "SAT/HSM/CAPMC Component Power State Mismatch Because of various hardware or communication issues, the node state reported by SAT and HSM (Hardware State Manager) may become out of sync with the actual hardware state reported by CAPMC or Redfish. In most cases this will be noticed when trying to power on or off nodes with BOS/BOA, and will present as SAT or HSM reporting nodes are On while CAPMC reports them as Off (or vice versa).\nPossible Causes Possible reasons the power state got out of sync include but are not limited to:\nA known issue with Gigabyte nodes where Redfish power events can get sent out of order when rebooting nodes. Network issues preventing the flow of Redfish events (telemetry will also be affected). Issues with the cray-hms-hmcollector pod. Fix In most cases, once the underlying cause has been corrected, this should correct itself when the node boots OS, starts heartbeating, and goes to the Ready state. If not, the power state for the affected nodes can be re-synced by kicking off HSM re-discovery of those nodes\u0026rsquo; BMCs.\nncn# cray hsm inventory discover create --xnames \u0026lt;list_of_BMC_xnames\u0026gt; For example:\nncn# cray hsm inventory discover create --xnames x3000c0s0b0,x3000c0s1b0 The power state will be re-synced after all of the BMCs listed have a LastDiscoveryStatus of DiscoverOK.\nncn# cray hsm inventory redfishEndpoints describe x3000c0s0b0 Example output:\n{ \u0026#34;ID\u0026#34;: \u0026#34;x3000c0s0b0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;NodeBMC\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x3000c0s0b0\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;UUID\u0026#34;: \u0026#34;808cde6e-debf-0010-e603-b42e993b708c\u0026#34;, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2021-08-12T16:00:56.937774Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;1.7.0\u0026#34; } } Any power operations done manually with CAPMC that alter the nodes\u0026rsquo; power state may also cause the power state to re-sync.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/ceph_health_states/",
	"title": "Ceph Health States",
	"tags": [],
	"description": "",
	"content": "Ceph Health States Ceph reports several different health states depending on the condition of a cluster. These health states can provide a lot of information about the current functionality of the Ceph cluster, what troubleshooting steps needs to be taken, and if a support ticket needs to be filed.\nThe health of a Ceph cluster can be viewed with the following command:\nncn-m001# ceph -s Example output:\ncluster: id: 5f3b4031-d6c0-4118-94c0-bffd90b534eb health: HEALTH_OK \u0026lt;\u0026lt;-- Health state [...] The following is an overview of potential health states:\nHEALTH_OK\nThe cluster is operating as expected with no issues.\nHEALTH_WARN\nThere is a WARNING condition on the cluster. There are lots of potential causes, but this warning does not mean any functionality is lost. For example, this health state could occur if a pool is at its quota. This health state does not mean that the cluster is not servicing data.\nMost HEALTH_WARN states resolve on their own as they pertain to functionality that tends to self correct.\nHEALTH_ERROR\nThere is an ERROR condition on the cluster. This is typical for a configuration issue or if there is a component that is having issues completing its functions. This does not mean that the cluster is not servicing data. The HEALTH_ERROR state is primarily for individual components experiencing issues.\nMost HEALTH_ERROR states may not be covered by troubleshooting documentation and should result in a ticket to customer support for guidance.\nHEALTH_CRITICAL\nThere is a CRITICAL condition on the cluster. This means that cluster functions have stopped or have gone into read-only mode to protect data. When this is present, the cluster is not servicing data properly, or even at all in order to protect data integrity. This will be dependent on the configuration and the reason behind the CRITICAL health state.\nAll HEALTH_CRITICAL states should result in an immediate ticket to customer support for guidance on returning the cluster back to service.\nFor a list of possible states, refer to https://docs.ceph.com/docs/master/rados/operations/health-checks/.\n"
},
{
	"uri": "/docs-csm/en-12/operations/system_layout_service/restore_sls_postgres_database_from_backup/",
	"title": "Restore SLS Postgres Database from Backup",
	"tags": [],
	"description": "",
	"content": "Restore SLS Postgres Database from Backup This procedure can be used to restore the SLS Postgres database from a previously taken backup. This can be a manual backup created by the Create a Backup of the SLS Postgres Database procedure, or an automatic backup created by the cray-sls-postgresql-db-backup Kubernetes cronjob.\nPrerequisites Healthy Postgres Cluster.\nUse patronictl list on the SLS Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:\nncn# kubectl exec cray-sls-postgres-0 -n services -c postgres -it -- patronictl list + Cluster: cray-sls-postgres (6975238790569058381) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +---------------------+------------+--------+---------+----+-----------+ | cray-sls-postgres-0 | 10.44.0.40 | Leader | running | 1 | | | cray-sls-postgres-1 | 10.36.0.37 | | running | 1 | 0 | | cray-sls-postgres-2 | 10.42.0.42 | | running | 1 | 0 | +---------------------+------------+--------+---------+----+-----------+ Previously taken backup of the SLS Postgres cluster either a manual or automatic backup.\nCheck for any available automatic SLS Postgres backups:\nncn# cray artifacts list postgres-backup --format json | jq -r \u0026#39;.artifacts[].Key | select(contains(\u0026#34;sls\u0026#34;))\u0026#39; cray-sls-postgres-2021-07-11T23:10:08.manifest cray-sls-postgres-2021-07-11T23:10:08.psql Procedure Retrieve a previously taken SLS Postgres backup. This can be either a previously taken manual SLS backup or an automatic Postgres backup in the postgres-backup S3 bucket.\nFrom a previous manual backup:\nCopy over the folder or tarball containing the Postgres back up to be restored. If it is a tarball extract it.\nSet the environment variable POSTGRES_SQL_FILE to point toward the .psql file in the backup folder:\nncn# export POSTGRES_SQL_FILE=/root/cray-sls-postgres-backup_2021-07-07_16-39-44/cray-sls-postgres-backup_2021-07-07_16-39-44.psql Set the environment variable POSTGRES_SECRET_MANIFEST to point toward the .manifest file in the backup folder:\nncn# export POSTGRES_SECRET_MANIFEST=/root/cray-sls-postgres-backup_2021-07-07_16-39-44/cray-sls-postgres-backup_2021-07-07_16-39-44.manifest From a previous automatic Postgres backup:\nCheck for available backups:\nncn# cray artifacts list postgres-backup --format json | jq -r \u0026#39;.artifacts[].Key | select(contains(\u0026#34;sls\u0026#34;))\u0026#39; cray-sls-postgres-2021-07-11T23:10:08.manifest cray-sls-postgres-2021-07-11T23:10:08.psql Then set the following environment variables for the name of the files in the backup:\nncn# export POSTGRES_SECRET_MANIFEST_NAME=cray-sls-postgres-2021-07-11T23:10:08.manifest ncn# export POSTGRES_SQL_FILE_NAME=cray-sls-postgres-2021-07-11T23:10:08.psql Download the .psql file for the postgres backup:\nncn# cray artifacts get postgres-backup \u0026#34;$POSTGRES_SQL_FILE_NAME\u0026#34; \u0026#34;$POSTGRES_SQL_FILE_NAME\u0026#34; Download the .manifest file for the SLS backup:\nncn# cray artifacts get postgres-backup \u0026#34;$POSTGRES_SECRET_MANIFEST_NAME\u0026#34; \u0026#34;$POSTGRES_SECRET_MANIFEST_NAME\u0026#34; Setup environment variables pointing to the full path of the .psql and .manifest files:\nncn# export POSTGRES_SQL_FILE=$(realpath \u0026#34;$POSTGRES_SQL_FILE_NAME\u0026#34;) ncn# export POSTGRES_SECRET_MANIFEST=$(realpath \u0026#34;$POSTGRES_SECRET_MANIFEST_NAME\u0026#34;) Verify the POSTGRES_SQL_FILE and POSTGRES_SECRET_MANIFEST environment variables are set correctly:\nncn# echo \u0026#34;$POSTGRES_SQL_FILE\u0026#34; /root/cray-sls-postgres-backup_2021-07-07_16-39-44/cray-sls-postgres-backup_2021-07-07_16-39-44.psql ncn# echo \u0026#34;$POSTGRES_SECRET_MANIFEST\u0026#34; /root/cray-sls-postgres-backup_2021-07-07_16-39-44/cray-sls-postgres-backup_2021-07-07_16-39-44.manifest Re-run the SLS loader job:\nncn# kubectl -n services get job cray-sls-init-load -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | kubectl replace --force -f - Wait for the job to complete:\nncn# kubectl wait -n services job cray-sls-init-load --for=condition=complete --timeout=5m Determine leader of the Postgres cluster:\nncn# export POSTGRES_LEADER=$(kubectl exec cray-sls-postgres-0 -n services -c postgres -t -- patronictl list -f json | jq -r \u0026#39;.[] | select(.Role == \u0026#34;Leader\u0026#34;).Member\u0026#39;) Check the environment variable to see the current leader of the Postgres cluster:\nncn# echo $POSTGRES_LEADER cray-sls-postgres-0 Determine the database schema version of the currently running SLS database and verify that it matches the database schema version from the Postgres backup:\nDatabase schema of the currently running SLS Postgres instance.\nncn# kubectl exec $POSTGRES_LEADER -n services -c postgres -it -- bash -c \u0026#34;psql -U slsuser -d sls -c \u0026#39;SELECT * FROM schema_migrations\u0026#39;\u0026#34; Example output:\nversion | dirty ---------+------- 3 | f (1 row) The output above shows the database schema is at version 3.\nDatabase schema version from the Postgres backup:\nncn# cat \u0026#34;$POSTGRES_SQL_FILE\u0026#34; | grep \u0026#34;COPY public.schema_migrations\u0026#34; -A 2 Example output:\nCOPY public.schema_migrations (version, dirty) FROM stdin; 3 f \\. The output above shows the database schema is at version 3.\nIf the database schema versions match, proceed to the next step. Otherwise, the Postgres backup taken is not applicable to the currently running instance of SLS.\nWARNING: If the database schema versions do not match the version of the SLS deployed will need to be either upgraded/downgraded to a version with a compatible database schema version. Ideally to the same version of SLS that was used to create the Postgres backup.\nRestore the database from the backup using the restore_sls_postgres_from_backup.sh script. This script requires the POSTGRES_SQL_FILE and POSTGRES_SECRET_MANIFEST environment variables to be set.\nTHIS WILL DELETE AND REPLACE THE CURRENT CONTENTS OF THE SLS DATABASE\nncn# /usr/share/doc/csm/scripts/operations/system_layout_service/restore_sls_postgres_from_backup.sh Verify the health of the SLS Postgres cluster by running the following scripts. If there are issues with run_hms_ct_tests.sh, then follow Interpreting HMS Health Check Results. If there are issues with verify_hsm_discovery.py, then follow the \u0026ldquo;Interpreting HSM discovery results\u0026rdquo; section of the Validate CSM Health document.\nncn# /opt/cray/tests/ncn-smoke/hms/hms-sls/sls_smoke_test_ncn-smoke.sh ncn# /opt/cray/platform-utils/ncnPostgresHealthChecks.sh ncn# /opt/cray/csm/scripts/hms_verification/verify_hsm_discovery.py Verify that the service is functional:\nncn# cray sls version list Example output:\nCounter = 5 LastUpdated = \u0026#34;2021-04-05T22:51:36.575276Z\u0026#34; Get the number of hardware objects stored in SLS:\nncn# cray sls hardware list --format json | jq .[].Xname | wc -l Get the name of networks stored in SLS:\nIf the system does not have liquid cooled hardware, the HMN_MTN and NMN_MTN networks may not be present.\nncn# cray sls networks list --format json | jq -r .[].Name Example output:\nHMN_MTN HMN_RVR NMNLB NMN NMN_MTN NMN_RVR CAN HMN HMNLB HSN MTL "
},
{
	"uri": "/docs-csm/en-12/operations/system_management_health/system_management_health_checks_and_alerts/",
	"title": "System Management Health Checks and Alerts",
	"tags": [],
	"description": "",
	"content": "System Management Health Checks and Alerts A health check corresponds to a Prometheus query against metrics aggregated to the Prometheus instance. Core platform components like Kubernetes and Istio collect service-related metrics by default, which enables the System Management Health service to implement generic service health checks without custom instrumentation. Health checks are intended to be coarse-grained and comprehensive, as opposed to fine-grained and exhaustive. Health checks related to infrastructure adhere to the Utilization Saturation Errors (USE) method whereas services follow the Rate Errors Duration (RED) method.\nPrometheus alerting rules periodically evaluate health checks and trigger alerts to Alertmanager, which manages silencing, inhibition, aggregation, and sending out notifications. Alertmanager supports a number of notification options, but the most relevant ones are listed below:\nEmail - Sends notification emails periodically regarding alerts Slack - Publishes notifications to a Slack channel Web hook- Send an HTTP request to a configurable URL (requires custom integration) Similar to Prometheus metrics, alerts use labels to identify a particular dimensional instantiation, and the Alertmanager dashboard enables operators to preemptively silence alerts based on them.\nCheck Active Alerts from NCNs Prometheus includes the /api/v1/alerts endpoint, which returns a JSON object containing the active alerts. From a non-compute node (NCN), can connect to sysmgmt-health/cray-sysmgmt-health-promet-prometheus directly and bypass service authentication and authorization.\nObtain the cluster IP address:\nncn-w001# kubectl -n sysmgmt-health get svc cray-sysmgmt-health-promet-prometheus Example output:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cray-sysmgmt-health-promet-prometheus ClusterIP 10.16.201.80 \u0026lt;none\u0026gt; 9090/TCP 2d6h Get active alerts, which includes KubeletTooManyPods if it is going off:\nncn-w001# curl -s http://CLUSTER-IP:PORT/api/v1/alerts | jq . | grep -B 10 -A 20 KubeletTooManyPods Example output:\n{ \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: { \u0026#34;alerts\u0026#34;: [ { \u0026#34;labels\u0026#34;: { \u0026#34;alertname\u0026#34;: \u0026#34;KubeletTooManyPods\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;https-metrics\u0026#34;, \u0026#34;instance\u0026#34;: \u0026#34;10.252.1.6:10250\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;kubelet\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;kube-system\u0026#34;, \u0026#34;node\u0026#34;: \u0026#34;ncn-w003\u0026#34;, \u0026#34;prometheus\u0026#34;: \u0026#34;kube-monitoring/cray-prometheus-operator-prometheus\u0026#34;, \u0026#34;prometheus_replica\u0026#34;: \u0026#34;prometheus-cray-prometheus-operator-prometheus-0\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;cray-prometheus-operator-kubelet\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;warning\u0026#34; }, \u0026#34;annotations\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;Kubelet 10.252.1.6:10250 is running 107 Pods, close to the limit of 110.\u0026#34;, \u0026#34;runbook_url\u0026#34;: \u0026#34;https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods\u0026#34; }, \u0026#34;state\u0026#34;: \u0026#34;firing\u0026#34;, \u0026#34;activeAt\u0026#34;: \u0026#34;2020-01-11T18:13:35.086499854Z\u0026#34;, \u0026#34;value\u0026#34;: 107 }, { \u0026#34;labels\u0026#34;: { In the example above, the alert actually indicates it is getting close to the limit, but the value included in the alert is the actual number of pods on ncn-w003.\nTroubleshooting: If an alert titled KubeCronJobRunning is encountered, this could be an indication that a Kubernetes cronjob is misbehaving. The Labels section under the firing alert will indicate the name of the cronjob that is taking longer than expected to complete. Refer to the \u0026ldquo;CHECK CRON JOBS\u0026rdquo; header in the Power On and Start the Management Kubernetes Cluster procedure for instructions on how to troubleshoot the cronjob, as well as how to restart (export and reapply) the cronjob.\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/backup_and_restore_vault_clusters/",
	"title": "Backup and Restore Vault Clusters",
	"tags": [],
	"description": "",
	"content": "Backup and Restore Vault Clusters View the existing Vault backups on the system and use a completed backup to perform a restore operation.\nVelero is used to perform a nightly backup of Vault. The backup includes Kubernetes object state, in addition to pod volume data for the Vault statefulset. For more information on Velero, refer to the external Velero documentation.\nCAUTION: A restore operation should only be performed in extreme situations. Performing a restore from a backup may cause secrets stored in Vault to change to an earlier state or get out of sync.\nPrerequisites View backup schedules and completed backups Restore from a backup Prerequisites All of the steps listed in this section should be performed from a Kubernetes master or worker node. Ceph must be healthy to maximize the chance of a successful restore. View backup schedules and completed backups View the backup schedules.\nncn-mw# velero get schedule Example output:\nNAME STATUS CREATED SCHEDULE BACKUP TTL LAST BACKUP SELECTOR vault-daily-backup Enabled 2021-01-26 14:14:04 +0000 UTC 0 2 * * * 0s 19h ago vault_cr=cray-vault View the completed backups.\nncn-mw# velero get backup Example output:\nNAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR vault-daily-backup-20210217020038 Completed 0 0 2021-02-17 02:00:38 +0000 UTC 29d default vault_cr=cray-vault vault-daily-backup-20210216020035 Completed 0 0 2021-02-16 02:00:35 +0000 UTC 28d default vault_cr=cray-vault vault-daily-backup-20210215020035 Completed 0 0 2021-02-15 02:00:35 +0000 UTC 27d default vault_cr=cray-vault View the details of a completed backup.\nReplace the BACKUP_NAME value with the name of a backup returned in the previous step.\nncn-mw# velero describe backup BACKUP_NAME --details Example output:\nName: vault-daily-backup-20210217020038 Namespace: velero Labels: app.kubernetes.io/managed-by=Helm velero.io/schedule-name=vault-daily-backup velero.io/storage-location=default Annotations: velero.io/source-cluster-k8s-gitversion=v1.18.6 velero.io/source-cluster-k8s-major-version=1 velero.io/source-cluster-k8s-minor-version=18 Phase: Completed Errors: 0 Warnings: 0 Namespaces: Included: vault Excluded: \u0026lt;none\u0026gt; Resources: Included: pv, pvc, secret, sealedsecret, vault, configmap, deployment, service, statefulset, pod, ingress, replicaset Excluded: \u0026lt;none\u0026gt; Cluster-scoped: included Label selector: vault_cr=cray-vault Storage Location: default Velero-Native Snapshot PVs: auto TTL: 720h0m0s Hooks: \u0026lt;none\u0026gt; Backup Format Version: 1.1.0 Started: 2021-02-17 02:00:38 +0000 UTC Completed: 2021-02-17 02:00:52 +0000 UTC Expiration: 2021-03-19 02:00:38 +0000 UTC Total items to be backed up: 21 Items backed up: 21 Resource List: apps/v1/Deployment: - vault/cray-vault-configurer apps/v1/ReplicaSet: - vault/cray-vault-configurer-56df7f768d apps/v1/StatefulSet: - vault/cray-vault v1/ConfigMap: - vault/cray-vault-configurer - vault/cray-vault-statsd-mapping v1/PersistentVolume: - pvc-0ea5065b-d5e1-45f9-8b54-b8f56281b81b - pvc-34d11110-1ff3-4267-8e66-696045f35af4 - pvc-e3d07b75-1b27-4a55-b8d5-8e57857ad619 v1/PersistentVolumeClaim: - vault/vault-raft-cray-vault-0 - vault/vault-raft-cray-vault-1 - vault/vault-raft-cray-vault-2 v1/Pod: - vault/cray-vault-0 - vault/cray-vault-1 - vault/cray-vault-2 - vault/cray-vault-configurer-56df7f768d-z2wzn v1/Secret: - vault/cray-vault-unseal-keys v1/Service: - vault/cray-vault - vault/cray-vault-0 - vault/cray-vault-1 - vault/cray-vault-2 - vault/cray-vault-configurer Velero-Native Snapshots: \u0026lt;none included\u0026gt; Restic Backups: Completed: vault/cray-vault-0: vault-raft vault/cray-vault-1: vault-raft vault/cray-vault-2: vault-raft Restore from a backup Verify the backup being restored contains a manifest of resources and Restic volume backups.\nObject names will vary.\nncn-mw# velero describe backup BACKUP_NAME --details Example output:\nName: vault-daily-backup-20210217020038 Namespace: velero Labels: app.kubernetes.io/managed-by=Helm velero.io/schedule-name=vault-daily-backup velero.io/storage-location=default Annotations: velero.io/source-cluster-k8s-gitversion=v1.18.6 velero.io/source-cluster-k8s-major-version=1 velero.io/source-cluster-k8s-minor-version=18 Phase: Completed Errors: 0 Warnings: 0 Namespaces: Included: vault Excluded: \u0026lt;none\u0026gt; Resources: Included: pv, pvc, secret, sealedsecret, vault, configmap, deployment, service, statefulset, pod, ingress, replicaset Excluded: \u0026lt;none\u0026gt; Cluster-scoped: included Label selector: vault_cr=cray-vault Storage Location: default Velero-Native Snapshot PVs: auto TTL: 720h0m0s Hooks: \u0026lt;none\u0026gt; Backup Format Version: 1.1.0 Started: 2021-02-17 02:00:38 +0000 UTC Completed: 2021-02-17 02:00:52 +0000 UTC Expiration: 2021-03-19 02:00:38 +0000 UTC Total items to be backed up: 21 Items backed up: 21 Resource List: apps/v1/Deployment: - vault/cray-vault-configurer apps/v1/ReplicaSet: - vault/cray-vault-configurer-56df7f768d apps/v1/StatefulSet: - vault/cray-vault v1/ConfigMap: - vault/cray-vault-configurer - vault/cray-vault-statsd-mapping v1/PersistentVolume: - pvc-0ea5065b-d5e1-45f9-8b54-b8f56281b81b - pvc-34d11110-1ff3-4267-8e66-696045f35af4 - pvc-e3d07b75-1b27-4a55-b8d5-8e57857ad619 v1/PersistentVolumeClaim: - vault/vault-raft-cray-vault-0 - vault/vault-raft-cray-vault-1 - vault/vault-raft-cray-vault-2 v1/Pod: - vault/cray-vault-0 - vault/cray-vault-1 - vault/cray-vault-2 - vault/cray-vault-configurer-56df7f768d-z2wzn v1/Secret: - vault/cray-vault-unseal-keys v1/Service: - vault/cray-vault - vault/cray-vault-0 - vault/cray-vault-1 - vault/cray-vault-2 - vault/cray-vault-configurer Velero-Native Snapshots: \u0026lt;none included\u0026gt; Restic Backups: Completed: vault/cray-vault-0: vault-raft vault/cray-vault-1: vault-raft vault/cray-vault-2: vault-raft Scale down the Vault operator.\nThis will prevent it from attempting to reconcile the instance while the restore is in progress.\nSubmit the request to scale down the Vault operator.\nncn-mw# kubectl -n vault scale deployment cray-vault-operator --replicas=0 Example output:\ndeployment.apps/cray-vault-operator scaled Verify that the changes were successfully made.\nncn-mw# kubectl -n vault get deployment Example output:\nNAME READY UP-TO-DATE AVAILABLE AGE cray-vault-operator 0/0 0 0 19h Delete the Vault instance.\nThis is done to minimize the risk of Vault being in a partially restored state.\nVault will be inaccessible (if not already) after running the following commands.\nncn-mw# kubectl -n vault delete vault -l vault_cr=cray-vault ncn-mw# kubectl -n vault delete pvc -l vault_cr=cray-vault ncn-mw# kubectl -n vault delete secret -l vault_cr=cray-vault Submit the restore action.\nMonitor the progress of the restore job until it is in a completed phase. The progress can be viewed by using the logs command shown in the output.\nncn-mw# velero restore create --from-backup BACKUP_NAME Example output:\nRestore request \u0026#34;vault-daily-backup-20210217100000\u0026#34; submitted successfully. Run `velero restore describe vault-daily-backup-20210217100000` or `velero restore logs vault-daily-backup-20210217100000` for more details. Scale the Vault operator back to one replica.\nSubmit the request to scale the Vault operator.\nncn-mw# kubectl -n vault scale deployment cray-vault-operator --replicas=1 Example output:\ndeployment.apps/cray-vault-operator scaled Verify that the changes were successfully made.\nncn-mw# kubectl -n vault get deployment Example output:\nNAME READY UP-TO-DATE AVAILABLE AGE cray-vault-operator 1/1 1 1 19h If necessary, delete the Vault pods and allow the operator to restart them.\nThe pods need to be manually restarted if the Vault statefulset pods are in CrashLoopBackOff after 5-10 minutes of performing the restore operation. The Vault statefulset pods normally go through a number of restarts on a clean start-up.\nCheck that the pods are in a CrashLoopBackOff state.\nncn-mw# kubectl -n vault get pod -o wide -l vault_cr=cray-vault Example output:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cray-vault-0 4/5 CrashLoopBackOff 9 30m 10.44.0.33 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-vault-1 4/5 CrashLoopBackOff 9 30m 10.42.0.10 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-vault-2 4/5 CrashLoopBackOff 9 30m 10.40.0.12 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-vault-configurer-56df7f768d-c228k 2/2 Running 0 30m 10.44.0.8 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Delete the pods to restart them.\nncn-mw# kubectl delete pod -n vault -l vault_cr=cray-vault Example output:\npod \u0026#34;cray-vault-0\u0026#34; deleted pod \u0026#34;cray-vault-1\u0026#34; deleted pod \u0026#34;cray-vault-2\u0026#34; deleted pod \u0026#34;cray-vault-configurer-56df7f768d-c228k\u0026#34; deleted Verify that the pods are in a Running state.\nncn-mw# kubectl get pod -n vault -l vault_cr=cray-vault Example output:\nNAME READY STATUS RESTARTS AGE cray-vault-0 5/5 Running 2 105s cray-vault-1 5/5 Running 2 67s cray-vault-2 5/5 Running 2 38s cray-vault-configurer-56df7f768d-c7mk2 2/2 Running 0 2m21s "
},
{
	"uri": "/docs-csm/en-12/operations/package_repository_management/package_repository_management_with_nexus/",
	"title": "Package Repository Management with Nexus",
	"tags": [],
	"description": "",
	"content": "Package Repository Management with Nexus Overview of RPM repositories and container registry in Nexus.\nRPM repositories Container registry Adding images Registry mirror configuration Pull example using CRI Pull example using containerd Pull example using Podman RPM repositories Repositories are available at https://packages.local/repository/REPO_NAME. For example, to configure the csm-sle-15sp2 repository on a non-compute node (NCN):\nncn# zypper addrepo -fG https://packages.local/repository/csm-sle-15sp2 csm-sle-15sp2 Example output:\nAdding repository \u0026#39;csm-sle-15sp2\u0026#39; .................................................................................................[done] Warning: GPG checking is disabled in configuration of repository \u0026#39;csm-sle-15sp2\u0026#39;. Integrity and origin of packages cannot be verified. Repository \u0026#39;csm-sle-15sp2\u0026#39; successfully added URI : https://packages.local/repository/csm-sle-15sp2 Enabled : Yes GPG Check : No Autorefresh : Yes Priority : 99 (default priority) Repository priorities are without effect. All enabled repositories share the same priority. ncn# zypper ref csm-sle-15sp2 Retrieving repository \u0026#39;csm-sle-15sp2\u0026#39; metadata ....................................................................................[done] Building repository \u0026#39;csm-sle-15sp2\u0026#39; cache .........................................................................................[done] Specified repositories have been refreshed. The -G option is used in this example to disable GPG checks. However, if the named repository is properly signed, it is not recommended to use the -G option.\nContainer registry The container registry is available at https://registry.local on the NCNs or compute nodes. By default, access to the container registry is not available over the Customer Access Network (CAN). If desired, a corresponding route may be added to the nexus VirtualService resource in the nexus namespace:\nWARNING: If access to the container registry in Nexus is exposed over CAN, it is strongly recommended to setup and configure fine-grained access control. However, the default setup assumes the OPA policy only permits administrative users access.\nncn-mw# kubectl -n nexus get vs nexus Example output:\nNAME GATEWAYS HOSTS AGE nexus [services/services-gateway] [packages.local registry.local nexus.odin.dev.cray.com] 21d Adding images The only way to add images to the container registry is with the Docker API. Use a client (such as Skopeo, Podman, or Docker) to push images. By default, product installers use Podman with a vendor version of the Skopeo image to sync container images included in a release distribution to registry.local.\nThe Cray System Management (CSM) product adds a recent version of quay.io/skopeo/stable to the container registry, and it may be used to copy images into registry.local.\nFor example, to update the version of quay.io/skopeo/stable:\nncn-mw# podman run --rm registry.local/skopeo/stable copy --dest-tls-verify=false docker://quay.io/skopeo/stable docker://registry.local/skopeo/stable Example output:\nGetting image source signatures Copying blob sha256:85a74b04b5b84b45c763e9763cc0f62269390bb30058d3e2b2545d820d3558f7 Copying blob sha256:ab9d1e8c4764f52ed5041c38bd3d64b6ae9c27d0f436be50f658ece38440a97b Copying blob sha256:e5c8e56645c4d70308640ede3f72f76386b466cf5d97010b9c2f31054caf30a5 Copying blob sha256:bcf471c5e964dc3ce3e7249bd2b1493acf3dd103a28af0cfe5af70351ad399d0 Copying blob sha256:d62975d5ffa72581b912ee3e1a850e2ac14435a4238253a8ebf80f5d10f2df4c Copying blob sha256:8c87d899c1ab2cc2d25708ba0ff9a1726fe6b57bf415c8fdc7de973e6b185f63 Copying config sha256:49f2b6d9790b48aadb2ac29f5bfef56ebb2fccec6319b3981639d04452887848 Writing manifest to image destination Storing signatures Registry mirror configuration Kubernetes pods are expected to rely on the registry mirror configuration in /etc/containerd/config.toml to automatically fetch container images from it using upstream references. By default, the following upstream registries are automatically redirected to registry.local:\ndtr.dev.cray.com docker.io (and registry-1.docker.io) quay.io gcr.io k8s.gcr.io WARNING: The registry mirror configuration in /etc/containerd/config.toml only applies to the CRI. When using the ctr command or another container runtime (For example, podman or docker), the administrator must explicitly reference registry.local.\nPull example using CRI The following is an example of pulling dtr.dev.cray.com/baseos/alpine:3.12.0 using CRI:\nncn-mw# crictl pull dtr.dev.cray.com/baseos/alpine:3.12.0 Example output:\nImage is up to date for sha256:5779738096ecb47dd7192d44ceef7032110edd38204f66c9ca4e35fca952975c Pull example using containerd Using containerd or Podman requires changing dtr.dev.cray.com to registry.local in order to guarantee that the runtime fetches the image from the container registry in Nexus.\nThe following is an example for containerd:\nncn-mw# ctr image pull registry.local/baseos/alpine:3.12.0 Example output:\nregistry.local/baseos/alpine:3.12.0: resolved |++++++++++++++++++++++++++++++++++++++| manifest-sha256:e25f4e287fad9c0ee0a47af590e999f9ff1f043fb636a9dc7a61af6d13fc40ca: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:3ab6766f6281be4c2349e2122bab3b4d1ba1b524236b85fce0784453e759b516: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:df20fa9351a15782c64e6dddb2d4a6f50bf6d3688060a34c4014b0d9a752eb4c: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:62694d7552ccd2338f8a4d775bef09ea56f6d2bcfdfafb9e2a4e0241f360fca5: done |++++++++++++++++++++++++++++++++++++++| config-sha256:5779738096ecb47dd7192d44ceef7032110edd38204f66c9ca4e35fca952975c: done |++++++++++++++++++++++++++++++++++++++| elapsed: 0.2 s total: 0.0 B (0.0 B/s) unpacking linux/amd64 sha256:e25f4e287fad9c0ee0a47af590e999f9ff1f043fb636a9dc7a61af6d13fc40ca... done Pull example using Podman The following is an example for Podman:\nncn-mw# podman pull registry.local/baseos/alpine:3.12.0 Example output:\nTrying to pull registry.local/baseos/alpine:3.12.0... Getting image source signatures Copying blob df20fa9351a1 [--------------------------------------] 0.0b / 0.0b Copying blob 3ab6766f6281 [--------------------------------------] 0.0b / 0.0b Copying blob 62694d7552cc [--------------------------------------] 0.0b / 0.0b Copying config 5779738096 done Writing manifest to image destination Storing signatures 5779738096ecb47dd7192d44ceef7032110edd38204f66c9ca4e35fca952975c "
},
{
	"uri": "/docs-csm/en-12/operations/power_management/power_on_and_boot_compute_nodes_and_user_access_nodes/",
	"title": "Power On and Boot Compute and User Access Nodes",
	"tags": [],
	"description": "",
	"content": "Power On and Boot Compute and User Access Nodes Use Boot Orchestration Service (BOS) and choose the appropriate session template to power on and boot compute and UANs.\nThis procedure boots all compute nodes and user access nodes (UANs) in the context of a full system power-up.\nPrerequisites All compute cabinet PDUs, servers, and switches must be powered on. The Slingshot Fabric is up and configured. Refer to the following documentation for more information on how to bring up the Slingshot Fabric: The HPE Slingshot Operations Guide PDF for HPE Cray EX systems. The HPE Slingshot Troubleshooting PDF. An authentication token is required to access the API gateway and to use the sat command. See the \u0026ldquo;SAT Authentication\u0026rdquo; section of the HPE Cray EX System Admin Toolkit (SAT) product stream documentation (S-8031) for instructions on how to acquire a SAT authentication token. Procedure List detailed information about the available boot orchestration service (BOS) session template names.\nIdentify the BOS session template names (such as \u0026quot;cos-2.0.x\u0026quot;, slurm, or uan-slurm), and choose the appropriate compute and UAN node templates for the power on and boot.\nncn-mw# cray bos v1 sessiontemplate list --format toml Example output excerpts:\n[[results]] name = \u0026#34;cos-2.0.x\u0026#34; description = \u0026#34;BOS session template for booting compute nodes, generated by the installation\u0026#34; . . . name = \u0026#34;slurm\u0026#34; description = \u0026#34;BOS session template for booting compute nodes, generated by the installation\u0026#34; . . . name = \u0026#34;uan-slurm\u0026#34; description = \u0026#34;Template for booting UANs with Slurm\u0026#34; To display more information about a session template, for example cos-2.0.0, use the describe option.\nncn-mw# cray bos sessiontemplate describe cos-2.0.x Use sat bootsys boot to power on and boot UANs and compute nodes.\nAttention: Specify the required session template name for COS_SESSION_TEMPLATE and UAN_SESSION_TEMPLATE in the following command line.\nUse --loglevel debug command line option to provide more information as the system boots.\nncn-mw# sat bootsys boot --stage bos-operations \\ --bos-templates COS_SESSION_TEMPLATE,UAN_SESSION_TEMPLATE Example output:\nStarted boot operation on BOS session templates: cos-2.0.x, uan. Waiting up to 900 seconds for sessions to complete. Waiting for BOA k8s job with id boa-a1a697fc-e040-4707-8a44-a6aef9e4d6ea to complete. Session template: uan. To monitor the progress of this job, run the following command in a separate window: \u0026#39;kubectl -n services logs -c boa -f --selector job-name=boa-a1a697fc-e040-4707-8a44-a6aef9e4d6ea\u0026#39; Waiting for BOA k8s job with id boa-79584ffe-104c-4766-b584-06c5a3a60996 to complete. Session template: cos-2.0.0. To monitor the progress of this job, run the following command in a separate window: \u0026#39;kubectl -n services logs -c boa -f --selector job-name=boa-79584ffe-104c-4766-b584-06c5a3a60996\u0026#39; [...] All BOS sessions completed. Note the returned job ID for each session; for example: \u0026quot;boa-caa15959-2402-4190-9243-150d568942f6\u0026quot;\nUse the job ID strings to monitor the progress of the boot job.\nTip: The commands needed to monitor the progress of the job are provided in the output of the sat bootsys boot command.\nncn-mw# kubectl -n services logs -c boa -f --selector job-name=boa-caa15959-2402-4190-9243-150d568942f6 In another shell window, use a similar command to monitor the UAN session.\nncn-mw# kubectl -n services logs -c boa -f --selector job-name=boa-a1a697fc-e040-4707-8a44-a6aef9e4d6ea Wait for compute nodes and UANs to boot and check the Configuration Framework Service (CFS) log for errors.\nVerify that nodes have booted and indicate Ready.\nncn-mw# sat status Example output:\n+----------------+------+----------+-------+------+---------+------+----------+-------------+----------+ | xname | Type | NID | State | Flag | Enabled | Arch | Class | Role | Net Type | +----------------+------+----------+-------+------+---------+------+----------+-------------+----------+ | x1000c0s0b0n0 | Node | 1001 | Ready | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s0b0n1 | Node | 1002 | Ready | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s0b1n0 | Node | 1003 | Ready | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s0b1n1 | Node | 1004 | Ready | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s1b0n0 | Node | 1005 | Ready | OK | True | X86 | Mountain | Compute | Sling | [...] Important: For Gigabyte nodes only: If the console for any application nodes shows that the node fails to PXE boot over the expected interface, it may be necessary to reapply BIOS settings for the node. Gigabyte BIOS C27 and earlier may have this issue. To ensure that the first LAN port is configured in BIOS for PXE boot, see \u0026ldquo;Configure the BIOS of a Gigabyte UAN\u0026rdquo; in the HPE Cray User Access Node (UAN) Installation Guide (S-8032).\nMake nodes available to customers and refer to Validate CSM Health to check system health and status.\nNext step Return to System Power On Procedures and continue with next step.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/rebuild_ncns/rebuild_ncns/",
	"title": "Rebuild NCNs",
	"tags": [],
	"description": "",
	"content": "Rebuild NCNs Rebuild a master, worker, or storage non-compute node (NCN). Use this procedure in the event that a node has a hardware failure, or some other issue with the node has occurred that warrants rebuilding the node.\nPrerequisites Procedure Validation Prerequisites The system is fully installed and has transitioned off of the LiveCD.\nVariables set with the name of the node being rebuilt and its component name (xname) are required.\nSet NODE to the hostname of the node being rebuilt (e.g. ncn-w001, ncn-w002, etc). Set XNAME to the component name (xname) of that node. ncn# NODE=ncn-w00n ncn# XNAME=$(ssh $NODE cat /etc/cray/xname) ncn# echo $XNAME Procedure Only follow the steps in the section for the node type that is being rebuilt.\nNOTE After rebuilding an NCN, kernel dump will need to be fixed. See Kernel Dump Hotfix for more information.\nWorker node Master node Storage node Worker node Make sure that not all pods of ingressgateway-hmn or spire-server are running on the same worker node.\nFor either of those two deployments, if all pods are running on a single worker node, then use the /opt/cray/platform-utils/move_pod.sh script to move at least one pod to a different worker node.\nRebuild the node.\nncn-m001# /usr/share/doc/csm/upgrade/1.2/scripts/rebuild/ncn-rebuild-worker-nodes.sh ncn-w001 Master node Run ncn-rebuild-master-nodes.sh from a master node that is not the one being rebuilt.\nncn-m# /usr/share/doc/csm/upgrade/1.2/scripts/rebuild/ncn-rebuild-master-nodes.sh ncn-m002 Storage node See Prepare Storage Nodes.\nValidation After completing all of the steps, run the Final Validation steps.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_remove_replace_ncns/redeploy_services/",
	"title": "Redeploy Services Impacted by Adding or Permanently Removing Storage Nodes",
	"tags": [],
	"description": "",
	"content": "Redeploy Services Impacted by Adding or Permanently Removing Storage Nodes This procedure redeploys S3 and sysmgmt-health services to add or remove storage node endpoints.\nThis procedure can be skipped if a worker or master node has been added. In that case, proceed to the next step to Validate NCN or return to the main Add, Remove, Replace, or Move NCNs page.\nThis procedure can be skipped if a worker or master node have been removed. In that case, proceed to the next step to Validate Health or return to the main Add, Remove, Replace, or Move NCNs page.\nOtherwise, if a storage node has been added or removed, proceed with the following steps.\nPrerequisite The docs-csm RPM has been installed on the NCN. Verify that the following file exists:\nncn-mw# ls /usr/share/docs/csm/scripts/operations/node_management/Add_Remove_Replace_NCNs/update_customizations.sh Update the nmn_ncn_storage list Update the nmn_ncn_storage list to include the IP addresses for any added or removed storage nodes.\nAcquire site-init Before redeploying the desired charts, update the customizations.yaml file in the site-init secret in the loftsman namespace.\nIf the site-init repository is available as a remote repository as described here, then clone it to ncn-m001. Otherwise, ensure that the site-init repository is available on ncn-m001.\nncn-mw# git clone \u0026#34;$SITE_INIT_REPO_URL\u0026#34; site-init Acquire customizations.yaml from the currently running system.\nncn-mw# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; site-init/customizations.yaml Review, add, and commit customizations.yaml to the local site-init repository as appropriate.\nNOTE: If site-init was cloned from a remote repository in step 1, there may not be any differences and hence nothing to commit. This is okay. If there are differences between what is in the repository and what was stored in the site-init, then it suggests settings were changed at some point.\nncn-mw# cd site-init ncn-mw# git diff ncn-mw# git add customizations.yaml ncn-mw# git commit -m \u0026#39;Add customizations.yaml from site-init secret\u0026#39; Modify the customizations Modify the customizations to include the added or removed storage node.\nRetrieve an API token.\nncn-mw# export TOKEN=$(curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth \\ -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Update the customizations spec.network.netstaticips.nmn_ncn_storage for the added or removed storage IP address.\nncn-mw# cd /usr/share/docs/csm/scripts/operations/node_management/Add_Remove_Replace_NCNs ncn-mw# ./update_customizations.sh Check that the updated customizations.yaml contains the change to add or remove a storage IP address.\nncn-mw# yq r /tmp/customizations.original.yaml -P \u0026gt; /tmp/customizations.original.yaml.pretty ncn-mw# diff /tmp/customizations.original.yaml.pretty /tmp/customizations.yaml Example output:\n10.252.1.13 Check in changes made to customizations.yaml.\nncn-mw# cd site-init ncn-mw# cp /tmp/customizations.yaml customizations.yaml ncn-mw# git diff ncn-mw# git add customizations.yaml ncn-mw# git commit -m \u0026#39;Update customizations.yaml nmn_ncn_storage storage IPs\u0026#39; Push to the remote repository as appropriate.\nncn-mw# git push Update site-init sealed secret in loftsman namespace.\nncn-mw# kubectl delete secret -n loftsman site-init ncn-mw# kubectl create secret -n loftsman generic site-init --from-file=/tmp/customizations.yaml Redeploy S3 Redeploy S3 to pick up any changes for storage node endpoints. Follow the Redeploying a Chart procedure with the following specifications:\nName of chart to be redeployed: cray-s3\nBase name of manifest: platform\nNo customization changes need to be made during the redeploy procedure \u0026ndash; they were already done earlier on this page.\nWhen reaching the step to validate that the redeploy was successful, perform the following step:\nOnly follow this step as part of the previously linked chart redeploy procedure.\nCheck that the new endpoint has been updated.\nncn-mw# kubectl get endpoints -l app.kubernetes.io/instance=cray-s3 -n ceph-rgw -o jsonpath=\u0026#39;{.items[*].subsets[].addresses}\u0026#39; | jq -r \u0026#39;.[] | .ip\u0026#39; Example output:\n10.252.1.13 10.252.1.4 10.252.1.5 10.252.1.6 Redeploy sysmgmt-health Redeploy sysmgmt-health to pick up any changes for storage node endpoints.\nFollow the Redeploying a Chart procedure with the following specifications:\nName of chart to be redeployed: cray-sysmgmt-health\nBase name of manifest: platform\nNo customization changes need to be made during the redeploy procedure \u0026ndash; they were already done earlier on this page.\nWhen reaching the step to validate that the redeploy was successful, perform the following step:\nOnly follow this step as part of the previously linked chart redeploy procedure.\nCheck that the new endpoint has been updated.\nncn-mw# kubectl get endpoints -l app=cray-sysmgmt-health-ceph-exporter -n sysmgmt-health -o jsonpath=\u0026#39;{.items[*].subsets[].addresses}\u0026#39; | jq -r \u0026#39;.[] | .ip\u0026#39; ncn-mw# kubectl get endpoints -l app=cray-sysmgmt-health-ceph-node-exporter -n sysmgmt-health -o jsonpath=\u0026#39;{.items[*].subsets[].addresses}\u0026#39; | jq -r \u0026#39;.[] | .ip\u0026#39; Example output:\n10.252.1.13 10.252.1.4 10.252.1.5 10.252.1.6 Cleanup Remove temporary files.\nncn-mw# rm /tmp/customizations.yaml /tmp/customizations.original.yaml /tmp/customizations.original.yaml.pretty Next step Proceed to the next step:\nIf a storage NCN was added, proceed to Validate NCN or return to the main Add, Remove, Replace, or Move NCNs page. If a storage NCN was removed, proceed to Validate Health or return to the main Add, Remove, Replace, or Move NCNs page. "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/check_current_dhcp_leases/",
	"title": "Check current DHCP leases",
	"tags": [],
	"description": "",
	"content": "Check current DHCP leases We will use the Kea API to retrieve data from the DHCP lease database. First you need to get the auth token, On ncn-w001 or a worker/manager with kubectl, run:\nexport TOKEN=$(curl -s -k -S -d grant_type=client_credentials -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Once you generate the auth token you can run these commands on a worker or manager node.\nIf you want to retrieve all the Leases, (warning this may cause your terminal to crash based on the size of the output.)\nGet all leases:\ncurl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api_gw_service.local/apis/dhcp-kea | jq If you have the IP and are looking for the hostname/MAC address. IP Address Lookup:\ncurl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ], \u0026#34;arguments\u0026#34;: { \u0026#34;ip-address\u0026#34;: \u0026#34;x.x.x.x\u0026#34; } }\u0026#39; https://api_gw_service.local/apis/dhcp-kea | jq If you have the MAC and are looking for the hostname/IP Address. MAC lookup:\ncurl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api_gw_service.local/apis/dhcp-kea | jq \u0026#39;.[].arguments.leases[] | select(.\u0026#34;hw-address\u0026#34;==\u0026#34;XX:XX:XX:XX:XX:5d\u0026#34;)\u0026#39; If you have the hostname and are looking for the MAC/IP address. Hostname lookup:\ncurl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api_gw_service.local/apis/dhcp-kea | jq \u0026#39;.[].arguments.leases[] | select(.\u0026#34;hostname\u0026#34;==\u0026#34;xNAME\u0026#34;)\u0026#39; If you want to see the total amount of leases. Total Leases:\ncurl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api_gw_service.local/apis/dhcp-kea | jq \u0026#39;.[].text\u0026#39; Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/igmp/",
	"title": "Configure Internet Group Multicast Protocol (IGMP)",
	"tags": [],
	"description": "",
	"content": "Configure Internet Group Multicast Protocol (IGMP) The Internet Group Multicast Protocol (IGMP) is a communications protocol used by hosts and adjacent routers on IP networks to establish multicast group memberships. The host joins a multicast-group by sending a join request message towards the network router, and responds to queries sent from the network router by dispatching a join report.\nConfiguration Command switch(config)# ip igmp snooping enable Expected Results show ip igmp-snooping vlan 1 should show IGMP enabled on the VLAN, but no IGMP Querier set\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/canu/quick_start_guide_to_canu/",
	"title": "Quick start guide to CANU",
	"tags": [],
	"description": "",
	"content": "Quick start guide to CANU [Usage] Validate a fresh system using CSI and CANU Preparation Check network firmware Check network cabling Validate BGP status Validate cabling Validation using the system\u0026rsquo;s SHCD Validate the SHCD Validate the SHCD against network cabling Generate switch configuration for the network Usage To run, type canu. It should run and display help.\nTo see a list of commands and arguments, just append --help.\nWhen running CANU, the Shasta version is required; it can be specified with either -s or --shasta. For example:\nncn# canu -s 1.5 Validate a fresh system using CSI and CANU Preparation Make a new directory to save switch IP addresses.\nncn# mkdir ips_folder ncn# cd ips_folder Parse CSI files and save switch IP addresses.\nncn# canu -s 1.5 init --csi-folder /var/www/prep/SYSTEMNAME/ --out ips.txt Check network firmware ncn# canu -s 1.5 network firmware --ips-file ips.txt Check network cabling ncn# canu -s 1.5 network cabling --ips-file ips.txt Validate BGP status ncn# canu -s 1.5 validate bgp --ips-file ips.txt –verbose Validate cabling ncn# canu -s 1.5 validate cabling --ips-file ips.txt Validation using the system\u0026rsquo;s SHCD With the system\u0026rsquo;s SHCD, CANU can also validate the configuration and cabling.\nValidate the SHCD ncn# canu -s 1.5 validate shcd --shcd SHCD.xlsx Validate the SHCD against network cabling ncn# canu -s 1.5 validate shcd-cabling --shcd SHCD.xlsx --ips-file ips.txt Generate switch configuration for the network ncn# canu -s 1.5 network config --shcd SHCD.xlsx --csi-folder /var/www/prep/SYSTEMNAME/ --folder configs "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/cable_diagnostics/",
	"title": "Cable Diagnostics",
	"tags": [],
	"description": "",
	"content": "Cable Diagnostics Use the cable-diagnostic feature to test cables in the event where there might be a bad copper cable.\nNOTE: This feature is only available on non-SFP copper ports.\nProcedure Enter diagnostics to open up the diagnostics menu:\nswitch# diagnostics Once done, the diagnostics command set is now available for use, and the cable-diagnostics command can be executed:\nswitch# diag cable-diagnostic \u0026lt;IFACE\u0026gt; Example output 6300# diagnostics \u0026lt;CR\u0026gt; 6300# diag ? asic ASIC diagnostics audit-failure-notification Configure audit failure notification bgp IP information cable-diagnostic Cable diagnostic test ...snip for brevity 6300# diag cable-diagnostic ? IFNAME Expected Results Administrators can enter diagnostics mode successfully Administrators can test the cable and see the results in the CLI output Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/external_dns/update_the_cmn-external-dns_value_post-installation/",
	"title": "Update the cmn-external-dns value post-installation",
	"tags": [],
	"description": "",
	"content": "Update the cmn-external-dns value post-installation By default, the services/cray-dns-powerdns-cmn-tcp and services/cray-dns-powerdns-cmn-udp services both share the same Customer Management Network (CMN) external IP address. This is defined by the cmn-external-dns value, which is specified during the csi config init input.\nThe IP address must be in the static range reserved in MetalLB\u0026rsquo;s cmn-static-pool subnet. Currently, this is the only CMN IP address that must be known external to the system, in order for external DNS to delegate the system-name.site-domain zone to services/cray-dns-powerdns deployment.\nChanging this value after install is relatively straightforward, and only requires the external IP address for services/cray-dns-powerdns-cmn-tcp and services/cray-dns-powerdns-cmn-udp services to be changed. This procedure will update the IP addresses that DNS queries.\nPrerequisites The system is installed.\nProcedure Update the LoadBalancer IP address Find the external IP address for the services/cray-dns-powerdns-cmn-tcp and services/cray-dns-powerdns-cmn-tcp services.\nncn-m001# kubectl -n services get svc | grep cray-dns-powerdns-cmn- Example output:\ncray-dns-powerdns-cmn-tcp LoadBalancer 10.25.211.48 10.102.14.113 53:31111/TCP 2d2h cray-dns-powerdns-cmn-udp LoadBalancer 10.25.156.88 10.102.14.113 53:32674/UDP 2d2h Edit the services and change spec.loadBalancerIP to the desired CMN IP address.\nEdit the cray-dns-powerdns-cmn-tcp service.\nncn-m001# kubectl -n services edit svc cray-dns-powerdns-cmn-tcp Edit the cray-dns-powerdns-cmn-udp service.\nncn-m001# kubectl -n services edit svc cray-dns-powerdns-cmn-udp Update SLS The external-dns IP address reservation in the SLS CMN cmn_metallb_static_pool subnet should be updated to the desired CMN IP address.\nRetrieve the SLS data for CMN.\nncn-m001# export TOKEN=$(curl -s -k -S -d grant_type=client_credentials \\ -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth \\ -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) ncn-m001# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ https://api-gw-service-nmn.local/apis/sls/v1/networks/CMN|jq \u0026gt; CMN.json ncn-m001# cp CMN.json CMN.json.bak Update the external-dns IP address in CMN.json to the desired CMN IP address.\n{ \u0026#34;Comment\u0026#34;: \u0026#34;site to system lookups\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;x.x.x.x\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;external-dns\u0026#34; } Upload the updated CMN.json file to SLS.\nncn-m001# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; --header \\ \u0026#34;Content-Type: application/json\u0026#34; --request PUT --data @CMN.json \\ https://api-gw-service-nmn.local/apis/sls/v1/networks/CMN Update customizations.yaml IMPORTANT: If this step is not performed, then the PowerDNS configuration will be overwritten with the previous value the next time CSM or the cray-dns-powerdns Helm chart is upgraded.\nExtract customizations.yaml from the site-init secret in the loftsman namespace.\nncn-m001# kubectl -n loftsman get secret site-init -o json | jq -r \u0026#39;.data.\u0026#34;customizations.yaml\u0026#34;\u0026#39; | base64 -d \u0026gt; customizations.yaml Update system_to_site_lookups in customizations.yaml to the desired CMN IP address.\nspec: network: netstaticips: site_to_system_lookups: x.x.x.x Update the site-init secret in the loftsman namespace.\nncn-m001# kubectl delete secret -n loftsman site-init ncn-m001# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml "
},
{
	"uri": "/docs-csm/en-12/operations/network/dns/troubleshoot_powerdns/",
	"title": "Troubleshoot PowerDNS",
	"tags": [],
	"description": "",
	"content": "Troubleshoot PowerDNS List DNS zone contents PowerDNS logging Verify DNSSEC operation Verify zones are being signed with the zone signing key Verify TSIG operation List DNS zone contents The PowerDNS zone database is populated with data from two sources:\nThe cray-powerdns-manager service creates the zones and DNS records based on data sourced from the System Layout Service (SLS). The external DNS records are populated by the cray-externaldns-external-dns service using data sourced from Kubernetes annotations and virtual service definitions. Use the cray-powerdns-visualizer command to view the zone structure that cray-powerdns-manager will create.\nncn# kubectl -n services exec deployment/cray-powerdns-manager -c cray-powerdns-manager -- cray-powerdns-visualizer Example output:\n. ├── 252.10.in-addr.arpa. │ ├── [PTR] 5.252.10.in-addr.arpa. │ │ └── sw-leaf-002.nmn.system.dev.cray.com. │ ├── [PTR] 4.252.10.in-addr.arpa. │ │ └── sw-leaf-001.nmn.system.dev.cray.com. │ ├── [PTR] 3.252.10.in-addr.arpa. │ │ └── sw-spine-002.nmn.system.dev.cray.com. │ ├── [PTR] 6.2.252.10.in-addr.arpa. │ │ └── pbs_comm_service.nmn.system.dev.cray.com. │ ├── [PTR] 5.2.252.10.in-addr.arpa. │ │ └── pbs_service.nmn.system.dev.cray.com. │ ├── [PTR] 4.2.252.10.in-addr.arpa. │ │ └── slurmdbd_service.nmn.system.dev.cray.com. │ ├── [PTR] 3.2.252.10.in-addr.arpa. │ │ └── slurmctld_service.nmn.system.dev.cray.com. │ ├── [PTR] 2.2.252.10.in-addr.arpa. │ │ └── uai_macvlan_bridge.nmn.system.dev.cray.com. [...] For more information on External DNS and troubleshooting steps, see the External DNS documentation.\nPowerDNS logging When troubleshooting DNS problems, it may prove helpful to increase the level of logging from the default value of 3 (error).\nEdit the cray-dns-powerdns Kubernetes ConfigMap.\nncn# kubectl -n services edit cm cray-dns-powerdns Set the loglevel parameter in pdns.conf to the desired setting.\npdns.conf: | config-dir=/etc/pdns include-dir=/etc/pdns/conf.d guardian=yes loglevel=3 setgid=pdns setuid=pdns socket-dir=/var/run version-string=anonymous Restart the PowerDNS service.\nncn# kubectl -n services rollout restart deployment cray-dns-powerdns Example output:\ndeployment.apps/cray-dns-powerdns restarted Refer to the external PowerDNS documentation for more information.\nVerify DNSSEC operation Verify zones are being signed with the zone signing key Check that the required zone has a DNSKEY entry; this should match the public key portion of the zone signing key.\nIn the command below, be sure to replace system.dev.cray.com with the correct value for the system.\nncn# kubectl -n services exec deployment/cray-dns-powerdns -c cray-dns-powerdns -- pdnsutil show-zone system.dev.cray.com Example output:\nThis is a Master zone Last SOA serial number we notified: 2021090901 == 2021090901 (serial in the database) Zone has following allowed TSIG key(s): system-key Zone uses following TSIG key(s): system-key Metadata items: AXFR-MASTER-TSIG system-key SOA-EDIT-API DEFAULT TSIG-ALLOW-AXFR system-key Zone has NSEC semantics keys: ID = 1 (CSK), flags = 257, tag = 26690, algo = 13, bits = 256 Active Published ( ECDSAP256SHA256 ) CSK DNSKEY = system.dev.cray.com. IN DNSKEY 257 3 13 TAi+aXL+Z8ZSFHxz+iEWB3MEdi1JWgM/tb3Q1M76yVOq5Kaur9k+oIAHXvCSR19Iuu+0ZUAyLB0vKkhScJp3Tw== ; ( ECDSAP256SHA256 ) DS = system.dev.cray.com. IN DS 26690 13 1 8c926281afb822a2bea767f08c79b856a2427c26 ; ( SHA1 digest ) DS = system.dev.cray.com. IN DS 26690 13 2 2bfd71e5403f99d25496f5f7f352e71747bb72ee6eb240dcaf8b56b95d18ef6c ; ( SHA256 digest ) DS = system.dev.cray.com. IN DS 26690 13 4 df40f23a7ee051d7e3d40d4059640bda3558cd74a37110b25f7b8cf4e60506c77bf33a660400710d397df0a1cde26d70 ; ( SHA-384 digest ) If the DNSKEY record is incorrect, then verify that the zone name is correct in the dnssec SealedSecret in customizations.yaml, and that the desired zone signing key was used. See the PowerDNS Configuration Guide for more information.\nVerify TSIG operation IMPORTANT: These examples are for informational purposes only. The use of the dig command -y option to present the key should be avoided in favor of the -k option with the secret in a file, in order to avoid the key being displayed in ps command output or the shell history.\nDetermine the IP address of the external DNS service.\nncn# kubectl -n services get service cray-dns-powerdns-can-tcp Example output:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cray-dns-powerdns-can-tcp LoadBalancer 10.27.91.157 10.101.8.113 53:30726/TCP 6d Verify that an AXFR query to the external DNS service works when the correct TSIG key is presented.\nncn# dig -t axfr system.dev.cray.com @10.101.8.113 -y \\ \u0026#34;hmac-sha256:system-key:dnFC5euKixIKXAr6sZhI7kVQbQCXoDG5R5eHSYZiBxY=\u0026#34; +nocrypto | head Example output:\n; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; -t axfr system.dev.cray.com @10.101.8.113 -y hmac-sha256:system-key:dnFC5euKixIKXAr6sZhI7kVQbQCXoDG5R5eHSYZiBxY= +nocrypto ;; global options: +cmd system.dev.cray.com. 3600 IN SOA a.misconfigured.dns.server.invalid. hostmaster.system.dev.cray.com. 2021090901 10800 3600 604800 3600 system.dev.cray.com. 3600 IN RRSIG SOA 13 4 3600 20210930000000 20210909000000 26690 system.dev.cray.com. [omitted] system-key. 0 ANY TSIG hmac-sha256. 1632302505 300 32 XoySAOtCD52OzO/2MeFk0/x7MG6m93IxtWaNfhzaRkg= 44483 NOERROR 0 system.dev.cray.com. 3600 IN DNSKEY 257 3 13 [key id = 26690] system.dev.cray.com. 3600 IN RRSIG DNSKEY 13 4 3600 20210930000000 20210909000000 26690 system.dev.cray.com. [omitted] sma-kibana.system.dev.cray.com. 300 IN A 10.101.8.128 sma-kibana.system.dev.cray.com. 300 IN RRSIG A 13 5 300 20210930000000 20210909000000 26690 system.dev.cray.com. [omitted] When presented with an invalid key, the transfer should fail.\nncn# dig -t axfr system.dev.cray.com @10.101.8.113 -y \u0026#34;hmac-sha256:system-key:B7n/sK74pa7r0ygOZkKpW9mWkPjq8fV71j1SaTpzJMQ=\u0026#34; Example output:\n;; Couldn\u0026#39;t verify signature: expected a TSIG or SIG(0) ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; -t axfr system.dev.cray.com @10.101.8.113 -y hmac-sha256:system-key:B7n/sK74pa7r0ygOZkKpW9mWkPjq8fV71j1SaTpzJMQ= ;; global options: +cmd ; Transfer failed. The cray-dns-powerdns pod log will also indicate that the request failed.\nncn# kubectl -n services logs cray-dns-powerdns-64fdf6597c-pqgdt -c cray-dns-powerdns Example output:\n--- Sep 22 09:31:17 Packet for \u0026#39;system.dev.cray.com\u0026#39; denied: Signature with TSIG key \u0026#39;system-key\u0026#39; failed to validate "
},
{
	"uri": "/docs-csm/en-12/operations/network/customer_accessible_networks/bi-can_arista_metallb_peering/",
	"title": "MetalLB Peering with Arista Edge Router",
	"tags": [],
	"description": "",
	"content": "MetalLB Peering with Arista Edge Router This is an example configuration of how to connect a pair of Arista switches to MetalLB running inside of Kubernetes.\nPrerequisites Pair of Arista switches already connected to the high-speed network. Updated System Layout Service (SLS) file that has the CHN network configured. Example Configuration Below is a snippet from an upgraded SLS.\n\u0026#34;CHN\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;CHN\u0026#34;, \u0026#34;FullName\u0026#34;: \u0026#34;Customer High-Speed Network\u0026#34;, \u0026#34;IPRanges\u0026#34;: [ \u0026#34;10.103.9.0/25\u0026#34; ], \u0026#34;Type\u0026#34;: \u0026#34;ethernet\u0026#34;, \u0026#34;LastUpdated\u0026#34;: 1646843463, \u0026#34;LastUpdatedTime\u0026#34;: \u0026#34;2022-03-09 16:31:03.504156 +0000 +0000\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;CIDR\u0026#34;: \u0026#34;10.103.9.0/25\u0026#34;, \u0026#34;MTU\u0026#34;: 9000, \u0026#34;MyASN\u0026#34;: 65530, \u0026#34;PeerASN\u0026#34;: 65533, \u0026#34;Subnets\u0026#34;: [ { \u0026#34;CIDR\u0026#34;: \u0026#34;10.103.9.64/27\u0026#34;, \u0026#34;FullName\u0026#34;: \u0026#34;CHN Dynamic MetalLB\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;10.103.9.65\u0026#34;, \u0026#34;MetalLBPoolName\u0026#34;: \u0026#34;customer-high-speed\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;chn_metallb_address_pool\u0026#34;, \u0026#34;VlanID\u0026#34;: 5 }, { \u0026#34;CIDR\u0026#34;: \u0026#34;10.103.9.0/25\u0026#34;, \u0026#34;DHCPEnd\u0026#34;: \u0026#34;10.103.9.62\u0026#34;, \u0026#34;DHCPStart\u0026#34;: \u0026#34;10.103.9.16\u0026#34;, \u0026#34;FullName\u0026#34;: \u0026#34;CHN Bootstrap DHCP Subnet\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;10.103.9.1\u0026#34;, \u0026#34;IPReservations\u0026#34;: [ { \u0026#34;IPAddress\u0026#34;: \u0026#34;10.103.9.2\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;chn-switch-1\u0026#34; }, { \u0026#34;IPAddress\u0026#34;: \u0026#34;10.103.9.3\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;chn-switch-2\u0026#34; }, { \u0026#34;Aliases\u0026#34;: [ \u0026#34;ncn-w004-chn\u0026#34;, \u0026#34;time-chn\u0026#34;, \u0026#34;time-chn.local\u0026#34; ], \u0026#34;Comment\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.103.9.7\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;ncn-w004\u0026#34; }, { \u0026#34;Aliases\u0026#34;: [ \u0026#34;ncn-w003-chn\u0026#34;, \u0026#34;time-chn\u0026#34;, \u0026#34;time-chn.local\u0026#34; ], \u0026#34;Comment\u0026#34;: \u0026#34;x3000c0s6b0n0\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.103.9.8\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;ncn-w003\u0026#34; }, { \u0026#34;Aliases\u0026#34;: [ \u0026#34;ncn-w002-chn\u0026#34;, \u0026#34;time-chn\u0026#34;, \u0026#34;time-chn.local\u0026#34; ], \u0026#34;Comment\u0026#34;: \u0026#34;x3000c0s5b0n0\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.103.9.9\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;ncn-w002\u0026#34; }, { \u0026#34;Aliases\u0026#34;: [ \u0026#34;ncn-w001-chn\u0026#34;, \u0026#34;time-chn\u0026#34;, \u0026#34;time-chn.local\u0026#34; ], \u0026#34;Comment\u0026#34;: \u0026#34;x3000c0s4b0n0\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.103.9.10\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;ncn-w001\u0026#34; }, In this example, chn-switch-1 and chn-switch-2 will be the Arista pair.\nSLS entries from the above output:\n{ \u0026#34;IPAddress\u0026#34;: \u0026#34;10.103.9.2\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;chn-switch-1\u0026#34; }, { \u0026#34;IPAddress\u0026#34;: \u0026#34;10.103.9.3\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;chn-switch-2\u0026#34; } The following configuration is needed on both switches:\nThe prefix list will be the subnet of the CHN, the ge will equal the CIDR.\nThis prevents routes from other networks being installed into the routing table. router bgp 65533 will match the ASN from SLS.\u0026quot;MyASN\u0026quot;: 65533,\nThe neighbor will match every worker node.\nip prefix-list CHN seq 10 permit 10.103.9.64/27 ge 27 ! route-map CHN permit 5 match ip address prefix-list CHN ! router bgp 65533 maximum-paths 32 neighbor 10.103.9.7 remote-as 65530 neighbor 10.103.9.7 passive neighbor 10.103.9.7 route-map CHN in neighbor 10.103.9.7 maximum-routes 12000 neighbor 10.103.9.8 remote-as 65530 neighbor 10.103.9.8 passive neighbor 10.103.9.8 route-map CHN in neighbor 10.103.9.8 maximum-routes 12000 neighbor 10.103.9.9 remote-as 65530 neighbor 10.103.9.9 passive neighbor 10.103.9.9 route-map CHN in neighbor 10.103.9.9 maximum-routes 12000 neighbor 10.103.9.10 remote-as 65530 neighbor 10.103.9.10 passive neighbor 10.103.9.10 route-map CHN in neighbor 10.103.9.10 maximum-routes 12000 "
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/check_for_and_clear_etcd_cluster_alarms/",
	"title": "Check for and Clear etcd Cluster Alarms",
	"tags": [],
	"description": "",
	"content": "Check for and Clear etcd Cluster Alarms Check for any etcd cluster alarms and clear them as needed. An etcd cluster alarm must be manually cleared.\nFor example, a cluster\u0026rsquo;s database NOSPACE alarm is set when database storage space is no longer available. A subsequent defrag may free up database storage space, but writes to the database will continue to fail while the NOSPACE alarm is set.\nPrerequisites This procedure requires root privileges. The etcd clusters are in a healthy state. Procedure Check for etcd cluster alarms.\nAn empty list will be returned if no alarms are set.\nCheck if any etcd alarms are set for etcd clusters in the services namespace.\nncn-mw# for pod in $(kubectl get pods -l etcd_cluster=cray-bos-etcd \\ -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod} Alarms Set: ###\u0026#34; kubectl -n services exec ${pod} -c etcd -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl alarm list\u0026#34; done Example output:\n### cray-bos-etcd-7cxq6qrhz5 Alarms Set: ### ### cray-bos-etcd-b9m4k5qfrd Alarms Set: ### ### cray-bos-etcd-tnpv8x6cxv Alarms Set: ### ### cray-bss-etcd-q4k54rbbfj Alarms Set: ### ### cray-bss-etcd-r75mlv6ffd Alarms Set: ### ### cray-bss-etcd-xprv5ht5d4 Alarms Set: ### ### cray-cps-etcd-8hpztfkjdp Alarms Set: ### ### cray-cps-etcd-fp4kfsf799 Alarms Set: ### ### cray-cps-etcd-g6gz9vmmdn Alarms Set: ### ### cray-crus-etcd-6z9zskl6cr Alarms Set: ### ### cray-crus-etcd-krp255f97q Alarms Set: ### ### cray-crus-etcd-tpclqfln67 Alarms Set: ### [...] Check if any etcd alarms are set for a particular etcd cluster in the services namespace.\nncn-mw# for pod in $(kubectl get pods -l etcd_cluster=cray-bos-etcd \\ -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod} Alarms Set: ###\u0026#34; kubectl -n services exec ${pod} -c etcd -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl alarm list\u0026#34; done Example output:\n### cray-bos-etcd-7cxq6qrhz5 Alarms Set: ### ### cray-bos-etcd-b9m4k5qfrd Alarms Set: ### ### cray-bos-etcd-tnpv8x6cxv Alarms Set: ### Clear any etcd cluster alarms.\nA list of disarmed alarms will be returned. An empty list is returned if no alarms were set.\nClear all etcd alarms set in etcd clusters.\nncn-mw# for pod in $(kubectl get pods -l app=etcd -n services \\ -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod} Disarmed Alarms: ###\u0026#34; kubectl -n services exec ${pod} -c etcd -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl alarm disarm\u0026#34; done Example output:\n### cray-bos-etcd-7cxq6qrhz5 Disarmed Alarms: ### ### cray-bos-etcd-b9m4k5qfrd Disarmed Alarms: ### ### cray-bos-etcd-tnpv8x6cxv Disarmed Alarms: ### ### cray-bss-etcd-q4k54rbbfj Disarmed Alarms: ### ### cray-bss-etcd-r75mlv6ffd Disarmed Alarms: ### ### cray-bss-etcd-xprv5ht5d4 Disarmed Alarms: ### ### cray-cps-etcd-8hpztfkjdp Disarmed Alarms: ### ### cray-cps-etcd-fp4kfsf799 Disarmed Alarms: ### [...] Clear all alarms in one particular etcd cluster.\nncn-mw# for pod in $(kubectl get pods -l etcd_cluster=cray-bos-etcd \\ -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod} Disarmed Alarms: ###\u0026#34; kubectl -n services exec ${pod} -c etcd -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl alarm disarm\u0026#34; done Example output:\n### cray-bos-etcd-7cxq6qrhz5 Disarmed Alarms: ### memberID:14039380531903955557 alarm:NOSPACE memberID:10060051157615504224 alarm:NOSPACE memberID:9418794810465807950 alarm:NOSPACE ### cray-bos-etcd-b9m4k5qfrd Disarmed Alarms: ### ### cray-bos-etcd-tnpv8x6cxv Disarmed Alarms: ### "
},
{
	"uri": "/docs-csm/en-12/operations/image_management/delete_or_recover_deleted_ims_content/",
	"title": "Delete or Recover Deleted IMS Content",
	"tags": [],
	"description": "",
	"content": "Delete or Recover Deleted IMS Content The Image Management System (IMS) manages user-supplied SSH public keys, customizable image recipes, images, and IMS jobs that are used to build or customize images. In previous versions of IMS, deleting an IMS public key, recipe, or image resulted in that item being permanently deleted. Additionally, IMS recipes and images store linked artifacts in the Simple Storage Service (S3) datastore. These artifacts are referenced by the IMS recipe and image records. The default option when deleting an IMS recipe and image record was to also delete these linked S3 artifacts.\nncn# cray ims recipes list --format toml Excerpt of example output:\n[[results]] id = \u0026#34;76ef564d-47d5-415a-bcef-d6022a416c3c\u0026#34; name = \u0026#34;cray-sles15-barebones\u0026#34; created = \u0026#34;2020-02-05T19:24:22.621448+00:00\u0026#34; [results.link] path = \u0026#34;s3://ims/recipes/76ef564d-47d5-415a-bcef-d6022a416c3c/cray-sles15-barebones.tgz\u0026#34; etag = \u0026#34;28f3d78c8cceca2083d7d3090d96bbb7\u0026#34; type = \u0026#34;s3\u0026#34; ncn# cray ims images list --format toml Excerpt of example output:\n[[results]] created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; [results.link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/4e78488d-4d92-4675-9d83-97adfc17cb19/sles_15_image.squashfs\u0026#34; etag = \u0026#34;\u0026#34; Deleting an IMS image can create a situation where boot artifacts referenced by a Boot Orchestration Service (BOS) session template no longer exist, making that template unable to boot. Previously, to recover from this situation, an administrator would have had to rebuild the boot image using IMS and/or reinstall the prebuilt image from the installer, reapply any Cray and site customizations, and recreate a new BOS template for the IMS image.\nNew functionality has been added to IMS to enable administrators to soft delete, recover (undelete), or hard delete public keys, recipes, and images. The added functionality provides a way to recover IMS items that were mistakenly deleted. There is no undelete operation for IMS jobs.\nSoft deleting an IMS record effectively removes the record being deleted from the default collection, and moves it to a new deleted collection. Recovering a deleted IMS record (undelete operation) moves the IMS record from the deleted collection back to the collection of available items. Hard deleting an IMS record permanently deletes it from the deleted collection.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. IMS and Nexus are running. Delete an IMS artifact Use the cray CLI utility to delete either soft delete or hard delete an IMS public key, recipe, or image.\nSoft deleting an IMS public key, recipe, or image removes the records from the collection of available items. Hard deleting permanently removes the item from the deleted collection. Additionally, any linked artifacts are also permanently removed.\nDeleting an IMS public key, recipe, or image record performs the following actions:\nThe IMS records being deleted are moved from the collection of available items to a new deleted collection. Any newly created records within the deleted collection will have the same IMS ID values as before being moved there. Any Simple Storage Service (S3) artifacts that are associated with the record or records being deleted are renamed within their S3 buckets so as to make them unavailable under their original key name. Delete procedure Soft delete the desired IMS artifact.\nThe following substeps assume that an image is being deleted. The same process can be followed if deleting a public key or recipe.\nList the existing images in IMS.\nncn# cray ims images list --format toml Excerpt of example output (note the value in the id field):\n[[results]] created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; [results.link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/4e78488d-4d92-4675-9d83-97adfc17cb19/sles_15_image.squashfs\u0026#34; etag = \u0026#34;\u0026#34; Delete the image.\nncn# cray ims images delete IMS_IMAGE_ID Verify the image was successfully deleted.\nncn# cray ims images list View the recently deleted item in the deleted images list.\nncn# cray ims deleted images list --format toml Excerpt of example output:\n[[results]] created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; deleted = \u0026#34;2020-11-03T09:57:31.746521+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; [results.link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/deleted/4e78488d-4d92-4675-9d83-97adfc17cb19/sles_15_image.squashfs\u0026#34; etag = \u0026#34;\u0026#34; The created field shows the date that the record was originally created. The deleted field shows the date that the record was deleted. The link.path field shows the updated path to the artifact in S3. If the administrator desires the public key, recipe, or image to be permanently deleted, then proceed to the next step. If the deleted image might need to be recovered in the future, no more work is needed.\nHard delete the desired IMS artifact.\nDo not proceed with this step if the IMS artifact might be needed in the future. The following substeps assume that an image is being deleted. The same process can be followed if deleting a public key or recipe.\nList the deleted images.\nncn# cray ims deleted images list --format toml Excerpt of example output (note the value in the id field):\n[[results]] created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; deleted = \u0026#34;2020-11-03T09:57:31.746521+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; [results.link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/deleted/4e78488d-4d92-4675-9d83-97adfc17cb19/sles_15_image.squashfs\u0026#34; etag = \u0026#34;\u0026#34; Permanently delete the desired image from the deleted images list.\nncn# cray ims deleted images delete IMS_IMAGE_ID Recover deleted IMS artifacts Use the IMS undelete command to update the records within the deleted collection for an IMS public key, recipe, or image.\nRecovering a deleted IMS public key, recipe, or image record uses the following workflow:\nThe records being undeleted are moved to from the deleted collection to the collection of available items. Any restored records will have the same IMS ID values as before being undeleted. Any Simple Storage Service (S3) artifacts that are associated with the records being undeleted are renamed within their S3 buckets so as to make them available under their original key name. Undelete procedure The steps in this procedure assume that a deleted image is being recovered. The same process can be followed if recovering a deleted public key or recipe.\nList the deleted image.\nncn# cray ims deleted images list --format toml Excerpt of example output (note the value in the id field):\n[[results]] created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; deleted = \u0026#34;2020-11-03T09:57:31.746521+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; [results.link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/deleted/4e78488d-4d92-4675-9d83-97adfc17cb19/sles_15_image.squashfs\u0026#34; etag = \u0026#34;\u0026#34; Use the undelete operation to recover the image.\nncn# cray ims deleted images update IMS_IMAGE_ID --operation undelete List the deleted images to verify the recovered image is no longer in the collection of deleted items.\nncn# cray ims deleted images list List the IMS images to verify the image was recovered.\nncn# cray ims images list --format toml Excerpt of example output:\n[[results]] created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; [results.link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/4e78488d-4d92-4675-9d83-97adfc17cb19/sles_15_image.squashfs\u0026#34; etag = \u0026#34;\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/create_a_backup_of_the_hsm_postgres_database/",
	"title": "Create a Backup of the HSM Postgres Database",
	"tags": [],
	"description": "",
	"content": "Create a Backup of the HSM Postgres Database Perform a manual backup of the contents of the Hardware State Manager (HSM) Postgres database. This backup can be used to restore the contents of the HSM Postgres database at a later point in time using the Restore HSM Postgres from Backup procedure.\nPrerequisites Healthy HSM Postgres Cluster.\nUse patronictl list on the HSM Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:\nncn# kubectl exec cray-smd-postgres-0 -n services -c postgres -it -- patronictl list Example output:\n+ Cluster: cray-smd-postgres (6975238790569058381) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres-0 | 10.44.0.40 | Leader | running | 1 | | | cray-smd-postgres-1 | 10.36.0.37 | | running | 1 | 0 | | cray-smd-postgres-2 | 10.42.0.42 | | running | 1 | 0 | +---------------------+------------+--------+---------+----+-----------+ Healthy HSM Service.\nVerify all 3 HSM replicas are up and running:\nncn# kubectl -n services get pods -l cluster-name=cray-smd-postgres Example output:\nNAME READY STATUS RESTARTS AGE cray-smd-postgres-0 3/3 Running 0 18d cray-smd-postgres-1 3/3 Running 0 18d cray-smd-postgres-2 3/3 Running 0 18d Procedure Create a directory to store the HSM backup files.\nncn# BACKUP_LOCATION=\u0026#34;/root\u0026#34; ncn# export BACKUP_NAME=\u0026#34;cray-smd-postgres-backup_`date \u0026#39;+%Y-%m-%d_%H-%M-%S\u0026#39;`\u0026#34; ncn# export BACKUP_FOLDER=\u0026#34;${BACKUP_LOCATION}/${BACKUP_NAME}\u0026#34; ncn# mkdir -p \u0026#34;$BACKUP_FOLDER\u0026#34; The HSM backup will be located in the following directory:\nncn# echo $BACKUP_FOLDER /root/cray-smd-postgres-backup_2021-07-07_16-39-44 Run the backup_smd_postgres.sh script to take a backup of the HSM Postgres.\nncn# /usr/share/doc/csm/operations/hardware_state_manager/scripts/backup_smd_postgres.sh Example output:\n~/cray-smd-postgres-backup_2021-07-07_16-39-44 ~ HSM postgres backup file will land in /root/cray-smd-postgres-backup_2021-07-07_16-39-44 Determining the postgres leader... The HSM postgres leader is cray-smd-postgres-0 Using pg_dumpall to dump the contents of the HSM database... PSQL dump is available at /root/cray-smd-postgres-backup_2021-07-07_16-39-44/cray-smd-postgres-backup_2021-07-07_16-39-44.psql Saving Kubernetes secret service-account.cray-smd-postgres.credentials Saving Kubernetes secret hmsdsuser.cray-smd-postgres.credentials Saving Kubernetes secret postgres.cray-smd-postgres.credentials Saving Kubernetes secret standby.cray-smd-postgres.credentials Removing extra fields from service-account.cray-smd-postgres.credentials.yaml Removing extra fields from hmsdsuser.cray-smd-postgres.credentials.yaml Removing extra fields from postgres.cray-smd-postgres.credentials.yaml Removing extra fields from standby.cray-smd-postgres.credentials.yaml Adding Kubernetes secret service-account.cray-smd-postgres.credentials to secret manifest Adding Kubernetes secret hmsdsuser.cray-smd-postgres.credentials to secret manifest Adding Kubernetes secret postgres.cray-smd-postgres.credentials to secret manifest Adding Kubernetes secret standby.cray-smd-postgres.credentials to secret manifest Secret manifest is located at /root/cray-smd-postgres-backup_2021-07-07_16-39-44/cray-smd-postgres-backup_2021-07-07_16-39-44.manifest HSM Postgres backup is available at: /root/cray-smd-postgres-backup_2021-07-07_16-39-44 Copy the backup folder off of the cluster, and store it in a secure location.\nThe BACKUP_FOLDER environment variable is the name of the folder to backup.\nncn# echo $BACKUP_FOLDER /root/cray-smd-postgres-backup_2021-07-07_16-39-44 Optionally, create a tarball of the Postgres backup files:\nncn# cd $BACKUP_FOLDER \u0026amp;\u0026amp; cd .. ncn# tar -czvf $BACKUP_NAME.tar.gz $BACKUP_NAME "
},
{
	"uri": "/docs-csm/en-12/operations/firmware/updating_firmware_m001/",
	"title": "Updating BMC Firmware and BIOS for ncn-m001",
	"tags": [],
	"description": "",
	"content": "Updating BMC Firmware and BIOS for ncn-m001 Retrieve the model name and firmware image required to update an HPE or Gigabyte ncn-m001 node.\nNOTE\nOn HPE nodes, the BMC firmware is iLO 5 and BIOS is System ROM. The commands in the procedure must be run on ncn-m001. Prerequisites Find the model name Get the firmware images Flash the firmware Flash Gigabyte ncn-m001 Flash HPE ncn-m001 Prerequisites WARNING: This procedure should not be performed during a CSM install while ncn-m001 is booted as the PIT node using a remote ISO image. Doing so may reset the remote ISO mount, requiring a reboot to recover.\nThe following information is needed:\nIP address of ncn-m001 BMC IP address of ncn-m001 Root password for ncn-m001 BMC Find the model name Use one of the following commands to find the model name for the node type in use.\nFind HPE model name\nFind Gigabyte model name\nFind HPE model name.\nncn-m001# curl -k -u root:password https://ipaddressOfBMC/redfish/v1/Systems/1 | jq .Model Find Gigabyte model name.\nncn-m001# curl -k -u root:password https://ipaddressOfBMC/redfish/v1/Systems/Self | jq .Model Get the firmware images View a list of images stored in FAS that are ready to be flashed.\nIn the following example, ModelName is the name found in the previous section.\nncn-m001# cray fas images list --format json | jq \u0026#39;.[] | .[] | select(.models | index(\u0026#34;ModelName\u0026#34;))\u0026#39; Locate the images in the returned output for the ncn-m001 firmware and/or BIOS.\nLook for the returned s3URL. For example:\n\u0026#34;s3URL\u0026#34;: \u0026#34;s3:/fw-update/4e5f569a603311eb96b582a8e219a16d/image.RBU\u0026#34; Get the firmware images using the s3URL path from the previous step.\nIn the following example command, 4e5f569a603311eb96b582a8e219a16d/image.RBU is the path in the s3URL, and the image will be saved to the file image.RBU in the current directory.\nncn-m001# cray artifacts get fw-update 4e5f569a603311eb96b582a8e219a16d/image.RBU image.RBU Flash the firmware Flash Gigabyte ncn-m001 Flash HPE ncn-m001 Flash Gigabyte ncn-m001 Start a webserver from the directory containing the downloaded image:\nncn-m001# python3 -m http.server 8770 Update BMC.\nBe sure to substitute the correct values for the following strings in the example command:\npasswd = Root password of ncn-m001 BMC ipaddressOfBMC = IP address of ncn-m001 BMC ipaddressOfM001 = IP address of ncn-m001 node filename = Filename of the downloaded image ncn-m001# curl -k -u root:passwd https://ipaddressOfBMC/redfish/v1/UpdateService/Actions/SimpleUpdate -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;ImageURI\u0026#34;:\u0026#34;http://ipaddressOfM001:8770/filename\u0026#34;, \u0026#34;TransferProtocol\u0026#34;:\u0026#34;HTTP\u0026#34;, \u0026#34;UpdateComponent\u0026#34;:\u0026#34;BMC\u0026#34;}\u0026#39; Update BIOS.\nBe sure to substitute the correct values for the following strings in the example command:\npasswd = Root password of ncn-m001 BMC ipaddressOfBMC = IP address of ncn-m001 BMC ipaddressOfM001 = IP address of ncn-m001 node filename = Filename of the downloaded image ncn-m001# curl -k -u root:passwd https://ipaddressOfBMC/redfish/v1/UpdateService/Actions/SimpleUpdate -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;ImageURI\u0026#34;:\u0026#34;http://ipaddressOfM001:8770/filename\u0026#34;, \u0026#34;TransferProtocol\u0026#34;:\u0026#34;HTTP\u0026#34;, \u0026#34;UpdateComponent\u0026#34;:\u0026#34;BIOS\u0026#34;}\u0026#39; After updating BIOS, ncn-m001 will need to be rebooted. Follow the Reboot NCNs procedure to reboot ncn-m001.\nFlash HPE ncn-m001 The web interface will be used to update iLO 5 (BMC) firmware and/or System ROM (BIOS) on the HPE ncn-m001 node.\nCopy the iLO 5 firmware and/or System ROM files to a local computer from ncn-m001 using scp or other secure copy tools.\nlinux# scp root@ipaddressOfM001Node:pathToFile/filename . Open a web browser window and type in the name or IP address of the iLO device for ncn-m001.\nLog in with root and the root password for the iLO device.\nClick on \u0026quot;Firmware \u0026amp; OS Software\u0026quot; on the left menu. Click on \u0026quot;Update Firmware\u0026quot; on the right menu. Check \u0026quot;Local File\u0026quot;. Click \u0026quot;Choose File\u0026quot; and select the iLO firmware file or System ROM file. Click \u0026quot;Confirm TPM override\u0026quot;. Click \u0026quot;Flash\u0026quot;. After updating System ROM (BIOS), ncn-m001 will need to be rebooted. Follow the Reboot NCNs procedure to reboot ncn-m001.\n"
},
{
	"uri": "/docs-csm/en-12/operations/conman/manage_node_consoles/",
	"title": "Manage Node Consoles",
	"tags": [],
	"description": "",
	"content": "Manage Node Consoles ConMan is used for connecting to remote consoles and collecting console logs. These node logs can then be used for various administrative purposes, such as troubleshooting node boot issues.\nConMan runs on the system in a set of containers within Kubernetes pods named cray-console-operator and cray-console-node.\nThe cray-console-operator and cray-console-node pods determine which nodes they should monitor by checking with the Hardware State Manager (HSM) service. They do this once when they starts. If HSM has not discovered some nodes when they start, then HSM is unaware of them and therefore so are the cray-console-operator and cray-console-node pods.\nVerify that all nodes are being monitored for console logging and connect to them if desired.\nSee ConMan for other procedures related to remote consoles and node console logging.\nProcedure This procedure can be run from any member of the Kubernetes cluster to verify node consoles are being managed by ConMan and to connect to a console.\nNOTE: this procedure has changed since the CSM 0.9 release.\nFind the cray-console-operator pod.\nncn# OP_POD=$(kubectl get pods -n services \\ -o wide|grep cray-console-operator|awk \u0026#39;{print $1}\u0026#39;) ncn# echo $OP_POD Example output:\ncray-console-operator-6cf89ff566-kfnjr Find the cray-console-node pod that is connected to the node. Be sure to substitute the actual component name (xname) of the node in the command below.\nncn# XNAME=\u0026lt;xname\u0026gt; ncn# NODEPOD=$(kubectl -n services exec $OP_POD -c cray-console-operator -- sh -c \u0026#34;/app/get-node $XNAME\u0026#34; | jq .podname | sed \u0026#39;s/\u0026#34;//g\u0026#39;) ncn# echo $NODEPOD Example output:\ncray-console-node-2 Log into the cray-console-node container in this pod:\nncn# kubectl exec -n services -it $NODEPOD -c cray-console-node -- bash Example output:\ncray-console-node# Check the list of nodes being monitored.\ncray-console-node# conman -q Output looks similar to the following:\nx9000c0s1b0n0 x9000c0s20b0n0 x9000c0s22b0n0 x9000c0s24b0n0 x9000c0s27b1n0 x9000c0s27b2n0 x9000c0s27b3n0 Compute nodes or UANs are automatically added to this list a short time after they are discovered.\nTo access the node\u0026rsquo;s console, run the following command from within the pod. Again, remember to substitute the actual component name (xname) of the node.\ncray-console-node# conman -j \u0026lt;xname\u0026gt; The console session can be exited by entering \u0026amp;.\nRepeat the previous steps to verify that cray-console is now managing all nodes that are included in HSM.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/change_the_ansible_verbosity_logs/",
	"title": "Change the Ansible Verbosity Logs",
	"tags": [],
	"description": "",
	"content": "Change the Ansible Verbosity Logs It is useful to view the Ansible logs in a Configuration Framework Session (CFS) session with greater verbosity than the default. CFS sessions are able to set the Ansible verbosity from the command line when the session is created. The verbosity will apply to all configuration layers in the session.\nSpecify an integer using the \u0026ndash;ansible-verbosity option, where 1 = -v, 2 = -vv, and so on. Valid values range from 0 (default) to 4. See the ansible-playbook help for more information.\nIt is not recommended to use level 3 or 4 with sessions that target large numbers of hosts. When using --ansible-verbosity to debug Ansible plays or roles, consider also limiting the session targets with --ansible-limit to reduce log output.\nWARNING: Setting the --ansible-verbosity to 4 can cause CFS sessions to hang for unknown reasons. To correct this issue, reduce the verbosity to 3 or lower, or adjust the usage of the display_ok_hosts and display_skipped_hosts settings in the ansible.cfg file the session is using. Consider also reviewing the Ansible tasks being run and reducing the amount log output from these individual tasks.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/clean_up_after_a_bos-boa_job_is_completed_or_cancelled/",
	"title": "Clean Up After a BOS/BOA Job is Completed or Cancelled",
	"tags": [],
	"description": "",
	"content": "Clean Up After a BOS/BOA Job is Completed or Cancelled NOTE This section is for Boot Orchestration Service (BOS) v1 only. BOS v2 does not use Boot Orchestration Agent (BOA) jobs and does not require cleanup.\nWhen a BOS session is created, there are a number of items created on the system. When a session is cancelled or completed, these items need to be cleaned up to ensure there is not lingering content from the session on the system.\nWhen a session is launched, the following items are created:\nBOA job: The Kubernetes job that runs and handles the BOS session. ConfigMap for BOA: This ConfigMap contains the configuration information that the BOA job uses. The BOA pod mounts a ConfigMap named boot-session at /mnt/boot_session inside the pod. The name of the ConfigMap has a one-to-one relationship to the name of the BOS session created; however, the name of the BOS session can be different from the name of the BOA job. Etcd entries: BOS makes an entry for the session in its Etcd key/value store. If the BOA job has run for long enough, it will also have written a status entry into Etcd for this session. Configuration Framework Service (CFS) session: If configuration is enabled, and the session is doing a boot, reboot, or configure operation, then BOA will have instructed CFS to configure the nodes once they boot. There is not an easy way to link a BOA session to the CFS sessions that are spawned. Prerequisites A BOS session has been completed or cancelled. The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Procedure Identify the BOA job that needs to be deleted.\nDescribe the BOS session to find the name of the BOA job under the attribute boa_job_name.\nncn-mw# cray bos session describe --format json BOS_SESSION_ID Example output:\n{ \u0026#34;status_link\u0026#34;: \u0026#34;/v1/session/d200f7e4-1a9f-4466-9ef4-30add3bd87dd/status\u0026#34;, \u0026#34;complete\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;start_time\u0026#34;: \u0026#34;2020-08-11 21:02:09.137917\u0026#34;, \u0026#34;templateUuid\u0026#34;: \u0026#34;cle-1.3.0-nid1\u0026#34;, \u0026#34;error_count\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;boa_job_name\u0026#34;: \u0026#34;boa-d200f7e4-1a9f-4466-9ef4-30add3bd87dd\u0026#34;, \u0026#34;in_progress\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;boot\u0026#34;, \u0026#34;stop_time\u0026#34;: null } Find the ConfigMap for the BOA job.\nThe ConfigMap is listed as boot-session under the Volumes section. Retrieve the Name value from the returned output.\nncn-mw# kubectl -n services describe job BOA_JOB_NAME Excerpt of example output:\nVolumes: boot-session: Type: ConfigMap (a volume populated by a ConfigMap) **Name: e786def5-37a6-40db-b36b-6b67ebe174ee** Optional: false Delete the ConfigMap.\nncn-mw# kubectl -n services delete cm CONFIGMAP_NAME Delete the etcd entry for the BOS session.\nncn-mw# cray bos session delete BOS_SESSION_ID Stop CFS from configuring nodes.\nThere are several different use cases covered in this step. The process varies depending on whether a job is being cancelled, or if the CFS content is simply being cleaned up.\nIn the BOS session template, if configuration is enabled, then BOA instructs the nodes to configure on boot when doing a boot or reboot operation. When only doing a configure operation, the configuration happens right away. If configuration is disabled or the operation is shutdown, then BOA will not instruct CFS to configure the nodes, and nothing further needs to be done. The remainder of this step may be skipped.\nIf BOA instructs CFS to configure the nodes, then CFS will set the desired configuration for the nodes in its database. Once BOA tells CFS to configure the nodes, which happens early in the BOA job, then CFS will configure the nodes immediately if the operation is configure, or upon the node booting if the operation is boot or reboot.\nAttempting to prevent CFS from configuring the nodes is a multi-step, tedious process. It may be simpler to allow CFS to configure the nodes. If the nodes are going to be immediately rebooted, then the CFS configuration will be rerun once the nodes have booted, thus undoing any previous configuration. Alternatively, if the nodes are going to be immediately shutdown, then this will remove any need to prevent CFS from configuring the nodes.\nFollow the procedures for one of the following use cases:\nConfiguration has completed and the sessions need to be cleaned up the to reduce clutter:\nFind the old sessions that needs to be deleted.\nncn-mw# cray cfs sessions list Delete the sessions.\nncn-mw# cray cfs sessions delete CFS_SESSION_NAME Configuration has completed and the desired state needs to be cleaned up so that configuration does not happen on restart:\nUnset the desired state for all affected components.\nFind the component names (xnames) for the components with the desired configuration matching what was applied.\nncn-mw# cray cfs components list Prevent the configuration from running.\nncn-mw# cray cfs components update XNAME --desired-config \u0026#34;\u0026#34; This needs to be done for each component. It is enough to prevent configuration from running, and it does not revert to the previous desired state. The previous desired state has already been overwritten at this point, so if the user is trying to completely revert, they will either need to know and apply the previous desired state manually, or create a BOS session with the previous template using the configure operation.\nConfiguration was set/started and needs to be cancelled:\nUnset the desired state for all components affected.\nFind the impacted component names (xnames) for the components with the desired configuration matching what was applied.\nncn-mw# cray cfs components list Prevent the configuration from running.\nncn-mw# cray cfs components update XNAME --desired-config \u0026#34;\u0026#34; This needs to be done for each component. It is enough to prevent configuration from running, and it does not revert to the previous desired state. The previous desired state has already been overwritten at this point, so if the user is trying to completely revert, they will either need to know and apply the previous desired state manually, or create a BOS session with the previous template using the configure operation.\nRestart the batcher.\nThis will purge any information that CFS cached in relation to the BOA job that it was intending to act upon.\nGet the cfs-batcher pod ID.\nncn-mw# kubectl -n services get pods|grep cray-cfs-batcher Example output:\ncray-cfs-batcher-644599c6cc-rwl8f 2/2 Running 0 6d17h Restart the pod by scaling the replicas to 0 in order to stop it, and then back up to 1 in order to restart it.\nncn-mw# kubectl -n services scale CFS-BATCHER_POD_ID --replicas=0 ncn-mw# kubectl -n services scale CFS-BATCHER_POD_ID --replicas=1 Find the existing session that needs to be deleted.\nncn-mw# cray cfs sessions list Delete the sessions.\nThis step must be done after restarting cfs-batcher. If the cached information is not purged from the batcher first, then the batcher may start additional CFS sessions in response to them being killed. The batcher agent would fight against the user if it is not restarted.\nUnfortunately, it is hard to link a specific BOA session to a CFS session. At this time, they are identified by comparing the CFS timestamps with those of the BOA job, and associating them based on proximity. Additionally, one may examine the components in the CFS job to see that they match the components in the BOA job.\nncn-mw# cray cfs sessions delete CFS_SESSION_NAME Delete the BOA job.\nThe BOA job is not deleted right away because it is needed to find the ConfigMap name.\nncn-mw# kubectl -n services delete job BOA_JOB_NAME "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/configure_a_broker_uai_class/",
	"title": "Configure a Broker UAI Class",
	"tags": [],
	"description": "",
	"content": "Configure a Broker UAI Class Configuring a Broker UAI class consists of the following actions:\nCreate volumes to hold any site-specific authentication, SSH, or other configuration required Choose the End-User UAI class for which the Broker UAI will serve instances Create a UAI Class with (at a minimum): namespace set to uas default set to false volume_mounts set to the list of customization volume-ids created above public_ip set to true uai_compute_network set to false uai_creation_class set to the class-id of the End-User UAI class The basic contents of a Broker UAI Class is discussed in UAI Classes. Familiarity with that information is assumed in the example below.\nExample of Volumes to Connect Broker UAIs to LDAP Broker UAIs authenticate each user using SSH, and pass the SSH connection on to the selected or created End-User UAI for that user. An authentication source is required to authenticate users. For sites that use LDAP as a directory server for authentication, connecting Broker UAIs to LDAP is simply a matter of replicating the LDAP configuration used by other nodes or systems at the site (UANs can be a good source of this configuration) inside the Broker UAI. This section shows how to do that using volumes, which permits the standard Broker UAI image to be used out of the box and reconfigured at the site without direct modification.\nWhile it would be possible to make the configuration available as files volume mounted from the host node of the Broker UAI, this is difficult to set up and maintain because it means that the configuration files must be present and synchronized across all UAI host nodes. A more practical approach to this is to install the configuration files in Kubernetes as secrets, and then mount them from Kubernetes directly. This ensures that no matter where a Broker UAI runs, it has access to the configuration.\nThis example uses Kubernetes secrets and assumes that the Broker UAIs run in the uas Kubernetes namespace. If a different namespace is used, the creation of the ConfigMaps is different but the contents are the same. Using a namespace other than uas for Broker UAIs has implications beyond secrets and ConfigMaps; it is not recommended and is beyond the scope of this document.\nConfigure LDAP and determine which files need to be changed in the Broker UAI and what their contents should be.\nIn this example, the file is /etc/sssd/sssd.conf and its contents are representative but sanitized. Substitute your own site specific contents:\n[sssd] config_file_version = 2 services = nss, pam domains = My_DC [nss] filter_users = root filter_groups = root [pam] [domain/My_DC] ldap_search_base=dc=datacenter,dc=mydomain,dc=com ldap_uri=ldap://10.1.1.5,ldap://10.1.2.5 id_provider = ldap ldap_tls_reqcert = allow ldap_schema = rfc2307 cache_credentials = True entry_cache_timeout = 60 enumerate = False Add the content from the previous step to a secret.\nCreate a file with the appropriate content.\nncn-m001-pit# cat \u0026lt;\u0026lt;EOF \u0026gt; sssd.conf [sssd] config_file_version = 2 services = nss, pam domains = My_DC [nss] filter_users = root filter_groups = root [pam] [domain/My_DC] ldap_search_base=dc=datacenter,dc=mydomain,dc=com ldap_uri=ldap://10.1.1.5,ldap://10.1.2.5 id_provider = ldap ldap_tls_reqcert = allow ldap_schema = rfc2307 cache_credentials = True entry_cache_timeout = 60 enumerate = False EOF Make a secret from the file.\nncn-m001-pit# kubectl create secret generic -n uas broker-sssd-conf --from-file=sssd.conf Make a volume for the secret in the UAS configuration.\nncn-m001-pit# cray uas admin config volumes create \\ --mount-path /etc/sssd \\ --volume-description \\ \u0026#39;{\u0026#34;secret\u0026#34;: {\u0026#34;secret_name\u0026#34;: \u0026#34;broker-sssd-conf\u0026#34;, \u0026#34;default_mode\u0026#34;: 384}}\u0026#39; \\ --volumename broker-sssd-config Example output:\nmount_path = \u0026#34;/etc/sssd\u0026#34; volume_id = \u0026#34;1ec36af0-d5b6-4ad9-b3e8-755729765d76\u0026#34; volumename = \u0026#34;broker-sssd-config\u0026#34; [volume_description.secret] default_mode = 384 secret_name = \u0026#34;broker-sssd-conf\u0026#34; Two important things to notice here are:\nThe secret is mounted on the directory /etc/sssd not the file /etc/sssd/sssd.conf because Kubernetes does not permit the replacement of an existing regular file with a volume but does allow overriding a directory The value 384 is used here for the default mode of the file instead of 0600, which would be easier to read, because JSON does not accept octal numbers in the leading zero form Make a volume to hold an empty and writable /etc/sssd/conf.d in the Broker UAI:\nncn-m001# cray uas admin config volumes create --mount-path /etc/sssd/conf.d --volume-description \u0026#39;{\u0026#34;empty_dir\u0026#34;: {\u0026#34;medium\u0026#34;: \u0026#34;Memory\u0026#34;}}\u0026#39; --volumename sssd-conf-d --format yaml Example output:\nmount_path: /etc/sssd/conf.d volume_description: empty_dir: medium: Memory volume_id: 541980f9-fadc-41cd-8222-e2ffdb6421c4 volumename: sssd-conf-d Obtain the information needed to create a UAI class for the Broker UAI containing the updated configuration in the volume list.\nThe image-id of the Broker UAI image, the volume-ids of the volumes to be added to the broker class, and the class-id of the End-User UAI class managed by the broker are required:\nncn-m001-pit# cray uas admin config images list Example output:\n[[results]] default = true image_id = \u0026#34;1996c7f7-ca45-4588-bc41-0422fe2a1c3d\u0026#34; imagename = \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34; [[results]] default = false image_id = \u0026#34;5d2dd6a3-e9d3-43f1-aa3e-b9bf1589217d\u0026#34; imagename = \u0026#34;registry.local/cray/cray-uai-sanity-test:1.2.4\u0026#34; [[results]] default = false image_id = \u0026#34;8f180ddc-37e5-4ead-b261-2b401914a79f\u0026#34; imagename = \u0026#34;registry.local/cray/cray-uai-broker:1.2.4\u0026#34; ncn-m001-pit# cray uas admin config volumes list [[results]] mount_path = \u0026#34;/etc/localtime\u0026#34; volume_id = \u0026#34;11a4a22a-9644-4529-9434-d296eef2dc48\u0026#34; volumename = \u0026#34;timezone\u0026#34; [results.volume_description.host_path] path = \u0026#34;/etc/localtime\u0026#34; type = \u0026#34;FileOrCreate\u0026#34; [[results]] mount_path = \u0026#34;/etc/sssd\u0026#34; volume_id = \u0026#34;1ec36af0-d5b6-4ad9-b3e8-755729765d76\u0026#34; volumename = \u0026#34;broker-sssd-config\u0026#34; [results.volume_description.secret] default_mode = 384 secret_name = \u0026#34;broker-sssd-conf\u0026#34; [[results]] mount_path = \u0026#34;/lus\u0026#34; volume_id = \u0026#34;a3b149fd-c477-41f0-8f8d-bfcee87fdd0a\u0026#34; volumename = \u0026#34;lustre\u0026#34; [results.volume_description.host_path] path = \u0026#34;/lus\u0026#34; type = \u0026#34;DirectoryOrCreate\u0026#34; [[results]] mount_path = \u0026#34;/etc/sssd/conf.d\u0026#34; volume_id = \u0026#34;541980f9-fadc-41cd-8222-e2ffdb6421c4\u0026#34; volumename = \u0026#34;sssd-conf-d\u0026#34; [results.volume_description.empty_dir] medium = \u0026#34;Memory\u0026#34; Create the Broker UAI class with the content retrieved in the previous step.\nncn-m001-pit#cray uas admin config classes create \\ --image-id 8f180ddc-37e5-4ead-b261-2b401914a79f \\ --volume-list 11a4a22a-9644-4529-9434-d296eef2dc48,1ec36af0-d5b6-4ad9-b3e8-755729765d76,a3b149fd-c477-41f0-8f8d-bfcee87fdd0a,541980f9-fadc-41cd-8222-e2ffdb6421c4 \\ --replicas 3 \\ --namespace uas \\ --uai-compute-network no \\ --public-ip yes \\ --comment \u0026#34;UAI broker class\u0026#34; \\ --uai-creation-class bdb4988b-c061-48fa-a005-34f8571b88b4 Example output:\nclass_id = \u0026#34;d764c880-41b8-41e8-bacc-f94f7c5b053d\u0026#34; comment = \u0026#34;UAI broker class\u0026#34; default = false image_id = \u0026#34;8f180ddc-37e5-4ead-b261-2b401914a79f\u0026#34; namespace = \u0026#34;uas\u0026#34; opt_ports = [] priority_class_name = \u0026#34;uai-priority\u0026#34; public_ip = true replicas = 3 uai_compute_network = false uai_creation_class = \u0026#34;bdb4988b-c061-48fa-a005-34f8571b88b4\u0026#34; volume_list = [ \u0026#34;11a4a22a-9644-4529-9434-d296eef2dc48\u0026#34;, \u0026#34;1ec36af0-d5b6-4ad9-b3e8-755729765d76\u0026#34;, \u0026#34;a3b149fd-c477-41f0-8f8d-bfcee87fdd0a\u0026#34;,\u0026#34;541980f9-fadc-41cd-8222-e2ffdb6421c4\u0026#34;] [[volume_mounts]] mount_path = \u0026#34;/etc/localtime\u0026#34; volume_id = \u0026#34;11a4a22a-9644-4529-9434-d296eef2dc48\u0026#34; volumename = \u0026#34;timezone\u0026#34; [volume_mounts.volume_description.host_path] path = \u0026#34;/etc/localtime\u0026#34; type = \u0026#34;FileOrCreate\u0026#34; [[volume_mounts]] mount_path = \u0026#34;/etc/sssd\u0026#34; volume_id = \u0026#34;1ec36af0-d5b6-4ad9-b3e8-755729765d76\u0026#34; volumename = \u0026#34;broker-sssd-config\u0026#34; [volume_mounts.volume_description.secret] default_mode = 384 secret_name = \u0026#34;broker-sssd-conf\u0026#34; [[volume_mounts]] mount_path = \u0026#34;/lus\u0026#34; volume_id = \u0026#34;a3b149fd-c477-41f0-8f8d-bfcee87fdd0a\u0026#34; volumename = \u0026#34;lustre\u0026#34; [volume_mounts.volume_description.host_path] path = \u0026#34;/lus\u0026#34; type = \u0026#34;DirectoryOrCreate\u0026#34; [[results.volume_mounts]] mount_path = \u0026#34;/etc/sssd/conf.d\u0026#34; volume_id = \u0026#34;541980f9-fadc-41cd-8222-e2ffdb6421c4\u0026#34; volumename = \u0026#34;sssd-conf-d\u0026#34; [results.volume_mounts.volume_description.empty_dir] medium = \u0026#34;Memory\u0026#34; [uai_image] default = false image_id = \u0026#34;8f180ddc-37e5-4ead-b261-2b401914a79f\u0026#34; imagename = \u0026#34;registry.local/cray/cray-uai-broker:1.2.4\u0026#34; NOTE: In some versions of UAS, SSSD will not start correctly when customized as described above because /etc/sssd/sssd.conf is mounted with the wrong mode in spite of being configured with the right mode. If SSSD is not working in a Broker UAI, refer to this troubleshooting section.\nTop: User Access Service (UAS)\nNext Topic: Start a Broker UAI\n"
},
{
	"uri": "/docs-csm/en-12/operations/csm_product_management/post_install_customizations/",
	"title": "Post-Install Customizations",
	"tags": [],
	"description": "",
	"content": "Post-Install Customizations Post-install customizations may be needed as systems scale. These customizations also need to persist across future installs or upgrades. Not all resources can be customized post-install; common scenarios are documented in the following sections.\nThe following is a guide for determining where issues may exist, how to adjust the resources, and how to ensure the changes will persist. Different values may be be needed for systems as they scale.\nSystem domain name kubectl events OOMKilled Prometheus CPUThrottlingHigh alerts Grafana \u0026ldquo;Kubernetes / Compute Resources / Pod\u0026rdquo; dashboard CPU throttling Memory usage Common customization scenarios Prerequisites Prometheus pod is OOMKilled or CPU throttled Postgres pods are OOMKilled or CPU throttled Scale cray-bss service Postgres PVC resize cray-hms-hmcollector pods are OOMKilled References System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system\u0026rsquo;s fully qualified domain name (FQDN).\nThe FQDN can be found by running the following command on any Kubernetes NCN.\nncn-mw# kubectl get secret site-init -n loftsman -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d | yq r - spec.network.dns.external Example output:\nsystem.hpc.amslabs.hpecorp.net Be sure to modify the example URLs on this page by replacing SYSTEM_DOMAIN_NAME with the actual value found using the above command.\nkubectl events OOMKilled Check to see if there are any recent out of memory events.\nCheck kubectl events to see if there are any recent out of memory events.\nncn-mw# kubectl get event -A | grep OOM Log in to Grafana at the following URL: https://grafana.cmn.SYSTEM_DOMAIN_NAME/\nSearch for the \u0026ldquo;Kubernetes / Compute Resources / Pod\u0026rdquo; dashboard to view the memory utilization graphs over time for any pod that has been OOMKilled.\nPrometheus CPUThrottlingHigh alerts Check Prometheus for recent CPUThrottlingHigh alerts.\nLog in to Prometheus at the following URL: https://prometheus.cmn.SYSTEM_DOMAIN_NAME/\nSelect the Alert tab.\nScroll down to the alert for CPUThrottlingHigh.\nLog in to Grafana at the following URL: https://grafana.cmn.SYSTEM_DOMAIN_NAME/\nSearch for the \u0026ldquo;Kubernetes / Compute Resources / Pod\u0026rdquo; dashboard to view the throttling graphs over time for any pod that is alerting. Grafana \u0026ldquo;Kubernetes / Compute Resources / Pod\u0026rdquo; dashboard Use Grafana to investigate and analyze CPU throttling and memory usage.\nLog in to Grafana at the following URL: https://grafana.cmn.SYSTEM_DOMAIN_NAME/\nSearch for the \u0026ldquo;Kubernetes / Compute Resources / Pod\u0026rdquo; dashboard.\nSelect the datasource, namespace, and pod based on the pod being examined.\nFor example:\ndatasource: default namespace: sysmgmt-health pod: prometheus-cray-sysmgmt-health-promet-prometheus-0 CPU throttling Select the CPU Throttling drop-down to see the CPU Throttling graph for the pod during the selected time (from the top right).\nSelect the container (from the legends under the x axis).\nReview the graph and adjust the resources.limits.cpu value as needed.\nThe presence of CPU throttling does not always indicate a problem, but if a service is being slow or experiencing latency issues, adjusting resources.limits.cpu may be beneficial.\nFor example:\nIf the pod is being throttled at or near 100% for any period of time, then adjustments are likely needed. If the service\u0026rsquo;s response time is critical, then adjusting the pod\u0026rsquo;s resources to greatly reduce or eliminate any CPU throttling may be required. NOTE: The resources.requests.cpu values are used by the Kubernetes scheduler to decide which node to place the pod on and do not impact CPU throttling. The value of resources.limits.cpu can never be lower than the value of resources.requests.cpu.\nMemory usage Select the Memory Usage drop-down to see the memory usage graph for the pod during the selected time (from the top right).\nSelect the container (from the legends under the x axis).\nDetermine the steady state memory usage by looking at the memory usage graph for the container.\nThis is where the resources.requests.memory value should be minimally set. But more importantly, determine the spike usage for the container and set the resources.limits.memory value based on the spike values with some additional headroom.\nCommon customization scenarios Prerequisites Prometheus pod is OOMKilled or CPU throttled Postgres pods are OOMKilled or CPU throttled Scale cray-bss service Postgres PVC resize cray-hms-hmcollector pods are OOMKilled References Prerequisites Most of these procedures instruct the administrator to perform the Redeploying a Chart procedure for a specific chart. In these cases, the section on this page provides the administrator with the information necessary in order to carry out that procedure. It is recommended to keep both pages open in different browser windows for easy reference.\nPrometheus pod is OOMKilled or CPU throttled Update resources associated with Prometheus in the sysmgmt-health namespace. This example is based on what was needed for a system with 4000 compute nodes. Trial and error may be needed to determine what is best for a given system at scale.\nFollow the Redeploying a Chart procedure with the following specifications:\nChart name: cray-sysmgmt-health\nBase manifest name: platform\nWhen reaching the step to update the customizations, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nEdit the customizations by adding or updating spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources.\nIf the number of NCNs is less than 20, then:\nncn-mw# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources.requests.cpu\u0026#39; --style=double \u0026#39;2\u0026#39; ncn-mw# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources.requests.memory\u0026#39; \u0026#39;15Gi\u0026#39; ncn-mw# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources.limits.cpu\u0026#39; --style=double \u0026#39;6\u0026#39; ncn-mw# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources.limits.memory\u0026#39; \u0026#39;30Gi\u0026#39; If the number of NCNs is 20 or more, then:\nncn-mw# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources.requests.cpu\u0026#39; --style=double \u0026#39;6\u0026#39; ncn-mw# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources.requests.memory\u0026#39; \u0026#39;50Gi\u0026#39; ncn-mw# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources.limits.cpu\u0026#39; --style=double \u0026#39;12\u0026#39; ncn-mw# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources.limits.memory\u0026#39; \u0026#39;60Gi\u0026#39; Check that the customization file has been updated.\nncn-mw# yq read customizations.yaml \u0026#39;spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources\u0026#39; Example output:\nrequests: cpu: \u0026#34;3\u0026#34; memory: 15Gi limits: cpu: \u0026#34;6\u0026#34; memory: 30Gi When reaching the step to validate the redeployed chart, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nVerify that the pod restarts and that the desired resources have been applied.\nWatch the prometheus-cray-sysmgmt-health-promet-prometheus-0 pod restart.\nncn-mw# watch \u0026#34;kubectl get pods -n sysmgmt-health -l prometheus=cray-sysmgmt-health-promet-prometheus\u0026#34; It may take about 10 minutes for the prometheus-cray-sysmgmt-health-promet-prometheus-0 pod to terminate. It can be forced deleted if it remains in the terminating state:\nncn-mw# kubectl delete pod prometheus-cray-sysmgmt-health-promet-prometheus-0 --force --grace-period=0 -n sysmgmt-health Verify that the resource changes are in place.\nncn-mw# kubectl get pod prometheus-cray-sysmgmt-health-promet-prometheus-0 -n sysmgmt-health -o json | jq -r \u0026#39;.spec.containers[] | select(.name == \u0026#34;prometheus\u0026#34;).resources\u0026#39; Make sure to perform the entire linked procedure, including the step to save the updated customizations.\nPostgres pods are OOMKilled or CPU throttled Update resources associated with spire-postgres in the spire namespace. This example is based on what was needed for a system with 4000 compute nodes. Trial and error may be needed to determine what is best for a given system at scale.\nA similar flow can be used to update the resources for cray-sls-postgres, cray-smd-postgres, or gitea-vcs-postgres.\nThe following table provides values the administrator will need based on which pods are experiencing problems.\nChart name Base manifest name Resource path name Kubernetes namespace cray-sls-postgres core-services cray-hms-sls services cray-smd-postgres core-services cray-hms-smd services gitea-vcs-postgres sysmgmt gitea services spire-postgres sysmgmt spire spire Using the values from the above table, follow the Redeploying a Chart with the following specifications:\nWhen reaching the step to update the customizations, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nSet the rpname variable to the appropriate resource path name from the table above.\nncn-mw# rpname=\u0026lt;put resource path name from table here\u0026gt; Edit the customizations by adding or updating spec.kubernetes.services.${rpname}.cray-service.sqlCluster.resources.\nncn-mw# yq write -i customizations.yaml \u0026#34;spec.kubernetes.services.${rpname}.cray-service.sqlCluster.resources.requests.cpu\u0026#34; --style=double \u0026#39;4\u0026#39; ncn-mw# yq write -i customizations.yaml \u0026#34;spec.kubernetes.services.${rpname}.cray-service.sqlCluster.resources.requests.memory\u0026#34; \u0026#39;4Gi\u0026#39; ncn-mw# yq write -i customizations.yaml \u0026#34;spec.kubernetes.services.${rpname}.cray-service.sqlCluster.resources.limits.cpu\u0026#34; --style=double \u0026#39;8\u0026#39; ncn-mw# yq write -i customizations.yaml \u0026#34;spec.kubernetes.services.${rpname}.cray-service.sqlCluster.resources.limits.memory\u0026#34; \u0026#39;8Gi\u0026#39; Check that the customization file has been updated.\nncn-mw# yq read customizations.yaml \u0026#34;spec.kubernetes.services.${rpname}.cray-service.sqlCluster.resources\u0026#34; Example output:\nrequests: cpu: \u0026#34;4\u0026#34; memory: 4Gi limits: cpu: \u0026#34;8\u0026#34; memory: 8Gi When reaching the step to validate the redeployed chart, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nVerify that the pods restart and that the desired resources have been applied. Commands in this section use the $CHART_NAME variable which should have been set as part of the Redeploying a Chart procedure.\nSet the ns variable to the name of the appropriate Kubernetes namespace from the earlier table.\nncn-mw# ns=\u0026lt;put kubernetes namespace here\u0026gt; Watch the pod restart.\nncn-mw# watch \u0026#34;kubectl get pods -n ${ns} -l application=spilo,cluster-name=${CHART_NAME}\u0026#34; Verify that the desired resources have been applied.\nncn-mw# kubectl get pod ${CHART_NAME}-0 -n \u0026#34;${ns}\u0026#34; -o json | jq -r \u0026#39;.spec.containers[] | select(.name == \u0026#34;postgres\u0026#34;).resources\u0026#39; Example output:\n{ \u0026#34;limits\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;8\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;8Gi\u0026#34; }, \u0026#34;requests\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;4Gi\u0026#34; } } Make sure to perform the entire linked procedure, including the step to save the updated customizations.\nScale cray-bss service Scale the replica count associated with the cray-bss service in the services namespace. This example is based on what was needed for a system with 4000 compute nodes. Trial and error may be needed to determine what is best for a given system at scale.\nFollow the Redeploying a Chart procedure with the following specifications:\nChart name: cray-hms-bss\nBase manifest name: sysmgmt\nWhen reaching the step to update the customizations, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nEdit the customizations by adding or updating spec.kubernetes.services.cray-hms-bss.cray-service.replicaCount.\nncn-mw# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-hms-bss.cray-service.replicaCount\u0026#39; \u0026#39;5\u0026#39; Check that the customization file has been updated.\nncn-mw# yq read customizations.yaml \u0026#39;spec.kubernetes.services.cray-hms-bss.cray-service.replicaCount\u0026#39; Example output:\n5 When reaching the step to validate the redeployed chart, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nVerify the cray-bss pods scale.\nWatch the cray-bss pods scale to the desired number (in this example, 5), with each pod reaching a 2/2 ready state.\nncn-mw# watch \u0026#34;kubectl get pods -l app.kubernetes.io/instance=cray-hms-bss -n services\u0026#34; Example output:\nNAME READY STATUS RESTARTS AGE cray-bss-fccbc9f7d-7jw2q 2/2 Running 0 82m cray-bss-fccbc9f7d-l524g 2/2 Running 0 93s cray-bss-fccbc9f7d-qwzst 2/2 Running 0 93s cray-bss-fccbc9f7d-sw48b 2/2 Running 0 82m cray-bss-fccbc9f7d-xr26l 2/2 Running 0 82m Verify that the replicas change is present in the Kubernetes cray-bss deployment.\nncn-mw# kubectl get deployment cray-bss -n services -o json | jq -r \u0026#39;.spec.replicas\u0026#39; In this example, 5 will be the returned value.\nMake sure to perform the entire linked procedure, including the step to save the updated customizations.\nPostgres PVC resize Increase the PVC volume size associated with cray-smd-postgres cluster in the services namespace. This example is based on what was needed for a system with 4000 compute nodes. Trial and error may be needed to determine what is best for a given system at scale. The PVC size can only ever be increased.\nA similar flow can be used to update the resources for cray-sls-postgres, gitea-vcs-postgres, or spire-postgres.\nThe following table provides values the administrator will need based on which pods are experiencing problems.\nChart name Base manifest name Resource path name Kubernetes namespace cray-sls-postgres core-services cray-hms-sls services cray-smd-postgres core-services cray-hms-smd services gitea-vcs-postgres sysmgmt gitea services spire-postgres sysmgmt spire spire Using the values from the above table, follow the Redeploying a Chart with the following specifications:\nWhen reaching the step to update the customizations, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nSet the rpname variable to the appropriate resource path name from the table above.\nncn-mw# rpname=\u0026lt;put resource path name from table here\u0026gt; Edit the customizations by adding or updating spec.kubernetes.services.${rpname}.cray-service.sqlCluster.volumeSize.\nncn-mw# yq write -i customizations.yaml \u0026#34;spec.kubernetes.services.${rpname}.cray-service.sqlCluster.volumeSize\u0026#34; \u0026#39;100Gi\u0026#39; Check that the customization file has been updated.\nncn-mw# yq read customizations.yaml \u0026#34;spec.kubernetes.services.${rpname}.cray-service.sqlCluster.volumeSize\u0026#34; Example output:\n100Gi When reaching the step to validate the redeployed chart, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nVerify that the pods restart and that the desired resources have been applied. Commands in this section use the $CHART_NAME variable which should have been set as part of the Redeploying a Chart procedure.\nSet the ns variable to the name of the appropriate Kubernetes namespace from the earlier table.\nncn-mw# ns=\u0026lt;put kubernetes namespace here\u0026gt; Verify that the increased volume size has been applied.\nncn-mw# watch \u0026#34;kubectl get postgresql ${CHART_NAME} -n $ns\u0026#34; Example output:\nNAME TEAM VERSION PODS VOLUME CPU-REQUEST MEMORY-REQUEST AGE STATUS cray-smd-postgres cray-smd 11 3 100Gi 500m 8Gi 45m Running If the status on the above command is SyncFailed instead of Running, refer to Case 1 in the SyncFailed section of Troubleshoot Postgres Database.\nAt this point the Postgres cluster is healthy, but additional steps are required to complete the resize of the Postgres PVCs.\nMake sure to perform the entire linked procedure, including the step to save the updated customizations.\ncray-hms-hmcollector pods are OOMKilled Update resources associated with cray-hms-hmcollector in the services namespace. Trial and error may be needed to determine what is best for a given system at scale. See Adjust HM Collector Ingress Replicas and Resource Limits.\nReferences To make changes that will not persist across installs or upgrades, see the following references. These procedures will also help to verify and eliminate any issues in the short term. As other resource customizations are needed, contact support to request the feature.\nDetermine if Pods are Hitting Resource Limits Increase Pod Resource Limits "
},
{
	"uri": "/docs-csm/en-12/install/collect_mac_addresses_for_ncns/",
	"title": "Collect MAC Addresses for NCNs",
	"tags": [],
	"description": "",
	"content": "Collect MAC Addresses for NCNs Now that the PIT node has been booted with the LiveCD and the management network switches have been configured, the actual MAC addresses for the management nodes can be collected. This process will include repetition of some of the steps done up to this point because csi config init will need to be run with the proper MAC addresses and some services will need to be restarted.\nNote: If an install or reinstall of the CSM software release is being done on this system and the ncn_metadata.csv file already has valid MAC addresses for both BMC and node interfaces before csi config init was run, then skip this topic and move to Deploy Management Nodes.\nTopics Collect BMC MAC addresses Restart services after BMC MAC addresses collected Collect NCN MAC addresses Restart services after NCN MAC addresses collected Next topic 1. Collect BMC MAC addresses The BMC MAC addresses can be collected from the switches using knowledge about the cabling of the NMN from the SHCD.\nSee Collecting BMC MAC Addresses.\n2. Restart services after BMC MAC addresses collected The previous step updated ncn_metadata.csv with the BMC MAC addresses, so several earlier steps need to be repeated.\nChange into the preparation directory.\npit# cd /var/www/ephemeral/prep Confirm that the ncn_metadata.csv file in this directory has the new information. There should be no remaining dummy data (de:ad:be:ef:00:00) for the BMC MAC column in the file, but that string may be present for the Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1 columns.\npit# cat ncn_metadata.csv Rename the incorrectly generated configurations.\nBefore deleting the incorrectly generated configurations, consider making a backup of them, in case they need to be examined at a later time.\nWarning: Ensure that the SYSTEM_NAME environment variable is correctly set.\npit# export SYSTEM_NAME=eniac pit# echo $SYSTEM_NAME Rename the old directory.\npit# mv /var/www/ephemeral/prep/${SYSTEM_NAME} /var/www/ephemeral/prep/${SYSTEM_NAME}.oldBMC Copy over the system_config.yaml file from the first attempt at generating the system configuration files.\npit# cp /var/www/ephemeral/prep/${SYSTEM_NAME}.oldBMC/system_config.yaml /var/www/ephemeral/prep/ Generate system configuration again.\nThe needed files should be in the current directory.\npit# ls -1 Expected output looks similar to the following:\napplication_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml The system_config.yaml file will make it easier to run the next command because it has the saved information from the command line arguments which were used initially for this command.\npit# csi config init A new directory matching your --system-name argument will now exist in your working directory.\nThese warnings from csi config init for issues in hmn_connections.json can be ignored.\nThe node with the external connection (ncn-m001) will have a warning similar to this because its BMC is connected to the site and not the HMN like the other management NCNs. It can be ignored.\n\u0026#34;Couldn\u0026#39;t find switch port for NCN: x3000c0s1b0\u0026#34; An unexpected component may have this message. If this component is an application node with an unusual prefix, it should be added to the application_node_config.yaml file. Then rerun csi config init. See the procedure to Create Application Node Config YAML.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1610405168.8705149,\u0026#34;msg\u0026#34;:\u0026#34;Found unknown source prefix! If this is expected to be an Application node, please update application_node_config.yaml\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u33\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3002\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u48\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;}} If a cooling door is found in hmn_connections.json, there may be a message like the following. It can be safely ignored.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1612552159.2962296,\u0026#34;msg\u0026#34;:\u0026#34;Cooling door found, but xname does not yet exist for cooling doors!\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;}} Copy the interface configuration files generated earlier by csi config init into /etc/sysconfig/network/.\npit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/pit-files/* /etc/sysconfig/network/ \u0026amp;\u0026amp; wicked ifreload all \u0026amp;\u0026amp; systemctl restart wickedd-nanny \u0026amp;\u0026amp; sleep 5 Check that IP addresses are set for each interface and investigate any failures.\nNote: The bond0.can0 interface is optional in CSM 1.2+\npit# wicked show bond0 bond0.nmn0 bond0.hmn0 bond0.can0 Example output:\nbond0 up link: #7, state up, mtu 1500 type: bond, mode ieee802-3ad, hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-bond0 leases: ipv4 static granted addr: ipv4 10.1.1.2/16 [static] bond0.nmn0 up link: #8, state up, mtu 1500 type: vlan bond0[2], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-bond0.nmn0 leases: ipv4 static granted addr: ipv4 10.252.1.4/17 [static] route: ipv4 10.92.100.0/24 via 10.252.0.1 proto boot bond0.can0 up link: #9, state up, mtu 1500 type: vlan bond0[7], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-bond0.can0 leases: ipv4 static granted addr: ipv4 10.102.9.5/24 [static] bond0.hmn0 up link: #10, state up, mtu 1500 type: vlan bond0[4], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-bond0.hmn0 leases: ipv4 static granted addr: ipv4 10.254.1.4/17 [static] Copy the service configuration files generated earlier by csi config init for dnsmasq, Metal Basecamp (cloud-init), and ConMan.\nCopy files (files only, -r is expressly not used).\npit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/dnsmasq.d/* /etc/dnsmasq.d/ \u0026amp;\u0026amp; cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/conman.conf /etc/conman.conf \u0026amp;\u0026amp; cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/basecamp/* /var/www/ephemeral/configs/ Restart all PIT services.\npit# systemctl restart basecamp nexus dnsmasq conman Verify that all BMCs can be pinged.\nNote: It may take about 10 minutes from when dnsmasq is restarted to when the BMCs pick up new DHCP leases.\nThis step will check all management nodes except ncn-m001-mgmt because that has an external connection and could not be booted by itself as the PIT node.\npit# export mtoken=\u0026#39;ncn-m(?!001)\\w+-mgmt\u0026#39; ; export stoken=\u0026#39;ncn-s\\w+-mgmt\u0026#39; ; export wtoken=\u0026#39;ncn-w\\w+-mgmt\u0026#39; pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ping -c3 {} 3. Collect NCN MAC addresses Now that the BMC MAC addresses are correct in ncn_metadata.csv and the PIT node services have been restarted, a partial boot of the management nodes can be done to collect the remaining information from the conman console logs on the PIT node using the Procedure: iPXE Consoles\nSee Procedure: iPXE Consoles.\n4. Restart services after NCN MAC addresses collected The previous step updated ncn_metadata.csv with the NCN MAC addresses for Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1 so several earlier steps need to be repeated.\nChange into the preparation directory.\npit# cd /var/www/ephemeral/prep Confirm that the ncn_metadata.csv file in this directory has the new information. There should be no remaining dummy data (\u0026quot;de:ad:be:ef:00:00\u0026quot;) for columns or rows in the file. Every row should have uniquely different MAC addresses from the other rows.\npit# grep \u0026#34;de:ad:be:ef:00:00\u0026#34; ncn_metadata.csv Expected output looks similar to the following, that is, no lines that still have \u0026quot;de:ad:be:ef:00:00\u0026quot;:\nDisplay the file and confirm the contents are unique between the different rows.\npit# cat ncn_metadata.csv Remove the incorrectly generated configurations. Before deleting the incorrectly generated configurations, consider making a backup of them, in case they need to be examined at a later time.\nWARNING Ensure that the SYSTEM_NAME environment variable is correctly set.\npit# export SYSTEM_NAME=eniac pit# echo $SYSTEM_NAME Rename the old directory.\npit# mv /var/www/ephemeral/prep/${SYSTEM_NAME} /var/www/ephemeral/prep/${SYSTEM_NAME}.oldNCN Copy over the system_config.yaml file from the second attempt at generating the system configuration files.\npit# cp /var/www/ephemeral/prep/${SYSTEM_NAME}.oldNCN/system_config.yaml /var/www/ephemeral/prep/ Generate system configuration again.\nCheck for the expected files that should exist be in the current directory.\npit# ls -1 Expected output looks similar to the following:\napplication_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml Regenerate the system configuration. The system_config.yaml file contains all of the options that were used to generate the initial system configuration, and can be used in place of specifying CLI flags to CSI.\npit# csi config init A new directory matching your $SYSTEM_NAME environment variable will now exist in your working directory.\nThese warnings from csi config init for issues in hmn_connections.json can be ignored.\nThe node with the external connection (ncn-m001) will have a warning similar to this because its BMC is connected to the site and not the HMN like the other management NCNs. It can be ignored.\n\u0026#34;Couldn\u0026#39;t find switch port for NCN: x3000c0s1b0\u0026#34; An unexpected component may have this message. If this component is an application node with an unusual prefix, it should be added to the application_node_config.yaml file. Then rerun csi config init. See the procedure to Create Application Node Config YAML.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1610405168.8705149,\u0026#34;msg\u0026#34;:\u0026#34;Found unknown source prefix! If this is expected to be an Application node, please update application_node_config.yaml\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u33\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3002\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u48\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;}} If a cooling door is found in hmn_connections.json, there may be a message like the following. It can be safely ignored.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1612552159.2962296,\u0026#34;msg\u0026#34;:\u0026#34;Cooling door found, but xname does not yet exist for cooling doors!\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;}} Copy the interface configuration files generated earlier by csi config init into /etc/sysconfig/network/.\npit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/pit-files/* /etc/sysconfig/network/ \u0026amp;\u0026amp; wicked ifreload all \u0026amp;\u0026amp; systemctl restart wickedd-nanny \u0026amp;\u0026amp; sleep 5 Check that IP addresses are set for each interface and investigate any failures.\nNote: The bond0.can0 interface is optional in CSM 1.2+\npit# wicked show bond0 bond0.nmn0 bond0.hmn0 bond0.can0 Example output:\nbond0 up link: #7, state up, mtu 1500 type: bond, mode ieee802-3ad, hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-bond0 leases: ipv4 static granted addr: ipv4 10.1.1.2/16 [static] bond0.nmn0 up link: #8, state up, mtu 1500 type: vlan bond0[2], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-bond0.nmn0 leases: ipv4 static granted addr: ipv4 10.252.1.4/17 [static] route: ipv4 10.92.100.0/24 via 10.252.0.1 proto boot bond0.can0 up link: #9, state up, mtu 1500 type: vlan bond0[7], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-bond0.can0 leases: ipv4 static granted addr: ipv4 10.102.9.5/24 [static] bond0.hmn0 up link: #10, state up, mtu 1500 type: vlan bond0[4], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-bond0.hmn0 leases: ipv4 static granted addr: ipv4 10.254.1.4/17 [static] Copy the service configuration files generated earlier by csi config init for dnsmasq, Metal Basecamp (cloud-init), and ConMan.\nCopy files (files only, -r is expressly not used).\npit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/dnsmasq.d/* /etc/dnsmasq.d/ pit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/conman.conf /etc/conman.conf pit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/basecamp/* /var/www/ephemeral/configs/ Update CA Cert on the copied data.json file for Basecamp with the generated certificate in site-init:\npit# csi patch ca \\ --cloud-init-seed-file /var/www/ephemeral/configs/data.json \\ --customizations-file /var/www/ephemeral/prep/site-init/customizations.yaml \\ --sealed-secret-key-file /var/www/ephemeral/prep/site-init/certs/sealed_secrets.key Restart all PIT services.\npit# systemctl restart basecamp nexus dnsmasq conman Ensure system-specific settings generated by CSI are merged into customizations.yaml.\nThe yq tool used in the following procedures is available under /var/www/ephemeral/prep/site-init/utils/bin once the SHASTA-CFG repository has been cloned.\npit# alias yq=\u0026#34;/var/www/ephemeral/prep/site-init/utils/bin/$(uname | awk \u0026#39;{print tolower($0)}\u0026#39;)/yq\u0026#34; pit# yq merge -xP -i /var/www/ephemeral/prep/site-init/customizations.yaml \u0026lt;(yq prefix -P \u0026#34;/var/www/ephemeral/prep/${SYSTEM_NAME}/customizations.yaml\u0026#34; spec) Next topic After completing the collection of BMC MAC addresses and NCN MAC addresses in order to update ncn_metadata.csv, and after restarting the services dependent on correct data in ncn_metadata.csv, the next step is deployment of the management nodes.\nSee Deploy Management Nodes\n"
},
{
	"uri": "/docs-csm/en-12/background/ncn_networking/",
	"title": "NCN Networking",
	"tags": [],
	"description": "",
	"content": "NCN Networking Non-compute nodes (NCNs) and compute nodes (CNs) have different network interfaces used for booting; this topic focuses on the network interfaces for management NCNs.\nNCN network interfaces Device naming Vendor and bus ID identification NCN network interfaces The following table includes information about the different NCN network interfaces:\nName Type MTU mgmt0 Port 1 Slot 1 on the SMNET card 9000 mgmt1 Port 1 Slot 2 on the SMNET card 9000 bond0 LACP link aggregate of mgmt0 and mgmt1 9000 bond0.nmn0 Virtual LAN for managing nodes 1500 bond0.hmn0 Virtual LAN for managing hardware 1500 bond0.can0 Virtual LAN for the Customer Access Network 1500 sun0 Port 2 Slot 2 on the SMNET card 9000 sun1 Port 2 Slot 2 on the SMNET card 9000 bond1 LACP link aggregate of sun0 and sun1 9000 bond1.sun0 Virtual LAN for the Storage Utility Network 9000 lan0 Externally facing interface (DHCP) 1500 lan1 Another externally facing interface, or anything (unused) 1500 hsn0 High-Speed Network interface 9000 hsnN+1 Another High-Speed Network interface 9000 These interfaces can be observed on an NCN with the following command.\nncn# ip link Device naming The underlying naming relies on biosdevname. This helps conform device naming into a smaller set of possible names. It also helps reveal when driver issues occur; if an administrator observes an interface with name that does not conform to this naming scheme, then this is an indication of a problem.\nThe MAC-based udev rules set the interfaces during initial boot in iPXE. When a node boots, iPXE will dump the PCI buses and sort network interfaces into 3 buckets:\nmgmt: internal/management network connection sun: internal/storage network connection hsn: high-speed connection lan: external/site connection The source code for the rule generation is in metal-ipxe. Continue reading for technical information on the PCI configuration/reading.\nVendor and bus ID identification The initial boot of an NCN sets interface udev rules because it has no discovery method yet.\nThe information needed is:\nPCI Vendor IDs for devices/cards to be used on the Management network. PCI Device IDs for the devices/cards to be used on the High-Speed Network. The 16-bit vendor ID is allocated by the PCI-SIG (Peripheral Component Interconnect Special Interest Group).\nThe information belongs to the first 4 bytes of the PCI header, and administrators can obtain it by reading the PCI bus (for example, by using the lspci command).\nncn# lspci | grep -i ethernet ncn# lspci | grep c6:00.0 The device and vendor IDs are used in iPXE for bootstrapping the nodes. This allows generators to swap IDs out for certain systems until smarter logic can be added to cloud-init.\nThe following table includes popular vendor and device IDs.\nThe numbers in bold are the defaults in metal-ipxe\u0026rsquo;s boot script.\nVendor Model Device ID Vendor ID Intel Corporation Ethernet Connection X722 37d2 8086 Intel Corporation 82576 1526 8086 Mellanox Technologies ConnectX-4 1013 15b3 Mellanox Technologies ConnectX-5 1017 15b3 Giga-Byte Intel Corporation I350 1521 8086 QLogic Corporation FastLinQ QL41000 8070 1077 "
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/plan_and_coordinate_network_upgrade/",
	"title": "Plan and coordinate network upgrade",
	"tags": [],
	"description": "",
	"content": "Plan and coordinate network upgrade Prior to CSM 1.2, a single Customer Access Network (CAN) carried both the administrative network traffic and the user network traffic. CSM 1.2 introduces bifurcated CAN (BICAN), which is designed to separate administrative network traffic and user network traffic. With BICAN, the pre-1.2 CAN network is split into two separate networks:\nCustomer Management Network (CMN)\nThis network allows only system administrative access from the customer site. The pre-1.2 CAN is renamed to CMN. By the end of the CSM 1.2 upgrade, all non-administrative access, such as from UANs, will be removed from CMN.\nDuring the CSM 1.2 upgrade, UANs will retain their pre-1.2 CAN IP addresses in order to minimize disruption to UANs. However, toward the end of the CSM 1.2 upgrade, UANs will stop registering themselves on CMN and will receive new IP addresses on the CAN/CHN network. This process is described in more detail in UAN Migration.\nPivoting the pre-1.2 CAN to the new CMN allows administrative traffic (already on the pre-1.2 CAN) to remain as-is while moving standard user traffic to a new site-routable network (CAN / CHN).\nCustomer Access Network (CAN) / Customer High-speed Network (CHN)\nFor user traffic only (e.g. users running and monitoring jobs), CSM 1.2 allows a choice of one of two networks:\nCustomer Access Network (CAN) [Recommended]: this is a new network (VLAN6 in switches) that runs over the management network. This network must not be confused with pre-1.2 CAN, which was a monolithic network that allowed both user and administrative traffic, was configured as VLAN7 in switches, and is now renamed to CMN. The new CAN allows only user traffic.\nCustomer High-speed Network (CHN) [CSM 1.2 Tech Preview]: this is a new network (VLAN5 in switches) that runs over the high-speed fabric.\nEither the new CAN or CHN must be chosen, but not both. Note that the CHN is a technical preview in CSM 1.2, and the new CAN is the recommended upgrade. The rest of the upgrade guide provides options for configuring either the new CAN or CHN.\nUAN migration Steps are taken in order to minimize disruption to UANs during the CSM 1.2 upgrade process. Read these steps carefully and follow any recommendations and warnings to minimize disruptions to user activity. Note that these steps apply to all types of application nodes and not just UANs \u0026ndash; the term \u0026ldquo;UAN\u0026rdquo; just happens to be more commonly used and understood when referring to user activity.\nDuring the upgrade, the switch 1.2 Preconfig will not remove UAN ports from the CMN VLAN (the pre-1.2 CAN), allowing UANs to retain their existing IP addresses during the CSM 1.2 upgrade process. Traffic to and from UANs will still flow through CMN, but may also flow through CAN/CHN networks, if desired.\nCFS will be temporarily disabled for UANs, in order to prevent CFS plays from removing CMN interfaces from UANs. Note that network configuration is controlled by data in SLS but CFS plays also pick up the same SLS data, which can lead to UANs being prematurely removed from the CMN and causing UAN outages. As such, CFS plays need to be disabled for UANs.\nTo disable CFS plays for UANs, remove CFS assignment for UANs by running the following command:\nncn-m001# for xname in $(cray hsm state components list --role Application --subrole UAN --type node --format json | jq -r .Components[].ID) ; do cray cfs components update --enabled false --desired-config \u0026#34;\u0026#34; --format json $xname done Note that the above command will disable CFS plays for UANs only. If wishing to disable CFS plays for all types of application nodes (recommended), then remove the --subrole UAN portion in the snippet above.\nUAN reboots must be avoided and are not a supported operation during the CSM 1.2 upgrade. Rebooting a UAN during a CSM 1.2 upgrade can re-enable CFS and ultimately lead to removing the CMN interface from UANs, disrupting UAN access for users. System administrators must inform users to avoid UAN reboots during the CSM 1.2 upgrade process.\nHowever, if a UAN is rebooted, then the roles/uan_interfaces/tasks/can-v2.yml file in the vcs/cray/uan-config-management.git repository must be patched for the current CSM release. The UAN must then be rebooted again to bring the CMN (pre-1.2 CAN) interface back in the UAN. Use the following patch file and follow the instructions in Configuration Management to restore CMN access in the UAN.\n--- a/roles/uan_interfaces/tasks/can-v2.yml +++ b/roles/uan_interfaces/tasks/can-v2.yml @@ -33,21 +33,16 @@ - name: Get Customer Access Network info from SLS local_action: module: uri - url: \u0026#34;http://cray-sls/v1/search/networks?name={{ sls_can_name }}\u0026#34; + url: \u0026#34;http://cray-sls/v1/search/networks?name=CMN\u0026#34; method: GET register: sls_can -- name: Get Customer Access Network CIDR from SLS, if network exists. - # This assumes that the CAN network is _always_ the third item in the array. This makes the - # implementation fragile. See CASMCMS-6714. - set_fact: - customer_access_network: \u0026#34;{{ sls_can.json[0].ExtraProperties.Subnets[2].CIDR }}\u0026#34; - when: sls_can.status == 200 - -- name: Get Customer Access Network Gateway from SLS, if network exists - set_fact: - customer_access_gateway: \u0026#34;{{ sls_can.json[0].ExtraProperties.Subnets[2].Gateway }}\u0026#34; - when: sls_can.status == 200 +- name: \u0026#34;Get {{ uan_user_access_cfg | upper }} CIDR from SLS, if network exists.\u0026#34; + set_fact: + customer_access_network: \u0026#34;{{ item.CIDR }}\u0026#34; + customer_access_gateway: \u0026#34;{{ item.Gateway }}\u0026#34; + loop: \u0026#34;{{ sls_can.json[0].ExtraProperties.Subnets }}\u0026#34; + when: item.FullName == \u0026#34;CMN Bootstrap DHCP Subnet\u0026#34; Once UAN has been upgraded to 2.4, the UANs may be rebooted for the new network configuration changes to take effect. UANs will not receive an IP address on the CMN network and instead will default their traffic through the new CAN/CHN. For concrete details on UAN transition plan for users, see Minimize UAN Downtime.\nNote that in CSM 1.2, UAN ports will not be removed from the CMN VLAN7 in switches. In the next CSM release, switch configuration will be updated to remove UAN ports from the CMN VLAN7. This enables non-rebooted UANs to continue to work and allows for better easing into BICAN in CSM 1.2. For more details about this transition plan, see Minimize UAN Downtime.\nManually removing UAN switch ports from the CMN VLAN 7 After the upgrade to UAN 2.4, the UAN switch ports should be removed from CMN VLAN 7, to prevent user traffic from being able to reach endpoints on the CMN. In the CSM 1.2 and UAN 2.4 upgrade, this removal is not done automatically. A future release or hotfix to CSM will introduce this automation.\nThe switch configurations can be updated manually to remove VLAN7 from the UAN port configurations. This procedure is currently being tested and will be linked here when finished.\nWatch this page for updates and always use the latest documentation, in order to have the latest procedures. See Check for Latest Documentation for details on obtaining and installing the latest CSM documentation.\nUAI migration Access to UAIs will be disrupted until CSM 1.2 upgrade completes. After the upgrade is completed, UAIs need to be restarted.\nDecide on subnet ranges for new CAN/CHN After deciding whether to use the new CAN or to use CHN for user access, the subnet range must be decided. Refer to Customer Accessible Networks for subnet ranges and defaults for CAN/CHN.\nPreserving CMN subnet range It is vital that the subnet range is preserved for the pre-1.2 CAN that is now being renamed to CMN. Changing the subnet size during the CSM 1.2 upgrade process is unsupported and will break the upgrade.\nChanges to service endpoints With the introduction of BICAN, URLs for certain services are now different, as it is now necessary to include the network path in the fully qualified domain name. Furthermore, certain services are only available on CMN:\nAccess to administrative services is now restricted to the CMN. API access is available via the CMN, new CAN, and CHN. The following table is a set of examples of how domain names of existing services are impacted. It assumes the system was configured with a system-name of shasta and a site-domain of dev.cray.com.\nOld Name New Name auth.shasta.dev.cray.com auth.cmn.shasta.dev.cray.com nexus.shasta.dev.cray.com nexus.cmn.shasta.dev.cray.com grafana.shasta.dev.cray.com grafana.cmn.shasta.dev.cray.com prometheus.shasta.dev.cray.com prometheus.cmn.shasta.dev.cray.com alertmanager.shasta.dev.cray.com alertmanager.cmn.shasta.dev.cray.com vcs.shasta.dev.cray.com vcs.cmn.shasta.dev.cray.com kiali-istio.shasta.dev.cray.com kiali-istio.cmn.shasta.dev.cray.com s3.shasta.dev.cray.com s3.cmn.shasta.dev.cray.com sma-grafana.shasta.dev.cray.com sma-grafana.cmn.shasta.dev.cray.com sma-kibana.shasta.dev.cray.com sma-kibana.cmn.shasta.dev.cray.com api.shasta.dev.cray.com api.cmn.shasta.dev.cray.com, api.chn.shasta.dev.cray.com, api.can.shasta.dev.cray.com Users must be informed of the change to the api.* endpoint to avoid any unexpected disruptions.\nNote that the *.cmn.\u0026lt;system-domain\u0026gt;, *.can.\u0026lt;system-domain\u0026gt;, and *.chn.\u0026lt;system-domain\u0026gt; suffixes are not configurable. That is, *.cmn.\u0026lt;system-domain\u0026gt; cannot be configured to instead be *.my-mgmt-network.\u0026lt;system-domain\u0026gt;, for example.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/craycli_403_forbidden_errors/",
	"title": "Cray CLI 403 Forbidden Errors",
	"tags": [],
	"description": "",
	"content": "Cray CLI 403 Forbidden Errors There is a known issue where the Keycloak configuration obtained from LDAP is incomplete causing the keycloak-users-localize job to fail to complete. This, in turn, causes 403 Forbidden errors when trying to use the cray CLI. This can also cause a Keycloak test to fail during CSM health validation.\nFix To recover from this situation, the following can be done.\nLog into the Keycloak admin console. See Access the Keycloak User Management UI\nDelete the shasta-user-federation-ldap entry from the \u0026ldquo;User Federation\u0026rdquo; page.\nWait three minutes for the configuration to re-sync.\nRe-run the Keycloak localize job.\nncn# kubectl get job -n services -l app.kubernetes.io/name=cray-keycloak-users-localize \\ -ojson | jq \u0026#39;.items[0]\u0026#39; \u0026gt; keycloak-users-localize-job.json ncn# kubectl delete job -n services -l app.kubernetes.io/name=cray-keycloak-users-localize ncn# cat keycloak-users-localize-job.json | jq \u0026#39;del(.spec.selector)\u0026#39; | \\ jq \u0026#39;del(.spec.template.metadata.labels)\u0026#39; | kubectl apply -f - Expected output looks similar to:\njob.batch \u0026#34;keycloak-users-localize-1\u0026#34; deleted job.batch/keycloak-users-localize-1 created Check to see if the keycloak-users-localize job has completed.\nncn# kubectl -n services wait --for=condition=complete --timeout=10s job/`kubectl -n services get jobs | grep users-localize | awk \u0026#39;{print $1}\u0026#39;` If the above command returns output containing condition met then the issue is resolved and you can skip the rest of the steps.\nIf the above command returns output containing error: timed out waiting for the condition then check the logs of the keycloak-users-localize pod.\nncn# kubectl -n services logs `kubectl -n services get pods | grep users-localize | awk \u0026#39;{print $1}\u0026#39;` keycloak-localize If you see an error showing that there is a duplicate group, complete the next step.\nGo to the Groups page in the Keycloak admin console and delete the groups.\nWait three minutes for the configuration to re-sync.\nRe-run the Keycloak localize job.\nncn# kubectl get job -n services -l app.kubernetes.io/name=cray-keycloak-users-localize \\ -ojson | jq \u0026#39;.items[0]\u0026#39; \u0026gt; keycloak-users-localize-job.json ncn# kubectl delete job -n services -l app.kubernetes.io/name=cray-keycloak-users-localize ncn# cat keycloak-users-localize-job.json | jq \u0026#39;del(.spec.selector)\u0026#39; | \\ jq \u0026#39;del(.spec.template.metadata.labels)\u0026#39; | kubectl apply -f - Expected output looks similar to:\njob.batch \u0026#34;keycloak-users-localize-1\u0026#34; deleted job.batch/keycloak-users-localize-1 created Check again to make sure the job has now completed.\nncn# kubectl -n services wait --for=condition=complete --timeout=10s job/`kubectl -n services get jobs | grep users-localize | awk \u0026#39;{print $1}\u0026#39;` You should see output containing condition met.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/ceph_orchestrator_usage/",
	"title": "Ceph Orchestrator Usage",
	"tags": [],
	"description": "",
	"content": "Ceph Orchestrator Usage The Ceph orchestrator provides a centralized interface for the management of the Ceph cluster. It orchestrates ceph-mgr modules that interface with external orchestration services.\nRefer to the external Ceph documentation for more information.\nThe orchestrator manages Ceph clusters with the following capabilities:\nSingle command upgrades (assuming all images are in place) Reduces the need to be on the physical server to address a large number of ceph service restarts or configuration changes Better integration with the Ceph Dashboard (coming soon) Ability to write custom orchestration modules Troubleshoot Ceph Orchestrator Watch cephadm Log Messages Watching log messages is useful when making changes with the orchestrator, such as add/remove/scale services or upgrades.\nncn-s# ceph -w cephadm To watch log messages with debug:\nncn-s# ceph config set mgr mgr/cephadm/log_to_cluster_level debug ncn-s# ceph -W cephadm --watch-debug NOTE: For use with orchestration tasks, this can be typically run from a node running the ceph mon process. In most cases, this is ncn-s00(1/2/3). There may be cases where a cephadm is run locally on a host and it will be more efficient to tail /var/log/ceph/cephadm.log.\nUsage Examples This section will provide some in-depth usage with examples of the more commonly used ceph orch subcommands.\nList Service Deployments ncn-s# ceph orch ls Example output:\nNAME RUNNING REFRESHED AGE PLACEMENT IMAGE NAME IMAGE ID alertmanager 1/1 6m ago 4h count:1 registry.local/prometheus/alertmanager:v0.20.0 0881eb8f169f crash 3/3 6m ago 4h * registry.local/ceph/ceph:v15.2.8 5553b0cb212c grafana 1/1 6m ago 4h count:1 registry.local/ceph/ceph-grafana:6.6.2 a0dce381714a mds.cephfs 3/3 6m ago 4h ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c mgr 3/3 6m ago 4h ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c mon 3/3 6m ago 4h ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c node-exporter 3/3 6m ago 4h * registry.local/prometheus/node-exporter:v0.18.1 e5a616e4b9cf osd.all-available-devices 9/9 6m ago 4h * registry.local/ceph/ceph:v15.2.8 5553b0cb212c prometheus 1/1 6m ago 4h count:1 docker.io/prom/prometheus:v2.18.1 de242295e225 rgw.site1.zone1 3/3 6m ago 4h ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c FILTERS: Apply filters by adding --service_type \u0026lt;service type\u0026gt; or --service_name \u0026lt;service name\u0026gt;.\nReference Key:\nPLACEMENT - Represents a service deployed on all nodes. Otherwise the listed placement is where it is expected to be deployed. NAME - The deployment name. This is a generalized name to reference the deployment. This is being noted as additional subcommands the name is more specific to the actual deployed daemon. List Deployed Daemons ncn-s# ceph orch ps Example output:\nNAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID alertmanager.ncn-s001 ncn-s001 running (5h) 5m ago 5h 0.20.0 registry.local/prometheus/alertmanager:v0.20.0 0881eb8f169f 0e6a24469465 crash.ncn-s001 ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c b6a582ed7573 crash.ncn-s002 ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 3778e29099eb crash.ncn-s003 ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c fe085e310cbd grafana.ncn-s001 ncn-s001 running (5h) 5m ago 5h 6.6.2 registry.local/ceph/ceph-grafana:6.6.2 a0dce381714a 2fabb486928c mds.cephfs.ncn-s001.qrxkih ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 03a3a1ce682e mds.cephfs.ncn-s002.qhferv ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 56dca5cca407 mds.cephfs.ncn-s003.ihwkop ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 38ab6a6c8bc6 mgr.ncn-s001.vkfdue ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 456587705eab mgr.ncn-s002.wjaxkl ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 48222c38dd7e mgr.ncn-s003.inwpij ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 76ff8e485504 mon.ncn-s001 ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c bcca26f69191 mon.ncn-s002 ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 43c8472465b2 mon.ncn-s003 ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 7aa1b1f19a00 node-exporter.ncn-s001 ncn-s001 running (5h) 5m ago 5h 0.18.1 registry.local/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 0be431766c8e node-exporter.ncn-s002 ncn-s002 running (5h) 5m ago 5h 0.18.1 registry.local/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 6ae81d01d963 node-exporter.ncn-s003 ncn-s003 running (5h) 5m ago 5h 0.18.1 registry.local/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 330dc09d0845 osd.0 ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c a8c7314b484b osd.1 ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 8f9941887053 osd.2 ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 49cf2c532efb osd.3 ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 69e89cf18216 osd.4 ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 72d7f51a3690 osd.5 ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 76d598c40824 osd.6 ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c d2372e45c8eb osd.7 ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 5bd22f1d4cad osd.8 ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 7c5282f2e107 prometheus.ncn-s001 ncn-s001 running (5h) 5m ago 5h 2.18.1 docker.io/prom/prometheus:v2.18.1 de242295e225 bf941a1306e9 rgw.site1.zone1.ncn-s001.qegfux ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c e833fc05acfe rgw.site1.zone1.ncn-s002.wqrzoa ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 83a131a7022c rgw.site1.zone1.ncn-s003.tzkxya ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c c67d75adc620 FILTERS: Apply filters by adding any or all of [\u0026ndash;hostname \u0026ndash;service_name \u0026lt;service_name\u0026gt; \u0026ndash;daemon_type \u0026lt;daemon_type\u0026gt; \u0026ndash;daemon_id \u0026lt;daemon_id\u0026gt;].\nCeph Daemon start|stop|restart|reconfig NOTE: The service name is from ceph orch ps NOT ceph orch ls.\nncn-s# ceph orch daemon restart alertmanager.ncn-s001 A message stating \u0026ldquo;Scheduled to restart alertmanager.ncn-s001 on host \u0026rsquo;ncn-s001\u0026rsquo;\u0026rdquo; will be returned.\nMonitor the restart using the ceph orch ps command and the time associated with the STATUS should be reset and show \u0026ldquo;running (time since started).\u0026rdquo;\nDeploy or Scale Services NOTE: The service name is from ceph orch ls NOT ceph orch ps.\nncn-s# ceph orch apply alertmanager --placement=\u0026#34;2 ncn-s001 ncn-s002\u0026#34; A message stating \u0026ldquo;Scheduled alertmanager update\u0026hellip;\u0026rdquo; will be returned.\nReference Key:\nPLACEMENT - This will show the nodes and the count. If only specifying --placement=\u0026quot;2\u0026quot;, then it will automatically pick where to put it. IMPORTANT: There are several combinations available when working with the placement. For example, a placement of 1 can be specified, but then a list of a sub-set of nodes can be used. This is a good way to contain the process to those nodes. IMPORTANT: This is not available for any deployments with a PLACEMENT of *\nList Hosts Known to Ceph Orchestrator ncn-s# ceph orch host ls Example output:\nHOST ADDR LABELS STATUS ncn-s001 ncn-s001 ncn-s002 ncn-s002 ncn-s003 ncn-s003 List Drives on Hosts Known to Ceph Orchestrator ncn-s# ceph orch device ls Example output:\nHostname Path Type Serial Size Health Ident Fault Available ncn-s001 /dev/vdb hdd fb794832-f402-4f4f-a 107G Unknown N/A N/A No ncn-s001 /dev/vdc hdd 9bdef369-6bac-40ca-a 107G Unknown N/A N/A No ncn-s001 /dev/vdd hdd 3cda8ba2-ccaf-4515-b 107G Unknown N/A N/A No ncn-s002 /dev/vdb hdd 775639a6-092e-4f3a-9 107G Unknown N/A N/A No ncn-s002 /dev/vdc hdd 261e8a40-2349-484e-8 107G Unknown N/A N/A No ncn-s002 /dev/vdd hdd 8f01f9c6-2c6c-449c-a 107G Unknown N/A N/A No ncn-s003 /dev/vdb hdd 46467f02-1d11-44b2-b 107G Unknown N/A N/A No ncn-s003 /dev/vdc hdd 4797e919-667e-4376-b 107G Unknown N/A N/A No ncn-s003 /dev/vdd hdd 3b2c090d-37a0-403b-a 107G Unknown N/A N/A No IMPORTANT: If --wide is used, it will give the reasons a drive is not Available. This DOES NOT mean something is wrong. If Ceph already has the drive provisioned, there may be similar reasons.\nGeneral Use Update the size or placement for a service or apply a large YAML spec:\nncn-s# ceph orch apply [mon|mgr|rbd-mirror|crash|alertmanager|grafana|node-exporter|prometheus] [\u0026lt;placement\u0026gt;] [--dry-run] [plain|json|json-pretty|yaml] [--unmanaged] Scale an iSCSI service:\nncn-s# ceph orch apply iscsi \u0026lt;pool\u0026gt; \u0026lt;api_user\u0026gt; \u0026lt;api_password\u0026gt; [\u0026lt;trusted_ip_list\u0026gt;][\u0026lt;placement\u0026gt;] [--dry-run] [plain|json|json-pretty|yaml] [--unmanaged] Update the number of MDS instances for the given fs_name:\nncn-s# ceph orch apply mds \u0026lt;fs_name\u0026gt; [\u0026lt;placement\u0026gt;] [--dry-run] [--unmanaged] [plain|json|json-pretty|yaml] Scale an NFS service:\nncn-s# ceph orch apply nfs \u0026lt;svc_id\u0026gt; \u0026lt;pool\u0026gt; [\u0026lt;namespace\u0026gt;] [\u0026lt;placement\u0026gt;] [--dry-run] [plain|json|json-pretty|yaml] [--unmanaged] Create OSD daemon(s) using a drive group spec:\nncn-s# ceph orch apply osd [--all-available-devices] [--dry-run] [--unmanaged] [plain|json|json-pretty|yaml] Update the number of RGW instances for the given zone:\nncn-s# ceph orch apply rgw \u0026lt;realm_name\u0026gt; \u0026lt;zone_name\u0026gt; [\u0026lt;subcluster\u0026gt;] [\u0026lt;port:int\u0026gt;] [--ssl] [\u0026lt;placement\u0026gt;] [--dry-run] [plain|json|json-pretty|yaml] [--unmanaged] Cancel ongoing operations:\nncn-s# ceph orch cancel Add daemon(s):\nncn-s# ceph orch daemon add [mon|mgr|rbd-mirror|crash|alertmanager|grafana|node-exporter|prometheus] [\u0026lt;placement\u0026gt;] Start iscsi daemon(s):\nncn-s# ceph orch daemon add iscsi \u0026lt;pool\u0026gt; \u0026lt;api_user\u0026gt; \u0026lt;api_password\u0026gt; [\u0026lt;trusted_ip_list\u0026gt;] [\u0026lt;placement\u0026gt;] Start MDS daemon(s):\nncn-s# ceph orch daemon add mds \u0026lt;fs_name\u0026gt; [\u0026lt;placement\u0026gt;] Start NFS daemon(s):\nncn-s# ceph orch daemon add nfs \u0026lt;svc_id\u0026gt; \u0026lt;pool\u0026gt; [\u0026lt;namespace\u0026gt;] [\u0026lt;placement\u0026gt;] Create an OSD service:\nEither \u0026ndash;svc_arg=host:drives\nncn-s# ceph orch daemon add osd [\u0026lt;svc_arg\u0026gt;] Start RGW daemon(s):\nncn-s# ceph orch daemon add rgw \u0026lt;realm_name\u0026gt; \u0026lt;zone_name\u0026gt; [\u0026lt;subcluster\u0026gt;] [\u0026lt;port:int\u0026gt;] [--ssl] [\u0026lt;placement\u0026gt;] Redeploy a daemon (with a specific image):\nncn-s# ceph orch daemon redeploy \u0026lt;name\u0026gt; [\u0026lt;image\u0026gt;] Remove specific daemon(s):\nncn-s# ceph orch daemon rm \u0026lt;names\u0026gt;... [--force] Start, stop, restart, (redeploy,) or reconfig a specific daemon:\nncn-s# ceph orch daemon start|stop|restart|reconfig \u0026lt;name\u0026gt; List devices on a host:\nncn-s# ceph orch device ls [\u0026lt;hostname\u0026gt;...] [plain|json|json-pretty|yaml] [--refresh] [--wide] Zap (erase!) a device so it can be re-used:\nncn-s# ceph orch device zap \u0026lt;hostname\u0026gt; \u0026lt;path\u0026gt; [--force] Add a host:\nncn-s# ceph orch host add \u0026lt;hostname\u0026gt; [\u0026lt;addr\u0026gt;] [\u0026lt;labels\u0026gt;...] Add a host label:\nncn-s# ceph orch host label add \u0026lt;hostname\u0026gt; \u0026lt;label\u0026gt; Remove a host label:\nncn-s# ceph orch host label rm \u0026lt;hostname\u0026gt; \u0026lt;label\u0026gt; List hosts:\nncn-s# ceph orch host ls [plain|json|json-pretty|yaml] Check if the specified host can be safely stopped without reducing availability:\nncn-s# ceph orch host ok-to-stop \u0026lt;hostname\u0026gt; Remove a host:\nncn-s# ceph orch host rm \u0026lt;hostname\u0026gt; Update a host address:\nncn-s# ceph orch host set-addr \u0026lt;hostname\u0026gt; \u0026lt;addr\u0026gt; List services known to orchestrator:\nncn-s# ceph orch ls [\u0026lt;service_type\u0026gt;] [\u0026lt;service_name\u0026gt;] [--export] [plain|json|json-pretty|yaml] [--refresh] Remove OSD services:\nncn-s# ceph orch osd rm \u0026lt;svc_id\u0026gt;... [--replace] [--force] Status of OSD removal operation:\nncn-s# ceph orch osd rm status [plain|json|json-pretty|yaml] Remove OSD services:\nncn-s# ceph orch osd rm stop \u0026lt;svc_id\u0026gt;... Pause orchestrator background work:\nncn-s# ceph orch pause List daemons known to orchestrator:\nncn-s# ceph orch ps [\u0026lt;hostname\u0026gt;] [\u0026lt;service_name\u0026gt;] [\u0026lt;daemon_type\u0026gt;] [\u0026lt;daemon_id\u0026gt;] [plain|json|json-pretty|yaml] [--refresh] Resume orchestrator background work (if paused):\nncn-s# ceph orch resume Remove a service:\nncn-s# ceph orch rm \u0026lt;service_name\u0026gt; [--force] Select orchestrator module backend:\nncn-s# ceph orch set backend \u0026lt;module_name\u0026gt; Start, stop, restart, redeploy, or reconfig an entire service (i.e. all daemons):\nncn-s# ceph orch start|stop|restart|redeploy|reconfig \u0026lt;service_name\u0026gt; Report configured backend and its status:\nncn-s# ceph orch status [plain|json|json-pretty|yaml] Check service versions vs available and target containers:\nncn-s# ceph orch upgrade check [\u0026lt;image\u0026gt;] [\u0026lt;ceph_version\u0026gt;] Pause an in-progress upgrade:\nncn-s# ceph orch upgrade pause Resume paused upgrade:\nncn-s# ceph orch upgrade resume Initiate upgrade:\nncn-s# ceph orch upgrade start [\u0026lt;image\u0026gt;] [\u0026lt;ceph_version\u0026gt;] Check service versions vs available and target containers:\nncn-s# ceph orch upgrade status Stop an in-progress upgrade:\nncn-s# ceph orch upgrade stop "
},
{
	"uri": "/docs-csm/en-12/operations/system_management_health/troubleshoot_grafana_dashboard/",
	"title": "Troubleshoot Grafana Dashboard",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Grafana Dashboard General Grafana dashboard troubleshooting topics\nCeph - OSD Overview Dashboard Ceph - RBD Overview Dashboard Ceph - RGW Instance Detail Dashboard Ceph - RGW Overview Dashboard Ceph - OSD Overview Dashboard: 3 panels not found This means that the cray-sysmgmt-health pie chart plugin is not installed. If the system is not airgapped, then it can be installed by commenting out or removing the plugins property in customizations.yaml. This will be fixed in a future release.\nCommand to extract customizations.yaml from the site-init secret.\nncn-mw# kubectl -n loftsman get secret site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d - \u0026gt; customizations.yaml Example:\ncray-sysmgmt-health: grafana: externalAuthority: grafana.cmn.{{ network.dns.external }} plugins: \u0026#34;\u0026#34; In the above section, comment or delete the plugins line in order to install the pie chart plugin.\nUpload customizations.yaml file to Kubernetes so that the changes persist.\nncn-mw# kubectl delete secret -n loftsman site-init ncn-mw# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml Ceph - RBD Overview Dashboard: No Data This means that the Grafana dashboard is not getting any ceph_rbd_* metrics from the Ceph exporter. This will be fixed in a future release.\nCeph - RGW Instance Detail Dashboard: Panel missing and no data Ceph - RGW Instance Detail Dashboard uses a 30 second time range in queries, which is a very short duration. Because of this, the dashboard is unable to load the data. This will be fixed in a future release.\nCeph - RGW Overview Dashboard: No data Ceph - RGW Overview Dashboard uses a 30 second time range in queries, which is a very short duration. Because of this, the dashboard is unable to load the data. This will be fixed in a future release.\n"
},
{
	"uri": "/docs-csm/en-12/operations/system_layout_service/restore_sls_postgres_without_an_existing_backup/",
	"title": "Restore SLS Postgres without an Existing Backup",
	"tags": [],
	"description": "",
	"content": "Restore SLS Postgres without an Existing Backup This procedure is intended to repopulate SLS in the event when no Postgres backup exists.\nPrerequisite Healthy SLS Service.\nVerify all 3 SLS replicas are up and running:\nncn# kubectl -n services get pods -l cluster-name=cray-sls-postgres Expected output should look similar to the following:\nNAME READY STATUS RESTARTS AGE cray-sls-postgres-0 3/3 Running 0 18d cray-sls-postgres-1 3/3 Running 0 18d cray-sls-postgres-2 3/3 Running 0 18d Procedure Retrieve the initial sls_input_file.json that was used to initially install the system with from sls S3 bucket.\nncn# cray artifacts get sls sls_input_file.json sls_input_file.json Perform an SLS load state operation to replace the contents of SLS with the data from the sls_input_file.json file.\nGet an API Token:\nncn# export TOKEN=$(curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Perform the load state operation:\nncn# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -F sls_dump=@sls_input_file.json \\ https://api-gw-service-nmn.local/apis/sls/v1/loadstate Any previously made customizations made to SLS will need to be applied again. This includes any SLS API operations that modified the state of SLS.\nThe HSN network in SLS will be missing HSN subnet data, this data will need to be repopulated again using the \u0026ldquo;Set up DNS for HSN IP addresses\u0026rdquo; procedure in the Slingshot Operations Guide.\nAny hardware that was added or moved in the system using one of the following procedures will need to be performed again.\nAdd a Standard Rack Node Move a Standard Rack Node Same Rack/Same HSN Ports Move a Standard Rack Node "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/certificate_types/",
	"title": "Certificate Types",
	"tags": [],
	"description": "",
	"content": "Certificate Types The system software installation process creates an X.509 Certificate Authority (CA) on the primary non-compute node (NCN) and uses the CA to create an NCN host X.509 certificate. This host certificate is used during the installation process to configure the API gateway for TLS so that communications to the gateway can use HTTPS.\nClients should use HTTPS to talk to services behind the API gateway and need to ensure that the NCN CA certificate is known by the client software when making requests.\nKeycloak, which is the Identity and Access Management (IAM) server, will also have a certificate created at install time for the Shasta realm. This certificate is known as the Shasta realm certificate and is used when signing a JSON Web Token (JWT). The Shasta realm certificate is registered with the API gateway and is used by the API gateway to validate that a JWT passed when requests are made actually originate from the IAM server.\nThis document does not cover the process for updating any of the certificates described below.\nNCN CA certificate NCN host certificate API gateway TLS certificate IAM service Shasta realm JWT certificate NCN CA certificate The NCN CA is created by the installer and located on at /var/opt/cray/certificate_authority/certificate_authority.crt. The signature algorithm used is sha256WithRSAEncryption and the key length is 2048 bits. The CA Issuer is generated at the time of creation and therefore specific to each installation.\nThis and other certificate details can be viewed by executing:\nncn# openssl x509 -in /var/opt/cray/certificate_authority/certificate_authority.crt -noout -text NCN host certificate The NCN host certificate is created by the installer and located at sms-1:/var/opt/cray/certificate\\_authority/hosts/host.crt\nThe signature algorithm used is sha256WithRSAEncryption and the key length is 2048 bits.\nAdditional certificate details can be viewed by executing:\nncn# openssl x509 -in /var/opt/cray/certificate_authority/hosts/host.crt -noout -text API gateway TLS certificate The API gateway is configured with the NCN host certificate and key to allow enabling TLS/HTTPS on the gateway. Configuration details are handled by the installer when the API gateway Kubernetes pods are created.\nIAM service Shasta realm JWT certificate The IAM service (Keycloak) Shasta realm contains an RSA certificate. This is known as the realm certificate. The realm certificate is used by the API gateway during JWT validation.\nThe installer adds a Shasta realm to the IAM service. The RSA realm certificate is created as part of that process. This certificate can be viewed from the Keycloak Admin Console by selecting the Shasta realm, then Realm Settings, then Keys tab. The certificate can be viewed by clicking the Public Key button for the RSA certificate. The key length is 2048 bits.\n"
},
{
	"uri": "/docs-csm/en-12/operations/power_management/power_on_and_start_the_management_kubernetes_cluster/",
	"title": "Power On and Start the Management Kubernetes Cluster",
	"tags": [],
	"description": "",
	"content": "Power On and Start the Management Kubernetes Cluster Power on and start management services on the HPE Cray EX management Kubernetes cluster.\nPrerequisites All management rack PDUs are connected to facility power and facility power is on. An authentication token is required to access the API gateway and to use the sat command. See the \u0026ldquo;SAT Authentication\u0026rdquo; section of the HPE Cray EX System Admin Toolkit (SAT) product stream documentation (S-8031) for instructions on how to acquire a SAT authentication token. Procedure If necessary, power on the management cabinet CDU and chilled doors.\nSet all management cabinet PDU circuit breakers to ON (all cabinets that contain Kubernetes master nodes, worker nodes, or storage nodes).\nPower on the HPE Cray EX cabinet PDUs and standard rack cabinet PDUs.\nBe sure that management switches in all racks and CDU cabinets are powered on and healthy.\nFrom a remote system, start the Lustre file system, if it was stopped.\nActivate the serial console window to ncn-m001.\nread -s is used to prevent the password from being written to the screen or the shell history.\nremote# USERNAME=root remote# read -r -s -p \u0026#34;ncn-m001 BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD remote# export IPMI_PASSWORD remote# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN_M001_BMC_HOSTNAME sol activate In a separate window, power on the master node 1 (ncn-m001) chassis using IPMI tool.\nread -s is used to prevent the password from being written to the screen or the shell history.\nremote# USERNAME=root remote# read -r -s -p \u0026#34;ncn-m001 BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD remote# export IPMI_PASSWORD remote# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN_M001_BMC_HOSTNAME chassis power on Wait for the login prompt.\nIf ncn-m001 boots into the PIT node, then perform the following procedure:\nSet boot order to boot from disk.\nSee NCN Boot Workflow.\nShutdown the PIT node.\npit# shutdown -h now Power cycle again to boot into ncn-m001.\nremote# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN_M001_BMC_HOSTNAME chassis power on Wait for ncn-m001 to boot, then ping the node to check status.\nremote# ping NCN_M001_HOSTNAME Important: For Gigabyte nodes only: If the console for ncn-m001 shows that the node fails to boot from its local storage and repeats in a boot loop never finding the local storage as a viable boot option, it may be necessary to clear CMOS. This problem has been seen with BIOS C27 and earlier.\nSee Clear Gigabyte CMOS. Then power the node on again and watch its console. Log in to ncn-m001 as root.\nremote# ssh root@NCN_M001_HOSTNAME Power on all other management NCNs Power on and boot other management NCNs.\nncn-m001# sat bootsys boot --stage ncn-power Example output:\nIPMI username: root IPMI password: The following Non-compute Nodes (NCNs) will be included in this operation: managers: - ncn-m002 - ncn-m003 storage: - ncn-s001 - ncn-s002 - ncn-s003 workers: - ncn-w001 - ncn-w002 - ncn-w003 The following Non-compute Nodes (NCNs) will be excluded from this operation: managers: - ncn-m001 storage: [] workers: [] Are the above NCN groupings and exclusions correct? [yes,no] yes Powering on NCNs and waiting up to 300 seconds for them to be reachable via SSH: ncn-m002, ncn-m003 Waiting for condition \u0026#34;Hosts accessible via SSH\u0026#34; timed out after 300 seconds ERROR: Unable to reach the following NCNs via SSH after powering them on: ncn-m003, ncn-s002.. Troubleshoot the issue and then try again. In the preceding example, the ssh command to the NCN nodes timed out and reported ERROR messages. Repeat the above step until you see Succeeded with boot of other management NCNs. Each iteration should get further in the process.\nMonitor the consoles for each NCN.\nUse tail to monitor the log files in /var/log/cray/console_logs for each NCN.\nAlternatively, attach to the screen session (screen sessions real time, but not saved):\nncn-m001# screen -ls Example output:\nThere are screens on: 26745.SAT-console-ncn-m003-mgmt (Detached) 26706.SAT-console-ncn-m002-mgmt (Detached) 26666.SAT-console-ncn-s003-mgmt (Detached) 26627.SAT-console-ncn-s002-mgmt (Detached) 26589.SAT-console-ncn-s001-mgmt (Detached) 26552.SAT-console-ncn-w003-mgmt (Detached) 26514.SAT-console-ncn-w002-mgmt (Detached) 26444.SAT-console-ncn-w001-mgmt (Detached) ncn-m001# screen -x 26745.SAT-console-ncn-m003-mgmt Important: For Gigabyte nodes only: If the console for any of these management nodes shows that the node fails to boot from its local storage and repeats in a boot loop never finding the local storage as a viable boot option, it may be necessary to clear CMOS. This problem has been seen with BIOS C27 and earlier.\nSee Clear Gigabyte CMOS. Then power the node on again to have it boot to local storage. Verify access to Lustre file system Verify that the Lustre file system is available from the management cluster.\nStart Kubernetes and other services Start the Kubernetes cluster.\nNote that the default timeout for Ceph to become healthy is 600 seconds, which is excessive. To work around this issue, set the timeout to a more reasonable value (like 60 seconds) using the --ceph-timeout option, as shown below.\nncn-m001# sat bootsys boot --stage platform-services --ceph-timeout 60 Example output:\nThe following Non-compute Nodes (NCNs) will be included in this operation: managers: - ncn-m001 - ncn-m002 - ncn-m003 storage: - ncn-s001 - ncn-s002 - ncn-s003 workers: - ncn-w001 - ncn-w002 - ncn-w003 Are the above NCN groupings correct? [yes,no] yes The sat bootsys boot command may fail with a message like the following:\nExecuting step: Start inactive Ceph services, unfreeze Ceph cluster and wait for Ceph health. Waiting up to 60 seconds for Ceph to become healthy after unfreeze Waiting for condition \u0026#34;Ceph cluster in healthy state\u0026#34; timed out after 60 seconds ERROR: Fatal error in step \u0026#34;Start inactive Ceph services, unfreeze Ceph cluster and wait for Ceph health.\u0026#34; of platform services start: Ceph is not healthy. Please correct Ceph health and try again. If a failure like the above occurs, then see the info-level log messages for details about the Ceph health check failure. Depending on the configured log level for SAT, the log messages may appear in stderr, or only in the log file. For example:\nncn-m001# grep \u0026#34;fatal Ceph health warnings\u0026#34; /var/log/cray/sat/sat.log | tail -n 1 Example output:\n2021-08-04 17:28:21,945 - INFO - sat.cli.bootsys.ceph - Ceph is not healthy: The following fatal Ceph health warnings were found: POOL_NO_REDUNDANCY The particular Ceph health warning may vary. In this example, it is POOL_NO_REDUNDANCY.\nIf the warning is PG_NOT_DEEP_SCRUBBED, this alert should clear once Ceph deep scrubs of PGs have completed. The time to complete this operation depends on the number of outstanding deep scrub operations and the load on the Ceph cluster. See Ceph Deep Scrubs for more information on deep scrubs. This alert is more likely to occur if the system is powered off for an extended duration.\nSee Manage Ceph Services for Ceph troubleshooting steps, which may include restarting Ceph services.\nOnce Ceph is healthy, repeat the sat bootsys boot --stage platform-services command to finish starting the Kubernetes cluster.\nCheck the space available on the Ceph cluster.\nncn-m001# ceph df Example output:\nRAW STORAGE: CLASS SIZE AVAIL USED RAW USED %RAW USED ssd 63 TiB 60 TiB 2.8 TiB 2.8 TiB 4.45 TOTAL 63 TiB 60 TiB 2.8 TiB 2.8 TiB 4.45 POOLS: POOL ID STORED OBJECTS USED %USED MAX AVAIL cephfs_data 1 40 MiB 382 124 MiB 0 18 TiB cephfs_metadata 2 262 MiB 117 787 MiB 0 18 TiB .rgw.root 3 3.5 KiB 8 384 KiB 0 18 TiB default.rgw.buckets.data 4 71 GiB 27.07k 212 GiB 0.38 18 TiB default.rgw.control 5 0 B 8 0 B 0 18 TiB default.rgw.buckets.index 6 7.7 MiB 13 7.7 MiB 0 18 TiB default.rgw.meta 7 21 KiB 111 4.2 MiB 0 18 TiB default.rgw.log 8 0 B 207 0 B 0 18 TiB kube 9 67 GiB 26.57k 197 GiB 0.35 18 TiB smf 10 806 GiB 271.69k 2.4 TiB 4.12 18 TiB default.rgw.buckets.non-ec 11 0 B 0 0 B 0 18 TiB If %USED for any pool approaches 80% used, then resolve the space issue.\nTo resolve the space issue, see Troubleshoot Ceph OSDs Reporting Full.\nManually mount S3 filesystems on the master and worker nodes. The workers try to mount several S3 filesystems when they are booted, but Ceph is not available yet at that time, so this workaround is required. The boot-images S3 filesystem is required for CPS pods to successfully start on workers.\nncn-m001# pdsh -w ncn-m00[1-3],ncn-w00[1-3] \u0026#34;awk \u0026#39;{ if (\\$3 == \\\u0026#34;fuse.s3fs\\\u0026#34;) { print \\$2; }}\u0026#39; /etc/fstab | xargs -I {} -n 1 sh -c \\\u0026#34;mountpoint {} || mount {}\\\u0026#34;\u0026#34; Ensure all masters and workers are included in the host list for this pdsh command.\nMonitor the status of the management cluster and which pods are restarting (as indicated by either a Running or Completed state).\nncn-m001# kubectl get pods -A -o wide | grep -v -e Running -e Completed The pods and containers are normally restored in approximately 10 minutes.\nBecause no containers are running, all pods first transition to an Error state. The error state indicates that their containers were stopped. The kubelet on each node restarts the containers for each pod. The RESTARTS column of the kubectl get pods -A command increments as each pod progresses through the restart sequence.\nIf there are pods in the MatchNodeSelector state, delete these pods. Then verify that the pods restart and are in the Running state.\nCheck the status of the slurmctld and slurmdbd pods to determine if they are starting:\nncn-m001# kubectl describe pod -n user -lapp=slurmctld ncn-m001# kubectl describe pod -n user -lapp=slurmdbd An error similar to the following may be seen:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \u0026#34;314ca4285d0706ec3d76a9e953e412d4b0712da4d0cb8138162b53d807d07491\u0026#34;: Multus: Err in tearing down failed plugins: Multus: error in invoke Delegate add - \u0026#34;macvlan\u0026#34;: failed to allocate for range 0: no IP addresses available in range set: 10.252.2.4-10.252.2.4 Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox If the preceding error is displayed, then remove all files in the following directories on all worker nodes:\n/var/lib/cni/networks/macvlan-slurmctld-nmn-conf /var/lib/cni/networks/macvlan-slurmdbd-nmn-conf Check that spire pods have started.\nncn-m001# kubectl get pods -n spire -o wide | grep spire-jwks Example output:\nspire-jwks-6b97457548-gc7td 2/3 CrashLoopBackOff 9 23h 10.44.0.117 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; spire-jwks-6b97457548-jd7bd 2/3 CrashLoopBackOff 9 23h 10.36.0.123 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; spire-jwks-6b97457548-lvqmf 2/3 CrashLoopBackOff 9 23h 10.39.0.79 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; If Spire pods indicate CrashLoopBackOff, then restart the Spire deployment.\nncn-m001# kubectl rollout restart -n spire deployment spire-jwks Rejoin Spire on the worker and master NCNs, to avoid issues with Spire tokens.\nncn-m001# kubectl rollout restart -n spire daemonset request-ncn-join-token ncn-m001# kubectl rollout status -n spire daemonset request-ncn-join-token Rejoin Spire on the storage NCNs, to avoid issues with Spire tokens.\nncn-m001# /opt/cray/platform-utils/spire/fix-spire-on-storage.sh Check if any pods are in CrashLoopBackOff state because of errors connecting to Vault.\nIf so, restart the Vault operator, then the Vault pods, and finally the pod which is in CrashLoopBackOff. For example:\nFind the pods that are in CrashLoopBackOff state.\nncn-m001# kubectl get pods -A | grep CrashLoopBackOff Example output:\nservices cray-console-node-1 2/3 CrashLoopBackOff 206 6d21h View the logs for the pods in CrashLoopBackOff.\nncn-m001# kubectl -n services logs cray-console-node-1 cray-console-node | grep \u0026#34;connection failure\u0026#34; | grep vault Example output:\n2021/08/26 16:39:28 Error: \u0026amp;api.ResponseError{HTTPMethod:\u0026#34;PUT\u0026#34;, URL:\u0026#34;http://cray-vault.vault:8200/v1/auth/kubernetes/login\u0026#34;, StatusCode:503, RawError:true, Errors:[]string{\u0026#34;upstream connect error or disconnect/reset before headers. reset reason: connection failure\u0026#34;}} panic: Error: \u0026amp;api.ResponseError{HTTPMethod:\u0026#34;PUT\u0026#34;, URL:\u0026#34;http://cray-vault.vault:8200/v1/auth/kubernetes/login\u0026#34;, StatusCode:503, RawError:true, Errors:[]string{\u0026#34;upstream connect error or disconnect/reset before headers. reset reason: connection failure\u0026#34;}} Restart the vault-operator.\nncn-m001# kubectl delete pods -n vault -l app.kubernetes.io/name=vault-operator Wait for the cray-vault pods to restart with 5/5 ready and Running.\nncn-m001# kubectl get pods -n vault -l app.kubernetes.io/name=vault-operator Example output:\nNAME READY STATUS RESTARTS AGE cray-vault-operator-69b4b6887-dfn2f 2/2 Running 2 1m Restart the pods.\nIn this example, cray-console-node-1 is the only pod.\nncn-m001# kubectl delete pod cray-console-node-1 -n services Wait for the pods to restart with 3/3 ready and Running.\nIn this example, cray-console-node-1 is the only pod.\nncn-m001# kubectl get pods -n services | grep cray-console-node-1 Example output:\ncray-console-node-1 3/3 Running 0 2m Determine whether the cfs-state-reporter service is failing to start on each management node while trying to contact CFS.\nncn-m001# pdsh -w $(grep -oP \u0026#39;ncn-\\w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) systemctl status cfs-state-reporter Example output:\nncn-w001: cfs-state-reporter.service - cfs-state-reporter reports configuration level of the system ncn-w001: Loaded: loaded (/usr/lib/systemd/system/cfs-state-reporter.service; enabled; vendor preset: disabled) ncn-w001: Active: activating (start) since Thu 2021-03-18 22:29:15 UTC; 21h ago ncn-w001: Main PID: 5192 (python3) ncn-w001: Tasks: 1 ncn-w001: CGroup: /system.slice/cfs-state-reporter.service ncn-w001: └─5192 /usr/bin/python3 -m cfs.status_reporter ncn-w001: ncn-w001: Mar 19 19:33:19 ncn-w001 python3[5192]: Expecting value: line 1 column 1 (char 0) ncn-w001: Mar 19 19:33:49 ncn-w001 python3[5192]: Attempt 2482 of contacting CFS... ncn-w001: Mar 19 19:33:49 ncn-w001 python3[5192]: Unable to contact CFS to report component status: CFS returned a non-json response: Unauthorized Request ncn-w001: Mar 19 19:33:49 ncn-w001 python3[5192]: Expecting value: line 1 column 1 (char 0) ncn-w001: Mar 19 19:34:19 ncn-w001 python3[5192]: Attempt 2483 of contacting CFS... ncn-w001: Mar 19 19:34:20 ncn-w001 python3[5192]: Unable to contact CFS to report component status: CFS returned a non-json response: Unauthorized Request ncn-w001: Mar 19 19:34:20 ncn-w001 python3[5192]: Expecting value: line 1 column 1 (char 0) ncn-w001: Mar 19 19:34:50 ncn-w001 python3[5192]: Attempt 2484 of contacting CFS... ncn-w001: Mar 19 19:34:50 ncn-w001 python3[5192]: Unable to contact CFS to report component status: CFS returned a non-json response: Unauthorized Request ncn-w001: Mar 19 19:34:50 ncn-w001 python3[5192]: Expecting value: line 1 column 1 (char 0) pdsh@ncn-m001: ncn-w001: ssh exited with exit code 3 On each NCN where cfs-state-reporter is stuck in activating as shown in the preceding error messages, restart the cfs-state-reporter service.\nFor example:\nncn# systemctl restart cfs-state-reporter Check the status again.\nncn-m001# pdsh -w $(grep -oP \u0026#39;ncn-\\w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) systemctl status cfs-state-reporter If there are still issues with cfe-state-reporter on the storage nodes, then it might be because of a Spire issue which can be addressed with this script.\nncn-m001# /opt/cray/platform-utils/spire/fix-spire-on-storage.sh Verify BGP peering sessions Check the status of the Border Gateway Protocol (BGP).\nFor more information, see Check BGP Status and Reset Sessions.\nCheck the status and health of etcd clusters.\nSee Check the Health and Balance of etcd Clusters.\nCheck cronjobs Display all the Kubernetes cronjobs.\nncn-m001# kubectl get cronjobs.batch -A Example output:\nNAMESPACE NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE kube-system kube-etcdbackup */10 * * * * False 0 2d1h 29d operators kube-etcd-defrag 0 0 * * * False 0 18h 29d operators kube-etcd-defrag-cray-hbtd-etcd 0 */4 * * * False 0 178m 29d operators kube-etcd-periodic-backup-cron 0 * * * * False 0 58m 29d services cray-dns-unbound-manager */3 * * * * False 0 63s 18h services hms-discovery */3 * * * * False 1 63s 18h services hms-postgresql-pruner */5 * * * * False 0 3m3s 18h services sonar-sync */1 * * * * False 0 63s 18h sma sma-pgdb-cron 10 4 * * * False 0 14h 27d Attention: It is normal for the hms-discovery service to be suspended at this point if liquid-cooled cabinets have not been powered on. The hms-discovery service is un-suspended during the liquid-cooled cabinet power on procedure. Do not recreate the hms-discovery cronjob at this point.\nCheck for cronjobs that have a LAST SCHEDULE time that is older than the SCHEDULE time. These cronjobs must be restarted.\nCheck any cronjobs in question for errors.\nncn-m001# kubectl describe cronjobs.batch -n kube-system kube-etcdbackup | egrep -A 15 Events Example output:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedNeedsStart 4m15s (x15156 over 42h) cronjob-controller Cannot determine if job needs to be \\ started: too many missed start time (\u0026gt; 100). \\ Set or decrease .spec.startingDeadlineSeconds \\ or check clock skew For any cronjobs producing errors, get the YAML representation of the cronjob and edit the YAML file:\nncn-m001# cd ~/k8s ncn-m001# CRON_JOB_NAME=name-of-k8s-cron-job ncn-m001# kubectl get cronjobs.batch -n NAMESPACE \u0026#34;${CRON_JOB_NAME}\u0026#34; -o yaml \u0026gt; \u0026#34;${CRON_JOB_NAME}-cronjob.yaml\u0026#34; ncn-m001# vi \u0026#34;${CRON_JOB_NAME}-cronjob.yaml\u0026#34; Delete all lines that contain uid:.\nDelete the entire status: section, including the status key.\nSave the file and quit the editor.\nDelete the cronjob.\nncn-m001# kubectl delete -f \u0026#34;${CRON_JOB_NAME}-cronjob.yaml\u0026#34; Apply the cronjob.\nncn-m001# kubectl apply -f \u0026#34;${CRON_JOB_NAME}-cronjob.yaml\u0026#34; Verify that the cronjob has been scheduled.\nncn-m001# kubectl get cronjobs -n backups benji-k8s-backup-backups-namespace Example output:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE kube-etcdbackup */10 * * * * False 0 92s 29 Check the HSM inventory status of NCNs Use the sat command to check for management NCNs in an Off state.\nncn-m001# sat status --filter role=management --filter enabled=true \\ --filter=state=off --fields xname,aliases,state,flag,role,subrole Example output:\n+----------------+----------+-------+------+------------+---------+ | xname | Aliases | State | Flag | Role | SubRole | +----------------+----------+-------+------+------------+---------+ | x3000c0s13b0n0 | ncn-w004 | Off | OK | Management | Worker | | x3000c0s25b0n0 | ncn-w005 | Off | OK | Management | Worker | +----------------+----------+-------+------+------------+---------+ Attention: When the NCNs are brought back online after a power outage or planned shutdown, sat status may report them as being Off.\nRun a manual discovery of any NCNs in the Off state.\nncn-m001# cray hsm inventory discover create --xnames x3000c0s13b0n0,x3000c0s25b0n0 --format toml Example output:\n[[results]] URI = \u0026#34;/hsm/v2/Inventory/DiscoveryStatus/0\u0026#34; Check the NCN state.\nncn-m001# sat status --filter role=management --filter enabled=true \\ --filter=state=off --fields xname,aliases,state,flag,role,subrole Example output:\n+----------------+----------+-------+------+------------+---------+ | xname | Aliases | State | Flag | Role | SubRole | +----------------+----------+-------+------+------------+---------+ | x3000c0s1b0n0 | ncn-m001 | Ready | OK | Management | Master | | x3000c0s3b0n0 | ncn-m002 | Ready | OK | Management | Master | | x3000c0s5b0n0 | ncn-m003 | Ready | OK | Management | Master | | x3000c0s7b0n0 | ncn-w001 | Ready | OK | Management | Worker | | x3000c0s9b0n0 | ncn-w002 | Ready | OK | Management | Worker | | x3000c0s11b0n0 | ncn-w003 | Ready | OK | Management | Worker | | x3000c0s13b0n0 | ncn-w004 | Ready | OK | Management | Worker | | x3000c0s17b0n0 | ncn-s001 | Ready | OK | Management | Storage | | x3000c0s19b0n0 | ncn-s002 | Ready | OK | Management | Storage | | x3000c0s21b0n0 | ncn-s003 | Ready | OK | Management | Storage | | x3000c0s25b0n0 | ncn-w005 | Ready | OK | Management | Worker | +----------------+----------+-------+------+------------+---------+ Check whether CFS has run NCN personalization on the management nodes.\nIf a node has its Configuration Status set to configured, then that node has completed all configuration layers for post-boot CFS.\nIf any nodes have Configuration Status set to pending, then there should be a CFS session in progress which includes that node.\nIf any nodes have Configuration Status set to failed with Error Count set to 3, then the node was unable complete a layer of configuration.\nncn-m001# sat status --filter role=management --filter enabled=true --fields \\ xname,aliases,role,subrole,\u0026#34;desired config\u0026#34;,\u0026#34;configuration status\u0026#34;,\u0026#34;error count\u0026#34; Example output:\n+----------------+----------+------------+---------+---------------------+----------------------+-------------+ | xname | Aliases | Role | SubRole | Desired Config | Configuration Status | Error Count | +----------------+----------+------------+---------+---------------------+----------------------+-------------+ | x3000c0s1b0n0 | ncn-m001 | Management | Master | ncn-personalization | configured | 0 | | x3000c0s3b0n0 | ncn-m002 | Management | Master | ncn-personalization | configured | 0 | | x3000c0s5b0n0 | ncn-m003 | Management | Master | ncn-personalization | configured | 0 | | x3000c0s7b0n0 | ncn-w001 | Management | Worker | ncn-personalization | failed | 3 | | x3000c0s9b0n0 | ncn-w002 | Management | Worker | ncn-personalization | failed | 3 | | x3000c0s11b0n0 | ncn-w003 | Management | Worker | ncn-personalization | failed | 3 | | x3000c0s13b0n0 | ncn-w004 | Management | Worker | ncn-personalization | pending | 2 | | x3000c0s17b0n0 | ncn-s001 | Management | Storage | ncn-personalization | configured | 0 | | x3000c0s19b0n0 | ncn-s002 | Management | Storage | ncn-personalization | configured | 0 | | x3000c0s21b0n0 | ncn-s003 | Management | Storage | ncn-personalization | configured | 0 | | x3000c0s25b0n0 | ncn-w005 | Management | Worker | ncn-personalization | pending | 2 | +----------------+----------+------------+---------+---------------------+----------------------+-------------+ If some nodes are not fully configured, then find any CFS sessions in progress.\nncn-m001# kubectl -n services --sort-by=.metadata.creationTimestamp get pods | grep cfs Example output:\ncfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk 7/9 Error 0 21m cfs-157af6d5-b63d-48ba-9eb9-b33af9a8325d-tfj8x 3/9 Not Ready 0 11m CFS sessions which are in Not Ready status are still in progress. CFS sessions with status Error had a failure in one of the layers.\nInspect all layers of Ansible configuration to find a failed layer.\nncn-m001# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-0 ncn-m001# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-1 ncn-m001# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-2 ncn-m001# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-3 ncn-m001# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-4 ncn-m001# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-5 ncn-m001# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-6 ncn-m001# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-7 ncn-m001# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-8 Important: Some systems may have a failure to mount Lustre on the worker nodes during the COS 2.3 layer of configuration if their connection to the ClusterStor is via cables to Slingshot switches in the liquid-cooled cabinets which have not been powered up at this point in the power on procedure. This affects worker nodes which have Mellanox NICs. Worker nodes with Cassini NICs are unaffected. This may include systems which have Arista switches.\nNormally, CFS could be restarted for these worker nodes after the Slingshot switches in the liquid-cooled cabinets have been powered up. however there is a known problem with Slingshot 1.7.3a and earlier versions of the Slingshot Host Software (SHS) which require a special procedure in the COS 2.3 layer to address this problem.\nSee Worker Node COS Power Up Configuration.\nTo check the health and status of the management cluster after a power cycle, refer to the \u0026ldquo;Platform Health Checks\u0026rdquo; section in Validate CSM Health.\nNext step Return to System Power On Procedures and continue with next step.\n"
},
{
	"uri": "/docs-csm/en-12/operations/package_repository_management/repair_yum_repository_metadata/",
	"title": "Repair Yum Repository Metadata",
	"tags": [],
	"description": "",
	"content": "Repair Yum Repository Metadata Nexus may have trouble generating or regenerating repository metadata (for example, repodata/repomd.xml), especially for larger repositories. Configure the Repair - Rebuild Yum repository metadata (repodata) task in Nexus to create the metadata if the standard generation fails. This is not typically needed, so it is considered to be a repair task.\nThe example in this procedure is for creating a repair task to rebuild Yum metadata for the mirror-1.3.0-opensuse-leap-15 repository.\nSee the Nexus documentation on tasks for more information.\nPrerequisites System domain name Nexus web URL Procedure Troubleshooting Prerequisites CSM installation is complete.\nSystem domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system\u0026rsquo;s fully qualified domain name (FQDN).\nThe FQDN can be found by running the following command on any Kubernetes NCN.\nncn-mw# kubectl get secret site-init -n loftsman -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d | yq r - spec.network.dns.external Example output:\nsystem.hpc.amslabs.hpecorp.net Be sure to modify the example URLs on this page by replacing SYSTEM_DOMAIN_NAME with the actual value found using the above command.\nNexus web URL Nexus is accessible using a web browser at the following URL: https://nexus.cmn.SYSTEM_DOMAIN_NAME\nAn example of what the resulting URL will look like is: https://nexus.cmn.eniac.dev.cray.com.\nProcedure Log in to the Nexus web UI.\nUsers will need to log in through the Nexus UI. The account is configured through Keycloak with a role mapping for Nexus authentication. The role needed for administrative permissions is nx-admin in the system-nexus-client role. Scripts may connect by using a username and password in the request.\nClick on the gear icon at the top of the page.\nClicking on the gear will open up the repository administration page.\nClick on Tasks in the navigation bar on the left-hand side of the page.\nThe Tasks button is under the System heading.\nClick the Create Task button.\nClicking the Create Task button will open up the following page. Select the type of task. For this example, the Create Repair - Rebuild Yum repository metadata (repodata) option would be selected.\nEnter the required information for the task, such as the task name, repository, and task frequency.\nClick the Create task at the bottom of the page after entering all required information about the task.\nThe new task will now be available on the main Tasks page.\nClick on the newly created task.\nClick the Run button at the top of the page.\nSelect Yes when the confirmation pop-up appears.\nThere will now be information about the run that was just scheduled on the main Tasks page.\nClick on the box icon in the navigation bar at the top of the page, then click Browse in the navigation bar on the left-hand side of the page.\nTrack down the name of the repository being repaired.\nIn this example, mirror-1.3.0-opensuse-leap-15 is used.\nView the repodata for the repository.\nView the log file on the system.\nEven though the Nexus logs contains messages pertaining to tasks, it can be difficult to track messages for a specific task, especially because rebuilding Yum metadata takes a long time.\nRetrieve the Nexus pod name.\nncn-mw# kubectl -n nexus get pods | grep nexus Example output:\nnexus-55d8c77547-65k6q 2/2 Running 1 22h Access the running Nexus pod.\nncn-mw# kubectl -n nexus exec -ti nexus-55d8c77547-65k6q -c nexus -- ls -ltr /nexus-data/log/tasks Example output:\ntotal 8 -rw-r--r-- 1 nexus nexus 1763 Aug 23 00:50 repository.yum.rebuild.metadata-20200822235306934.log -rw-r--r-- 1 nexus nexus 1525 Aug 23 01:00 repository.cleanup-20200823010000013.log If multiple repositories are being rebuilt, search the logs for the specific repository to find the latest corresponding log file. The example below is for mirror-1.3.0-opensuse-leap-15:\nncn-mw# kubectl -n nexus exec -ti nexus-55d8c77547-65k6q -c nexus -- \\ grep -R \u0026#39;Rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15\u0026#39; /nexus-data/log/tasks Example output:\n/nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822235306934.log:2020-08-22 23:53:06,936+0000 INFO [event-12-thread-797] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15 View the log file for the rebuild.\nThe log file for a successful rebuild will look similar to the following:\nncn-mw# kubectl -n nexus exec -ti nexus-55d8c77547-65k6q -c nexus -- \\ cat /nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822235306934.log Example output:\n2020-08-22 23:53:06,934+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task information: 2020-08-22 23:53:06,935+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - ID: 35536bcd-3947-4ba9-8d6d-43dcadbb87ad 2020-08-22 23:53:06,935+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Type: repository.yum.rebuild.metadata 2020-08-22 23:53:06,935+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Name: Rebuild Yum metadata - mirror-1.3.0-opensuse-leap-15 2020-08-22 23:53:06,935+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Description: Rebuild metadata for mirror-1.3.0-opensuse-leap-15 2020-08-22 23:53:06,936+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task log: /nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822235306934.log 2020-08-22 23:53:06,936+0000 INFO [event-12-thread-797] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15 2020-08-22 23:53:06,936+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task complete 2020-08-23 00:50:47,468+0000 INFO [event-12-thread-797] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Finished rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15 The returned Finished rebuilding yum metadata for repository without any other ERROR or WARN messages indicates that the rebuild has completed successfully.\nIn this case, it took nearly 58 minutes to finish. The time it takes to run is related to the size of the repository, so expect the mirror-1.3.0- repositories to take a while.\nCheck the repodata for the repository again in the web UI.\nTroubleshooting When a rebuild fails, expect to see ERROR and WARN messages around the same time as the Finished rebuilding yum metadata for repository message.\nFor example, consider the log from a failed rebuild of mirror-1.3.0-opensuse-leap-15:\nncn-mw# kubectl -n nexus exec -ti nexus-55d8c77547-65k6q -c nexus -- \\ cat /nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822231259523.log Example output:\n2020-08-22 23:12:59,523+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task information: 2020-08-22 23:12:59,526+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - ID: 35536bcd-3947-4ba9-8d6d-43dcadbb87ad 2020-08-22 23:12:59,526+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Type: repository.yum.rebuild.metadata 2020-08-22 23:12:59,526+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Name: Rebuild Yum metadata - mirror-1.3.0-opensuse-leap-15 2020-08-22 23:12:59,527+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Description: Rebuild metadata for mirror-1.3.0-opensuse-leap-15 2020-08-22 23:12:59,529+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task log: /nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822231259523.log 2020-08-22 23:12:59,529+0000 INFO [event-12-thread-780] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15 2020-08-22 23:12:59,531+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task complete 2020-08-22 23:24:16,974+0000 INFO [Thread-1948 \u0026lt;command\u0026gt;sql.select from asset where (component IS NOT NULL AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)\u0026lt;/command\u0026gt;] *SYSTEM com.orientechnologies.common.profiler.OProfilerStub - $ANSI{green {db=component}} [TIP] Query \u0026#39;SELECT FROM asset WHERE (component IS NOT NULL AND attributes.yum.asset_kind = \u0026#34;RPM\u0026#34; ) AND (bucket = #59:1 )\u0026#39; returned a result set with more than 10000 records. Check if you really need all these records, or reduce the resultset by using a LIMIT to improve both performance and used RAM 2020-08-22 23:29:57,700+0000 INFO [event-12-thread-780] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Finished rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15 2020-08-22 23:29:57,701+0000 ERROR [event-12-thread-780] *SYSTEM com.google.common.eventbus.EventBus.nexus.async - Could not dispatch event org.sonatype.nexus.repository.yum.internal.createrepo.YumMetadataInvalidationEvent@75b487e7 to subscriber org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl$$EnhancerByGuice$$9db995@93053b8 method [public void org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.on(org.sonatype.nexus.repository.yum.internal.createrepo.YumMetadataInvalidationEvent)] org.sonatype.nexus.repository.InvalidContentException: Invalid RPM: external/noarch/redeclipse-data-1.5.6-lp151.2.5.noarch.rpm at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:108) at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:76) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.writeAssetToMetadata(CreateRepoServiceImpl.java:651) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.appendRpm(CreateRepoServiceImpl.java:511) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.loopAllRpmsAndAppend(CreateRepoServiceImpl.java:499) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.writeMetadata(CreateRepoServiceImpl.java:477) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.convertDirectoriesToMetadata(CreateRepoServiceImpl.java:180) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadata(CreateRepoServiceImpl.java:150) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadata(CreateRepoServiceImpl.java:134) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadataWithoutCaching(CreateRepoServiceImpl.java:125) at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:57) at org.sonatype.nexus.transaction.TransactionInterceptor.proceedWithTransaction(TransactionInterceptor.java:66) at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:55) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.buildMetadata(CreateRepoFacetImpl.java:196) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.on(CreateRepoFacetImpl.java:178) at sun.reflect.GeneratedMethodAccessor125.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:87) at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:144) at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:72) at org.sonatype.nexus.thread.internal.MDCAwareRunnable.run(MDCAwareRunnable.java:40) at org.apache.shiro.subject.support.SubjectRunnable.doRun(SubjectRunnable.java:120) at org.apache.shiro.subject.support.SubjectRunnable.run(SubjectRunnable.java:108) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.net.SocketTimeoutException: Read timed out at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:171) at java.net.SocketInputStream.read(SocketInputStream.java:141) at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137) at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198) at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176) at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) at java.io.BufferedInputStream.read(BufferedInputStream.java:345) at com.google.common.io.CountingInputStream.read(CountingInputStream.java:63) at java.security.DigestInputStream.read(DigestInputStream.java:161) at java.io.FilterInputStream.read(FilterInputStream.java:107) at com.google.common.io.ByteStreams.exhaust(ByteStreams.java:273) at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:97) ... 26 common frames omitted 2020-08-22 23:30:06,427+0000 WARN [Thread-1948 \u0026lt;command\u0026gt;sql.select from asset where (component IS NOT NULL AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)\u0026lt;/command\u0026gt;] *SYSTEM org.sonatype.nexus.repository.storage.OrientAsyncHelper$QueueFeedingResultListener - Timed out adding query result to queue 28dee0bf after 60 seconds, aborting query 2020-08-22 23:31:06,430+0000 WARN [Thread-1948 \u0026lt;command\u0026gt;sql.select from asset where (component IS NOT NULL AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)\u0026lt;/command\u0026gt;] *SYSTEM org.sonatype.nexus.repository.storage.OrientAsyncHelper$QueueFeedingResultListener - Timed out adding end marker to queue 28dee0bf after 60 seconds Any SQL warnings or notifications indicate the rebuild may have failed. Examine repodata/*.xml.gz file attributes, such as file size and last modified time, to determine if they are new compared to the timestamp on the Finished rebuilding yum metadata for repository message.\n2020-08-22 23:24:16,974+0000 INFO [Thread-1948 \u0026lt;command\u0026gt;sql.select from asset where (component IS NOT NULL AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)\u0026lt;/command\u0026gt;] *SYSTEM com.orientechnologies.common.profiler.OProfilerStub - $ANSI{green {db=component}} [TIP] Query \u0026#39;SELECT FROM asset WHERE (component IS NOT NULL AND attributes.yum.asset_kind = \u0026#34;RPM\u0026#34; ) AND (bucket = #59:1 )\u0026#39; returned a result set with more than 10000 records. Check if you really need all these records, or reduce the resultset by using a LIMIT to improve both performance and used RAM ... 2020-08-22 23:30:06,427+0000 WARN [Thread-1948 \u0026lt;command\u0026gt;sql.select from asset where (component IS NOT NULL AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)\u0026lt;/command\u0026gt;] *SYSTEM org.sonatype.nexus.repository.storage.OrientAsyncHelper$QueueFeedingResultListener - Timed out adding query result to queue 28dee0bf after 60 seconds, aborting query 2020-08-22 23:31:06,430+0000 WARN [Thread-1948 \u0026lt;command\u0026gt;sql.select from asset where (component IS NOT NULL AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)\u0026lt;/command\u0026gt;] *SYSTEM org.sonatype.nexus.repository.storage.OrientAsyncHelper$QueueFeedingResultListener - Timed out adding end marker to queue 28dee0bf after 60 seconds However, seeing an ERROR with a JVM stack trace is a key indication that the rebuild failed:\n2020-08-22 23:29:57,701+0000 ERROR [event-12-thread-780] *SYSTEM com.google.common.eventbus.EventBus.nexus.async - Could not dispatch event org.sonatype.nexus.repository.yum.internal.createrepo.YumMetadataInvalidationEvent@75b487e7 to subscriber org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl$$EnhancerByGuice$$9db995@93053b8 method [public void org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.on(org.sonatype.nexus.repository.yum.internal.createrepo.YumMetadataInvalidationEvent)] org.sonatype.nexus.repository.InvalidContentException: Invalid RPM: external/noarch/redeclipse-data-1.5.6-lp151.2.5.noarch.rpm at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:108) at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:76) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.writeAssetToMetadata(CreateRepoServiceImpl.java:651) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.appendRpm(CreateRepoServiceImpl.java:511) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.loopAllRpmsAndAppend(CreateRepoServiceImpl.java:499) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.writeMetadata(CreateRepoServiceImpl.java:477) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.convertDirectoriesToMetadata(CreateRepoServiceImpl.java:180) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadata(CreateRepoServiceImpl.java:150) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadata(CreateRepoServiceImpl.java:134) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadataWithoutCaching(CreateRepoServiceImpl.java:125) at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:57) at org.sonatype.nexus.transaction.TransactionInterceptor.proceedWithTransaction(TransactionInterceptor.java:66) at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:55) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.buildMetadata(CreateRepoFacetImpl.java:196) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.on(CreateRepoFacetImpl.java:178) at sun.reflect.GeneratedMethodAccessor125.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:87) at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:144) at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:72) at org.sonatype.nexus.thread.internal.MDCAwareRunnable.run(MDCAwareRunnable.java:40) at org.apache.shiro.subject.support.SubjectRunnable.doRun(SubjectRunnable.java:120) at org.apache.shiro.subject.support.SubjectRunnable.run(SubjectRunnable.java:108) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.net.SocketTimeoutException: Read timed out at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:171) at java.net.SocketInputStream.read(SocketInputStream.java:141) at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137) at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198) at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176) at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) at java.io.BufferedInputStream.read(BufferedInputStream.java:345) at com.google.common.io.CountingInputStream.read(CountingInputStream.java:63) at java.security.DigestInputStream.read(DigestInputStream.java:161) at java.io.FilterInputStream.read(FilterInputStream.java:107) at com.google.common.io.ByteStreams.exhaust(ByteStreams.java:273) at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:97) ... 26 common frames omitted "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/rebuild_ncns/validate_boot_loader/",
	"title": "Validate Boot Loader",
	"tags": [],
	"description": "",
	"content": "Validate Boot Loader Perform the following steps on ncn-m001 (where the latest docs-csm is installed).\nRun the script to ensure the local BOOTRAID has a valid kernel and initrd.\n/usr/share/doc/csm/scripts/check_bootloader_all_ncns.sh Workaround: CASMINST-2015 As a result of rebuilding any NCN(s), remove any dynamically assigned interface IP addresses that did not get released automatically by running the CASMINST-2015 script:\n/usr/share/doc/csm/scripts/CASMINST-2015.sh Once that is done only follow the steps in the section for the node type that was rebuilt:\nStorage Node "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_remove_replace_ncns/remove_ncn_data/",
	"title": "Remove NCN Data",
	"tags": [],
	"description": "",
	"content": "Remove NCN Data Description Remove NCN data to System Layout Service (SLS), Boot Script Service (BSS), and Hardware State Manager (HSM) as needed to remove an NCN.\nProcedure IMPORTANT: The following procedures assume that you have set the variables from the prerequisites section.\nPrepare for the procedure.\nObtain an API token.\nncn-mw# cd /usr/share/docs/csm/scripts/operations/node_management/Add_Remove_Replace_NCNs ncn-mw# export TOKEN=$(curl -s -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Set BMC credentials for the NCN (unless doing this for ncn-m001).\nThe default username is root. Set IPMI_USERNAME if this needs to be different for the given BMC.\nread -s is used in order to prevent the password from being echoed to the screen or saved in the shell history.\nncn-mw# read -r -s -p \u0026#34;BMC ${IPMI_USERNAME-root} password: \u0026#34; IPMI_PASSWORD ncn-mw# export IPMI_PASSWORD Fetch the status of the nodes.\nncn-mw# ./ncn_status.py --all Example output:\nfirst_master_hostname: ncn-m002 ncns: ncn-m001 x3000c0s1b0n0 master ncn-m002 x3000c0s3b0n0 master ncn-m003 x3000c0s5b0n0 master ncn-w001 x3000c0s7b0n0 worker ncn-w002 x3000c0s9b0n0 worker ncn-w003 x3000c0s11b0n0 worker ncn-w004 x3000c0s34b0n0 worker ncn-s001 x3000c0s13b0n0 storage ncn-s002 x3000c0s15b0n0 storage ncn-s003 x3000c0s17b0n0 storage ncn-s004 x3000c0s26b0n0 storage Fetch the status of the node to be removed.\nncn-mw# ./ncn_status.py --xname \u0026#34;${XNAME}\u0026#34; Example output:\n... x3000c0s26b0n0: xname: x3000c0s26b0n0 name: ncn-s004 parent: x3000c0s26b0 type: Node, Management, Storage sources: bss, hsm, sls ip_reservations: 10.1.1.19, 10.101.5.150, 10.101.5.214, 10.101.5.36, 10.252.1.21, 10.254.1.38 ip_reservations_name: ncn-s004-mtl, ncn-s004-can, x3000c0s26b0n0, ncn-s004-cmn, ncn-s004-nmn, ncn-s004-hmn ip_reservations_mac: a4:bf:01:38:f4:50, a4:bf:01:38:f4:50, , , a4:bf:01:38:f4:50, a4:bf:01:38:f4:50 ifnames: mgmt0:a4:bf:01:38:f4:50, mgmt1:a4:bf:01:38:f4:51, lan0:b8:59:9f:de:b4:8c, lan1:b8:59:9f:de:b4:8d ncn_macs: ifnames: mgmt0:a4:bf:01:38:f4:50, mgmt1:a4:bf:01:38:f4:51, lan0:b8:59:9f:de:b4:8c, lan1:b8:59:9f:de:b4:8d bmc_mac: a4:bf:01:38:f4:54 Important: Save the ifnames and bmc_mac information if planning to add this NCN back at some time in the future.\nShutdown cray-reds.\nncn-mw# kubectl -n services scale deployment cray-reds --replicas=0 Remove the node from SLS, HSM, and BSS.\nncn-mw# ./remove_management_ncn.py --xname \u0026#34;${XNAME}\u0026#34; Example output:\n... Permanently remove x3000c0s26b0n0 - ncn-s004 (y/n)? y ... Summary: Logs: /tmp/remove_management_ncn/x3000c0s26b0n0 xname: x3000c0s26b0n0 ncn_name: ncn-s004 ncn_macs: ifnames: mgmt0:a4:bf:01:38:f4:50, mgmt1:a4:bf:01:38:f4:51, lan0:b8:59:9f:de:b4:8c, lan1:b8:59:9f:de:b4:8d bmc_mac: a4:bf:01:38:f4:54 Successfully removed x3000c0s26b0n0 - ncn-s004 NOTE: If workers have been removed and the worker count is currently at two, then a timeout restarting cray-bss can be ignored. For example, the following failure output from remove_management_ncn.py can be ignored.\nWaiting for cray-bss to start. Do not kill this script. The wait will timeout in 10 minutes if bss does not fully start up. Ran: kubectl -n services rollout status deployment cray-bss --timeout=600s Error: Failed: 1 stdout: Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 0 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... stderr: error: timed out waiting for the condition Start cray-reds.\nncn-mw# kubectl -n services scale deployment cray-reds --replicas=1 Verify the results by fetching the status of the management nodes.\nncn-mw# ./ncn_status.py --all Example output:\nfirst_master_hostname: ncn-m002 ncns: ncn-m001 x3000c0s1b0n0 master ncn-m002 x3000c0s3b0n0 master ncn-m003 x3000c0s5b0n0 master ncn-w001 x3000c0s7b0n0 worker ncn-w002 x3000c0s9b0n0 worker ncn-w003 x3000c0s11b0n0 worker ncn-w004 x3000c0s34b0n0 worker ncn-s001 x3000c0s13b0n0 storage ncn-s002 x3000c0s15b0n0 storage ncn-s003 x3000c0s17b0n0 storage Fetch the status of the node that was removed.\nncn-mw# ./ncn_status.py --xname \u0026#34;${XNAME}\u0026#34; Example output:\nNot found: x3000c0s26b0n0 Proceed to Remove Switch Configuration or return to the main Add, Remove, Replace, or Move NCNs page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/check_dhcp_lease_is_getting_allocated/",
	"title": "Check DHCP lease is getting allocated",
	"tags": [],
	"description": "",
	"content": "Check DHCP lease is getting allocated Check the KEA logs and verify that the lease is getting allocated. kubectl logs -n services pod/$(kubectl get -n services pods | grep kea | head -n1 | cut -f 1 -d \u0026#39; \u0026#39;) -c cray-dhcp-kea 2021-04-21 00:13:05.416 INFO [kea-dhcp4.leases/24.139710796402304] DHCP4_LEASE_ ALLOC [hwtype=1 02:23:28:01:30:10], cid=[00:78:39:30:30:30:63:31:73:30:62:31], tid=0x21f2433a: lease 10.104.0.23 has been allocated for 300 seconds\nHere we can see that KEA is allocating a lease to 10.104.0.23. The lease MUST say DHCP4_LEASE_ALLOC, if it says DHCP4_LEASE_ADVERT, there is likely a problem. Restarting KEA will fix this issue most of the time. 2021-06-21 16:44:31.124 INFO [kea-dhcp4.leases/18.139837089017472] DHCP4_LEASE_ ADVERT [hwtype=1 14:02:ec:d9:79:88], cid=[no info], tid=0xe87fad10: lease 10.252.1.16 will be advertised\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/",
	"title": "Dell Installation and Configuration Guide",
	"tags": [],
	"description": "",
	"content": "Dell Installation and Configuration Guide This documentation helps network administrators and support personnel install and manage Dell network devices in a CSM install.\nThe HPE Cray recommended way of configuring the network is by using the CANU tool. Therefore this guide will not go into detail on how to configure each switch manually using the CLI. Instead, it will give helpful examples of how to configure/use features generated by CANU, in order to provide administrators easy ways to customize their installation.\nAlso included in this guide are the current documented and supported network scenarios.\nNOTE: Not every configuration option is covered here; for any configuration outside of the scope of this document, refer to the official Dell user manual. See Dell.\nThis document is intended for network administrators and support personnel.\nNOTE: The display and command lines illustrated in this document are examples and might not exactly match any particular environment. The switch and accessory drawings in this document are for illustration only, and may not exactly match installed products.\nCANU See CSM Automatic Network Utility (CANU)\nManaging Switches from the CLI Device Management Management Interface Network Time Protocol (NTP) Client Domain Name System (DNS) Client Hostname Domain Name Secure Shell (SSH) Remote Logging SNMPv2c Community System images Layer One Features Physical Interfaces Locator LED Layer Two Features Link Layer Discovery Protocol (LLDP) Virtual Local Access Networks (VLANs) VLAN Trunking 802.1Q Link Aggregation Group (LAG) Multiple Spanning Tree Protocol (MSTP) Layer Three Features VLAN Interface Address Resolution Protocol (ARP) Loopback Interface Multicast IGMP Security Access Control Lists (ACLs) Quality of Service (QoS) QoS Performing Upgrade on Dell Switches Software upgrade Backing up Switch Configuration Backing up switch configuration Factory Reset Reset configuration "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/canu/uninstall_canu/",
	"title": "Uninstall CANU",
	"tags": [],
	"description": "",
	"content": "Uninstall CANU Uninstalling CANU can be achieved by:\nlinux# pip3 uninstall canu "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/check_bgp_and_metallb/",
	"title": "Check BGP and MetalLB",
	"tags": [],
	"description": "",
	"content": "Check BGP and MetalLB Use the following procedure to verify if the spine switches are available and that MetalLB peering to the spine switches via Border Gateway Protocol (BGP) is established.\nPrerequisites Access to the spine switches is required.\nProcedure Log in to the spine switches.\nCheck that MetalLB is peering to the spines via BGP.\nCheck both spines if they are available (powered up):\nsw-spine# show ip bgp summary All the neighbors should be in the Established state.\nExample working state:\nVRF name : default BGP router identifier : 10.252.0.1 local AS number : 65533 BGP table version : 6 Main routing table version: 6 IPV4 Prefixes : 84 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.0.4 4 65533 465 501 6 0 0 0:03:37:43 ESTABLISHED/28 10.252.0.5 4 65533 463 501 6 0 0 0:03:36:51 ESTABLISHED/28 10.252.0.6 4 65533 463 500 6 0 0 0:03:36:39 ESTABLISHED/28 If the State/PfxRcd is IDLE, then restart the BGP process.\nsw-spine# clear ip bgp all Back to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/gateway_testing/",
	"title": "Gateway Testing",
	"tags": [],
	"description": "",
	"content": "Gateway Testing With the introduction of BiCAN, service APIs are now available on one or more networks depending on who is allowed access to the services and from where. The services are accessed via three different ingress gateways using a token that can be retrieved from Keycloak.\nThis page describes how to run a set of tests that determine if the gateways are functioning properly. The gateway test will obtain an API token from Keycloak and then use that token to attempt to access a set of service APIs on one or more networks, as defined in the gateway test definition file (gateway-test-defn.yaml). The test will check the return code to make sure it gets the expected response.\nWhen the nmnlb network is tested, the test will use api-gw-service-nmn.local as an override for nmnlb.\u0026lt;system-domain\u0026gt; in CSM v1.2. Optionally, setting use-api-gw-override: false in gateway-test-defn.yaml disables that override, and the test will use nmnlb.\u0026lt;system-domain\u0026gt;.\nTopics Running gateway tests on an NCN management node Running gateway tests on a UAN or compute node Running gateway tests on a UAI Running gateway tests on a device outside the system Example results Running gateway tests on an NCN management node The gateway test scripts can be found in /usr/share/doc/csm/scripts/operations/gateway-test. When gateway-test.py is run from an NCN, it has access to the admin client secret using kubectl. It will use the admin client secret to get the token for accessing the APIs.\nExecute the test by running following command.\nncn# /usr/share/doc/csm/scripts/operations/gateway-test/ncn-gateway-test.sh The test will cycle through all of the test networks specified in gateway-test-defn.yaml.\ntest-networks: - name: nmnlb gateway: services-gateway - name: cmn gateway: services-gateway - name: can gateway: customer-user-gateway - name: chn gateway: customer-user-gateway For each network, the test will attempt to obtain a token from Keycloak. On an NCN, it should be able to get a token from NMNLB, CMN, and either CAN or CHN depending on how the system is configured. It will then use that token to attempt to access each of the services defined in gateway-test-defn.yaml, on each of the test-networks. The test will determine whether it should or should not be able to access the service, and it will output a PASS or FAIL for each service, as appropriate. At the end of the tests it will compile and output a final overall PASS/FAIL status.\nRunning gateway tests on a UAN or compute node The same set of tests will be run from a UAN or Compute Node by executing the following command from an NCN that has the docs-csm RPM installed. The hostname of the UAN or Compute Node under test must be specified.\nBoth scripts will fetch the admin client secret, the configured user network, and the site domain from the system. It will use that information to generate a script that will be transferred to the UAN, executed, and removed. The networks that should be accessible are different on a UAN versus a Compute node. The script will determine the networks that should be accessible on the node based on the node type.\nThe test will determine whether it should or should not be able to access the service, and it will output a PASS or FAIL for each service, as appropriate. At the end of the tests it will compile and output a final overall PASS/FAIL status.\nUAN test execution ncn# /usr/share/doc/csm/scripts/operations/gateway-test/uan-gateway-test.sh \u0026lt;uan-hostname\u0026gt; Compute node test execution ncn# /usr/share/doc/csm/scripts/operations/gateway-test/cn-gateway-test.sh \u0026lt;cn-hostname\u0026gt; Running gateway tests on a UAI In order to test the gateways from a UAI, the /usr/share/doc/csm/scripts/operations/gateway-test/uai-gateway-test.sh script is used.\nThis script will execute the following steps:\nCreate a UAI with a cray-uai-gateway-test image. Pass the system domain, user network, and admin client secret to the test UAI. Execute gateway-test.py on the node. Output the results. Delete the test UAI. Run the test by executing the following command.\nncn# /usr/share/doc/csm/scripts/operations/gateway-test/uai-gateway-test.sh The test will find the first UAI cray-uai-gateway-test image to create the test UAI. A different image may optionally be specified by using the --imagename option.\nRunning gateway tests on a device outside the system The following steps must be performed on the system where the test is to be run:\npython3 must be installed (if it is not already).\nObtain the test code.\nThere are two options for doing this:\nInstall the docs-csm RPM.\nSee Check for Latest Documentation.\nCopy over the following files from a system where the docs-csm RPM is installed:\n/usr/share/doc/csm/scripts/operations/gateway-test/gateway-test.py /usr/share/doc/csm/scripts/operations/gateway-test/gateway-test-defn.yaml Obtain the admin client secret.\nBecause access to kubectl is not possible from outside of the cluster, obtain the admin client secret by running the following command on an NCN.\nncn# kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d Example output:\n26947343-d4ab-403b-14e937dbd700 Export the admin client secret in an environment variable.\nBack on the system where the tests will be run, set and export the ADMIN_CLIENT_SECRET environment variable, using the admin-client-auth secret obtained in the previous step.\nlinux# export ADMIN_CLIENT_SECRET=26947343-d4ab-403b-14e937dbd700 Execute the test.\nExecute the test by running following command. The system domain (in this example, eniac.dev.cray.com) must be specified.\nlinux# /usr/share/doc/csm/scripts/operations/gateway-test/gateway-test.py eniac.dev.cray.com outside Example results The results of running the tests will show the following:\nRetrieval of a token on the CMN network; the token is used to get SLS data, which determines which user network is configured on the system. If CMN is not accessible, then the test will get the user network from the command line. For each of the test networks defined in gateway-test-defn.yaml: Retrieval of a token on the network under test. It will attempt to access each of the services with the token and check the expected results. It will show PASS or FAIL depending on the expected response for the service and the token being used. It will show SKIP for services that are not expected to be installed on the system. The return code of gateway-test.py will be non-zero if any of the tests within it fail. Running from an NCN with CHN as the user network ncn# /usr/share/doc/csm/scripts/operations/gateway-test/gateway-test.py eniac.dev.cray.com auth.cmn.eniac.dev.cray.com is reachable Token successfully retrieved at https://auth.cmn.eniac.dev.cray.com/keycloak/realms/shasta/protocol/openid-connect/token Getting token for nmnlb api-gw-service-nmn.local is reachable Token successfully retrieved at https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token ------------- api-gw-service-nmn.local ------------------- api-gw-service-nmn.local is reachable PASS - [cray-bos]: https://api-gw-service-nmn.local/apis/bos/v1/session - 200 PASS - [cray-bss]: https://api-gw-service-nmn.local/apis/bss/boot/v1/bootparameters - 200 PASS - [cray-capmc]: https://api-gw-service-nmn.local/apis/capmc/capmc/get_node_rules - 200 PASS - [cray-cfs-api]: https://api-gw-service-nmn.local/apis/cfs/v2/sessions - 200 PASS - [cray-console-data]: https://api-gw-service-nmn.local/apis/consoledata/liveness - 204 PASS - [cray-console-node]: https://api-gw-service-nmn.local/apis/console-node/console-node/liveness - 204 PASS - [cray-console-operator]: https://api-gw-service-nmn.local/apis/console-operator/console-operator/liveness - 204 SKIP - [cray-cps]: https://api-gw-service-nmn.local/apis/v2/cps/contents - virtual service not found PASS - [cray-fas]: https://api-gw-service-nmn.local/apis/fas/v1/snapshots - 200 PASS - [cray-hbtd]: https://api-gw-service-nmn.local/apis/hbtd/hmi/v1/health - 200 PASS - [cray-hmnfd]: https://api-gw-service-nmn.local/apis/hmnfd/hmi/v2/health - 200 PASS - [cray-ims]: https://api-gw-service-nmn.local/apis/ims/images - 200 PASS - [cray-powerdns-manager]: https://api-gw-service-nmn.local/apis/powerdns-manager/v1/liveness - 204 PASS - [cray-reds]: https://api-gw-service-nmn.local/apis/reds/v1/liveness - 204 PASS - [cray-scsd]: https://api-gw-service-nmn.local/apis/scsd/v1/health - 200 PASS - [cray-sls]: https://api-gw-service-nmn.local/apis/sls/v1/health - 200 PASS - [cray-smd]: https://api-gw-service-nmn.local/apis/smd/hsm/v1/service/ready - 200 PASS - [cray-sts]: https://api-gw-service-nmn.local/apis/sts/healthz - 200 PASS - [cray-uas-mgr]: https://api-gw-service-nmn.local/apis/uas-mgr/v1/images - 200 SKIP - [nmdv2-service]: https://api-gw-service-nmn.local/apis/v2/nmd/dumps - virtual service not found SKIP - [slingshot-fabric-manager]: https://api-gw-service-nmn.local/apis/fabric-manager/fabric/port-policies - virtual service not found SKIP - [sma-telemetry]: https://api-gw-service-nmn.local/apis/sma-telemetry-api/v1/ping - virtual service not found ------------- api.cmn.eniac.dev.cray.com ------------------- api.cmn.eniac.dev.cray.com is reachable PASS - [cray-bos]: https://api.cmn.eniac.dev.cray.com/apis/bos/v1/session - 200 PASS - [cray-bss]: https://api.cmn.eniac.dev.cray.com/apis/bss/boot/v1/bootparameters - 200 PASS - [cray-capmc]: https://api.cmn.eniac.dev.cray.com/apis/capmc/capmc/get_node_rules - 200 PASS - [cray-cfs-api]: https://api.cmn.eniac.dev.cray.com/apis/cfs/v2/sessions - 200 PASS - [cray-console-data]: https://api.cmn.eniac.dev.cray.com/apis/consoledata/liveness - 204 PASS - [cray-console-node]: https://api.cmn.eniac.dev.cray.com/apis/console-node/console-node/liveness - 204 PASS - [cray-console-operator]: https://api.cmn.eniac.dev.cray.com/apis/console-operator/console-operator/liveness - 204 SKIP - [cray-cps]: https://api.cmn.eniac.dev.cray.com/apis/v2/cps/contents - virtual service not found PASS - [cray-fas]: https://api.cmn.eniac.dev.cray.com/apis/fas/v1/snapshots - 200 PASS - [cray-hbtd]: https://api.cmn.eniac.dev.cray.com/apis/hbtd/hmi/v1/health - 200 PASS - [cray-hmnfd]: https://api.cmn.eniac.dev.cray.com/apis/hmnfd/hmi/v2/health - 200 PASS - [cray-ims]: https://api.cmn.eniac.dev.cray.com/apis/ims/images - 200 PASS - [cray-powerdns-manager]: https://api.cmn.eniac.dev.cray.com/apis/powerdns-manager/v1/liveness - 204 PASS - [cray-reds]: https://api.cmn.eniac.dev.cray.com/apis/reds/v1/liveness - 204 PASS - [cray-scsd]: https://api.cmn.eniac.dev.cray.com/apis/scsd/v1/health - 200 PASS - [cray-sls]: https://api.cmn.eniac.dev.cray.com/apis/sls/v1/health - 200 PASS - [cray-smd]: https://api.cmn.eniac.dev.cray.com/apis/smd/hsm/v1/service/ready - 200 PASS - [cray-sts]: https://api.cmn.eniac.dev.cray.com/apis/sts/healthz - 200 PASS - [cray-uas-mgr]: https://api.cmn.eniac.dev.cray.com/apis/uas-mgr/v1/images - 200 SKIP - [nmdv2-service]: https://api.cmn.eniac.dev.cray.com/apis/v2/nmd/dumps - virtual service not found SKIP - [slingshot-fabric-manager]: https://api.cmn.eniac.dev.cray.com/apis/fabric-manager/fabric/port-policies - virtual service not found SKIP - [sma-telemetry]: https://api.cmn.eniac.dev.cray.com/apis/sma-telemetry-api/v1/ping - virtual service not found ------------- api.can.eniac.dev.cray.com ------------------- ping: api.can.eniac.dev.cray.com: Name or service not known api.can.eniac.dev.cray.com is NOT reachable can is not reachable and is not expected to be ------------- api.chn.eniac.dev.cray.com ------------------- api.chn.eniac.dev.cray.com is reachable PASS - [cray-bos]: https://api.chn.eniac.dev.cray.com/apis/bos/v1/session - 404 PASS - [cray-bss]: https://api.chn.eniac.dev.cray.com/apis/bss/boot/v1/bootparameters - 404 PASS - [cray-capmc]: https://api.chn.eniac.dev.cray.com/apis/capmc/capmc/get_node_rules - 404 PASS - [cray-cfs-api]: https://api.chn.eniac.dev.cray.com/apis/cfs/v2/sessions - 404 PASS - [cray-console-data]: https://api.chn.eniac.dev.cray.com/apis/consoledata/liveness - 404 PASS - [cray-console-node]: https://api.chn.eniac.dev.cray.com/apis/console-node/console-node/liveness - 404 PASS - [cray-console-operator]: https://api.chn.eniac.dev.cray.com/apis/console-operator/console-operator/liveness - 404 SKIP - [cray-cps]: https://api.chn.eniac.dev.cray.com/apis/v2/cps/contents - virtual service not found PASS - [cray-fas]: https://api.chn.eniac.dev.cray.com/apis/fas/v1/snapshots - 404 PASS - [cray-hbtd]: https://api.chn.eniac.dev.cray.com/apis/hbtd/hmi/v1/health - 404 PASS - [cray-hmnfd]: https://api.chn.eniac.dev.cray.com/apis/hmnfd/hmi/v2/health - 404 PASS - [cray-ims]: https://api.chn.eniac.dev.cray.com/apis/ims/images - 404 PASS - [cray-powerdns-manager]: https://api.chn.eniac.dev.cray.com/apis/powerdns-manager/v1/liveness - 404 PASS - [cray-reds]: https://api.chn.eniac.dev.cray.com/apis/reds/v1/liveness - 404 PASS - [cray-scsd]: https://api.chn.eniac.dev.cray.com/apis/scsd/v1/health - 404 PASS - [cray-sls]: https://api.chn.eniac.dev.cray.com/apis/sls/v1/health - 404 PASS - [cray-smd]: https://api.chn.eniac.dev.cray.com/apis/smd/hsm/v1/service/ready - 404 PASS - [cray-sts]: https://api.chn.eniac.dev.cray.com/apis/sts/healthz - 404 PASS - [cray-uas-mgr]: https://api.chn.eniac.dev.cray.com/apis/uas-mgr/v1/images - 404 SKIP - [nmdv2-service]: https://api.chn.eniac.dev.cray.com/apis/v2/nmd/dumps - virtual service not found SKIP - [slingshot-fabric-manager]: https://api.chn.eniac.dev.cray.com/apis/fabric-manager/fabric/port-policies - virtual service not found SKIP - [sma-telemetry]: https://api.chn.eniac.dev.cray.com/apis/sma-telemetry-api/v1/ping - virtual service not found Getting token for cmn auth.cmn.eniac.dev.cray.com is reachable Token successfully retrieved at https://auth.cmn.eniac.dev.cray.com/keycloak/realms/shasta/protocol/openid-connect/token ------------- api-gw-service-nmn.local ------------------- api-gw-service-nmn.local is reachable PASS - [cray-bos]: https://api-gw-service-nmn.local/apis/bos/v1/session - 200 PASS - [cray-bss]: https://api-gw-service-nmn.local/apis/bss/boot/v1/bootparameters - 200 PASS - [cray-capmc]: https://api-gw-service-nmn.local/apis/capmc/capmc/get_node_rules - 200 PASS - [cray-cfs-api]: https://api-gw-service-nmn.local/apis/cfs/v2/sessions - 200 PASS - [cray-console-data]: https://api-gw-service-nmn.local/apis/consoledata/liveness - 204 PASS - [cray-console-node]: https://api-gw-service-nmn.local/apis/console-node/console-node/liveness - 204 PASS - [cray-console-operator]: https://api-gw-service-nmn.local/apis/console-operator/console-operator/liveness - 204 SKIP - [cray-cps]: https://api-gw-service-nmn.local/apis/v2/cps/contents - virtual service not found PASS - [cray-fas]: https://api-gw-service-nmn.local/apis/fas/v1/snapshots - 200 PASS - [cray-hbtd]: https://api-gw-service-nmn.local/apis/hbtd/hmi/v1/health - 200 PASS - [cray-hmnfd]: https://api-gw-service-nmn.local/apis/hmnfd/hmi/v2/health - 200 PASS - [cray-ims]: https://api-gw-service-nmn.local/apis/ims/images - 200 PASS - [cray-powerdns-manager]: https://api-gw-service-nmn.local/apis/powerdns-manager/v1/liveness - 204 PASS - [cray-reds]: https://api-gw-service-nmn.local/apis/reds/v1/liveness - 204 PASS - [cray-scsd]: https://api-gw-service-nmn.local/apis/scsd/v1/health - 200 PASS - [cray-sls]: https://api-gw-service-nmn.local/apis/sls/v1/health - 200 PASS - [cray-smd]: https://api-gw-service-nmn.local/apis/smd/hsm/v1/service/ready - 200 PASS - [cray-sts]: https://api-gw-service-nmn.local/apis/sts/healthz - 200 PASS - [cray-uas-mgr]: https://api-gw-service-nmn.local/apis/uas-mgr/v1/images - 200 SKIP - [nmdv2-service]: https://api-gw-service-nmn.local/apis/v2/nmd/dumps - virtual service not found SKIP - [slingshot-fabric-manager]: https://api-gw-service-nmn.local/apis/fabric-manager/fabric/port-policies - virtual service not found SKIP - [sma-telemetry]: https://api-gw-service-nmn.local/apis/sma-telemetry-api/v1/ping - virtual service not found ------------- api.cmn.eniac.dev.cray.com ------------------- api.cmn.eniac.dev.cray.com is reachable PASS - [cray-bos]: https://api.cmn.eniac.dev.cray.com/apis/bos/v1/session - 200 PASS - [cray-bss]: https://api.cmn.eniac.dev.cray.com/apis/bss/boot/v1/bootparameters - 200 PASS - [cray-capmc]: https://api.cmn.eniac.dev.cray.com/apis/capmc/capmc/get_node_rules - 200 PASS - [cray-cfs-api]: https://api.cmn.eniac.dev.cray.com/apis/cfs/v2/sessions - 200 PASS - [cray-console-data]: https://api.cmn.eniac.dev.cray.com/apis/consoledata/liveness - 204 PASS - [cray-console-node]: https://api.cmn.eniac.dev.cray.com/apis/console-node/console-node/liveness - 204 PASS - [cray-console-operator]: https://api.cmn.eniac.dev.cray.com/apis/console-operator/console-operator/liveness - 204 SKIP - [cray-cps]: https://api.cmn.eniac.dev.cray.com/apis/v2/cps/contents - virtual service not found PASS - [cray-fas]: https://api.cmn.eniac.dev.cray.com/apis/fas/v1/snapshots - 200 PASS - [cray-hbtd]: https://api.cmn.eniac.dev.cray.com/apis/hbtd/hmi/v1/health - 200 PASS - [cray-hmnfd]: https://api.cmn.eniac.dev.cray.com/apis/hmnfd/hmi/v2/health - 200 PASS - [cray-ims]: https://api.cmn.eniac.dev.cray.com/apis/ims/images - 200 PASS - [cray-powerdns-manager]: https://api.cmn.eniac.dev.cray.com/apis/powerdns-manager/v1/liveness - 204 PASS - [cray-reds]: https://api.cmn.eniac.dev.cray.com/apis/reds/v1/liveness - 204 PASS - [cray-scsd]: https://api.cmn.eniac.dev.cray.com/apis/scsd/v1/health - 200 PASS - [cray-sls]: https://api.cmn.eniac.dev.cray.com/apis/sls/v1/health - 200 PASS - [cray-smd]: https://api.cmn.eniac.dev.cray.com/apis/smd/hsm/v1/service/ready - 200 PASS - [cray-sts]: https://api.cmn.eniac.dev.cray.com/apis/sts/healthz - 200 PASS - [cray-uas-mgr]: https://api.cmn.eniac.dev.cray.com/apis/uas-mgr/v1/images - 200 SKIP - [nmdv2-service]: https://api.cmn.eniac.dev.cray.com/apis/v2/nmd/dumps - virtual service not found SKIP - [slingshot-fabric-manager]: https://api.cmn.eniac.dev.cray.com/apis/fabric-manager/fabric/port-policies - virtual service not found SKIP - [sma-telemetry]: https://api.cmn.eniac.dev.cray.com/apis/sma-telemetry-api/v1/ping - virtual service not found ------------- api.can.eniac.dev.cray.com ------------------- ping: api.can.eniac.dev.cray.com: Name or service not known api.can.eniac.dev.cray.com is NOT reachable can is not reachable and is not expected to be ------------- api.chn.eniac.dev.cray.com ------------------- api.chn.eniac.dev.cray.com is reachable PASS - [cray-bos]: https://api.chn.eniac.dev.cray.com/apis/bos/v1/session - 404 PASS - [cray-bss]: https://api.chn.eniac.dev.cray.com/apis/bss/boot/v1/bootparameters - 404 PASS - [cray-capmc]: https://api.chn.eniac.dev.cray.com/apis/capmc/capmc/get_node_rules - 404 PASS - [cray-cfs-api]: https://api.chn.eniac.dev.cray.com/apis/cfs/v2/sessions - 404 PASS - [cray-console-data]: https://api.chn.eniac.dev.cray.com/apis/consoledata/liveness - 404 PASS - [cray-console-node]: https://api.chn.eniac.dev.cray.com/apis/console-node/console-node/liveness - 404 PASS - [cray-console-operator]: https://api.chn.eniac.dev.cray.com/apis/console-operator/console-operator/liveness - 404 SKIP - [cray-cps]: https://api.chn.eniac.dev.cray.com/apis/v2/cps/contents - virtual service not found PASS - [cray-fas]: https://api.chn.eniac.dev.cray.com/apis/fas/v1/snapshots - 404 PASS - [cray-hbtd]: https://api.chn.eniac.dev.cray.com/apis/hbtd/hmi/v1/health - 404 PASS - [cray-hmnfd]: https://api.chn.eniac.dev.cray.com/apis/hmnfd/hmi/v2/health - 404 PASS - [cray-ims]: https://api.chn.eniac.dev.cray.com/apis/ims/images - 404 PASS - [cray-powerdns-manager]: https://api.chn.eniac.dev.cray.com/apis/powerdns-manager/v1/liveness - 404 PASS - [cray-reds]: https://api.chn.eniac.dev.cray.com/apis/reds/v1/liveness - 404 PASS - [cray-scsd]: https://api.chn.eniac.dev.cray.com/apis/scsd/v1/health - 404 PASS - [cray-sls]: https://api.chn.eniac.dev.cray.com/apis/sls/v1/health - 404 PASS - [cray-smd]: https://api.chn.eniac.dev.cray.com/apis/smd/hsm/v1/service/ready - 404 PASS - [cray-sts]: https://api.chn.eniac.dev.cray.com/apis/sts/healthz - 404 PASS - [cray-uas-mgr]: https://api.chn.eniac.dev.cray.com/apis/uas-mgr/v1/images - 404 SKIP - [nmdv2-service]: https://api.chn.eniac.dev.cray.com/apis/v2/nmd/dumps - virtual service not found SKIP - [slingshot-fabric-manager]: https://api.chn.eniac.dev.cray.com/apis/fabric-manager/fabric/port-policies - virtual service not found SKIP - [sma-telemetry]: https://api.chn.eniac.dev.cray.com/apis/sma-telemetry-api/v1/ping - virtual service not found Getting token for can ping: auth.can.eniac.dev.cray.com: Name or service not known auth.can.eniac.dev.cray.com is NOT reachable Getting token for chn auth.chn.eniac.dev.cray.com is reachable Token successfully retrieved at https://auth.chn.eniac.dev.cray.com/keycloak/realms/shasta/protocol/openid-connect/token ------------- api-gw-service-nmn.local ------------------- api-gw-service-nmn.local is reachable PASS - [cray-bos]: https://api-gw-service-nmn.local/apis/bos/v1/session - 403 PASS - [cray-bss]: https://api-gw-service-nmn.local/apis/bss/boot/v1/bootparameters - 403 PASS - [cray-capmc]: https://api-gw-service-nmn.local/apis/capmc/capmc/get_node_rules - 403 PASS - [cray-cfs-api]: https://api-gw-service-nmn.local/apis/cfs/v2/sessions - 403 PASS - [cray-console-data]: https://api-gw-service-nmn.local/apis/consoledata/liveness - 403 PASS - [cray-console-node]: https://api-gw-service-nmn.local/apis/console-node/console-node/liveness - 403 PASS - [cray-console-operator]: https://api-gw-service-nmn.local/apis/console-operator/console-operator/liveness - 403 SKIP - [cray-cps]: https://api-gw-service-nmn.local/apis/v2/cps/contents - virtual service not found PASS - [cray-fas]: https://api-gw-service-nmn.local/apis/fas/v1/snapshots - 403 PASS - [cray-hbtd]: https://api-gw-service-nmn.local/apis/hbtd/hmi/v1/health - 403 PASS - [cray-hmnfd]: https://api-gw-service-nmn.local/apis/hmnfd/hmi/v2/health - 403 PASS - [cray-ims]: https://api-gw-service-nmn.local/apis/ims/images - 403 PASS - [cray-powerdns-manager]: https://api-gw-service-nmn.local/apis/powerdns-manager/v1/liveness - 403 PASS - [cray-reds]: https://api-gw-service-nmn.local/apis/reds/v1/liveness - 403 PASS - [cray-scsd]: https://api-gw-service-nmn.local/apis/scsd/v1/health - 403 PASS - [cray-sls]: https://api-gw-service-nmn.local/apis/sls/v1/health - 403 PASS - [cray-smd]: https://api-gw-service-nmn.local/apis/smd/hsm/v1/service/ready - 403 PASS - [cray-sts]: https://api-gw-service-nmn.local/apis/sts/healthz - 403 PASS - [cray-uas-mgr]: https://api-gw-service-nmn.local/apis/uas-mgr/v1/images - 403 SKIP - [nmdv2-service]: https://api-gw-service-nmn.local/apis/v2/nmd/dumps - virtual service not found SKIP - [slingshot-fabric-manager]: https://api-gw-service-nmn.local/apis/fabric-manager/fabric/port-policies - virtual service not found SKIP - [sma-telemetry]: https://api-gw-service-nmn.local/apis/sma-telemetry-api/v1/ping - virtual service not found ------------- api.cmn.eniac.dev.cray.com ------------------- api.cmn.eniac.dev.cray.com is reachable PASS - [cray-bos]: https://api.cmn.eniac.dev.cray.com/apis/bos/v1/session - 403 PASS - [cray-bss]: https://api.cmn.eniac.dev.cray.com/apis/bss/boot/v1/bootparameters - 403 PASS - [cray-capmc]: https://api.cmn.eniac.dev.cray.com/apis/capmc/capmc/get_node_rules - 403 PASS - [cray-cfs-api]: https://api.cmn.eniac.dev.cray.com/apis/cfs/v2/sessions - 403 PASS - [cray-console-data]: https://api.cmn.eniac.dev.cray.com/apis/consoledata/liveness - 403 PASS - [cray-console-node]: https://api.cmn.eniac.dev.cray.com/apis/console-node/console-node/liveness - 403 PASS - [cray-console-operator]: https://api.cmn.eniac.dev.cray.com/apis/console-operator/console-operator/liveness - 403 SKIP - [cray-cps]: https://api.cmn.eniac.dev.cray.com/apis/v2/cps/contents - virtual service not found PASS - [cray-fas]: https://api.cmn.eniac.dev.cray.com/apis/fas/v1/snapshots - 403 PASS - [cray-hbtd]: https://api.cmn.eniac.dev.cray.com/apis/hbtd/hmi/v1/health - 403 PASS - [cray-hmnfd]: https://api.cmn.eniac.dev.cray.com/apis/hmnfd/hmi/v2/health - 403 PASS - [cray-ims]: https://api.cmn.eniac.dev.cray.com/apis/ims/images - 403 PASS - [cray-powerdns-manager]: https://api.cmn.eniac.dev.cray.com/apis/powerdns-manager/v1/liveness - 403 PASS - [cray-reds]: https://api.cmn.eniac.dev.cray.com/apis/reds/v1/liveness - 403 PASS - [cray-scsd]: https://api.cmn.eniac.dev.cray.com/apis/scsd/v1/health - 403 PASS - [cray-sls]: https://api.cmn.eniac.dev.cray.com/apis/sls/v1/health - 403 PASS - [cray-smd]: https://api.cmn.eniac.dev.cray.com/apis/smd/hsm/v1/service/ready - 403 PASS - [cray-sts]: https://api.cmn.eniac.dev.cray.com/apis/sts/healthz - 403 PASS - [cray-uas-mgr]: https://api.cmn.eniac.dev.cray.com/apis/uas-mgr/v1/images - 403 SKIP - [nmdv2-service]: https://api.cmn.eniac.dev.cray.com/apis/v2/nmd/dumps - virtual service not found SKIP - [slingshot-fabric-manager]: https://api.cmn.eniac.dev.cray.com/apis/fabric-manager/fabric/port-policies - virtual service not found SKIP - [sma-telemetry]: https://api.cmn.eniac.dev.cray.com/apis/sma-telemetry-api/v1/ping - virtual service not found ------------- api.can.eniac.dev.cray.com ------------------- ping: api.can.eniac.dev.cray.com: Name or service not known api.can.eniac.dev.cray.com is NOT reachable can is not reachable and is not expected to be ------------- api.chn.eniac.dev.cray.com ------------------- api.chn.eniac.dev.cray.com is reachable PASS - [cray-bos]: https://api.chn.eniac.dev.cray.com/apis/bos/v1/session - 404 PASS - [cray-bss]: https://api.chn.eniac.dev.cray.com/apis/bss/boot/v1/bootparameters - 404 PASS - [cray-capmc]: https://api.chn.eniac.dev.cray.com/apis/capmc/capmc/get_node_rules - 404 PASS - [cray-cfs-api]: https://api.chn.eniac.dev.cray.com/apis/cfs/v2/sessions - 404 PASS - [cray-console-data]: https://api.chn.eniac.dev.cray.com/apis/consoledata/liveness - 404 PASS - [cray-console-node]: https://api.chn.eniac.dev.cray.com/apis/console-node/console-node/liveness - 404 PASS - [cray-console-operator]: https://api.chn.eniac.dev.cray.com/apis/console-operator/console-operator/liveness - 404 SKIP - [cray-cps]: https://api.chn.eniac.dev.cray.com/apis/v2/cps/contents - virtual service not found PASS - [cray-fas]: https://api.chn.eniac.dev.cray.com/apis/fas/v1/snapshots - 404 PASS - [cray-hbtd]: https://api.chn.eniac.dev.cray.com/apis/hbtd/hmi/v1/health - 404 PASS - [cray-hmnfd]: https://api.chn.eniac.dev.cray.com/apis/hmnfd/hmi/v2/health - 404 PASS - [cray-ims]: https://api.chn.eniac.dev.cray.com/apis/ims/images - 404 PASS - [cray-powerdns-manager]: https://api.chn.eniac.dev.cray.com/apis/powerdns-manager/v1/liveness - 404 PASS - [cray-reds]: https://api.chn.eniac.dev.cray.com/apis/reds/v1/liveness - 404 PASS - [cray-scsd]: https://api.chn.eniac.dev.cray.com/apis/scsd/v1/health - 404 PASS - [cray-sls]: https://api.chn.eniac.dev.cray.com/apis/sls/v1/health - 404 PASS - [cray-smd]: https://api.chn.eniac.dev.cray.com/apis/smd/hsm/v1/service/ready - 404 PASS - [cray-sts]: https://api.chn.eniac.dev.cray.com/apis/sts/healthz - 404 PASS - [cray-uas-mgr]: https://api.chn.eniac.dev.cray.com/apis/uas-mgr/v1/images - 404 SKIP - [nmdv2-service]: https://api.chn.eniac.dev.cray.com/apis/v2/nmd/dumps - virtual service not found SKIP - [slingshot-fabric-manager]: https://api.chn.eniac.dev.cray.com/apis/fabric-manager/fabric/port-policies - virtual service not found SKIP - [sma-telemetry]: https://api.chn.eniac.dev.cray.com/apis/sma-telemetry-api/v1/ping - virtual service not found Overall Gateway Test Status: PASS Running from a UAI ncn# /usr/share/doc/csm/scripts/operations/gateway-test/uai-gateway-test.sh Creating Gateway Test UAI with image artifactory.algol60.net/csm-docker/stable/cray-gateway_test:1.4.0-20220418215843_786bfac Waiting for uai-vers-733eea45 to be ready status = Running: Not Ready status = Running: Ready System domain is eniac.dev.cray.com User Network on eniac is chn Got admin client secret Running gateway tests on the UAI...(this may take 1-2 minutes) Getting token for nmnlb api-gw-service-nmn.local is NOT reachable Getting token for cmn auth.cmn.eniac.dev.cray.com is NOT reachable Getting token for can auth.can.eniac.dev.cray.com is reachable Token successfully retrieved at https://auth.can.eniac.dev.cray.com/keycloak/realms/shasta/protocol/openid-connect/token ------------- api-gw-service-nmn.local ------------------- api-gw-service-nmn.local is NOT reachable nmnlb is not reachable and is not expected to be ------------- api.cmn.eniac.dev.cray.com ------------------- api.cmn.eniac.dev.cray.com is NOT reachable cmn is not reachable and is not expected to be ------------- api.can.eniac.dev.cray.com ------------------- api.can.eniac.dev.cray.com is reachable PASS - [cray-bos]: https://api.can.eniac.dev.cray.com/apis/bos/v1/session - 404 PASS - [cray-bss]: https://api.can.eniac.dev.cray.com/apis/bss/boot/v1/bootparameters - 404 PASS - [cray-capmc]: https://api.can.eniac.dev.cray.com/apis/capmc/capmc/get_node_rules - 404 PASS - [cray-cfs-api]: https://api.can.eniac.dev.cray.com/apis/cfs/v2/sessions - 404 PASS - [cray-console-data]: https://api.can.eniac.dev.cray.com/apis/consoledata/liveness - 404 PASS - [cray-console-node]: https://api.can.eniac.dev.cray.com/apis/console-node/console-node/liveness - 404 PASS - [cray-console-operator]: https://api.can.eniac.dev.cray.com/apis/console-operator/console-operator/liveness - 404 SKIP - [cray-cps]: https://api.can.eniac.dev.cray.com/apis/v2/cps/contents PASS - [cray-fas]: https://api.can.eniac.dev.cray.com/apis/fas/v1/snapshots - 404 PASS - [cray-hbtd]: https://api.can.eniac.dev.cray.com/apis/hbtd/hmi/v1/health - 404 PASS - [cray-hmnfd]: https://api.can.eniac.dev.cray.com/apis/hmnfd/hmi/v2/health - 404 PASS - [cray-ims]: https://api.can.eniac.dev.cray.com/apis/ims/images - 404 PASS - [cray-powerdns-manager]: https://api.can.eniac.dev.cray.com/apis/powerdns-manager/v1/liveness - 404 PASS - [cray-reds]: https://api.can.eniac.dev.cray.com/apis/reds/v1/liveness - 404 PASS - [cray-scsd]: https://api.can.eniac.dev.cray.com/apis/scsd/v1/health - 404 PASS - [cray-sls]: https://api.can.eniac.dev.cray.com/apis/sls/v1/health - 404 PASS - [cray-smd]: https://api.can.eniac.dev.cray.com/apis/smd/hsm/v1/service/ready - 404 PASS - [cray-sts]: https://api.can.eniac.dev.cray.com/apis/sts/healthz - 404 PASS - [cray-uas-mgr]: https://api.can.eniac.dev.cray.com/apis/uas-mgr/v1/images - 404 SKIP - [nmdv2-service]: https://api.can.eniac.dev.cray.com/apis/v2/nmd/dumps SKIP - [slingshot-fabric-manager]: https://api.can.eniac.dev.cray.com/apis/fabric-manager/fabric/port-policies SKIP - [sma-telemetry]: https://api.can.eniac.dev.cray.com/apis/sma-telemetry-api/v1/ping ------------- api.chn.eniac.dev.cray.com ------------------- api.chn.eniac.dev.cray.com is reachable PASS - [cray-bos]: https://api.chn.eniac.dev.cray.com/apis/bos/v1/session - 404 PASS - [cray-bss]: https://api.chn.eniac.dev.cray.com/apis/bss/boot/v1/bootparameters - 404 PASS - [cray-capmc]: https://api.chn.eniac.dev.cray.com/apis/capmc/capmc/get_node_rules - 404 PASS - [cray-cfs-api]: https://api.chn.eniac.dev.cray.com/apis/cfs/v2/sessions - 404 PASS - [cray-console-data]: https://api.chn.eniac.dev.cray.com/apis/consoledata/liveness - 404 PASS - [cray-console-node]: https://api.chn.eniac.dev.cray.com/apis/console-node/console-node/liveness - 404 PASS - [cray-console-operator]: https://api.chn.eniac.dev.cray.com/apis/console-operator/console-operator/liveness - 404 SKIP - [cray-cps]: https://api.chn.eniac.dev.cray.com/apis/v2/cps/contents PASS - [cray-fas]: https://api.chn.eniac.dev.cray.com/apis/fas/v1/snapshots - 404 PASS - [cray-hbtd]: https://api.chn.eniac.dev.cray.com/apis/hbtd/hmi/v1/health - 404 PASS - [cray-hmnfd]: https://api.chn.eniac.dev.cray.com/apis/hmnfd/hmi/v2/health - 404 PASS - [cray-ims]: https://api.chn.eniac.dev.cray.com/apis/ims/images - 404 PASS - [cray-powerdns-manager]: https://api.chn.eniac.dev.cray.com/apis/powerdns-manager/v1/liveness - 404 PASS - [cray-reds]: https://api.chn.eniac.dev.cray.com/apis/reds/v1/liveness - 404 PASS - [cray-scsd]: https://api.chn.eniac.dev.cray.com/apis/scsd/v1/health - 404 PASS - [cray-sls]: https://api.chn.eniac.dev.cray.com/apis/sls/v1/health - 404 PASS - [cray-smd]: https://api.chn.eniac.dev.cray.com/apis/smd/hsm/v1/service/ready - 404 PASS - [cray-sts]: https://api.chn.eniac.dev.cray.com/apis/sts/healthz - 404 PASS - [cray-uas-mgr]: https://api.chn.eniac.dev.cray.com/apis/uas-mgr/v1/images - 404 SKIP - [nmdv2-service]: https://api.chn.eniac.dev.cray.com/apis/v2/nmd/dumps SKIP - [slingshot-fabric-manager]: https://api.chn.eniac.dev.cray.com/apis/fabric-manager/fabric/port-policies SKIP - [sma-telemetry]: https://api.chn.eniac.dev.cray.com/apis/sma-telemetry-api/v1/ping Getting token for chn auth.chn.eniac.dev.cray.com is reachable Token successfully retrieved at https://auth.chn.eniac.dev.cray.com/keycloak/realms/shasta/protocol/openid-connect/token ------------- api-gw-service-nmn.local ------------------- api-gw-service-nmn.local is NOT reachable nmnlb is not reachable and is not expected to be ------------- api.cmn.eniac.dev.cray.com ------------------- api.cmn.eniac.dev.cray.com is NOT reachable cmn is not reachable and is not expected to be ------------- api.can.eniac.dev.cray.com ------------------- api.can.eniac.dev.cray.com is reachable PASS - [cray-bos]: https://api.can.eniac.dev.cray.com/apis/bos/v1/session - 404 PASS - [cray-bss]: https://api.can.eniac.dev.cray.com/apis/bss/boot/v1/bootparameters - 404 PASS - [cray-capmc]: https://api.can.eniac.dev.cray.com/apis/capmc/capmc/get_node_rules - 404 PASS - [cray-cfs-api]: https://api.can.eniac.dev.cray.com/apis/cfs/v2/sessions - 404 PASS - [cray-console-data]: https://api.can.eniac.dev.cray.com/apis/consoledata/liveness - 404 PASS - [cray-console-node]: https://api.can.eniac.dev.cray.com/apis/console-node/console-node/liveness - 404 PASS - [cray-console-operator]: https://api.can.eniac.dev.cray.com/apis/console-operator/console-operator/liveness - 404 SKIP - [cray-cps]: https://api.can.eniac.dev.cray.com/apis/v2/cps/contents PASS - [cray-fas]: https://api.can.eniac.dev.cray.com/apis/fas/v1/snapshots - 404 PASS - [cray-hbtd]: https://api.can.eniac.dev.cray.com/apis/hbtd/hmi/v1/health - 404 PASS - [cray-hmnfd]: https://api.can.eniac.dev.cray.com/apis/hmnfd/hmi/v2/health - 404 PASS - [cray-ims]: https://api.can.eniac.dev.cray.com/apis/ims/images - 404 PASS - [cray-powerdns-manager]: https://api.can.eniac.dev.cray.com/apis/powerdns-manager/v1/liveness - 404 PASS - [cray-reds]: https://api.can.eniac.dev.cray.com/apis/reds/v1/liveness - 404 PASS - [cray-scsd]: https://api.can.eniac.dev.cray.com/apis/scsd/v1/health - 404 PASS - [cray-sls]: https://api.can.eniac.dev.cray.com/apis/sls/v1/health - 404 PASS - [cray-smd]: https://api.can.eniac.dev.cray.com/apis/smd/hsm/v1/service/ready - 404 PASS - [cray-sts]: https://api.can.eniac.dev.cray.com/apis/sts/healthz - 404 PASS - [cray-uas-mgr]: https://api.can.eniac.dev.cray.com/apis/uas-mgr/v1/images - 404 SKIP - [nmdv2-service]: https://api.can.eniac.dev.cray.com/apis/v2/nmd/dumps SKIP - [slingshot-fabric-manager]: https://api.can.eniac.dev.cray.com/apis/fabric-manager/fabric/port-policies SKIP - [sma-telemetry]: https://api.can.eniac.dev.cray.com/apis/sma-telemetry-api/v1/ping ------------- api.chn.eniac.dev.cray.com ------------------- api.chn.eniac.dev.cray.com is reachable PASS - [cray-bos]: https://api.chn.eniac.dev.cray.com/apis/bos/v1/session - 404 PASS - [cray-bss]: https://api.chn.eniac.dev.cray.com/apis/bss/boot/v1/bootparameters - 404 PASS - [cray-capmc]: https://api.chn.eniac.dev.cray.com/apis/capmc/capmc/get_node_rules - 404 PASS - [cray-cfs-api]: https://api.chn.eniac.dev.cray.com/apis/cfs/v2/sessions - 404 PASS - [cray-console-data]: https://api.chn.eniac.dev.cray.com/apis/consoledata/liveness - 404 PASS - [cray-console-node]: https://api.chn.eniac.dev.cray.com/apis/console-node/console-node/liveness - 404 PASS - [cray-console-operator]: https://api.chn.eniac.dev.cray.com/apis/console-operator/console-operator/liveness - 404 SKIP - [cray-cps]: https://api.chn.eniac.dev.cray.com/apis/v2/cps/contents PASS - [cray-fas]: https://api.chn.eniac.dev.cray.com/apis/fas/v1/snapshots - 404 PASS - [cray-hbtd]: https://api.chn.eniac.dev.cray.com/apis/hbtd/hmi/v1/health - 404 PASS - [cray-hmnfd]: https://api.chn.eniac.dev.cray.com/apis/hmnfd/hmi/v2/health - 404 PASS - [cray-ims]: https://api.chn.eniac.dev.cray.com/apis/ims/images - 404 PASS - [cray-powerdns-manager]: https://api.chn.eniac.dev.cray.com/apis/powerdns-manager/v1/liveness - 404 PASS - [cray-reds]: https://api.chn.eniac.dev.cray.com/apis/reds/v1/liveness - 404 PASS - [cray-scsd]: https://api.chn.eniac.dev.cray.com/apis/scsd/v1/health - 404 PASS - [cray-sls]: https://api.chn.eniac.dev.cray.com/apis/sls/v1/health - 404 PASS - [cray-smd]: https://api.chn.eniac.dev.cray.com/apis/smd/hsm/v1/service/ready - 404 PASS - [cray-sts]: https://api.chn.eniac.dev.cray.com/apis/sts/healthz - 404 PASS - [cray-uas-mgr]: https://api.chn.eniac.dev.cray.com/apis/uas-mgr/v1/images - 404 SKIP - [nmdv2-service]: https://api.chn.eniac.dev.cray.com/apis/v2/nmd/dumps SKIP - [slingshot-fabric-manager]: https://api.chn.eniac.dev.cray.com/apis/fabric-manager/fabric/port-policies SKIP - [sma-telemetry]: https://api.chn.eniac.dev.cray.com/apis/sma-telemetry-api/v1/ping Overall Gateway Test Status: PASS Deleting UAI uai-vers-733eea45 results = [ \u0026#34;Successfully deleted uai-vers-733eea45\u0026#34;,] "
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/check_the_health_and_balance_of_etcd_clusters/",
	"title": "Check the Health and Balance of etcd Clusters",
	"tags": [],
	"description": "",
	"content": "Check the Health and Balance of etcd Clusters Check to see if all of the etcd clusters have healthy pods, are balanced, and have a healthy cluster database. There needs to be the same number of pods running on each worker node for the etcd clusters to be balanced. If the number of pods is not the same for each worker node, the cluster is not balanced.\nAny clusters that do not have healthy pods will need to be rebuilt. Kubernetes cluster data will not be stored as efficiently when etcd clusters are not balanced.\nPrerequisites This procedure requires root privileges.\nProcedure Check the health of the clusters.\nTo check the health of the etcd clusters in the services namespace without TLS authentication:\nncn-mw# for pod in $(kubectl get pods -l app=etcd -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do echo \u0026#34;### ${pod} ###\u0026#34; kubectl -n services exec ${pod} -c etcd -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl endpoint health\u0026#34; done Example output:\n### cray-bos-etcd-6nkn6dzhv7 ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.670457ms ### cray-bos-etcd-6xtp2gqs64 ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 954.462µs ### cray-bos-etcd-gnt9rxcbvl ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.313505ms ### cray-bss-etcd-4jsn7p49rj ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 2.054509ms ### cray-bss-etcd-9q6xf5wl5q ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.174929ms ### cray-bss-etcd-ncwkjmlq8b ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.632738ms ### cray-cps-etcd-8ml5whzhjh ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.792795ms [...] If any of the etcd clusters are not healthy, refer to Rebuild Unhealthy etcd Clusters.\nCheck the number of pods in each cluster and verify they are balanced.\nEach cluster should contain at least three pods, but may contain more. Ensure that no two pods in a given cluster exist on the same worker node.\nncn-mw# kubectl get pod -n services -o wide | head -n 1 ; \\ for cluster in $(kubectl get etcdclusters.etcd.database.coreos.com -n services | grep -v NAME | awk \u0026#39;{print $1}\u0026#39;) do kubectl get pod -n services -o wide | grep $cluster; echo \u0026#34;\u0026#34; done Example output:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATE cray-bos-etcd-7gl9dccmrq 1/1 Running 0 8d 10.40.0.88 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-bos-etcd-g65fjhhlbg 1/1 Running 0 8d 10.42.0.36 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-bos-etcd-lbsppj5kt7 1/1 Running 0 20h 10.47.0.98 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-bss-etcd-dbhxvz824w 1/1 Running 0 8d 10.42.0.45 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-bss-etcd-hzpbrcn2pb 1/1 Running 0 20h 10.47.0.99 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-bss-etcd-kpc64v64wd 1/1 Running 0 8d 10.40.0.43 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-cps-etcd-8ndvn4dlx4 1/1 Running 0 20h 10.47.0.100 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-cps-etcd-gvlql48gwk 1/1 Running 0 8d 10.40.0.89 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-cps-etcd-wsvhmp4f7p 1/1 Running 0 8d 10.42.0.64 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-crus-etcd-2wvb2bpczb 1/1 Running 0 20h 10.47.0.117 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-crus-etcd-fhbcvknghh 1/1 Running 0 8d 10.42.0.34 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-crus-etcd-nrxqzftrzr 1/1 Running 0 8d 10.40.0.45 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-fas-etcd-29qcrd8qdt 1/1 Running 0 20h 10.47.0.102 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-fas-etcd-987c87m4mv 1/1 Running 0 8d 10.40.0.66 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-fas-etcd-9fxbzkzrsv 1/1 Running 0 8d 10.42.0.43 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-hbtd-etcd-2sf24nw5zs 1/1 Running 0 8d 10.40.0.78 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-hbtd-etcd-5r6mgvjct8 1/1 Running 0 20h 10.47.0.105 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-hbtd-etcd-t78x5wqkjt 1/1 Running 0 8d 10.42.0.51 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-hmnfd-etcd-99j5zt5ln6 1/1 Running 0 8d 10.40.0.74 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-hmnfd-etcd-h9gnvvs7rs 1/1 Running 0 8d 10.42.0.39 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-hmnfd-etcd-lj72f8xjkv 1/1 Running 0 20h 10.47.0.103 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-reds-etcd-97wr66d4pj 1/1 Running 0 20h 10.47.0.129 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-reds-etcd-kmggscpzrf 1/1 Running 0 8d 10.40.0.64 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-reds-etcd-zcwrhm884l 1/1 Running 0 8d 10.42.0.53 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-uas-mgr-etcd-7gmh92t2hx 1/1 Running 0 20h 10.47.0.94 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-uas-mgr-etcd-7m4qmtgp6t 1/1 Running 0 8d 10.42.0.67 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-uas-mgr-etcd-pldlkpr48w 1/1 Running 0 8d 10.40.0.94 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; If the etcd clusters are not balanced, see Rebalance Healthy etcd Clusters.\nCheck the health of an etcd cluster database.\nTo check the health of an etcd cluster\u0026rsquo;s database in the services namespace:\nncn-mw# for pod in $(kubectl get pods -l app=etcd -n services \\ -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod} Etcd Database Check: ###\u0026#34; dbc=$(kubectl -n services exec ${pod} -- /bin/sh \\ -c \u0026#34;ETCDCTL_API=3 etcdctl put foo fooCheck \u0026amp;\u0026amp; \\ ETCDCTL_API=3 etcdctl get foo \u0026amp;\u0026amp; \\ ETCDCTL_API=3 etcdctl del foo \u0026amp;\u0026amp; \\ ETCDCTL_API=3 etcdctl get foo\u0026#34; 2\u0026gt;\u0026amp;1) echo $dbc | awk \u0026#39;{ if ( $1==\u0026#34;OK\u0026#34; \u0026amp;\u0026amp; $2==\u0026#34;foo\u0026#34; \u0026amp;\u0026amp; \\ $3==\u0026#34;fooCheck\u0026#34; \u0026amp;\u0026amp; $4==\u0026#34;1\u0026#34; \u0026amp;\u0026amp; $5==\u0026#34;\u0026#34; ) print \\ \u0026#34;PASS: \u0026#34; PRINT $0; else \\ print \u0026#34;FAILED DATABASE CHECK - EXPECTED: OK foo fooCheck 1 \\ GOT: \u0026#34; PRINT $0 }\u0026#39; done Example output:\n### cray-bos-etcd-7cxq6qrhz5 Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-bos-etcd-b9m4k5qfrd Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-bos-etcd-tnpv8x6cxv Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-bss-etcd-q4k54rbbfj Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-bss-etcd-r75mlv6ffd Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-bss-etcd-xprv5ht5d4 Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-cps-etcd-8hpztfkjdp Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-cps-etcd-fp4kfsf799 Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-cps-etcd-g6gz9vmmdn Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-crus-etcd-6z9zskl6cr Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-crus-etcd-krp255f97q Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-crus-etcd-tpclqfln67 Etcd Database Check: ### PASS: OK foo fooCheck 1 [...] To check one cluster:\nncn-mw# for pod in $(kubectl get pods -l etcd_cluster=cray-bos-etcd -n services \\ -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod} Etcd Database Check: ###\u0026#34; dbc=$(kubectl -n services exec ${pod} -- /bin/sh \\ -c \u0026#34;ETCDCTL_API=3 etcdctl put foo fooCheck \u0026amp;\u0026amp; \\ ETCDCTL_API=3 etcdctl get foo \u0026amp;\u0026amp; \\ ETCDCTL_API=3 etcdctl del foo \u0026amp;\u0026amp; \\ ETCDCTL_API=3 etcdctl get foo\u0026#34; 2\u0026gt;\u0026amp;1) echo $dbc | awk \u0026#39;{ if ( $1==\u0026#34;OK\u0026#34; \u0026amp;\u0026amp; $2==\u0026#34;foo\u0026#34; \u0026amp;\u0026amp; \\ $3==\u0026#34;fooCheck\u0026#34; \u0026amp;\u0026amp; $4==\u0026#34;1\u0026#34; \u0026amp;\u0026amp; $5==\u0026#34;\u0026#34; ) print \\ \u0026#34;PASS: \u0026#34; PRINT $0; else \\ print \u0026#34;FAILED DATABASE CHECK - EXPECTED: OK foo fooCheck 1 \\ GOT: \u0026#34; PRINT $0 }\u0026#39; done Example output:\n### cray-bos-etcd-7cxq6qrhz5 Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-bos-etcd-b9m4k5qfrd Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-bos-etcd-tnpv8x6cxv Etcd Database Check: ### PASS: OK foo fooCheck 1 If any of the etcd cluster databases are not healthy, then refer to the following procedures:\nCheck for and Clear etcd Cluster Alarms Clear Space in an etcd Cluster Database "
},
{
	"uri": "/docs-csm/en-12/operations/image_management/image_management/",
	"title": "Image Management",
	"tags": [],
	"description": "",
	"content": "Image Management The Image Management Service (IMS) uses the open source Kiwi-NG tool to build image roots from compressed Kiwi image descriptions. These compressed Kiwi image descriptions are referred to as \u0026ldquo;recipes.\u0026rdquo; Kiwi-NG builds images based on a variety of different Linux distributions, specifically SUSE, RHEL, and their derivatives. Kiwi image descriptions must follow the Kiwi development schema. More information about the development schema and the Kiwi-NG tool can be found in the documentation: https://doc.opensuse.org/projects/kiwi/doc/.\nEven though Kiwi recipes can be developed from scratch or found on the Internet, it is recommended that recipes are based on existing HPE Cray image recipes. HPE Cray provides multiple types of recipes including, but not limited to the following:\nBarebones Image Recipes: The barebones recipes contain only the upstream Linux packages needed to successfully boot the image on an HPE Cray compute node using upstream packages. Bare-bones recipes are primarily meant to be used to validate the IMS tools, without requiring HPE Cray Operating System (COS) content. COS Recipes: COS recipes contain a Linux environment with an HPE Cray customized kernel and optimized HPE Cray services for our most demanding customers and workloads. HPE Cray provided recipes are uploaded to the Simple Storage Service (S3) and registered with IMS as part of the install.\nImages built by IMS contain only the packages and settings that are referenced in the Kiwi-NG recipe used to build the image. The only exception is that IMS will dynamically install the system\u0026rsquo;s root CA certificate to allow zypper (via Kiwi-NG) to talk securely with the required Nexus RPM repositories. Images that are intended to be used to boot a CN or other node must be configured with DNS and other settings that enable the image to talk to vital services. A base level of customization is provided by the default Ansible plays used by the Configuration Framework Service (CFS) to enable DNS resolution, which are typically run against an image after it is built by IMS.\nWhen customizing an image via IMS image customization, once chrooted into the image root (or if using a `jailed` environment), the image will only have access to whatever configuration the image already contains. In order to talk to services, including Nexus RPM repositories, the image root must first be configured with DNS and other settings. A base level of customization is provided by the default Ansible plays used by the CFS to enable DNS resolution.\nThe Nexus Repository Manager service provides local RPM repositories for use when building or customizing an image. The Kiwi image descriptions should reference these repositories as needed. In order to include the custom-repo repository in an IMS Kiwi-NG recipe, the repository source path should be modified to the URI for a repository instance hosted by Nexus.\n\u0026lt;repository type=\u0026#34;rpm-md\u0026#34; alias=\u0026#34;custom-repo\u0026#34; imageinclude=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;source path=\u0026#34;https://packages.local/repository/REPO_NAME\u0026#34; /\u0026gt; \u0026lt;/repository\u0026gt; "
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/hsm_roles_and_subroles/",
	"title": "HSM Roles and Subroles",
	"tags": [],
	"description": "",
	"content": "HSM Roles and Subroles The Hardware State Manager (HSM) contains several pre-defined roles and subroles that can be assigned to components and used to target specific hardware devices.\nRoles and subroles assignments come from the System Layout Service (SLS) and are applied by HSM when a node is discovered.\nHSM roles HSM subroles Add custom roles and subroles HSM roles The following is a list of all pre-defined roles:\nManagement Compute Application Service System Storage The Management role refers to NCNs and will generally have the Master, Worker, or Storage subrole assigned.\nThe Compute role generally refers to compute nodes.\nThe Application role is used for more specific node uses and will generally have the UAN, LNETRouter, Visualization, Gateway, or UserDefined subrole assigned.\nHSM subroles The following is a list of all pre-defined subroles:\nWorker Master Storage UAN Gateway LNETRouter Visualization UserDefined The Master, Worker, and Storage subroles are generally used with the Management role to indicate NCN types.\nThe UAN, LNETRouter, Visualization, Gateway, and UserDefined subroles are generally used with the Application role to indicate specific use nodes.\nAdd custom roles and subroles Custom roles and subroles can also be created and added to the HSM. New roles or subroles can be added anytime after SMD has been deployed.\nTo add new roles, add them to the cray-hms-base-config ConfigMap under data.hms_config.json.HMSExtendedDefinitions.Role. To add new subroles, add them to the cray-hms-base-config ConfigMap under data.hms_config.json.HMSExtendedDefinitions.SubRole.\nEdit the ConfigMap with the following command:\nncn-mw# kubectl edit configmap -n services cray-hms-base-config data: hms_config.json: |- { \u0026#34;HMSExtendedDefinitions\u0026#34;:{ \u0026#34;Role\u0026#34;:[ \u0026#34;Compute\u0026#34;, \u0026#34;Service\u0026#34;, \u0026#34;System\u0026#34;, \u0026#34;Application\u0026#34;, \u0026#34;Storage\u0026#34;, \u0026#34;Management\u0026#34; ], \u0026#34;SubRole\u0026#34;:[ \u0026#34;Worker\u0026#34;, \u0026#34;Master\u0026#34;, \u0026#34;Storage\u0026#34;, \u0026#34;UAN\u0026#34;, \u0026#34;Gateway\u0026#34;, \u0026#34;LNETRouter\u0026#34;, \u0026#34;Visualization\u0026#34;, \u0026#34;UserDefined\u0026#34; ] } } Deleting roles/subroles from this list will also remove them from HSM. However, deleting any of the pre-defined roles or subroles will have no effect.\n"
},
{
	"uri": "/docs-csm/en-12/operations/firmware/updating_firmware_without_fas/",
	"title": "Updating BMC Firmware and BIOS for NCNs without FAS",
	"tags": [],
	"description": "",
	"content": "Updating BMC Firmware and BIOS for NCNs without FAS NOTE\nOn HPE nodes, the BMC firmware is iLO 5 and BIOS is System ROM. The commands in the procedure must be run on ncn-m001. This procedure should only be used if FAS is not available, such as during initial CSM install. In order to update the firmware or BIOS for ncn-m001 itself, see Updating BMC Firmware and BIOS for ncn-m001. Prerequisites Obtain the required firmware Flash the firmware Gigabyte NCNs HPE NCNs Using the ilorest command Using the iLO GUI Prerequisites The following information is needed:\nIP address of each NCN BMC IP address of ncn-m001 root user password for each NCN BMC Obtain the required firmware The firmware or BIOS can be obtained from the HFP tarball if it has been installed, or from the HPE Support Center (HPESC).\nThe correct version of firmware / BIOS must be selected.\nMove the firmware to be updated into an accessible directory.\nFlash the firmware Gigabyte NCNs This procedure can be followed on any Linux system with network connectivity to the NCN BMCs.\nStart a webserver from the directory containing the downloaded firmware / BIOS image:\npython3 -m http.server 8770 Update BMC firmware.\npasswd = root user password of BMC ipaddressOfBMC = IP address of NCN BMC ipaddressOfM001 = IP address of ncn-m001 filename = Filename of the downloaded image curl -k -u root:passwd https://ipaddressOfBMC/redfish/v1/UpdateService/Actions/SimpleUpdate -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;ImageURI\u0026#34;:\u0026#34;http://ipaddressOfM001:8770/filename\u0026#34;, \u0026#34;TransferProtocol\u0026#34;:\u0026#34;HTTP\u0026#34;, \u0026#34;UpdateComponent\u0026#34;:\u0026#34;BMC\u0026#34;}\u0026#39; Update BIOS.\npasswd = root user password of BMC ipaddressOfBMC = IP address of BMC ipaddressOfM001 = IP address of ncn-m001 filename = Filename of the downloaded image curl -k -u root:passwd https://ipaddressOfBMC/redfish/v1/UpdateService/Actions/SimpleUpdate -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;ImageURI\u0026#34;:\u0026#34;http://ipaddressOfM001:8770/filename\u0026#34;, \u0026#34;TransferProtocol\u0026#34;:\u0026#34;HTTP\u0026#34;, \u0026#34;UpdateComponent\u0026#34;:\u0026#34;BIOS\u0026#34;}\u0026#39; After updating its BIOS, an NCN must be rebooted. Follow the Reboot NCNs procedure to reboot NCNs.\nRepeat the previous two steps for all NCNs to be updated.\nStop the webserver started in the first step.\nHPE NCNs Using the ilorest command If the command ilorest is available, then follow the procedure in this section to update the NCNs. Otherwise, see Using the iLO GUI.\nUse the ilorest command to flash the firmware for each NCN that requires an update:\npasswd = root user password of BMC ipaddressOfBMC = IP address of BMC ipaddressOfM001 = IP address of ncn-m001 filename.fwpkg = Filename of the downloaded image ilorest flashfwpkg filename.fwpkg --url ipaddressOfBMC -u root -p passwd After updating its System ROM (BIOS), an NCN must be rebooted. Follow the Reboot NCNs procedure to reboot NCNs.\nUsing the iLO GUI The web interface will be used to update iLO 5 (BMC) firmware and/or System ROM (BIOS) on the HPE NCNs.\nCopy the iLO 5 firmware and/or System ROM files to a local computer from ncn-m001 using scp or other secure copy tools.\nscp root@ipaddressOfM001Node:pathToFile/filename . On a machine external to the cluster (for example, a laptop), do the following steps for each NCN to be updated:\nCreate an SSH tunnel.\n-L creates the tunnel -N prevents a shell and stubs the connection ssh -L 6443:ipaddressOfNCNBMC:443 -N ipaddressofM001 Open the following URL in a web browser: https://127.0.0.1:6443\nLog in with root and the root user password for the iLO device.\nClick on Firmware \u0026amp; OS Software on the left menu. Click on Update Firmware on the right menu. Check Local File. Click Choose File and select the iLO firmware file or System ROM file. Click Confirm TPM override. Click Flash. After updating its System ROM (BIOS), an NCN must be rebooted. Follow the Reboot NCNs procedure to reboot NCNs.\n"
},
{
	"uri": "/docs-csm/en-12/operations/conman/troubleshoot_conman_asking_for_password_on_ssh_connection/",
	"title": "Troubleshoot ConMan Asking for Password on SSH Connection",
	"tags": [],
	"description": "",
	"content": "Troubleshoot ConMan Asking for Password on SSH Connection If ConMan starts to ask for a password when there is an SSH connection to the node on liquid-cooled hardware, that usually indicates there is a problem with the SSH key that was established on the node BMC. The key may have been replaced or overwritten on the hardware.\nUse this procedure to renew or reinstall the SSH key on the BMCs.\nPrerequisites This procedure requires administrative privileges. If checking the existing SSH key on BMCs, the BMC username and password are also required.\nProcedure Note: this procedure has changed since the CSM 0.9 release.\nScale the cray-console-operator pods to 0 replicas.\nncn-mw# kubectl -n services scale --replicas=0 deployment/cray-console-operator Example output:\ndeployment.apps/cray-console-operator scaled Verify that the cray-console-operator service is no longer running.\nThe following command will give no output when the pod is no longer running.\nncn-mw# kubectl -n services get pods | grep console-operator Delete the SSH keys in a cray-console-node pod.\nncn-mw# kubectl -n services exec -it cray-console-node-0 -- rm -v /var/log/console/conman.key /var/log/console/conman.key.pub Restart the cray-console-operator pod.\nncn-mw# kubectl -n services scale --replicas=1 deployment/cray-console-operator Example output:\ndeployment.apps/cray-console-operator scaled It may take some time to regenerate the keys and get them deployed to the BMCs, but in a while the console connections using SSH should be reestablished. Note that it may be worthwhile to determine how the SSH key was modified and establish site procedures to coordinate SSH key use; otherwise they may be overwritten again at a later time.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/configuration_layers/",
	"title": "Configuration Layers",
	"tags": [],
	"description": "",
	"content": "Configuration Layers The Configuration Framework Service (CFS) uses configuration layers to specify the location of configuration content that will be applied. Configurations may include one or more layers. Each layer is defined by a Git repository clone URL, a Git commit, a name (optional), and the path in the repository to an Ansible playbook to execute.\nConfigurations with a single layer are useful when testing out a new configuration on targets, or when configuring system components with one product at a time. To fully configure a node or boot image component with all of the software products required, multiple layers can be used to apply all configurations in a single CFS session. When applying layers in a session, CFS runs through the configuration layers serially in the order specified.\nExample Configuration (Single Layer) The following is an example configuration with a single layer. This can be used as a template to create a new configuration JSON file to input to CFS.\nncn# cat configuration-single.json Example configuration:\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;43ecfa8236bed625b54325ebb70916f55884b3a4\u0026#34; } ] } Example Configuration (Multiple Layers) The following is an example configuration with multiple layers from one or more different configuration repositories. This can be used as a template to create a new configuration JSON file to input to CFS.\nncn# cat configuration-multiple.json Example configuration:\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;43ecfa8236bed625b54325ebb70916f55884b3a4\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-2\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site-custom.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;43ecfa8236bed625b54325ebb70916f55884b3a4\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-3\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/second-example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;8236bed625b4b3a443ecfa54325ebb70916f5588\u0026#34; } ] } Use Branches in Configuration Layers When defining a configuration layer, the branch or commit values can be used to reference a Git commit. The commit value is the recommended way to reference a Git commit. In the following example, when the configuration is created or updated, CFS will automatically check with VCS to get the commit at the head of the branch. Both the commit and the branch are then stored. The commit acts as normal, and the branch is stored to make future updates to the commit easier.\nncn-m001# cat configurations-example.json Example configuration:\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34; } ] } ncn-m001# cray cfs configurations update configurations-example \\ --file ./configurations-example.json \\ --format json Example output:\n{ \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:30:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; } If changes are made to a repository and branches are specified in the configuration, users can then use the --update-branches flag to update a configuration so that all commits reflect the latest commit on the branches specified.\nncn-m001# cray cfs configurations update configurations-example --update-branches Example output:\n{ \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:30:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;latest git commit id\u0026gt;\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; } Manage Configurations Use the cray cfs configurations --help command to manage CFS configurations on the system. The following operations are available:\nlist: List all configurations. describe: Display info about a single configuration and its layer(s). update: Create a new configuration or modify an existing configuration. delete: Delete an existing configuration. "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/clean_up_logs_after_a_boa_kubernetes_job/",
	"title": "Clean Up Logs After a BOA Kubernetes Job",
	"tags": [],
	"description": "",
	"content": "Clean Up Logs After a BOA Kubernetes Job NOTE This section is for Boot Orchestration Service (BOS) v1 only. BOS v2 does not use Boot Orchestration Agent (BOA) jobs and does not require cleanup.\nDelete log entries from previous boot orchestration jobs. BOS launches a BOA Kubernetes job. BOA then launches a Configuration Framework Service (CFS) session, resulting in a CFS-BOA Kubernetes job. Thus, there are two separate sets of jobs that can be removed.\nDeleting log entries creates more space and helps improve the usability of viewing logs.\nPrerequisites A Boot Orchestration Service (BOS) session has finished. Procedure List the current BOA jobs.\nncn-mw# kubectl get jobs -n services | grep boa Example output:\nboa-2c2211aa-9876-4aa7-92e2-c8a64d9bd9a6 1/1 6m58s 13d boa-51918dbd-bde2-4836-9500-2a7bad93787c 1/1 65s 9d boa-6fc198cc-486b-4340-81e0-f17c199a1ec6 1/1 97s 9d boa-8656f64d-baa9-43ea-9e11-2a0b27e89037 1/1 17m 13d boa-86b78489-1d76-4957-9c0e-a7b1d6665c35 1/1 15m 13d boa-a939bd32-9d27-433f-afc2-735e77ec8e58 1/1 13m 13d boa-e9adfa63-24dc-4da6-b870-b3535adf0bcc 1/1 7m53s 13d Delete any jobs that are no longer needed.\nDo not delete any jobs that are currently running.\nncn-mw# kubectl delete jobs BOA_JOB_ID "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/configure_a_default_uai_class_for_legacy_mode/",
	"title": "Configure a Default UAI Class for Legacy Mode",
	"tags": [],
	"description": "",
	"content": "Configure a Default UAI Class for Legacy Mode Using a default UAI class is optional but recommended for any site using the legacy UAI management mode that wants to have some control over UAIs created by users. UAI classes used for this purpose need to have certain minimum configuration in them:\nThe image_id field set to identify the image used to construct UAIs The volume_list field set to the list of volumes to mount in UAIs The public_ip field set to true The uai_compute_network flag set to true (if workload management will be used) The default flag set to true to make this the default UAI class To make UAIs useful, there is a minimum set of volumes that should be defined in the UAS configuration:\n/etc/localtime for default timezone information The directory on the host nodes that holds persistent end-user storage, typically /lus In addition to this, there may be volumes defined to support a workload manager (Slurm or PBS Professional) or the Cray Programming Environment (PE) or other packages the full extent of these volumes is outside the scope of this document, but whatever list of these other volumes is needed to get a suitable End-User UAI should be included in the default UAI class configuration.\nThe UAI Classes section has more information on what goes in End-User UAI classes and, specifically, the Non-Brokered End-User UAI classes used for Legacy mode.\nTop: User Access Service (UAS)\nNext Topic: Create and Use Default UAIs in Legacy Mode\n"
},
{
	"uri": "/docs-csm/en-12/operations/csm_product_management/redeploying_a_chart/",
	"title": "Redeploying a Chart",
	"tags": [],
	"description": "",
	"content": "Redeploying a Chart Administrators are able to customize many aspects of the system in order to address problems or tailor it to better suit their requirements. Often this requires redeploying one or more Helm charts. This page outlines the procedure for doing this in CSM. Other parts of the CSM documentation will reference this page if you are instructed to redeploy a chart. In those cases, the source page that links to this one should specify which charts should be redeployed and what customizations (if any) should be made to them.\nPrerequisites Procedure Preparation Obtain and optionally update customizations Redeploy charts Save updated customizations Cleanup Prerequisites CSM is fully installed and operational. The latest CSM documentation RPMs are installed on the node where this procedure is being performed. See Check for latest documentation. If this procedure was linked from another page, the administrator must have the following information from that other page: The name of the charts to be redeployed (for example, cray-hms-bss, cray-sysmgmt-health, or spire). The base name of the manifest for each of these charts (for example, sysmgmt, platform, or storage). The customization changes to make, if any. The steps to validate that the chart deployment was successful. Procedure 1. Preparation Create a temporary directory to use during this procedure.\nncn-mw# TEMPDIR=$(mktemp -d) ; echo \u0026#34;${TEMPDIR}\u0026#34; Change the current working directory to the new directory.\nncn-mw# cd \u0026#34;${TEMPDIR}\u0026#34; 2. Obtain and optionally update customizations Save the current set of chart customizations to a file.\nncn-mw# CUSTOMIZATIONS=\u0026#34;${TEMPDIR}/customizations.yaml\u0026#34; ncn-mw# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; \u0026#34;${CUSTOMIZATIONS}\u0026#34; Edit the ${CUSTOMIZATIONS} file, if appropriate.\nThe step involves updating the system customizations. In cases where a chart is being redeployed with no changes, or only changes to the chart version, then this step should be skipped.\nIf this procedure was linked from another page, that page should provide instructions on what edits to make, if any.\n3. Redeploy charts If redeploying more than one chart at once, perform the steps in this section for each chart being redeployed.\nSet helper variables.\nSet variable with the name of the chart.\nIf this procedure was linked from another page, that page should provide the chart name. Examples of chart names are cray-hms-bss, cray-sysmgmt-health, or spire.\nncn-mw# CHART_NAME=\u0026lt;put actual name here\u0026gt; ncn-mw# echo \u0026#34;${CHART_NAME}\u0026#34; Set variable with the base manifest name for this chart.\nIf this procedure was linked from another page, that page should provide the base manifest name. Examples of base manifest names are sysmgmt, platform, or storage.\nBASE_MANIFEST_NAME=\u0026lt;put actual name here\u0026gt; echo \u0026#34;${BASE_MANIFEST_NAME}\u0026#34; Set convenience variables for files that will be created during this procedure.\nncn-mw# BASE_MANIFEST_FILE=\u0026#34;${TEMPDIR}/${BASE_MANIFEST_NAME}.yaml\u0026#34; ; echo \u0026#34;${BASE_MANIFEST_FILE}\u0026#34; ncn-mw# BASE_CHART_FILE=\u0026#34;${TEMPDIR}/${CHART_NAME}.yaml\u0026#34; ; echo \u0026#34;${BASE_CHART_FILE}\u0026#34; ncn-mw# CUSTOMIZED_CHART_FILE=\u0026#34;${TEMPDIR}/${CHART_NAME}-customized.yaml\u0026#34; ; echo \u0026#34;${CUSTOMIZED_CHART_FILE}\u0026#34; Save the base manifest to a file.\nIf redeploying multiple charts, this step does not need to be repeated for additional charts that use the same base manifest name.\nncn-mw# kubectl get cm -n loftsman \u0026#34;loftsman-${BASE_MANIFEST_NAME}\u0026#34; -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; \u0026gt; \u0026#34;${BASE_MANIFEST_FILE}\u0026#34; Make a copy of this file to use for redeploying the chart.\nncn-mw# cp -v \u0026#34;${BASE_MANIFEST_FILE}\u0026#34; \u0026#34;${BASE_CHART_FILE}\u0026#34; Edit the name of the manifest in the new file.\nThis is to prevent it from overwriting the original manifest when it is redeployed.\nncn-mw# NEW_MANIFEST_NAME=\u0026#34;${CHART_NAME}-$(date +%Y%m%d%H%M%S)\u0026#34; ncn-mw# yq w -i \u0026#34;${BASE_CHART_FILE}\u0026#34; \u0026#39;metadata.name\u0026#39; \u0026#34;${NEW_MANIFEST_NAME}\u0026#34; Edit the spec.charts list in the ${BASE_CHART_FILE} file so that it only contains the stanza for the chart to be redeployed.\nFor example, if the chart being redeployed was cray-cfs-api, then the charts list would resemble the following example after editing. Note that the exact form and values may differ, because this is an example.\ncharts: - name: cray-cfs-api namespace: services source: csm-algol60 version: 1.12.1 Remove chart source fields from the ${BASE_CHART_FILE} file.\nRemove the spec.sources stanza and remove the source field from each chart listing.\nThese commands will work even if the stanzas and fields are not present in the file.\nncn-mw# yq d -i \u0026#34;${BASE_CHART_FILE}\u0026#34; spec.sources \u0026amp;\u0026amp; yq d -i \u0026#34;${BASE_CHART_FILE}\u0026#34; spec.charts[\\*].source Update the version numbers in ${BASE_CHART_FILE} if necessary.\nThe information in the chart stanza is from the time the chart was deploying during the most recent install or upgrade of the CSM software. However, it is possible that the chart has been redeployed more recently, using a newer chart version. This may happen when a hotfix is installed, or when the procedure on this page was previously followed.\nShow most recently deployed chart versions.\nA record is saved in Kubernetes of every Loftsman chart deploy that has happened. This step uses a helper script to find and display the most recent successful deployment of this chart, in order for the administrator to check the version numbers it used.\nncn-mw# /usr/share/doc/csm/scripts/operations/kubernetes/latest_chart_manifest.sh \u0026#34;${CHART_NAME}\u0026#34; Example output for the cray-cfs-api chart may resemble the following:\nDisplaying chart manifest for \u0026#39;cray-cfs-api\u0026#39; from loftsman-sysmgmt name: cray-cfs-api namespace: services source: csm-algol60 swagger: - name: cfs url: https://raw.githubusercontent.com/Cray-HPE/config-framework-service/v1.12.2/api/openapi.yaml version: v1 version: 1.12.1 Edit the version numbers in ${BASE_CHART_FILE} if necessary.\nIn the example output from the previous step, the version numbers matches what we saw previously, so no updates are required. If the output shows different version numbers, then edit the version numbers in the chart stanza of ${BASE_CHART_FILE} to match them.\nApply customizations to the manifest.\nThis must be done whether or not any changes were made to the customizations in the previous section.\nncn-mw# manifestgen -c \u0026#34;${CUSTOMIZATIONS}\u0026#34; -i \u0026#34;${BASE_CHART_FILE}\u0026#34; -o \u0026#34;${CUSTOMIZED_CHART_FILE}\u0026#34; Review the customized manifest file to verify that it contains the expected version numbers and customizations.\nncn-mw# cat \u0026#34;${CUSTOMIZED_CHART_FILE}\u0026#34; Redeploy the chart.\nIn most cases, the Helm chart to be used is already in Nexus. Unless this procedure was linked from another page which specified an alternative location for the Helm chart, then run the following command:\nncn-mw# loftsman ship --charts-repo https://packages.local/repository/charts --manifest-path \u0026#34;${CUSTOMIZED_CHART_FILE}\u0026#34; If this procedure was linked from another page, and that page specified a directory location for the Helm chart, then run the following command, substituting the directory name provided by the linking page.\nncn-mw# loftsman ship --charts-path \u0026lt;helm_chart_path\u0026gt; --manifest-path \u0026#34;${CUSTOMIZED_CHART_FILE}\u0026#34; Validate that the redeploy was successful.\nHow to do this will vary based on what was redeployed. If this procedure was linked from another page, that page should provide details on how to do this validation.\nIf additional charts are being redeployed, then repeat the previous steps in this section for each such chart.\n4. Save updated customizations If no changes were made to customizations, then skip this section. If changes were made to customizations, then this step is critical.\nUpdate the copy of the customizations in Kubernetes. If this is not done, then the customization changes will not persist after the next CSM upgrade, the next hotfix that is applied, or the next time that this procedure is followed.\nncn-mw# kubectl delete secret -n loftsman site-init ncn-mw# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml 5. Cleanup The temporary directory created at the beginning of the procedure may be deleted if desired.\n"
},
{
	"uri": "/docs-csm/en-12/install/collecting_bmc_mac_addresses/",
	"title": "Collecting the BMC MAC Addresses",
	"tags": [],
	"description": "",
	"content": "Collecting the BMC MAC Addresses This guide will detail how to collect BMC MAC addresses from an HPE Cray EX system with configured switches. The BMC MAC address is the exclusive, dedicated LAN for the onboard BMC.\nResults may vary if an unconfigured switch is being used.\nPrerequisites There is a configured switch with SSH access or unconfigured with COM access (Serial Over LAN/DB-9). A file is available to record the collected BMC information. Procedure Start a session on the leaf-bmc switch, either using SSH or a USB serial cable.\nSSH\nNOTE: These IP addresses are examples; 10.X.0.4 may not match the setup.\nover METAL MANAGEMENT\npit# ssh admin@10.1.0.4 over NODE MANAGEMENT\npit# ssh admin@10.252.0.4 SSH over HARDWARE MANAGEMENT\npit# ssh admin@10.254.0.4 Serial\nSee Connect to Switch over USB-Serial Cable, if wanting to use that option.\nDisplay the MAC addresses for the BMC ports (if known).\nIf they exist on the same VLAN, then dump the VLAN to get the MAC addresses. In order to find the ports of the BMCs, cross-reference the HMN tab of the SHCD file.\nReference the CLI for more information (press ? or tab).\nPrint using the VLAN ID:\nDellOS 10\nsw-leaf-bmc-001# show mac address-table vlan 4 The output should look similar to:\nVlanId Mac Address Type Interface 4 00:1e:67:98:fe:2c dynamic ethernet1/1/11 4 a4:bf:01:38:f0:b1 dynamic ethernet1/1/27 4 a4:bf:01:38:f1:44 dynamic ethernet1/1/25 4 a4:bf:01:48:1e:ac dynamic ethernet1/1/28 4 a4:bf:01:48:1f:70 dynamic ethernet1/1/31 4 a4:bf:01:48:1f:e0 dynamic ethernet1/1/26 4 a4:bf:01:48:20:03 dynamic ethernet1/1/30 4 a4:bf:01:48:20:57 dynamic ethernet1/1/29 4 a4:bf:01:4d:d9:9a dynamic ethernet1/1/32 Aruba AOS-CX\nsw-leaf-bmc-001# show mac-address-table vlan 4 The output should look similar to:\nMAC age-time : 300 seconds Number of MAC addresses : 21 MAC Address VLAN Type Port -------------------------------------------------------------- b4:2e:99:df:f3:61 4 dynamic 1/1/36 b4:2e:99:df:ec:f1 4 dynamic 1/1/35 b4:2e:99:df:ec:49 4 dynamic 1/1/33 94:40:c9:37:04:84 4 dynamic 1/1/26 94:40:c9:35:03:06 4 dynamic 1/1/27 94:40:c9:37:0a:2a 4 dynamic 1/1/29 94:40:c9:37:67:60 4 dynamic 1/1/43 94:40:c9:37:67:80 4 dynamic 1/1/37 94:40:c9:37:77:26 4 dynamic 1/1/31 94:40:c9:37:77:b8 4 dynamic 1/1/28 94:40:c9:37:87:5a 4 dynamic 1/1/30 94:40:c9:37:f9:b4 4 dynamic 1/1/25 b4:2e:99:df:eb:c1 4 dynamic 1/1/34 Print using the interface and trunk:\nDellOS 10\nsw-leaf-bmc-001# show mac address-table interface ethernet 1/1/32 The output should look similar to:\nVlanId Mac Address Type Interface 4 a4:bf:01:4d:d9:9a dynamic ethernet1/1/32 Aruba AOS-CX\nThe final argument of the command is the list of ports. For example: 1/1/1, 1/1/1-1/1/3, or lag1.\nsw-leaf-bmc-001# show mac-address-table port 1/1/36 The output should look similar to:\nMAC age-time : 300 seconds Number of MAC addresses : 1 MAC Address VLAN Type Port -------------------------------------------------------------- b4:2e:99:df:f3:61 4 dynamic 1/1/36 Print everything:\nDellOS 10\nsw-leaf-bmc-001# show mac address-table The output should look similar to:\nVlanId Mac Address Type Interface 4 a4:bf:01:4d:d9:9a dynamic ethernet1/1/32 .... Aruba AOS-CX\nsw-leaf-bmc-001# show mac-address-table The output should look similar to:\nMAC age-time : 300 seconds Number of MAC addresses : 52 MAC Address VLAN Type Port -------------------------------------------------------------- ec:eb:b8:3d:89:41 1 dynamic 1/1/42 Ensure that the management NCNs are present in the ncn_metadata.csv file.\nThe output from the previous show mac address-table command will display information for all management NCNs that do not have an external connection for their BMC, such as ncn-m001. The BMC MAC address for ncn-m001 will be collected in the next step, as this BMC is not connected to the system\u0026rsquo;s management network like the other management nodes.\nAll of the management NCNs should be present in the ncn_metadata.csv file.\nFill in the Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1 columns with a placeholder value, such as de:ad:be:ef:00:00, as a marker that the correct value is not in this file yet.\nIMPORTANT NCNs of each type (master, storage, and worker) are grouped together in the file and are listed in descending numerical order within their group (for example, ncn-s003 is listed directly before ncn-s002).\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Storage,a4:bf:01:38:f1:44,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s8b0n0,Management,Storage,a4:bf:01:48:1f:e0,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s7b0n0,Management,Storage,a4:bf:01:38:f0:b1,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 ^^^^^^^^^^^^^^^^^ BMC MAC address The column heading line must match that shown above in order for csi to parse it correctly.\nCollect the BMC MAC address information for the PIT node.\nThe PIT node BMC is not connected to the switch like the other management nodes.\nFor HPE and Gigabyte nodes:\npit# ipmitool lan print 1 | grep \u0026#34;MAC Address\u0026#34; For Intel nodes:\npit# ipmitool lan print 3 | grep \u0026#34;MAC Address\u0026#34; Example output:\nMAC Address : a4:bf:01:37:87:32 Add this information for ncn-m001 to the ncn_metadata.csv file.\nThere should be ncn-m003, then ncn-m002, and this new entry for ncn-m001 as the last line in the file.\nx3000c0s1b0n0,Management,Master,a4:bf:01:37:87:32,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 ^^^^^^^^^^^^^^^^^ BMC MAC address Verify that the ncn_metadata.csv file has a row for every management node in the SHCD.\nThere may be placeholder entries for some MAC addresses.\nBelow is a sample file showing storage nodes 3, 2, and 1, then worker nodes 3, 2, and 1, and finally master nodes 3, 2, and 1, with valid BMC MAC addresses, but placeholder value de:ad:be:ef:00:00 for the Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1.\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Storage,a4:bf:01:38:f1:44,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s8b0n0,Management,Storage,a4:bf:01:48:1f:e0,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s7b0n0,Management,Storage,a4:bf:01:38:f0:b1,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s6b0n0,Management,Worker,a4:bf:01:48:1e:ac,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s5b0n0,Management,Worker,a4:bf:01:48:20:57,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s4b0n0,Management,Worker,a4:bf:01:48:20:03,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s3b0n0,Management,Master,a4:bf:01:48:1f:70,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s2b0n0,Management,Master,a4:bf:01:4d:d9:9a,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s1b0n0,Management,Master,a4:bf:01:37:87:32,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 "
},
{
	"uri": "/docs-csm/en-12/background/ncn_operating_system_releases/",
	"title": "NCN Operating System Releases",
	"tags": [],
	"description": "",
	"content": "NCN Operating System Releases The management non-compute nodes (NCNs) define their products per image layer:\nKubernetes NCN images are always SLE_HPC (SuSE High Performance Computing) Ceph Storage NCN images are always SLE_HPC (SuSE High Performance Computing) with SES (SuSE Enterprise Storage) The sles-release RPM is uninstalled for NCNs, and instead, the sle_HPC-release RPM is installed. These both provide the same files, but differ for os-release and /etc/product.d/baseproduct.\nThe ses-release RPM is installed on top of the sle_HPC-release RPM in the Ceph images.\nThe following example shows the two product files for a utility storage node booted from the Ceph image. This node is capable of high performance computing and serving enterprise storage.\nncn-s# ls -l /etc/products.d/ Example output:\ntotal 5 lrwxrwxrwx 1 root root 12 Jan 1 06:43 baseproduct -\u0026gt; SLE_HPC.prod -rw-r--r-- 1 root root 1587 Oct 21 15:27 ses.prod -rw-r--r-- 1 root root 2956 Jun 10 2020 SLE_HPC.prod ncn-s# grep \u0026#39;\u0026lt;summary\u0026#39; /etc/products.d/*.prod Example output:\n/etc/products.d/ses.prod: \u0026lt;summary\u0026gt;SUSE Enterprise Storage 7\u0026lt;/summary\u0026gt; /etc/products.d/SLE_HPC.prod: \u0026lt;summary\u0026gt;SUSE Linux Enterprise High Performance Computing 15 SP3\u0026lt;/summary\u0026gt; Kubernetes nodes will report SLE HPC only, which is reflected in the kubectl output.\nncn-mw# kubectl get nodes -o wide Example output:\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ncn-m001 Ready control-plane,master 27h v1.20.13 10.252.1.4 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-m002 Ready control-plane,master 8d v1.20.13 10.252.1.5 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-m003 Ready control-plane,master 8d v1.20.13 10.252.1.6 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-w001 Ready \u0026lt;none\u0026gt; 8d v1.20.13 10.252.1.7 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-w002 Ready \u0026lt;none\u0026gt; 8d v1.20.13 10.252.1.8 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-w003 Ready \u0026lt;none\u0026gt; 8d v1.20.13 10.252.1.9 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/discovery_job_not_creating_redfish_endpoints/",
	"title": "HMS Discovery Job Not Creating RedfishEndpoints In Hardware State Manager",
	"tags": [],
	"description": "",
	"content": "HMS Discovery Job Not Creating RedfishEndpoints In Hardware State Manager It is a known issue with the HMS Discovery cronjob that when a BMC does not respond by its IP address, the discovery job will not create a RedfishEndpoint for the BMC in Hardware State Manager (HSM). However, it does update the BMC MAC address in HSM with its component name (xname). The discovery job only creates a new RedfishEndpoints when it encounters an unknown MAC address without a component name (xname) associated with it.\nThis troubleshooting procedure is only applicable for air-cooled NodeBMCs and RouterBMCs.\nPrerequisites Only applicable to an air-cooled NodeBMC or RouterBMC. Symptoms The MAC address for the BMC in HSM has an IP address and component ID. The BMC is pingable. There is no RedfishEndpoint for the BMC in HSM. Check For Symptoms Setup an environment variable with to store the xname of the BMC.\nThis should be either the component name (xname) for a NodeBMC (xXcCsSbB) or RouterBMC (xXcCrRbB).\nncn# export BMC=x3000c0s18b0 Check to see in HSM if the component ID for a BMC has a MAC address and IP associated with it.\nncn# cray hsm inventory ethernetInterfaces list --component-id $BMC Example output:\n[[results]] ID = \u0026#34;54802852b706\u0026#34; Description = \u0026#34;\u0026#34; MACAddress = \u0026#34;54:80:28:52:b7:06\u0026#34; LastUpdate = \u0026#34;2021-06-15T14:30:21.195015Z\u0026#34; ComponentID = \u0026#34;x3000c0s18b0\u0026#34; Type = \u0026#34;NodeBMC\u0026#34; [[results.IPAddresses]] IPAddress = \u0026#34;10.254.1.27\u0026#34; [[results]] ID = \u0026#34;54802852b707\u0026#34; Description = \u0026#34;Configuration of this Manager Network Interface\u0026#34; MACAddress = \u0026#34;54:80:28:52:b7:07\u0026#34; LastUpdate = \u0026#34;2021-06-15T14:37:52.078528Z\u0026#34; ComponentID = \u0026#34;x3000c0s18b0\u0026#34; Type = \u0026#34;NodeBMC\u0026#34; IPAddresses = [] Set an environment variable to store the MAC address of the BMC that has an IP address:\nMake sure to use the normalized MAC address from the ID field.\nncn# export BMC_MAC=54802852b706 Verify that the IP address associated with the MAC address is pingable.\nncn# ping $BMC If it is pingable, then output will look similar to the following:\nPING x3000c0s18b0 (10.254.1.27) 56(84) bytes of data. 64 bytes from x3000c0s18b0 (10.254.1.27): icmp_seq=1 ttl=255 time=0.342 ms 64 bytes from x3000c0s18b0 (10.254.1.27): icmp_seq=2 ttl=255 time=0.152 ms 64 bytes from x3000c0s18b0 (10.254.1.27): icmp_seq=3 ttl=255 time=0.205 ms 64 bytes from x3000c0s18b0 (10.254.1.27): icmp_seq=4 ttl=255 time=0.291 ms ^C --- x3000c0s18b0 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3067ms rtt min/avg/max/mdev = 0.152/0.247/0.342/0.075 ms Verify that no Redfish endpoint for the NodeBMC or RouterBMC is present in HSM.\nncn# cray hsm inventory redfishEndpoints describe $BMC If the endpoint is missing from HMC, then output will look similar to the following:\nUsage: cray hsm inventory redfishEndpoints describe [OPTIONS] XNAME Try \u0026#39;cray hsm inventory redfishEndpoints describe --help\u0026#39; for help. Error: Missing argument \u0026#39;XNAME\u0026#39;. If the BMC has a MAC Address with a component ID and does not have a RedfishEndpoint in HSM, then proceed to the next section.\nSolution Correcting this River Redfish endpoint discovery issue can be done by running the river_rf_endpoint_discovery_fixup.py script:\nncn# /opt/cray/csm/scripts/hms_verification/river_rf_endpoint_discovery_fixup.py The return value of the script is 0 if the correction was successful or if no correction was needed. A non-zero return value means that manual intervention may be needed to correct the issue. Continue to the next section if there were failures.\nScript Debugging Steps Check that the hms-discovery cronjob has run to completion since running the script.\nncn# kubectl -n services get pods -l app=hms-discovery Example output:\nNAME READY STATUS RESTARTS AGE hms-discovery-1624901400-wsfxv 0/2 Completed 0 28m hms-discovery-1624901580-xpsj7 0/2 Completed 0 25m hms-discovery-1624901760-tbw6t 0/2 Completed 0 22m hms-discovery-1624901940-rxwjk 0/2 Completed 0 19m hms-discovery-1624902120-4njrx 0/2 Completed 0 16m hms-discovery-1624902300-jcgd8 0/2 Completed 0 13m hms-discovery-1624902480-468sx 0/2 Completed 0 10m hms-discovery-1624902660-gdkmh 0/2 Completed 0 7m52s hms-discovery-1624902840-nlzw2 0/2 Completed 0 4m50s hms-discovery-1624903020-qk6ww 0/2 Completed 0 109s If not, wait until it has and then continue to the next step.\nVerify that the MAC address has a component ID associated with it.\nncn# cray hsm inventory ethernetInterfaces describe $BMC_MAC Example output:\nID = \u0026#34;54802852b706\u0026#34; Description = \u0026#34;\u0026#34; MACAddress = \u0026#34;54:80:28:52:b7:06\u0026#34; LastUpdate = \u0026#34;2021-06-28T18:18:15.960235Z\u0026#34; ComponentID = \u0026#34;x3000c0s18b0\u0026#34; Type = \u0026#34;NodeBMC\u0026#34; [[IPAddresses]] IPAddress = \u0026#34;10.254.1.27\u0026#34; If ComponentID remains empty, then check the hms-discovery logs for errors. Otherwise, move on to the next step.\nVerify that a RedfishEndpoint now exists for the BMC.\nThe BMC when first added to HSM may not be DiscoverOK right away. It may take up 5 minutes for BMC hostname to start resolving in DNS. The HMS Discovery cronjob should automatically trigger a discovery for any RedfishEndpoints that are not in the DiscoveryOk or DiscoveryStarted states, such as HTTPsGetFailed.\nncn# cray hsm inventory redfishEndpoints describe $BMC Example output:\nID = \u0026#34;x3000c0s18b0\u0026#34; Type = \u0026#34;NodeBMC\u0026#34; Hostname = \u0026#34;x3000c0s18b0\u0026#34; Domain = \u0026#34;\u0026#34; FQDN = \u0026#34;x3000c0s18b0\u0026#34; Enabled = true UUID = \u0026#34;9a856688-e286-54ff-989f-1f8475430231\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34;\u0026#34; MACAddr = \u0026#34;54802852b706\u0026#34; RediscoverOnUpdate = true [DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2021-06-28T18:26:05.902976Z\u0026#34; LastDiscoveryStatus = \u0026#34;DiscoverOK\u0026#34; RedfishVersion = \u0026#34;1.6.0\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/ceph_service_check_script_usage/",
	"title": "Ceph Service Check Script Usage",
	"tags": [],
	"description": "",
	"content": "Ceph Service Check Script Usage A new Ceph service script that will check the status of Ceph and then verify that status against the individual Ceph storage nodes.\nLocation /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh\nUsage usage: ceph-service-status.sh # runs a simple Ceph health check ceph-service-status.sh -n \u0026lt;node\u0026gt; -s \u0026lt;service\u0026gt; # checks a single service on a single node ceph-service-status.sh -n \u0026lt;node\u0026gt; -a true # checks all Ceph services on a node ceph-service-status.sh -A true # checks all Ceph services on all nodes in a rolling fashion ceph-service-status.sh -s \u0026lt;service name\u0026gt; # will find the where the service is running and report its status Important: By default, the output of this command will not be verbose. This is to accommodate goss testing. For manual runs, please use the -v true flag.\nTroubleshooting If the message parse error: Invalid numeric literal at line 1, column 5 is displayed, it is indicating that the cached SSH keys in known_hosts are no longer valid. The simple fix is \u0026gt; ~/.ssh/known_hosts and re-run the script. It will update the keys.\nExamples Simple Ceph Health Check ncn# /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -v true Example output:\nFSID: c84ecf41-c535-4588-96c3-f6892bbd81ce FSID_STR: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce Ceph is reporting a status of HEALTH_OK Updating SSH keys.. Tests run: 1 Tests Passed: 1 Service Check for a Single Service on a Single Node ncn# /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -n ncn-s001 -v true -s mon.ncn-s001 Example output:\nFSID: c84ecf41-c535-4588-96c3-f6892bbd81ce FSID_STR: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce Ceph is reporting a status of HEALTH_OK Updating SSH keys.. HOST: ncn-s001####################### Service mon.ncn-s001 on ncn-s001 has been restarted and up for 9280 seconds mon.ncn-s001\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-mon.ncn-s001 Status: running Tests run: 2 Tests Passed: 2 Service Check for All Services on a Single Node ncn# /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -n ncn-s001 -a true -v true Example output:\nFSID: c84ecf41-c535-4588-96c3-f6892bbd81ce FSID_STR: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce Ceph is reporting a status of HEALTH_OK Updating SSH keys.. HOST: ncn-s001####################### Service mds.cephfs.ncn-s001.rmisfx on ncn-s001 has been restarted and up for 9206 seconds mds.cephfs.ncn-s001.rmisfx\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-mds.cephfs.ncn-s001.rmisfx Status: running Service mgr.ncn-s001 on ncn-s001 has been restarted and up for 9201 seconds mgr.ncn-s001\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-mgr.ncn-s001 Status: running Service mon.ncn-s001 on ncn-s001 has been restarted and up for 9228 seconds mon.ncn-s001\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-mon.ncn-s001 Status: running Service node-exporter.ncn-s001 on ncn-s001 has been restarted and up for 1231 seconds node-exporter.ncn-s001\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-node-exporter.ncn-s001 Status: running Service on ncn-s001 is reporting up for 9209 seconds osd.0\u0026#39;s status is reporting up: 1 in: 1 Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-osd.0 Status: running Service on ncn-s001 is reporting up for 9200 seconds osd.11\u0026#39;s status is reporting up: 1 in: 1 Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-osd.11 Status: running Service on ncn-s001 is reporting up for 9208 seconds osd.14\u0026#39;s status is reporting up: 1 in: 1 Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-osd.14 Status: running Service on ncn-s001 is reporting up for 9206 seconds osd.17\u0026#39;s status is reporting up: 1 in: 1 Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-osd.17 Status: running Service on ncn-s001 is reporting up for 9213 seconds osd.5\u0026#39;s status is reporting up: 1 in: 1 Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-osd.5 Status: running Service on ncn-s001 is reporting up for 9207 seconds osd.8\u0026#39;s status is reporting up: 1 in: 1 Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-osd.8 Status: running Service rgw.site1.zone1.ncn-s001.kvxhwi on ncn-s001 has been restarted and up for 9210 seconds rgw.site1.zone1.ncn-s001.kvxhwi\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-rgw.site1.zone1.ncn-s001.kvxhwi Status: running Tests run: 12 Tests Passed: 12 Service Check for a Service Type ncn# /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -v true -s mon Example output:\nFSID: c84ecf41-c535-4588-96c3-f6892bbd81ce FSID_STR: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce Ceph is reporting a status of HEALTH_OK Updating SSH keys.. HOST: ncn-s001####################### Service mon on ncn-s001 has been restarted and up for 9547 seconds mon\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-mon.ncn-s001 Status: running HOST: ncn-s002####################### Service mon on ncn-s002 has been restarted and up for 5643 seconds mon\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-mon.ncn-s002 Status: running HOST: ncn-s003####################### Service mon on ncn-s003 has been restarted and up for 2588 seconds mon\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-mon.ncn-s003 Status: running Tests run: 4 Tests Passed: 4 Service Check for All Services and All Nodes The output of the following command is similar to the above output, except it shows all services on all nodes. It is excluded in this case for brevity.\nncn# /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -v true -A true IMPORTANT: This script can be run without the verbose flag and with an echo for the return code echo $?. A return code of 0 means the check was clean. A return code of 1 or greater means that there was an issue. In the latter case, re-run the command with the -v true flag.\n"
},
{
	"uri": "/docs-csm/en-12/operations/system_management_health/troubleshoot_prometheus_alerts/",
	"title": "Troubleshoot Prometheus Alerts",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Prometheus Alerts CephMgrIsAbsent and CephMgrIsMissingReplicas CephNetworkPacketsDropped CPUThrottlingHigh KubePodNotReady PostgresqlFollowerReplicationLagSMA PostgresqlHighRollbackRate PostgresqlInactiveReplicationSlot PostgresqlNotEnoughConnections TargetDown CephMgrIsAbsent and CephMgrIsMissingReplicas If the CephMgrIsAbsent and/or CephMgrIsMissingReplicas alerts fire, use the following steps to ensure the prometheus module has been enabled for Ceph. The following steps should be executed on ncn-s001:\nncn-s001# ceph mgr module ls | jq \u0026#39;.enabled_modules\u0026#39; Example output:\n[ \u0026#34;cephadm\u0026#34;, \u0026#34;iostat\u0026#34;, \u0026#34;restful\u0026#34; ] If prometheus is missing from the output, enable with the following command:\nncn-s001# ceph mgr module enable prometheus Confirm the module is now enabled:\nncn-s001# ceph mgr module ls | jq \u0026#39;.enabled_modules\u0026#39; Example output:\n[ \u0026#34;cephadm\u0026#34;, \u0026#34;iostat\u0026#34;, \u0026#34;prometheus\u0026#34;, \u0026#34;restful\u0026#34; ] The CephMgrIsAbsent and CephMgrIsMissingReplicas alerts should now clear in Prometheus.\nCephNetworkPacketsDropped The CephNetworkPacketsDropped alert does not necessarily indicate there are packets being dropped on an interface on a storage node. In a future release this alert will be renamed to be more generic. If this alert fires, inspect the IP address in the details of the alert to determine the node in question (it can be storage, master, or worker node). If the interface in question is determined to be healthy, then this alert can be ignored.\nCPUThrottlingHigh Alerts for CPUThrottlingHigh on gatekeeper-audit can be ignored. This pod is not utilized in this release.\nAlerts for CPUThrottlingHigh on gatekeeper-controller-manager can be ignored. These have low CPU requests, and it is normal for resource usage to spike when it is in use.\nAlerts for CPUThrottlingHigh on smartmon pods can be ignored. It is normal for smartmon pods\u0026rsquo; resource usage to spike when it is polling. This will be fixed in a future release.\nAlerts for CPUThrottlingHigh on CFS services such as cfs-batcher and cfs-trust can be ignored. Because CFS is idle most of the time, these services have low CPU requests, and it is normal for CFS service resource usage to spike when it is in use.\nKubePodNotReady Alerts for KubePodNotReady on cray-crus may be ignored if the Slurm software has not been installed. The cray-crus pod interacts with Slurm to manage compute node rolling upgrades.\nPostgresqlFollowerReplicationLagSMA Alerts for PostgresqlFollowerReplicationLagSMA on sma-postgres-cluster pods with slot_name=\u0026quot;permanent_physical_1\u0026quot; can be ignored. This slot_name is disabled and will be removed in a future release.\nPostgresqlHighRollbackRate Alerts for PostgresqlHighRollbackRate on spire-postgres and smd-postgres pods can be ignored. This is caused by an idle session that requires a timeout. This will be fixed in a future release.\nPostgresqlInactiveReplicationSlot Alerts for PostgresqlInactiveReplicationSlot on sma-postgres-cluster pods with slot_name=\u0026quot;permanent_physical_1\u0026quot; can be ignored. This slot_name is disabled and will be removed in a future release.\nPostgresqlNotEnoughConnections Alerts for PostgresqlNotEnoughConnections for datname=\u0026quot;foo\u0026quot; and datname=\u0026quot;bar\u0026quot; can be ignored. These databases are not used and will be removed in a future release.\nTargetDown Many of the alerts for TargetDown for sysmgmt-health/cray-sysmgmt-health-kubernetes-pods/0 are due to job pods that have Completed and no longer have an active endpoint that can be scraped. If the target that is down is from a job pod that has completed, the TargetDown alert for that pod can be ignored. This is being fixed in a future release.\n"
},
{
	"uri": "/docs-csm/en-12/operations/system_layout_service/system_layout_service_sls/",
	"title": "System Layout Service (SLS)",
	"tags": [],
	"description": "",
	"content": "System Layout Service (SLS) The System Layout Service (SLS) holds information about the system design, such as the physical locations of network hardware, compute nodes, and cabinets. It also stores information about the network, such as which port on which switch should be connected to each compute node.\nSLS stores a generalized abstraction of the system that other services can access. The Hardware State Manager (HSM) keeps track of information for hardware state or identifiers. SLS does not need to change as hardware within the system is replaced.\nInteraction with SLS is required if the system setup changes. For example, if system cabling is altered, or if the system is expanded or reduced. SLS does not interact with the hardware. Interaction with SLS should occur only during system installation, expansion, and contraction.\nSLS is responsible for the following:\nProviding an HTTP API to access site information Storing a list of all hardware Storing a list of all network links Storing a list of all power links "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/change_air-cooled_node_bmc_credentials/",
	"title": "Change Air-Cooled Node BMC Credentials",
	"tags": [],
	"description": "",
	"content": "Change Air-Cooled Node BMC Credentials This procedure will use the System Configuration Service (SCSD) to change all air-cooled Node BMCs in the system to the same global credential.\nLimitations All air-cooled and liquid-cooled BMCs share the same global credentials. The air-cooled Slingshot switch controllers (Router BMCs) must have the same credentials as the liquid-cooled Slingshot switch controllers.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Review procedures for SCSD in the Cray System Management (CSM) Administration Guide. Procedure Set the NEW_BMC_CREDENTIAL to specify the new root user password for air-cooled node BMCs.\nncn-m001# read -s NEW_BMC_CREDENTIAL ncn-m001# echo $NEW_BMC_CREDENTIAL Expected output:\nnew.root.password Create an SCSD payload file to change all air-cooled node BMCs to the same global credential.\nncn-m001# cat \u0026gt; bmc_creds_glb.json \u0026lt;\u0026lt;DATA { \u0026#34;Force\u0026#34;:false, \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;$NEW_BMC_CREDENTIAL\u0026#34;, \u0026#34;Targets\u0026#34;: $(cray hsm state components list --class River --type NodeBMC --format json | jq -r \u0026#39;[.Components[] | .ID]\u0026#39;) } DATA Inspect the generated SCSD payload file.\nncn-m001# cat bmc_creds_glb.json | jq Apply the new BMC credentials.\nncn-m001# cray scsd bmc globalcreds create ./bmc_creds_glb.json Troubleshooting: If the above command has any components that do not have the status of OK, they must be retried until they work, or the retries are exhausted and noted as failures. Failed modules need to be taken out of the system until they are fixed.\n"
},
{
	"uri": "/docs-csm/en-12/operations/power_management/power_on_the_external_lustre_file_system/",
	"title": "Power On the External Lustre File System",
	"tags": [],
	"description": "",
	"content": "Power On the External Lustre File System Use this procedure as a general guide to power on an external ClusterStor system. Refer to the detailed procedures that support each ClusterStor hardware and software release:\nClusterStor E1000 Administration Guide 4.2 - S-2758 for ClusterStor E1000 systems ClusterStor Administration Guide 3.4 - S-2756 for ClusterStor L300, L300N systems ClusterStor Administration Guide - S-2755 for Legacy ClusterStor systems Power up storage nodes in the following sequence:\nStorage Management Unit (SMU) nodes Metadata Server MGS/MDS nodes Object Storage Server (OSS) nodes Prerequisites Facility power must be connected to the PDUs and the PDU circuit breakers must be set to ON. This procedure assumes that power switches on all storage equipment are set to OFF. Procedure Set the System Management Unit (SMU) chassis power switches to ON.\nSet the Metadata Unit (MDU) chassis power switches to ON.\nSet the Metadata Management Unit (MMU) or Advanced Metadata Management Unit (AMMU) chassis power switches to ON.\nSet the object storage server (OSS), scalable storage unit (SSU), extension storage unit (ESU), and Scalable Flash Unit (SFU) chassis power switches to ON.\nssh to the primary management node.\nFor example, on system cls01234.\nremote$ ssh -l admin cls01234n000.systemname.com Check that the shared storage targets are available for the management nodes.\n[n000]$ pdsh -g mgmt cat /proc/mdstat | dshbak -c Example output:\n---------------- cls01234n000 ---------------- Personalities : [raid1] [raid6] [raid5] [raid4] [raid10] md64 : active raid10 sda[0] sdc[3] sdw[2] sdl[1] 1152343680 blocks super 1.2 64K chunks 2 near-copies [4/4] [UUUU] bitmap: 2/9 pages [8KB], 65536KB chunk md127 : active raid1 sdy[0] sdz[1] 439548848 blocks super 1.0 [2/2] [UU] unused devices: \u0026lt;none\u0026gt; ---------------- cls01234n001 ---------------- Personalities : [raid1] [raid6] [raid5] [raid4] [raid10] md67 : active raid1 sdi[0] sdt[1] 576171875 blocks super 1.2 [2/2] [UU] bitmap: 0/5 pages [0KB], 65536KB chunk md127 : active raid1 sdy[0] sdz[1] 439548848 blocks super 1.0 [2/2] [UU] unused devices: \u0026lt;none\u0026gt; Check HA status once the node is up and HA configuration has been established.\n[n000]$ sudo crm_mon -1r The output indicates that all resources have started and are balanced between two nodes.\nIn cases when all resources started on a single node (for example, all resources have started on node 00 and did not fail back to node 01, run the failback operation:\n[n000]$ cscli failback –n primary_MGMT_node As root on the primary management node, power on the MGS and MDS nodes.\nFor example:\n[n000]# cscli power_manage -n cls01234n[02-03] --power-on Power on the OSS nodes and, if present, the ADU nodes.\n[n000]# cscli power_manage -n oss_adu_nodes --power-on Check the status of the nodes.\n[n000]# pdsh -a date Example output:\ncls01234n000: Thu Aug 7 01:29:28 PDT 2014 cls01234n003: Thu Aug 7 01:29:28 PDT 2014 cls01234n002: Thu Aug 7 01:29:28 PDT 2014 cls01234n001: Thu Aug 7 01:29:28 PDT 2014 cls01234n007: Thu Aug 7 01:29:28 PDT 2014 cls01234n006: Thu Aug 7 01:29:28 PDT 2014 cls01234n004: Thu Aug 7 01:29:28 PDT 2014 cls01234n005: Thu Aug 7 01:29:28 PDT 2014 Check the health of the system.\n[n000]# cscli csinfo [n000]# cscli show_nodes [n000]# cscli fs_info Check resources before mounting the file system.\n[n000]# ssh cls01234n000 crm_mon -r1 | grep fsys [n000]# ssh cls01234n002 crm_mon -r1 | grep fsys [n000]# ssh cls01234n004 crm_mon -r1 | grep fsys [n000]# ssh cls01234n006 crm_mon -r1 | grep fsys [n000]# ssh cls01234n008 crm_mon -r1 | grep fsys [n000]# ssh cls01234n010 crm_mon -r1 | grep fsys [n000]# ssh cls01234n012 crm_mon -r1 | grep fsys Mount the file system.\n[n000]# cscli mount -f cls01234 Next step Return to System Power On Procedures and continue with next step.\n"
},
{
	"uri": "/docs-csm/en-12/operations/package_repository_management/restrict_admin_privileges_in_nexus/",
	"title": "Restrict Admin Privileges in Nexus",
	"tags": [],
	"description": "",
	"content": "Restrict Admin Privileges in Nexus Prior to making the system available to users, change the ingress settings to disable connections to packages.local and registry.local from automatically gaining administrative privileges.\nConnections to packages.local and registry.local automatically login clients as the admin user. Administrative privileges enable any user to make anonymous writes to Nexus, which means unauthenticated users can perform arbitrary actions on Nexus itself through the REST API, as well as in repositories by uploading or deleting assets.\nProduct installers currently do not expect to authenticate to Nexus, so it is necessary to retain the default ingress settings during installation.\nPrerequisites Procedure Removing the patch Prerequisites CSM installation is complete.\nProcedure Verify that the registry repository has docker.forceBasicAuth set to true.\nncn-mw# curl -sS https://packages.local/service/rest/beta/repositories \\ | jq \u0026#39;.[] | select(.name == \u0026#34;registry\u0026#34;) | .docker.forceBasicAuth = true\u0026#39; \\ | curl -sSi -X PUT \u0026#39;https://packages.local/service/rest/beta/repositories/docker/hosted/registry\u0026#39; \\ -H \u0026#34;Content-Type: application/json\u0026#34; -d @- Set the SYSTEM_DOMAIN_NAME variable.\nncn-mw# SYSTEM_DOMAIN_NAME=$(kubectl get secret site-init -n loftsman -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | \\ base64 -d | yq r - \u0026#39;spec.network.dns.external\u0026#39;) ncn-mw# echo \u0026#34;System domain name is: ${SYSTEM_DOMAIN_NAME}\u0026#34; Patch the Nexus VirtualService resource in the nexus namespace to remove the X-WEBAUTH-USER request header when the authority matches packages.local or registry.local.\nncn-mw# kubectl patch virtualservice -n nexus nexus --type merge --patch \\ \u0026#34;{\\\u0026#34;spec\\\u0026#34;:{\\\u0026#34;http\\\u0026#34;:[{\\\u0026#34;match\\\u0026#34;:[{\\\u0026#34;authority\\\u0026#34;:{\\\u0026#34;exact\\\u0026#34;:\\\u0026#34;packages.local\\\u0026#34;}}],\\ \\\u0026#34;route\\\u0026#34;:[{\\\u0026#34;destination\\\u0026#34;:{\\\u0026#34;host\\\u0026#34;:\\\u0026#34;nexus\\\u0026#34;,\\\u0026#34;port\\\u0026#34;:{\\\u0026#34;number\\\u0026#34;:80}},\\\u0026#34;headers\\\u0026#34;:{\\ \\\u0026#34;request\\\u0026#34;:{\\\u0026#34;remove\\\u0026#34;:[\\\u0026#34;X-WEBAUTH-USER\\\u0026#34;]}}}]},{\\\u0026#34;match\\\u0026#34;:[{\\\u0026#34;authority\\\u0026#34;:\\ {\\\u0026#34;exact\\\u0026#34;:\\\u0026#34;registry.local\\\u0026#34;}}],\\\u0026#34;route\\\u0026#34;:[{\\\u0026#34;destination\\\u0026#34;:{\\\u0026#34;host\\\u0026#34;:\\\u0026#34;nexus\\\u0026#34;,\\ \\\u0026#34;port\\\u0026#34;:{\\\u0026#34;number\\\u0026#34;:5003}},\\\u0026#34;headers\\\u0026#34;:{\\\u0026#34;request\\\u0026#34;:{\\\u0026#34;remove\\\u0026#34;:[\\\u0026#34;X-WEBAUTH-USER\\\u0026#34;]}}}]},\\ {\\\u0026#34;match\\\u0026#34;:[{\\\u0026#34;authority\\\u0026#34;:{\\\u0026#34;exact\\\u0026#34;:\\\u0026#34;nexus.cmn.${SYSTEM_DOMAIN_NAME}\\\u0026#34;}}],\\\u0026#34;route\\\u0026#34;:\\ [{\\\u0026#34;destination\\\u0026#34;:{\\\u0026#34;host\\\u0026#34;:\\\u0026#34;nexus\\\u0026#34;,\\\u0026#34;port\\\u0026#34;:{\\\u0026#34;number\\\u0026#34;:80}},\\\u0026#34;headers\\\u0026#34;:\\ {\\\u0026#34;request\\\u0026#34;:{\\\u0026#34;add\\\u0026#34;:{\\\u0026#34;X-WEBAUTH-USER\\\u0026#34;:\\\u0026#34;admin\\\u0026#34;},\\\u0026#34;remove\\\u0026#34;:[\\\u0026#34;Authorization\\\u0026#34;]}}}]}]}}\u0026#34; The following is an example of the Nexus VirtualService resource before the patch:\nspec: http: - match: - authority: exact: packages.local route: - destination: host: nexus port: number: 80 headers: request: add: X-WEBAUTH-USER: admin remove: - Authorization - match: - authority: exact: registry.local route: - destination: host: nexus port: number: 5003 headers: request: add: X-WEBAUTH-USER: admin remove: - Authorization The patch will update the information to the following:\nspec: http: - match: - authority: exact: packages.local route: - destination: host: nexus port: number: 80 headers: request: remove: - X-WEBAUTH-USER - match: - authority: exact: registry.local route: - destination: host: nexus port: number: 5003 headers: request: remove: - X-WEBAUTH-USER Removing the patch If the patch needs to be removed for maintenance activities or any other purpose, then first make sure that $SYSTEM_DOMAIN_NAME is set, then run the following command:\nncn-mw# kubectl patch virtualservice -n nexus nexus --type merge --patch \\ \u0026#34;{\\\u0026#34;spec\\\u0026#34;:{\\\u0026#34;http\\\u0026#34;:[{\\\u0026#34;match\\\u0026#34;:[{\\\u0026#34;authority\\\u0026#34;:{\\\u0026#34;exact\\\u0026#34;:\\\u0026#34;packages.local\\\u0026#34;}}]\\ ,\\\u0026#34;route\\\u0026#34;:[{\\\u0026#34;destination\\\u0026#34;:{\\\u0026#34;host\\\u0026#34;:\\\u0026#34;nexus\\\u0026#34;,\\\u0026#34;port\\\u0026#34;:{\\\u0026#34;number\\\u0026#34;:80}},\\\u0026#34;headers\\\u0026#34;:\\ {\\\u0026#34;request\\\u0026#34;:{\\\u0026#34;add\\\u0026#34;:{\\\u0026#34;X-WEBAUTH-USER\\\u0026#34;:\\\u0026#34;admin\\\u0026#34;},\\\u0026#34;remove\\\u0026#34;:[\\\u0026#34;Authorization\\\u0026#34;]}}}]},\\ {\\\u0026#34;match\\\u0026#34;:[{\\\u0026#34;authority\\\u0026#34;:{\\\u0026#34;exact\\\u0026#34;:\\\u0026#34;registry.local\\\u0026#34;}}],\\\u0026#34;route\\\u0026#34;:[{\\\u0026#34;destination\\\u0026#34;:\\ {\\\u0026#34;host\\\u0026#34;:\\\u0026#34;nexus\\\u0026#34;,\\\u0026#34;port\\\u0026#34;:{\\\u0026#34;number\\\u0026#34;:5003}},\\\u0026#34;headers\\\u0026#34;:{\\\u0026#34;request\\\u0026#34;:{\\\u0026#34;add\\\u0026#34;:\\ {\\\u0026#34;X-WEBAUTH-USER\\\u0026#34;:\\\u0026#34;admin\\\u0026#34;},\\\u0026#34;remove\\\u0026#34;:[\\\u0026#34;Authorization\\\u0026#34;]}}}]},{\\\u0026#34;match\\\u0026#34;:\\ [{\\\u0026#34;authority\\\u0026#34;:{\\\u0026#34;exact\\\u0026#34;:\\\u0026#34;nexus.cmn.${SYSTEM_DOMAIN_NAME}\\\u0026#34;}}],\\\u0026#34;route\\\u0026#34;:\\ [{\\\u0026#34;destination\\\u0026#34;:{\\\u0026#34;host\\\u0026#34;:\\\u0026#34;nexus\\\u0026#34;,\\\u0026#34;port\\\u0026#34;:{\\\u0026#34;number\\\u0026#34;:80}},\\\u0026#34;headers\\\u0026#34;:\\ {\\\u0026#34;request\\\u0026#34;:{\\\u0026#34;add\\\u0026#34;:{\\\u0026#34;X-WEBAUTH-USER\\\u0026#34;:\\\u0026#34;admin\\\u0026#34;},\\\u0026#34;remove\\\u0026#34;:[\\\u0026#34;Authorization\\\u0026#34;]}}}]}]}}\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/rebuild_ncns/wipe_drives/",
	"title": "Wipe Drives",
	"tags": [],
	"description": "",
	"content": "Wipe Drives WARNING: This is the point of no return. Once the disks are wiped, the node must be rebuilt.\nAll commands in this section must be run on the node being rebuilt (unless otherwise indicated). These commands can be done from the ConMan console window.\nOnly follow the steps in the section for the node type that is being rebuilt:\nWipe Disks: Master Wipe Disks: Worker Node Wipe Disks: Utility Storage Node Wipe Disks: Master Unmount the etcd volume and remove the volume group.\nNOTE: etcd should already be stopped as part of the \u0026ldquo;Prepare Master Node\u0026rdquo; steps.\n/run/lib-etcd vgremove -f etcdvg0-ETCDK8S Unmount the SDU mountpoint and remove the volume group.\numount /var/lib/sdu vgremove -f metalvg0-CRAYSDU Wipe the drives\nmdisks=$(lsblk -l -o SIZE,NAME,TYPE,TRAN | grep -E \u0026#39;(sata|nvme|sas)\u0026#39; | sort -h | awk \u0026#39; {print \u0026#34;/dev/\u0026#34; $2}\u0026#39;) wipefs --all --force $mdisks Wipe Disks: Worker Node Stop containerd and wipe drives.\nsystemctl stop containerd.service Unmount partitions.\numount /var/lib/kubelet umount /run/lib-containerd umount /run/containerd Wipe the drives.\nwipefs --all --force /dev/sd* /dev/disk/by-label/* Wipe Disks: Utility Storage Node Stop running OSDs on the node being wiped.\nncn-s# systemctl stop ceph-osd.target Make sure the OSDs (if any) are not running after running the first command.\nncn-s# ls -1 /dev/sd* /dev/disk/by-label/* ncn-s# vgremove -f --select \u0026#39;vg_name=~ceph*\u0026#39; Unmount and remove the metalvg0 volume group.\numount /etc/ceph umount /var/lib/ceph umount /var/lib/containers vgremove -f metalvg0 Wipe the disks and RAIDs.\nwipefs --all --force /dev/sd* /dev/disk/by-label/* Next Step Proceed to the next step to Power Cycle and Rebuild Nodes or return to the main Rebuild NCNs page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_remove_replace_ncns/remove_ncn_from_role/",
	"title": "Remove NCN from Role",
	"tags": [],
	"description": "",
	"content": "Remove NCN from Role Description Remove a master, worker, or storage NCN from current roles. Select the procedure below based on the node type, complete the remaining steps to wipe the drives, and then power off the node.\nProcedure IMPORTANT: The following procedures assume that the variables from the prerequisites section have been set.\nRemove roles Master node Worker node Storage node Wipe the drives Master node Worker node Storage node Power off the node Next step 1. Remove roles Master node remove roles First master node Determine if the master node being removed is the first master node.\nFetch the defined first-master-hostname.\nncn-mw# cray bss bootparameters list --hosts Global --format json |jq -r \u0026#39;.[].\u0026#34;cloud-init\u0026#34;.\u0026#34;meta-data\u0026#34;.\u0026#34;first-master-hostname\u0026#34;\u0026#39; Example output:\nncn-m002 If the node returned is not the one being removed, then skip the substeps here and proceed to Reset Kubernetes on master node being removed. IMPORTANT: The first master node is the node others contact to join the Kubernetes cluster. If this is the node being removed, then perform the remaining substeps here in order to promote another master node to the initial node, before proceeding with the rest of the overall procedure. Reconfigure the Boot Script Service (BSS) to point to a new first master node.\nncn-mw# cray bss bootparameters list --name Global --format=json | jq \u0026#39;.[]\u0026#39; \u0026gt; Global.json Edit the Global.json file and edit the indicated line.\nChange the first-master-hostname value to another node that will be promoted to the first master node. For example, in order to change the first master node to ncn-m001, then change the line to the following:\n\u0026#34;first-master-hostname\u0026#34;: \u0026#34;ncn-m001\u0026#34;, Get a token to interact with BSS using the REST API.\nncn-mw# TOKEN=$(curl -s -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Do a PUT action for the new JSON file.\nncn-mw# curl -i -s -H \u0026#34;Content-Type: application/json\u0026#34; -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ \u0026#34;https://api-gw-service-nmn.local/apis/bss/boot/v1/bootparameters\u0026#34; -X PUT -d @./Global.json Ensure that a good response, such as HTTP code 200, is returned in the curl output.\nConfigure the newly promoted first master node so it is able to have other nodes join the cluster.\nUse SSH to log in to the newly promoted master node chosen in the previous steps.\nCopy/paste the following script to a file, and then execute it.\n#!/bin/bash source /srv/cray/scripts/metal/lib.sh export KUBERNETES_VERSION=\u0026#34;v$(cat /etc/cray/kubernetes/version)\u0026#34; echo $(kubeadm init phase upload-certs --upload-certs 2\u0026gt;\u0026amp;1 | tail -1) \u0026gt; /etc/cray/kubernetes/certificate-key export CERTIFICATE_KEY=$(cat /etc/cray/kubernetes/certificate-key) export MAX_PODS_PER_NODE=$(craysys metadata get kubernetes-max-pods-per-node) export PODS_CIDR=$(craysys metadata get kubernetes-pods-cidr) export SERVICES_CIDR=$(craysys metadata get kubernetes-services-cidr) envsubst \u0026lt; /srv/cray/resources/common/kubeadm.yaml \u0026gt; /etc/cray/kubernetes/kubeadm.yaml kubeadm token create --print-join-command \u0026gt; /etc/cray/kubernetes/join-command 2\u0026gt;/dev/null echo \u0026#34;$(cat /etc/cray/kubernetes/join-command) --control-plane --certificate-key $(cat /etc/cray/kubernetes/certificate-key)\u0026#34; \u0026gt; /etc/cray/kubernetes/join-command-control-plane mkdir -p /srv/cray/scripts/kubernetes cat \u0026gt; /srv/cray/scripts/kubernetes/token-certs-refresh.sh \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; #!/bin/bash export KUBECONFIG=/etc/kubernetes/admin.conf if [[ \u0026#34;$1\u0026#34; != \u0026#34;skip-upload-certs\u0026#34; ]]; then kubeadm init phase upload-certs --upload-certs --config /etc/cray/kubernetes/kubeadm.yaml fi kubeadm token create --print-join-command \u0026gt; /etc/cray/kubernetes/join-command 2\u0026gt;/dev/null echo \u0026#34;$(cat /etc/cray/kubernetes/join-command) --control-plane --certificate-key $(cat /etc/cray/kubernetes/certificate-key)\u0026#34; \\ \u0026gt; /etc/cray/kubernetes/join-command-control-plane EOF chmod +x /srv/cray/scripts/kubernetes/token-certs-refresh.sh /srv/cray/scripts/kubernetes/token-certs-refresh.sh skip-upload-certs echo \u0026#34;0 */1 * * * root /srv/cray/scripts/kubernetes/token-certs-refresh.sh \u0026gt;\u0026gt; /var/log/cray/cron.log 2\u0026gt;\u0026amp;1\u0026#34; \u0026gt; /etc/cron.d/cray-k8s-token-certs-refresh cp /srv/cray/resources/common/cronjob_kicker.py /usr/bin/cronjob_kicker.py chmod +x /usr/bin/cronjob_kicker.py echo \u0026#34;0 */2 * * * root KUBECONFIG=/etc/kubernetes/admin.conf /usr/bin/cronjob_kicker.py \u0026gt;\u0026gt; /var/log/cray/cron.log 2\u0026gt;\u0026amp;1\u0026#34; \u0026gt; /etc/cron.d/cray-k8s-cronjob-kicker Reset Kubernetes on master node being removed Run the following command on the node being removed. The command can be run from a ConMan console window.\nncn-m# kubeadm reset --force Stop running containers on master node being removed Run the commands in this section on the node being removed. The commands can be run from a ConMan console window.\nList any containers running in containerd.\nncn-m# crictl ps Example output:\nCONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 66a78adf6b4c2 18b6035f5a9ce About a minute ago Running spire-bundle 1212 6d89f7dee8ab6 7680e4050386d c8344c866fa55 24 hours ago Running speaker 0 5460d2bffb4d7 b6467c907f063 8e6730a2b718c 3 days ago Running request-ncn-join-token 0 a3a9ca9e1ca78 e8ce2d1a8379f 64d4c06dc3fb4 3 days ago Running istio-proxy 0 6d89f7dee8ab6 c3d4811fc3cd0 0215a709bdd9b 3 days ago Running weave-npc 0 f5e25c12e617e If there are any running containers from the output of the crictl ps command, then stop them.\nncn-m# crictl stop \u0026lt;container id from the CONTAINER column\u0026gt; Remove the master node from the Kubernetes cluster IMPORTANT: Run this command from a master or worker node that is NOT being deleted.\nRemove the master node from the Kubernetes cluster.\nncn-mw# kubectl delete node \u0026#34;${NODE}\u0026#34; Remove the master node from Etcd Determine the member ID of the master node being removed.\nRun the following command and find the line with the name of the master being removed. Note the member ID and IP address for use in subsequent steps.\nThe member ID is the alphanumeric string in the first field of that line. The IP address is in the URL in the fourth field in the line. On any master node:\nncn-m# etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/ca.crt \\ --key=/etc/kubernetes/pki/etcd/ca.key --endpoints=localhost:2379 member list Remove the master node from the Etcd cluster backing Kubernetes.\nReplace the \u0026lt;MEMBER_ID\u0026gt; value with the value returned in the previous sub-step.\nncn-m# etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/ca.crt \\ --key=/etc/kubernetes/pki/etcd/ca.key --endpoints=localhost:2379 member remove \u0026lt;MEMBER_ID\u0026gt; Stop services on master node being removed Stop kubelet, containerd, and Etcd services on the master node being removed.\nncn-m# systemctl stop kubelet.service ; systemctl stop containerd.service ; systemctl stop etcd.service Add the master node back into the Etcd cluster This will allow the node to rejoin the cluster automatically when it gets added back.\nThe IP address and hostname of the rebuilt node is needed for the following command. Replace the \u0026lt;IP_ADDRESS\u0026gt; address value with the IP address noted in an earlier step from the etcdctl command. Ensure that the NODE variable is set. ncn-mw# etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/ca.crt \\ --key=/etc/kubernetes/pki/etcd/ca.key --endpoints=localhost:2379 member add \u0026#34;${NODE}\u0026#34; \\ --peer-urls=https://\u0026lt;IP_ADDRESS\u0026gt;:2380 Remove Etcd data directory on master node being removed Remove the Etcd data directory on the master node being removed.\nncn-m# rm -rf /var/lib/etcd/* Save lan0 configuration from ncn-m001 Skip this step if ncn-m001 is not being removed.\nSave a copy of the lan0 configuration from ncn-m001 only if ncn-m001 is being removed.\nncn-m001# rsync /etc/sysconfig/network/ifcfg-lan0 ncn-m002:/tmp/ifcfg-lan0-m001 Master node role removal complete The master node role removal is complete. Proceed to Wipe the drives.\nWorker node remove roles Drain the node Drain the node in order to clear any pods running on the node.\nIMPORTANT: The following command will cordon and drain the node.\nncn-mw# kubectl drain --ignore-daemonsets --delete-local-data \u0026#34;${NODE}\u0026#34; There may be pods that cannot be gracefully evicted due to Pod Disruption Budgets (PDB). For example:\nerror when evicting pod \u0026#34;\u0026lt;pod\u0026gt;\u0026#34; (will retry after 5s): Cannot evict pod as it would violate the pod\u0026#39;s disruption budget. In this case, there are some options. If the service is scalable, the scale can be increased. The goal is to get another pod to start up on another node, allowing the drain to delete the original pod.\nHowever, it will probably be necessary to force the deletion of the pod:\nncn-mw# kubectl delete pod [-n \u0026lt;namespace\u0026gt;] --force --grace-period=0 \u0026lt;pod\u0026gt; This will delete the offending pod, and Kubernetes should schedule a replacement on another node. Then rerun the kubectl drain command, and it should report that the node is drained.\nReset Kubernetes on worker node being removed Run the following command on the node being removed. The command can be run from a ConMan console window.\nncn-w# kubeadm reset --force Stop running containers on worker node being removed Run the commands in this section on the node being removed. The commands can be run from a ConMan console window.\nList any containers running in containerd.\nncn-w# crictl ps Example output:\nCONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 66a78adf6b4c2 18b6035f5a9ce About a minute ago Running spire-bundle 1212 6d89f7dee8ab6 7680e4050386d c8344c866fa55 24 hours ago Running speaker 0 5460d2bffb4d7 b6467c907f063 8e6730a2b718c 3 days ago Running request-ncn-join-token 0 a3a9ca9e1ca78 e8ce2d1a8379f 64d4c06dc3fb4 3 days ago Running istio-proxy 0 6d89f7dee8ab6 c3d4811fc3cd0 0215a709bdd9b 3 days ago Running weave-npc 0 f5e25c12e617e If there are any running containers from the output of the crictl ps command, then stop them.\nncn-w# crictl stop \u0026lt;container id from the CONTAINER column\u0026gt; Remove the worker node from the Kubernetes cluster after the node is drained ncn-mw# kubectl delete node \u0026#34;${NODE}\u0026#34; Ensure that all pods are stopped on the worker node ncn-mw# kubectl get pods -A -o wide | grep \u0026#34;${NODE}\u0026#34; If no pods are returned, then proceed to the next step. Otherwise, wait for any remaining pods to terminate.\nNo mapped rbd devices on the worker node Ensure that there are no mapped rbd devices on the worker node being removed.\nRun the following command on the node being removed. The command can be run from a ConMan console window.\nncn-w# rbd showmapped If mapped devices still exist, then perform the Stop running containers on worker node being removed step again. If devices are still mapped, then forcibly unmap them using rbd unmap -o force /dev/rbd#, where /dev/rbd# is the device that is still returned as mapped.\nWorker node role removal complete The worker node role removal is complete. Proceed to Wipe the drives.\nStorage node remove roles Open a new tab and follow the Remove Ceph Node procedure in order to remove Ceph role from the storage node.\nOnce the storage node role removal is complete, then proceed to Wipe the drives.\n2. Wipe the drives Wipe disks: master node NOTE: etcd should already be stopped as part of the Remove roles steps.\nAll commands in this section must be run on the node being removed (unless otherwise indicated). These commands can be done from a ConMan console window.\nUnmount etcd and SDU, and remove the volume group.\nncn-m# umount -v /run/lib-etcd /var/lib/etcd /var/lib/s3fs_cache /var/lib/admin-tools ncn-m# vgremove -f -v --select \u0026#39;vg_name=~metal*\u0026#39; Wipe the drives.\nncn-m# mdisks=$(lsblk -l -o SIZE,NAME,TYPE,TRAN | grep -E \u0026#39;(sata|nvme|sas)\u0026#39; | sort -h | awk \u0026#39; {print \u0026#34;/dev/\u0026#34; $2}\u0026#39;) ; echo $mdisks ncn-m# wipefs --all --force ${mdisks} Once the wipe of the drives is complete, proceed to Power off the node.\nWipe disks: worker node All commands in this section must be run on the node being removed (unless otherwise indicated). These commands can be done from a ConMan console window.\nStop containerd.\nncn-w# systemctl stop containerd.service Unmount partitions and remove the volume group.\nncn-w# umount -v /var/lib/kubelet /run/lib-containerd /run/containerd /var/lib/s3fs_cache ncn-w# vgremove -f -v --select \u0026#39;vg_name=~metal*\u0026#39; Wipe the disks and RAIDs.\nncn-w# wipefs --all --force /dev/disk/by-label/* ncn-w# wipefs --all --force /dev/sd* Once the wipe of the drives is complete, proceed to Power off the node.\nWipe disks: utility storage node All commands in this section must be run on the node being removed (unless otherwise indicated). These commands can be done from a ConMan console window.\nMake sure the OSDs (if any) are not running.\nncn-s# podman ps Examine the output. There should be no running Ceph processes or containers.\nRemove the Ceph volume groups.\nncn-s# ls -1 /dev/sd* /dev/disk/by-label/* ncn-s# vgremove -f --select \u0026#39;vg_name=~ceph*\u0026#39; Unmount and remove the metalvg0 volume group.\nncn-s# umount -v /etc/ceph /var/lib/ceph /var/lib/containers ncn-s# vgremove -f metalvg0 Wipe the disks and RAIDs.\nncn-s# wipefs --all --force /dev/disk/by-label/* ncn-s# wipefs --all --force /dev/sd* Once the wipe of the drives is complete, proceed to Power off the node.\n3. Power off the node IMPORTANT: Run these commands from a node NOT being powered off.\nSet the BMC variable to the hostname of the BMC of the node being powered off.\nlinux# BMC=\u0026#34;${NODE}-mgmt\u0026#34; For ncn-m001 only: Collect and record the BMC IP address for ncn-m001 and the CMN IP address for ncn-m002.\nDo this before ncn-m001 is powered off. These may be needed later.\nRecord the BMC IP address for ncn-m001.\nncn-m001# BMC_IP=$(ipmitool lan print | grep \u0026#39;IP Address\u0026#39; | grep -v \u0026#39;Source\u0026#39; | awk -F \u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;) ncn-m001# echo ${BMC_IP} Example output:\n172.30.52.74 Record the CMN IP address for ncn-m002.\nncn# CMN_IP=$(ssh ncn-m002 ip -4 a show bond0.cmn0 | grep inet | awk \u0026#39;{print $2}\u0026#39; | cut -d / -f1) ncn# echo ${CMN_IP} Example output:\n10.102.4.9 Set and export the root user password of the BMC.\nread -s is used in order to prevent the password from being echoed to the screen or saved in the shell history.\nlinux# read -r -s -p \u0026#34;BMC root password: \u0026#34; IPMI_PASSWORD linux# export IPMI_PASSWORD Power off the node.\nlinux# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC}\u0026#34; chassis power off Verify that the node is off.\nlinux# ipmitool -I lanplus -U root -E -H \u0026#34;${BMC}\u0026#34; chassis power status Ensure that the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before proceeding.\nNext step Proceed to Remove NCN Data or return to the main Add, Remove, Replace, or Move NCNs page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/check_hsm/",
	"title": "Check HSM",
	"tags": [],
	"description": "",
	"content": "Check HSM Hardware State Manager has two important parts:\nSystem Layout Service (SLS): This is the \u0026ldquo;expected\u0026rdquo; state of the system (as populated by networks.yaml and other sources). State Manager Daemon (SMD): This is the \u0026ldquo;discovered\u0026rdquo; or active state of the system during runtime. Prerequisites The API calls on this page require an authorization token to be set in the TOKEN variable. See Retrieve an Authentication Token. The cray CLI commands on this page require the Cray command line interface to be configured. See Configure the Cray CLI. SLS API call\nncn# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api_gw_service.local/apis/sls/v1/hardware | jq CLI command\nncn# cray sls hardware list --format json In either case, the output from SLS should consist of a list of objects that look like the following:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x1000c7s1b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x1000c7s1b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;Mountain\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid001228\u0026#34; ], \u0026#34;NID\u0026#34;: 1228, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34; } } SMD API call\nncn# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api_gw_service.local/apis/smd/hsm/v2/Inventory/EthernetInterfaces | jq CLI command\nncn# cray hsm inventory ethernetInterfaces list --format json In either case, the output from SMD should consist of a list of objects that look like the following:\n{ \u0026#34;ID\u0026#34;: \u0026#34;0040a6838b0e\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;0040a6838b0e\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.1.147\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2020-07-24T23:44:24.578476Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1000c7s1b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34; } Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/lag/",
	"title": "Configure Link Aggregation Group (LAG)",
	"tags": [],
	"description": "",
	"content": "Configure Link Aggregation Group (LAG) Link aggregation allows administrators to assign multiple physical links to one logical link that functions as a single, higher-speed link providing dramatically increased bandwidth.\nConfiguration Commands Create and configure the LAG interface:\nswitch(config)# interface port-channel 10 switch(config-if-po-1)# no shutdown Associate member links with the LAG interface:\nExample: switch(config)# interface IFACE\nswitch(config)# interface ethernet 1/1/1 switch(conf-if-eth1/1/1)# channel-group 10 To enable LACP on the LAG:\nswitch(config)# interface ethernet 1/1/1 switch(conf-if-eth1/1/1)#channel-group 10 mode active Show commands to validate functionality:\nswitch# show interface port-channel Expected Results Administrators can create and configure a LAG Administrators can add ports to a LAG Administrators can configure a LAG interface Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/canu/update_canu_from_csm_tarball/",
	"title": "Update CANU From CSM Release Tarball",
	"tags": [],
	"description": "",
	"content": "Update CANU From CSM Release Tarball If doing a CSM install or upgrade, the release tarball contains a CANU RPM. It can be extracted and installed using the following steps.\nProcedure Display the current CANU version.\nncn# canu --version Set the TARBALL variable to the path and filename of the CSM release tarball:\nncn# TARBALL=/your/path/here/csm-version.tar.gz Extract the CANU RPM from the tarball:\nncn# tar -xzvf \u0026#34;$TARBALL\u0026#34; --wildcards \u0026#34;*/canu*.rpm\u0026#34; Output should look similar to the following:\ncsm-1.2.0-beta.81/rpm/cray/csm/sle-15sp2/x86_64/canu-1.2.1-1.x86_64.rpm Note the path to the RPM from the output of the previous command, and install it:\nncn# rpm -Uvh \u0026lt;path-to-canu-rpm\u0026gt; Display the new CANU version.\nncn# canu --version "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/check_current_dhcp_leases/",
	"title": "Check Current DHCP Leases",
	"tags": [],
	"description": "",
	"content": "Check Current DHCP Leases Use the Kea API to retrieve data from the DHCP lease database.\nPrerequisites An auth token is set up. If one has not been set up, log on to ncn-w001 or a worker/manager with kubectl and run the following:\nexport TOKEN=$(curl -s -k -S -d grant_type=client_credentials -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Once an auth token is generated, these commands can be run on a worker or manager node.\nCommands to Check Leases Get all leases:\nWARNING: This may cause the terminal to crash based on the size of the output.\ncurl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api_gw_service.local/apis/dhcp-kea | jq If you have the IP and are looking for the hostname/MAC address. IP Lookup:\ncurl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ], \u0026#34;arguments\u0026#34;: { \u0026#34;ip-address\u0026#34;: \u0026#34;x.x.x.x\u0026#34; } }\u0026#39; https://api_gw_service.local/apis/dhcp-kea | jq Use the MAC to find the hostname/IP Address:\ncurl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api_gw_service.local/apis/dhcp-kea | jq \u0026#39;.[].arguments.leases[] | select(.\u0026#34;hw-address\u0026#34;==\u0026#34;XX:XX:XX:XX:XX:5d\u0026#34;)\u0026#39; Use the hostname to find the MAC/IP address:\ncurl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api_gw_service.local/apis/dhcp-kea | jq \u0026#39;.[].arguments.leases[] | select(.\u0026#34;hostname\u0026#34;==\u0026#34;xNAME\u0026#34;)\u0026#39; View the total amount of leases:\ncurl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api_gw_service.local/apis/dhcp-kea | jq \u0026#39;.[].text\u0026#39; Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/clear_space_in_an_etcd_cluster_database/",
	"title": "Clear Space in an etcd Cluster Database",
	"tags": [],
	"description": "",
	"content": "Clear Space in an etcd Cluster Database Use this procedure to clear the etcd cluster NOSPACE alarm. Once it is set it will remain set. If needed, defrag the database cluster before clearing the NOSPACE alarm.\nDefragging the database cluster and clearing the etcd cluster NOSPACE alarm will free up database space.\nPrerequisites This procedure requires root privileges The etcd clusters are in a healthy state Procedure Clear up space when the etcd database space has exceeded and has been defragged, but the NOSPACE alarm remains set.\nVerify that the attempt to store a new key-value fails.\nReplace hbtd-ETCD_CLUSTER before running the following command. hbtd-etcd-h59j42knjv is an example replacement value.\nncn-mw# kubectl -n services exec -it hbtd-ETCD_CLUSTER -c etcd -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl put foo bar\u0026#34; Example output:\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2020-10-23T23:56:48.408Z\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;clientv3/retry_interceptor.go:62\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;retrying of unary invoker failed\u0026#34;,\u0026#34;target\u0026#34;:\u0026#34;endpoint://client-208534eb-2ab4-4c58-8853-58bff088c394/127.0.0.1:2379\u0026#34;,\u0026#34;attempt\u0026#34;:0,\u0026#34;error\u0026#34;:\u0026#34;rpc error: code = ResourceExhausted desc = etcdserver: mvcc: database space exceeded\u0026#34;} Error: etcdserver: mvcc: database space exceeded Check to see if the default 2G disk usage space (unless defined differently in the Helm chart) is currently exceeded.\nIn the following example, the disk usage is 375.5 M, which means the disk space has not been exceeded.\nReplace hbtd-ETCD_CLUSTER before running the following command. hbtd-etcd-h59j42knjv is an example replacement value.\nncn-mw# kubectl -n services exec -it hbtd-ETCD_CLUSTER -c etcd -- df -h Example output:\nFilesystem Size Used Available Use% Mounted on overlay 396.3G 59.6G 316.5G 16% / tmpfs 64.0M 0 64.0M 0% /dev tmpfs 125.7G 0 125.7G 0% /sys/fs/cgroup /dev/rbd21 2.9G 375.5M 2.5G 13% /var/etcd. \u0026lt;------/dev/sdc4 396.3G 22.0G 354.1G 6% /etc/hosts /dev/sdc4 396.3G 22.0G 354.1G 6% /dev/termination-log /dev/sdc5 396.3G 59.6G 316.5G 16% /etc/hostname /dev/sdc5 396.3G 59.6G 316.5G 16% /etc/resolv.conf Clear the NOSPACE alarm.\nncn-mw# for pod in $(kubectl get pods -l etcd_cluster=hbtd-etcd \\ -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod} ###\u0026#34; kubectl -n services exec ${pod} -- /bin/sh \\ -c \u0026#34;ETCDCTL_API=3 etcdctl alarm disarm\u0026#34; done Example output:\n### hbtd-etcd-h59j42knjv ### memberID:6004340417806974740 alarm:NOSPACE memberID:10618826089438871005 alarm:NOSPACE memberID:6927946043724325475 alarm:NOSPACE ### hbtd-etcd-jfwh9l49lm ### ### hbtd-etcd-mhklm4n5qd ### Verify that a new key-value can now be successfully stored.\nReplace hbtd-ETCD_CLUSTER before running the following command. hbtd-etcd-h59j42knjv is an example replacement value.\nncn-mw# kubectl -n services exec -it hbtd-ETCD_CLUSTER -c etcd -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl put foo bar\u0026#34; Example output:\nOK Clear the NOSPACE alarm. If the database needs to be defragged, then the alarm will be reset.\nConfirm that the \u0026ldquo;database space exceeded\u0026rdquo; message is present.\nncn-mw# kubectl logs -n services --tail=-1 --prefix=true -l \u0026#34;app.kubernetes.io/name=cray-hbtd\u0026#34; -c cray-hbtd | grep \u0026#34;x3005c0s19b1n0\u0026#34; Example output:\n[pod/cray-hbtd-56bc4f6fdb-92bqx/cray-hbtd] 2020/09/15 20:00:44 INTERNAL ERROR storing key {\u0026#34;Component\u0026#34;:\u0026#34;x3005c0s19b1n0\u0026#34;,\u0026#34;Last_hb_rcv_time\u0026#34;:\u0026#34;5f611d6c\u0026#34;,\u0026#34;Last_hb_timestamp\u0026#34;:\u0026#34;2020-09-15T15:00:44.878876-06:00\u0026#34;,\u0026#34;Last_hb_status\u0026#34;:\u0026#34;OK\u0026#34;,\u0026#34;Had_warning\u0026#34;:\u0026#34;\u0026#34;} : etcdserver: mvcc: database space exceeded [pod/cray-hbtd-56bc4f6fdb-92bqx/cray-hbtd] 2020/09/15 20:00:47 INTERNAL ERROR storing key {\u0026#34;Component\u0026#34;:\u0026#34;x3005c0s19b1n0\u0026#34;,\u0026#34;Last_hb_rcv_time\u0026#34;:\u0026#34;5f611d6f\u0026#34;,\u0026#34;Last_hb_timestamp\u0026#34;:\u0026#34;2020-09-15T15:00:47.893757-06:00\u0026#34;,\u0026#34;Last_hb_status\u0026#34;:\u0026#34;OK\u0026#34;,\u0026#34;Had_warning\u0026#34;:\u0026#34;\u0026#34;} : etcdserver: mvcc: database space exceeded [pod/cray-hbtd-56bc4f6fdb-92bqx/cray-hbtd] 2020/09/15 20:00:53 INTERNAL ERROR storing key {\u0026#34;Component\u0026#34;:\u0026#34;x3005c0s19b1n0\u0026#34;,\u0026#34;Last_hb_rcv_time\u0026#34;:\u0026#34;5f611d75\u0026#34;,\u0026#34;Last_hb_timestamp\u0026#34;:\u0026#34;2020-09-15T15:00:53.926195-06:00\u0026#34;,\u0026#34;Last_hb_status\u0026#34;:\u0026#34;OK\u0026#34;,\u0026#34;Had_warning\u0026#34;:\u0026#34;\u0026#34;} : etcdserver: mvcc: database space exceeded [pod/cray-hbtd-56bc4f6fdb-92bqx/cray-hbtd] 2020/09/15 20:01:02 INTERNAL ERROR storing key {\u0026#34;Component\u0026#34;:\u0026#34;x3005c0s19b1n0\u0026#34;,\u0026#34;Last_hb_rcv_time\u0026#34;:\u0026#34;5f611d7e\u0026#34;,\u0026#34;Last_hb_timestamp\u0026#34;:\u0026#34;2020-09-15T15:01:02.970168-06:00\u0026#34;,\u0026#34;Last_hb_status\u0026#34;:\u0026#34;OK\u0026#34;,\u0026#34;Had_warning\u0026#34;:\u0026#34;\u0026#34;} : etcdserver: mvcc: database space exceeded [pod/cray-hbtd-56bc4f6fdb-92bqx/cray-hbtd] 2020/09/15 20:01:05 INTERNAL ERROR storing key {\u0026#34;Component\u0026#34;:\u0026#34;x3005c0s19b1n0\u0026#34;,\u0026#34;Last_hb_rcv_time\u0026#34;:\u0026#34;5f611d81\u0026#34;,\u0026#34;Last_hb_timestamp\u0026#34;:\u0026#34;2020-09-15T15:01:05.983828-06:00\u0026#34;,\u0026#34;Last_hb_status\u0026#34;:\u0026#34;OK\u0026#34;,\u0026#34;Had_warning\u0026#34;:\u0026#34;\u0026#34;} : etcdserver: mvcc: database space exceeded Check if the default 2G (unless defined differently in the Helm chart) disk usage has been exceeded.\nReplace ETCD_CLUSTER_NAME before running the following command. For example, cray-hbtd-etcd-6p4tc4jdgm could be used.\nncn-mw# kubectl exec -it -n services ETCD_CLUSTER_NAME -c etcd -- df -h Example output:\nFilesystem Size Used Available Use% Mounted on overlay 439.1G 15.2G 401.5G 4% / tmpfs 64.0M 0 64.0M 0% /dev tmpfs 125.7G 0 125.7G 0% /sys/fs/cgroup /dev/rbd3 7.8G 2.4G 5.4G 31% /var/etcd Resolve the space issue by either increasing the frequency of how often the etcd-defrag cron job is run, or by triggering it manually.\nSelect one of the following options:\nIncrease the frequency of the kube-etcd-defrag from every 24 hours to 12 hours.\nncn-mw# kubectl edit -n operators cronjob.batch/kube-etcd-defrag Example output:\n[...] name: etcd-defrag name: etcd-defrag schedule: 0 */12 * * * successfulJobsHistoryLimit: 1 suspend: false status: [...] Trigger the job manually.\nncn-mw# kubectl -n operators create job --from=cronjob/kube-etcd-defrag kube-etcd-defrag Check the log messages after the defrag job is triggered.\nncn-mw# kubectl logs -f -n operators pod/kube-etcd-defrag-1600171200-fxpn7 Example output:\nDefragging cray-bos-etcd-j7czpr9pbr Defragging cray-bos-etcd-k4qtjtgqjb Defragging cray-bos-etcd-wcm8cs7dvc Defragging cray-bss-etcd-2h6k4l4j2g Defragging cray-bss-etcd-5dqwvrdtnf Defragging cray-bss-etcd-zlwmzkcjhz Defragging cray-cps-etcd-6cqw8sw5k6 Defragging cray-cps-etcd-psjm9lpw66 Defragging cray-cps-etcd-rp6fp94ccv Defragging cray-crus-etcd-228mdpm2h6 Defragging cray-crus-etcd-hldtxr6f9s Defragging cray-crus-etcd-sfsckpv4vw [...] Verify that the disk space is less than the size limit.\nReplace ETCD_CLUSTER_NAME before running the following command. For example, cray-hbtd-etcd-6p4tc4jdgm could be used.\nncn-mw# kubectl exec -it -n services ETCD_CLUSTER_NAME -c etcd -- df -h Example output:\nFilesystem Size Used Available Use% Mounted on overlay 439.1G 15.2G 401.5G 4% / tmpfs 64.0M 0 64.0M 0% /dev tmpfs 125.7G 0 125.7G 0% /sys/fs/cgroup /dev/rbd3 7.8G 403.0M 7.4G 5% /var/etcd. Turn off the NOSPACE alarm.\nReplace ETCD_CLUSTER_NAME before running the following command. For example, cray-hbtd-etcd-6p4tc4jdgm could be used.\nncn-mw# kubectl exec -it -n services ETCD_CLUSTER_NAME -c etcd -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl alarm disarm\u0026#34; Example output:\nmemberID:14039380531903955557 alarm:NOSPACE memberID:10060051157615504224 alarm:NOSPACE memberID:9418794810465807950 alarm:NOSPACE "
},
{
	"uri": "/docs-csm/en-12/operations/image_management/image_management_workflows/",
	"title": "Image Management Workflows",
	"tags": [],
	"description": "",
	"content": "Image Management Workflows Overview of how to create an image and how to customize and image.\nThe following workflows are intended to be high-level overviews of image management tasks. These workflows depict how services interact with each other during image management and help to provide a quicker and deeper understanding of how the system functions.\nThe workflows in this section include:\nCreate a new image Customize an image Create a new image Use Case: The system administrator creates an image root from a customized recipe. The new image can be used to boot compute nodes.\nComponents: This workflow is based on the interaction of the Image Management Service (IMS) with other services during the image build process. The process of image creation builds an image from a recipe. An administrator may choose to use the Cray-provided recipes or customize Kiwi recipes to define the image to be built.\nMentioned in this workflow:\nImage Management Service (IMS) allows administrators and users to build or customize (pre-boot) images from kiwi-ng recipes. This service is responsible for enabling the creation of bootable and non-bootable images, enabling image customization via an SSH-able environment, and packaging and association of new/customized image artifacts (kernel, rootfs, initrd, etc) with a new IMS image record. Nexus is needed for image creation and image customization. Nexus provides local RPM repositories for use when building or customizing an image. Administrators may define zypper or Yum package repositories and provide the RPM content for installing and updating software for every compute and non-compute node in the system. The Simple Storage Service (Ceph S3) is an artifact repository that stores boot artifacts. Recipes are stored in the ims bucket and images are stored in the boot-images bucket. Workflow Overview: The following sequence of steps occurs during this workflow.\nAdministrator downloads an existing recipe from S3.\nTo create a custom recipe, the administrator can first download an existing recipe from S3. A Kiwi recipe consists of multiple files and directories, which together define the repositories, packages and post-install actions to take during the Kiwi build process.\nAdministrator modifies the recipe and uploads to S3.\nModify the recipe by editing the files and subdirectories in the image-recipe directory. Edit the config.xml file to modify the name of the recipe, the set of RPM packages being installed or the RPM repositories being referenced. The recipe should be uploaded to S3 in a .tgz archive via CLI.\nAdministrator registers the recipe with IMS.\nRegistering the recipe creates a recipe record for your custom recipe in IMS.\nncn-mw# cray ims recipes list Administrator uploads public key.\nA public key is uploaded to allow them to access SSH shells that IMS provides.\nncn-mw# cray ims public-keys create --name \u0026#34;username public key\u0026#34; --public-key ~/.ssh/id_rsa.pub Administrator starts the creation job.\nCreate a new IMS image by providing request body parameter, job_type=\u0026ldquo;create\u0026rdquo;. The following steps 6-10 happen automatically as a part of the image creation process.\nncn-mw# cray ims jobs create \\ --job-type create \\ --image-root-archive-name cray-sles15-barebones \\ --artifact-id $IMS_RECIPE_ID \\ --public-key-id $IMS_PUBLIC_KEY_ID \\ --enable-debug False IMS to Ceph S3.\nIMS fetches the recipe from S3 and decompresses the recipe to a temporary directory.\nIMS to Nexus.\nIMS waits for repositories to be available from Nexus. The repositories are needed to build the image.\nIMS creates a custom RPM.\nIMS creates a custom RPM to install the CA root certificate from the system into the image. The build-ca-rpm container creates an RPM with the private root-CA certificate for the system and this RPM is installed automatically by Kiwi-NG. The CA root certificate is required to enable secure HTTPS access to the RPM repositories when building the image root.\nIMS calls Kiwi-NG to build the image.\nIMS calls Kiwi-NG to build the image root from the recipe and accesses packages in zypper/Yum repositories. The building of the image using kiwi happens in the build-image container. After kiwi is done building the image (either success or fail), the buildenv-sidecar container packages the artifacts, or in the case of failure, enables the debug shell if enable-debug is True. In the buildenv-sidecar container, the image artifacts are packaged and new image artifact records are created for each.\nIf there is a failure and enable-debug is true, a debug SSH shell is established. Admin can inspect image build root. Use commands touch /mnt/image/complete in a non-jailed environment or touch /tmp/complete in a jailed (chroot) environment to exit.\nSave the new image record in IMS.\nThe metadata for the new image artifacts is stored in IMS.\nUpload the new image artifacts to Ceph S3.\nThe new image artifacts are uploaded to Ceph S3.\nCustomize an image Use Case: The system administrator customizes an existing image and makes desired changes.\nComponents: This workflow is based on the interaction of the Image Management Service (IMS) with Ceph S3 during the image customization process. The customization workflow sets up a temporary image customization environment within a Kubernetes pod and mounts the image to be customized in that environment. A system administrator then makes the desired changes to the image root within the customization environment. IMS then compresses the customized image root and uploads it and its associated initrd image and kernel image (needed to boot a node) to Ceph S3.\nMentioned in this workflow:\nImage Management Service (IMS) allows administrators and users to build or customize (pre-boot) images from kiwi-ng recipes. This service is responsible for enabling the creation of bootable and non-bootable images, enabling image customization via an SSH-able environment, and packaging and association of new/customized image artifacts (kernel, rootfs, initrd, etc) with a new IMS image record. The Simple Storage Service (Ceph S3) is an artifact repository that stores artifacts. Recipes are stored in the ims bucket and images are stored in the boot-images bucket. Workflow Overview: The following sequence of steps occurs during this workflow.\nThe administrator identifies an existing image to be customized.\nRetrieve a list of ImageRecords indicating images that have been registered with IMS. IMS uses the ImageRecord to read the image\u0026rsquo;s manifest.yaml to find the image\u0026rsquo;s root file system (rootfs) artifact. Note the id of the image that you want to customize.\nncn-mw# cray ims images list The administrator uploads a public key.\nA public key is uploaded to allow them to access SSH shells that IMS provides.\nncn-mw# cray ims public-keys create --name \u0026#34;username public key\u0026#34; --public-key ~/.ssh/id_rsa.pub The administrator starts the image customization job.\nCreate a new IMS image by providing the --job-type customize argument to the cray ims jobs create command. The following steps (4-8) happen automatically as a part of the image customization process.\nncn-mw# cray ims jobs create \\ --job-type customize \\ --image-root-archive-name my_customized_image \\ --kernel-file-name vmlinuz \\ --initrd-file-name initrd \\ --artifact-id $IMS_IMAGE_ID \\ --public-key-id $IMS_PUBLIC_KEY_ID \\ --enable-debug False IMS to Ceph S3.\nIMS downloads the image root (rootfs) from Ceph S3 and decompresses the image root to a temporary directory.\nIMS creates an SSH environment for image customization.\nIMS spins up an sshd container so that the administrator can modify the image. The administrator accesses the sshd container and makes changes to the image. For example, it may be necessary to modify the timezone, or modify the programming environment, etc. Use touch /mnt/image/complete in a non-jailed environment or touch /tmp/complete in a jailed (chroot) environment to exit. The shell can be run in either a jailed or non-jailed mode.\nThe output is a new image. Note that the original image also exists. IMS customizes a copy of the original image.\nbuildenv-sidecar container packages new image artifacts.\nThe buildenv-sidecar container waits for the administrator to exit the SSH session. Upon completion, new records are created for each image artifact. It also adds the root CA certificate to the image and packages the new image artifacts (kernel, initrd, rootfs).\nSave the new image record in IMS.\nThe metadata for the new image artifacts is stored in IMS.\nUpload the new image artifacts to Ceph S3.\nThe new image artifacts are uploaded to Ceph S3.\n"
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/hardware_management_services_hms_locking_api/",
	"title": "Hardware Management Services (HMS) Locking API",
	"tags": [],
	"description": "",
	"content": "Hardware Management Services (HMS) Locking API The locking feature is a part of the Hardware State Manager (HSM) API. The locking API enables administrators to lock components on the system. Locking components ensures other system actors, such as administrators or running services, cannot perform a firmware update with the Firmware Action Service (FAS) or a power state change with the Cray Advanced Platform Monitoring and Control (CAPMC). Locks only constrain FAS and CAPMC from each other and help ensure that a firmware update action will not be interfered with by a request to power off the device through CAPMC. Locks only work with HMS services and will not impact other system services.\nLocks can only be used to prevent actions firmware updates with FAS or power state changes with CAPMC. Administrators can still use HMS APIs to view the state of various hardware components on the system, even if a lock is in place. There is no automatic locking for hardware devices. Locks need to be manually set or unset by an admin. A scenario that might be encountered is when a larger hardware state change job is run, and one of the components in the job has a lock on it. If FAS is the service running the job, FAS will attempt to update the firmware on each component, and will update all devices that do not have a lock on it. The job will not complete until the node lock ends, or if a timeout is set for the job.\nThe locking API also includes actions to repair or disable a node\u0026rsquo;s locking ability with respect to HMS services. The disable function will make it so a device cannot be firmware updated or power controlled (via an HMS service) until a repair is done. Future requests to perform a firmware update via FAS or power state change via CAPMC cannot be made on that component until the repair action is used.\nWARNING: System administrators should LOCK NCNs after the system has been brought up to prevent an admin from unintentionally firmware updating or powering off an NCN. If this lock is not engaged, an authorized request to FAS or CAPMC could power off the NCNs, which will negatively impact system stability and the health of services running on those NCNs.\n"
},
{
	"uri": "/docs-csm/en-12/operations/conman/troubleshoot_conman_blocking_access_to_a_node_bmc/",
	"title": "Troubleshoot ConMan Blocking Access to a Node BMC",
	"tags": [],
	"description": "",
	"content": "Troubleshoot ConMan Blocking Access to a Node BMC Disable ConMan if it is blocking access to a node by other means. ConMan runs on the system as a containerized service, and it is enabled by default. However, the use of ConMan to connect to a node blocks access to that node by other Serial over LAN (SOL) utilities or by a virtual KVM.\nFor information about how ConMan works, see ConMan.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI.\nProcedure Disable the console services.\nBecause the console services are looking for new hardware and continually verifying that console connections are established with all the nodes in the system, these services must be disabled to stop automatic console connections.\nFollow the directions in Disable ConMan After the System Software Installation to disable the automatic console connections.\nDisable the SOL session.\nEven after the console services are disabled, the ConMan SOL session might need to be directly disabled using ipmitool. Note: This is only required for River nodes because Mountain hardware does not use IPMI.\nread -s is used to prevent the password from appearing in the command history.\nncn# USERNAME=root ncn# read -r -s -p \u0026#34;BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -I lanplus -H BMC_IP -U \u0026#34;${USERNAME}\u0026#34; -E sol deactivate Restart the console services.\nRefer to the directions in Disable ConMan After the System Software Installation to restart the console services when all work is complete.\n"
},
{
	"uri": "/docs-csm/en-12/operations/firmware/upload_olympus_bmc_recovery_firmware_into_tftp_server/",
	"title": "Upload BMC Recovery Firmware into TFTP Server",
	"tags": [],
	"description": "",
	"content": "Upload BMC Recovery Firmware into TFTP Server cray-upload-recovery-images is a utility for uploading the BMC recovery files for ChassisBMCs, NodeBMCs, and RouterBMCs to be served by the cray-tftp service. The tool uses the cray CLI (fas, artifacts) and cray-tftp to download the S3 recovery images (as remembered by FAS), then upload them into the PVC that is used by cray-tftp. cray-upload-recovery-images should be run on every system.\nPrerequisites Cray System Management (CSM) software is installed. The Cray Command Line Interface (CLI) tool is initialized and configured on the system. Firmware is loaded into FAS as part of the HPC Firmware Pack (HFP) install; refer to the HPE Cray EX System HPC Firmware Pack Install Guide on the HPE Customer Support Center for more information. Procedure Execute the cray-upload-recovery-images script. ncn# cray-upload-recovery-images Attempting to retrieve ChassisBMC .itb file s3:/fw-update/d7bb5be9eecc11eab18c26c5771395a4/cc-1.3.10.itb d7bb5be9eecc11eab18c26c5771395a4/cc-1.3.10.itb Uploading file: /tmp/cc.itb Defaulting container name to cray-ipxe. Successfully uploaded /tmp/cc.itb! removed /tmp/cc.itb ChassisBMC recovery image upload complete ======================================== Attempting to retrieve NodeBMC .itb file s3:/fw-update/d81157f7eecc11ea943d26c5771395a4/nc-1.3.10.itb d81157f7eecc11ea943d26c5771395a4/nc-1.3.10.itb Uploading file: /tmp/nc.itb Defaulting container name to cray-ipxe. Successfully uploaded /tmp/nc.itb! removed /tmp/nc.itb NodeBMC recovery image upload complete ======================================== Attempting to retrieve RouterBMC .itb file s3:/fw-update/d85398f2eecc11ea94ff26c5771395a4/rec-1.3.10.itb d85398f2eecc11ea94ff26c5771395a4/rec-1.3.10.itb Uploading file: /tmp/rec.itb Defaulting container name to cray-ipxe. Successfully uploaded /tmp/rec.itb! removed /tmp/rec.itb RouterBMC recovery image upload complete "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/configuration_management/",
	"title": "Configuration Management",
	"tags": [],
	"description": "",
	"content": "Configuration Management The Configuration Framework Service (CFS) is available on systems for remote execution and configuration management of nodes and boot images. This includes nodes available in the Hardware State Manager (HSM) inventory (compute, management, and application nodes), and boot images hosted by the Image Management Service (IMS).\nCFS configures nodes and images via a gitops methodology. All configuration content is stored in a version control service (VCS), and is managed by authorized system administrators. CFS provides a scalable Ansible Execution Environment (AEE) for the configuration to be applied with flexible inventory and node targeting options.\nUse Cases CFS is available for the following use cases on systems:\nImage customization: Pre-configure bootable images available via IMS. This use case enables partitioning a full configuration of a target node. Non-node-specific settings are applied pre-boot, which reduces the amount of configuration required after a node boots, and therefore reduces the bring-up time for nodes. Post-boot configuration: Fully configure or reconfigure booted nodes in a scalable, performant way to add the required settings. \u0026ldquo;Push-based\u0026rdquo; deployment: When using post-boot configuration with only node-specific configuration data, the target undergoes node personalization. The two-step process of pre-boot image customization and post-boot node personalization results in a fully configured node, optimized for minimal bring-up times. \u0026ldquo;Pull-based\u0026rdquo; deployment: Provide configuration management to nodes by prescribing a desired configuration state and ensuring the current node configuration state matches the desired state automatically. This is achieved via the CFS Hardware Synchronization Agent and the CFS Batcher implementation. CFS Components CFS is comprised of a group of services and components interacting within the Cray System Management (CSM) service mesh, and provides a means for system administrators to configure nodes and boot images via Ansible. CFS includes the following components:\nA REST API service. A command-line interface (CLI) to the API (via the cray cfs command). Pre-packaged AEE(s) with values tuned for performant configuration for executing Ansible playbooks, and reporting plug-ins for communication with CFS. The CFS Hardware Sync Agent, which pulls in node information from the system inventory to the CFS database to track the node configuration state. The CFS Batcher, which manages the configuration state of system components (nodes). Although it is not a formal part of the service, CFS integrates with a Gitea instance (VCS) running in the CSM service mesh for management of the configuration content life-cycle.\nHigh-Level Configuration Workflow CFS remotely executes Ansible configuration content on nodes or boot images with the following workflow:\nCreating a configuration with one or more layers within a specific Git repository, and committing it to be executed by Ansible. Targeting a node, boot image, or group of nodes to apply the configuration. Creating a configuration session to apply and track the status of Ansible, applying each configuration layer to the targets specified in the session metadata. Additionally, configuration management of specific components (nodes) can also be achieved by doing the following:\nCreating a configuration with one or more layers within a specific Git repository, and committing it to be executed by Ansible. Setting the desired configuration state of a node to the prescribed layers. Enabling the CFS Batcher to automatically configure nodes by creating one or more configuration sessions to apply the configuration layer(s). "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_duplicate_address_warnings_and_declined_dhcp_offers_in_logs/",
	"title": "Compute Node Boot Issue Symptom Duplicate Address Warnings and Declined DHCP Offers in Logs",
	"tags": [],
	"description": "",
	"content": "Compute Node Boot Issue Symptom: Duplicate Address Warnings and Declined DHCP Offers in Logs If the DHCP and node logs show duplicate address warnings and indicate declined DHCP offers, it may be because another component owns the IP address that DHCP is trying to assign to a node. If this happens, the node will not accept the IP address and will repeatedly submit a DHCP discover request. As a result, the node and DHCP become entangled in a loop of requesting and rejecting. This often happens when DHCP is statically assigning IP addresses to nodes, but the assigned IP address for a node has already been assigned to another component.\nSymptoms This scenario results in node logs similar to the following:\n[ 97.946332] dracut Warning: Duplicate address detected for 10.100.160.195 while doing dhcp. retrying [ 97.789015] dracut-initqueue[604]: dracut Warning: Duplicate address detected for 10.100.160.195 while doing dhcp. \\ retrying [ 108.007243] dracut Warning: Duplicate address detected for 10.100.160.195 while doing dhcp. retrying [ 107.873650] dracut-initqueue[604]: dracut Warning: Duplicate address detected for 10.100.160.195 while doing dhcp. \\ retrying [ 110.082877] dracut Warning: Duplicate address detected for 10.100.160.195 while doing dhcp. retrying And DHCP logs similar to the following:\nAbandoning IP address 10.100.160.195: declined. DHCPDECLINE of 10.100.160.195 from a4:bf:01:2e:81:4c (undefined) via eth0: abandoned DHCPOFFER on 10.100.160.195 to \u0026#34;\u0026#34; (undefined) via eth0 DHCPREQUEST for 10.100.160.195 (10.100.160.2) from a4:bf:01:29:92:be via eth0: unknown lease 10.100.160.195. DHCPREQUEST for 10.100.160.195 (10.100.160.2) from a4:bf:01:29:92:eb via eth0: unknown lease 10.100.160.195. DHCPOFFER on 10.100.160.195 to a4:bf:01:2e:81:4c via eth0 DHCPREQUEST for 10.100.160.195 (10.100.160.2) from a4:bf:01:2e:81:4c via eth0 DHCPACK on 10.100.160.195 to a4:bf:01:2e:81:4c via eth0 Abandoning IP address 10.100.160.195: declined. Notice that two different components (identifiable by the two different MAC addresses a4:bf:01:29:92:eb and a4:bf:01:2e:81:4c) have made DHCP requests for the IP address 10.100.160.195.\na4:bf:01:29:92:eb is the component that owns the IP address 10.100.160.195, while a4:bf:01:2e:81:4c has been statically assigned the IP address 10.100.160.195 in the DHCP configuration file. As such, DHCP keeps trying to assign it that address, but after being offered the address, a4:bf:01:2e:81:4c declines it because it realizes that a4:bf:01:29:92:eb already owns it.\nProblem Detection There are multiple ways to check if this problem exists:\nPing the IP address and see if another component responds. Log into the component and determine its IP address. If it is the same as the IP address that DHCP is attempting to assign, then this issue does exist.\nCheck the Address Resolution Protocol (ARP) cache using the arp command. Because it is a cache, it is possible that IP addresses can age out of the cache, so the IP address may not be present. If the address that is failing to be assigned is in the ARP cache, and it is assigned to a node with a different MAC address, then that is confirmation that this problem has occurred.\nncn-m001# arp Example output:\nAddress HWtype HWaddress Flags Mask Iface ncn-w002.local ether 98:03:9b:b4:f1:fe C bond0.nmn0 10.46.11.201 ether ca:d3:dc:33:29:e7 C weave 10.46.12.7 ether 7e:7e:7f:f0:0d:2d C weave 10.46.11.197 ether 62:4c:91:91:ec:9f C weave 10.46.11.193 ether 52:dd:02:01:34:ab C weave 10.32.0.5 ether ba:ff:65:af:a7:4e C weave 10.46.11.191 ether be:36:79:07:84:08 C weave 10.45.1.121 ether fe:93:50:63:9a:fd C weave 10.46.11.187 ether e6:2e:8c:ed:f8:78 C weave 10.46.11.250 ether c6:73:6d:c4:b9:77 C weave 10.48.15.0 ether da:c2:40:ed:f4:ec CM flannel.2 [...] Resolution Force the component that has been assigned an incorrect IP address to request another one. This may involve powering that component down and then back up.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/create_uais_from_specific_uai_images_in_legacy_mode/",
	"title": "Create UAIs From Specific UAI Images in Legacy Mode",
	"tags": [],
	"description": "",
	"content": "Create UAIs From Specific UAI Images in Legacy Mode A user can create a UAI from a specific UAI image (assuming no default UAI class exists) using a command of the form:\nuser\u0026gt; cray uas create --publickey \u0026lt;path\u0026gt; --imagename \u0026lt;image-name\u0026gt; \u0026lt;image-name\u0026gt; is the name shown above in the list of UAI images.\nFor example:\nvers\u0026gt; cray uas images list default_image = \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34; image_list = [ \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34;, \u0026#34;registry.local/cray/cray-uai-sanity-test:1.2.4\u0026#34;, \u0026#34;registry.local/cray/cray-uai-broker:1.2.4\u0026#34;,] vers\u0026gt; cray uas create --publickey ~/.ssh/id_rsa.pub --imagename registry.local/cray/cray-uai-sles15sp2:1.2.4 uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@34.136.140.107\u0026#34; uai_host = \u0026#34;ncn-w003\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34; uai_ip = \u0026#34;34.136.140.107\u0026#34; uai_msg = \u0026#34;ContainerCreating\u0026#34; uai_name = \u0026#34;uai-vers-1ad83473\u0026#34; uai_status = \u0026#34;Waiting\u0026#34; username = \u0026#34;vers\u0026#34; [uai_portmap] Top: User Access Service (UAS)\nNext Topic: UAS and UAI Legacy Mode Health Checks\n"
},
{
	"uri": "/docs-csm/en-12/operations/csm_product_management/remove_artifacts_from_product_installations/",
	"title": "Remove Artifacts from Product Installations",
	"tags": [],
	"description": "",
	"content": "Remove Artifacts from Product Installations Remove product artifacts that were imported from various Cray products. These instructions provide guidance for removing Image Management Service (IMS) images, IMS recipes, and Git repositories present in the Cray Product Catalog from the system.\nThe examples in this procedure show how to remove the product artifacts for the Cray System Management (CSM) product.\nWARNING: If individual Cray products have removal procedures, those instructions supersede this procedure.\nProcedure View the imported artifacts by printing them from the Cray Product Catalog ConfigMap.\nncn-mw# kubectl get cm cray-product-catalog -n services -o json | jq -r .data.csm Example output:\n1.0.0: configuration: clone_url: https://vcs.cmn.SYSTEM_DOMAIN_NAME/vcs/cray/csm-config-management.git commit: 123264ba75c809c0db7742ea83ff57f713bc1562 import_branch: cray/csm/1.4.5 import_date: 2021-03-12 15:12:49.938936 ssh_url: git@vcs.cmn.SYSTEM_DOMAIN_NAME:cray/csm-config-management.git images: cray-shasta-csm-sles15sp1-barebones.x86_64-shasta-1.4: id: 4871cb4a-e055-4131-a228-c0a26f0903cd recipes: cray-shasta-csm-sles15sp1-barebones.x86_64-shasta-1.4: id: 5f5a74e0-108e-4159-9699-47dd2a952205 Remove the imported IMS images using the ID of each image in the images mapping.\nThe example in step 1 includes one image with the id: 4871cb4a-e055-4131-a228-c0a26f0903cd value. Remove the image with the following command:\nncn-mw# cray ims images delete 4871cb4a-e055-4131-a228-c0a26f0903cd Remove the imported IMS recipes using the ID of each recipe in the recipes mapping.\nThe example in step 1 includes one recipe with the id: 5f5a74e0-108e-4159-9699-47dd2a952205 value. Remove the image with the following command:\nncn-mw# cray ims recipes delete 5f5a74e0-108e-4159-9699-47dd2a952205 Remove the Gitea repositories or branches.\nTo delete a Git branch as specified in the product catalog, follow the external instructions to delete Git remote branches. The branch name is located in the import_branch field.\nIf only one version of the product exists (as in the CSM example), the user can remove the entire repository instead of a single branch. Gitea repositories can be removed via the Gitea web interface or via the Gitea REST API.\nGitea web interface\nLog in to Gitea as the crayvcs user. From the dashboard, select the repository to delete based on the name of the repository in the clone_url field of the product catalog. Click on Settings and scroll to the bottom of the page to the Danger Zone section. Follow the instructions to delete the repository. Gitea REST API\nRun the following commands, replacing the name of the repository in the second command.\nncn-mw# VCSPWD=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode) ncn-mw# curl -X DELETE -u crayvcs:${VCSPWD} https://api-gw-service-nmn.local/vcs/api/v1/repos/cray/{name of repository} Update the product catalog.\nOnce the images, recipes, and repositories/branches have been removed from the system, update the product catalog to remove the references to them. This is done by editing the cray-product-catalog Kubernetes ConfigMap in the services namespace.\nncn-mw# kubectl edit configmap -n services cray-product-catalog In the editor, delete the entries for the artifacts that were deleted on the system for the specific version of the product. In this example, all artifacts were deleted and only a single product version exists, so the entire entry in the product catalog for the CSM product can be deleted. Save the changes and exit the editor to persist the changes in the ConfigMap.\n"
},
{
	"uri": "/docs-csm/en-12/install/collecting_ncn_mac_addresses/",
	"title": "Collecting NCN MAC Addresses",
	"tags": [],
	"description": "",
	"content": "Collecting NCN MAC Addresses This procedure details how to collect the NCN MAC addresses from an HPE Cray EX system. The MAC addresses needed for the Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1 columns in ncn_metadata.csv will be collected. This data will feed into the cloud-init metadata.\nThe Bootstrap MAC address will be used for identification of this node during the early part of the PXE boot process, before the bonded interface can be established.\nBond0 MAC0 and Bond0 MAC1 are the MAC addresses for the physical interfaces that the node will use for the various VLANs. Bond0 MAC0 and Bond0 MAC1 should be on different network cards in order to establish redundancy for a failed network card. On the other hand, this is not an absolute requirement. If the node has only a single network card, then this will force MAC1 and MAC0 to reside on the same physical card; while not optimal, this will still produce a valid configuration.\nSections Procedure: iPXE consoles Requirements MAC address collection Procedure: Serial consoles Procedure: Recovering from an incorrect ncn_metadata.csv file The easy way to do this leverages the NIC dump provided by the metal-ipxe package on the LiveCD. This option is outlined in Procedure: iPXE consoles.\nThe alternative is to use serial cables (or SSH) to collect the MAC addresses from the switch ARP tables. This can become exponentially difficult for large systems, and is not recommended. This option is outlined in Procedure: Serial consoles.\nProcedure: iPXE consoles This procedure is faster for those with the LiveCD (CRAY Pre-Install Toolkit). It can be used to quickly boot-check nodes to dump network device information without an operating system. This works by accessing the PCI configuration space.\nRequirements If CSI does not work because of a file requirement, then file a ticket. By default, dnsmasq and ConMan are already running on the LiveCD, but bond0 needs to be configured. dnsmasq needs to serve/listen over bond0, and ConMan needs the BMC information.\nLiveCD dnsmasq is configured for the bond0/metal network (NMN/HMN/CAN do not matter) BMC MAC addresses already collected LiveCD ConMan is configured for each BMC For help with either of those, see LiveCD Setup.\nMAC address collection (Optional) Modify the boot so that nodes stop network booting after dumping their network devices.\nNOTE Removing the iPXE script will prevent network booting. Be aware that the nodes may still disk boot.\npit# mv /var/www/boot/script.ipxe /var/www/boot/script.ipxe.bak Verify that consoles are active with conman -q.\nThe following command lists all nodes that ConMan is configured to monitor.\npit# conman -q ncn-m002-mgmt ncn-m003-mgmt ncn-s001-mgmt ncn-s002-mgmt ncn-s003-mgmt ncn-w001-mgmt ncn-w002-mgmt ncn-w003-mgmt Set the nodes to PXE boot and restart them.\nRecord the username for the NCN BMCs.\npit# USERNAME=root Record the password for this user.\nread -s is used in order to prevent the credentials from being displayed on the screen or recorded in the shell history.\npit# read -r -s -p \u0026#34;NCN BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD Set the nodes to PXE boot and restart them.\npit# export IPMI_PASSWORD pit# grep -oP \u0026#34;(${mtoken}|${stoken}|${wtoken})\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H {} chassis bootdev pxe options=persistent pit# grep -oP \u0026#34;(${mtoken}|${stoken}|${wtoken})\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H {} chassis bootdev pxe options=efiboot pit# grep -oP \u0026#34;(${mtoken}|${stoken}|${wtoken})\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H {} power off pit# sleep 10 pit# grep -oP \u0026#34;(${mtoken}|${stoken}|${wtoken})\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H {} power on Wait for the nodes to network boot.\nThis can be monitored using ConMan; the -m option follows the console output in read-only mode, and the -j option joins an interactive console session. The available node names were listed in step 2 above. The boot usually starts in less than 3 minutes. After that, log data should start flowing through ConMan; the speed depends on how quickly the nodes POST. To see a ConMan help screen for all supported escape sequences, use \u0026amp;?.\npit# conman -m ncn-m002-mgmt Exit ConMan.\nThis is done by typing \u0026amp;. (that is, press and release \u0026amp;, then press and release .).\nPrint off what has been found in the console logs.\nThis snippet will omit duplicates from multiple boot attempts:\npit# for file in /var/log/conman/*; do echo ${file} grep -Eoh \u0026#39;(net[0-9] MAC .*)\u0026#39; \u0026#34;${file}\u0026#34; | sort -u | grep PCI \u0026amp;\u0026amp; echo ----- done Use the output from the previous step to collect two MAC addresses to use for bond0, and two more to use for bond1, based on the topology.\nThe Bond0 MAC0 must be the first port of the first PCIe card, specifically the port connecting the NCN to the lower spine. For example, if connected to spines01 and spines02, this is going to sw-spine-001. If connected to sw-spine-007 and sw-spine-008, then this is sw-spine-007.\nThe 2nd MAC for bond0 is the first port of the 2nd PCIe card, or 2nd port of the first when only one card exists.\nUse the table provided on NCN Networking for referencing commonly seen devices.\nWorker nodes also have the high-speed network cards. If these cards are known, filter their device IDs out from the above output using this snippet:\npit# unset did # clear it if set pit# did=1017 # ConnectX-5 example. pit# for file in /var/log/conman/*; do echo ${file} grep -Eoh \u0026#39;(net[0-9] MAC .*)\u0026#39; \u0026#34;${file}\u0026#34; | sort -u | grep PCI | grep -Ev \u0026#34;${did}\u0026#34; \u0026amp;\u0026amp; echo ----- done To filter out onboard NICs, or site-link cards, omit their device IDs as well. Use the above snippet but add the other IDs.\nThis snippet prints out only mgmt MAC addresses; the did is the HSN and onboard NICs that are being ignored.\npit# unset did # clear it if set pit# did=\u0026#39;(1017|8086|ffff)\u0026#39; pit# for file in /var/log/conman/*; do echo ${file} grep -Eoh \u0026#39;(net[0-9] MAC .*)\u0026#39; \u0026#34;${file}\u0026#34; | sort -u | grep PCI | grep -Ev \u0026#34;${did}\u0026#34; \u0026amp;\u0026amp; echo ----- done Examine the output to identify Bond0 MAC addresses for each NCN.\nUse the lowest value MAC address per PCIe card.\nExample: One PCIe card with two ports, for a total of two ports per node.\n----- /var/log/conman/console.ncn-w003-mgmt net2 MAC b8:59:9f:d9:9e:2c PCI.DeviceID 1013 PCI.VendorID 15b3 \u0026lt;-bond0-mac0 (0x2c \u0026lt; 0x2d) net3 MAC b8:59:9f:d9:9e:2d PCI.DeviceID 1013 PCI.VendorID 15b3 \u0026lt;-bond0-mac1 ----- The above output identifies MAC0 and MAC1 of the bond as b8:59:9f:d9:9e:2c and b8:59:9f:d9:9e:2d, respectively.\nExample: Two PCIe cards with two ports each, for a total of four ports per node.\n----- /var/log/conman/console.ncn-w006-mgmt net0 MAC 94:40:c9:5f:b5:df PCI.DeviceID 8070 PCI.VendorID 1077 \u0026lt;-bond0-mac0 (0xdf \u0026lt; 0xe0) net1 MAC 94:40:c9:5f:b5:e0 PCI.DeviceID 8070 PCI.VendorID 1077 (future use) net2 MAC 14:02:ec:da:b9:98 PCI.DeviceID 8070 PCI.VendorID 1077 \u0026lt;-bond0-mac1 (0x98 \u0026lt; 0x99) net3 MAC 14:02:ec:da:b9:99 PCI.DeviceID 8070 PCI.VendorID 1077 (future use) ----- The above output identifies MAC0 and MAC1 of the bond as 94:40:c9:5f:b5:df and 14:02:ec:da:b9:99, respectively.\nCollect the NCN MAC address for the PIT node.\nThis information will be used to populate the MAC addresses for ncn-m001.\npit# grep -i perm /proc/net/bonding/bond0 For example:\nPermanent HW addr: b8:59:9f:c7:12:f2 \u0026lt;-bond0-mac0 Permanent HW addr: b8:59:9f:c7:12:f3 \u0026lt;-bond0-mac1 Update ncn_metadata.csv with the collected MAC addresses for Bond0 from all of the management NCNs.\nNOTE: Each type of NCN (master, storage, and worker) are grouped together in the file and are listed in descending numerical order within their group (for example, ncn-s003 is listed directly before ncn-s002).\nFor each NCN, update the corresponding row in ncn_metadata with the values for Bond0 MAC0 and Bond0 MAC1. For Bootstrap MAC, copy the value from Bond0 MAC0.\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Worker,94:40:c9:37:77:26,b8:59:9f:c7:12:f2,b8:59:9f:c7:12:f2,b8:59:9f:c7:12:f3 ^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^ bond0-mac0 bond0-mac0 bond0-mac1 Power off the NCNs.\npit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power off If the script.ipxe file was renamed in the first step of this procedure, then restore it to its original location.\npit# mv /var/www/boot/script.ipxe.bak /var/www/boot/script.ipxe Procedure: Serial consoles Pick out the MAC addresses for the Bond from both the sw-spine-001 and sw-spine-002 switches, following the Collecting BMC MAC Addresses procedure.\nNOTE: The node must be booted into an operating system in order for the Bond MAC addresses to appear on the spine switches.\nA PCIe card with dual-heads may go to either spine switch, meaning MAC0 must be collected from spine-01. Refer to the cabling diagram or the actual rack (in-person).\nFollow Metadata BMC on each spine switch that port1 and port2 of the bond are plugged into.\nUsually the 2nd/3rd/4th/Nth MAC address on the PCIe card will be a 0x1 or 0x2 deviation from the first port.\nCollection is quicker if this can be easily confirmed.\nProcedure: Recovering from an incorrect ncn_metadata.csv file If the ncn_metadata.csv file is incorrect, the NCNs will be unable to deploy. This section details a recovery procedure in case that happens.\nRemove the incorrectly generated configurations.\nBefore deleting the incorrectly generated configurations, make a backup of them in case they need to be examined at a later time.\nWARNING Ensure that the SYSTEM_NAME environment variable is correctly set. If SYSTEM_NAME is not set, then the command below could potentially remove the entire prep directory.\npit# export SYSTEM_NAME=eniac pit# rm -rvf /var/www/ephemeral/prep/$SYSTEM_NAME Manually edit ncn_metadata.csv, replacing the bootstrap MAC address with Bond0 MAC0 address for the afflicted nodes that failed to boot.\nRe-run csi config init with the required flags.\nCopy all of the newly generated files into place.\npit# cp -pv /var/www/ephemeral/prep/$SYSTEM_NAME/dnsmasq.d/* /etc/dnsmasq.d/ \u0026amp;\u0026amp; cp -pv /var/www/ephemeral/prep/$SYSTEM_NAME/basecamp/* /var/www/ephemeral/configs/ \u0026amp;\u0026amp; cp -pv /var/www/ephemeral/prep/$SYSTEM_NAME/conman.conf /etc/ \u0026amp;\u0026amp; cp -pv /var/www/ephemeral/prep/$SYSTEM_NAME/pit-files/* /etc/sysconfig/network/ Update the CA certificates on the copied data.json file. Provide the path to the data.json file, the path to the customizations.yaml file, and the sealed_secrets.key file.\npit# csi patch ca \\ --cloud-init-seed-file /var/www/ephemeral/configs/data.json \\ --customizations-file /var/www/ephemeral/prep/site-init/customizations.yaml \\ --sealed-secret-key-file /var/www/ephemeral/prep/site-init/certs/sealed_secrets.key Restart everything to apply the new configurations:\npit# wicked ifreload all \u0026amp;\u0026amp; systemctl restart dnsmasq conman basecamp \u0026amp;\u0026amp; systemctl restart nexus Ensure system-specific settings generated by CSI are merged into customizations.yaml:\nThe yq tool used in the following procedures is available under /var/www/ephemeral/prep/site-init/utils/bin once the SHASTA-CFG repo has been cloned.\npit# alias yq=\u0026#34;/var/www/ephemeral/prep/site-init/utils/bin/$(uname | awk \u0026#39;{print tolower($0)}\u0026#39;)/yq\u0026#34; pit# yq merge -xP -i /var/www/ephemeral/prep/site-init/customizations.yaml \u0026lt;(yq prefix -P \u0026#34;/var/www/ephemeral/prep/${SYSTEM_NAME}/customizations.yaml\u0026#34; spec) Wipe the disks before relaunching the NCNs.\nSee full wipe from Wipe NCN Disks for Reinstallation.\nSet BMCs to DHCP, if needed.\nFor any NCNs which booted far enough to begin running cloud-init, set their BMCs to DHCP. If in doubt, it does no harm to perform this step. See Set node BMCs to DHCP.\n"
},
{
	"uri": "/docs-csm/en-12/background/ncn_packages/",
	"title": "NCN Packages",
	"tags": [],
	"description": "",
	"content": "NCN Packages The management nodes boot from images which have many (RPM) packages installed. Lists of the packages for these images are generated on running nodes. A list of images can be collected by running a zypper command on one of the storage, worker, or master nodes.\nKubernetes Images The Kubernetes image is used to boot the master nodes and worker nodes.\nCollection ncn-w002# zypper --disable-repositories se --installed-only | grep i+ | tr -d \u0026#39;|\u0026#39; | awk \u0026#39;{print $2}\u0026#39; The List SLE_HPC SLE_HPC-release acpid apparmor-profiles arptables at autoyast2 autoyast2-installation base bc bind-utils biosdevname ceph-common cfs-state-reporter cloud-init conntrack-tools cpupower crash cray-cos-release cray-cps-utils cray-dvs-kmp-default cray-dvs-service cray-heartbeat cray-lustre-client cray-lustre-client-devel cray-lustre-client-kmp-default cray-network-config cray-node-identity cray-orca cray-power-button cray-sat-podman craycli-wrapper createrepo_c dnsmasq dosfstools dracut-kiwi-live dracut-metal-mdsquash dump e2fsprogs ebtables emacs ethtool expect fping gdb genders git-core glibc gnuplot gperftools gptfdisk grub2 haproxy hpe-csm-scripts hplip ipmitool iproute2-bash-completion ipset ipvsadm irqbalance java-1_8_0-ibm java-1_8_0-ibm-plugin kdump kdumpid keepalived kernel-mft-mlnx-kmp-default kernel-source kernel-syms kexec-tools kmod-bash-completion kubeadm kubectl kubelet less libcanberra-gtk0 libecpg6 libpq5 libunwind-devel ltrace lvm2 man mariadb mariadb-tools mdadm minicom mlocate nmap numactl open-lldp patterns-base-base pdsh perf perl-MailTools perl-doc pixz podman podman-cni-config postgresql postgresql-contrib postgresql-docs postgresql-server python python-urlgrabber python3-Jinja2 python3-MarkupSafe python3-click python3-colorama python3-curses python3-ipaddr python3-lxml python3-netaddr python3-pexpect python3-pip python3-rpm python3-sip rarpd rasdaemon rsync rsyslog rzsz screen slingshot-network-config smartmontools socat spire-agent squashfs squid strace sudo sysstat systemtap tar tcpdump unixODBC usbutils vim wget wireshark xfsdump yast2-add-on yast2-bootloader yast2-country yast2-network yast2-services-manager yast2-users yum-metadata-parser zip CEPH The Ceph image is used to boot the utility storage nodes.\nCollection ncn-s002# zypper --disable-repositories se --installed-only | grep i+ | tr -d \u0026#39;|\u0026#39; | awk \u0026#39;{print $2}\u0026#39; The List SLE_HPC SLE_HPC-release acpid apparmor-profiles arptables at autoyast2 autoyast2-installation base bc bind-utils biosdevname ceph-mds ceph-mgr ceph-mon ceph-osd ceph-radosgw cfs-state-reporter cloud-init cpupower crash cray-cos-release cray-dvs-kmp-default cray-dvs-service cray-heartbeat cray-lustre-client cray-lustre-client-devel cray-lustre-client-kmp-default cray-network-config cray-node-identity cray-power-button craycli-wrapper createrepo_c dnsmasq dosfstools dracut-kiwi-live dracut-metal-mdsquash dump e2fsprogs ebtables emacs ethtool expect fping gdb genders git-core glibc gnuplot gperftools gptfdisk grub2 haproxy hpe-csm-scripts hplip ipmitool iproute2-bash-completion ipset irqbalance java-1_8_0-ibm java-1_8_0-ibm-plugin kdump kdumpid keepalived kernel-mft-mlnx-kmp-default kernel-source kernel-syms kexec-tools kmod-bash-completion kubectl less libcanberra-gtk0 libecpg6 libpq5 libunwind-devel ltrace lvm2 man mariadb mariadb-tools mdadm minicom mlocate netcat-openbsd nmap numactl open-lldp patterns-base-base pdsh perf perl-MailTools perl-doc pixz podman podman-cni-config postgresql postgresql-contrib postgresql-docs postgresql-server python python-urlgrabber python3-Jinja2 python3-MarkupSafe python3-boto3 python3-click python3-colorama python3-curses python3-ipaddr python3-lxml python3-netaddr python3-pexpect python3-pip python3-rpm python3-sip rarpd rasdaemon rsync rsyslog rzsz screen slingshot-network-config smartmontools socat spire-agent squashfs squid strace sudo sysstat systemtap tar tcpdump unixODBC usbutils vim wget wireshark xfsdump yast2-add-on yast2-bootloader yast2-country yast2-network yast2-services-manager yast2-users yum-metadata-parser zip "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/etcd_cluster_backup_timeout/",
	"title": "Etcd Cluster Backup Fails Due to Timeout",
	"tags": [],
	"description": "",
	"content": "Etcd Cluster Backup Fails Due to Timeout Description There is a known issue where an Etcd cluster backup will fails if it takes longer than 1 minute to complete.\nSymptoms An Etcd cluster backup was not created in the last 24 hours. The etcdbackup status contains MultipartUpload: upload multipart failed. Check for recent Etcd cluster backups listed in the etcd-backup S3 bucket. ncn-mw# /opt/cray/platform-utils/s3/list-objects.py --bucket-name etcd-backup | grep -v bare-metal Example output:\ncray-bos/etcd.backup_v273436_2022-09-12-13:55:20 cray-bos/etcd.backup_v273436_2022-09-13-13:55:20 cray-bos/etcd.backup_v276316_2022-09-14-13:55:20 cray-bos/etcd.backup_v279196_2022-09-15-13:55:20 cray-bos/etcd.backup_v282076_2022-09-16-13:55:20 cray-bos/etcd.backup_v545936_2022-09-17-13:55:20 cray-bos/etcd.backup_v562933_2022-09-18-13:55:20 \u0026lt;---- Missing backups for 2022-09-19 and 2022-09-20 cray-bss/etcd.backup_v452224_2022-09-20-13:54:09 cray-bss/etcd.backup_v458007_2022-09-19-13:54:09 . . . If the latest backup listed in the etcd-backup S3 bucket for a given Etcd cluster is older than 24 hours, then check the status of the etcdbackup resource. This example is checking cray-bos etcdbackup resource:\nncn-mw# kubectl describe etcdbackup cray-bos-etcd-cluster-periodic-backup -n services | grep -A8 \u0026#34;Status\u0026#34;: Example output:\nStatus: Reason: failed to save snapshot (failed to write snapshot (MultipartUpload: upload multipart failed upload id: 2~V6e_CehW2ULDNNmAgL01mkt2zObm4pg caused by: RequestCanceled: request context canceled caused by: context deadline exceeded)) Last Execution Date: 2022-09-20T13:58:04Z Last Success Date: 2022-09-18-13:55:20Z Succeeded: false Events: \u0026lt;none\u0026gt; Solution Add a backupPolicy.timeoutInSecond of 600 to the etcdbackup resource to allow the backup to take up to 10 minutes to complete.\nPatch the etcdbackup resource.\nThis example patches the cray-bos etcdbackup resource.\nncn-mw# kubectl patch etcdbackup cray-bos-etcd-cluster-periodic-backup -n services --type=merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;backupPolicy\u0026#34;:{\u0026#34;timeoutInSecond\u0026#34;: 600}}}\u0026#39; Example output:\netcdbackup.etcd.database.coreos.com/cray-bos-etcd-cluster-periodic-backup patched Verify that backups can now be successfully created. Temporarily set the backupIntervalInSecond to force a backup every minute. This example patches the cray-bos etcdbackup resource:\nncn-mw# INTERVAL=$(kubectl get etcdbackups cray-bos-etcd-cluster-periodic-backup -n services -o json | jq -r \u0026#39;.spec.backupPolicy.backupIntervalInSecond\u0026#39;) ncn-mw# TMPINTERVAL=60 ncn-mw# kubectl patch etcdbackup cray-bos-etcd-cluster-periodic-backup -n services --type=json \\ -p=\u0026#34;[{\u0026#39;op\u0026#39; : \u0026#39;replace\u0026#39;, \u0026#39;path\u0026#39;:\u0026#39;/spec/backupPolicy/backupIntervalInSecond\u0026#39;, \u0026#39;value\u0026#39; : \\\u0026#34;$TMPINTERVAL\\\u0026#34; }]\u0026#34; Example output:\netcdbackup.etcd.database.coreos.com/cray-bos-etcd-cluster-periodic-backup patched Re-check the list of Etcd cluster backups. It will take a few minutes for the new backup to show in the list.\nncn-mw# /opt/cray/platform-utils/s3/list-objects.py --bucket-name etcd-backup | grep -v bare-metal Example output:\ncray-bos/etcd.backup_v276316_2022-09-13-13:55:20 cray-bos/etcd.backup_v276316_2022-09-14-13:55:20 cray-bos/etcd.backup_v279196_2022-09-15-13:55:20 cray-bos/etcd.backup_v282076_2022-09-16-13:55:20 cray-bos/etcd.backup_v545936_2022-09-17-13:55:20 cray-bos/etcd.backup_v562933_2022-09-18-13:55:20 cray-bos/etcd.backup_v569459_2022-09-21-07:36:15 \u0026lt;---- A new backup exists for cray-bos Etcd cluster cray-bss/etcd.backup_v452224_2022-09-20-13:54:09 cray-bss/etcd.backup_v458007_2022-09-19-13:54:09 . . . Reset the backupIntervalInSecond to the original value so backups are not running every minute. ncn-mw# kubectl patch etcdbackup cray-bos-etcd-cluster-periodic-backup -n services --type=json \\ -p=\u0026#34;[{\u0026#39;op\u0026#39; : \u0026#39;replace\u0026#39;, \u0026#39;path\u0026#39;:\u0026#39;/spec/backupPolicy/backupIntervalInSecond\u0026#39;, \u0026#39;value\u0026#39; : \\\u0026#34;$INTERVAL\\\u0026#34; }]\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/ceph_storage_types/",
	"title": "Ceph Storage Types",
	"tags": [],
	"description": "",
	"content": "Ceph Storage Types As a reference, the following ceph and rbd commands are run from a master node or ncn-s001/2/3. Certain commands will work on different systems. For example, the rbd command can be used on the worker nodes if specifying the proper key.\nCeph Block (rbd) List block devices in a specific pool:\nncn-m001# rbd -p POOL_NAME ls -l Example output:\nNAME SIZE PARENT FMT PROT LOCK kube_vol 4 GiB 2 Create a block device:\nncn-m001# rbd create -p POOL_NAME VOLUME_NAME -size SIZE Remove a block device:\nncn-m001# rbd -p POOL_NAME remove VOLUME_NAME Show mapped devices:\nncn-m001# rbd showmapped Example output:\nid pool namespace image snap device 0 test test_vol - /dev/rbd0 1 kube kube_vol - /dev/rbd1 2 smf smf_vol - /dev/rbd2 Ceph MDS (File) Display CephFS shares with their pool information:\nncn-m001# ceph fs ls Example output:\nname: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ] Show the status of all CephFS components:\nncn-m001# ceph fs status Example output:\ncephfs - 0 clients \u0026lt;\u0026lt;-- Containers or hosts attached to cephfs are represented here ====== +------+--------+-----------+---------------+-------+-------+ | Rank | State | MDS | Activity | dns | inos | +------+--------+-----------+---------------+-------+-------+ | 0 | active | ceph-2 | Reqs: 0 /s | 10 | 13 | \u0026lt;\u0026lt;-- Active server +------+--------+-----------+---------------+-------+-------+ +-----------------+----------+-------+-------+ | Pool | type | used | avail | +-----------------+----------+-------+-------+ | cephfs_metadata | metadata | 1536k | 13.1G | | cephfs_data | data | 0 | 13.1G | \u0026lt;\u0026lt;-- Where files get stored +-----------------+----------+-------+-------+ +-------------+ | Standby MDS | +-------------+ | ceph-1 | | ceph-3 | +-------------+ MDS version: ceph version 14.2.0-300-gacd2f2b9e1 (acd2f2b9e196222b0350b3b59af9981f91706c7f) nautilus (stable) Ceph RadosGW (object/s3) List the services to learn more about the radosgw service:\nThe following command lists more than just the radosgw service, so ensure the correct sections are used.\nncn-m001# ceph service dump Example output:\n{ \u0026#34;epoch\u0026#34;: 2, \u0026#34;modified\u0026#34;: \u0026#34;2019-08-11 04:37:31.464120\u0026#34;, \u0026#34;services\u0026#34;: { \u0026#34;rgw\u0026#34;: { \u0026lt;\u0026lt;-- Note this section \u0026#34;daemons\u0026#34;: { \u0026#34;summary\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;\u0026lt;hostname redacted\u0026gt;.rgw0\u0026#34;: { \u0026#34;start_epoch\u0026#34;: 2, \u0026#34;start_stamp\u0026#34;: \u0026#34;2019-08-11 04:37:31.454975\u0026#34;, \u0026#34;gid\u0026#34;: 24609, \u0026#34;addr\u0026#34;: \u0026#34;10.2.0.1:0/3889467377\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;arch\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;ceph_release\u0026#34;: \u0026#34;nautilus\u0026#34;, \u0026#34;ceph_version\u0026#34;: \u0026#34;ceph version 14.2.0-300-gacd2f2b9e1 (acd2f2b9e196222b0350b3b59af9981f91706c7f) nautilus (stable)\u0026#34;, \u0026#34;ceph_version_short\u0026#34;: \u0026#34;14.2.0-300-gacd2f2b9e1\u0026#34;, \u0026#34;cpu\u0026#34;: \u0026#34;Intel(R) Xeon(R) Platinum 8176 CPU @ 2.10GHz\u0026#34;, \u0026#34;distro\u0026#34;: \u0026#34;sles\u0026#34;, \u0026#34;distro_description\u0026#34;: \u0026#34;SUSE Linux Enterprise Server 15\u0026#34;, \u0026#34;distro_version\u0026#34;: \u0026#34;15\u0026#34;, \u0026#34;frontend_config#0\u0026#34;: \u0026#34;beast endpoint=\u0026lt;ip address redacted\u0026gt;:8080\u0026#34;, \u0026#34;frontend_type#0\u0026#34;: \u0026#34;beast\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;\u0026lt;hostname redacted\u0026gt;\u0026#34;, \u0026#34;kernel_description\u0026#34;: \u0026#34;#1 SMP Thu Jul 11 11:24:28 UTC 2019 (bf2abc2)\u0026#34;, \u0026#34;kernel_version\u0026#34;: \u0026#34;4.12.14-150.27-default\u0026#34;, \u0026#34;mem_swap_kb\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;mem_total_kb\u0026#34;: \u0026#34;196736052\u0026#34;, \u0026#34;num_handles\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;pid\u0026#34;: \u0026#34;48512\u0026#34;, \u0026#34;zone_id\u0026#34;: \u0026#34;f9b1f6cc-3396-4161-b694-f2d5019b80c6\u0026#34;, \u0026#34;zone_name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;zonegroup_id\u0026#34;: \u0026#34;cea2e773-7e4e-4673-b6fd-91adb76e25f5\u0026#34;, \u0026#34;zonegroup_name\u0026#34;: \u0026#34;default\u0026#34; } }, Edit and view user information:\nThe following command is an example of how to get information about a specific user.\nncn-m001# radosgw-admin user info --uid TEST_USER Example output:\n{ \u0026#34;user_id\u0026#34;: \u0026#34;test_user\u0026#34;, \u0026#34;display_name\u0026#34;: \u0026#34;test_user\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;suspended\u0026#34;: 0, \u0026#34;max_buckets\u0026#34;: 1000, \u0026#34;subusers\u0026#34;: [], \u0026lt;\u0026lt;-- Any users created and maintained by this user \u0026#34;keys\u0026#34;: [ { \u0026#34;user\u0026#34;: \u0026#34;test_user\u0026#34;, \u0026#34;access_key\u0026#34;: \u0026#34;QEA6PG8VDSJ41JR4C6GZ\u0026#34;, \u0026lt;\u0026lt;-- Random key unique to this user and system \u0026#34;secret_key\u0026#34;: \u0026#34;SzNCqWwZ7XlGZ1tdtuVdhLTno48ugthx5YwCF6E8\u0026#34; \u0026lt;\u0026lt;-- Random key unique to this user and system } ], \u0026#34;swift_keys\u0026#34;: [], \u0026#34;caps\u0026#34;: [], \u0026#34;op_mask\u0026#34;: \u0026#34;read, write, delete\u0026#34;, \u0026#34;default_placement\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default_storage_class\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;placement_tags\u0026#34;: [], \u0026#34;bucket_quota\u0026#34;: { \u0026#34;enabled\u0026#34;: false, \u0026#34;check_on_raw\u0026#34;: false, \u0026#34;max_size\u0026#34;: -1, \u0026#34;max_size_kb\u0026#34;: 0, \u0026#34;max_objects\u0026#34;: -1 }, \u0026#34;user_quota\u0026#34;: { \u0026#34;enabled\u0026#34;: false, \u0026#34;check_on_raw\u0026#34;: false, \u0026#34;max_size\u0026#34;: -1, \u0026#34;max_size_kb\u0026#34;: 0, \u0026#34;max_objects\u0026#34;: -1 }, \u0026#34;temp_url_keys\u0026#34;: [], \u0026#34;type\u0026#34;: \u0026#34;rgw\u0026#34;, \u0026#34;mfa_ids\u0026#34;: [] } The radosgw-admin bucket command is used to remove or view buckets.\nTo list the buckets:\nncn-m001# radosgw-admin bucket list To remove a specific bucket:\nncn-m001# radosgw-admin bucket rm --bucket-name BUCKET_NAME "
},
{
	"uri": "/docs-csm/en-12/operations/system_layout_service/update_sls_with_uan_aliases/",
	"title": "Update SLS with UAN Aliases",
	"tags": [],
	"description": "",
	"content": "Update SLS with UAN Aliases This guide shows the process for manually adding an alias to a UAN in SLS and ensuring that the node is being monitored by conman for console logs.\nPrerequisites SLS is up and running and has been populated with data. Access to the API gateway api-gw-service (legacy: api-gw-service-nmn.local) Procedure Authenticate with Keycloak to obtain an API token:\nexport TOKEN=$(curl -k -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; \\ | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Find the component name (xname) of the UAN by searching through all Application nodes until found.\ncurl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ \u0026#34;https://api-gw-service-nmn.local/apis/sls/v1/search/hardware?extra_properties.Role=Application\u0026#34; \\ | jq This will return an array of application nodes currently known in SLS:\nncn-w001# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ \u0026#34;https://api-gw-service-nmn.local/apis/sls/v1/search/hardware?extra_properties.Role=Application\u0026#34; \\ | jq [ { \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s19b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s19b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;LastUpdated\u0026#34;: 1606332877, \u0026#34;LastUpdatedTime\u0026#34;: \u0026#34;2020-11-25 19:34:37.183293 +0000 +0000\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Role\u0026#34;: \u0026#34;Application\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;UAN\u0026#34; } } ] Update the UAN object in SLS by adding Aliases array with the UAN\u0026rsquo;s hostname.\nReplace UAN_XNAME in the URL and JSON object with the UAN\u0026rsquo;s component name (xname). Replace UAN_PARENT_XNAME in the JSON object with the UAN\u0026rsquo;s parent\u0026rsquo;s component name (xname). Replace UAN_ALIAS in the Aliases array with the UAN\u0026rsquo;s hostname. The LastUpdated and LastUpdatedTime fields are not required to be in the PUT payload.\ncurl -X PUT -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ \u0026#34;https://api-gw-service-nmn.local/apis/sls/v1/hardware/\u0026lt;UAN_XNAME\u0026gt;\u0026#34; -d \u0026#39; { \u0026#34;Parent\u0026#34;: \u0026#34;UAN_PARENT_XNAME\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;UAN_XNAME\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Role\u0026#34;: \u0026#34;Application\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;UAN\u0026#34;, \u0026#34;Aliases\u0026#34;: [\u0026#34;UAN_ALIAS\u0026#34;] } }\u0026#39; Using the response from the previous step, we can build the following command. The hostname for this uan is uan01, and this is reflected in the added Aliases field in the UAN\u0026rsquo;s ExtraProperties.\ncurl -X PUT -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ \u0026#34;https://api-gw-service-nmn.local/apis/sls/v1/hardware/x3000c0s19b0n0\u0026#34; -d \u0026#39; { \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s19b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s19b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Role\u0026#34;: \u0026#34;Application\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;UAN\u0026#34;, \u0026#34;Aliases\u0026#34;: [\u0026#34;uan01\u0026#34;] } }\u0026#39; Example response:\nncn-w001# curl -X PUT -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ \u0026gt;\u0026#34;https://api-gw-service-nmn.local/apis/sls/v1/hardware/x3000c0s19b0n0\u0026#34; -d \u0026#39; \u0026gt; { \u0026gt; \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s19b0\u0026#34;, \u0026gt; \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s19b0n0\u0026#34;, \u0026gt; \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026gt; \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026gt; \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026gt; \u0026#34;ExtraProperties\u0026#34;: { \u0026gt; \u0026#34;Role\u0026#34;: \u0026#34;Application\u0026#34;, \u0026gt; \u0026#34;SubRole\u0026#34;: \u0026#34;UAN\u0026#34;, \u0026gt; \u0026#34;Aliases\u0026#34;: [\u0026#34;uan01\u0026#34;] \u0026gt; } \u0026gt; }\u0026#39; {\u0026#34;Parent\u0026#34;:\u0026#34;x3000c0s19b0\u0026#34;,\u0026#34;Xname\u0026#34;:\u0026#34;x3000c0s19b0n0\u0026#34;,\u0026#34;Type\u0026#34;:\u0026#34;comptype_node\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;River\u0026#34;,\u0026#34;TypeString\u0026#34;:\u0026#34;Node\u0026#34;,\u0026#34;LastUpdated\u0026#34;:1606332877,\u0026#34;LastUpdatedTime\u0026#34;:\u0026#34;2020-11-25 19:34:37.183293 +0000 +0000\u0026#34;,\u0026#34;ExtraProperties\u0026#34;:{\u0026#34;Aliases\u0026#34;:[\u0026#34;uan01\u0026#34;],\u0026#34;Role\u0026#34;:\u0026#34;Application\u0026#34;,\u0026#34;SubRole\u0026#34;:\u0026#34;UAN\u0026#34;}} After a few minutes the -mgmt name should begin resolving. Communication with the BMC should be available via the alias uan01-mgmt.\nConfirm that the BMC for the UAN is up and running at the aliased address.\nncn-w001# ping -c 4 uan01-mgmt Example output:\nPING uan01-mgmt (10.254.2.53) 56(84) bytes of data. 64 bytes from x3000c0s19b0 (10.254.2.53): icmp_seq=1 ttl=255 time=0.170 ms 64 bytes from x3000c0s19b0 (10.254.2.53): icmp_seq=2 ttl=255 time=0.228 ms 64 bytes from x3000c0s19b0 (10.254.2.53): icmp_seq=3 ttl=255 time=0.311 ms 64 bytes from x3000c0s19b0 (10.254.2.53): icmp_seq=4 ttl=255 time=0.240 ms --- uan01-mgmt ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3061ms rtt min/avg/max/mdev = 0.170/0.237/0.311/0.051 ms When this node boots, the DHCP request of its -nmn interface will cause the uan01 to be created and resolved.\nConfirm that the UAN is being monitored by the console services. Follow the procedure in Manage Node Consoles.\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/change_credentials_on_servertech_pdus/",
	"title": "Change Credentials on ServerTech PDUs",
	"tags": [],
	"description": "",
	"content": "Change Credentials on ServerTech PDUs This procedure changes password used by the admn user on ServerTech PDUs. Either a single PDU can be updated to a new credential, or all ServerTech PDUs in the system can be updated to the same global credentials.\nNOTES:\nThis procedure does not update the default credentials that RTS uses for new ServerTech PDUs added to a system. To change the default credentials, see Update default ServerTech PDU Credentials used by the Redfish Translation Service. ServerTech PDUs running firmware version 8.0q or greater must have the password of the admn user changed before the JAWS REST API will function as expected. The default username and password for ServerTech PDUs is admn and admn. Prerequisites The Cray command line interface (CLI) is initialized and configured on the system. See Configure the Cray CLI.\nThe PDU is accessible over the network. A PDU can be reachable by its component name (xname) hostname, but may not yet be discovered by HSM.\nPDUs are manufactured by ServerTech.\nThis can be verified by the following command\nncn-mw# PDU=x3000m0 ncn-mw# curl -k -s --compressed https://${PDU} -i | grep Server: Expected output for a ServerTech PDU:\nServer: ServerTech-AWS/v8.0v NOTE: The firmware version is listed after the \u0026lsquo;/\u0026rsquo;. In this case, the firmware version is 8.0v.\nProcedure List the ServerTech PDUs currently discovered in the system.\nncn-mw# cray hsm inventory redfishEndpoints list --type CabinetPDUController --format json | jq -r \u0026#39;.RedfishEndpoints[] | select(.FQDN | contains(\u0026#34;rts\u0026#34;)).ID\u0026#39; Example output:\nx3000m0 If any of the PDUs are not discovered by HSM, then the component name (xname) for each of the ServerTech PDUs on the system must be obtained.\nSet up Vault password variable and command alias.\nncn-mw# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-mw# alias vault=\u0026#39;kubectl -n vault exec -i cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 VAULT_FORMAT=json vault\u0026#39; Look up the existing password for the admn user.\nTo extract the global credentials from Vault for the PDUs:\nncn-mw# vault kv get secret/pdu-creds/global/pdu To extract the credentials from Vault for a single PDU:\nncn-mw# PDU=x3000m0 ncn-mw# vault kv get secret/pdu-creds/$PDU Store the existing password for the admn user.\nncn-mw# read -s OLD_PDU_PASSWORD Specify the new desired password for the admn user. The new password must follow the following criteria:\nMinimum of 8 characters At least 1 uppercase letter At least 1 lowercase letter At least 1 number character ncn-mw# read -s NEW_PDU_PASSWORD Change and update the password for ServerTech PDUs.\nEither change the credentials on a single PDU or change all ServerTech PDUs to the same global default value:\nUpdate the password on a single ServerTech PDU\nSet the PDU hostname to change the admn credentials:\nncn-mw# PDU=x3000m0 Verify that the PDU is reachable:\nncn-mw# ping $PDU Change password for the admn user on the ServerTech PDU.\nncn-mw# curl -i -k -u \u0026#34;admn:${OLD_PDU_PASSWORD}\u0026#34; -X PATCH https://${PDU}/jaws/config/users/local/admn \\ -d $(jq --arg PASSWORD \u0026#34;${NEW_PDU_PASSWORD}\u0026#34; -nc \u0026#39;{password: $PASSWORD}\u0026#39;) Expected output upon a successful password change:\nHTTP/1.1 204 No Content Content-Type: text/html Transfer-Encoding: chunked Server: ServerTech-AWS/v8.0p Set-Cookie: C5=1883488164; path=/ Connection: close Pragma: JAWS v1.01 Update the PDU credentials stored in Vault.\nncn-mw# vault kv get secret/pdu-creds/$PDU | jq --arg PASSWORD \u0026#34;$NEW_PDU_PASSWORD\u0026#34; \u0026#39;.data | .Password=$PASSWORD\u0026#39; | vault kv put secret/pdu-creds/$PDU - Update all ServerTech PDUs in the system to the same password.\nNOTE: In order to change the password on all PDUs, the PDUs must be successfully discovered by HSM.\nChange password for the admn user on the ServerTech PDUs currently discovered in the system.\nncn-mw# for PDU in $(cray hsm inventory redfishEndpoints list --type CabinetPDUController --format json | jq -r \u0026#39;.RedfishEndpoints[] | select(.FQDN | contains(\u0026#34;rts\u0026#34;)).ID\u0026#39;); do echo \u0026#34;Updating password on ${PDU}\u0026#34; curl -i -k -u \u0026#34;admn:${OLD_PDU_PASSWORD}\u0026#34; -X PATCH https://${PDU}/jaws/config/users/local/admn \\ -d $(jq --arg PASSWORD \u0026#34;${NEW_PDU_PASSWORD}\u0026#34; -nc \u0026#39;{password: $PASSWORD}\u0026#39;) done Expected output upon a successful password change:\nUpdating password on x3000m0 HTTP/1.1 204 No Content Content-Type: text/html Transfer-Encoding: chunked Server: ServerTech-AWS/v8.0p Set-Cookie: C5=1883488164; path=/ Connection: close Pragma: JAWS v1.01 Updating password on x3001m0 HTTP/1.1 204 No Content Content-Type: text/html Transfer-Encoding: chunked Server: ServerTech-AWS/v8.0p Set-Cookie: C5=1883488164; path=/ Connection: close Pragma: JAWS v1.01 Update Vault for all ServerTech PDUs in the system to the same password:\nncn-mw# for PDU in $(cray hsm inventory redfishEndpoints list --type CabinetPDUController --format json | jq -r \u0026#39;.RedfishEndpoints[] | select(.FQDN | contains(\u0026#34;rts\u0026#34;)).ID\u0026#39;); do echo \u0026#34;Updating password on ${PDU}\u0026#34; vault kv get secret/pdu-creds/${PDU} | jq --arg PASSWORD \u0026#34;${NEW_PDU_PASSWORD}\u0026#34; \u0026#39;.data | .Password=$PASSWORD\u0026#39; | vault kv put secret/pdu-creds/${PDU} - done NOTE: After five minutes, the previous credential should stop working as the existing sessions time out.\nRestart the Redfish Translation Service (RTS) to pickup the new PDU credentials.\nncn-mw# kubectl -n services rollout restart deployment cray-hms-rts ncn-mw# kubectl -n services rollout status deployment cray-hms-rts Wait for RTS to initialize itself.\nncn-mw# sleep 3m Verify that RTS was able to communicate with the PDUs with the updated credentials.\nncn-mw# kubectl -n services exec -it deployment/cray-hms-rts -c cray-hms-rts-redis -- redis-cli keys \u0026#39;*/redfish/v1/Managers\u0026#39; Expected output for a system with two PDUs.\n1) \u0026#34;x3000m0/redfish/v1/Managers\u0026#34; 2) \u0026#34;x3001m0/redfish/v1/Managers\u0026#34; After waiting 10 minutes, Check that the PDU has been correctly discovered by HSM:\nncn-mw# cray hsm inventory redfishEndpoints describe x3000m0 --format json Example output:\n{ \u0026#34;ID\u0026#34;: \u0026#34;x3000m0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;CabinetPDUController\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;x3000m0-rts:8083\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x3000m0-rts:8083\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MACAddr\u0026#34;: \u0026#34;000a9c6236a5\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2022-11-30T22:11:30.712119Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;2019.1\u0026#34; } } If the FQDN does not contain rts:8083, then a manual update to the HSM record is required:\nncn-mw# cray hsm inventory redfishEndpoints update x3000m0 --fqdn x3000m0-rts:8083 --id x3000m0 --hostname x3000m0-rts:8083 Recheck cray hsm inventory redfishEndpoints to verify the FQDN was updated. Repeat this step for each ServerTech PDU.\n"
},
{
	"uri": "/docs-csm/en-12/operations/power_management/prepare_the_system_for_power_off/",
	"title": "Prepare the System for Power Off",
	"tags": [],
	"description": "",
	"content": "Prepare the System for Power Off This procedure prepares the system to remove power from all system cabinets. Be sure the system is healthy and ready to be shut down and powered off.\nThe sat bootsys shutdown and sat bootsys boot commands are used to shut down the system.\nPrerequisites An authentication token is required to access the API gateway and to use the sat command. See the \u0026ldquo;SAT Authentication\u0026rdquo; section of the HPE Cray EX System Admin Toolkit (SAT) product stream documentation (S-8031) for instructions on how to acquire a SAT authentication token.\nProcedure Obtain the user ID and passwords for system components:\nObtain user ID and passwords for all the system management network switches.\nIf necessary, obtain the user ID and password for the ClusterStor primary management node. For example, cls01053n00.\nIf the Slingshot network includes edge switches, then obtain the user ID and password for these switches.\nUse sat auth to authenticate to the API gateway within SAT.\nIf SAT has already been authenticated to the API gateway, then this step may be skipped.\nSee the \u0026ldquo;SAT Authentication\u0026rdquo; section in the HPE Cray EX System Admin Toolkit (SAT) product stream documentation (S-8031) for instructions on how to acquire a SAT authentication token.\nDetermine which Boot Orchestration Service (BOS) templates to use to shut down compute nodes and UANs.\nThere will be separate session templates for UANs and computes nodes.\nList all the session templates.\nIf it is unclear what session template is in use, then proceed to the next substep.\nncn-mw# cray bos sessiontemplate list Find the xname with sat status.\nncn-mw# sat status --filter role!=management --filter enabled=true \\ --fields xname,aliases,role,subrole,\u0026#34;desired config\u0026#34; Example output:\n+----------------+-----------+-------------+---------+--------------------+ | xname | Aliases | Role | SubRole | Desired Config | +----------------+-----------+-------------+---------+--------------------+ | x1000c0s0b0n0 | nid001000 | Compute | None | cos-config-2.3.101 | | x1000c0s0b0n1 | nid001001 | Compute | None | cos-config-2.3.101 | | x1000c0s0b1n0 | nid001002 | Compute | None | cos-config-2.3.101 | | x1000c0s0b1n1 | nid001003 | Compute | None | cos-config-2.3.101 | | x3000c0s23b0n0 | uan01 | Application | UAN | uan-config-2.4.3 | +----------------+-----------+-------------+---------+--------------------+ Find the bos_session value via the Configuration Framework Service (CFS).\nncn-mw# cray cfs components describe XNAME --format toml | grep bos_session Example output:\nbos_session = \u0026#34;e98cdc5d-3f2d-4fc8-a6e4-1d301d37f52f\u0026#34; Find the required templateName or templateUuid value with BOS (they are interchangeable).\nncn-mw# cray bos session describe BOS_SESSION --format toml | grep -E \u0026#39;template(Name|Uuid)\u0026#39; Example output:\ntemplateUuid = \u0026#34;cos-2.3.101\u0026#34; Determine the list of xnames or role groups or HSM groups associated with the desired boot session template.\nncn-mw# cray bos sessiontemplate describe SESSION_TEMPLATE_NAME_OR_UUID --format toml | egrep \u0026#34;node_list|node_roles_groups|node_groups\u0026#34; Example outputs:\nnode_list = [ \u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;x3000c0s19b2n0\u0026#34;, \u0026#34;x3000c0s19b3n0\u0026#34;, \u0026#34;x3000c0s19b4n0\u0026#34;,] node_roles_groups = [ \u0026#34;Compute\u0026#34;,] Use SAT to capture state of the system before the shutdown.\nncn-mw# sat bootsys shutdown --stage capture-state Optional system health checks.\nImportant: Running the System Diagnostic Utility (SDU) may take 20 to 60 minutes to run so start this command in one terminal session and then start another session for the other optional system health checks to be run while SDU is in progress.\nUse the System Diagnostic Utility (SDU) to capture current state of system before the shutdown.\nncn-m# sdu --scenario triage --start_time \u0026#39;-4 hours\u0026#39; \\ --reason \u0026#34;saving state before powerdown\u0026#34; Important: If sdu is not installed on a particular master node, then it may be installed on a different master node. If it is not installed and configured on any master nodes, then see the SDU documentation about how to install the cray-sdu-rda RPM, start the cray-sdu-rda serivce, wait for the cray-sdu-rda service to become ready, and configure it using the sdu setup command. See the Install and Configure topics in the HPE Cray EX CSM System Diagnostic Utility (SDU) Installation Guide (S-8034).\nCapture the state of all nodes.\nncn-mw# sat status | tee sat.status Capture the list of disabled nodes.\nncn-mw# sat status --filter Enabled=false | tee sat.status.disabled Capture the list of nodes that are off.\nncn-mw# sat status --filter State=Off | tee sat.status.off Capture the state of nodes in the workload manager.\nFor example, if the system uses Slurm:\nncn-mw# ssh uan01 sinfo | tee uan01.sinfo Capture the list of down nodes in the workload manager and the reason.\nncn-mw# ssh nid000001-nmn sinfo --list-reasons | tee sinfo.reasons Check Ceph status.\nncn-mw# ceph -s | tee ceph.status Check Kubernetes pod status for all pods.\nncn-mw# kubectl get pods -o wide -A | tee k8s.pods Additional Kubernetes status check examples:\nncn-mw# kubectl get pods -o wide -A | egrep \u0026#34;CrashLoopBackOff\u0026#34; | tee k8s.pods.CLBO ncn-mw# kubectl get pods -o wide -A | egrep \u0026#34;ContainerCreating\u0026#34; | tee k8s.pods.CC ncn-mw# kubectl get pods -o wide -A | egrep -v \u0026#34;Run|Completed\u0026#34; | tee k8s.pods.errors Check HSN status.\nRun fmn_status in the slingshot-fabric-manager pod and save the output to a file:\nncn-mw# kubectl exec -it -n services $(kubectl get pods \\ -l app.kubernetes.io/name=slingshot-fabric-manager -n services | tail -1 \\ | cut -f1 -d\u0026#34; \u0026#34;) -c slingshot-fabric-manager -- fmn_status --details \\ | tee fabric.status.details Check management switches to verify they are reachable.\nNote: The switch host names depend on the system configuration.\nLook in /etc/hosts for the management network switches on this system. The names of all spine switches, leaf switches, leaf BMC switches, and CDU switches need to be used in the next step.\nncn-mw# grep sw- /etc/hosts Example output:\n10.254.0.2 sw-spine-001 10.254.0.3 sw-spine-002 10.254.0.4 sw-leaf-bmc-001 10.254.0.5 sw-leaf-bmc-002 10.100.0.2 sw-cdu-001 10.100.0.3 sw-cdu-002 Ping all switches using the proper list of hostnames in the index of the for loop.\nncn-mw# for switch in sw-leaf-00{1,2} sw-leaf-bmc-00{1-2} sw-spine-00{1,2} sw-cdu-00{1,2}l; do while true; do ping -c 1 $switch \u0026gt; /dev/null \u0026amp;\u0026amp; break echo \u0026#34;switch $switch is not yet up\u0026#34; sleep 5 done echo \u0026#34;switch $switch is up\u0026#34; done | tee switches Check Lustre server health.\nncn-mw# ssh admin@cls01234n00.us.cray.com admin@cls01234n00# cscli csinfo admin@cls01234n00# cscli show_nodes admin@cls01234n00# cscli fs_info From a node which has the Lustre file system mounted.\nuan01# lfs check servers uan01# lfs df Check for running sessions.\nncn-mw# sat bootsys shutdown --stage session-checks | tee sat.session-checks Example output:\nChecking for active BOS sessions. Found no active BOS sessions. Checking for active CFS sessions. Found no active CFS sessions. Checking for active CRUS upgrades. Found no active CRUS upgrades. Checking for active FAS actions. Found no active FAS actions. Checking for active NMD dumps. Found no active NMD dumps. Checking for active SDU sessions. Found no active SDU sessions. No active sessions exist. It is safe to proceed with the shutdown procedure. If active sessions are running, then either wait for them to complete or cancel the session. See the following step.\nCancel the running BOS sessions.\nIdentify the BOS Sessions and associated BOA Kubernetes jobs to delete.\nDetermine which BOS sessions to cancel. To cancel a BOS session, kill its associated Boot Orchestration Agent (BOA) Kubernetes job.\nTo find a list of BOA jobs that are still running:\nncn-mw# kubectl -n services get jobs|egrep -i \u0026#34;boa|Name\u0026#34; Output similar to the following will be returned:\nNAME COMPLETIONS DURATION AGE boa-0216d2d9-b2bc-41b0-960d-165d2af7a742 0/1 36m 36m boa-0dbd7adb-fe53-4cda-bf0b-c47b0c111c9f 1/1 36m 3d5h boa-4274b117-826a-4d8b-ac20-800fcac9afcc 1/1 36m 3d7h boa-504dd626-d566-4f58-9974-3c50573146d6 1/1 8m47s 3d5h boa-bae3fc19-7d91-44fc-a1ad-999e03f1daef 1/1 36m 3d7h boa-bd95dc0b-8cb2-4ad4-8673-bb4cc8cae9b0 1/1 36m 3d7h boa-ccdd1c29-cbd2-45df-8e7f-540d0c9cf453 1/1 35m 3d5h boa-e0543eb5-3445-4ee0-93ec-c53e3d1832ce 1/1 36m 3d5h boa-e0fca5e3-b671-4184-aa21-84feba50e85f 1/1 36m 3d5h Any job with a 0/1 COMPLETIONS column is still running and is a candidate to be forcibly deleted. The BOA Job ID appears in the NAME column.\nClean up prior to BOA job deletion.\nThe BOA pod mounts a ConfigMap under the name boot-session at the directory /mnt/boot_session inside the pod. This ConfigMap has a random UUID name like e0543eb5-3445-4ee0-93ec-c53e3d1832ce. Prior to deleting a BOA job, delete its ConfigMap. Find the BOA job\u0026rsquo;s ConfigMap with the following command:\nncn-mw# kubectl -n services describe job \u0026lt;BOA Job ID\u0026gt; |grep ConfigMap -A 1 -B 1 Example:\nncn-mw# kubectl -n services describe job boa-0216d2d9-b2bc-41b0-960d-165d2af7a742 |grep ConfigMap -A 1 -B 1 Example output:\nboot-session: Type: ConfigMap (a volume populated by a ConfigMap) Name: e0543eb5-3445-4ee0-93ec-c53e3d1832ce \u0026lt;\u0026lt;\u0026lt; ConfigMap name. Delete this one. -- ca-pubkey: Type: ConfigMap (a volume populated by a ConfigMap) Name: cray-configmap-ca-public-key Delete the ConfigMap associated with the boot-session, not the ca-pubkey.\nTo delete the ConfigMap:\nncn-mw# kubectl -n services delete cm \u0026lt;ConfigMap name\u0026gt; Example:\nncn-mw# kubectl -n services delete cm e0543eb5-3445-4ee0-93ec-c53e3d1832ce Example output:\nconfigmap \u0026#34;e0543eb5-3445-4ee0-93ec-c53e3d1832ce\u0026#34; deleted Delete the BOA jobs.\nncn-mw# kubectl -n services delete job \u0026lt;BOA JOB ID\u0026gt; This will kill the BOA job and the BOS session associated with it.\nWhen a job is killed, BOA will no longer attempt to execute the operation it was attempting to perform. This does not mean that nothing continues to happen. If BOA has instructed a node to power on, the node will continue to power even after the BOA job has been killed.\nDelete the BOS session. BOS keeps track of sessions in its database. These entries need to be deleted. The BOS Session ID is the same as the BOA Job ID minus the prepended boa- string. Use the following command to delete the BOS database entry.\nncn-mw# cray bos session delete \u0026lt;session ID\u0026gt; Example:\nncn-mw# cray bos session delete 0216d2d9-b2bc-41b0-960d-165d2af7a742 Coordinate with the site to prevent new sessions from starting in the services listed.\nThere is no method to prevent new sessions from being created as long as the service APIs are accessible on the API gateway.\nFollow the vendor workload manager documentation to drain processes running on compute nodes. For Slurm, see the scontrol man page. For PBS Professional, see the pbsnodes man page.\nNext step Return to System Power Off Procedures and continue with next step.\n"
},
{
	"uri": "/docs-csm/en-12/operations/package_repository_management/troubleshoot_nexus/",
	"title": "Troubleshoot Nexus",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Nexus This page contains general Nexus troubleshooting topics.\nlookup registry.local: no such host Error initiating layer upload \u0026hellip; in registry.local error: not ready: https://packages.local Unable to log in with Keycloak accounts lookup registry.local: no such host The following error may occur when running ./lib/setup-nexus.sh:\ntime=\u0026#34;2021-02-23T19:55:54Z\u0026#34; level=fatal msg=\u0026#34;Error copying tag \\\u0026#34;dir:/image/grafana/grafana:7.0.3\\\u0026#34;: Error writing blob: Head \\\u0026#34;https://registry.local/v2/grafana/grafana/blobs/sha256:cf254eb90de2dc62aa7cce9737ad7e143c679f5486c46b742a1b55b168a736d3\\\u0026#34;: dial tcp: lookup registry.local: no such host\u0026#34; + return Or a similar error:\ntime=\u0026#34;2021-03-04T22:45:07Z\u0026#34; level=fatal msg=\u0026#34;Error copying ref \\\u0026#34;dir:/image/cray/cray-ims-load-artifacts:1.0.4\\\u0026#34;: Error trying to reuse blob sha256:1ec886c351fa4c330217411b0095ccc933090aa2cd7ae7dcd33bb14b9f1fd217 at destination: Head \\\u0026#34;https://registry.local/v2/cray/cray-ims-load-artifacts/blobs/sha256:1ec886c351fa4c330217411b0095ccc933090aa2cd7ae7dcd33bb14b9f1fd217\\\u0026#34;: dial tcp: lookup registry.local: Temporary failure in name resolution\u0026#34; + return These errors are most likely intermittent and running ./lib/setup-nexus.sh again is expected to succeed.\nError initiating layer upload \u0026hellip; in registry.local The following error may occur when running ./lib/setup-nexus.sh:\ntime=\u0026#34;2021-02-07T20:25:22Z\u0026#34; level=info msg=\u0026#34;Copying image tag 97/144\u0026#34; from=\u0026#34;dir:/image/jettech/kube-webhook-certgen:v1.2.1\u0026#34; to=\u0026#34;docker://registry.local/jettech/kube-webhook-certgen:v1.2.1\u0026#34; Getting image source signatures Copying blob sha256:f6e131d355612c71742d71c817ec15e32190999275b57d5fe2cd2ae5ca940079 Copying blob sha256:b6c5e433df0f735257f6999b3e3b7e955bab4841ef6e90c5bb85f0d2810468a2 Copying blob sha256:ad2a53c3e5351543df45531a58d9a573791c83d21f90ccbc558a7d8d3673ccfa time=\u0026#34;2021-02-07T20:25:33Z\u0026#34; level=fatal msg=\u0026#34;Error copying tag \\\u0026#34;dir:/image/jettech/kube-webhook-certgen:v1.2.1\\\u0026#34;: Error writing blob: Error initiating layer upload to /v2/jettech/kube-webhook-certgen/blobs/uploads/ in registry.local: received unexpected HTTP status: 200 OK\u0026#34; + return This error is most likely intermittent and running ./lib/setup-nexus.sh again is expected to succeed.\nerror: not ready: https://packages.local The error: not ready: https://packages.local indicates that from the caller\u0026rsquo;s perspective, Nexus is not ready to receive writes. However, it most likely indicates that a Nexus setup utility was unable to connect to Nexus via the packages.local name. Because the install does not attempt to connect to packages.local until Nexus has been successfully deployed, the error does not usually indicate something is actually wrong with Nexus. Instead, it is most commonly a network issue with name resolution (i.e., DNS), IP routes from the PIT node, switch misconfiguration, or Istio ingress.\nVerify that packages.local resolves to ONLY the load balancer IP address for the istio-ingressgateway service in the istio-system namespace, typically 10.92.100.71. If name resolution returns addresses on other networks (such as HMN), this must be corrected. Prior to DNS/DHCP hand-off to Unbound, these settings are controlled by dnsmasq. Unbound settings are based on SLS settings in sls_input_file.json and must be updated via the Unbound manager.\nIf packages.local resolves to the correct addresses, verify basic connectivity using ping. If ping packages.local is unsuccessful, verify the IP routes from the PIT node to the NMN load balancer network. The typical ip route configuration is 10.92.100.0/24 via 10.252.0.1 dev bond0.nmn0. If ping attempts are successful, then try checking the status of Nexus by running curl -sS https://packages.local/service/rest/v1/status/writable. If the connection times out, it indicates there is a more complex connection issue. Lastly, check Istio and OPA logs to see if connections to packages.local are not reaching Nexus, perhaps because of an authorization issue.\nIf https://packages.local/service/rest/v1/status/writable returns an HTTP code other than 200 OK, it indicates there is an issue with Nexus. Verify that the loftsman ship deployment of the nexus.yaml manifest was successful. If helm status -n nexus cray-nexus indicates the status is NOT deployed, then something is most likely wrong with the Nexus deployment and additional diagnosis is required. In this case, the current Nexus deployment probably needs to be uninstalled and the nexus-data PVC removed before attempting to deploy again.\nUnable to log in with Keycloak accounts First ensure the Keycloak account that is attempting to log in has proper Nexus permissions. See Manage Repositories with Nexus for information on what Keycloak permissions they need.\nIf the account has all the proper permissions and still fails to log in to Nexus, then the Keycloak integration configuration may have failed setup. See Nexus Fails Authentication with Keycloak Users for steps to fix the configuration.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/removing_a_liquid-cooled_blade_from_a_system/",
	"title": "Removing a Liquid-cooled blade from a System",
	"tags": [],
	"description": "",
	"content": "Removing a Liquid-cooled blade from a System This procedure will remove a liquid-cooled blades from an HPE Cray EX system.\nPerquisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI.\nKnowledge of whether Data Virtualization Service (DVS) is operating over the Node Management Network (NMN) or the High Speed Network (HSN).\nThe Slingshot fabric must be configured with the desired topology for desired state of the blades in the system.\nThe System Layout Service (SLS) must have the desired HSN configuration.\nCheck the status of the HSN and record link status before the procedure.\nThe blades must have the coolant drained and filled during the swap to minimize cross-contamination of cooling systems.\nReview procedures in HPE Cray EX Coolant Service Procedures H-6199 Review the HPE Cray EX Hand Pump User Guide H-6200 Procedure 1. Prepare the source system blade for removal Use the workload manager (WLM) to drain running jobs from the affected nodes on the blade.\nRefer to the vendor documentation for the WLM for more information.\nUse Boot Orchestration Services (BOS) to shut down the affected nodes in the source blade.\nIn this example, x9000c3s0 is the source blade. Specify the appropriate component name (xname) and BOS template for the node type in the following command.\nncn-mw# BOS_TEMPLATE=cos-2.0.30-slurm-healthy-compute ncn-mw# cray bos session create --template-uuid $BOS_TEMPLATE --operation shutdown --limit x9000c3s0b0n0,x9000c3s0b0n1,x9000c3s0b1n0,x9000c3s0b1n1 2. Disable the Redfish endpoints for the nodes Temporarily disable the Redfish endpoints for NodeBMCs present in the blade.\nncn-mw# cray hsm inventory redfishEndpoints update --enabled false x9000c3s0b0 ncn-mw# cray hsm inventory redfishEndpoints update --enabled false x9000c3s0b1 3. Clear Redfish event subscriptions from BMCs on the blade Set the environment variable SLOT to the blade\u0026rsquo;s location.\nncn-mw# SLOT=\u0026#34;x9000c3s0\u0026#34; Clear the Redfish event subscriptions.\nncn-mw# for BMC in $(cray hsm inventory redfishEndpoints list --type NodeBMC --format json | jq .RedfishEndpoints[].ID -r | grep $SLOT); do PASSWD=$(cray scsd bmc creds list --targets $BMC --format json | jq .Targets[].Password -r) SUBS=$(curl -sk -u root:$PASSWD https://${BMC}/redfish/v1/EventService/Subscriptions | jq -r \u0026#39;.Members[].\u0026#34;@odata.id\u0026#34;\u0026#39;) for SUB in $SUBS; do echo \u0026#34;Deleting event subscription: https://${BMC}${SUB}\u0026#34; curl -i -sk -u root:$PASSWD -X DELETE https://${BMC}${SUB} done done Each event subscription deleted that was deleted will have output like the following:\nDeleting event subscription: https://x9000c3s2b0/redfish/v1/EventService/Subscriptions/1 HTTP/2 204 access-control-allow-credentials: true access-control-allow-headers: X-Auth-Token access-control-allow-origin: * access-control-expose-headers: X-Auth-Token cache-control: no-cache, must-revalidate content-type: text/html; charset=UTF-8 date: Tue, 19 Jan 2038 03:14:07 GMT odata-version: 4.0 server: Cray Embedded Software Redfish Service 4. Clear the node controller settings Remove the system-specific settings from each node controller on the blade.\nncn-mw# curl -k -u root:PASSWORD -X POST -H \\ \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;ResetType\u0026#34;:\u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://x9000c3s0b0/redfish/v1/Managers/BMC/Actions/Manager.Reset ncn-mw# curl -k -u root:PASSWORD -X POST -H \\ \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;ResetType\u0026#34;:\u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://x9000c3s0b1/redfish/v1/Managers/BMC/Actions/Manager.Reset Use Ctrl-C to return to the prompt if command does not return.\n5. Power off the chassis slot Suspend the hms-discovery cron job.\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : true }}\u0026#39; Verify that the hms-discovery cron job has stopped (ACTIVE = 0 and SUSPEND = True).\nncn-mw# kubectl get cronjobs -n services hms-discovery Example output:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * True 0 117s 15d Power off the chassis slot.\nThis examples powers off slot 0, chassis 3, in cabinet 9000.\nncn-mw# cray capmc xname_off create --xnames x9000c3s0 --recursive true 6. Disable the chassis slot Disable the chassis slot.\nDisabling the slot prevents hms-discovery from automatically powering on the slot. This example disables slot 0, chassis 3, in cabinet 9000.\nncn-mw# cray hsm state components enabled update --enabled false x9000c3s0 7. Record MAC and IP addresses for nodes IMPORTANT: Record the NMN MAC and IP addresses for each node in the blade (labeled Node Maintenance Network). To prevent disruption in DVS when over operating the NMN, these addresses must be maintained in the HSM when the blade is swapped and discovered.\nThe NodeBMC MAC and IP addresses are assigned algorithmically and must not be deleted from the HSM.\nSkip this step if DVS is operating over the HSN, otherwise proceed with this step. Query HSM to determine the ComponentID, MAC addresses, and IP addresses for each node in the blade.\nThe prerequisites show an example of how to gather HSM values and store them to a file.\nncn-mw# cray hsm inventory ethernetInterfaces list --component-id x9000c3s0b0n0 --format json Example output:\n[ { \u0026#34;ID\u0026#34;: \u0026#34;0040a6836339\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:63:39\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-04-09T21:51:04.662063Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x9000c3s0b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;IPAddresses\u0026#34;: [ { \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.10\u0026#34; } ] } ] Record the following values for the blade:\n`ComponentID: \u0026#34;x9000c3s0b0n0\u0026#34;` `MACAddress: \u0026#34;00:40:a6:83:63:39\u0026#34;` `IPAddress: \u0026#34;10.100.0.10\u0026#34;` Repeat the command to record the ComponentID, MAC addresses, and IP addresses for the Node Maintenance Network for the other nodes in the blade.\n8. Cleanup Hardware State Manager Set an environment variable that corresponds to the chassis slot of the blade.\nncn-mw# CHASSIS_SLOT=x9000c3s0 Delete the Redfish endpoints for each node.\nncn-mw# for xname in $(cray hsm inventory redfishEndpoints list --format json | jq -r --arg CHASSIS_SLOT \u0026#34;${CHASSIS_SLOT}\u0026#34; \\ \u0026#39;.RedfishEndpoints[] | select(.ID | startswith($CHASSIS_SLOT)) | .ID\u0026#39;) do echo \u0026#34;Removing $xname from HSM Inventory RedfishEndpoints\u0026#34; cray hsm inventory redfishEndpoints delete \u0026#34;$xname\u0026#34; done Remove entries from the state components.\nncn-mw# for xname in $(cray hsm state components list --format json | jq -r --arg CHASSIS_SLOT \u0026#34;${CHASSIS_SLOT}\u0026#34; \\ \u0026#39;.Components[] | select((.ID | startswith($CHASSIS_SLOT)) and (.ID != $CHASSIS_SLOT)) | .ID\u0026#39; ) do echo \u0026#34;Removing $xname from HSM State components\u0026#34; cray hsm state components delete \u0026#34;$xname\u0026#34; done Delete the NMN MAC and IP addresses each node in the blade from the HSM.\nDo not delete the MAC and IP addresses for the node BMC.\nncn-mw# for mac in $(cray hsm inventory ethernetInterfaces list --type Node --format json | jq -r --arg CHASSIS_SLOT \u0026#34;${CHASSIS_SLOT}\u0026#34; \\ \u0026#39;.[] | select(.ComponentID | startswith($CHASSIS_SLOT)) | .ID\u0026#39;) do echo \u0026#34;Removing $mac from HSM Inventory EthernetInterfaces\u0026#34; cray hsm inventory ethernetInterfaces delete \u0026#34;$mac\u0026#34; done Restart Kea.\nncn-mw# kubectl delete pods -n services -l app.kubernetes.io/name=cray-dhcp-kea 9. Remove the blade Remove the blade from the source location.\nReview the Remove a Compute Blade Using the Lift procedure in HPE Cray EX Hardware Replacement Procedures H-6173 for detailed instructions for replacing liquid-cooled blades. These procedures can be found on the HPE Support Center. Drain the coolant from the blade and fill with fresh coolant to minimize cross-contamination of cooling systems.\nReview HPE Cray EX Coolant Service Procedures H-6199. If using the hand pump, then review procedures in the HPE Cray EX Hand Pump User Guide H-6200. These procedures can be found on the HPE Support Center. Install the blade from the source system in a storage rack or leave it on the cart.\n10. Rediscover the Chassis BMC of the chassis the blade was removed from Determine the name of the Chassis BMC.\nncn-mw# CHASSIS_BMC=\u0026#34;$(echo $CHASSIS_SLOT | egrep -o \u0026#39;x[0-9]+c[0-9]+\u0026#39;)b0\u0026#34; ncn-mw# echo $CHASSIS_BMC Example output:\nx9000c3b0 Rediscover the Chassis BMC.\nncn-mw# cray hsm inventory discover create --xnames $CHASSIS_BMC 11. Re-enable the hms-discovery cronjob Un-suspend the hms-discovery cron job if no more liquid-cooled blades are planned to be removed from the system.\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : false }}\u0026#39; Verify that the hms-discovery cron job has stopped (ACTIVE = 0 and SUSPEND = False).\nncn-mw# kubectl get cronjobs -n services hms-discovery Example output:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * False 1 46s 15d "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_remove_replace_ncns/remove_switch_config/",
	"title": "Remove Switch Configuration for NCN",
	"tags": [],
	"description": "",
	"content": "Remove Switch Configuration for NCN Description Update the network switches for the NCN that was removed.\nProcedure Update Networking to Remove NCN Details coming soon.\nNext Step Proceed to the next step to Redeploy Services or return to the main Add, Remove, Replace, or Move NCNs page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/check_kea_dhcp_logs/",
	"title": "Check KEA DHCP logs",
	"tags": [],
	"description": "",
	"content": "Check KEA DHCP logs In order to check the logs for the pod you will need to know the pod name, run this command to see the pod name:\nkubectl logs -n services -l app.kubernetes.io/instance=cray-dhcp-kea -c cray-dhcp-kea Example:\nncn-w001# kubectl logs -n services -l app.kubernetes.io/instance=cray-dhcp-kea -c cray-dhcp-kea 2020-08-03 21:47:50.580 INFO [kea-dhcp4.dhcpsrv/10] DHCPSRV_MEMFILE_LEASE_FILE_LOAD loading leases from file /cray-dhcp-kea-socket/dhcp4.leases 2020-08-03 21:47:50.580 INFO [kea-dhcp4.dhcpsrv/10] DHCPSRV_MEMFILE_LFC_SETUP setting up the Lease File Cleanup interval to 3600 sec 2020-08-03 21:47:50.580 WARN [kea-dhcp4.dhcpsrv/10] DHCPSRV_OPEN_SOCKET_FAIL failed to open socket: the interface eth0 has no usable IPv4 addresses configured 2020-08-03 21:47:50.580 WARN [kea-dhcp4.dhcpsrv/10] DHCPSRV_NO_SOCKETS_OPEN no interface configured to listen to DHCP traffic 2020-08-03 21:48:00.602 INFO [kea-dhcp4.commands/10] COMMAND_RECEIVED Received command \u0026#39;lease4-get-all\u0026#39; {\u0026#34;Dhcp4\u0026#34;: {\u0026#34;control-socket\u0026#34;: {\u0026#34;socket-name\u0026#34;: \u0026#34;/cray-dhcp-kea-socket/cray-dhcp-kea.socket\u0026#34;, \u0026#34;socket-type\u0026#34;: \u0026#34;unix\u0026#34;}, \u0026#34;hooks-libraries\u0026#34;: [{\u0026#34;library\u0026#34;: \u0026#34;/usr/local/lib/kea/hooks/libdhcp_lease_cmds.so\u0026#34;}, ...SNIP... waiting 10 seconds for any leases to be given out... [{\u0026#39;arguments\u0026#39;: {\u0026#39;leases\u0026#39;: []}, \u0026#39;result\u0026#39;: 3, \u0026#39;text\u0026#39;: \u0026#39;0 IPv4 lease(s) found.\u0026#39;}] 2020-08-03 21:48:22.734 INFO [kea-dhcp4.commands/10] COMMAND_RECEIVED Received command \u0026#39;config-get\u0026#39; This command will output kea logs:\nkubectl logs -n services -l app.kubernetes.io/instance=cray-dhcp-kea -c cray-dhcp-kea | grep -i error Shell into a Kea Pod\nkubectl exec -n services -it pod/$(kubectl get -n services pods | grep kea | head -n 1) -c cray-dhcp-kea -- /bin/bash Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/lldp/",
	"title": "Link layer discovery protocol (LLDP)",
	"tags": [],
	"description": "",
	"content": "Link layer discovery protocol (LLDP) By default, LLDP is enabled for each interface and globally. Administrators can disable LLDP on an interface or globally. If LLDP is disabled globally, LLDP is disabled on all interfaces irrespective of whether LLDP is previously enabled or disabled on an interface. When administrators enable LLDP globally, the LLDP configuration at the interface level takes precedence over the global LLDP configuration.\nConfiguration Commands Disable the LLDPDU transmit or receive in INTERFACE mode:\nno lldp transmit no lldp receive Disable the LLDP holdtime-multiplier value in CONFIGURATION mode:\nno lldp holdtime-multiplier Disable the LLDP initialization in CONFIGURATION mode:\nno lldp reinit Disable the LLDP MED in CONFIGURATION or INTERFACE mode:\nno lldp med Disable LLDP TLV in INTERFACE mode:\nno lldp tlv-select Disable LLDP globally in CONFIGURATION mode:\nno lldp enable Expected Results Link status between the peer devices is UP LLDP is enabled Local device LLDP Information is displayed Remote device LLDP information is displayed LLDP statistics are displayed Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/canu/using_canu_to_generate_full_network_config/",
	"title": "Use CANU to Generate Full Network Configuration",
	"tags": [],
	"description": "",
	"content": "Use CANU to Generate Full Network Configuration CANU can also generate switch configurations for all the switches on a network.\nIn order to generate network configurations, a valid SHCD must be passed in and system variables must be read in from either CSI output or the SLS API.\nThe instructions are exactly the same as the above except there will not be a hostname and a folder must be specified for configuration output using the --folder FOLDERNAME flag.\nTo generate switch configurations run:\nlinux# canu -s 1.5 network config -a full --shcd FILENAME.xlsx --tabs \u0026#39;INTER_SWITCH_LINKS,NON_COMPUTE_NODES,HARDWARE_MANAGEMENT,COMPUTE_NODES\u0026#39; --corners \u0026#39;J14,T44,J14,T48,J14,T24,J14,T23\u0026#39; --csi-folder /CSI/OUTPUT/FOLDER/ADDRESS --folder FOLDERNAME linux# canu -s 1.3 network config -a full --shcd FILENAME.xlsx --tabs INTER_SWITCH_LINKS,NON_COMPUTE_NODES,HARDWARE_MANAGEMENT,COMPUTE_NODES --corners J14,T44,J14,T48,J14,T24,J14,T23 --csi-folder /CSI/OUTPUT/FOLDER/ADDRESS --folder switch_config Expected results:\nsw-spine-001 Config Generated sw-spine-002 Config Generated sw-leaf-001 Config Generated sw-leaf-002 Config Generated sw-leaf-003 Config Generated sw-leaf-004 Config Generated sw-cdu-001 Config Generated sw-cdu-002 Config Generated sw-leaf-bmc-001 Config Generated "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/check_dhcp_lease_is_getting_allocated/",
	"title": "Check DHCP Lease is Getting Allocated",
	"tags": [],
	"description": "",
	"content": "Check DHCP Lease is Getting Allocated Checking Check the Kea logs and verify that the DHCP lease is getting allocated.\nncn-mw# KEA_POD=$(kubectl get pods -n services -l app.kubernetes.io/name=cray-dhcp-kea -o custom-columns=:.metadata.name --no-headers) ncn-mw# echo \u0026#34;${KEA_POD}\u0026#34; ncn-mw# kubectl logs -n services pod/\u0026#34;${KEA_POD}\u0026#34; -c cray-dhcp-kea The following example shows that Kea is allocating a lease to 10.104.0.23. The lease must say DHCP4_LEASE_ALLOC; if it says DHCP4_LEASE_ADVERT, then there is likely a problem.\n2021-04-21 00:13:05.416 INFO [kea-dhcp4.leases/24.139710796402304] DHCP4_LEASE_ ***ALLOC*** [hwtype=1 02:23:28:01:30:10], cid=[00:78:39:30:30:30:63:31:73:30:62:31], tid=0x21f2433a: lease 10.104.0.23 has been allocated for 300 seconds The following is an example of the lease showing DHCP4_LEASE_ADVERT:\n2021-06-21 16:44:31.124 INFO [kea-dhcp4.leases/18.139837089017472] DHCP4_LEASE_ ***ADVERT*** [hwtype=1 14:02:ec:d9:79:88], cid=[no info], tid=0xe87fad10: lease 10.252.1.16 will be advertised Remediation Restarting Kea will fix the DHCP4_LEASE_ADVERT issue in most cases.\nRestart Kea.\nncn-mw# kubectl rollout restart deployment -n services cray-dhcp-kea Wait for deployment to restart.\nncn-mw# kubectl rollout status deployment -n services cray-dhcp-kea Back to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/configure_kubectl_credentials_to_access_the_kubernetes_apis/",
	"title": "Configure kubectl Credentials to Access the Kubernetes APIs",
	"tags": [],
	"description": "",
	"content": "Configure kubectl Credentials to Access the Kubernetes APIs The credentials for kubectl are located in the admin configuration file on all non-compute node (NCN) master and worker nodes. They can be found at /etc/kubernetes/admin.conf for the root user. Use kubectl to access the Kubernetes cluster from a device outside the cluster.\nFor more information, refer to the Kubernetes home page.\nPrerequisites This procedure requires administrative privileges and assumes that the device being used has:\nkubectl is installed Access to the site admin network Procedure Access the credentials file used by kubectl at /etc/kubernetes/admin.conf on any one of the master or worker NCNs.\nIf copying this file to another system, be sure to set the environmental variable KUBECONFIG to the new location on that system.\nVerify access by executing the following command:\nncn# kubectl get nodes Successful output will resemble the following:\nNAME STATUS ROLES AGE VERSION ncn-m001 Ready control-plane,master 27h v1.20.13 ncn-m002 Ready control-plane,master 8d v1.20.13 ncn-m003 Ready control-plane,master 8d v1.20.13 ncn-w001 Ready \u0026lt;none\u0026gt; 8d v1.20.13 ncn-w002 Ready \u0026lt;none\u0026gt; 8d v1.20.13 ncn-w003 Ready \u0026lt;none\u0026gt; 8d v1.20.13 "
},
{
	"uri": "/docs-csm/en-12/operations/image_management/import_external_image_to_ims/",
	"title": "Import an External Image to IMS",
	"tags": [],
	"description": "",
	"content": "Import an External Image to IMS The Image Management Service (IMS) is typically used to build images from IMS recipes and customize Images that are already known to IMS. However, it is sometimes the case that an image is built using a mechanism other than by IMS and needs to be added to IMS. In these cases, the following procedure can be used to add this external image to IMS and upload the image\u0026rsquo;s artifact(s) to the Simple Storage Service (S3).\nPrerequisites Limitations Procedure Ensure supported format Set helper variables Record artifact checksums Create image record in IMS Upload artifacts to S3 Create, upload, and register image manifest Prerequisites CSM is fully installed, configured, and healthy.\nThe Image Management Service (IMS) is healthy. The Simple Storage Service (S3) is healthy. The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system. These may be validated by performing the following health checks: Platform health checks Software Management Service health checks The Cray CLI is configured.\nSee Configure the Cray CLI. Image artifact files are available.\nAn image root file is required. Optionally, additional image artifacts may be specified including a kernel, initrd, and kernel parameters file. A token providing S3 credentials has been generated.\nLimitations The commands in this procedure must be run as the root user. Images in the .txz compressed format need to be converted to SquashFS in order to use IMS image customization. Procedure This procedure may be run on any master or worker NCN.\nThe procedure on this page uses example commands that assume the image has an associated kernel and initrd artifact. This is the case, for example, for NCN boot images. If the actual set of image artifacts differs from this, then be sure to modify the commands accordingly.\n1. Ensure supported format Ensure that the image root is in a supported format.\nIMS requires that an image\u0026rsquo;s root filesystem is in SquashFS format. Select one of the following options based on the current state of the image root being used:\nIf the image being added is in tgz format, then refer to Convert TGZ Archives to SquashFS Images. If the image being added meets the above requirements, then proceed to Create image record in IMS. If the image root is in a format other than tgz or SquashFS, then convert the image root to tgz/SquashFS before continuing. 2. Set helper variables Set variables for all of the image artifact files, if needed. For example, IMS_ROOTFS_FILENAME, IMS_INITRD_FILENAME, and IMS_KERNEL_FILENAME.\nIf this procedure is being done as part of NCN Worker Image Customization, then these should already be set. In this case, skip this section and proceed to Record artifact checksums.\nSet the IMS_ROOTFS_FILENAME variable to the file name of the SquashFS image root file to be uploaded.\nFor example:\nncn-mw# IMS_ROOTFS_FILENAME=sles_15_image.squashfs Set the IMS_INITRD_FILENAME variable to the file name of the initrd file to be uploaded.\nSkip this if no initrd file is associated with this image.\nFor example:\nncn-mw# IMS_INITRD_FILENAME=initrd Set the IMS_KERNEL_FILENAME variable to the file name of the kernel file to be uploaded.\nSkip this if no kernel file is associated with this image.\nFor example:\nncn-mw# IMS_KERNEL_FILENAME=vmlinuz 3. Record artifact checksums Navigate to the directory containing the artifact files.\nVerify that all image artifacts exist in the current working directory.\nIf necessary, modify the following command to reflect the actual set of artifacts included in the image.\nncn-mw# ls -al \u0026#34;${IMS_ROOTFS_FILENAME}\u0026#34; \u0026#34;${IMS_INITRD_FILENAME}\u0026#34; \u0026#34;${IMS_KERNEL_FILENAME}\u0026#34; Record the checksums of all of the artifacts.\nRecord the SquashFS image root checksum in the IMS_ROOTFS_MD5SUM variable.\nncn-mw# IMS_ROOTFS_MD5SUM=$(md5sum \u0026#34;${IMS_ROOTFS_FILENAME}\u0026#34; | awk \u0026#39;{ print $1 }\u0026#39;) ncn-mw# echo \u0026#34;${IMS_ROOTFS_MD5SUM}\u0026#34; Record the initrd checksum in the IMS_INITRD_MD5SUM variable.\nSkip this if no initrd file is associated with this image.\nncn-mw# IMS_INITRD_MD5SUM=$(md5sum \u0026#34;${IMS_INITRD_FILENAME}\u0026#34; | awk \u0026#39;{ print $1 }\u0026#39;) ncn-mw# echo \u0026#34;${IMS_INITRD_MD5SUM}\u0026#34; Record the kernel checksum in the IMS_KERNEL_MD5SUM variable.\nSkip this if no kernel file is associated with this image.\nncn-mw# IMS_KERNEL_MD5SUM=$(md5sum \u0026#34;${IMS_KERNEL_FILENAME}\u0026#34; | awk \u0026#39;{ print $1 }\u0026#39;) ncn-mw# echo \u0026#34;${IMS_KERNEL_MD5SUM}\u0026#34; 4. Create image record in IMS Create a new IMS image record for the image.\nncn-mw# cray ims images create --name \u0026#34;${IMS_ROOTFS_FILENAME}\u0026#34; --format toml Example output:\ncreated = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; Create an environment variable for the ID of the new IMS image record.\nSet the IMS_IMAGE_ID variable, using the id field value from the returned data in the previous step.\nncn-mw# IMS_IMAGE_ID=4e78488d-4d92-4675-9d83-97adfc17cb19 5. Upload artifacts to S3 If this procedure is being done as part of NCN Worker Image Customization, then these artifacts should already exist in S3. In this case, skip this section and proceed to Create image manifest file and upload to S3.\nNavigate to the directory containing the artifact files.\nVerify that all image artifacts exist in the current working directory.\nIf necessary, modify the following command to reflect the actual set of artifacts included in the image.\nncn-mw# ls -al \u0026#34;${IMS_ROOTFS_FILENAME}\u0026#34; \u0026#34;${IMS_INITRD_FILENAME}\u0026#34; \u0026#34;${IMS_KERNEL_FILENAME}\u0026#34; Upload the artifacts to S3.\nUpload the SquashFS image root to S3.\nncn-mw# cray artifacts create boot-images \u0026#34;${IMS_IMAGE_ID}/${IMS_ROOTFS_FILENAME}\u0026#34; \u0026#34;${IMS_ROOTFS_FILENAME}\u0026#34; Upload the kernel to S3.\nncn-mw# cray artifacts create boot-images \u0026#34;${IMS_IMAGE_ID}/${IMS_KERNEL_FILENAME}\u0026#34; \u0026#34;${IMS_KERNEL_FILENAME}\u0026#34; Upload the initrd to S3.\nncn-mw# cray artifacts create boot-images \u0026#34;${IMS_IMAGE_ID}/${IMS_INITRD_FILENAME}\u0026#34; \u0026#34;${IMS_INITRD_FILENAME}\u0026#34; 6. Create, upload, and register image manifest HPE Cray uses a manifest file that associates multiple related boot artifacts (kernel, initrd, image root, etc.) into an image description that is used by IMS and other services to boot nodes. Artifacts listed within the manifest are identified by a type value:\napplication/vnd.cray.image.rootfs.squashfs application/vnd.cray.image.initrd application/vnd.cray.image.kernel application/vnd.cray.image.parameters.boot Generate an image manifest file.\nIf necessary, modify the following example to reflect the actual set of artifacts included in the image.\nNote that the following command makes use of several variables that have been set during this procedure. The command must be run from the Bash shell in order for them to be properly evaluated.\nncn-mw# cat \u0026lt;\u0026lt;EOF\u0026gt; manifest.json { \u0026#34;created\u0026#34;: \u0026#34;`date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;`\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;artifacts\u0026#34;: [ { \u0026#34;link\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/${IMS_IMAGE_ID}/${IMS_ROOTFS_FILENAME}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;${IMS_ROOTFS_MD5SUM}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.rootfs.squashfs\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/${IMS_IMAGE_ID}/${IMS_KERNEL_FILENAME}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;${IMS_KERNEL_MD5SUM}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.kernel\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/${IMS_IMAGE_ID}/${IMS_INITRD_FILENAME}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;${IMS_INITRD_MD5SUM}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.initrd\u0026#34; } ] } EOF Upload the manifest to S3.\nncn-mw# cray artifacts create boot-images \u0026#34;${IMS_IMAGE_ID}/manifest.json\u0026#34; manifest.json Update the IMS image record with the image manifest information.\nncn-mw# cray ims images update \u0026#34;${IMS_IMAGE_ID}\u0026#34; \\ --link-type s3 \\ --link-path \u0026#34;s3://boot-images/${IMS_IMAGE_ID}/manifest.json\u0026#34; \\ --format toml Example output:\ncreated = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; [link] type = \u0026#34;s3\u0026#34; path = \u0026#34;s3://boot-images/4e78488d-4d92-4675-9d83-97adfc17cb19/manifest.json\u0026#34; etag = \u0026#34;\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/hardware_state_manager/",
	"title": "Hardware State Manager (HSM)",
	"tags": [],
	"description": "",
	"content": "Hardware State Manager (HSM) The Hardware State Manager (HSM) monitors and interrogates hardware components in the HPE Cray EX system, tracking hardware state and inventory information, and making it available via REST queries and message bus events when changes occur.\nIn the CSM 0.9.3 release, v1 of the HSM API has begun its deprecation process in favor of the new HSM v2 API. Refer to the HSM API documentation for more information on the changes.\n"
},
{
	"uri": "/docs-csm/en-12/operations/conman/troubleshoot_conman_failing_to_connect_to_a_console/",
	"title": "Troubleshoot ConMan Failing to Connect to a Console",
	"tags": [],
	"description": "",
	"content": "Troubleshoot ConMan Failing to Connect to a Console There are many reasons that ConMan may not be able to connect to a specific console. This procedure outlines several things to check that may impact the connectivity with a console.\nPrerequisites This procedure requires administrative privileges.\nProcedure Find the cray-console-operator pod.\nncn-mw# OP_POD=$(kubectl get pods -n services -o wide|grep cray-console-operator|awk \u0026#39;{print $1}\u0026#39;) ncn-mw# echo ${OP_POD} Example output:\ncray-console-operator-6cf89ff566-kfnjr Set the XNAME variable to the component name (xname) of the node whose console is of interest.\nncn-mw# XNAME=\u0026lt;xname\u0026gt; Find the cray-console-node pod that is connecting with the console.\nncn-mw# NODE_POD=$(kubectl -n services exec \u0026#34;${OP_POD}\u0026#34; -c cray-console-operator -- sh -c \u0026#34;/app/get-node ${XNAME}\u0026#34; | jq .podname | sed \u0026#39;s/\u0026#34;//g\u0026#39;) ncn-mw# echo ${NODE_POD} Example output:\ncray-console-node-2 Check for general network availability.\nIf the Kubernetes worker node hosting a cray-console-node pod cannot access the network address of a console, then the connection will fail.\nFind the worker node on which this pod is running.\nncn-mw# WNODE=$(kubectl get pods -o custom-columns=:.spec.nodeName -n services --no-headers \u0026#34;${NODE_POD}\u0026#34;) ncn-mw# echo ${WNODE} Example output:\nncn-w003 SSH to the worker node that the pod is running on.\nncn-mw# ssh \u0026#34;${WNODE}\u0026#34; Check that the BMC for this node is accessible from this worker.\nThe component name (xname) of the BMC is the same as the node, but with the node designation at the end removed. For example, if the node is x3000c0s15b0n0, then the BMC is x3000c0s15b0.\nncn-w# ping BMC_XNAME Example output:\nPING x3000c0s7b0.hmn (10.254.1.7) 56(84) bytes of data. From ncn-m002.hmn (10.254.1.18) icmp_seq=1 Destination Host Unreachable From ncn-m002.hmn (10.254.1.18) icmp_seq=2 Destination Host Unreachable From ncn-m002.hmn (10.254.1.18) icmp_seq=3 Destination Host Unreachable From ncn-m002.hmn (10.254.1.18) icmp_seq=4 Destination Host Unreachable This indicates that there is a network issue between the worker node and the node of interest. When the issue is resolved, the console connection will be reestablished automatically.\nCheck for something else using the serial console connection.\nFor IPMI-based connections, there can only be one active connection at a time. If something else has taken that connection, then ConMan will not be able to connect to it.\nCheck the log information for the node.\nncn-mw# kubectl -n services logs \u0026#34;${NODE_POD}\u0026#34; cray-console-node | grep \u0026#34;${XNAME}\u0026#34; If something else is using the connection, then there will be log entries like the following:\n2021/05/20 15:42:43 INFO: Console [x3000c0s15b0n0] disconnected from \u0026lt;x3000c0s15b0\u0026gt; 2021/05/20 15:43:23 INFO: Unable to connect to \u0026lt;x3000c0s15b0\u0026gt; via IPMI for [x3000c0s15b0n0]: connection timeout 2021/05/20 15:44:24 INFO: Unable to connect to \u0026lt;x3000c0s15b0\u0026gt; via IPMI for [x3000c0s15b0n0]: SOL in use 2021/05/20 15:45:23 INFO: Unable to connect to \u0026lt;x3000c0s15b0\u0026gt; via IPMI for [x3000c0s15b0n0]: SOL in use 2021/05/20 16:13:25 INFO: Unable to connect to \u0026lt;x3000c0s15b0\u0026gt; via IPMI for [x3000c0s15b0n0]: SOL in use Force the connection to become available again.\nThe BMC username and password must be known for this command to work.\nread -s is used to prevent the password from appearing in the command history.\nncn# USERNAME=root ncn# read -r -s -p \u0026#34;BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -H \u0026lt;BMC_XNAME\u0026gt; -U \u0026#34;${USERNAME}\u0026#34; -E -I lanplus sol deactivate Retry ConMan to verify that the connection has been reestablished.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/configuration_management_of_system_components/",
	"title": "Configuration Management of System Components",
	"tags": [],
	"description": "",
	"content": "Configuration Management of System Components The configuration of individual system components is managed with the cray cfs components command. The Configuration Framework Service (CFS) contains a database of the configuration state of available hardware known to the Hardware State Manager (HSM). When new nodes are added to the HSM database, a CFS Hardware Sync Agent enters the component into the CFS database with a null state of configuration.\nAdministrators are able to set a desired CFS configuration for each component, and the CFS Batcher ensures the desired configuration state and the current configuration state match.\nAutomatic Configuration Whenever CFS detects that the desired configuration does not match the current configuration state, CFS Batcher will automatically start a CFS session to apply the necessary configuration. See Configuration Management with CFS Batcher for more information.\nThere are several situations that will cause automatic configuration:\nWhen rebooted, components that have the cfs-state-reporter package installed will register a null current configuration, resulting in a full configuration. When a configuration is updated, all components with that desired configuration will automatically get updates for the layers of the configuration that have changed. If a configuration is only partially applied because of a previous failed configuration session and the component has not exceeded its maximum retries, it will be configured with any layers of the configurations that have not yet been successfully applied. When a user manually resets the configuration state of a component, it will force reconfiguration without rebooting a node. If a manual CFS session applies a version of a playbook that conflicts with the version in the desired configuration, CFS will re-apply the desired version after the manual session is completed. Any other situation that causes the desired state to not match with the current state of a component will trigger automatic configuration. CFS only tracks the current state of components as they are configured by CFS sessions. It does not track configuration state created or modified by other tooling on the system. View Component Configuration Configuration status of a given component (using the component name (xname)) is available through the cray cfs components describe command. The following fields are provided to determine the status and state of the component:\nconfigurationStatus\nThe status of the component\u0026rsquo;s configuration. Valid status values are unconfigured, failed, pending, and configured.\ndesiredConfig\nThe CFS configurations entry assigned to this component.\nenabled\nIndicates whether the component will be configured by CFS or not.\nerrorCount\nThe number of times configuration sessions have failed to configure this component.\nretryPolicy\nThe number of times the configuration will be attempted if it fails. If errorCount \u0026gt;= retryPolicy, CFS will not continue attempts to apply the desiredConfig.\nstate\nThe list of configuration layers that have been applied to the component from the desiredConfig.\nTo view the configuration state of a given component, use the describe command for a given component name (xname):\nncn# cray cfs components describe XNAME --format json Example output:\n{ \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;configurations-example\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s13b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, \u0026#34;state\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;f6a2727a70fdd6d95df6ad9c883188e694d5b37f\u0026#34;, \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:00Z\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;sessionName\u0026#34;: \u0026#34;batcher-6c95df62-2fe3-451b-8cc5-21d3cf748f83\u0026#34; }, { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/another-example.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;282a9bfbf802d7b5c4d9bb5549b6e77957ec37f0\u0026#34;, \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:10Z\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;ncn.yml\u0026#34;, \u0026#34;sessionName\u0026#34;: \u0026#34;batcher-6c95df62-2fe3-451b-8cc5-21d3cf748f83\u0026#34; } ] } When a layer fails to configure, CFS will append a _failed status to the commit field. CFS Batcher will continue to attempt to configure this component with this configuration layer unless the errorCount has reached the retryPolicy limit.\n{ \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/another-example.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;282a9bfbf802d7b5c4d9bb5549b6e77957ec37f0_failed\u0026#34;, \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:20\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;ncn.yml\u0026#34;, \u0026#34;sessionName\u0026#34;: \u0026#34;batcher-74f83dad-9f90-4f5e-bf45-0498ffde8795\u0026#34; } In the event that a playbook is specified in the configuration that does not apply to the specific component, CFS will append _skipped to the commit field.\n{ \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/another-example.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;a8b132fa5ca04cbe1716501d7be38d9b34532a44_skipped\u0026#34;, \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:30\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;sessionName\u0026#34;: \u0026#34;batcher-e3152e08-77df-4719-9b15-4fd5ad696730\u0026#34; } If a playbook exits early because of the Ansible any_errors_fatal setting, CFS will append _incomplete to the commit field for all components that did not cause the failure. This situation would most likely occur only when using an Ansible linear playbook execution strategy.\n{ \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/another-example.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;282a9bfbf802d7b5c4d9bb5549b6e77957ec37f0_incomplete\u0026#34;, \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:40\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;sessionName\u0026#34;: \u0026#34;batcher-6c95df62-2fe3-451b-8cc5-21d3cf748f83\u0026#34; } Force Component Reconfiguration To force a component which has a specific desiredConfig to a different configuration, use the update subcommand to change the configuration:\nncn# cray cfs components update XNAME --desired-config new-config IMPORTANT: Ensure that the new configuration has been created with the cray cfs configurations update new-config command before assigning the configuration to any components.\nTo force a component to retry its configuration again after it failed, change the errorCount to less than the retryPolicy, or raise the retryPolicy. If the errorCount has not reached the retry limit, CFS will automatically keep attempting the configuration and no action is required.\nncn# cray cfs components update XNAME --error-count 0 Disable Component Configuration To disable CFS configuration of a component, use the --enabled option:\nWARNING: When a node reboots and the state-reporter reports in to CFS, it will automatically enable configuration. The following command only disables configuration until a node reboots.\nncn# cray cfs components update XNAME --enabled false Use --enabled true to re-enable the component.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_message_about_invalid_eeprom_checksum_in_node_console_or_log/",
	"title": "Compute Node Boot Issue Symptom Message About Invalid EEPROM Checksum in Node Console or Log",
	"tags": [],
	"description": "",
	"content": "Compute Node Boot Issue Symptom: Message About Invalid EEPROM Checksum in Node Console or Log On rare occasions, the processor hardware may lose the Serial Over Lan (SOL) connections and may need to be reseated to allow the node to successfully boot.\nSymptoms This issue can be identified if the following is displayed in the node\u0026rsquo;s console or log:\nconsole.38:2018-09-08 04:54:51 [ 16.721165] ixgbe 0000:18:00.0: The EEPROM Checksum Is Not Valid console.38:2018-09-08 04:55:00 [ 25.768872] ixgbe 0000:18:00.1: The EEPROM Checksum Is Not Valid The following figure shows that the EEPROM checksum errors lead to a dracut-initqueue timeout, and eventually cause the node to drop into the dracut emergency shell.\nProblem Detection Run dmidecode from the compute node to identify its model. H87926-500 is the silver model that may exhibit this issue, whereas the production model, H87926-550, does not exhibit SOL connection issues.\ncmp4:~ # dmidecode|grep H87926 Example output:\nVersion: H87926-550 Resolution One way to resolve this issue is to ensure that the latest ixgbe network driver is installed on the nodes.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/create_a_uai/",
	"title": "Create a UAI",
	"tags": [],
	"description": "",
	"content": "Create a UAI The UAS allows either administrators or authorized users using the Legacy Mode of UAI management to create UAIs. This section shows both methods.\nIt is rare that an an administrator would hand-craft an End-User UAI using this administrative procedure, but it is possible. This is, however, the procedure used to create Broker UAIs for Broker Mode UAI Management.\nPrerequisites For administrative procedures:\nThe administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) For the administrative procedure, the administrator must know at least the UAI Class ID to use in creating the UAI, or A default UAI Class must be defined that creates the desired class of UAI Optional: the administrator may choose a site defined name for the UAI to be used in conjunction with the HPE Cray EX System External DNS mechanism. This is only meaningful for UAIs presented on a public IP address.\nFor Legacy Mode user procedures:\nThe user must be logged into a host that has user access to the HPE Cray EX System API Gateway The user must have an installed initialized cray CLI and network access to the API Gateway The user must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The user must be logged in as to the HPE Cray EX System CLI (cray auth login command) The user must have a public SSH key configured on the host from which SSH connections to the UAI will take place The user must have access to a file containing the above public SSH key Procedure Create a UAI administratively.\nUse a command of the following form:\nncn# cray uas admin uais create OPTIONS The following OPTIONS are available for use:\n--class-id \u0026lt;class-id\u0026gt; - The class of the UAI to be created. This option must be specified unless a default UAI class exists, in which case, it can be omitted and the default will be used. --owner '\u0026lt;user-name\u0026gt;' - Create the UAI as owned by the specified user. --passwd str '\u0026lt;passwd-string\u0026gt;' - Specify the /etc/password format string for the user who owns the UAI. This will be used to set up credentials within the UAI for the owner when the owner logs into the UAI. --publickey-str '\u0026lt;public-ssh-key\u0026gt;' - Specify the SSH public key that will be used to authenticate with the UAI. The key should be, for example, the contents of an id_rsa.pub file used by SSH. --uai-name TEXT - Specify an optional name to be assigned to the UAI on creation. If this is not specified, a default name of the form \u0026lt;owner\u0026gt;-uai-\u0026lt;short-uuid\u0026gt; is used. The UAI name is used both as the name of the UAI in the UAS and as the external DNS hostname of a publicly accessible UAI. If the requested UAI name is the same as an already existing UAI, no new UAI is created, but the information about the existing UAI is returned. UAI names may contain up to 63 lower case alphanumeric or - characters, and must start and end with an alphanumeric character. Create a UAI in the Legacy Mode of UAI Management.\nUse a command of the following form:\nncn# cray uas create OPTIONS The following OPTIONS are available for use:\n--publickey \u0026lt;path\u0026gt; - the path to a file containing the public SSH key to be used to talk to this UAI. This option is required and must specify a valid public key file name. --ports \u0026lt;port-list\u0026gt; - a comma-separated list of TCP port numbers to be opened on the newly created UAI. This option is not required and will be overridden by a default UAI Class if a default UAI Class is configured. --imagename \u0026lt;uai-image-name\u0026gt; - The name of the UAI container image to be used to create the UAI. This option is not required. If omitted the default UAI image will be used. Both the default UAI image and anything specified here will be overridden by a default UAI Class if a default UAI Class is configured. Top: User Access Service (UAS)\nNext Topic: Examining a UAI Using a Direct Administrative Command\n"
},
{
	"uri": "/docs-csm/en-12/operations/csm_product_management/validate_signed_rpms/",
	"title": "Validate Signed RPMs",
	"tags": [],
	"description": "",
	"content": "Validate Signed RPMs The HPE Cray EX system signs RPMs to provide an extra level of security. Use the following procedure to import a key from either My HPE Software Center or a Kubernetes Secret, and then use that key to validate the RPM package signatures on each node type.\nThe RPMs will vary on compute, application, worker, master, and storage nodes. Check each node type to ensure the RPMs are correctly signed.\nProcedure Retrieve the signing key required to validate the RPMs.\nUse either the My HPE Software Center or Kubernetes Secret method to find the signing key.\nMy HPE Software Center:\nDownload the signing key.\nncn-mw# curl LINK_TO_KEY_IN_My_HPE_Software_Center Kubernetes Secret:\nFind the key and write it to a file.\nncn-mw# kubectl -n services get secrets hpe-signing-key -o jsonpath=\u0026#39;{.data.gpg-pubkey}\u0026#39; | base64 -d | tee hpe-signing-key.asc Example output:\n-----BEGIN PGP PUBLIC KEY BLOCK----- Version: GnuPG v2.0.22 (GNU/Linux) mQENBFZp0YMBCADNNhdrR/K7jk6iFh/D/ExEumPSdriJwDUlHY70bkEUChLyRACI QfLEmh0dGoUfFu2Uk8M/RgeGPayJUeO3jeJw/y7JJHvZENJwYjquZKTOza7GLXf6 HyCRanHrEeXeyRffhJlXLf6GvCqYVl9nSvxwSX9raotqMznLY5E1JXIqtfHLrVhJ qHQLiKulpEAHL9pOWamwZKeGbL9M/N6O3LINbHqisiC0EIcV6GIFCLSfCFMODO6C PgkJ/ECVLZEjGDFnSTT0mn5+DveqRUid/+YQejcraKlc3xRUF+qlg4ey+uz0kFzC SFUbKY68Pw6W/dFGrEhfau8A0TnMnIQ4qgLPABEBAAG0P0hld2xldHQgUGFja2Fy ZCBFbnRlcnByaXNlIENvbXBhbnkgUlNBLTIwNDgtMzAgPHNpZ25ocEBocGUuY29t PokBPQQTAQIAJwUCVmnRgwIbLwUJEswDAAYLCQgHAwIGFQgCCQoLAxYCAQIeAQIX gAAKCRDU2uHjnaOfRK5XCACRJLoMQ/nBa7Gna/96inbAHoKM6DUbNramBa1XCTeh KiTxA0bPE3kp7y143jpfOiSGAOTcU0RaCOKk6JMnJJMt60nR4UohVG2lLVtLxT0G H75jCu0nuZQJrKlMh04fJ3zHnqVuOduyUstgmMQ0qVg2lwPTV+KZeY5/eNPHzkcK 75pfos/svDRQNN2LX6qzsVWfAkEN/WdnlZJE76exvA9JsVmNtU3h3PKQTT86W4bb 1MdeDMkX9lDwMCEhClxLVU/sUfj10Kb8CO5+TFimmdqgXXY4BJJsE8STowy67t7Q zECkM4UFVpgcXFrapWW7IniC1OP0c4I+11mnHKCN15DFuQINBFZp0YMQCADo0UHN pIORPOLtaVI2W/aBpOWVlO74HZvMlWKOk+isf8pIKOujivncNZeeVPu2MTT7kOZ6 3Iwuj6B/dBz0hFXkqfzww+ibkhV1NWUx8Gk3FnGm6Ye6VZq2MbYHFMjSMbH3gJNd l76n4wOdwzC8TbLSmfIVxRyf+Uo5GhMrFy/+G28m/WO5nmH/AxKZxOp//NUVxE47 p6Dd2Rqg2IgBfQ99gudh75F/s6RHDYtV+87CsyFyKD7nJW54l/7r9jvvwhO0d89T s37j+bv81AEPYtu17uaRCcfF2B6RtPEdDslZ+J0G14TBsjp53ARh43HmH6BwQ3+4 pyB7QYWwN2ybFCqTAAMFCAC+1JtxaR7TEZsRDNy6ViHH+fHENl7+SB8GTQL7BZXB YgFEtsti+NZpkAAiJ+HXZihgcjCrHPejnlj5Su7dSkveRLHKZbVehvIbiM+LxfNv 7CdxfhLUVPkgPEpiCHGpCHjG/bKyKCL48SDPB5ClUtVu7v05dq/yu4AYaWwU1iix uH9dYQWC1J8pkZX/igHdbD/RYnPMuiil41guTNSWgjzxbOnxEVueaYFKnHdFlqz7 JpzJa10Lm9gEcGmzePVbJH0j8/+1ViwqLhbITq7Gv1S+RkNnewjLM9Vu2R/Fvpzh AUAinTEi5bYPtmVtddZQ94cOFLvh+LrETAC7v4zxvW/ciQElBBgBAgAPBQJWadGD AhsMBQkSzAMAAAoJENTa4eOdo59E6kAIAMC60HIPrr7ztUAF1vmuIdgSMDAjD7y0 UOzCm1L9fuHqeXNc/JQkKbqAv0tMjnRtrt1R13N3qy1qBeUTnG0qxwdHR0jsknHW S/1T24x03XioypowQObeh15PTD/TTAiLherzAWRNqqtf2Yh9Dy2zWLo204FQjK// Apw4IbO28hgYWvIbpFsyPG4WED3uJ7uTnkqdRkNWQl3M3J1GhEycgoXe703hllBP j2iOwecHkFHN2GJjAL67IH2amnp0JqrVy6FwN1fL47lOUfe3AgkjBmBUXT+r0y+e L+aILxdSiFNXn3sqpW2jQnT3r+UOCw5QdOYE8QC2VnJcm0p3bJ+OMVQ= =pzE0 -----END PGP PUBLIC KEY BLOCK----- Verify that HPE is the issuer of the signed packages.\nReplace the PATH-TO-KEY value in the following command with the path to the signing key.\nncn-mw# rpm -qpi PATH-TO-KEY/hpe-signing-key.asc Example output:\nName : gpg-pubkey Version : 9da39f44 Release : 5669d183 Architecture: (none) Install Date: Thu 25 Feb 2021 08:58:19 AM CST Group : Public Keys Size : 0 License : pubkey Signature : (none) Source RPM : (none) Build Date : Thu 10 Dec 2015 01:24:51 PM CST Build Host : localhost Relocations : (not relocatable) Packager : Hewlett Packard Enterprise Company RSA-2048-30 \u0026lt;signhp@hpe.com\u0026gt; Summary : gpg(Hewlett Packard Enterprise Company RSA-2048-30 \u0026lt;signhp@hpe.com\u0026gt;) Description : -----BEGIN PGP PUBLIC KEY BLOCK----- Version: rpm-4.11.3 (NSS-3) mQENBFZp0YMBCADNNhdrR/K7jk6iFh/D/ExEumPSdriJwDUlHY70bkEUChLyRACI QfLEmh0dGoUfFu2Uk8M/RgeGPayJUeO3jeJw/y7JJHvZENJwYjquZKTOza7GLXf6 HyCRanHrEeXeyRffhJlXLf6GvCqYVl9nSvxwSX9raotqMznLY5E1JXIqtfHLrVhJ qHQLiKulpEAHL9pOWamwZKeGbL9M/N6O3LINbHqisiC0EIcV6GIFCLSfCFMODO6C PgkJ/ECVLZEjGDFnSTT0mn5+DveqRUid/+YQejcraKlc3xRUF+qlg4ey+uz0kFzC SFUbKY68Pw6W/dFGrEhfau8A0TnMnIQ4qgLPABEBAAG0P0hld2xldHQgUGFja2Fy ZCBFbnRlcnByaXNlIENvbXBhbnkgUlNBLTIwNDgtMzAgPHNpZ25ocEBocGUuY29t PokBPQQTAQIAJwUCVmnRgwIbLwUJEswDAAYLCQgHAwIGFQgCCQoLAxYCAQIeAQIX gAAKCRDU2uHjnaOfRK5XCACRJLoMQ/nBa7Gna/96inbAHoKM6DUbNramBa1XCTeh KiTxA0bPE3kp7y143jpfOiSGAOTcU0RaCOKk6JMnJJMt60nR4UohVG2lLVtLxT0G H75jCu0nuZQJrKlMh04fJ3zHnqVuOduyUstgmMQ0qVg2lwPTV+KZeY5/eNPHzkcK 75pfos/svDRQNN2LX6qzsVWfAkEN/WdnlZJE76exvA9JsVmNtU3h3PKQTT86W4bb 1MdeDMkX9lDwMCEhClxLVU/sUfj10Kb8CO5+TFimmdqgXXY4BJJsE8STowy67t7Q zECkM4UFVpgcXFrapWW7IniC1OP0c4I+11mnHKCN15DFuQINBFZp0YMQCADo0UHN pIORPOLtaVI2W/aBpOWVlO74HZvMlWKOk+isf8pIKOujivncNZeeVPu2MTT7kOZ6 3Iwuj6B/dBz0hFXkqfzww+ibkhV1NWUx8Gk3FnGm6Ye6VZq2MbYHFMjSMbH3gJNd l76n4wOdwzC8TbLSmfIVxRyf+Uo5GhMrFy/+G28m/WO5nmH/AxKZxOp//NUVxE47 p6Dd2Rqg2IgBfQ99gudh75F/s6RHDYtV+87CsyFyKD7nJW54l/7r9jvvwhO0d89T s37j+bv81AEPYtu17uaRCcfF2B6RtPEdDslZ+J0G14TBsjp53ARh43HmH6BwQ3+4 pyB7QYWwN2ybFCqTAAMFCAC+1JtxaR7TEZsRDNy6ViHH+fHENl7+SB8GTQL7BZXB YgFEtsti+NZpkAAiJ+HXZihgcjCrHPejnlj5Su7dSkveRLHKZbVehvIbiM+LxfNv 7CdxfhLUVPkgPEpiCHGpCHjG/bKyKCL48SDPB5ClUtVu7v05dq/yu4AYaWwU1iix uH9dYQWC1J8pkZX/igHdbD/RYnPMuiil41guTNSWgjzxbOnxEVueaYFKnHdFlqz7 JpzJa10Lm9gEcGmzePVbJH0j8/+1ViwqLhbITq7Gv1S+RkNnewjLM9Vu2R/Fvpzh AUAinTEi5bYPtmVtddZQ94cOFLvh+LrETAC7v4zxvW/ciQElBBgBAgAPBQJWadGD AhsMBQkSzAMAAAoJENTa4eOdo59E6kAIAMC60HIPrr7ztUAF1vmuIdgSMDAjD7y0 UOzCm1L9fuHqeXNc/JQkKbqAv0tMjnRtrt1R13N3qy1qBeUTnG0qxwdHR0jsknHW S/1T24x03XioypowQObeh15PTD/TTAiLherzAWRNqqtf2Yh9Dy2zWLo204FQjK// Apw4IbO28hgYWvIbpFsyPG4WED3uJ7uTnkqdRkNWQl3M3J1GhEycgoXe703hllBP j2iOwecHkFHN2GJjAL67IH2amnp0JqrVy6FwN1fL47lOUfe3AgkjBmBUXT+r0y+e L+aILxdSiFNXn3sqpW2jQnT3r+UOCw5QdOYE8QC2VnJcm0p3bJ+OMVQ= =pzE0 -----END PGP PUBLIC KEY BLOCK----- Import the signing key.\nncn-mw# rpm --import hpe-signing-key.asc Search for the signed packages using the version number from the previous step.\nncn-mw# rpm -qa --qf \u0026#39;%{NAME}-%{VERSION}-%{RELEASE} %{SIGGPG:pgpsig}\\n\u0026#39; | grep \u0026#39;9da39f44\u0026#39; Validate the signature on an RPM.\nThe RPM in this example is csm-install-workarounds-0.1.11-20210504151148_bf748be.src.rpm.\nncn-mw# rpm -Kvv csm-install-workarounds-0.1.11-20210504151148_bf748be.src.rpm Example output:\nD: loading keyring from pubkeys in /var/lib/rpm/pubkeys/*.key D: couldn\u0026#39;t find any keys in /var/lib/rpm/pubkeys/*.key D: loading keyring from rpmdb D: opening db environment /var/lib/rpm cdb:0x401 D: opening db index /var/lib/rpm/Packages 0x400 mode=0x0 D: locked db index /var/lib/rpm/Packages D: opening db index /var/lib/rpm/Name 0x400 mode=0x0 D: read h# 442 Header SHA1 digest: OK (489efff35e604042709daf46fb78611fe90a75aa) D: added key gpg-pubkey-f4a80eb5-53a7ff4b to keyring D: read h# 493 Header SHA1 digest: OK (29ff3649c04c90eb654c1b3b8938e4940ff1fbbd) D: added key gpg-pubkey-4255bf0c-5ec2e252 to keyring D: read h# 494 Header SHA1 digest: OK (e934d6983ae30a7e12c9c1fb6e86abb1c76c69d3) D: added key gpg-pubkey-9da39f44-5669d183 to keyring D: read h# 496 Header SHA1 digest: OK (a93ccf43d5479ff84dc896a576d6f329fd7d723a) D: added key gpg-pubkey-e09422b3-57744e9e to keyring D: read h# 497 Header SHA1 digest: OK (019de42112ea85bfa979968273aafeca8d457936) D: added key gpg-pubkey-fd4bf915-5f573efe to keyring D: Using legacy gpg-pubkey(s) from rpmdb D: Expected size: 36575 = lead(96)+sigs(5012)+pad(4)+data(31463) D: Actual size: 36575 csm-install-workarounds-0.1.11-20210504151148_bf748be.src.rpm: Header V4 RSA/SHA256 Signature, key ID 9da39f44: OK Header SHA1 digest: OK (87c62923c905424eaddac56c5dda7f3b6421d30d) V4 RSA/SHA256 Signature, key ID 9da39f44: OK MD5 digest: OK (130e13f11aaca834408665a93b61a8e4) D: closed db index /var/lib/rpm/Name D: closed db index /var/lib/rpm/Packages D: closed db environment /var/lib/rpm "
},
{
	"uri": "/docs-csm/en-12/install/configure_administrative_access/",
	"title": "Configure Administrative Access",
	"tags": [],
	"description": "",
	"content": "Configure Administrative Access There are several operations which configure administrative access to different parts of the system. Ensuring that the cray CLI can be used with administrative credentials enables use of many management services via commands. The management nodes can be locked from accidental manipulation by the cray capmc and cray fas commands when the intent is to work on the entire system except the management nodes. The cray scsd command can change the SSH keys, NTP server, syslog server, and BMC/controller passwords.\nTopics Configure Keycloak account Configure the Cray command line interface Set Management role on the BMCs of management nodes Lock management nodes Configure BMC and controller parameters with SCSD Configure non-compute nodes with CFS Upload Olympus BMC recovery firmware into TFTP server Proceed to next topic NOTE: The procedures in this section of installation documentation are intended to be done in order, even though the topics are administrative or operational procedures. The topics themselves do not have navigational links to the next topic in the sequence.\n1. Configure Keycloak account Upcoming steps in the installation workflow require an account to be configured in Keycloak for authentication. This can be either a local Keycloak account or an external Identity Provider (IdP), such as LDAP. Having an account in Keycloak with administrative credentials enables the use of many management services via the cray command.\nSee Configure Keycloak Account.\n2. Configure the Cray command line interface The cray command line interface (CLI) is a framework created to integrate all of the system management REST APIs into easily usable commands.\nLater procedures in the installation workflow use the cray command to interact with multiple services. The cray CLI configuration needs to be initialized for the Linux account. The Keycloak user who initializes the CLI configuration needs to be authorized for administrative actions.\nSee Configure the Cray command line interface.\n3. Set Management role on the BMCs of management nodes The BMCs that control management nodes will not have been marked with the Management role in HSM. It is important to mark them with the Management role so that they can be easily included in the locking/unlocking operations required as protections for FAS and CAPMC actions.\nSet BMC Management roles now!\nSee Set BMC Management Role.\n4. Lock management nodes The management nodes are unlocked at this point in the installation. Locking the management nodes and their BMCs will prevent actions from FAS to update their firmware or CAPMC to power off or do a power reset. Doing any of these by accident will take down a management node. If the management node is a Kubernetes master or worker node, this can have serious negative effects on system operation.\nIf a single node is taken down by mistake, it is possible that things will recover. However, if all management nodes are taken down, or all Kubernetes worker nodes are taken down by mistake, the system is dead and has to be completely restarted.\nLock the management nodes now!\nRun the lock_management_nodes.py script to lock all management nodes and their BMCs that are not already locked:\nncn-mw# /opt/cray/csm/scripts/admin_access/lock_management_nodes.py The return value of the script is 0 if locking was successful. Otherwise, a non-zero return means that manual intervention may be needed to lock the nodes and their BMCs.\nFor more information about locking and unlocking nodes, see Lock and Unlock Nodes.\n5. Configure BMC and controller parameters with SCSD NOTE: If there are no liquid-cooled cabinets present in the HPE Cray EX system, then this step can be skipped.\nThe System Configuration Service (SCSD) allows administrators to set various BMC and controller parameters for components in liquid-cooled cabinets. At this point in the install, SCSD should be used to set the SSH key in the node controllers (BMCs) to enable troubleshooting. If any of the nodes fail to power down or power up as part of the compute node booting process, it may be necessary to look at the logs on the BMC for node power down or node power up.\nSee Configure BMC and Controller Parameters with SCSD.\n6. Configure non-compute nodes with CFS Non-compute Nodes (NCN) need to be configured after booting for administrative access, security, and other purposes. The Configuration Framework Service (CFS) is used to apply post-boot configuration in a decoupled, layered manner. Individual software products including CSM provide one or more layers of configuration in a process called \u0026ldquo;NCN personalization\u0026rdquo;.\nSee Configure Non-Compute Nodes with CFS.\n7. Upload Olympus BMC recovery firmware into TFTP server NOTE: This step requires the CSM software, Cray CLI, and HPC Firmware Pack (HFP) to be installed. If these are not currently installed, then skip this step and perform it later.\nThe Olympus hardware needs to have recovery firmware loaded to the cray-tftp server in case the BMC loses its firmware. The BMCs are configured to load a recovery firmware from a TFTP server. This procedure does not modify any BMC firmware, but only stages the firmware on the TFTP server for download in the event it is needed.\nSee Load Olympus BMC Recovery Firmware into TFTP server.\n8. Proceed to next topic After completing the operational procedures above which configure administrative access, the next step is to validate the health of management nodes and CSM services.\nSee Validate CSM Health.\n"
},
{
	"uri": "/docs-csm/en-12/background/ncn_plan_of_record/",
	"title": "NCN Plan of Record",
	"tags": [],
	"description": "",
	"content": "NCN Plan of Record This document outlines the hardware necessary to meet CSM\u0026rsquo;s Plan of Record (PoR). This serves as the minimum, necessary pieces required per each server in the management plane.\nIf the system\u0026rsquo;s NICs do not align to the PoR NICs outlined below (e.g. Onboard NICs are used instead of PCIe), then follow Customize PCIe Hardware before booting the NCN(s). If there are more disks than what is listed below in the PoR for disks, then follow Customize Disk Hardware before booting the NCN(s). Masters NCNs Disks NICs Workers NCNs Disks NICs Storage NCNs Disks NICs NOTE: Several components below are necessary to provide redundancy in the event of hardware failure.\nMasters NCNs Master disks Operating System: 2 SSDs of equal size, and less than 500 GiB (524288000000 bytes) ETCD: 1 SSD smaller than 500 GiB (524288000000 bytes) (This disk will be fully encrypted with LUKS2) Master NICs NOTE: The 2nd port on each card is unused/empty (reserved for future use).\nManagement Network: 2 PCIe cards, with 1 or 2 heads/ports each for a total of 4 ports split between two PCIe cards Workers NCNs Worker disks Operating System: 2 SSDs of equal size, and less than 500 GiB (524288000000 bytes) Ephemeral: 1 SSD larger than 1 TiB (1048576000000 bytes) Worker NICs NOTE: There is no PCIe redundancy for the management network for worker NCNs. The only redundancy set up for workers is port redundancy.\nManagement Network: 1 PCIe card with 2 heads/ports for a total of 2 ports dedicated to a single PCIe card High-Speed Network: 1 PCIe card capable of 100 Gbps (e.g. ConnectX-5 or Cassini), with 1 or 2 heads/ports Storage NCNs Storage disks Operating System: 2 SSDs of equal size, and less than 500 GiB (524288000000 bytes) Ceph: 8 SSDs of any size NOTE: Any available disk that is not consumed by the operating system will be used for Ceph, but a node needs a minimum of 8 disks for making an ideal Ceph pool for CSM.\nStorage NICs NOTE: The 2nd port on each card is filled but not configured (reserved for future use).\nManagement Network: 2 PCIe cards, each with 2 heads/ports for a total of 4 ports split between two PCIe cards "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/gitea_web_ui_requires_two_logins/",
	"title": "Known Issue Logging into the Gitea web UI requires logging in twice",
	"tags": [],
	"description": "",
	"content": "Known Issue: Logging into the Gitea web UI requires logging in twice When using the Gitea web UI, users are redirected to a keycloak login. The redirect is expected; however, the expectation is that logging in on this page should log users into Gitea. Unfortunately, that does not happen. Instead users are being redirected back to Gitea and have to login again through the Gitea web page using the existing git credentials. To obtain the git credentials for Gitea see the Version Control Service documentation. Currently, logging in twice is the only workaround.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/cephadm_reference_material/",
	"title": "Cephadm Reference Material",
	"tags": [],
	"description": "",
	"content": "Cephadm Reference Material cephadm is a new function introduced in Ceph Octopus 15. It allows for an easier method to install and manage Ceph nodes.\nThe following sections include common examples:\nInvoke Shells to Run Traditional Ceph Commands On ncn-s001/2/3:\nncn-s00[123]# cephadm shell # creates a container with access to run ceph commands the traditional way Optionally, execute the following command:\nncn-s00[123]# cephadm shell -- ceph -s Ceph-Volume There are multiple ways to do Ceph device operations now.\nUse cephadm ncn-s# cephadm ceph-volume Use cephadm shell Optionally, this can be done by invoking a cephadm shell by appending a ceph command to the cephadm command.\nncn-s# cephadm shell -- ceph-volume Use ceph orch Optionally, the following command will allow users to specify a single node name to just list that nodes drives.\nncn-s00[123]# ceph orch device ls ncn-s00[123]# ceph orch device ls \u0026lt;node name\u0026gt; "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/change_ex_liquid-cooled_cabinet_global_default_password/",
	"title": "Change Cray EX Liquid-Cooled Cabinet Global Default Password",
	"tags": [],
	"description": "",
	"content": "Change Cray EX Liquid-Cooled Cabinet Global Default Password This procedure changes the global default root credential on HPE Cray EX liquid-cooled cabinet embedded controllers (BMCs). The chassis management module (CMM) controller (cC), node controller (nC), and Slingshot switch controller (sC) are generically referred to as \u0026ldquo;BMCs\u0026rdquo; in these procedures.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray Command Line Interface (cray CLI) for more information. Review procedures in Manage System Passwords. Procedure If necessary, shut down compute nodes in each cabinet. Refer to Shut Down and Power Off Compute and User Access Nodes.\nncn-mw# sat bootsys shutdown --stage bos-operations --bos-templates COS_SESSION_TEMPLATE Disable the hms-discovery Kubernetes cron job.\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : true }}\u0026#39; Power off all compute slots in the cabinets the passwords are to be changed on.\nNote: If a chassis is not fully populated, specify each slot individually.\nExample showing fully populated cabinets 1000-1003:\nncn-mw# cray capmc xname_off create --xnames x[1000-1003]c[0-7]s[0-7] --format json Check the power status:\nncn-mw# cray capmc get_xname_status create --xnames x[1000-1003]c[0-7]s[0-7] --format json Continue when all compute slots are Off.\nPerform the procedures in Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials.\nPerform the procedures in Updating the Liquid-Cooled EX Cabinet Default Credentials after a CEC Password Change.\nTo update Slingshot switch BMCs, refer to \u0026ldquo;Change Rosetta Login and Redfish API Credentials\u0026rdquo; in the Slingshot Operations Guide (\u0026gt; 1.6.0).\n"
},
{
	"uri": "/docs-csm/en-12/operations/power_management/recover_from_a_liquid_cooled_cabinet_epo_event/",
	"title": "Recover from a Liquid Cooled Cabinet EPO Event",
	"tags": [],
	"description": "",
	"content": "Recover from a Liquid Cooled Cabinet EPO Event Identify an emergency power off (EPO) has occurred and restore cabinets to a healthy state.\nCAUTION: Verify the reason why the EPO occurred and resolve that problem before clearing the EPO state.\nIf a Cray EX liquid-cooled cabinet or cooling group experiences an EPO event, the compute nodes may not boot. Use CAPMC to force off all the chassis affected by the EPO event.\nProcedure Verify that the EPO event did not damage the system hardware.\nCheck the status of the chassis.\nncn-mw# cray capmc get_xname_status create --xnames x9000c[1,3] --format toml Example output:\ne = 0 err_msg = \u0026#34;\u0026#34; off = [ \u0026#34;x9000c1\u0026#34;, \u0026#34;x9000c3\u0026#34;,] Check the Chassis Controller Module (CCM) log for Critical messages and the EPO event.\nA cabinet has eight chassis.\nncn-mw# kubectl logs -n services -l app.kubernetes.io/name=cray-capmc -c cray-capmc --tail -1 | grep EPO -A 10 Example output:\n2019/10/24 02:37:30 capmcd.go:805: Message: Can not issue Enclosure Chassis.Reset \u0026#39;On\u0026#39;|\u0026#39;Off\u0026#39; while in EPO state 2019/10/24 02:37:30 capmcd.go:808: ExtendedInfo.Message: Can not issue Enclosure Chassis.Reset \u0026#39;On\u0026#39;|\u0026#39;Off\u0026#39; while in EPO state 2019/10/24 02:37:30 capmcd.go:809: ExtendedInfo.Resolution: Verify physical hardware, issue Enclosure Chassis.Reset --\u0026gt; \u0026#39;ForceOff\u0026#39;, and resubmit the request 2019/10/24 02:37:31 capmcd.go:136: Info: \u0026lt;-- Bad Request (400) POST https://x1000c7b0/redfish/v1/ Chassis/Enclosure/Actions/Chassis.Reset (1.045967005s) 2019/10/24 02:37:31 capmcd.go:799: POST https://x1000c7b0/redfish/v1/Chassis/Enclosure/Actions/Chassis.Reset !HTTP Error! Disable the hms-discovery Kubernetes CronJob.\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : true }}\u0026#39; CAUTION: Do not power the system on until it is safe to do so. Determine why the EPO event occurred before clearing the EPO state.\nIf it is safe to power on the hardware, clear all chassis in the EPO state in the cooling group.\nAll chassis in cabinets 1000-1003 are forced off in this example. Power off all chassis in a cooling group simultaneously, or the EPO condition may persist.\nncn-mw# cray capmc xname_off create --xnames x[1000-1003]c[0-7] --force true --format toml Example output:\ne = 0 err_msg = \u0026#34;\u0026#34; The HPE Cray EX EX TDS cabinet contains only two chassis: 1 (bottom) and 3 (top).\nncn-mw# cray capmc xname_off create --xnames x9000c[1,3] --force true --format toml Example output:\ne = 0 err_msg = \u0026#34;\u0026#34; Restart the hms-discovery CronJob.\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : false }}\u0026#39; About 5 minutes after hms-discovery restarts, the service will power on the chassis enclosures, switches, and compute blades. If components are not being powered back on, then power them on manually.\nncn-mw# cray capmc xname_on create --xnames x[1000-1003]c[0-7]r[0-7],x[1000-1003]c[0-7]s[0-7] --prereq true --continue true --format toml Example output:\ne = 0 err_msg = \u0026#34;\u0026#34; Verify the Slingshot fabric is up and healthy.\nRefer to the following documentation for more information on how to verify the health of the Slingshot Fabric:\nThe Slingshot Administration Guide PDF for HPE Cray EX systems. The Slingshot Troubleshooting Guide PDF. After the components have powered on, boot the nodes using the Boot Orchestration Services (BOS).\nSee Power On and Boot Compute and User Access Nodes.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/replace_a_compute_blade/",
	"title": "Replace a Compute Blade",
	"tags": [],
	"description": "",
	"content": "Replace a Compute Blade Replace an HPE Cray EX liquid-cooled compute blade.\nShutdown software and power off the blade Temporarily disable endpoint discovery service (MEDS) for the compute nodes(s) being replaced. This example disables MEDS for the compute node in cabinet 1000, chassis 3, slot 0 (x1000c3s0b0). If there is more than 1 node card, in the blade specify each node card (x1000c3s0b0, x1000c3s0b1).\nncn-mw# cray hsm inventory redfishEndpoints update --enabled false x1000c3s0b0 Verify that the workload manager (WLM) is not using the affected nodes.\nUse Boot Orchestration Services (BOS) to shut down the affected nodes. Specify the appropriate BOS template for the node type.\nncn-mw# cray bos v1 session create --template-uuid BOS_TEMPLATE \\ --operation shutdown --limit x1000c3s0b0n0,x1000c3s0b0n1,x1000c3s0b1n0,x1000c3s0b1n1 Specify all the nodes in the blade using a comma-separated list. This example shows the command to shut down an EX425 compute blade (Windom) in cabinet 1000, chassis 3, slot 5. This blade type includes two node cards, each with two logical nodes (4 processors).\nDisable the chassis slot in the Hardware State Manager (HSM).\nThis example shows cabinet 1000, chassis 3, slot 0 (x1000c3s0).\nncn-mw# cray hsm state components enabled update --enabled false x1000c3s0 Disabling the slot prevents hms-discovery from attempting to automatically power on slots. If the slot automatically powers on after using CAPMC to power the slot off, then temporarily suspend the hms-discovery cronjob in Kubernetes:\nSuspend the hms-discovery cronjob to prevent slot power on.\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : true }}\u0026#39; Verify that the hms-discovery cronjob has stopped (ACTIVE column = 0).\nncn-mw# kubectl get cronjobs -n services hms-discovery Example output:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE^M hms-discovery */3 * * * * True 0 117s 15d Use CAPMC to power off slot 0 in chassis 3.\nncn-mw# cray capmc xname_off create --xnames x1000c3s0 --recursive true --format json Delete the HSM entries Delete the node Ethernet interface MAC addresses and the Redfish endpoint from the Hardware State Manager (HSM).\nIMPORTANT: The HSM stores the node\u0026rsquo;s BMC NIC MAC addresses for the hardware management network and the node\u0026rsquo;s Ethernet NIC MAC addresses for the node management network. The MAC addresses for the node NICs must be updated in the DHCP/DNS configuration when a liquid-cooled blade is replaced. Their entries must be deleted from the HSM Ethernet interfaces table and be rediscovered. The BMC NIC MAC addresses for liquid-cooled blades are assigned algorithmically and should not be deleted from the HSM.\nFor each node delete the node\u0026rsquo;s NIC MAC addresses from the HSM Ethernet interfaces table.\nQuery HSM to determine the node\u0026rsquo;s NIC MAC addresses associated with the blade in cabinet 1000, chassis 3, slot 0, node card 0, node 0.\nncn-mw# cray hsm inventory ethernetInterfaces list --component-id x1000c3s0b0n0 --format json Example output:\n[ { \u0026#34;ID\u0026#34;: \u0026#34;b42e99be1a2b\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Ethernet Interface Lan1\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;b4:2e:99:be:1a:2b\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-01-27T00:07:08.658927Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1000c3s0b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;IPAddresses\u0026#34;: [ { \u0026#34;IPAddress\u0026#34;: \u0026#34;10.252.1.26\u0026#34; } ] }, { \u0026#34;ID\u0026#34;: \u0026#34;b42e99be1a2c\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Ethernet Interface Lan2\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;b4:2e:99:be:1a:2c\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-01-26T22:43:10.593193Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1000c3s0b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;IPAddresses\u0026#34;: [] } ] Delete each node NIC MAC address in the Hardware State Manager (HSM) Ethernet interfaces table.\nncn-mw# cray hsm inventory ethernetInterfaces delete b42e99be1a2b ncn-mw# cray hsm inventory ethernetInterfaces delete b42e99be1a2c Delete the Redfish endpoint for the removed node.\nReplace the blade hardware.\nFor detailed instructions, review the Remove a Compute Blade Using the Lift procedure in HPE Cray EX Hardware Replacement Procedures H-6173 at HPE Support.\nCAUTION: Always power off the chassis slot or device before removal. The best practice is to unlatch and unseat the device while the coolant hoses are still connected, then disconnect the coolant hoses. If this is not possible, disconnect the coolant hoses, then quickly unlatch/unseat the device (within 10 seconds). Failure to do so may damage the equipment.\nPower on and boot the compute nodes Un-suspend the hms-discovery cronjob in Kubernetes.\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : false }}\u0026#39; ncn-mw# kubectl get cronjobs.batch -n services hms-discovery Example output:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * False 1 41s 33d ncn-mw# kubectl -n services logs hms-discovery-1600117560-5w95d hms-discovery | grep \u0026#34;Mountain discovery finished\u0026#34; | jq \u0026#39;.discoveredXnames\u0026#39; Example output:\n[ \u0026#34;x1000c3s0b0\u0026#34; ] Enable MEDS for the compute node(s) in the blade.\nncn-mw# cray hsm inventory redfishEndpoints update --enabled true --rediscover-on-update true The updated component names (xnames) will be returned.\nWait for 3-5 minutes for the blade to power on and the node BMCs to be discovered.\nVerify that the affected nodes are enabled in the HSM.\nncn-mw# cray hsm state components describe x1000c3s0b0n0 --format toml Beginning of example output:\nType = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;Off\u0026#34; Verify that the BMCs have been discovered by the HSM.\nncn-mw# cray hsm inventory redfishEndpoints describe x1000c3s0b0 --format json Example output:\n{ \u0026#34;ID\u0026#34;: \u0026#34;x1000c3s0b0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;NodeBMC\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;x1000c3s0b0\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x1000c3s0b0\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;UUID\u0026#34;: \u0026#34;e005dd6e-debf-0010-e803-b42e99be1a2d\u0026#34;, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MACAddr\u0026#34;: \u0026#34;b42e99be1a2d\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2021-01-29T16:15:37.643327Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;1.7.0\u0026#34; } } When LastDiscoveryStatus displays as DiscoverOK, the node BMC has been successfully discovered. If the last discovery state is DiscoveryStarted then the BMC is currently being inventoried by HSM. If the last discovery state is HTTPsGetFailed or ChildVerificationFailed, then an error has occurred during the discovery process. Enable each node individually in the HSM database (in this example, the nodes are x1000c3s0b0n0-n3).\nOptional: Force rediscovery of the components in the chassis (the example shows cabinet 1000, chassis 3).\nncn-mw# cray hsm inventory discover create --xnames x1000c3 Optional: Verify that discovery has completed (LastDiscoveryStatus = \u0026ldquo;DiscoverOK\u0026rdquo;).\nncn-mw# cray hsm inventory redfishEndpoints describe x1000c3 --format toml Example output:\nType = \u0026#34;ChassisBMC\u0026#34; Domain = \u0026#34;\u0026#34; MACAddr = \u0026#34;02:13:88:03:00:00\u0026#34; Enabled = true Hostname = \u0026#34;x1000c3\u0026#34; RediscoverOnUpdate = true FQDN = \u0026#34;x1000c3\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34;\u0026#34; IPAddress = \u0026#34;10.104.0.76\u0026#34; ID = \u0026#34;x1000c3b0\u0026#34; [DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2020-09-03T19:03:47.989621Z\u0026#34; RedfishVersion = \u0026#34;1.2.0\u0026#34; LastDiscoveryStatus = \u0026#34;DiscoverOK\u0026#34; Verify that the correct firmware versions for node BIOS, node controller (nC), NIC mezzanine card (NMC), GPUs, and so on.\nOptional: If necessary, update the firmware.\nReview the Firmware Action Service (FAS) documentation.\nncn-mw# cray fas actions create CUSTOM_DEVICE_PARAMETERS.json Update the System Layout Service (SLS).\nDump the existing SLS configuration.\nncn-mw# cray sls networks describe HSN --format=json \u0026gt; existingHSN.json Copy existingHSN.json to a newHSN.json, edit newHSN.json with the changes, then run\nncn-mw# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://API_SYSTEM/apis/sls/v1/networks/HSN -X PUT -d @newHSN.json Reload DVS on NCNs.\nUse boot orchestration to power on and boot the nodes.\nSpecify the appropriate BOS template for the node type.\nncn-mw# cray bos v1 session create --template-uuid BOS_TEMPLATE --operation reboot \\ --limit x1000c3s0b0n0,x1000c3s0b0n1,x1000c3s0b1n0,x1000c3s0b1n1 "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_remove_replace_ncns/update_firmware/",
	"title": "Update Firmware",
	"tags": [],
	"description": "",
	"content": "Update Firmware Description Use FAS to update the firmware and set the BMC password.\nProcedure See Update Firmware.\nProceed to the next step to Boot NCN and Configure or return to the main Add, Remove, Replace, or Move NCNs page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/compute_uan_application_nodes/",
	"title": "Computes/UANs/Application Nodes",
	"tags": [],
	"description": "",
	"content": "Computes/UANs/Application Nodes If the Computes make it past PXE and go into the PXE shell you can verify DNS and connectivity.\niPXE\u0026gt; dhcp Configuring (net0 98:03:9b:a8:60:88).................. No configuration methods succeeded (http://ipxe.org/040ee186) Configuring (net1 b4:2e:99:be:1a:37)...... ok iPXE\u0026gt; show dns net1.dhcp/dns:ipv4 = 10.92.100.225 iPXE\u0026gt; nslookup address api-gw-service-nmn.local iPXE\u0026gt; echo ${address} 10.92.100.71 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/locator_led/",
	"title": "Configure Locator LED",
	"tags": [],
	"description": "",
	"content": "Configure Locator LED The Locator LED is an LED in the front of the chassis that can turn on or flash. This is a useful feature when guiding someone to the switch during a \u0026ldquo;remote hands\u0026rdquo; situation, such as asking an engineer to run a cable to the switch.\nConfiguration commands Enable LED:\nswitch# location-led system 1 on Disable LED:\nswitch# location-led system 1 off Expected results The Locator LED should be in the off state The Locator LED is now flashing Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/canu_install_update/",
	"title": "Upgrade CANU",
	"tags": [],
	"description": "",
	"content": "Upgrade CANU Prerequisite Before using the CSM Automatic Network Utility (CANU) to test, validate, or configure the network, ensure that CANU is running on the latest version.\nCANU can be run from a personal workstation (the instructions below are targeted at Mac users), or on the Non-Compute Nodes (NCNs).\nSince CANU is a Python application, it can be run on Linux and Mac, but the RPM is not currently designed to support multiple operating system environments.\nThe CANU project can be cloned from GitHub and run directly by Python, but the project dependencies will need to be installed manually. This process is not supported by this documentation.\nThe Windows operating system is untested and currently not officially supported. HPE Cray recommends that Windows users install or update CANU on the Shasta NCNs instead of attempting a workstation installation.\nIf CANU is already installed, then check the CANU version with following command.\ncanu --version Upgrade/install procedure Download the latest version of CANU from CANU releases.\nUpgrade or install CANU.\nTo fresh install CANU on system\nrpm -ihv \u0026lt;canu.rpm\u0026gt; To upgrade an existing version of CANU\nrpm -Uhv \u0026lt;canu.rpm\u0026gt; Remove CANU If it is necessary to remove CANU from the system, run the following command:\nrpm -e canu Back to README\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/check_hsm/",
	"title": "Check HSM",
	"tags": [],
	"description": "",
	"content": "Check HSM Hardware State Manager has two important parts:\nSystem Layout Service (SLS): This is the \u0026ldquo;expected\u0026rdquo; state of the system (as populated by networks.yaml and other sources). State Manager Daemon (SMD): This is the \u0026ldquo;discovered\u0026rdquo; or active state of the system during runtime. Prerequisites The API calls on this page require an authorization token to be set in the TOKEN variable. See Retrieve an Authentication Token. The cray CLI commands on this page require the Cray command line interface to be configured. See Configure the Cray CLI. SLS API call\nncn# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api_gw_service.local/apis/sls/v1/hardware | jq CLI command\nncn# cray sls hardware list --format json In either case, the output from SLS should consist of a list of objects that look like the following:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x1000c7s1b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x1000c7s1b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;Mountain\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid001228\u0026#34; ], \u0026#34;NID\u0026#34;: 1228, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34; } } SMD API call\nncn# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api_gw_service.local/apis/smd/hsm/v2/Inventory/EthernetInterfaces | jq CLI command\nncn# cray hsm inventory ethernetInterfaces list --format json In either case, the output from SMD should consist of a list of objects that look like the following:\n{ \u0026#34;ID\u0026#34;: \u0026#34;0040a6838b0e\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;0040a6838b0e\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.1.147\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2020-07-24T23:44:24.578476Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1000c7s1b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34; } Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/containerd/",
	"title": "containerd",
	"tags": [],
	"description": "",
	"content": "containerd containerd is a container runtime (systemd service) that runs on the host. It is used to run containers on the Kubernetes platform.\n/var/lib/containerd filling up containerd slow startup after reboot Restarting containerd on a worker NCN /var/lib/containerd filling up In older versions of containerd, there are cases where the /var/lib/containerd directory fills up. In the event that this occurs, the following steps can be used to remediate the issue.\nRestart containerd on the NCN.\nWhether or not this resolves the space issue, if this is a worker NCN, then also see the notes in the Restarting containerd on a worker NCN section for subsequent steps that must be taken after containerd is restarted.\nncn-mw# systemctl restart containerd Many times this will free up space in /var/lib/containerd \u0026ndash; if not, then proceed to the next step.\nRestart kubelet on the NCN.\nncn-mw# systemctl restart kubelet If restarting kubelet fails to free up space in /var/lib/containerd, then proceed to the next step.\nPrune unused container images on the NCN.\nncn-mw# crictl rmi --prune Any unused images will be pruned. If still encountering disk space issues in /var/lib/containerd, then proceed to the next step to reboot the NCN.\nReboot the NCN.\nFollow the Reboot NCNs process to properly cordon/drain the NCN and reboot. Generally this will free up space in /var/lib/containerd.\ncontainerd slow startup after reboot On some systems, containerd can take a very long time to start after a reboot. This has been fixed in CSM 1.3, but if this symptom occurs, messages indicating cleaning up dead shim may appear in the containerd log files. For example:\nAug 26 00:06:10 ncn-w001 containerd[4005]: time=\u0026#34;2022-08-26T00:06:10.522985910Z\u0026#34; level=info msg=\u0026#34;cleaning up dead shim\u0026#34; Aug 26 00:06:10 ncn-w001 containerd[4005]: time=\u0026#34;2022-08-26T00:06:10.556198245Z\u0026#34; level=warning msg=\u0026#34;cleanup warnings time=\\\u0026#34;2022-08-26T00:06:10Z\\\u0026#34; level=info msg=\\\u0026#34;starting signal loop\\\u0026#34; namespace=k8s.io pid=57627\\n\u0026#34; Aug 26 00:06:10 ncn-w001 containerd[4005]: time=\u0026#34;2022-08-26T00:06:10.556821890Z\u0026#34; level=info msg=\u0026#34;loading plugin \\\u0026#34;io.containerd.monitor.v1.cgroups\\\u0026#34;...\u0026#34; type=io.containerd.monitor.v1 Aug 26 00:06:10 ncn-w001 containerd[4005]: time=\u0026#34;2022-08-26T00:06:10.557576058Z\u0026#34; level=info msg=\u0026#34;loading plugin \\\u0026#34;io.containerd.service.v1.tasks-service\\\u0026#34;...\u0026#34; type=io.containerd.service.v1 Instructing containerd to remove shims when containerd is being shutdown will correct this issue.\nEdit the /srv/cray/resources/common/containerd/containerd.service file.\nAdd the following ExecStopPost line to the file:\nExecStopPost=/usr/bin/find /run/containerd/io.containerd.runtime.v2.task -name address -type f -delete After the edit, the relevant section of the file should look similar to the following:\n[Service] ExecStartPre=/sbin/modprobe overlay \u0026amp;\u0026amp; /sbin/modprobe br_netfilter ExecStart=/usr/local/bin/containerd ExecStopPost=/usr/bin/find /run/containerd/io.containerd.runtime.v2.task -name address -type f -delete Restart=always RestartSec=5 Delegate=yes Restart containerd to pick up the change.\nIf this is a worker NCN, then also see the notes in the Restarting containerd on a worker NCN section for subsequent steps that must be taken after containerd is restarted.\nncn-mw# systemctl restart containerd NOTE: If this NCN is rebuilt, then this change will need to be re-applied (until the system is upgraded to CSM 1.3).\nRestarting containerd on a worker NCN If the containerd service is restarted on a worker node, then this may cause the sonar-jobs-watcher pod running on that worker node to fail when attempting to cleanup unneeded containers. The following procedure determines if this is the case and remediates it, if necessary.\nRetrieve the name of the sonar-jobs-watcher pod that is running on this worker node.\nModify the following command to specify the name of the specific worker NCN where containerd was restarted.\nncn-mw# kubectl get pods -l name=sonar-jobs-watcher -n services -o wide | grep ncn-w001 Example output:\nsonar-jobs-watcher-8z6th 1/1 Running 0 95d 10.42.0.6 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; View the logs for the sonar-jobs-watcher pod.\nModify the following command to specify the pod name identified in the previous step.\nncn-mw# kubectl logs sonar-jobs-watcher-8z6th -n services Example output:\nFound pod cray-dns-unbound-manager-1631116980-h69h6 with restartPolicy \u0026#39;Never\u0026#39; and container \u0026#39;manager\u0026#39; with status \u0026#39;Completed\u0026#39; All containers of job pod cray-dns-unbound-manager-1631116980-h69h6 has completed. Killing istio-proxy (1c65dacb960c2f8ff6b07dfc9780c4621beb8b258599453a08c246bbe680c511) to allow job to complete time=\u0026#34;2021-09-08T16:44:18Z\u0026#34; level=fatal msg=\u0026#34;failed to connect: failed to connect, make sure you are running as root and the runtime has been started: context deadline exceeded\u0026#34; When this occurs, pods that are running on the node where containerd was restarted may remain in a NotReady state and never complete.\nCheck if pods are stuck in a NotReady state.\nncn-mw# kubectl get pods -o wide -A | grep NotReady Example output:\nservices cray-dns-unbound-manager-1631116980-h69h6 1/2 NotReady 0 10m 10.42.0.100 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; If any pods are stuck in a NotReady state, then restart the sonar-jobs-watcher daemonset to resolve the issue.\nncn-mw# kubectl rollout restart -n services daemonset sonar-jobs-watcher Expected output:\ndaemonset.apps/sonar-jobs-watcher restarted Verify that the restart completed successfully.\nncn-mw# kubectl rollout status -n services daemonset sonar-jobs-watcher Expected output:\ndaemon set \u0026#34;sonar-jobs-watcher\u0026#34; successfully rolled out Once the sonar-jobs-watcher pods restart, any pods that were in a NotReady state should complete within about a minute.\nTo learn more in general about containerd, refer to the containerd documentation.\n"
},
{
	"uri": "/docs-csm/en-12/operations/image_management/update_ims_job_access_network/",
	"title": "Update IMS Job Access Network",
	"tags": [],
	"description": "",
	"content": "Update IMS Job Access Network In the CSM V1.2.0 and V1.2.1 releases, the IMS jobs template was set up with the wrong service address pool. This means that the IMS job pods are unable to start on the customer-management network where they have permission to run.\nTo fix this on a running system, the ims-config configuration map will need to updated to use the correct address pool when starting jobs.\nIMPORTANT: Once this procedure has been done, it will not fix jobs that are currently running. This will only impact new jobs created after the settings have been updated. Old jobs that can not be accessed must be deleted and recreated.\nProcedure Edit the ims-config settings.\nncn-mw# kubectl -n services edit cm ims-config Find the JOB_CUSTOMER_ACCESS_NETWORK_ACCESS_POOL variable and set the value to customer-management.\nJOB_CUSTOMER_ACCESS_NETWORK_ACCESS_POOL: customer-management Exit the editor, saving the new value.\nRestart the cray-ims pod.\nncn-mw# IMS_POD=$(kubectl get pods -n services -o wide | grep cray-ims | awk \u0026#39;{print $1}\u0026#39;) ncn-mw# kubectl -n services delete pod $IMS_POD Wait for the new pod to be ready.\nncn-mw# watch \u0026#39;kubectl -n services get pods | grep cray-ims\u0026#39; Watch the status of the pod for output similar to the following:\ncray-ims-fbc5c5b45-lq4h7 0/2 PodInitializing 0 10s When it transitions to 2/2 Running, use Ctl-c to exit the watch command.\nNew jobs will now be created with the correct network settings.\n"
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/hardware_state_manager_hsm_state_and_flag_fields/",
	"title": "Hardware State Manager (HSM) State and Flag Fields",
	"tags": [],
	"description": "",
	"content": "Hardware State Manager (HSM) State and Flag Fields HSM manages important information for hardware components in the system. Administrators can use the data returned by HSM to learn about the state of the system. To do so, it is critical that the State and Flag fields are understood, and the next steps to take are known when viewing output returned by HSM commands. It is also beneficial to understand what services can cause State or Flag changes in HSM.\nThe following describes what causes State and Flag changes for all HSM components:\nInitial State/Flag is set upon discovery. This is generally Off/OK or On/OK. BMCs go to Ready/OK instead of On/OK. A component in the Populated state after discovery has an unknown power state. If one is expected for nodes, BMCs, or another component, this is likely due to a firmware issue. Flags can be set to Warning or Alert if the component\u0026rsquo;s Status.Health reads as Warning or Critical via Redfish during discovery. State for all components associated with a BMC is set to Empty if that BMC is removed from the network. State change events from components are consumed by HSM via subscriptions to Redfish events. These are subscribed to and placed on the Kafka bus by hmcollector for HSM\u0026rsquo;s consumption. HSM will update component state based on the information in the Redfish events. The following describes what causes State and Flag changes for nodes only:\nHeartbeat Tracking Daemon (HBTD) updates the state of nodes based on heartbeats it receives from nodes. HBTD sets the node to Ready/OK when it starts heartbeats. HBTD sets the node to Ready/Warning after a few missed heartbeats. HBTD sets the node to Standby after many missed heartbeats and the node is presumed dead. State descriptions:\nEmpty\nThe location is not populated with a component.\nPopulated\nPresent (not empty), but no further track can or is being done.\nOff\nPresent but powered off.\nOn\nPowered on. If no heartbeat mechanism is available, its software state may be unknown.\nStandby\nNo longer Ready and presumed dead. It typically means the heartbeat has been lost (w/ alert).\nReady\nBoth On and Ready to provide its expected services. For example, used for jobs.\nFlag descriptions:\nOK\nComponent is OK.\nWarning\nThere is a non-critical error. Generally coupled with a Ready state.\nAlert\nThere is a critical error. Generally coupled with a Standby state. Otherwise, reported via Redfish.\nHardware State Transitions The following table describes how to interpret when the state of hardware changes:\nPrior State New State Reason Ready Standby HBTD if node has many missed heartbeats Ready Ready/Warning HBTD if node has a few missed heartbeats Standby Ready HBTD node re-starts heartbeating On Ready HBTD node started heartbeating Off Ready HBTD sees heartbeats before Redfish Event (On) Standby On Redfish Event (On) or if re-discovered while in the standby state Off On Redfish Event (On) Standby Off Redfish Event (Off) Ready Off Redfish Event (Off) On Off Redfish Event (Off) Any State Empty Redfish Endpoint is disabled meaning component removal Generally, nodes transition from Off to On to Ready when going from Off to booted, and from Ready to Ready/Warning to Standby to Off when shut down.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/configuration_management_with_the_cfs_batcher/",
	"title": "Configuration Management with the CFS Batcher",
	"tags": [],
	"description": "",
	"content": "Configuration Management with the CFS Batcher Creating configuration sessions with the Configuration Framework Service (CFS) enables remote execution for configuring live nodes and boot images prior to booting. CFS also provides its Batcher component for configuration management of registered system components. The CFS Batcher periodically examines the aggregated configuration state of registered components and schedules CFS sessions against those that have not been configured to their desired state. The frequency of scheduling, the maximum number of components to schedule in the same CFS session, and the expiration time for scheduling less than full sessions are configurable.\nThe CFS-Batcher schedules CFS sessions according to the following rules:\nComponents are assigned to a batch if they need configuration, are not disabled, and are currently not assigned to a batch. Components are grouped according to their desired state information. A new batch is created if no partial batches match the desired state, and all similar batches are full. Batches are scheduled as CFS sessions when the batch is full or the batch window time has been exceeded. The timer for the batch window is started when the first component is added, and is never reset. Nodes should never wait more than the window period between being ready for configuration and being scheduled in a CFS session. CFS cannot guarantee that jobs for similar batches will start at the same time, even if all CFS sessions are created at the same time. This variability is due to the nature of Kubernetes scheduling. Checking the start time for the CFS session is more accurate than checking the pod start time when determining when a batch was scheduled. There are two safety mechanisms built into the Batcher scheduling that can delay batches more than the usual amount of time. Both mechanisms are indicated in the logs:\nCFS Batcher will not schedule multiple sessions to configure the same component. Batcher monitors on-going sessions that it started so that if one session is started and the desired configuration changes, Batcher can wait until the initial session is completed before scheduling the component with the new configuration to a new session. If Batcher is restarted, it will attempt to rebuild its state based on sessions with the \u0026ldquo;batcher-\u0026rdquo; naming scheme that are still in progress. This ensures that scheduling conflicts will not occur even if Batcher is restarted. On restart, some information on the in-flight sessions is lost, so this wait ensures that the Batcher does not schedule multiple configuration sessions for the same component at the same time. If several CFS sessions that are created by the Batcher Agent fail in a row (the most recent 20 sessions), Batcher will start throttling the creation of new sessions. The throttling is automatically reset if a single session succeeds. Users can also manually reset this by restarting Batcher. The back-off is increased if new sessions continue to fail. This helps protect against cases where high numbers of retries are allowed so that Batcher cannot flood Kubernetes with new jobs in a short period of time. Configure Batcher Several Batcher behaviors are configurable. All of the Batcher configuration is available through the CFS options:\nncn-mw# cray cfs options list --format toml | grep -i batch Example output:\nbatchSize = 25 batchWindow = 60 batcherCheckInterval = 10 defaultBatcherRetryPolicy = 3 See CFS Global Options for more information. Use the cray cfs options update command to change these values as needed.\nReview the following information about CFS Batcher options before changing the defaults. Setting these to non-optimal values may affect system performance. The optimal values will depend on system size and the specifics of the configuration layers that will be applied in the sessions created by CFS Batcher.\nbatchSize This option determines the maximum number of components that will be included in each session created by CFS Batcher.\nThe default value is 25 components per session.\nWARNING: Increasing this value will result in fewer batcher-created sessions, but will also require more resources for Ansible Execution Environment (AEE) containers to do the configuration.\nbatchWindow This option sets the number of seconds that CFS batcher will wait before scheduling a CFS session when the number of components needing configuration has not reached the batchSize limit. CFS Batcher will immediately create a session when the batchSize limit is reached. However, in the case where there are few components or long periods of time between components notifying CFS Batcher of the need for configuration, the batchWindow will time-box the creation of sessions so no component needs to wait for the queue to fill.\nThe default value is 60 seconds.\nWARNING: Lower values will cause CFS Batcher to be more responsive to creating sessions, but values too low may result in degraded performance of both the CFS APIs as well as the overall system.\nbatcherCheckInterval This option sets how often CFS batcher checks for components waiting to be configured. This value must be lower than the batchWindow value.\nThe default value is 10 seconds.\nWARNING: Lower values will cause CFS Batcher to be more responsive to creating sessions, but values too low may result in degraded performance of the CFS APIs on larger systems.\ndefaultBatcherRetryPolicy When a component (node) requiring configuration fails to configure from a previous configuration session launched by CFS Batcher, the error is logged. defaultBatcherRetryPolicy is the maximum number of failed configurations allowed per component before CFS Batcher will stop attempts to configure the component.\nThis value can be overridden on a per component basis.\nList CFS Batcher sessions The CFS Batcher prepends all CFS session names it creates with batcher-. Sessions that have be created by CFS Batcher are found by using the following command with the --name-contains option:\nncn-mw# cray cfs sessions list --name-contains batcher- To list the batcher sessions that are currently running, filter with the cray cfs sessions list command options:\nncn-mw# cray cfs sessions list --name-contains batcher- --status running Use the cray cfs sessions list --help command output for all filtering options, including session age, tags, status, and success.\nMap CFS Batcher sessions to BOS sessions To find all of the sessions created by the CFS Batcher because of configuration requests made by a specific Boot Orchestration Service (BOS) session, filter the sessions by the name of the BOS session, which is added as a tag on the CFS sessions. The BOS session ID is required to run the following command.\nncn-mw# cray cfs sessions list --tags bos_session=BOS_SESSION_ID "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_node_is_not_able_to_download_the_required_artifacts/",
	"title": "Compute Node Boot Issue Symptom Node is Not Able to Download the Required Artifacts",
	"tags": [],
	"description": "",
	"content": "Compute Node Boot Issue Symptom: Node is Not Able to Download the Required Artifacts If either or both of the kernel or the initrd boot artifacts are missing from the artifact repository, Boot Script Service (BSS), or both, the node will not be able to download the required boot artifacts and will fail to boot.\nSymptoms The node\u0026rsquo;s console or log will display lines beginning with, \u0026lsquo;\u0026rsquo;Could not start download\u0026rsquo;\u0026rsquo;. Refer to the image below for an example of this error message.\nProblem Detection Use the following command from a non-compute node (NCN) to see which boot artifacts BSS assumes as those used for booting the node.\nncn-m001# cray bss bootparameters list Each boot artifact has a download URL, as shown in the following example output:\n[[results]] kernel = \u0026#34;s3://boot-images/dc87a741-f7cc-4167-afae-592c5a8ca7ec/vmlinuz-4.12.14-197.29_9.1.14-cray_shasta_c\u0026#34; [[results]] kernel = \u0026#34;s3://boot-images/89e5a1dc-0caa-418a-9742-a832829db0ab/kernel\u0026#34; [[results]] kernel = \u0026#34;s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/kernel\u0026#34; [[results]] kernel = \u0026#34;s3://boot-images/29c2cc23-a9d6-4e9a-ab1a-b5fa9270c975/kernel\u0026#34; [[results]] initrd = \u0026#34;s3://boot-images/dc87a741-f7cc-4167-afae-592c5a8ca7ec/initrd-4.12.14-197.29_9.1.14-cray_shasta_c\u0026#34; [[results]] initrd = \u0026#34;s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/initrd\u0026#34; [[results]] initrd = \u0026#34;s3://boot-images/89e5a1dc-0caa-418a-9742-a832829db0ab/initrd\u0026#34; [[results]] initrd = \u0026#34;s3://boot-images/29c2cc23-a9d6-4e9a-ab1a-b5fa9270c975/initrd\u0026#34; [[results]] params = \u0026#34;console=ttyS0,115200n8 console=tty0 initrd=37103ceb-3813-45ba-85b0-a8fc53edd5da rw selinux=0 nofb rd.shell rd.net.timeout.carrier=20 ip=dhcp rd.neednet=1 rd.retry=60 crashkernel=360M reds=use_server api_gw_ip=api-gw-service-nmn.local\u0026#34; initrd = \u0026#34;s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/initrd\u0026#34; hosts = [ \u0026#34;Unknown-x86_64\u0026#34;,] kernel = \u0026#34;s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/kernel\u0026#34; Use the artifact\u0026rsquo;s S3 key to download it:\nncn-m001# cray artifacts get S3_BUCKET S3_OBJECT_KEY DOWNLOAD_FILE_PATH For example, if s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/initrd is the S3 URI for the file initrd, run the following command:\nncn-m001# cray artifacts get boot-images \\ 97b548b9-2ea9-45c9-95ba-dfc77e5522eb/initrd initrd This command will return 404 errors if the specified object does not exist in the S3 bucket.\nResolution Ensure that the required boot artifacts are stored in the artifact repository and/or BSS. If the artifact\u0026rsquo;s name is different than what is already in BSS, then BSS needs to be updated to match.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/create_a_uai_class/",
	"title": "Create a UAI Class",
	"tags": [],
	"description": "",
	"content": "Create a UAI Class Add a new User Access Instance (UAI) class to the User Access Service (UAS) so that the class can be used to configure UAIs.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) Procedure Add a UAI class by using the command in the following example.\nncn# cray uas admin config classes create --image-id \u0026lt;image-id\u0026gt; [options] The only required option is --image-id IMAGE_ID which sets the container image that will be used to create a UAI from this UAI class.\nOther options and arguments can be discovered using the following command:\nncn# cray uas admin config classes create --help See UAI Classes for more information on what the settings in a UAI class mean and how to use them.\nTop: User Access Service (UAS)\nNext Topic: View a UAI Class\n"
},
{
	"uri": "/docs-csm/en-12/operations/component_names_xnames/",
	"title": "Component Names (xnames)",
	"tags": [],
	"description": "",
	"content": "Component Names (xnames) Component names (xnames) identify the geolocation for hardware components in the HPE Cray EX system. Every component is uniquely identified by these component names. Some, like the system cabinet number or the CDU number, can be changed by site needs. There is no geolocation encoded within the cabinet number, such as an X-Y coordinate system to relate to the floor layout of the cabinets. Other component names refer to the location within a cabinet and go down to the port on a card or switch or the socket holding a processor or a memory DIMM location.\nName Pattern Range Description s0 n/a Wildcard: Specifies all of a given type of component in the system. Can be used for \u0026ldquo;all nodes\u0026rdquo;, or to refer to the management system NCN cluster by a single logical name. ncnN N: 1-n Non-compute Node (NCN): A management node in the management plane. Management NCNs are located in standard EIA racks. all n/a Wildcard: Similar to s0 and can be used to specify all components in a system. all_comp n/a Wildcard: Specifies all compute nodes. all_svc n/a Wildcard: Specifies all service or management nodes. pH.S H: 0-n S: 0-n Partition: A hardware or software partition (hard or soft partition). H specifies a hardware partition; HSN cabling, switches, and so on. The S specifies a software partition. A hard partition can have more than 1 soft partition. A soft partition cannot have more than 1 hard partition. Example: p1.2 is soft partition 2 of hard partition 1. dD D: 0-999 Coolant Distribution Unit (CDU): 1 CDU for up to 6 cabinets. Example: d3 (CDU 3). dDwW W: 0-31 Management Switch in a CDU: Example: d3w1 is switch 1 in CDU 3. xX X: 0-9999 Liquid-cooled Cabinet or Standard Rack: Liquid-cooled cabinets include 8 chassis and do not have a cabinet-level controller; only chassis-level controllers. A standard rack is always considered chassis 0. Examples: x3000 is rack number 3000. xXdD D: 0-1 Rack-mounted CDU: Example: x1000d0 is CDU 0 for cabinet 1000. xXmM M: 0-3 Rack PDU Controller (BMC): Controller or BMC for one or more rack PDUs. A primary PDU controller many manage other PDUs. Example: x3000m0 is PDU controller 0, cabinet 3000. xXmMpP P: 0-7 Rack PDU: managed by a controller. Example: x3000m0p0 is PDU 0, PDU controller 0, cabinet 3000. xXmMpPjJ J: 1-32 Rack PDU Outlet: Example: x3000m0p0j12 is power outlet 12 on PDU 0, PDU controller 0, rack 3000. xXmMiI I: 1-3 PDU NIC: The NIC associated with the PDU management controller, not a specific PDU. Example: x3000m0i1 is management NIC 1 of PDU controller 0, cabinet 3000. xXm0pPvV V: 1-64 PDU Power Connector: Power connectors are connected to node cards or enclosures and also control and monitor power. Example: x3000m0p0v32 is power plug/outlet 32 of cabinet PDU 0, under cab PDU controller 0, rack 3000. xXeE E: 0-1 CEC: There are Cabinet Environmental Controllers (CEC) per liquid-cooled cabinet. CEC 0 (right) and CEC 1 (left). Example: x1016e0 is the CEC on the right side of cabinet 1016. xXcC C: 0-7 Chassis: An enclosure within a liquid-cooled cabinet. Standard EIA racks are always considered a single chassis (chassis 0). This component name is used as a component group or prefix and not a single component. Example: x1016c3 is chassis 3 of cabinet 1016. xXcCbB B: 0 Chassis BMC: Liquid-cooled cabinet chassis management module (CMM) controller (cC). A standard EIA rack is always chassis 0. Example: x1016c4b0 is BMC 0 (cC) for chassis 4, cabinet 1016. xXcCbBiI I: 0 Chassis BMC NIC: CMM BMC Ethernet NIC. Example: x1000c1b0i0 is NIC 0 of BMC 0, chassis 1, cabinet 1000. xXcCtT T: 0-2 PSU: Power rectifier (PSU) in the a liquid-cooled chassis rectifier shelf. Three PSUs support a chassis (n+1). Example: x1016c3t2 is PSU 2 for chassis 3, cabinet 1016. xXcCfF F: 0 CMM FPGA: CMM FPGA. Example: x1016c1f0 is FPGA 0 in chassis 1 CMM, cabinet 1016. xXcCwW W: 1-48 Management Network Switch: Specifies bottom U-position for the switch. Example: x3000c0w47 is management switch in U47, chassis 0, rack 3000. xXcCwWjJ J: 1-32 Management Network Switch Connector: Cable connector (port) on a management switch. Example: x3000c0w47j31 is cable connector 31 of switch in U47, chassis 0, rack 3000. xXcChH H: 1-48 High-level Management Switch Enclosure: Typically spine switch. Example: x3000c0h47 is U47, chassis 0, in rack 3000. xXcChHsS S: 1-4 High-level Management Network Switch: Typically a spine switch. May be a half-width device specified with a rack U position H and a horizontal space number S. Horizontal space numbers are assigned arbitrarily to physical locations. Example: x3000c0h47s1 is space 1 of U47, chassis 0, rack 3000. xXcCrR R: 0-64 HSN Switch: Liquid-cooled blade switch slot number or ToR switch bottom U position. Switch blades are numbered 0-7 in a chassis. ToR HSN switches are numbered by bottom U position. Example: x1016c3r6 is switch blade 6 in chassis 3, cabinet 1016. Example: x3000c0r31 is ToR switch in U31, chassis 0, rack 3000. xXcCrRaA A: 0 HSN Switch ASIC: Example: x3000c0r1a0 is ASIC 0 of ToR switch in U1, chassis 0, of rack 3000. Example: x1016c3r7a0 (ASIC 0 of liquid-cooled switch blade 7, chassis 3, cabinet 1016). xXcCrRaAlL L: 0-N HSN Switch ASIC Link: The decimal number for the maximum number of links is network-dependent. Example: x1016c0r1a0l25 is link 25 of ASIC 0, switch 1, chassis 0, cabinet 1016). xXcCrReE E: 0 HSN Switch Submodule: Example: x3000c0r2e0 is HSN switch submodule of ToR switch 2, chassis 0, rack 3000. xXcCrRbB B: 0 HSN Switch Controller (sC) or BMC: A BMC or embedded controller of a switch blade. Example: x1000c3r4b0 is BMC 0 of switch 4, chassis 3, cabinet 1000. Example: x3000c0r1b0 is BMC 0 of ToR switch in U1, chassis 0, rack 3000. xXcCrRbBiI I: 0-3 HSN Switch Management NIC: Example: x1016c2r3b0i0 is NIC 0 of controller 0, switch 3, of chassis 2, cabinet 1016. xXcCrRfF F: 0 HSN Switch Card FPGA: Example: x1016c3r2f0 is FPGA 0 of blade switch 2, chassis 3, cabinet 1016. xXcCrRtT T: 0 ToR component in a ToR Switch. Example: x3000c0r1t0 ToR switch 0 in U1, chassis 0, cabinet 3000. xXcCrRjJ J: 1-32 HSN Switch Cable Connector: Example: x1016c3r4j7 is HSN connector 7 in, switch 4, chassis 3, cabinet 1016. xXcCrRjJpP P: 0-1 HSN Switch Cable Connector Port: Example: x1016c3r4j7p0 is port 0 of HSN connector 7, switch blade 4, chassis 3, cabinet 1016. xXcCsS S: 0-64 Node Slot or U Position: Liquid-cooled blades are numbered 0-7 in each chassis; a rack system U position specifies the bottom-most U number for the enclosure. An EIA rack is always chassis 0. Example: x1016c1s7 is compute blade 7 in chassis 1, cabinet 1016. Example: x3000c0s24 is node enclosure in U24, chassis 0, of rack 3000. xXcCsSvV V: 1-2 Power Connector for Rack Node Enclosure: Power connector for an air-cooled node enclosure/blade. There may be one or two power connectors per node. Example: x3000c0s4v1 is power connector 1, server in U4, chassis 0, rack 3000. xXcCrRvV V: 1-2 Power Connector for ToR HSN Switch: There may be one or two power connectors per ToR HSN switch. Example: x3000c0r4v1 is power connector 1 of ToR switch in U4, chassis 0, rack 3000. xXcCsSbB B: 0-1 Node Controller or BMC: Liquid-cooled compute blade node card controller (nC), or rack node card BMC. Example: x1016c3s1b0 (node card 0 controller (nC) of compute blade 1, chassis 3, cabinet 1016). xXcCsSbBiI I: 0-3 Node controller or BMC NIC: NIC associated with a node controller or BMC. Liquid-cooled nC NIC numbers start with 0. Standard rack node card BMC NICs are numbered according to the OEM hardware. Example: x1016c2s1b0i0 is NIC 0 of node card 0 controller (nC), compute blade in slot 1, chassis 3, cabinet 16. Example: x3000c0s24b1i1 is NIC 1 of BMC 1, compute node in rack U-position 24, chassis 0, cabinet 3000. xXcCsSbBnN N: 0-7 Node: Liquid-cooled node or rack server node. Nodes are numbered 0-N and are children of the parent node controller or BMC. Node names have a bB component which specifies the node controller or BMC number. Component names can support one BMC for several nodes, multiple BMCs for one node, or one BMC per node. Example: x1016c2s3b0n1 is node 1 of node card 0, compute blade in slot 3, chassis 2, cabinet 1016. xXcCsSeE E: 0 Node Enclosure: Liquid-cooled nodes are located on a node card which includes node card controller (nC). The node card is considered an enclosure. There may be 1 or more node cards in a rack system server or liquid-cooled blade. Rack node enclosures can include multiple subslots inside of a multi-U enclosure. Example: x3000c0s16e0 is node enclosure 0, at U16, chassis 0, cabinet 3000. xXcCsSbBnNpPx P: 0-3 Node Processor Socket: Example: x1016c2s3b0n1p1 is processor socket 1 of node 1, of node card 0, compute blade in slot 3, chassis 2, cabinet 1016. xXcCsSbBnNdD D: 0-15 Node DIMM Example: x1016c3s0b0n1d3 is DIMM 3 of node 1, node card 0, compute blade 0, chassis 3, cabinet 1016). xXcCsSbBnNhH H: 0-3 Node HSN NIC: Example: x1016c3s0b0n1h1 is HSN NIC 1, node 1, node card 0, compute blade in slot 0, chassis 3, cabinet 1016. xXcCsSbBnNiI I: 1-3 Node Management NIC: Example: x1016c3s0b0n1i1 is node management NIC 1 of node 1, node card 0, compute blade in slot 0, chassis 3, of cabinet 1016. xXcCsSbBfF F: 0-7 Node Controller FPGA: Node card controller FPGA (FPGA). Example: x16c3s4b1f0 is FPGA 0 of node card 1, compute blade in slot 4, chassis 3, cabinet 1016. xXcCsSbBnNaA A: 0-7 GPU: Accelerator (GPU) associated with a node. Example: x16c3s0b1n0a1 is accelerator 1, node 0, of node card 1, compute blade 0, of chassis 3, of cabinet 1016. xXcCsSbBnNgG G: 0-63 Storage Group or Group of Disk Drives for a Node: Example: x1016c3s0b0n1g3 is storage group 3 of node 1, node card 0, compute blade in slot 0, chassis 3, cabinet 1016. xXcCsSbBnNgGkK K: 0-63 Storage Group Disk: Example: x1016c3s0b0n1g3k1 is disk 1 of storage group 3, node 1, node card 0, of compute blade in slot 0, chassis 3, cabinet 1016. "
},
{
	"uri": "/docs-csm/en-12/install/configure_management_network/",
	"title": "Configure Management Network",
	"tags": [],
	"description": "",
	"content": "Configure Management Network HPE Cray EX systems can have network switches in many roles: spine switches, leaf switches, leaf-bmc switches, and Coolant Distribution Unit (CDU) switches. Newer systems have HPE Aruba switches, while older systems have Dell and Mellanox switches. Switch IP addresses are generated by Cray Site Init (CSI).\nDocumentation for the Management Network can be found in the HPE Cray EX Management Network Installation and Configuration Guide.\nThe configuration for these switches will be generated from the CSM Automatic Network Utility (CANU). For more details, see the CANU documentation.\nIt is assumed that the administrator configuring the Management Network has a basic understanding of networking protocols (STP, VLAN, OSPF, LAG/MLAG, BGP, etc.), and knows how to configure network equipment. It is also assumed that they understand and know how to read a Shasta Cabling Diagram (SHCD) file.\nBefore configuring/reconfiguring any switches, make sure to get the current running configuration and save that in case the configuration must be reverted.\nAll switch configuration should be done with a console or out of band connection.\nSave the output of the following:\nsw# show run Details HPE Aruba switch configuration The management network switches should be configured in this order: spine, leaf (if present), CDU (if present), and leaf-bmc. Only systems with liquid-cooled cabinets will have the CDU switches. Only systems with many nodes in air-cooled cabinets will have leaf switches.\nDell and Mellanox switch configuration The management network switches should be configured in this order: spine, leaf (if present), CDU (if present), and leaf-bmc. Only systems with liquid-cooled cabinets will have the CDU switches. Only systems with many nodes in air-cooled cabinets will have leaf switches.\nOn a typical system, the Mellanox switches are spine switches and the Dell switches are used for leaf, CDU, and leaf-bmc.\nSite connections Currently CANU does not automatically create site connections (LAGs/uplink interfaces or default routes).\nHence, administrators need to manually configure the uplinks for site connections as well as default routes.\nExample configuration:\nNOTE: These are very simplistic examples and depending on the install scenario, administrators may need to also configure LAGs, etc.\nSite connections: Mellanox sw-spine# ena sw-spine# conf t sw-spine# interface ethernet 1/16 no switchport force sw-spine# interface ethernet 1/16 speed 10G force sw-spine# interface ethernet 1/16 vrf forwarding Customer sw-spine# interface ethernet 1/16 ip address 10.102.255.10/30 primary sw-spine# ip route vrf Customer 0.0.0.0/0 10.102.3.3 sw-spine# ip route vrf Customer 0.0.0.0/0 10.102.255.9 Site connections: Aruba sw-spine# config sw-spine# Interface 1/1/16 sw-spine# vrf attach Customer sw-spine# ip address 10.102.255.10/30 sw-spine# no shutdown sw-spine# exit sw-spine# ip route 0.0.0.0/0 10.102.3.3 vrf Customer sw-spine# ip route 0.0.0.0/0 0.102.255.9 vrf Customer Next topic After completing this procedure, collect MAC addresses for the management nodes using the PIT node and the management network switches configured in this procedure.\nSee Collect MAC addresses for NCNs.\n"
},
{
	"uri": "/docs-csm/en-12/glossary/",
	"title": "Glossary",
	"tags": [],
	"description": "",
	"content": "Glossary Glossary of terms used in CSM documentation.\nAnsible Execution Environment (AEE) Application Node (AN) Baseboard Management Controller (BMC) Bifurcated CAN (BICAN) Blade Switch Controller (sC) Boot Orchestration Service (BOS) Boot Script Service (BSS) Cabinet Cooling Group Cabinet Environmental Controller (CEC) CEC microcontroller (eC) Chassis Management Module (CMM) Compute Node (CN) Compute Rolling Upgrade Service (CRUS) Configuration Framework Service (CFS) Content Projection Service (CPS) Coolant Distribution Unit (CDU) Cray Advanced Platform Monitoring and Control (CAPMC) Cray CLI (cray) Cray Operating System (COS) Cray Programming Environment (CPE) Cray Security Token Service (STS) Cray Site Init (CSI) Cray System Management (CSM) CSM Automatic Network Utility (CANU) Customer Access Network (CAN) Customer High Speed Network (CHN) Customer Management Network (CMN) Data Virtualization Service (DVS) EX Compute Cabinet EX TDS Cabinet Fabric Firmware Action Service (FAS) Floor Standing CDU Hardware Management Network (HMN) Hardware Management Notification Fanout Daemon (HMNFD) Hardware State Manager (HSM) Hardware State Manager (SMD) Heartbeat Tracker Daemon (HBTD) High Speed Network (HSN) Image Management Service (IMS) JSON Web Token (JWT) Kubernetes NCNs Management Cabinet Management Nodes Mountain Cabinet Mountain Endpoint Discovery Service (MEDS) NIC Mezzanine Card (NMC) Node Controller (nC) Node Management Network (NMN) Node Memory Dump (NMD) Non-Compute Node (NCN) Online Certificate Status Protocol (OCSP) Olympus Cabinet Parallel Application Launch Service (PALS) Power Distribution Unit (PDU) Pre-Install Toolkit (PIT) LiveCD Public Key Infrastructure (PKI) Rack-Mounted CDU Rack System Compute Cabinet Redfish Translation Service (RTS) River Cabinet River Endpoint Discovery Service (REDS) Rosetta ASIC Service/IO Cabinet Shasta Cabling Diagram (SHCD) Simple Storage Service (S3) Slingshot Slingshot Blade Switch Slingshot Host Software (SHS) Slingshot Top of Rack (ToR) Switch Supply/Return Cutoff Valves System Admin Toolkit (SAT) System Configuration Service (SCSD) System Diagnostic Utility (SDU) System Layout Service (SLS) System Management Network (SMNet) System Management Services (SMS) System Management Services (SMS) nodes System Monitoring Application (SMA) System Monitoring Framework (SMF) Top of Rack Switch Controller (sC-ToR) User Access Instance (UAI) User Access Node (UAN) User Access Service (UAS) Version Control Service (VCS) Virtual Network Identifier Daemon (VNID) xname Ansible Execution Environment (AEE) A component used by the Configuration Framework Service (CFS) to execute Ansible code from its configuration layers.\nFor more information, see Ansible Execution Environments.\nApplication Node (AN) An application node (AN) is an NCN which is not providing management functions for the HPE Cray EX system. The AN is not part of the Kubernetes cluster to which management nodes belong. One special type of AN is the User Access Node (UAN), but different systems may have need for other types of ANs, such as:\nnodes which provide a Lustre routing function (LNet router) gateways between HSN and InfiniBand data movers between two different network file systems visualization servers other special-purpose nodes Baseboard Management Controller (BMC) Air-Cooled cabinet COTS servers that include a Redfish-enabled baseboard management controller (BMC) and REST endpoint for API control and management. Either IPMI commands or REST API calls can be used to manage a BMC.\nBifurcated CAN (BICAN) Introduced in CSM 1.2, a major feature of CSM is the Bifurcated Customer Access Network. The BICAN is designed to separate administrative network traffic from user network traffic.\nFor more information, see:\nBICAN Technical Summary BICAN Technical Details Blade Switch Controller (sC) The Slingshot blade switch embedded controller (sC) provides a hardware management REST endpoint to monitor environmental conditions and manage the blade power, switch ASIC, FPGA buffer/interfaces, and firmware.\nBoot Orchestration Service (BOS) The Boot Orchestration Service (BOS) is responsible for booting, configuring, and shutting down collections of nodes. This is accomplished using BOS components, such as boot orchestration session templates and sessions. BOS uses other services which provide boot artifact configuration (BSS), power control (CAPMC), node status (HSM), and configuration (CFS).\nFor more information, see Boot Orchestration.\nBoot Script Service (BSS) The Boot Script Service stores the configuration information that is used to boot each hardware component. Nodes consult BSS for their boot artifacts and boot parameters when nodes boot or reboot.\nCabinet Cooling Group A cabinet cooling group is a group of Olympus cabinets that are connected to a floor-standing Coolant Distribution Unit (CDU). Management network CDU switches in the CDU aggregate all the Node Management Network (NMN) and Hardware Management Network (HMN) connections for the cabinet group.\nCabinet Environmental Controller (CEC) The Liquid-Cooled Olympus Cabinet Environmental Controller (CEC) sets the cabinet\u0026rsquo;s geolocation, monitors environmental sensors, and communicates status to the Coolant Distribution Unit (CDU). The CEC microcontroller (eC) signals the cooling distribution unit (CDU) to start liquid cooling and then enables the DC rectifiers so that a chassis can be powered on. The CEC does not provide a REST endpoint on SMNet, it simply provides the cabinet environmental and CDU status to the CMM for evaluation or action; the CEC takes no action. The CEC firmware is flashed automatically when the CMM firmware is flashed. If there are momentary erroneous signals because of a CEC reset or cable disconnection, the system can ride through these events without issuing an EPO.\nCEC microcontroller (eC) The CEC microcontroller (eC) sets the cabinet\u0026rsquo;s geolocation, monitors the cabinet environmental sensors, and communicates cabinet status to the Coolant Distribution Unit (CDU). The eC does not provide a REST endpoint on SMNet as do other embedded controllers, but simply monitors the cabinet sensors and provides the cabinet environmental and CDU status to the CMMs for evaluation and/or action.\nChassis Management Module (CMM) The cabinet chassis management module (CMM) provides a REST endpoint via its chassis controller (cC). The CMM is an embedded controller that monitors and controls all the blades in a chassis. Each chassis supports 8 compute blades and 8 switches and associated rectifiers/PSUs in the rectifier shelf. Power Considerations - Two CMMs in adjacent chassis share power from the rectifier shelf (a shelf connects two adjacent chassis - 0 and 1, 2 and 3, 4 and 5, 6 and 7). If both CMMs sharing shelf power are both enabling the rectifiers, one of the CMMs can be removed (but only one at a time) without the rectifier shelf powering off. Removing a CMM will shutdown all compute blades and switches in the chassis. Cooling Considerations - Any single CMM in any cabinet can enable Coolant Distribution Unit (CDU) cooling. Note that the CDU \u0026ldquo;enable path\u0026rdquo; has vertical control which means CMMs 0, 2, 4, and 6 and CEC0 are in a path (half of the cabinet), and CMMs 1, 3, 5, and 7 and CEC1 are in another path. Any CMM or CEC in the same half-cabinet path can be removed and CDU cooling will stay enabled as long as the other CMMs/CEC enables CDU cooling.\nCompute Node (CN) The compute node (CN) is where high performance computing application are run. These have hostnames that are of the form nidXXXXXX, that is, nid followed by six digits. where the XXXXXX is a six digit number starting with zero padding.\nCompute Rolling Upgrade Service (CRUS) The Compute Rolling Upgrade Service (CRUS) upgrades sets of compute nodes without requiring an entire set of nodes to be out of service at once. CRUS manages the workload management status of nodes, handling each of the steps required to upgrade compute nodes.\nFor more information, see Compute Rolling Upgrades.\nNote: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality. See Deprecated features.\nConfiguration Framework Service (CFS) The Configuration Framework Service (CFS) is available on systems for remote execution and configuration management of nodes and boot images. This includes nodes available in the Hardware State Manager (HSM) service inventory (compute, management, and application nodes), and boot images hosted by the Image Management Service (IMS).\nCFS configures nodes and images via a GitOps methodology. All configuration content is stored in the Version Control Service (VCS), and is managed by authorized system administrators. CFS provides a scalable Ansible Execution Environment (AEE) for the configuration to be applied with flexible inventory and node targeting options.\nFor more information, see Configuration Management.\nContent Projection Service (CPS) The Content Projection Service (CPS) provides the root filesystem for compute nodes and application nodes in conjunction with the Data Virtualization Service (DVS). Using CPS and DVS, the HPE Cray Programming Environment (CPE) and Analytics products are provided as separately mounted filesystems to compute nodes, application nodes (such as UANs), and worker nodes hosting UAI pods.\nCoolant Distribution Unit (CDU) See:\nCabinet Cooling Group Cabinet Environmental Controller (CEC) CEC microcontroller (eC) Chassis Management Module (CMM) Floor Standing CDU Rack-Mounted CDU Cray Advanced Platform Monitoring and Control (CAPMC) The Cray Advanced Platform Monitoring and Control (CAPMC) service enables direct hardware control of power on/off, power monitoring, or system-wide power telemetry and configuration parameters from Redfish. CAPMC implements a simple interface for powering on/off compute nodes and application nodes, querying node state information, and querying site-specific service usage rules. These controls enable external software to more intelligently manage system-wide power consumption or configuration parameters.\nFor more information, see Cray Advanced Platform Monitoring and Control.\nCray CLI (cray) The cray command line interface (CLI) is a framework created to integrate all of the system management REST APIs into easily usable commands.\nCray Operating System (COS) The Cray Operating System is a Cray product that may be installed on CSM systems.\nCray Programming Environment (CPE) The Cray Programming Environment is a Cray product that may be installed on CSM systems.\nCray Security Token Service (STS) The Cray Security Token Service (STS) generates short-lived Ceph S3 credentials.\nCray Site Init (CSI) The Cray Site Init (CSI) program creates, validates, installs, and upgrades an HPE Cray EX system. CSI can prepare the LiveCD for booting the PIT node and then is used from a booted PIT node to do its other functions during an installation. During an upgrade, CSI is installed on one of the nodes to facilitate the CSM software upgrade.\nCray System Management (CSM) Cray System Management (CSM) refers to the product stream which provides the infrastructure to manage an HPE Cray EX system using Kubernetes to manage the containerized workload of layered micro-services with well-defined REST APIs which provide the ability to discover and control the hardware platform, manage configuration of the system, configure the network, boot nodes, gather log and telemetry data, connect API access and user level access to Identity Providers (IdPs), and provide a method for system administrators and end-users to access the HPE Cray EX system.\nCSM Automatic Network Utility (CANU) CANU is a tool used to generate, validate, and test the network in a CSM environment.\nFor more information see CSM Automatic Network Utility.\nCustomer Access Network (CAN) The Customer Access Network (CAN) provides access from outside the customer network to services, Non-Compute Nodes (NCNs), and User Access Nodes (UANs) in the system. This allows for the following:\nClients outside of the system: Log in to each of the NCNs and UANs. Access web UIs within the system (e.g. Prometheus, Grafana, and more). Access the Rest APIs within the system. Access a DNS server within the system for resolution of names for the webUI and REST API services. Run Cray CLI commands from outside the system. Access the User Access Instances (UAIs). NCNs and UANs to access systems outside the cluster (e.g. LDAP, license servers, and more). Services within the cluster to access systems outside the cluster. These nodes and services need an IP address that routes to the customer\u0026rsquo;s network in order to be accessed from outside the network.\nFor more information, see:\nBifurcated CAN (BICAN) Customer Accessible Networks. Customer High Speed Network (CHN) For more information on the CHN, see Customer Accessible Networks.\nCustomer Management Network (CMN) For more information on the CMN, see Customer Accessible Networks.\nData Virtualization Service (DVS) The Data Virtualization Service (DVS) is a distributed network service that projects file systems mounted on Non-Compute Nodes (NCNs) to other nodes within the HPE Cray EX system. Projecting is the process of making a file system available on nodes where it does not physically reside. DVS-specific configuration settings enable clients to access a file system projected by DVS servers. These clients include compute nodes, User Access Nodes (UANs), and other management nodes running User Access Instances (UAIs). Thus DVS, while not a file system, represents a software layer that provides scalable transport for file system services. DVS is integrated with the Content Projection Service (CPS).\nEX Compute Cabinet This Liquid-Cooled Olympus cabinet is a dense compute cabinet that supports 64 compute blades and 64 High Speed Network (HSN) switches.\nEX TDS Cabinet A Liquid-Cooled TDS cabinet is a dense compute cabinet that supports 2-chassis, 16 compute blades and 16 High Speed Network (HSN) switches, and includes a rack-mounted 4U Coolant Distribution Unit (CDU) (MCDU-4U).\nFabric The Slingshot fabric consists of the switches, cables, ports, topology policy, and configuration settings for the Slingshot High-Speed Network.\nFirmware Action Service (FAS) The Firmware Action Service (FAS) provides an interface for managing firmware versions of Redfish-enabled hardware in the system. FAS interacts with the Hardware State Manager (HSM), device data, and image data in order to update firmware.\nFor more information, see Update firmware with FAS.\nFloor Standing CDU A floor-standing Coolant Distribution Unit (CDU) pumps liquid coolant through a cabinet group or cabinet chilled doors.\nHardware Management Network (HMN) The hardware management network (HMN) includes HMS embedded controllers. This includes chassis controllers (cC), node controllers (nC) and switch controllers (sC), for Liquid-Cooled TDS and Liquid-Cooled Olympus systems. For standard rack systems, this includes iPDUs, COTS server BMCs, or any other equipment that requires hardware-management with Redfish. The hardware management network is isolated from all other node management networks. An out-of-band Ethernet management switch and hardware management VLAN is used for customer access and administration of hardware.\nHardware Management Notification Fanout Daemon (HMNFD) The Hardware Management Notification Fanout Daemon (HMNFD) service receives component state change notifications from the HSM. It fans notifications out to subscribers (typically compute nodes).\nHardware State Manager (HSM) Hardware State Manager (HSM) service monitors and interrogates hardware components in an HPE Cray EX system, tracking hardware state and inventory information, and making it available via REST queries and message bus events when changes occur.\nFor more information, see Hardware State Manager.\nHardware State Manager (SMD) For historical reasons, SMD is also used to refer to the Hardware State Manager.\nHeartbeat Tracker Daemon (HBTD) The Heartbeat Tracker Daemon (HBTD) service listens for heartbeats from components (mainly compute nodes). It tracks changes in heartbeats and conveys changes to the HSM.\nHigh Speed Network (HSN) The High Speed Network (HSN) in an HPE Cray EX system is based on the Slingshot switches.\nImage Management Service (IMS) The Image Management Service (IMS) uses the open source Kiwi-NG tool to build image roots from recipes. IMS also uses CFS to apply image customization for pre-boot configuration of the image root. These images are bootable on compute nodes and application nodes.\nFor more information, see Image Management.\nJSON Web Token (JWT) For more information, see JSON Web Tokens (JWTs).\nKubernetes NCNs The Kubernetes NCNs are the management nodes which are known as Kubernetes master nodes (ncn-mXXX) or Kubernetes worker nodes (ncn-wXXX). The only type of management node which is excluded from this is the utility storage node (ncn-sXXX).\nManagement Cabinet At least one 19 inch IEA management cabinet is required for every HPE Cray EX system to support the management non-compute nodes (NCNs), system management network, utility storage, and other support equipment. This cabinet serves as the primary customer access point for managing the system.\nManagement Nodes The management nodes are one grouping of NCNs. The management nodes include the master nodes with hostnames of the form of ncn-mXXX, the worker nodes with hostnames of the form ncn-wXXX, and utility storage nodes, with hostnames of the form ncn-sXXX, where the XXX is a three digit number starting with zero padding. The utility storage nodes provide Ceph storage for use by the management nodes. The master nodes provide Kubernetes master functions and have the etcd cluster which provides a datastore for Kubernetes. The worker nodes provide Kubernetes worker functions where most of the containerized workload is scheduled by Kubernetes.\nThe management nodes have various roles:\nMasters nodes are Kubernetes masters. Worker nodes are Kubernetes workers and have physical connections to the High Speed Network. Storage nodes physically have more local storage for providing storage classes to Kubernetes. Mountain Cabinet See Olympus cabinet. Some software and documentation refers to the Olympus cabinet as a Mountain cabinet.\nMountain Endpoint Discovery Service (MEDS) The Mountain Endpoint Discovery Service (MEDS) manages initial discovery, configuration, and geolocation of Redfish-enabled BMCs in Liquid-Cooled Olympus cabinets. It periodically makes Redfish requests to determine if hardware is present or missing.\nNIC Mezzanine Card (NMC) The NIC mezzanine card (NMC) attaches to two host port connections on a Liquid-Cooled compute blade node card and provides the High Speed Network (HSN) controllers (NICs). There are typically two or four NICs on each node card. NMCs connect to the rear panel EXAMAX connectors on the compute blade through an internal L0 cable assembly in a single-, dual-, or quad-injection bandwidth configuration depending on the design of the node card.\nNode Controller (nC) Each compute blade node card includes an embedded node controller (nC) and REST endpoint to manage the node environmental conditions, power, HMS nFPGA interface, and firmware.\nNode Management Network (NMN) The Node Management Network (NMN) communicates with motherboard PCH-style hosts, typically 10GbE Ethernet LAN-on-motherboard (LOM) interfaces. This network supports node boot protocols (DHCP/TFTP/HTTP), in-band telemetry and event exchange, and general access to management REST APIs.\nNode Memory Dump (NMD) The Node Memory Dump service is used to interact with node memory dumps.\nNon-Compute Node (NCN) Any node which is not a compute node may be called a Non-Compute Node (NCN). The NCNs include management nodes and application nodes.\nFor more information, see Non-Compute Nodes.\nOlympus Cabinet The Olympus cabinet is a Liquid-Cooled dense compute cabinet that supports 64 compute blades and 64 High Speed Network (HSN) switches. Every HPE Cray EX system with Olympus cabinets will also have at least one River cabinet to house non-compute node components such as management nodes, management network switches, storage nodes, application nodes, and possibly other air-cooled compute nodes. Some software and documentation refers to the Olympus cabinet as a Mountain cabinet.\nParallel Application Launch Service (PALS) Parallel Application Launch Service is a Cray product that may be installed on CSM systems.\nPower Distribution Unit (PDU) The cabinet PDU receives 480VAC 3-phase facility power and provides circuit breaker, fuse protection, and EMI filtered power to the rectifier/power supplies that distribute ±190VDC (HVDC) to a chassis. PDUs are passive devices that do not connect to the SMNet.\nPre-Install Toolkit (PIT) The Pre-Install Toolkit is installed onto the initial node used as the inception node during software installation which is booted from a LiveCD. This is the node that will eventually become ncn-m001. The node running the Pre-Install Toolkit is known as the PIT node during the installation process until it reboots from a normal management node image like the other master nodes.\nEarly in the install process, before the Pre-Install Toolkit has been installed or booted, the documents may still refer to the PIT node. In this case, they are referring to the node which will eventually become the PIT node.\nIn this documentation, PIT node and LiveCD are sometimes used interchangeably.\nLiveCD The LiveCD has a complete bootable Linux operating system that can be run from a read-only CD or DVD, a writable USB flash drive, or a hard disk. It is used to bootstrap the installation process for CSM software. It contains the Pre-Install Toolkit (PIT). The node which boots from it during the install is known as the PIT node.\nRack-Mounted CDU The rack-mounted Coolant Distribution Unit (CDU) (MCDU-4U) pumps liquid coolant through the Liquid-Cooled TDS cabinet coolant manifolds.\nRack System Compute Cabinet Air-Cooled compute cabinets house a cluster of compute nodes, Slingshot ToR switches, and SMNet ToR switches.\nRedfish Translation Service (RTS) The Redfish Translation Service (RTS) aids in management of any hardware components which are not managed by Redfish, such as a ServerTech PDU in a River cabinet.\nRiver Cabinet At least one 19 inch IEA management cabinet is required for every HPE Cray EX system to support the management non-compute nodes (NCN), system management network, utility storage, and other support equipment. Additional River cabinets may be included to house storage storage or compute nodes which are not in an Olympus liquid-cooled cabinet.\nRiver Endpoint Discovery Service (REDS) The River Endpoint Discovery Service (REDS) manages initial discovery, configuration, and geolocation of Redfish-enabled BMCs in air-cooled River cabinets. It periodically makes Redfish requests to determine if hardware is present or missing.\nRosetta ASIC The Rosetta ASIC is a 64-port switch chip that forms the foundation for the Slingshot network. Each port can operate at either 100G or 200G. Each network edge port supports IEEE 802.3 Ethernet, optimized-IP based protocols, and portals (an enhanced frame format that supports higher rates of small messages).\nService/IO Cabinet An Air-Cooled service/IO cabinet houses a cluster of NCNs, Slingshot ToR switches, and management network ToR switches to support the managed ecosystem storage, network, user access services (UAS), and other IO services such as LNet and gateways.\nShasta Cabling Diagram (SHCD) The Shasta Cabling Diagram (SHCD) is a multiple tab spreadsheet prepared by HPE Cray Manufacturing with information about the components in an HPE Cray EX system. This document has much information about the system. Included in the SHCD are a configuration summary with revision history, floor layout plan, type and location of components in the air-cooled cabinets, type and location of components in the Liquid-Cooled cabinets, device diagrams for switches and nodes in the cabinets, list of source and destination of every HSN cable, list of source and destination of every cable connected to the spine switches, list of source and destination of every cable connected to the NMN, list of source and destination of every cable connected to the HMN. list of cabling for the KVM, and routing of power to the PDUs.\nSimple Storage Service (S3) CSM uses S3 to store a variety of data and artifacts.\nSlingshot Slingshot supports L1 and L2 network connectivity between 200 Gbs switch ports and L0 connectivity from a single 200 Gbs port to two 100 Gbs Mellanox ConnectX-5 NICs. Slingshot also supports edge ports and link aggregation groups (LAG) to external storage systems or networks.\nIEEE 802.3cd/bs (200 Gbps) Ethernet over 4 x 50 Gbs (PAM-4) lanes 200GBASE-DR4, 500 meter singlemode fiber 200GBASE-SR4, 100 meter multi-mode fiber 200GBASE-CR4, 3 meter copper cable IEEE 802.3cd (100 Gbps) Ethernet over 2 x 50 Gbs (PAM-4) lanes 100GBASE-SR2, 100 meter multimode fiber 100GBASE-CR2, 3 meter copper cable IEEE 802.3 2018 100 Gbps Ethernet over 4 x 25 Gbs (NRZ) lanes 100GBASE-CR4, 5 meter copper cable 100GBASE-SR4, 100 meter multi-mode fiber Optimized Ethernet and HPC fabric formats Lossy and lossless delivery Flow control, 802.1x (PAUSE), 802.1p (PFC), credit-based flow control on fabric links, fine-grain flow control on host links and edge ports, link-level retry, low latency FEC, Ethernet physical interfaces. See also:\nBlade Switch Controller (sC) Fabric High Speed Network (HSN) Rosetta ASIC Slingshot Blade Switch Slingshot Host Software (SHS) Slingshot Top of Rack (ToR) Switch Virtual Network Identifier Daemon (VNID) Slingshot Blade Switch The Liquid-Cooled Olympus cabinet blade switch supports one switch ASIC and 48 fabric ports. Eight connectors on the rear panel connect orthogonally to each compute blade then to NIC mezzanine cards (NMCs) inside the compute blade. Each rear panel EXAMAX connector supports two switch ports (a total of 16 fabric ports per blade). Twelve QSFP-DD cages on the front panel (4 fabric ports per QSFP-DD cage), fan out 48 external fabric ports to other switches. The front-panel top ports support passive electrical cables (PEC) or active optical cables (AOC). The front-panel bottom ports support only PECs for proper cooling in the blade enclosure.\nSlingshot Host Software (SHS) Slingshot Host Software is a Cray product that may be installed on CSM systems to support Slingshot.\nSlingshot Top of Rack (ToR) Switch A standard River cabinet can support one, two, or four, rack-mounted Slingshot ToR switches. Each switch supports a total of 64 fabric ports. 32 QSFP-DD connectors on the front panel connect 64 ports to the fabric. All front-panel connectors support either passive electrical cables (PEC) or active optical cables (AOC).\nSupply/Return Cutoff Valves Manual coolant supply and return shutoff valves at the top of each cabinet can be closed to isolate a single cabinet from the other cabinets in the cooling group for maintenance. If the valves are closed during operation, the action automatically causes the CMMs to remove ±190VDC from each chassis in the cabinet because of the loss of coolant pressure.\nSystem Admin Toolkit (SAT) The System Admin Toolkit (SAT) product provides the sat command line interface which interacts with the REST APIs of many services to perform more complex system management tasks.\nSystem Configuration Service (SCSD) The System Configuration Service (SCSD) allows administrators to set various BMC and controller parameters. These parameters are typically set during discovery, but this tool enables parameters to be set before or after discovery.\nSystem Diagnostic Utility (SDU) The System Diagnostic Utility is a Cray product that may be installed on CSM systems to provide diagnostic tools.\nSystem Layout Service (SLS) The System Layout Service (SLS) serves as a \u0026ldquo;single source of truth\u0026rdquo; for the system design. It details the physical locations of network hardware, management nodes, application nodes, compute nodes, and cabinets. It also stores information about the network, such as which port on which switch should be connected to each node.\nFor more information, see System Layout Service.\nSystem Management Network (SMNet) The System Management Network (SMNet) is a dedicated out-of-band (OOB) spine-leaf topology Ethernet network that interconnects all the nodes in the system to management services.\nSystem Management Services (SMS) System Management Services (SMS) leverages open REST APIs, Kubernetes container orchestration, and a pool of commercial off-the-shelf (COTS) servers to manage the system. The management server pool, custom Redfish-enabled embedded controllers, iPDU controllers, and server BMCs are unified under a common software platform that provides 3 levels of management: Level 1 HaaS, Level 2 IaaS, and Level 3 PaaS.\nSystem Management Services (SMS) nodes System Management Services (SMS) nodes provide access to the entire management cluster and Kubernetes container orchestration.\nSystem Monitoring Application (SMA) The System Monitoring Application (SMA) is one of the services that collects CSM system data for administrators.\nSystem Monitoring Framework (SMF) Another name for the System Monitoring Application (SMA) Framework.\nTop of Rack Switch Controller (sC-ToR) The air-Cooled cabinet HSN ToR switch embedded controller (sC-ToR) provides a hardware management REST endpoint to monitor the ToR switch environmental conditions and manage the switch power, HSN ASIC, and FPGA interfaces.\nUser Access Instance (UAI) The User Access Instance (UAI) is a lightweight, disposable platform that runs under Kubernetes orchestration on worker nodes. The UAI provides a single user containerized environment for users on a Cray Ex system to develop, build, and execute their applications on the HPE Cray EX compute node. See UAN for another way for users to gain access.\nFor more information, see User Access Service.\nUser Access Node (UAN) The User Access Node (UAN) is an NCN, but is really one of the special types of application nodes. The UAN provides a traditional multi-user Linux environment for users on a Cray Ex system to develop, build, and execute their applications on the HPE Cray EX compute node. See UAI for another way for users to gain access. Some sites refer to their UANs as Login nodes.\nUser Access Service (UAS) The User Access Service (UAS) is a containerized service managed by Kubernetes that enables users to create and run user applications inside a UAI. UAS runs on a management node that is acting as a Kubernetes worker node. When a user requests a new UAI, the UAS service returns status and connection information to the newly created UAI. External access to UAS is routed through a node that hosts gateway services.\nFor more information, see User Access Service.\nVersion Control Service (VCS) The Version Control Service (VCS) provides configuration content to CFS via a GitOps methodology based on a git server (gitea) that can be accessed by the git command but also includes a web interface for repository management, pull requests, and a visual view of all repositories and organizations.\nFor more information, see Version Control Service.\nVirtual Network Identifier Daemon (VNID) The Virtual Network Identifier Daemon is part of the Cray Slingshot product that may be installed on CSM systems.\nxname Component names (xnames) identify the geolocation for hardware components in the HPE Cray EX system. Every component is uniquely identified by these component names. Some, like the system cabinet number or the Coolant Distribution Unit (CDU) number, can be changed by site needs. There is no geolocation encoded within the cabinet number, such as an X-Y coordinate system to relate to the floor layout of the cabinets. Other component names refer to the location within a cabinet and go down to the port on a card or switch or the socket holding a processor or a memory DIMM location.\nFor more information, see Component Names (xnames).\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/hpe_systems_not_transitioning_power_state/",
	"title": "HPE iLO dropping event subscriptions and not properly transitioning power state in CSM software",
	"tags": [],
	"description": "",
	"content": "HPE iLO dropping event subscriptions and not properly transitioning power state in CSM software HPE Systems impacted:\nDL325 DL385 Apollo 6500 When HPE iLO systems are not properly transitioning power state in HMS/SAT this could indicate that Redfish events are not being received by the HMS HM-Collector. When this occurs, the HPE iLO receives an error back from its attempt to send events and will delete the subscription if there are enough failures. To detect this state, look at the subscription numbers under /redfish/v1/EventService/Subscriptions. If subscriptions are missing or are extremely large and increasing, this would indicate the iLO is receiving an error when trying to send Redfish events.\nCheck subscriptions on affected BMC\nncn-m# curl -sk -u root:$PASSWD https://${BMC}/redfish/v1/EventService/Subscriptions | jq -c \u0026#39;.Members[]\u0026#39; If there is at least one subscription and it is a low number, everything is OK. No action is needed. If there is at least one subscription and it is large number, verify it is not increasing by executing the above command several times over ~10 minutes. If the subscription number is not increasing, everything is OK. No action is needed. If the subscription number is increasing, the BMC will need to be reset. If there are no subscriptions, the BMC will need to be reset. Reset BMC with ipmitool\nncn-m# ipmitool -H $BMC -U root -P $PASSWD -I lanplus mc reset cold "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/collect_information_about_the_ceph_cluster/",
	"title": "Collect Information about the Ceph Cluster",
	"tags": [],
	"description": "",
	"content": "Collect Information about the Ceph Cluster These general commands for Ceph are helpful for obtaining information pertinent to troubleshooting issues.\nAs a reference, the Ceph commands below are run from a ceph-mon node. Certain commands will work on different systems. For example, the rbd command can be used on the worker nodes if specifying the proper key.\nCeph Log and File Locations Ceph configurations are located under /etc/ceph/ceph.conf Ceph data structure and bootstrap is located under /var/lib/ceph// Ceph logs are now accessible by a couple of different methods Utilizing cephadm ls to retrieve the systemd_unit on the node for the process, then utilize journalctl to dump the logs ceph log last [\u0026lt;num:int\u0026gt;] [debug|info|sec|warn|error] [*|cluster|audit|cephadm] Note that that this will dump general cluster logs cephadm logs [-h] [--fsid FSID] --name \u0026lt;systemd_unit\u0026gt; Check the Status of Ceph Print the status of the Ceph cluster with the following command:\nncn-m001# ceph -s Example output:\ncluster: id: 5f3b4031-d6c0-4118-94c0-bffd90b534eb health: HEALTH_OK \u0026lt;\u0026lt;-- WARN/ERROR/CRITICAL are other states services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) \u0026lt;\u0026lt;-- Should have quorum and ideally an odd number of mon nodes mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 \u0026lt;\u0026lt;-- The watchdog for the cluster mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby \u0026lt;\u0026lt;-- Filesystem service osd: 18 osds: 18 up (since 20h), 18 in (since 9d) \u0026lt;\u0026lt;-- Data devices: 1 OSD = 1 hard drive designated for Ceph rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) \u0026lt;\u0026lt;-- Object storage data: \u0026lt;\u0026lt;-- Health stats related to the above services pools: 11 pools, 220 pgs objects: 825.66k objects, 942 GiB usage: 2.0 TiB used, 61 TiB / 63 TiB avail pgs: 220 active+clean io: client: 2.5 KiB/s rd, 13 MiB/s wr, 2 op/s rd, 1.27k op/s wr The -w option can be used to watch the cluster.\nPrint the OSD tree Check the OSD status, weight, and location.\nncn-m001# ceph osd tree Example output:\nID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 62.85938 root default -7 20.95312 host ncn-s001 1 ssd 3.49219 osd.1 up 1.00000 1.00000 5 ssd 3.49219 osd.5 up 1.00000 1.00000 6 ssd 3.49219 osd.6 up 1.00000 1.00000 7 ssd 3.49219 osd.7 up 1.00000 1.00000 8 ssd 3.49219 osd.8 up 1.00000 1.00000 9 ssd 3.49219 osd.9 up 1.00000 1.00000 -5 20.95312 host ncn-s002 2 ssd 3.49219 osd.2 up 1.00000 1.00000 4 ssd 3.49219 osd.4 up 1.00000 1.00000 10 ssd 3.49219 osd.10 up 1.00000 1.00000 11 ssd 3.49219 osd.11 up 1.00000 1.00000 12 ssd 3.49219 osd.12 up 1.00000 1.00000 13 ssd 3.49219 osd.13 up 1.00000 1.00000 -3 20.95312 host ncn-s003 0 ssd 3.49219 osd.0 up 1.00000 1.00000 3 ssd 3.49219 osd.3 up 1.00000 1.00000 14 ssd 3.49219 osd.14 up 1.00000 1.00000 15 ssd 3.49219 osd.15 up 1.00000 1.00000 16 ssd 3.49219 osd.16 up 1.00000 1.00000 17 ssd 3.49219 osd.17 up 1.00000 1.00000 Storage Utilization The following command shows the storage utilization of the cluster and pools:\nncn-m001# ceph df Example output:\nRAW STORAGE: CLASS SIZE AVAIL USED RAW USED %RAW USED kube 27 GiB 15 GiB 9.2 GiB 12 GiB 45.25 smf 57 GiB 30 GiB 24 GiB 27 GiB 48.09 TOTAL 84 GiB 44 GiB 34 GiB 40 GiB 47.18 POOLS: POOL ID STORED OBJECTS USED %USED MAX AVAIL cephfs_data 1 0 B 0 0 B 0 13 GiB cephfs_metadata 2 2.2 KiB 22 1.5 MiB 0 13 GiB .rgw.root 3 1.2 KiB 4 768 KiB 0 13 GiB defaults.rgw.buckets.data 4 0 B 0 0 B 0 13 GiB default.rgw.control 5 0 B 8 0 B 0 13 GiB defaults.rgw.buckets.index 6 0 B 0 0 B 0 13 GiB default.rgw.meta 7 0 B 0 0 B 0 13 GiB default.rgw.log 8 0 B 175 0 B 0 13 GiB kube 10 3.1 GiB 799 9.2 GiB 40.64 4.5 GiB smf 11 8.1 GiB 2.11k 24 GiB 47.71 8.9 GiB Show OSD Usage Show the utilization of the OSDs with the following command. This is very helpful to see if the data is not balanced across OSDs, which can create hotspots.\nncn-m001# ceph osd df Example output:\nID CLASS WEIGHT REWEIGHT SIZE RAW USE DATA OMAP META AVAIL %USE VAR PGS STATUS 1 kube 0.00879 1.00000 9 GiB 4.1 GiB 3.1 GiB 0 B 1 GiB 4.9 GiB 45.25 0.96 99 up 4 smf 0.01859 1.00000 19 GiB 9.1 GiB 8.1 GiB 0 B 1 GiB 9.9 GiB 48.09 1.02 141 up 0 kube 0.00879 1.00000 9 GiB 4.1 GiB 3.1 GiB 0 B 1 GiB 4.9 GiB 45.25 0.96 93 up 3 smf 0.01859 1.00000 19 GiB 9.1 GiB 8.1 GiB 0 B 1 GiB 9.9 GiB 48.09 1.02 147 up 2 kube 0.00879 1.00000 9 GiB 4.1 GiB 3.1 GiB 0 B 1 GiB 4.9 GiB 45.25 0.96 100 up 5 smf 0.01859 1.00000 19 GiB 9.1 GiB 8.1 GiB 0 B 1 GiB 9.9 GiB 48.09 1.02 140 up TOTAL 84 GiB 40 GiB 34 GiB 0 B 6 GiB 44 GiB 47.18 MIN/MAX VAR: 0.96/1.02 STDDEV: 1.51 Check the Status of a Single OSD Use the following command to obtain information about a single OSD using the OSD number. For example, osd.0 would be an OSD number.\nncn-m001# ceph osd find OSD.ID Example output:\n{ \u0026#34;osd\u0026#34;: 1, \u0026#34;addrs\u0026#34;: { \u0026#34;addrvec\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;addr\u0026#34;: \u0026#34;10.248.2.127:6800\u0026#34;, \u0026#34;nonce\u0026#34;: 4966 } ] }, \u0026#34;osd_fsid\u0026#34;: \u0026#34;9d41f723-e86f-4b98-b1e7-12c0f5f15546\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;ceph-1\u0026#34;, \u0026#34;crush_location\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;ceph-1\u0026#34;, \u0026#34;root\u0026#34;: \u0026#34;default\u0026#34; } } List Storage Pools List the storage pools with the following commands:\nncn-m001# ceph osd lspools Example output:\n1 cephfs_data 2 cephfs_metadata 3 .rgw.root 4 defaults.rgw.buckets.data 5 default.rgw.control 6 defaults.rgw.buckets.index 7 default.rgw.meta 8 default.rgw.log 10 kube 11 smf "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/change_keycloak_token_lifetime/",
	"title": "Change the Keycloak Token Lifetime",
	"tags": [],
	"description": "",
	"content": "Change the Keycloak Token Lifetime This document outlines how to change the Keycloak default token lifetime or the token lifetime for a specific client.\nNote: The default value for these settings is 365 days.\nProcedure Log in to Keycloak with the default admin credentials.\nPoint a browser at https://auth.cmn.SYSTEM_DOMAIN_NAME/keycloak/admin, replacing SYSTEM_DOMAIN_NAME with the actual NCN\u0026rsquo;s DNS name. Use of the auth.cmn. sub-domain is required for administrative access to Keycloak.\nThe following is an example URL for a system: https://auth.cmn.system1.us.cray.com/keycloak/admin\nUse the following admin login credentials:\nUsername: admin The password can be obtained with the following command: ncn# kubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode Change Global Token Lifetime Values Select Realm Settings under Configure on the left of the admin page. Select the Tokens tab. Change the following options to the appropriate lifetime values: SSO Session Idle SSO Session Max Access Token Lifespan Access Token Lifespan for Implicit Flow Click Save at the bottom of the page. Change A Specific Client\u0026rsquo;s Token Lifetime Select Clients under Configure on the left of the admin page. Select the client that you wish to change the token lifetime for. Expand Advanced Settings. Change the Access Token Lifespan to the appropriate lifetime value. Click Save at the bottom of the page. "
},
{
	"uri": "/docs-csm/en-12/operations/power_management/save_management_network_switch_configurations/",
	"title": "Save Management Network Switch Configuration Settings",
	"tags": [],
	"description": "",
	"content": "Save Management Network Switch Configuration Settings Switches must be powered on and operating. This procedure is optional if switch configurations have not changed.\nOptional Task: Save management network switch configurations before removing power from cabinets or the CDU. Management switch names are listed in the /etc/hosts file.\nObtain the list of switches From the command line on any NCN run:\nncn# grep \u0026#39;sw-\u0026#39; /etc/hosts Example output:\n10.252.0.2 sw-spine-001 10.252.0.3 sw-spine-002 10.252.0.4 sw-leaf-001 Save switch configurations Aruba switch and HPE server systems On Aruba-based systems all management network switches will be Aruba and the following procedure. For each switch:\nssh to the switch Execute the write memory command Exit the switch shell Example:\nncn# ssh admin@sw-spine-001.nmn admin@sw-spine-001 password: sw-spine-001# write memory sw-spine-001# exit Dell and Mellanox switch and Gigabyte/Intel server systems On Dell and Mellanox based systems, all spine and any leaf switches will be Mellanox. Any Leaf-BMC and CDU switches will be Dell. The overall procedure is the same but the specifics of execution are slightly different.\nssh to the switch Enter enable mode (Mellanox only) Execute the write memory command Exit the switch shell Mellanox example:\nsw-spine-001# enable sw-spine-001# write memory sw-spine-001# exit Dell example:\nsw-leaf-001# write memory sw-leaf-001# exit Edge routers and storage switches Save configuration settings on Edge Router switches (Arista, Aruba or Juniper) that connect customer storage networks to the Slingshot network if these switches exist in the site and the configurations have changed. Edge switches are accessible from the ClusterStor management network and the CSM management network.\nExample:\nncn# ssh admin@cls01053n00 admin@cls01053n00 password: cls01053n00# ssh r0-100gb-sw01 r0-100gb-sw01# enable r0-100gb-sw01# write memory r0-100gb-sw01# exit Next step Return to System Power Off Procedures and continue with next step.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/reset_credentials_on_redfish_devices_for_reinstallation/",
	"title": "Reset Credentials on Redfish Devices",
	"tags": [],
	"description": "",
	"content": "Reset Credentials on Redfish Devices Before re-installing or upgrading the system the credentials need to be changed back to their defaults for any devices that had their credentials changed post-install. This is necessary for the installation process to properly discover and communicate with these devices.\nPrerequisites Administrative privileges are required.\nProcedure Create an SCSD payload file with the default credentials for the Redfish devices that have been changed from the defaults.\nThe following example shows a payload file that will set the devices x0c0s0b0 and x0c0s1b0 back to the default Redfish credentials.\n{ \u0026#34;Force\u0026#34;: false, \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;\u0026lt;BMC root password\u0026gt;\u0026#34;, \u0026#34;Targets\u0026#34;: [ \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;x0c0s1b0\u0026#34; ] } Set credentials for multiple targets.\nncn-m001# cray scsd bmc globalcreds create PAYLOAD_FILE --format json Example output:\n{ \u0026#34;Targets\u0026#34;: [ { \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s1b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, ] } For more information about using the System Configuration Service, refer to System Configuration Service.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_remove_replace_ncns/validate_health/",
	"title": "Validate Health",
	"tags": [],
	"description": "",
	"content": "Validate Health Description Validate that the system is healthy.\nProcedure The following procedures can be run from any master or worker node.\nCollect data about the system management platform health.\nncn-mw# /opt/cray/platform-utils/ncnHealthChecks.sh ncn-mw# /opt/cray/platform-utils/ncnPostgresHealthChecks.sh NOTE: If workers have been removed and the worker count is currently at two, the following failures can be ignored. A re-check will be needed once workers are added and the count returns to three or above.\nthe ncnPostgresHealthChecks may report Unable to determine a leader and one of the three Postgres pods may be in Pending state. the ncnHealthChecks may report Error from server...FAILED - Pod Not Healthy, FAILED DATABASE CHECK and one of the three Etcd pods may be in Pending state. NOTE: If ncn-s001, ncn-s002, or ncn-s003 has been temporarily removed, HEALTH_WARN may be seen until the storage node is added back to the cluster.\nthe ncnHealthChecks may report FAIL: Ceph's health status is not \u0026quot;HEALTH_OK\u0026quot;. If Ceph health is HEALTH_WARN, this failure can be ignored. Restart the Goss server on all the management nodes.\nncn-mw# pdsh -w $(grep -oP \u0026#39;ncn-\\w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) \\ systemctl restart goss-servers Specify the admin user password for the management switches in the system.\nread -s is used in order to prevent the password from being echoed to the screen or saved in the shell history.\nncn-mw# read -s SW_ADMIN_PASSWORD ncn-mw# export SW_ADMIN_PASSWORD Collect data about the various subsystems.\nncn-mw# /opt/cray/tests/install/ncn/automated/ncn-healthcheck-master ncn-mw# /opt/cray/tests/install/ncn/automated/ncn-healthcheck-worker ncn-mw# /opt/cray/tests/install/ncn/automated/ncn-healthcheck-storage ncn-mw# /opt/cray/tests/install/ncn/automated/ncn-kubernetes-checks NOTE: The following errors can be ignored if \u0026lt;NODE\u0026gt; has been removed and it is one of the first three worker, master, or storage nodes:\nServer URL: http://\u0026lt;NODE\u0026gt; ... ERROR: Server endpoint could not be reached. NOTE: If workers have been removed and the worker count is currently at two, failures for the following tests can be ignored:\nKubernetes Postgres Clusters have the Correct Number of Pods 'Running' Kubernetes Postgres Clusters Have Leaders Kubernetes Postgres Check for Replication Lag Across Pods in a Cluster Verify cray etcd is healthy A re-check will be needed once workers are added and the count returns to three or above.\nNOTE: If a storage node has been added, then ncn-healthcheck-storage failures for the following test may need to be remediated based on the test description information. After that is done, the ncn-healthcheck-storage tests should then be re-run to verify that all tests pass.\nSpire Health Check The procedure is complete. Return to Main Page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/dhcp_decline/",
	"title": "Large Number of DHCP Declines During a Node Boot",
	"tags": [],
	"description": "",
	"content": "Large Number of DHCP Declines During a Node Boot If something similar to the following is in the logs, then this indicates an issue that an IP address being allocated is already being used. It is not able to get the IP address assigned to the device.\ndracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.56 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.57 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.58 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.59 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.60 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.51 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.53 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.54 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.61 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.62 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.63 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.64 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.65 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.66 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.67 Procedure Check by MAC address (no colons):\nThis requires an API token. See Retrieve an Authentication Token for more information.\nncn# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api_gw_service.local/apis/smd/hsm/v2/Inventory/EthernetInterfaces/18c04d13d73c Check by component name (xname):\nThis requires an API token. See Retrieve an Authentication Token for more information.\nncn# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api_gw_service.local/apis/smd/hsm/v2/Inventory/EthernetInterfaces?ComponentID=x3000c0s25b0n0 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/loopback/",
	"title": "Configure Loopback Interface",
	"tags": [],
	"description": "",
	"content": "Configure Loopback Interface Loopbacks can be thought of as internal virtual interfaces. Loopback interfaces are not bound to a physical port and are used for device management and routing protocols.\nConfiguration Commands switch(config)# interface loopback LOOPBACK switch(config-loopback-if)# ip address IP-ADDR/\u0026lt;SUBNET|PREFIX\u0026gt; Expected Results Create a loopback interface. Give a loopback interface an IP address. Validate the configuration using the show commands. Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/collect_data/",
	"title": "Collect Data",
	"tags": [],
	"description": "",
	"content": "Collect Data Collect the input data needed to generate switch configurations.\nPrerequisites SSH access to the switches System Layout Service (SLS) API access Procedure Retrieve the most up-to-date SHCD spreadsheet. Accuracy in this spreadsheet is critical.\nFor example:\nInternal repository Customer repository Get an SLS file from a Shasta system.\nLog into any NCN where the Cray CLI is configured. Then run this command to create an SLS file named sls_file.json in the current directory.\nIMPORTANT: If this is an upgrade SLS needs to be updated to the correct CSM version first.\nncn# cray sls dumpstate list --format json \u0026gt;\u0026gt; sls_file.json Retrieve switch running configurations.\nCANU can backup all the management network switches using either the SLS input file or the SLS API. This can also be done from outside the cluster using the CMN switch IP addresses.\nncn# canu backup network --folder switch_backups/ --sls-file ./sls_input_file_1_2.json Example output:\nEnter the switch password: - Running Configs Saved --------------------- sw-spine-001.cfg sw-spine-002.cfg sw-leaf-001.cfg sw-leaf-002.cfg sw-leaf-003.cfg sw-leaf-004.cfg sw-leaf-bmc-001.cfg sw-leaf-bmc-002.cfg sw-cdu-001.cfg sw-cdu-002.cfg If the SLS API is up, an SLS file does not need to be provided.\nRetrieve the customizations file.\nLog into ncn-m001 and run the following command:\nncn# kubectl -n loftsman get secret site-init -o json | jq -r \u0026#39;.data.\u0026#34;customizations.yaml\u0026#34;\u0026#39; | base64 -d \u0026gt; customizations.yaml This will output the customizations file to a file called customizations.yaml in the current working directory.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/check_kea_dhcp_logs/",
	"title": "Check KEA DHCP Logs",
	"tags": [],
	"description": "",
	"content": "Check KEA DHCP Logs Use this procedure to check the logs for a cray-dhcp-kea pod.\nProcedure Retrieve the pod name.\nThe pod name is required to check the logs for the pod in question.\nkubectl logs -n services -l app.kubernetes.io/instance=cray-dhcp-kea -c cray-dhcp-kea Example output:\n2020-08-03 21:47:50.580 INFO [kea-dhcp4.dhcpsrv/10] DHCPSRV_MEMFILE_LEASE_FILE_LOAD loading leases from file /cray-dhcp-kea-socket/dhcp4.leases 2020-08-03 21:47:50.580 INFO [kea-dhcp4.dhcpsrv/10] DHCPSRV_MEMFILE_LFC_SETUP setting up the Lease File Cleanup interval to 3600 sec 2020-08-03 21:47:50.580 WARN [kea-dhcp4.dhcpsrv/10] DHCPSRV_OPEN_SOCKET_FAIL failed to open socket: the interface eth0 has no usable IPv4 addresses configured 2020-08-03 21:47:50.580 WARN [kea-dhcp4.dhcpsrv/10] DHCPSRV_NO_SOCKETS_OPEN no interface configured to listen to DHCP traffic 2020-08-03 21:48:00.602 INFO [kea-dhcp4.commands/10] COMMAND_RECEIVED Received command \u0026#39;lease4-get-all\u0026#39; {\u0026#34;Dhcp4\u0026#34;: {\u0026#34;control-socket\u0026#34;: {\u0026#34;socket-name\u0026#34;: \u0026#34;/cray-dhcp-kea-socket/cray-dhcp-kea.socket\u0026#34;, \u0026#34;socket-type\u0026#34;: \u0026#34;unix\u0026#34;}, \u0026#34;hooks-libraries\u0026#34;: [{\u0026#34;library\u0026#34;: \u0026#34;/usr/local/lib/kea/hooks/libdhcp_lease_cmds.so\u0026#34;}, [...] waiting 10 seconds for any leases to be given out... [{\u0026#39;arguments\u0026#39;: {\u0026#39;leases\u0026#39;: []}, \u0026#39;result\u0026#39;: 3, \u0026#39;text\u0026#39;: \u0026#39;0 IPv4 lease(s) found.\u0026#39;}] 2020-08-03 21:48:22.734 INFO [kea-dhcp4.commands/10] COMMAND_RECEIVED Received command \u0026#39;config-get\u0026#39; View the Kea logs.\nncn-w001# kubectl logs -n services -l app.kubernetes.io/instance=cray-dhcp-kea -c cray-dhcp-kea | grep -i error Shell into a Kea Pod.\nncn-w001# kubectl exec -n services -it pod/$(kubectl get -n services pods | grep kea | head -n 1) -c cray-dhcp-kea -- /bin/bash Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/create_a_manual_backup_of_a_healthy_etcd_cluster/",
	"title": "Create a Manual Backup of a Healthy etcd Cluster",
	"tags": [],
	"description": "",
	"content": "Create a Manual Backup of a Healthy etcd Cluster Manually create a backup of a healthy etcd cluster and check to see if the backup was created successfully.\nBackups of healthy etcd clusters can be used to restore the cluster if it becomes unhealthy at any point.\nThe commands in this procedure can be run on any master node (ncn-mXXX) or worker node (ncn-wXXX) on the system.\nPrerequisites A healthy etcd cluster is available on the system. See Check the Health and Balance of etcd Clusters.\nProcedure Create a backup for the desired etcd cluster.\nThe example below is backing up the etcd cluster for the Boot Orchestration Service (BOS). The returned backup name (cray-bos-etcd-cluster-manual-backup-25847) will be used in the next step.\nncn-w001# kubectl exec -it -n operators \\ $(kubectl get pod -n operators | grep etcd-backup-restore | head -1 | awk \u0026#39;{print $1}\u0026#39;) \\ -c util -- create_backup cray-bos wednesday-manual-backup Example output:\netcdbackup.etcd.database.coreos.com/cray-bos-etcd-cluster-manual-backup-25847 created Check the status of the backup using the name returned in the output of the previous step.\nncn-w001# kubectl -n services get BACKUP_NAME -o yaml Example output:\nstatus: etcdRevision: 1 etcdVersion: 3.3.8 lastSuccessDate: \u0026#34;2020-01-13T21:38:47Z\u0026#34; succeeded: true "
},
{
	"uri": "/docs-csm/en-12/operations/image_management/upload_and_register_an_image_recipe/",
	"title": "Upload and Register an Image Recipe",
	"tags": [],
	"description": "",
	"content": "Upload and Register an Image Recipe Download and expand recipe archives from S3 and IMS. Modify and upload a recipe archive, and then register that recipe archive with IMS.\nPrerequisites Limitations Register recipe with IMS Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCNs) and include the following deployment: cray-ims, the Image Management Service (IMS) The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system. A token providing Simple Storage Service (S3) credentials has been generated. Limitations The commands in this procedure must be run as the root user. The IMS tool currently only supports Kiwi-NG recipe types. Register recipe with IMS Locate the desired recipe to download from S3.\nThere may be multiple records returned. Ensure that the correct record is selected in the returned data.\nncn-mw# cray ims recipes list --format toml Excerpt from example output:\n[[results]] id = \u0026#34;76ef564d-47d5-415a-bcef-d6022a416c3c\u0026#34; name = \u0026#34;cray-sles15-barebones\u0026#34; created = \u0026#34;2020-02-05T19:24:22.621448+00:00\u0026#34; [results.link] path = \u0026#34;s3://ims/recipes/76ef564d-47d5-415a-bcef-d6022a416c3c/cray-sles15-barebones.tgz\u0026#34; etag = \u0026#34;28f3d78c8cceca2083d7d3090d96bbb7\u0026#34; type = \u0026#34;s3\u0026#34; Create variables for the S3 bucket and key values from the S3 path in the returned data of the previous step.\nncn-mw# S3_ARTIFACT_BUCKET=ims ncn-mw# S3_ARTIFACT_KEY=recipes/76ef564d-47d5-415a-bcef-d6022a416c3c/cray-sles15-barebones.tgz ncn-mw# ARTIFACT_FILENAME=cray-sles15-barebones.tgz Download the recipe archive.\nUse the variables created in the previous step when running the following command.\nncn-mw# cray artifacts get $S3_ARTIFACT_BUCKET $S3_ARTIFACT_KEY $ARTIFACT_FILENAME Expand the recipe with tar.\nncn-mw# mkdir image-recipe ncn-mw# tar xvf $ARTIFACT_FILENAME -C image-recipe Modify the recipe by editing the files and subdirectories in the image-recipe directory.\nA Kiwi recipe consists of multiple files and directories, which together define the repositories, packages, and post-install actions to take during the Kiwi build process.\nEdit the config.xml file to modify the name of the recipe, the set of RPM packages being installed, or the RPM repositories being referenced. Kiwi-NG supports multiple ways to modify the post-install configuration of the image root, including some shell scripts (config.sh, images.sh) and the root/overlay directory. To learn how these can be used to add specific configuration to the image root, see the Kiwi-NG documentation. Recipes built by IMS are required to reference repositories that are hosted on the NCN by the Nexus service. Locate the directory containing the Kiwi-NG image description files.\nThis step should be done after the recipe has been changed.\nncn-mw# cd image-recipe Set an environment variable for the name of the file that will contain the archive of the image recipe.\nncn-mw# ARTIFACT_FILE=my_recipe.tgz Create a tgz archive of the image recipe.\nncn-mw# tar cvfz ../$ARTIFACT_FILE . ncn-mw# cd .. Create a new IMS recipe record.\nncn-mw# cray ims recipes create --name \u0026#34;My Recipe\u0026#34; \\ --recipe-type kiwi-ng --linux-distribution sles15 \\ --format toml Example output:\ncreated = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34; linux_distribution = \u0026#34;sles15\u0026#34; name = \u0026#34;my_recipe.tgz\u0026#34; recipe_type = \u0026#34;kiwi-ng\u0026#34; Create a variable for the id value in the returned data.\nncn-mw# IMS_RECIPE_ID=2233c82a-5081-4f67-bec4-4b59a60017a6 Upload the customized recipe to S3.\nIt is suggested as a best practice that the S3 object name start with recipes/ and contain the IMS recipe ID, in order to remove ambiguity.\nncn-mw# cray artifacts create ims recipes/$IMS_RECIPE_ID/$ARTIFACT_FILE $ARTIFACT_FILE Update the IMS recipe record with the S3 path to the recipe archive.\nncn-mw# cray ims recipes update $IMS_RECIPE_ID --link-type s3 \\ --link-path s3://ims/recipes/$IMS_RECIPE_ID/$ARTIFACT_FILE --format toml Example output:\nid = \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34; recipe_type = \u0026#34;kiwi-ng\u0026#34; linux_distribution = \u0026#34;sles15\u0026#34; name = \u0026#34;my_recipe.tgz\u0026#34; created = \u0026#34;2020-02-05T19:24:22.621448+00:00\u0026#34; [link] path = \u0026#34;s3://ims/recipes/2233c82a-5081-4f67-bec4-4b59a60017a6/my_recipe.tgz\u0026#34; etag = \u0026#34;\u0026#34; type = \u0026#34;s3\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/lock_and_unlock_management_nodes/",
	"title": "Lock and Unlock Management Nodes",
	"tags": [],
	"description": "",
	"content": "Lock and Unlock Management Nodes The ability to ignore non-compute nodes (NCNs) is turned off by default. Management nodes, NCNs, and their BMCs are also not locked by default. The administrator must lock the NCNs and their BMCs to prevent unwanted actions from affecting these nodes.\nThis section only covers using locks with the Hardware State Manager (HSM). For more information on ignoring nodes, refer to the following sections:\nFirmware Action Service (FAS): See Ignore Node within FAS. Cray Advanced Platform Monitoring and Control (CAPMC): See Ignore Nodes with CAPMC The following actions can be prevented when a node and its BMC is locked.\nFirmware upgrades with FAS Power off operations with CAPMC Reset operations with CAPMC Doing any of these actions by accident will shut down a management node. If the node is a Kubernetes master or worker node, this can have serious negative effects on system operations. If a single node is taken down by mistake, it is possible that services will recover. If all management nodes are taken down, or all Kubernetes worker nodes are taken down by mistake, the system must be restarted.\nAfter critical nodes are locked, power/reset (CAPMC) or firmware (FAS) operations cannot affect the nodes unless they are unlocked. For example, any locked node that is included in a list of nodes to be reset will result in a failure.\nTopics Lock and Unlock Management Nodes Topics When To Lock Management Nodes When To Unlock Management Nodes How To Lock Management Nodes Script Manual Steps To lock all nodes (and their BMCs) with the Management role To lock single nodes or lists of specific nodes (and their BMCs) How To Check For Locked Management Nodes How To Unlock Management Nodes To unlock all nodes (and their BMCs) with the Management role To unlock single or lists of specific nodes (and their BMCs) When To Lock Management Nodes To best protect system health, NCNs and their BMCs should be locked as early as possible in the install/upgrade cycle. The later in the process, the more risk there is of accidentally taking down a critical node. NCN locking must be done after Kubernetes is running and the HSM service is operational.\nCheck whether HSM is running with the following command:\nncn# kubectl -n services get pods | grep smd Example output:\ncray-smd-848bcc875c-6wqsh 2/2 Running 0 9d cray-smd-848bcc875c-hznqj 2/2 Running 0 9d cray-smd-848bcc875c-tp6gf 2/2 Running 0 6d22h cray-smd-init-2tnnq 0/2 Completed 0 9d cray-smd-postgres-0 2/2 Running 0 19d cray-smd-postgres-1 2/2 Running 0 6d21h cray-smd-postgres-2 2/2 Running 0 19d cray-smd-wait-for-postgres-4-7c78j 0/3 Completed 0 9d The cray-smd pods need to be in the Running state, except for cray-smd-init and cray-smd-wait-for-postgres which should be in Completed state.\nWhen To Unlock Management Nodes Any time a management NCN has to be power cycled, reset, or have its firmware updated, it and its BMC will first need to be unlocked. After the operation is complete, the targeted nodes and BMCs should once again be locked.\nHow To Lock Management Nodes Script Run the lock_management_nodes.py script to lock all management nodes and BMCs that are not already locked:\nncn# /opt/cray/csm/scripts/admin_access/lock_management_nodes.py The return value of the script is 0 if locking was successful. A non-zero return code means that manual intervention may be needed to lock the nodes. Continue below for manual steps.\nManual Steps Use the cray hsm locks lock command to perform locking.\nNOTE: When locking NCNs, you must lock their node BMCs as well.\nNOTE: The following steps assume both the management nodes and their BMCs are marked with the Management role in HSM. If they are not, see Set BMC Management Role.\nTo lock all nodes (and their BMCs) with the Management role The processing-model rigid parameter means that the operation must succeed on all target nodes or the entire operation will fail.\nLock the management nodes and BMCs.\nncn# cray hsm locks lock create --role Management --processing-model rigid Example output:\nFailure = [] [Counts] Total = 16 Success = 16 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x3000c0s5b0n0\u0026#34;, \u0026#34;x3000c0s4b0n0\u0026#34;, \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;x3000c0s6b0n0\u0026#34;, \u0026#34;x3000c0s3b0n0\u0026#34;, \u0026#34;x3000c0s2b0n0\u0026#34;, \u0026#34;x3000c0s9b0n0\u0026#34;, \u0026#34;x3000c0s8b0n0\u0026#34;, \u0026#34;x3000c0s5b0\u0026#34;, \u0026#34;x3000c0s4b0\u0026#34;, \u0026#34;x3000c0s7b0\u0026#34;, \u0026#34;x3000c0s6b0\u0026#34;, \u0026#34;x3000c0s3b0\u0026#34;, \u0026#34;x3000c0s2b0\u0026#34;, \u0026#34;x3000c0s9b0\u0026#34;, \u0026#34;x3000c0s8b0\u0026#34;,] To lock single nodes or lists of specific nodes (and their BMCs) Note: The BMC of ncn-m001 typically does not exist in HSM under HSM State Components, and therefore cannot be locked.\nLock the management nodes and BMCs.\nncn# cray hsm locks lock create --role Management --component-ids x3000c0s6b0n0,x3000c0s6b0 --processing-model rigid Example output:\nFailure = [] [Counts] Total = 2 Success = 2 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x3000c0s6b0n0\u0026#34;, \u0026#34;x3000c0s6b0\u0026#34;,] How To Check For Locked Management Nodes NOTE The BMC of ncn-m001 typically does not exist in HSM under HSM State Components, and therefore would not show up in the following command output.\nCheck the lock status of the management nodes and BMCs.\ncray hsm state components list --type Node --role Management --format json | \\ jq -c \u0026#39;.Components[]|.ID\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$/\\n/\u0026#39; | \\ xargs cray hsm locks status create --format toml --component-ids Example output:\n[[Components]] ID = \u0026#34;x3000c0s1b0n0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s5b0n0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s4b0n0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s7b0n0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s6b0n0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s3b0n0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s3b0n0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s9b0n0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s8b0n0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s5b0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s4b0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s7b0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s6b0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s3b0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s3b0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s9b0\u0026#34; Locked = true Reserved = false ReservationDisabled = false [[Components]] ID = \u0026#34;x3000c0s8b0\u0026#34; Locked = true Reserved = false ReservationDisabled = false How To Unlock Management Nodes Use the cray hsm locks unlock command to perform unlocking.\nNOTE: When unlocking NCNs, you must unlock their node BMCs as well.\nNOTE: The following steps assume both the management nodes and their BMCs are marked with the Management role in HSM. If they are not, see Set BMC Management Role.\nTo unlock all nodes (and their BMCs) with the Management role Unlock the management nodes and BMCs.\nncn# cray hsm locks unlock create --role Management --processing-model rigid Example output:\nFailure = [] [Counts] Total = 16 Success = 16 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;x3000c0s6b0n0\u0026#34;, \u0026#34;x3000c0s3b0n0\u0026#34;, \u0026#34;x3000c0s2b0n0\u0026#34;, \u0026#34;x3000c0s9b0n0\u0026#34;, \u0026#34;x3000c0s8b0n0\u0026#34;, \u0026#34;x3000c0s5b0n0\u0026#34;, \u0026#34;x3000c0s4b0n0\u0026#34;, \u0026#34;x3000c0s5b0\u0026#34;, \u0026#34;x3000c0s4b0\u0026#34;, \u0026#34;x3000c0s7b0\u0026#34;, \u0026#34;x3000c0s6b0\u0026#34;, \u0026#34;x3000c0s3b0\u0026#34;, \u0026#34;x3000c0s2b0\u0026#34;, \u0026#34;x3000c0s9b0\u0026#34;, \u0026#34;x3000c0s8b0\u0026#34;,] To unlock single or lists of specific nodes (and their BMCs) Unlock the management nodes.\nncn# cray hsm locks unlock create --role Management --component-ids x3000c0s6b0n0,x3000c0s6b0 --processing-model rigid Example output:\nFailure = [] [Counts] Total = 2 Success = 2 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x3000c0s6b0n0\u0026#34;, \u0026#34;x3000c0s6b0\u0026#34;,] "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/configuration_sessions/",
	"title": "Configuration Sessions",
	"tags": [],
	"description": "",
	"content": "Configuration Sessions Once configurations have been created with the required layers and values set in the configuration repositories (or the additional inventory repository), create a Configuration Framework Session (CFS) session to apply the configuration to the targets.\nSessions are created via the Cray CLI or through the CFS REST API. A session stages Ansible inventory (whether dynamic, static, or image customization), launches Ansible Execution Environments (AEE) in order for each configuration layer in the service mesh, tears down the environments as required, and reports the session status to the CFS API.\nWhen a session target is an Image Management Service (IMS) image ID for the purposes of pre-boot image customization, the CFS session workflow varies slightly. The inventory staging instead calls IMS to expose the requested image root(s) via SSH. After the AEE(s) finish applying the configuration layers, CFS then instructs IMS to tear down the image root environment and package up the resultant image and records the new image ID in the session metadata.\nSession Naming Conventions CFS follows the same naming conventions for session names as Kubernetes does for jobs. Session names must follow the Kubernetes naming conventions and are limited to 45 characters.\nRefer to the external Kubernetes naming conventions documentation for more information.\nConfiguration Session Filters CFS provides several filters for use when listing sessions or using the bulk delete option. These following filters are available:\n--status: Session status options include pending, running, and complete.\n--succeeded: If the session has not yet completed, this will be set to none. Otherwise, this will be set to true, false, or unknown in the event that CFS was unable to find the Kubernetes job associated with the session.\n--min-age/--max-age: Returns only the sessions that fall within the given age. For example, --max-age could be used to list only the recent sessions, or --min-age could be used to find old sessions for cleanup. Age is given in the format \u0026ldquo;1d\u0026rdquo; for days, or \u0026ldquo;6h\u0026rdquo; for hours.\n--tags: Sessions can be created with searchable tags. By default, this includes the bos_session tag when CFS is triggered by BOS. This can be searched using the following command:\nncn-m001# cray cfs sessions list --tags bos_session=BOS_SESSION_NAME Configuration Session Workflow CFS progresses through a session by running a series of commands in containers located in a Kubernetes job pod. Four container types are present in the job pod which pertain to CFS session setup, execution, and teardown:\ngit-clone-*\nThese init containers are responsible for cloning the configuration repository and checking out the branch/commit specified in each configuration layer.\nThese containers are run in the same order as the layers are specified, and their names are indexed appropriately: git-clone-0, git-clone-1, and so on.\ngit-clone-hosts\nThis init container clones the repository specified in the parameter additionalInventoryUrl, if specified.\ninventory\nThis container is responsible for generating the dynamic inventory or for communicating to IMS to stage boot image roots that need to be made available via SSH when the session --target-definition is image.\nansible-*\nThese containers run the AEE after CFS injects the inventory and Git repository content from previous containers. One container is executed for each configuration layer specified.\nThese containers are run in the same order as the layers are specified, and their names are indexed appropriately: ansible-0, ansible-1, and so on.\nteardown\nThis container waits for the last ansible-* to complete and subsequently calls IMS to package up customized image roots. The container only exists when the session --target-definition is image.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_sequence/",
	"title": "Compute Node Boot Sequence",
	"tags": [],
	"description": "",
	"content": "Compute Node Boot Sequence Provides an overview of the compute node boot process and touches upon the fact that issues can be encountered during this process.\nThe following is a high-level overview of the boot sequence for compute nodes:\nThe compute node is powered on.\nThe BIOS issues a DHCP discover request.\nDHCP responds with the following:\nnext-server, which is the IP address of the TFTP server. The name of the file to download from the TFTP server. The node\u0026rsquo;s PXE sends a TFTP request to the TFTP server.\nIf the TFTP server has the requested file, it sends it to the node\u0026rsquo;s PXE. In this case, the file name is ipxe.efi, which is a Cray-crafted iPXE binary that points at the Boot Script Service (BSS). The BSS will then serve up another iPXE boot script.\nThe ipxe.efi file downloads another iPXE boot script from the BSS.\nThis script provides information for downloading:\nThe location of the kernel. The location of the initrd. A string containing the kernel parameters. The node attempts to download the kernel and initrd boot artifacts. If successful, it will boot using these and the kernel parameters. Otherwise, it will retry to download these boot artifacts indefinitely.\nThere may be times when certain issues may be encountered during the compute node boot up process. In order to resolve these issues, it is important to understand the underlying cause, symptoms, and stage at which the issue has occurred. The exact process and tools required to resolve the issue depend on this information.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/create_a_uai_resource_specification/",
	"title": "Create a UAI Resource Specification",
	"tags": [],
	"description": "",
	"content": "Create a UAI Resource Specification Add a resource specification to UAS. Once added, a resource specification can be used to request or limit specific resource consumption on a host node or gain access to host node features managed by Kubernetes resources. The examples in this documentation focus on memory and CPU usage, but Kubernetes does use resources in some configurations to manage access to other kinds of resources.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) Procedure Add a resource specification.\nUse a command of the following form:\nncn# cray uas admin config resources create [--limit \u0026lt;k8s-resource-limit\u0026gt;] [--request \u0026lt;k8s-resource-request\u0026gt;] [--comment \u0026#39;\u0026lt;string\u0026gt;\u0026#39;] For example:\nncn# cray uas admin config resources create --request \u0026#39;{\u0026#34;cpu\u0026#34;: \u0026#34;300m\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;1Gi\u0026#34;}\u0026#39; \\ --limit \u0026#39;{\u0026#34;cpu\u0026#34;: \u0026#34;300m\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;1Gi\u0026#34;}\u0026#39; \\ --comment \u0026#34;Resource Specification to use with Brokered End-User UAIs\u0026#34; See Elements of a UAI for an explanation of UAI resource specifications.\nThe example above specifies a request / limit pair that requests and is constrained to 300 milli-CPUs (0.3 CPUs) and 1 GiB of memory (1 * 1024 * 1024 * 1024 bytes) for any UAI created with this limit specification. By keeping the request and the limit the same, this ensures that a host node will not be oversubscribed by UAIs. It is also legitimate to request less than the limit, though that risks over-subscription and is not recommended in most cases. If the request is greater than the limit, UAIs created with the request specification will never be scheduled because their pods will not be able to provide the requested resources.\nAll of the configurable parts are optional when adding a resource specification. If none are provided, an empty resource specification with only a resource_id will be created.\nTop: User Access Service (UAS)\nNext Topic: Retrieve Resource Specification Details\n"
},
{
	"uri": "/docs-csm/en-12/install/connect_to_switch_over_usb_serial_cable/",
	"title": "Connect to Switch over USB-Serial Cable",
	"tags": [],
	"description": "",
	"content": "Connect to Switch over USB-Serial Cable In the event that network plumbing is lacking, down, or unconfigured for procuring devices, then it is recommended to use the Serial/COM ports on the management switches.\nThis guide will instruct the user on procuring MAC addresses for the NCNs metadata files with the serial console.\nMileage may vary, as some obstacles such as BAUDRATE and terminal usage vary per manufacturer.\nCommon Manufacturers Refer to the external support/documentation portals for more information:\nAruba Dell Mellanox Setup / Connection Use minicom, screen, or cu to connect to the switch\u0026rsquo;s console.\nPrerequisites A USB-DB-9 or USB-RJ-45 cable is connected between the switch and the NCN.\nscreen screen /dev/ttyUSB1 screen /dev/ttyUSB1 115200 minicom minicom -b 9600 -D /dev/ttyUSB1 minicom -b 115200 -D /dev/ttyUSB1 cu cu -l /dev/ttyUSB1 -s 115200 Troubleshoot Connections Tip : Mellanox On Mellanox switches, if the console is not responding when opened, try holding CTRL + R (or control + R for macOS) to initiate a screen refresh. This should take 5-10 seconds.\nTip : No USB TTY Device If there is no device in /dev/tty*, follow dmesg -w and try reseating the USB cable (unplug the end in the NCN, and plug it back in).\nObserve the dmesg -w output. Does it show errors pertaining to USB? The cable may be bad, or a reboot may be required.\nAdditional External References USB-B to RJ-45 rs232 Cable USB-B to USB-C adapter "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/initrd.img.zx_not_found/",
	"title": "Known Issue initrd.img.xz Not Found",
	"tags": [],
	"description": "",
	"content": "Known Issue: initrd.img.xz Not Found This is a problem that is fixed in CSM 1.0 and later, but if your system was upgraded from CSM 0.9 you may run into this. Below is the full error seen when attempting to boot:\nLoading Linux ... Loading initial ramdisk ... error: file `/boot/grub2/../initrd.img.xz\u0026#39; not found. Press any key to continue... [ 2.528752] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) [ 2.537264] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 5.3.18-24.64-default #1 SLE15-SP2 [ 2.545499] Hardware name: Cray Inc. R272-Z30-00/MZ32-AR0-00, BIOS C27 05/12/2021 [ 2.553196] Call Trace: [ 2.555716] dump_stack+0x66/0x8b [ 2.559127] panic+0xfe/0x2d7 [ 2.562184] mount_block_root+0x27d/0x2e1 [ 2.566306] ? set_debug_rodata+0x11/0x11 [ 2.570431] prepare_namespace+0x130/0x166 [ 2.574645] kernel_init_freeable+0x23f/0x26b [ 2.579125] ? rest_init+0xb0/0xb0 [ 2.582623] kernel_init+0xa/0x110 [ 2.586127] ret_from_fork+0x22/0x40 [ 2.590747] Kernel Offset: 0x0 from 0xffffffff81000000 (relocation range: 0xffffffff80000000-0xffffffffbfffffff) [ 2.690969] ---[ end Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ]--- Fix Follow these steps on any NCN to fix the issue:\nInstall the Shasta 1.4 (a.k.a. CSM 0.9) csm-install-workarounds RPM. See the CSM 0.9 documentation for details on how to do this.\nRun the CASMINST-2689.sh script from the CASMINST-2689 workaround at the livecd-post-reboot breakpoint:\nncn# /opt/cray/csm/workarounds/livecd-post-reboot/CASMINST-2689/CASMINST-2689.sh Run these commands:\nncn# for i in $(grep -oP \u0026#39;ncn-\\w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39;); do scp -r /opt/cray/csm/workarounds/livecd-post-reboot/CASMINST-2689 \\ $i:/opt/cray/csm/workarounds/livecd-post-reboot/ done ncn# pdsh -b -S -w $(grep -oP \u0026#39;ncn-\\w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) \\ \u0026#39;/opt/cray/csm/workarounds/livecd-post-reboot/CASMINST-2689/CASMINST-2689.sh\u0026#39; Remove the Shasta 1.4 install workaround RPM from the NCN.\nncn# rpm -e csm-install-workarounds Validate Running the script again will produce this output:\nExamining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. Examining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. Examining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. Examining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. Examining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. Examining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. Examining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. Examining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/dump_ceph_crash_data/",
	"title": "Dump Ceph Crash Data",
	"tags": [],
	"description": "",
	"content": "Dump Ceph Crash Data Ceph includes an option to dump crash data. Retrieve this data to get more information on a Ceph cluster that has crashed.\nPrerequisites Ceph is reporting the cluster [WRN] overall HEALTH_WARN 1 daemons have recently crashed error in the output of the ceph -s or ceph health detail commands.\nProcedure Get the Ceph crash listing and the corresponding IDs.\nncn-m001# ceph crash ls Example output:\nID ENTITY NEW 2021-02-02_13:45:18.543633Z_a31173f7-44c8-45b1-a253-80efa25b45f1 mon.ncn-s003 * Get information about the crash to include in a support ticket.\nReplace the CRASH_ID value with the ID returned in the previous step.\nncn-m001# ceph crash info CRASH_ID Example output:\n{ \u0026#34;crash_id\u0026#34;: \u0026#34;2021-02-02_13:45:18.543633Z_a31173f7-44c8-45b1-a253-80efa25b45f1\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2021-02-02 13:45:18.543633Z\u0026#34;, \u0026#34;process_name\u0026#34;: \u0026#34;ceph-mon\u0026#34;, \u0026#34;entity_name\u0026#34;: \u0026#34;mon.ncn-s003\u0026#34;, \u0026#34;ceph_version\u0026#34;: \u0026#34;14.2.11-394-g9cbbc473c0\u0026#34;, \u0026#34;utsname_hostname\u0026#34;: \u0026#34;ncn-s003\u0026#34;, \u0026#34;utsname_sysname\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;utsname_release\u0026#34;: \u0026#34;5.3.18-24.46-default\u0026#34;, \u0026#34;utsname_version\u0026#34;: \u0026#34;#1 SMP Tue Jan 5 16:11:50 UTC 2021 (4ff469b)\u0026#34;, \u0026#34;utsname_machine\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;os_name\u0026#34;: \u0026#34;SLE_HPC\u0026#34;, \u0026#34;os_id\u0026#34;: \u0026#34;15.2\u0026#34;, \u0026#34;os_version_id\u0026#34;: \u0026#34;15.2\u0026#34;, \u0026#34;os_version\u0026#34;: \u0026#34;15-SP2\u0026#34;, \u0026#34;assert_condition\u0026#34;: \u0026#34;session_map.sessions.empty()\u0026#34;, \u0026#34;assert_func\u0026#34;: \u0026#34;virtual Monitor::~Monitor()\u0026#34;, \u0026#34;assert_file\u0026#34;: \u0026#34;/home/abuild/rpmbuild/BUILD/ceph-14.2.11-394-g9cbbc473c0/src/mon/Monitor.cc\u0026#34;, \u0026#34;assert_line\u0026#34;: 267, \u0026#34;assert_thread_name\u0026#34;: \u0026#34;ceph-mon\u0026#34;, \u0026#34;assert_msg\u0026#34;: \u0026#34;/home/abuild/rpmbuild/BUILD/ceph-14.2.11-394-g9cbbc473c0/src/mon/Monitor.cc: In function \u0026#39;virtual Monitor::~Monitor()\u0026#39; thread 7f6f68869480 time 2021-02-02 13:45:18.538782\\n/home/abuild/rpmbuild/BUILD/ceph-14.2.11-394-g9cbbc473c0/src/mon/Monitor.cc: 267: FAILED ceph_assert(session_map.sessions.empty())\\n\u0026#34;, \u0026#34;backtrace\u0026#34;: [ \u0026#34;(()+0x132d0) [0x7f6f5e9322d0]\u0026#34;, \u0026#34;(gsignal()+0x110) [0x7f6f5da6e520]\u0026#34;, \u0026#34;(abort()+0x151) [0x7f6f5da6fb01]\u0026#34;, \u0026#34;(ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x1a3) [0x7f6f5fac99f7]\u0026#34;, \u0026#34;(ceph::__ceph_assertf_fail(char const*, char const*, int, char const*, char const*, ...)+0) [0x7f6f5fac9b81]\u0026#34;, \u0026#34;(Monitor::~Monitor()+0x962) [0x5602667940f2]\u0026#34;, \u0026#34;(Monitor::~Monitor()+0x9) [0x560266794169]\u0026#34;, \u0026#34;(main()+0x289b) [0x5602667231cb]\u0026#34;, \u0026#34;(__libc_start_main()+0xea) [0x7f6f5da5934a]\u0026#34;, \u0026#34;(_start()+0x2a) [0x5602667526da]\u0026#34; ] } Archive the crash data for further triage.\nThis command can be used to archive all crash data, or just the data for a specific Ceph entity.\nncn-m001# ceph crash archive ALL/CRASH_ID "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/change_ncn_image_root_password_and_ssh_keys/",
	"title": "Set NCN Image Root Password, SSH Keys, and Timezone",
	"tags": [],
	"description": "",
	"content": "Set NCN Image Root Password, SSH Keys, and Timezone Modify the NCN images by setting the root user password and adding SSH keys for the root user account. If desired, also change the timezone for the NCNs.\nThis procedure shows this process being done any time after the first time installation of the CSM software has been completed and the PIT node is booted as a regular master node. To change the NCN images from the PIT node during CSM installation, see Set NCN Image Root Password, SSH Keys, and Timezone on PIT Node.\nAll of the commands in this procedure are intended to be run on a single master or worker node.\nPrerequisites This procedure can only be done after the PIT node is rebuilt to become a normal master node. The Cray CLI must be configured on the node where the procedure is being done. See Configure the Cray Command Line Interface. The CSM documentation RPM must be installed on the node where the procedure is being run. See Check for Latest Documentation. Procedure Preparation\nGet NCN artifacts\nCustomize the images\nSSH keys Script-generated keys Administrator-provided keys Password Use node password Enter password and generate hash Timezone Examples Example 1: New keys, copy password, keep UTC Example 2: Provide keys, prompt for password, change timezone Example 3: New keys, no password change, keep UTC, no prompting Upload artifacts into S3\nUpdate BSS\nCleanup\nRebuild NCNs\n1. Preparation Change to a working directory with enough space to hold the images once they have been expanded.\nncn-mw# mkdir -pv /run/initramfs/overlayfs/workingarea \u0026amp;\u0026amp; cd /run/initramfs/overlayfs/workingarea 2. Get NCN artifacts List available Kubernetes NCN images.\nThe Kubernetes image is used by the master and worker nodes.\nncn-mw# cray artifacts list ncn-images --format json | jq \u0026#39;.artifacts[] .Key\u0026#39; | grep k8s | grep squashfs Example output:\n\u0026#34;k8s-filesystem.squashfs\u0026#34; \u0026#34;k8s/0.1.107/filesystem.squashfs\u0026#34; \u0026#34;k8s/0.1.109/filesystem.squashfs\u0026#34; \u0026#34;k8s/0.1.48/filesystem.squashfs\u0026#34; Set Kubernetes image version variables.\nSet K8SVERSION to the version of the image to be modified. Set K8SNEW to the version label to use for the modified image. This example uses k8s/0.1.109 for the current version and adds a suffix for the new version.\nncn-mw# K8SVERSION=0.1.109 ncn-mw# K8SNEW=${K8SVERSION}-2 Make a temporary directory for the Kubernetes artifacts using the current version string.\nncn-mw# mkdir -pv k8s/${K8SVERSION} Download the Kubernetes NCN artifacts.\nncn-mw# for art in filesystem.squashfs initrd kernel ; do cray artifacts get ncn-images k8s/${K8SVERSION}/${art} k8s/${K8SVERSION}/${art} done List available Ceph images.\nThe Ceph image is used by the utility storage nodes.\nncn-mw# cray artifacts list ncn-images --format json | jq \u0026#39;.artifacts[] .Key\u0026#39; | grep ceph | grep squashfs Example output:\n\u0026#34;ceph-filesystem.squashfs\u0026#34; \u0026#34;ceph/0.1.107/filesystem.squashfs\u0026#34; \u0026#34;ceph/0.1.113/filesystem.squashfs\u0026#34; \u0026#34;ceph/0.1.48/filesystem.squashfs\u0026#34; Set Ceph image version variables.\nSet CEPHVERSION to the version of the image to be modified. Set CEPHNEW to the version label to use for the modified image. This example uses ceph/0.1.113 for the current version and adds a suffix for the new version.\nncn-mw# CEPHVERSION=0.1.113 ncn-mw# CEPHNEW=${CEPHVERSION}-2 Make a temporary directory for the Ceph artifacts using the current version string.\nncn-mw# mkdir -pv ceph/${CEPHVERSION} Download the storage NCN artifacts.\nncn-mw# for art in filesystem.squashfs initrd kernel ; do cray artifacts get ncn-images ceph/${CEPHVERSION}/${art} ceph/${CEPHVERSION}/${art} done 3. Customize the images Add SSH keys and the root password to the NCN SquashFS images. Optionally set their timezone, if a timezone other than UTC (the default) is desired. This is all done by running the ncn-image-modification.sh script. Set the path to the script:\nncn-mw# NCN_MOD_SCRIPT=$(rpm -ql docs-csm | grep ncn-image-modification[.]sh) This document provides common ways of using the script to accomplish this. However, specific environments may require deviations from these examples. In those cases, it may be helpful to view the complete script usage statement by running it with only the -h argument.\nThe Kubernetes NCN image location is specified with the -k argument to the script, and the storage NCN image location is specified with the -s argument to the script. Both images should be customized with a single call to the script to ensure that they receive matching customizations, unless specifically desiring otherwise.\nThe new customized images are created in their original image\u0026rsquo;s directory. They have the same name as the original image, except with the secure- prefix added. The original image is moved into a subdirectory named old, for backup purposes.\nThere are several choices to be made during this process:\nSSH key files can be provided to the script, or the script can generate them itself. The hashed root password can be provided to the script, or the script can prompt for password entry when it is running. To use a non-default timezone, that must be passed into the script. SSH keys Script-generated keys To have the script generate the SSH keys automatically, it must be provided with the ssh-keygen options to use.\nTo view the complete list of supported ssh-keygen options, view the script usage statement by running it with the -h argument. If the -N option is not used to specify the passphrase, then the script will prompt for the passphrase when it generates the keys. Even specifying an empty passphrase will prevent being prompted to enter the passphrase during script execution. See Example 3. Administrator-provided keys To provide SSH keys to the script, specify the directory containing them with the -d argument.\nThe script assumes that public keys in that directory have the .pub file extension. The entire contents of this directory will be copied into the /root/.ssh directory in the images. After copying the directory contents, the script updates the /root/.ssh/authorized_keys file in the images with the new public keys. This is usually the desired behavior, but it can be overridden by specifying the -a argument. In that case, the script will not update the authorized_keys file after copying the directory contents. Password In order for the script to set root passwords in the images, the -p argument must be included when calling it.\nIf the SQUASHFS_ROOT_PW_HASH environment variable is exported, the script will use that as the new root password hash for the images. Otherwise, the script will prompt for the password to be entered during its execution.\nUse node password If wanting to use the same root user password that is being used on the node where this procedure is being run, then the following command can be used to set the SQUASHFS_ROOT_PW_HASH variable.\nncn-mw# export SQUASHFS_ROOT_PW_HASH=$(awk -F\u0026#39;:\u0026#39; /^root:/\u0026#39;{print $2}\u0026#39; \u0026lt; /etc/shadow) Enter password and generate hash The following script can be used to manually enter a new password, and then generate its hash.\nThis script uses read -s to prevent the password from being echoed to the screen or saved in the shell history. It unsets the plaintext password variables at the end, so that only the hash is preserved.\nncn-mw# read -r -s -p \u0026#34;Enter root password for NCN images: \u0026#34; PW1 ; echo ; if [[ -z ${PW1} ]]; then echo \u0026#34;ERROR: Password cannot be blank\u0026#34; else read -r -s -p \u0026#34;Enter again: \u0026#34; PW2 echo if [[ ${PW1} != ${PW2} ]]; then echo \u0026#34;ERROR: Passwords do not match\u0026#34; else export SQUASHFS_ROOT_PW_HASH=$(echo -n \u0026#34;${PW1}\u0026#34; | openssl passwd -6 -salt $(\u0026lt; /dev/urandom tr -dc ./A-Za-z0-9 | head -c4) --stdin) [[ -n ${SQUASHFS_ROOT_PW_HASH} ]] \u0026amp;\u0026amp; echo \u0026#34;Password hash set and exported\u0026#34; || echo \u0026#34;ERROR: Problem generating hash\u0026#34; fi fi ; unset PW1 PW2 Timezone The default timezone in the NCN images is UTC. This can optionally be changed by passing the -z argument to the script. Valid timezone options can be listed by running timedatectl list-timezones.\nExamples Example 1: New keys, copy password, keep UTC This example has the script generate new SSH keys (prompting the administrator for the SSH key passphrase) and copies the root user password from the current node. It does not change the timezone from the UTC default.\nncn-mw# export SQUASHFS_ROOT_PW_HASH=$(awk -F\u0026#39;:\u0026#39; /^root:/\u0026#39;{print $2}\u0026#39; \u0026lt; /etc/shadow) ncn-mw# $NCN_MOD_SCRIPT -p \\ -t rsa \\ -k k8s/${K8SVERSION}/filesystem.squashfs \\ -s ceph/${CEPHVERSION}/filesystem.squashfs Example 2: Provide keys, prompt for password, change timezone This example uses existing SSH keys located in the /my/pre-existing/keys directory. The script prompts the administrator for the root user password during execution. It changes the timezone to America/Chicago.\nncn-mw# $NCN_MOD_SCRIPT -p \\ -d /my/pre-existing/keys \\ -z America/Chicago \\ -k k8s/${K8SVERSION}/filesystem.squashfs \\ -s ceph/${CEPHVERSION}/filesystem.squashfs Example 3: New keys, no password change, keep UTC, no prompting This example has the script generate new SSH keys. It does not change the root password, nor does it change the timezone from the UTC default. A blank passphrase is provided, so that the script requires no input from the administrator while it is running.\nncn-mw# $NCN_MOD_SCRIPT -t rsa \\ -N \u0026#34;\u0026#34; \\ -k k8s/${K8SVERSION}/filesystem.squashfs \\ -s ceph/${CEPHVERSION}/filesystem.squashfs 4. Upload artifacts into S3 Upload the new Kubernetes image into S3.\nncn-mw# /usr/share/doc/csm/scripts/ceph-upload-file-public-read.py \\ --bucket-name ncn-images \\ --key-name \u0026#34;k8s/${K8SNEW}/filesystem.squashfs\u0026#34; \\ --file-name k8s/${K8SVERSION}/secure-filesystem.squashfs Upload the Kubernetes kernel and initrd into S3 under the new version string.\nncn-mw# for art in initrd kernel ; do /usr/share/doc/csm/scripts/ceph-upload-file-public-read.py \\ --bucket-name ncn-images \\ --key-name \u0026#34;k8s/${K8SNEW}/${art}\u0026#34; \\ --file-name k8s/${K8SVERSION}/${art} done Upload the new Ceph image into S3.\nncn-mw# /usr/share/doc/csm/scripts/ceph-upload-file-public-read.py \\ --bucket-name ncn-images \\ --key-name \u0026#34;ceph/${CEPHNEW}/filesystem.squashfs\u0026#34; \\ --file-name ceph/${CEPHVERSION}/secure-filesystem.squashfs Upload the Ceph kernel and initrd into S3 under the new version string.\nncn-mw# for art in initrd kernel ; do /usr/share/doc/csm/scripts/ceph-upload-file-public-read.py \\ --bucket-name ncn-images \\ --key-name \u0026#34;ceph/${CEPHNEW}/${art}\u0026#34; \\ --file-name ceph/${CEPHVERSION}/${art} done The Kubernetes and storage images now have the image changes.\n5. Update BSS WARNING: If doing a CSM software upgrade, then skip this section and proceed to Cleanup.\nThis step updates the entries in BSS for the NCNs to use the new images.\nUpdate BSS for master and worker nodes.\nThis uses the K8SVERSION and K8SNEW variables defined earlier.\nncn-mw# for node in $(grep -oP \u0026#34;(ncn-[mw]\\w+)\u0026#34; /etc/hosts | sort -u); do echo $node xname=$(ssh $node cat /etc/cray/xname) echo $xname cray bss bootparameters list --name $xname --format json \u0026gt; bss_$xname.json sed -i.$(date +%Y%m%d_%H%M%S%N).orig \u0026#34;s@/k8s/${K8SVERSION}\\([\\\u0026#34;/[:space:]]\\)@/k8s/${K8SNEW}\\1@g\u0026#34; bss_$xname.json kernel=$(cat bss_$xname.json | jq \u0026#39;.[] .kernel\u0026#39;) initrd=$(cat bss_$xname.json | jq \u0026#39;.[] .initrd\u0026#39;) params=$(cat bss_$xname.json | jq \u0026#39;.[] .params\u0026#39;) cray bss bootparameters update --initrd $initrd --kernel $kernel --params \u0026#34;$params\u0026#34; --hosts $xname --format json done Update BSS for utility storage nodes.\nThis uses the CEPHVERSION and CEPHNEW variables defined earlier.\nncn-mw# for node in $(grep -oP \u0026#34;(ncn-s\\w+)\u0026#34; /etc/hosts | sort -u); do echo $node xname=$(ssh $node cat /etc/cray/xname) echo $xname cray bss bootparameters list --name $xname --format json \u0026gt; bss_$xname.json sed -i.$(date +%Y%m%d_%H%M%S%N).orig \u0026#34;s@/ceph/${CEPHVERSION}\\([\\\u0026#34;/[:space:]]\\)@/ceph/${CEPHNEW}\\1@g\u0026#34; bss_$xname.json kernel=$(cat bss_$xname.json | jq \u0026#39;.[] .kernel\u0026#39;) initrd=$(cat bss_$xname.json | jq \u0026#39;.[] .initrd\u0026#39;) params=$(cat bss_$xname.json | jq \u0026#39;.[] .params\u0026#39;) cray bss bootparameters update --initrd $initrd --kernel $kernel --params \u0026#34;$params\u0026#34; --hosts $xname --format json done 6. Cleanup Remove the temporary working area in order to reclaim the space.\nncn-mw# rm -rvf /run/initramfs/overlayfs/workingarea 7. Rebuild NCNs WARNING: If doing a CSM software upgrade, then skip this step because the upgrade process does a rolling rebuild with some additional steps.\nDo a rolling rebuild of all NCNs. See Rebuild NCNs.\n"
},
{
	"uri": "/docs-csm/en-12/operations/power_management/set_the_turbo_boost_limit/",
	"title": "Set the Turbo Boost Limit",
	"tags": [],
	"description": "",
	"content": "Set the Turbo Boost Limit Turbo boost limiting is supported on the Intel® and AMD® processors. Because processors have a high degree of variability in the amount of turbo boost each processor can supply, limiting the amount of turbo boost can reduce performance variability and reduce power consumption.\nTurbo boost can be limited by setting the turbo_boost_limit kernel parameter to one of these values:\n0 - Disable turbo boost 999 - (default) No limit is applied. The following values are not supported in COS v1.4:\n100 - Limits turbo boost to 100 MHz 200 - Limits turbo boost to 200 MHz 300 - Limits turbo boost to 300 MHz 400 - Limits turbo boost to 400 MHz The limit applies only when a high number of cores are active. On an N-core processor, the limit is in effect when the active core count is N, N-1, N-2, or N-3. For example, on a 12-core processor, the limit is in effect when 12, 11, 10, or 9 cores are active.\nSet or change the turbo boost limit parameter Modify the Boot Orchestration Service (BOS) template for the nodes. This example below disables turbo boost. The default setting is 999 (no limit).\n{ \u0026#34;boot_sets\u0026#34;: { \u0026#34;boot_set61\u0026#34;: { \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=tty0 console=ttyS0,115200n8 root=crayfs imagename=/SLES15 selinux=0 rd.shell rd.net.timeout.carrier=40 rd.retry=40 ip=dhcp rd.neednet=1 crashkernel=256M turbo_boost_limit=0\u0026#34;, \u0026#34;node_groups\u0026#34;: [ \u0026#34;group1\u0026#34;, \u0026#34;group2\u0026#34; ], \u0026#34;node_list\u0026#34;: [ \u0026#34;x0c0s28b0n0\u0026#34;, \u0026#34;node2\u0026#34;, \u0026#34;node3\u0026#34; ], \u0026#34;node_roles_groups\u0026#34;: [ \u0026#34;compute\u0026#34; ], \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:nmn0\u0026#34; }, }, \u0026#34;cfs\u0026#34;: { \u0026#34;configuration\u0026#34;: \u0026#34;my-cfs-configuration\u0026#34; }, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;st6\u0026#34; } "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/s3fs_usage_and_guidelines/",
	"title": "S3FS Usage and Guidelines for Shasta",
	"tags": [],
	"description": "",
	"content": "S3FS Usage and Guidelines for Shasta Introduction S3FS is being deployed as tool to provide temporary relief of space usage as well as supporting SDU/NMD services as a near-posix file system to provide a landing point for dumps.\nWhen to Use If the need is a landing point for large files that may fill up the root volume. Short term storage of large files or rpms. When NOT to Use For long term storage of code, test images, test rpms, or tar files. This is ONLY meant to provide temporary relief. Exercising a vigilant practice of cleaning up unused files should be enforced. As a landing point to uncompress tar files. This will put unnecessary load on the storage cluster as uncompressing a tar file will require a lot of reads and writes back to the object storage endpoints. Running programs from the S3FS mount point. Although this can be done but will eat into memory for long running programs and may not perform properly. Additional Considerations Ensure it is only temporary use on master nodes. SDU utilizes S3FS on the master servers and ideally we would like to reserve the S3FS cache partition for SDU. The cache partition is shared if utilizing automatically mounted partitions. Make sure you are utilizing the correct S3 credentials and buckets. How To Use Gather creds from radosgw\nNOTES:\nPlease replace \u0026lt;radosgw-user\u0026gt; below with the UID for the radosgw/s3 user id. Make sure to use a meaningful filename for storing the credentials and replace \u0026lt;filename\u0026gt; below. Make sure to create a mount location and use that below to replace \u0026lt;mount path\u0026gt; radosgw-admin user info --uid \u0026lt;radosgw-user\u0026gt;|jq -r \u0026#39;.keys[]|.access_key +\u0026#34;:\u0026#34;+ .secret_key\u0026#39; \u0026gt;\u0026gt;${HOME}/.\u0026lt;filename\u0026gt;.s3fs chmod 600 ~/.\u0026lt;filename\u0026gt;.s3fs mkdir \u0026lt;mount path\u0026gt; Mounting the volume\nMount w/o cache\n# s3fs \u0026lt;radosgw-user\u0026gt; \u0026lt;mount path\u0026gt; -o passwd_file=${HOME}/.\u0026lt;filename\u0026gt;.s3fs,url=http://rgw-vip.nmn,use_path_request_style Mount w/ cache\nIMPORTANT: To use this option there must be a dedicated landing space that is a partition. This ensures that the usage does not impact the root drive.\ns3fs \u0026lt;radosgw-user\u0026gt; \u0026lt;mount path\u0026gt; -o passwd_file=${HOME}/.\u0026lt;filename\u0026gt;.s3fs,url=http://rgw-vip.nmn,use_path_request_style,use_cache=\u0026lt;dedicated_cache_partition_location\u0026gt;,check_cache_dir_exist=true Unmounting the volume\numount \u0026lt;mount path\u0026gt; "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_remove_replace_ncns/validate_ncn/",
	"title": "Validate Added NCN",
	"tags": [],
	"description": "",
	"content": "Validate Added NCN Only follow the steps in the section for the node type that was added:\nMaster Node Worker Node Storage Node Validate: Master Node Validate that the master node added successfully.\nVerify that the new node is in the cluster.\nRun the following command from any master or worker node that is already in the cluster. It is helpful to run this command several times to watch for the newly rebuilt node to join the cluster. This should occur within 10 to 20 minutes.\nncn-mw# kubectl get nodes Example output:\nNAME STATUS ROLES AGE VERSION ncn-m001 Ready master 113m v1.19.9 ncn-m002 Ready master 113m v1.19.9 ncn-m003 Ready master 112m v1.19.9 ncn-w001 Ready \u0026lt;none\u0026gt; 112m v1.19.9 ncn-w002 Ready \u0026lt;none\u0026gt; 112m v1.19.9 ncn-w003 Ready \u0026lt;none\u0026gt; 112m v1.19.9 Confirm the ETCDLVM disk has the correct lvm.\nRun the following command on the added node.\nncn-m# lsblk `blkid -L ETCDLVM` Example output:\nsdc 8:32 0 447.1G 0 disk └─ETCDLVM 254:0 0 447.1G 0 crypt └─etcdvg0-ETCDK8S 254:1 0 32G 0 lvm /run/lib-etcd Confirm etcd is running and shows the node as a member once again.\nThe newly built master node should be in the returned list.\nncn-m# etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/ca.crt \\ --key=/etc/kubernetes/pki/etcd/ca.key --endpoints=localhost:2379 member list Validate: Worker Node Validate that the worker node added successfully.\nVerify the new node is in the cluster.\nRun the following command from any master or worker node that is already in the cluster. It is helpful to run this command several times to watch for the newly added node to join the cluster. This should occur within 10 to 20 minutes.\nncn-mw# kubectl get nodes Example output:\nNAME STATUS ROLES AGE VERSION ncn-m001 Ready master 113m v1.19.9 ncn-m002 Ready master 113m v1.19.9 ncn-m003 Ready master 112m v1.19.9 ncn-w001 Ready \u0026lt;none\u0026gt; 112m v1.19.9 ncn-w002 Ready \u0026lt;none\u0026gt; 112m v1.19.9 ncn-w003 Ready \u0026lt;none\u0026gt; 112m v1.19.9 Confirm /var/lib/containerd is on overlay on the node which was added.\nRun the following command on the added node.\nncn-w# df -h /var/lib/containerd Example output:\nFilesystem Size Used Avail Use% Mounted on containerd_overlayfs 378G 245G 133G 65% /var/lib/containerd After several minutes of the node joining the cluster, pods should be in a Running state for the worker node.\nConfirm that the three mount points exist.\nRun the following command on the added node.\nncn-w# lsblk `blkid -L CONRUN` `blkid -L CONLIB` `blkid -L K8SLET` Example output:\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sdc1 8:33 0 69.8G 0 part /run/containerd sdc2 8:34 0 645.5G 0 part /run/lib-containerd sdc3 8:35 0 178.8G 0 part /var/lib/kubelet Confirm the pods are beginning to get scheduled and reach a Running state on the worker node.\nRun this command on any master or worker node. This command assumes that you have set the variables from the prerequisites section.\nncn# kubectl get po -A -o wide | grep $NODE Validate: Storage Node Validate that the storage node added successfully. The following examples are based on a storage cluster that was expanded from three nodes to four.\nVerify the Ceph status looks correct.\nGet the current Ceph status:\nncn-m# ceph -s Example output:\ncluster: id: b13f1282-9b7d-11ec-98d9-b8599f2b2ed2 health: HEALTH_OK services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 4h) mgr: ncn-s001.pdeosn(active, since 4h), standbys: ncn-s002.wjnqvu, ncn-s003.avkrzl mds: cephfs:1 {0=cephfs.ncn-s001.ldlvfj=up:active} 1 up:standby-replay 1 up:standby osd: 18 osds: 18 up (since 4h), 18 in (since 4h) rgw: 4 daemons active (site1.zone1.ncn-s001.ktslgl, site1.zone1.ncn-s002.inynsh, site1.zone1.ncn-s003.dvyhak, site1.zone1.ncn-s004.jnhqvt) task status: data: pools: 12 pools, 713 pgs objects: 37.20k objects, 72 GiB usage: 212 GiB used, 31 TiB / 31 TiB avail pgs: 713 active+clean io: client: 7.0 KiB/s rd, 300 KiB/s wr, 2 op/s rd, 49 op/s wr Verify that the status shows the following:\nThree mons Three mds Three mgr processes One rgw for each storage node (four total in this example) Verify that the added host contains OSDs and that the OSDs are up.\nncn-m# ceph osd tree Example output:\nID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 31.43875 root default -7 6.98639 host ncn-s001 0 ssd 1.74660 osd.0 up 1.00000 1.00000 10 ssd 1.74660 osd.10 up 1.00000 1.00000 11 ssd 1.74660 osd.11 up 1.00000 1.00000 15 ssd 1.74660 osd.15 up 1.00000 1.00000 -3 6.98639 host ncn-s002 3 ssd 1.74660 osd.3 up 1.00000 1.00000 5 ssd 1.74660 osd.5 up 1.00000 1.00000 7 ssd 1.74660 osd.7 up 1.00000 1.00000 12 ssd 1.74660 osd.12 up 1.00000 1.00000 -5 6.98639 host ncn-s003 1 ssd 1.74660 osd.1 up 1.00000 1.00000 4 ssd 1.74660 osd.4 up 1.00000 1.00000 8 ssd 1.74660 osd.8 up 1.00000 1.00000 13 ssd 1.74660 osd.13 up 1.00000 1.00000 -9 10.47958 host ncn-s004 2 ssd 1.74660 osd.2 up 1.00000 1.00000 6 ssd 1.74660 osd.6 up 1.00000 1.00000 9 ssd 1.74660 osd.9 up 1.00000 1.00000 14 ssd 1.74660 osd.14 up 1.00000 1.00000 16 ssd 1.74660 osd.16 up 1.00000 1.00000 17 ssd 1.74660 osd.17 up 1.00000 1.00000 Verify that the radosgw and haproxy are correct.\nRun the following command on the added storage node.\nThere will be output (without an error) if radosgw and haproxy are correct.\nncn-s# curl -k https://rgw-vip.nmn Example output:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt;\u0026lt;ListAllMyBucketsResult xmlns=\u0026#34;http://s3.amazonaws.com/doc/2006-03-01/ \u0026#34;\u0026gt;\u0026lt;Owner\u0026gt;\u0026lt;ID\u0026gt;anonymous\u0026lt;/ID\u0026gt;\u0026lt;DisplayName\u0026gt;\u0026lt;/DisplayName\u0026gt;\u0026lt;/Owner\u0026gt;\u0026lt;Buckets\u0026gt;\u0026lt;/Buckets\u0026gt;\u0026lt;/ListAllMyBucketsResult Next Step Proceed to the next step to Validate Health or return to the main Add, Remove, Replace, or Move NCNs page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/dns-client/",
	"title": "Domain name system (DNS) client",
	"tags": [],
	"description": "",
	"content": "Domain name system (DNS) client The Domain Name System (DNS) translates domain and host names to and from IP addresses. A DNS client resolves hostnames to IP addresses by querying assigned DNS servers for the appropriate IP address.\nRelevant Configuration\nConfigure the switch to resolve queries via a DNS server\nswitch(config)# ip name-server \u0026lt;IPv4/IPv6 address\u0026gt; Configure a domain name\nswitch(config)# ip domain-list mydomain2.com Show Commands to Validate Functionality\nswitch# show hosts Expected Results\nStep 1: You can configure the DNS client Step 2: The output is correct Step 3: You can ping the device Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/management_interface/",
	"title": "Configure Management Interface",
	"tags": [],
	"description": "",
	"content": "Configure Management Interface The management interface can be used to gain remote management access to the switch. The management interface is accessible using the mgmt VRF and is separate from the data plane interfaces, which are in the default VRF.\nAlternatively, a loopback interface can be configured to be used as management interface.\nConfiguration commands Configure the Management interface in CONFIGURATION mode:\ninterface mgmt 1/1/1 Configure an IP address and mask on the Management interface in INTERFACE mode:\nip address A.B.C.D/prefix-length (Optional) Configure DHCP client operations in INTERFACE mode. By default, DHCP client is enabled on the Management interface:\ndhcp Enable the Management interface in INTERFACE mode:\nno shutdown Expected results Administrators can enable/disable the management interface Administrators can assign an IP address to the management interface Administrators can configure a loopback interface to be use for switch management Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/config_management/",
	"title": "Configuration Management",
	"tags": [],
	"description": "",
	"content": "Configuration Management This page is designed for:\nShowing users how initially save switch configurations so they can be used again. Switching between saved configurations. CAUTION All of these steps should be done using an out of band connection. This process is disruptive and will require downtime.\nAll this information can be found in the switch External User Guides.\nPrerequisites It is recommended to do a show run on each switch and save that configuration before attempting the following procedures.\nAruba Change Configuration Maximum number of checkpoints:\nMaximum checkpoints: 64 (including the startup configuration) Maximum user checkpoints: 32 Maximum system checkpoints: 32 Save the current configuration into a checkpoint (needs be done when in both CSM1.0 and CSM1.2) Save the configuration to a checkpoint.\nsw-spine-001(config)# copy running-config checkpoint CSM1_0 Example output:\nNote: checkpoint name with special characters not allowed (only alphanumeric, hyphen, and underscore are allowed) Check on the saved checkpoints.\nsw-spine-001(config)# show checkpoint Example output:\nNAME TYPE WRITER DATE(YYYY/MM/DD) IMAGE VERSION CSM1_0 latest User 2022-01-13T16:51:37Z GL.10.08.1021 CSM1_2 latest User 2022-01-13T16:51:48Z GL.10.08.1021 startup-config startup User 2021-12-20T17:35:58Z GL.10.08.1021 Copying the existing checkpoint point to the startup config to switch between CSM 1.0 and CSM 1.2 configuration.\nCopying the checkpoint to startup:\nsw-spine-001(config)# copy checkpoint CSM1_2 startup-config Boot the system to start with configuration from different CSM version.\nsw-spine-001(config)# boot system The switch will now boot to the desired configuration.\nDell Save the configuration file from running system (going from one CSM release to the other for the first time) This should only need to be done once (unless hardware is added or PoR config has changed). Once this configuration file has been saved, the administrator should be able to switch between the two configuration files.\nThe following example is a 1.0 system that is going to 1.2.\nSave the startup config to new XML config.\nsw-leaf-bmc-001(config)# copy config://startup.xml config://csm1.0.xml Erase the startup config and reboot.\nErase the startup config.\nsw-leaf-bmc-001# delete startup-configuration Example output:\nProceed to delete startup-configuration [confirm yes/no(default)]:yes Reboot after erasing the config.\nsw-leaf-bmc-001# reload Example output:\nSystem configuration has been modified. Save? [yes/no]:no Continuing without saving system configuration Proceed to reboot the system? [confirm yes/no]:yes This will boot the switch to factory defaults.\nPaste in the new CANU generated config once the switch boots into the factory defaults.\nSave the config.\nsw-cdu-001(config)# do write memory sw-cdu-001(config)# copy config://startup.xml config://csm1.2.xml Copy completed will be returned if successful.\nVerify that both configs exist.\nsw-cdu-001(config)# dir config Example output:\nDirectory contents for folder: config Date (modified) Size (bytes) Name --------------------- ------------ ------------------------------------------ 2022-01-12T22:21:35Z 53441 csm1.0.xml 2022-01-12T22:34:03Z 97654 csm1.2.xml 2022-01-12T22:33:47Z 97654 startup.xml Reload the switch to a different CSM version configuration This process should be used when configuration files for the desired CSM version are currently on the switch.\nThe following example shows going from CSM 1.2 to CSM 1.0 switch configuration.\nView the current switch configuration files.\nOS10(config)# dir config Example output:\nDirectory contents for folder: config Date (modified) Size (bytes) Name --------------------- ------------ ------------------------------------------ 2022-01-12T22:21:35Z 53441 csm1.0.xml 2022-01-12T22:34:03Z 97654 csm1.2.xml 2022-01-12T22:40:58Z 53441 startup.xml Copy the desired switch config to the startup config and reload.\n(config)# copy config://csm1.0.xml config://startup.xml (config)# reload System configuration has been modified. Save? [yes/no]:no The switch will then boot to the desired configuration.\nMellanox Save a configuration file from running system (going from one CSM release to the other for the first time) This should only need to be done once (unless hardware is added or PoR config has changed). Once this configuration file has been saved, the administrator should be able to switch between the two configuration files.\nThe following example is a 1.0 system that is going to 1.2.\nWrite the current configuration to a file. This copies the current running config to a binary config file.\n(config) # configuration write to csm1.0 Verify the new configuration file was created.\n(config) # show configuration Example output:\nfiles csm1.0 (active) initial initial.bak Active configuration: csm1.0 Unsaved changes : no Create a new config file for CSM 1.2.\nWhen a new config file is created, no data is written to it. The administrator will boot to this new config file and paste the CANU generated config to it.\n(config) # configuration new csm1.2 Check that the configuration files contain the new csm1.2 blank config that was just created.\n(config) # show configuration Example output:\nfiles csm1.0 (active) csm1.2 initial initial.bak Active configuration: csm1.0 Unsaved changes : no Switch to the new config, which requires a reboot.\n(config) # configuration switch-to csm1.2 This requires a reboot. Type \u0026#39;yes\u0026#39; to confirm: yes Once the switch is rebooted, verify the config file is correct. It should reboot without any configuration.\nswitch-cc30b4 [standalone: master] # show configuration files Example output:\ncsm1.0 csm1.2 (active) initial initial.bak Active configuration: csm1.2 Unsaved changes : yes Paste in the new CANU generated 1.2 config.\nSave the config.\n(config) # write memory Reload a switch to a different CSM version configuration This process should be used when configuration files for the desired CSM version are currently on the switch.\nIn the following example, the switch configuration will go from CSM 1.2 to CSM 1.0.\nVerify that the correct configuration file exists on the switch.\nsw-spine-001 [mlag-domain: master] (config) # show configuration files Example output:\ncsm1.0 csm1.2 (active) csm1.2.bak initial initial.bak Active configuration: csm1.2 Unsaved changes : no Switch to desired config version, which requires a reboot.\n(config) # configuration switch-to csm1.0 This requires a reboot. Type \u0026#39;yes\u0026#39; to confirm: yes The switch should boot to the config version typed in the previous command.\nVerify the config version after the switch is booted.\n# show configuration files Example output:\ncsm1.0 (active) csm1.2 csm1.2.bak initial initial.bak Active configuration: csm1.0 Unsaved changes : yes "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/classifier_policies/",
	"title": "Classifier Policies",
	"tags": [],
	"description": "",
	"content": "Classifier Policies Classifier policies allow a network administrator to define sets of rules based on network traffic addressing or other header content and use these rules to restrict or alter the passage of traffic through the switch.\nChoosing the rule criteria is called classification, and one such rule, or list, is called a policy.\nClassification is achieved by creating a traffic class. There are three types of classes – MAC, IPv4, and IPv6 – which are each focused on relevant frame/packet characteristics. Classes can be configured to match or ignore almost any frame or packet header field.\nA policy contains one or more policy entries which are listed according to priority by sequence number. A single policy entry contains a class and corresponding policy action. Policy action is taken on traffic matched by its corresponding class.\nConfiguration commands Create a class\nswitch(config)# class \u0026lt;all|ip|ip6|mac\u0026gt; NAME Configure a class\nswitch(config-class-ip)# [SEQ] \u0026lt;match|ignore\u0026gt; \u0026lt;any|PROTOCOL\u0026gt; \u0026lt;any|SRC-IP\u0026gt; \u0026lt;any|DST-IP\u0026gt; switch(config-class-ip)# [SEQ] comment TEXT Create a policy\nswitch(config)# policy NAME Configure a policy\nswitch(config-policy)# [SEQ] class \u0026lt;ip|ipv6|mac\u0026gt; NAME [action [ip-precedence VALUE|pcp VALUE|dsc VALUE|cir kbps RATE cbs BYTES exceed drop|mirror MIRROR|drop] ...] Apply a policy\nswitch(config-if)# apply policy NAME [in|routed-in] switch(config-vlan)# apply policy NAME [in|routed-in] switch(config-tunnel)# apply policy NAME [in|routed-in] Show commands to validate functionality\nswitch# show class [ip|ipv6|mac] [NAME] switch# show policy [NAME] Expected results Administrators can configure a class Administrators can configure a policy Administrators can apply a policy to an interface The output of the show commands is correct Example switch(config)# class ip BROWSER switch(config-class-ip)# match tcp any any eq 80 switch(config-class-ip)# match tcp any any eq 8080 switch(config-class-ip)# match tcp any any eq 8081 switch(config-class-ip)# exit switch(config)# class ip NMS_CLASS switch(config-class-ip)# match udp any any eq 161 switch(config-class-ip)# exit switch(config)# policy USERPORTS switch(config-policy)# class ip NMS_CLASS action dscp CS6 action pcp 6 switch(config-policy)# class ip BROWSER action dscp CS1 action pcp 1 switch(config-policy)# exit switch(config)# interface 1/1/1 switch(config-if)# apply policy USERPORTS i switch(config-if)# end switch# show class ip BROWSER Type Name Sequence Comment Action Source IP Address Destination IP Address Additional Parameters L3 Protocol Source L4 Port(s) Destination L4 Port(s) ------------------------------------------------------------------------------- IPv4 BROWSER 10 match any any 20 match any any 30 match any tcp = 80 tcp = 8080 tcp any ------------------------------------------------------------------------------- switch# show class ip NMS_CLASS Type Name Sequence Comment Action Source IP Address Destination IP Address Additional Parameters = 8081 L3 Protocol Source L4 Port(s) Destination L4 Port(s) ------------------------------------------------------------------------------- IPv4 NMS_CLASS 10 match udp any any = 161 ------------------------------------------------------------------------------- switch# show policy USERPORTS Name Sequence Comment Class Type action ------------------------------------------------------------------------------- USERPORTS 10 NMS_CLASS ipv4 pcp 6 dscp CS6 20 BROWSER ipv4 pcp 1 dscp CS1 ------------------------------------------------------------------------------- switch# show policy configuration commands policy USERPORTS 10 class ip NMS_CLASS action pcp 6 action dscp CS6 20 class ip BROWSER action pcp 1 action dscp CS1 interface 1/1/1 apply policy USERPORTS in switch# show policy hitcounts USERPORTS Statistics for Policy USERPORTS: Interface 1/1/1* (in): Hit Count Configuration 10 class ip NMS_CLASS action pcp 6 action dscp CS6 - 10 match udp any any eq 161 20 class ip BROWSER action pcp 1 action dscp CS1 - 10matchtcpanyany eq80 - 20 match tcp any any eq 8080 - 30 match tcp any any eq 8081 - 40 (null) any any any * policy statistics are shared among all applied interfaces use \u0026#39;policy NAME copy\u0026#39; to create a uniquely-named policy "
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/determine_if_pods_are_hitting_resource_limits/",
	"title": "Determine if Pods are Hitting Resource Limits",
	"tags": [],
	"description": "",
	"content": "Determine if Pods are Hitting Resource Limits Determine if a pod is being CPU throttled or hitting its memory limits (OOMKilled). Use the /opt/cray/platform-utils/detect_cpu_throttling.sh script to determine if any pods are being CPU throttled, and check the Kubernetes events to see if any pods are hitting a memory limit.\nIMPORTANT: The presence of CPU throttling does not always indicate a problem, but if a service is being slow or experiencing latency issues, this procedure can be used to evaluate if it is not performing well as a result of CPU throttling.\nIdentify pods that are hitting resource limits in order to increase the resource limits for those pods.\nPrerequisites kubectl is installed.\nProcedure Use the /opt/cray/platform-utils/detect_cpu_throttling.sh script to determine if any pods are being CPU throttled (this script is installed on master and worker NCNs). The script can be used in two different ways:\nPass in a substring of the desired pod names.\nIn this example, the externaldns pods are being used:\nncn-mw# /opt/cray/platform-utils/detect_cpu_throttling.sh externaldns Example output:\nChecking cray-externaldns-external-dns-6988c5d5c5-795lb *** CPU throttling for containerid 76f45c4c18bf8ee6d4f777a602430e021c2a0d0e024380d22341414ca25ccffd: *** nr_periods 6066669 nr_throttled 23725 throttled_time 61981768066252 Call the script without a parameter to evaluate all pods.\nThis can take two minutes or more to run when evaluating all pods:\nncn-mw# /opt/cray/platform-utils/detect_cpu_throttling.sh Example output:\nChecking cray-ceph-csi-cephfs-nodeplugin-zqqvv Checking cray-ceph-csi-cephfs-provisioner-6458879894-cbhlx Checking cray-ceph-csi-cephfs-provisioner-6458879894-wlg86 Checking cray-ceph-csi-cephfs-provisioner-6458879894-z85vj Checking cray-ceph-csi-rbd-nodeplugin-62478 *** CPU throttling for containerid 0f52122395dcf209d851eed7a1125b5af8a7a6ea1d8500287cbddc0335e434a0: *** nr_periods 14338 nr_throttled 2 throttled_time 590491866 [...] Interpret the script results.\nnr_periods: How many full periods have been elapsed. nr_throttled: The number of times the full allowed bandwidth was exhausted. throttled_time: The total time the tasks were not run because of being over quota. See cgroup documentation for more details. Check if a pod was killed/restarted because it reached its memory limit.\nLook for a Kubernetes event associated with the pod being killed/restarted.\nncn-mw# kubectl get events -A | grep -C3 OOM Example output:\ndefault 54m Warning OOMKilling node/ncn-w003 Memory cgroup out of memory: Kill process 1223856 (prometheus) score 1966 or sacrifice child default 44m Warning OOMKilling node/ncn-w003 Memory cgroup out of memory: Kill process 1372634 (prometheus) score 1966 or sacrifice child Determine which pod was killed using the output of the previous command.\nSearch the pods in Kubernetes for the string returned in the previous step to find the exact pod name. Based on the previous example command output, prometheus is used in this example:\nncn-mw# kubectl get pod -A | grep prometheus Increase the resource limits for the pods identified in this procedure.\nSee Increase Pod Resource Limits for how to increase these limits.\n"
},
{
	"uri": "/docs-csm/en-12/operations/",
	"title": "Cray System Management (CSM) Administration Guide",
	"tags": [],
	"description": "",
	"content": "Cray System Management (CSM) Administration Guide The Cray System Management (CSM) operational activities are administrative procedures required to operate an HPE Cray EX system with CSM software installed.\nThe following administrative topics can be found in this guide:\nCSM product management Pre-install Image management Boot orchestration System power off procedures System power on procedures Power management Artifact management Compute rolling upgrades Configuration management Kubernetes Package repository management Security and authentication Resiliency ConMan Utility storage System management health System Layout Service (SLS) System configuration service Hardware State Manager (HSM) Hardware Management (HM) collector HPE Power Distribution Unit (PDU) Node management Network Management network Customer accessible networks (CMN/CAN/CHN) Dynamic Host Configuration Protocol (DHCP) Domain Name Service (DNS) External DNS MetalLB in BGP-mode Spire Update firmware with FAS User Access Service (UAS) CSM product management Validate CSM Health Apply Security Hardening Configure Keycloak Account Configure the Cray Command Line Interface (Cray CLI) Change Passwords and Credentials Configure Non-Compute Nodes with CFS Configure CSM Packages with CFS Perform NCN Personalization Access the LiveCD USB Device After Reboot Post-Install Customizations Validate Signed RPMs Remove Artifacts from Product Installation Security Hardening Pre-install General information on what needs to be done before the initial install of CSM.\nPre-Install Steps Change Air-Cooled BMC Credentials Change ServerTech PDU Credentials Image management Build and customize image recipes with the Image Management Service (IMS).\nImage Management Image Management Workflows Upload and Register an Image Recipe Build a New UAN Image Using the Default Recipe Build an Image Using IMS REST Service Import External Image to IMS Customize an Image Root Using IMS Create UAN Boot Images Convert TGZ Archives to SquashFS Images Delete or Recover Deleted IMS Content Configure IMS to Validate RPMs Update IMS Job Access Network Boot orchestration Use the Boot Orchestration Service (BOS) to boot, configure, and shut down collections of nodes.\nBoot Orchestration Service (BOS) BOS Workflows BOS Session Templates Manage a Session Template Create a Session Template to Boot Compute Nodes with CPS Boot UANs BOS Sessions Manage a BOS Session View the Status of a BOS Session Limit the Scope of a BOS Session Configure the BOS Timeout When Booting Compute Nodes Kernel Boot Parameters Check the Progress of BOS Session Operations Clean Up Logs After a BOA Kubernetes Job Clean Up After a BOS/BOA Job is Completed or Cancelled Troubleshoot UAN Boot Issues Troubleshoot Booting Nodes with Hardware Issues BOS Limitations for Gigabyte BMC Hardware Stage Changes without BOS Compute Node Boot Sequence Healthy Compute Node Boot Process Node Boot Root Cause Analysis Compute Node Boot Issue Symptom: Duplicate Address Warnings and Declined DHCP Offers in Logs Compute Node Boot Issue Symptom: Node is Not Able to Download the Required Artifacts Compute Node Boot Issue Symptom: Message About Invalid EEPROM Checksum in Node Console or Log Boot Issue Symptom: Node HSN Interface Does Not Appear or Show Detected Links Detected Compute Node Boot Issue Symptom: Node Console or Logs Indicate that the Server Response has Timed Out Tools for Resolving Compute Node Boot Issues Troubleshoot Compute Node Boot Issues Related to Unified Extensible Firmware Interface (UEFI) Troubleshoot Compute Node Boot Issues Related to Dynamic Host Configuration Protocol (DHCP) Troubleshoot Compute Node Boot Issues Related to the Boot Script Service Troubleshoot Compute Node Boot Issues Related to Trivial File Transfer Protocol (TFTP) Troubleshoot Compute Node Boot Issues Using Kubernetes Log File Locations and Ports Used in Compute Node Boot Troubleshooting Troubleshoot Compute Node Boot Issues Related to Slow Boot Times Edit the iPXE Embedded Boot Script Redeploy the iPXE and TFTP Services Upload Node Boot Information to Boot Script Service (BSS) Hang Listing BOS Sessions System power off procedures Procedures required for a full power off of an HPE Cray EX system.\nSystem Power Off Procedures Additional links to power off sub-procedures provided for reference. Refer to the main procedure linked above before using any of these sub-procedures:\nPrepare the System for Power Off Shut Down and Power Off Compute and User Access Nodes Save Management Network Switch Configuration Settings Power Off Compute and IO Cabinets Shut Down and Power Off the Management Kubernetes Cluster Power Off the External Lustre File System System power on procedures Procedures required for a full power on of an HPE Cray EX system.\nSystem Power On Procedures Additional links to power on sub-procedures provided for reference. Refer to the main procedure linked above before using any of these sub-procedures:\nPower On the External Lustre File System Power On and Start the Management Kubernetes Cluster Power On Compute and IO Cabinets Power On and Boot Compute and User Access Nodes Recover from a Liquid Cooled Cabinet EPO Event Worker Node COS Power Up Configuration Power management HPE Cray System Management (CSM) software manages and controls power out-of-band through Redfish APIs.\nPower Management Cray Advanced Platform Monitoring and Control (CAPMC) Liquid Cooled Node Power Management User Access to Compute Node Power Data Standard Rack Node Power Management Ignore Nodes with CAPMC Set the Turbo Boost Limit Artifact management Use the Ceph Object Gateway Simple Storage Service (S3) API to manage artifacts on the system.\nArtifact Management Manage Artifacts with the Cray CLI Use S3 Libraries and Clients Generate Temporary S3 Credentials Compute rolling upgrades Upgrade sets of compute nodes with the Compute Rolling Upgrade Service (CRUS) without requiring an entire set of nodes to be out of service at once. CRUS enables administrators to limit the impact on production caused from upgrading compute nodes by working through one step of the upgrade process at a time.\nNote: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality. See Deprecated features.\nCompute Rolling Upgrade Service (CRUS) CRUS Workflow Upgrade Compute Nodes with CRUS Troubleshoot Nodes Failing to Upgrade in a CRUS Session Troubleshoot a Failed CRUS Session Because of Unmet Conditions Troubleshoot a Failed CRUS Session Because of Bad Parameters Configuration management The Configuration Framework Service (CFS) is available on systems for remote execution and configuration management of nodes and boot images.\nConfiguration Management Configuration Layers Create a CFS Configuration Update a CFS Configuration Ansible Inventory Specifying Hosts and Groups Manage Multiple Inventories in a Single Location Configuration Sessions Create a CFS Session with Dynamic Inventory Create an Image Customization CFS Session Set Limits for a Configuration Session Use a Specific Inventory for a Configuration Session Change the Ansible Verbosity Logs Set the ansible.cfg for a Session Delete CFS Sessions Automatic Session Deletion with sessionTTL Track the Status of a Session View Configuration Session Logs Troubleshoot Ansible Play Failures in CFS Sessions Troubleshoot CFS Session Failing to Complete Troubleshoot CFS SessionS Failing to Start Configuration Management with the CFS Batcher CFS Flow Diagrams Configuration Management of System Components Ansible Execution Environments Use a Custom ansible-cfg File Enable Ansible Profiling CFS Global Options Version Control Service (VCS) Git Operations VCS Branching Strategy Customize Configuration Values Update the Privacy Settings for Gitea Configuration Content Repositories Create and Populate a VCS Configuration Repository Write Ansible Code for CFS Target Ansible Tasks for Image Customization CFS Key Management NCN Worker Image Customization Kubernetes The system management components are broken down into a series of micro-services. Each service is independently deployable, fine-grained, and uses lightweight protocols. As a result, the system\u0026rsquo;s micro-services are modular, resilient, and can be updated independently. Services within the Kubernetes architecture communicate using REST APIs.\nKubernetes Architecture About kubectl Configure kubectl Credentials to Access the Kubernetes APIs About Kubernetes Taints and Labels Kubernetes Storage Kubernetes Networking Retrieve Cluster Health Information Using Kubernetes Pod Resource Limits Determine if Pods are Hitting Resource Limits Increase Pod Resource Limits Increase Kafka Pod Resource Limits About etcd Check the Health and Balance of etcd Clusters Rebuild Unhealthy etcd Clusters Backups for etcd-operator Clusters Create a Manual Backup of a Healthy etcd Cluster Restore an etcd Cluster from a Backup Repopulate Data in etcd Clusters When Rebuilding Them Restore Bare-Metal etcd Clusters from an S3 Snapshot Rebalance Healthy etcd Clusters Check for and Clear etcd Cluster Alarms Report the Endpoint Status for etcd Clusters Clear Space in an etcd Cluster Database About Postgres Troubleshoot Postgres Database Recover from Postgres WAL Event Restore Postgres Disaster Recovery for Postgres View Postgres Information for System Databases containerd Troubleshoot Intermittent HTTP 503 Code Failures TDS Lower CPU Requests Kubernetes and Bare Metal EtcD Certificate Renewal Configure API Audit Log Retention Package repository management Repositories are added to systems to extend the system functionality beyond what is initially delivered. The Sonatype Nexus Repository Manager is the primary method for repository management. Nexus hosts the Yum, Docker, raw, and Helm repositories for software and firmware content.\nPackage Repository Management Package Repository Management with Nexus Manage Repositories with Nexus Nexus Configuration Nexus Deployment Nexus Export and Restore Restrict Admin Privileges in Nexus Repair Yum Repository Metadata Nexus Space Cleanup Troubleshoot Nexus Security and authentication Mechanisms used by the system to ensure the security and authentication of internal and external requests.\nSystem Security and Authentication Manage System Passwords Update NCN Passwords Change Root Passwords for Compute Nodes Set NCN Image Root Password, SSH Keys, and Timezone Set NCN Image Root Password, SSH Keys, and Timezone on PIT Node Set NCN Image Root Password, SSH Keys, and Timezone Change EX Liquid-Cooled Cabinet Global Default Password Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials Updating the Liquid-Cooled EX Cabinet Default Credentials after a CEC Password Change Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials Change Air-Cooled Node BMC Credentials Change SNMP Credentials on Leaf-BMC Switches Update Default ServerTech PDU Credentials used by the Redfish Translation Service Change Credentials on ServerTech PDUs Add Root Service Account for Gigabyte Controllers Recovering from Mismatched BMC Credentials SSH Keys Authenticate an Account with the Command Line Default Keycloak Realms, Accounts, and Clients Certificate Types Change Keycloak Token Lifetime Change the Keycloak Admin Password Create a Service Account in Keycloak Retrieve the Client Secret for Service Accounts Get a Long-Lived Token for a Service Account Access the Keycloak User Management UI Create Internal User Accounts in the Keycloak Shasta Realm Delete Internal User Accounts in the Keycloak Shasta Realm Create Internal Groups in the Keycloak Shasta Realm Remove Internal Groups from the Keycloak Shasta Realm Remove the Email Mapper from the LDAP User Federation Re-Sync Keycloak Users to Compute Nodes Keycloak Operations Configure Keycloak for LDAP/AD authentication Configure the RSA Plugin in Keycloak Preserve Username Capitalization for Users Exported from Keycloak Change the LDAP Server IP Address for Existing LDAP Server Content Change the LDAP Server IP Address for New LDAP Server Content Remove the LDAP User Federation from Keycloak Add LDAP User Federation Keycloak User Management with kcadm.sh Keycloak User Localization Create a Backup of the Keycloak Postgres Database Public Key Infrastructure (PKI) PKI Certificate Authority (CA) Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster Transport Layer Security (TLS) for Ingress Services PKI Services HashiCorp Vault Backup and Restore Vault Clusters Troubleshoot Common Vault Cluster Issues API Authorization Retrieve an Authentication Token Manage Sealed Secrets Audit Logs Restrict Network Access to the ncn-images S3 Bucket Resiliency HPE Cray EX systems are designed so that system management services (SMS) are fully resilient and that there is no single point of failure.\nResiliency Resilience of System Management Services Restore System Functionality if a Kubernetes Worker Node is Down Recreate StatefulSet Pods on Another Node NTP Resiliency Resiliency Testing Procedure ConMan ConMan is a tool used for connecting to remote consoles and collecting console logs. These node logs can then be used for various administrative purposes, such as troubleshooting node boot issues.\nConMan Access Compute Node Logs Access Console Log Data Via the System Monitoring Framework (SMF) Manage Node Consoles Log in to a Node Using ConMan Establish a Serial Connection to NCNs Disable ConMan After System Software Installation Troubleshoot ConMan Blocking Access to a Node BMC Troubleshoot ConMan Failing to Connect to a Console Troubleshoot ConMan Asking for Password on SSH Connection Utility storage Ceph is the utility storage platform that is used to enable pods to store persistent data. It is deployed to provide block, object, and file storage to the management services running on Kubernetes, as well as for telemetry data coming from the compute nodes.\nUtility Storage Collect Information about the Ceph Cluster Manage Ceph Services Adjust Ceph Pool Quotas Add Ceph OSDs Shrink Ceph OSDs Ceph Health States Ceph Deep Scrubs Ceph Daemon Memory Profiling Ceph Service Check Script Usage Ceph Orchestrator Usage Ceph Storage Types Dump Ceph Crash Data Identify Ceph Latency Issues Cephadm Reference Material Adding a Ceph Node to the Ceph Cluster Shrink the Ceph Cluster Alternate Storage Pools Restore Nexus Data After Data Corruption Troubleshoot Failure to Get Ceph Health Troubleshoot a Down OSD Troubleshoot Ceph OSDs Reporting Full Troubleshoot System Clock Skew Troubleshoot an Unresponsive S3 Endpoint Troubleshoot Ceph-Mon Processes Stopping and Exceeding Max Restarts Troubleshoot Pods Multi-Attach Error Troubleshoot Large Object Map Objects in Ceph Health Troubleshoot Failure of RGW Health Check Troubleshoot Ceph MDS Client Connectivity Issues Troubleshooting Ceph MDS Reporting Slow Requests and Failure on Client Troubleshoot Ceph Services Not Starting After a Server Crash Troubleshoot Insufficient Standby MDS Daemons Available Troubleshoot S3FS Mount Issues System management health Enable system administrators to assess the health of their system. Operators need to quickly and efficiently troubleshoot system issues as they occur and be confident that a lack of issues indicates the system is operating normally.\nSystem Management Health System Management Health Checks and Alerts Access System Management Health Services Configure Prometheus Email Alert Notifications Grafana Dashboards by Component Troubleshoot Grafana Dashboard Grafterm Remove Kiali Troubleshoot Prometheus Alerts System Layout Service (SLS) The System Layout Service (SLS) holds information about the system design, such as the physical locations of network hardware, compute nodes, and cabinets. It also stores information about the network, such as which port on which switch should be connected to each compute node.\nSystem Layout Service (SLS) Dump SLS Information Load SLS Database with Dump File Add Liquid-Cooled Cabinets to SLS Add UAN CAN IP Addresses to SLS Update SLS with UAN Aliases Add an alias to a service Create a Backup of the SLS Postgres Database Restore SLS Postgres Database from Backup Restore SLS Postgres without an Existing Backup System configuration service The System Configuration Service (SCSD) allows administrators to set various BMC and controller parameters. These parameters are typically set during discovery, but this tool enables parameters to be set before or after discovery. The operations to change these parameters are available in the Cray CLI under the scsd command.\nSystem Configuration Service Configure BMC and Controller Parameters with SCSD Manage Parameters with the SCSD Service Set BMC Credentials Hardware State Manager (HSM) Use the Hardware State Manager (HSM) to monitor and interrogate hardware components in the HPE Cray EX system, tracking hardware state and inventory information, and making it available via REST queries and message bus events when changes occur.\nHardware State Manager (HSM) Hardware Management Services (HMS) Locking API Lock and Unlock Management Nodes Manage HMS Locks Component Groups and Partitions Manage Component Groups Component Group Members Manage Component Partitions Component Partition Members Component Memberships Hardware State Manager (HSM) State and Flag Fields HSM Roles and Subroles Add an NCN to the HSM Database Add a Switch to the HSM Database Create a Backup of the HSM Postgres Database Restore HSM Postgres from a Backup Restore HSM Postgres without a Backup Set BMC Management Role Hardware Management (HM) collector The Hardware Management (HM) Collector is used to collect telemetry and Redfish events from hardware in the system.\nAdjust HM Collector resource limits and requests HPE Power Distribution Unit (PDU) Procedures for managing and setting up HPE PDUs.\nHPE PDU Admin Procedure Node management Monitor and manage compute nodes (CNs) and non-compute nodes (NCNs) used in the HPE Cray EX system.\nNode Management Node Management Workflows Rebuild NCNs Identify Nodes and Update Metadata Prepare Storage Nodes Wipe Drives Power Cycle and Rebuild Nodes Adding a Ceph Node to the Ceph Cluster Customize PCIe Hardware Customize Disk Hardware Validate Boot Loader Validate Storage Node Final Validation Steps Reboot NCNs Check and Set the metalno-wipe Setting on NCNs Enable Nodes Disable Nodes Find Node Type and Manufacturer Add Additional Liquid-Cooled Cabinets to a System Updating Cabinet Routes on Management NCNs Move a liquid-cooled blade within a System Removing a Liquid-cooled blade from a System Adding a Liquid-cooled blade to a System Add a Standard Rack Node Move a Standard Rack Node Move a Standard Rack Node (Same Rack/Same HSN Ports) Verify Node Removal Clear Space in Root File System on Worker Nodes Troubleshoot Issues with Redfish Endpoint DiscoveryCheck for Redfish Events from Nodes Reset Credentials on Redfish Devices Access and Update Settings for Replacement NCNs Change Settings for HMS Collector Polling of Air Cooled Nodes Use the Physical KVM Launch a Virtual KVM on Gigabyte Nodes Launch a Virtual KVM on Intel Nodes Change Java Security Settings Configuration of NCN Bonding Change Settings in the Bond Troubleshoot Interfaces with IP Address Issues Troubleshoot Loss of Console Connections and Logs on Gigabyte Nodes Check the BMC Failover Mode Update Compute Node Mellanox HSN NIC Firmware TLS Certificates for Redfish BMCs Add TLS Certificates to BMCs Dump a Non-Compute Node Enable Passwordless Connections to Liquid Cooled Node BMCs View BIOS Logs for Liquid Cooled Nodes Configure NTP on NCNs Swap a Compute Blade with a Different System Update the Gigabyte Node BIOS Time S3FS Usage Guidelines NCN Drive Identification Build NCN Images Locally Replace a Compute Blade Network Overview of the several different networks supported by the HPE Cray EX system.\nNetwork Access to System Management Services Default IP Address Ranges Connect to the HPE Cray EX Environment Create a CSM Configuration Upgrade Plan Gateway Testing Management network HPE Cray EX systems can have network switches in many roles: spine switches, leaf switches, LeafBMC switches, and CDU switches. Newer systems have HPE Aruba switches, while older systems have Dell and Mellanox switches. Switch IP addresses are generated by Cray Site Init (CSI).\nHPE Cray EX Management Network Installation and Configuration Guide Aruba Installation and Configuration Dell Installation and Configuration Mellanox Installation and Configuration Update Management Network Firmware Load Saved Switch Configuration Customer accessible networks (CMN/CAN/CHN) The customer accessible networks (CMN/CAN/CHN) provide access from outside the customer network to services, NCNs, and User Access Nodes (UANs) in the system.\nCustomer Accessible Networks Externally Exposed Services Connect to the CMN and CAN BI-CAN Aruba/Arista Configuration MetalLB Peering with Arista Edge Router CAN/CMN with Dual-Spine Configuration Troubleshoot CMN Issues Dynamic Host Configuration Protocol (DHCP) The DHCP service on the HPE Cray EX system uses the Internet Systems Consortium (ISC) Kea tool. Kea provides more robust management capabilities for DHCP servers.\nDHCP Troubleshoot DHCP Issues Domain Name Service (DNS) The central DNS infrastructure provides the structural networking hierarchy and datastore for the system.\nDNS Manage the DNS Unbound Resolver Enable ncsd on UANs PowerDNS Configuration PowerDNS Migration Guide Troubleshoot Common DNS Issues Troubleshoot PowerDNS External DNS External DNS, along with the Customer Management Network (CMN), Border Gateway Protocol (BGP), and MetalLB, makes it simpler to access the HPE Cray EX API and system management services. Services are accessible directly from a laptop without needing to tunnel into a non-compute node (NCN) or override /etc/hosts settings.\nExternal DNS External DNS csi config init Input Values Update the cmn-external-dns Value Post-Installation Ingress Routing External DNS Failing to Discover Services Workaround Troubleshoot Connectivity to Services with External IP addresses Troubleshoot DNS Configuration Issues MetalLB in BGP-mode MetalLB is a component in Kubernetes that manages access to LoadBalancer services from outside the Kubernetes cluster. There are LoadBalancer services on the Node Management Network (NMN), Hardware Management Network (HMN), and Customer Access Network (CAN).\nMetalLB can run in either Layer2-mode or BGP-mode for each address pool it manages. BGP-mode is used for the NMN, HMN, and CAN. This enables true load balancing (Layer2-mode does failover, not load balancing) and allows for a more robust layer 3 configuration for these networks.\nMetalLB in BGP-Mode MetalLB Configuration Check BGP Status and Reset Sessions Troubleshoot Services without an Allocated IP Address Troubleshoot BGP not Accepting Routes from MetalLB Spire Spire provides the ability to authenticate nodes and workloads, and to securely distribute and manage their identities along with the credentials associated with them.\nRestore Spire Postgres without a Backup Troubleshoot Spire Failing to Start on NCNs Update Spire Intermediate CA Certificate Restore Missing Spire Meta-Data Create a Backup of the Spire Postgres Database Update firmware with FAS The Firmware Action Service (FAS) provides an interface for managing firmware versions of Redfish-enabled hardware in the system. FAS interacts with the Hardware State Manager (HSM), device data, and image data in order to update firmware.\nSee Update Firmware with FAS for a list components that are upgradable with FAS. Refer to the HPC Firmware Pack (HFP) product stream to update firmware on other components.\nUpdate Firmware with FAS FAS CLI FAS Filters FAS Recipes FAS Admin Procedures FAS Use Cases Upload Olympus BMC Recovery Firmware into TFTP Server Updating Firmware on m001 Updating Firmware without FAS User Access Service (UAS) The User Access Service (UAS) is a containerized service managed by Kubernetes that enables application developers to create and run user applications. Users launch a User Access Instance (UAI) using the cray command. Users can also transfer data between the Cray system and external systems using the UAI.\nUser Access Service (UAS) List UAS Version Information Clear UAS Configuration UAS Limitations End-User UAIs Special Purpose UAIs Elements of a UAI UAI Host Nodes UAI macvlans Network Attachments UAI Host Node Selection UAI Network Attachments Configure UAIs in UAS UAI Images Listing Registered UAI Images Register a UAI Image Retrieve UAI Image Registration Information Update a UAI Image Registration Delete a UAI Image Registration Volumes List Volumes Registered in UAS Add a Volume to UAS Obtain Configuration of a UAS Volume Update a UAS Volume Delete a Volume Configuration Resource Specifications List UAI Resource Specifications Create a UAI Resource Specification Retrieve Resource Specification Details Update a Resource Specification Delete a UAI Resource Specification UAI Classes List Available UAI Classes Create a UAI Class View a UAI Class Modify a UAI Class Delete a UAI Class Common UAI Configuration UAI Management List UAIs Creating a UAI Create a UAI with Additional Ports Examining a UAI Using a Direct Administrative Command Deleting a UAI Legacy Mode User-Driven UAI Management Configure A Default UAI Class for Legacy Mode Create and Use Default UAIs in Legacy Mode List Available UAI Images in Legacy Mode Create UAIs From Specific UAI Images in Legacy Mode UAS and UAI Legacy Mode Health Checks Broker Mode UAI Management Configure End-User UAI Classes for Broker Mode Configure a Broker UAI class Start a Broker UAI Log in to a Broker UAI Broker UAI Resiliency and Load Balancing UAI Images Customize the Broker UAI Image Customize End-User UAI Images UAI Image Customization Choosing UAI Resource Settings Setting UAI Timeouts Troubleshoot UAS Issues Troubleshoot UAS by Viewing Log Output Troubleshoot UAIs by Viewing Log Output Troubleshoot Stale Brokered UAIs Troubleshoot UAI Stuck in ContainerCreating Troubleshoot Duplicate Mount Paths in a UAI Troubleshoot Missing or Incorrect UAI Images Troubleshoot UAIs with Administrative Access Troubleshoot Common Mistakes when Creating a Custom End-User UAI Image Troubleshoot Broker UAI SSSD Cannot Use /etc/sssd/sssd.conf Troubleshoot UAS / CLI Authentication Issues "
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/manage_component_groups/",
	"title": "Manage Component Groups",
	"tags": [],
	"description": "",
	"content": "Manage Component Groups The creation, deletion, and modification of groups is enabled by the Hardware State Manager (HSM) APIs.\nExample group Prerequisites Create and modify a group Create a group Modify a group Retrieve a group Delete a group Example group The following is an example group that contains the optional fields tags and exclusiveGroup:\n{ \u0026#34;label\u0026#34; : \u0026#34;blue\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;blue node group\u0026#34;, \u0026#34;tags\u0026#34; : [ \u0026#34;tag1\u0026#34;, \u0026#34;tag2\u0026#34; ], \u0026#34;members\u0026#34; : { \u0026#34;ids\u0026#34; : [ \u0026#34;x0c0s0b0n0\u0026#34;, \u0026#34;x0c0s0b0n1\u0026#34;, \u0026#34;x0c0s0b1n0\u0026#34;, \u0026#34;x0c0s0b1n1\u0026#34; ] }, \u0026#34;exclusiveGroup\u0026#34; : \u0026#34;colors\u0026#34; } Prerequisites The commands on this page will not work unless the Cray CLI has been initialized on the node where the commands are being run. For more information, see Configure the Cray CLI.\nCreate and modify a group A group is defined by its members list and identifying label. It is also possible to add a description and a free form set of tags to help organize groups.\nThe members list may be set initially with the full list of member IDs, or can begin empty and have components added individually. The following examples show different ways to create and modify a group.\nCreate a group Create a new non-exclusive group with an empty members list and two optional tags:\nncn-mw# cray hsm groups create --label GROUP_LABEL \\ --tags TAG1,TAG2 \\ --description DESCRIPTION_OF_GROUP_LABEL Create a new group with a pre-set members list, which is part of an exclusive group:\nncn-mw# cray hsm groups create --label GROUP_LABEL \\ --description DESCRIPTION_OF_GROUP_LABEL \\ --exclusive-group EXCLUSIVE_GROUP_LABEL \\ --members-ids MEMBER_ID,MEMBER_ID,MEMBER_ID Create a new group:\nncn-mw# cray hsm groups create -v --label GROUP_LABEL Modify a group Add a description of a group:\nncn-mw# cray hsm groups update test_group --description \u0026#34;Description of group\u0026#34; Add a new component to a group:\nncn-mw# cray hsm groups members create --id XNAME GROUP_LABEL Retrieve a group Retrieve the complete group object to learn more about a group. This is also submitted when the group is created, except it is up-to-date with any additions or deletions from the members set.\nRetrieve all fields for a group, including the members list:\nncn-mw# cray hsm groups describe GROUP_LABEL Delete a group Entire groups can be removed. The group label is deleted and removed from all members who were formerly a part of the group.\nDelete a group with the following command:\nncn-mw# cray hsm groups delete GROUP_LABEL "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/create_a_cfs_configuration/",
	"title": "Create a CFS Configuration",
	"tags": [],
	"description": "",
	"content": "Create a CFS Configuration Create a Configuration Framework Service (CFS) configuration, which contains an ordered list of layers. Each layer is defined by a Git repository clone URL, a Git commit, a name, and the path in the repository to an Ansible playbook to execute.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. Procedure Create a JSON file to hold data about the CFS configuration.\nncn# cat configurations-example.json Example configuration:\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-2\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo2.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34; }, # { ... add more configuration layers here, if needed ... } ] } Add the configuration to CFS with the JSON file.\nncn# cray cfs configurations update configurations-example \\ --file ./configurations-example.json --format json Example output:\n{ \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:00:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ ... ], \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; } "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/configure_the_bos_timeout_when_booting_nodes/",
	"title": "Configure the BOS Timeout When Booting Compute Nodes",
	"tags": [],
	"description": "",
	"content": "Configure the BOS Timeout When Booting Compute Nodes NOTE This section is for Boot Orchestration Service (BOS) v1 only.\nManually update the boa-job-template ConfigMap to tune the timeout and sleep intervals for the Boot Orchestration Agent (BOA). Correcting the timeout value is a good troubleshooting option for when BOS sessions hang waiting for nodes to be in a Ready state.\nIf the BOS timeout occurs when booting compute nodes, the system will be unable to boot via BOS.\nPrerequisites A BOS session was run and compute nodes are failing to move to a Ready state.\nProcedure Edit the boa-job-template ConfigMap to add the new timeout values.\nncn-mw# kubectl edit configmap -n services boa-job-template Node boots can be set to time out faster by adding the following environment variables to the boa-job-template. These variables do not appear in the ConfigMap by default.\nNODE_STATE_CHECK_NUMBER_OF_RETRIES BOA will check on the expected state of nodes this many times before giving up. This number can be set to a very low number to make BOA time-out quickly. NODE_STATE_CHECK_SLEEP_INTERVAL This is how long BOA will sleep between checks, in seconds. This number can be set to a very low number to make BOA time-out quickly. The current default behavior in the absence of these parameters is 5 seconds (sleep interval) x 120 (retries), which has a timeout of 600 seconds or 10 minutes. The default values are shown below:\n- name: \u0026#34;NODE_STATE_CHECK_NUMBER_OF_RETRIES\u0026#34; value: \u0026#34;120\u0026#34; - name: \u0026#34;NODE_STATE_CHECK_SLEEP_INTERVAL\u0026#34; value: \u0026#34;5\u0026#34; The example below increases the number of retries to 360, which results in a timeout of 1800 seconds or 30 minutes if the sleep interval is not changed from the default value of 5 seconds. Different values might be needed depending on system size.\nAdd the following values to the ConfigMap:\n- name: \u0026#34;NODE_STATE_CHECK_NUMBER_OF_RETRIES\u0026#34; value: \u0026#34;360\u0026#34; The new variables need to be placed under the environment (env:) section in the ConfigMap. As an example, the env section in the ConfigMap looks as below.\nenv: - name: OPERATION value: \u0026#34;{{ operation }}\u0026#34; - name: SESSION_ID value: \u0026#34;{{ session_id }}\u0026#34; - name: SESSION_TEMPLATE_ID value: \u0026#34;{{ session_template_id }}\u0026#34; - name: SESSION_LIMIT value: \u0026#34;{{ session_limit }}\u0026#34; - name: DATABASE_NAME value: \u0026#34;{{ DATABASE_NAME }}\u0026#34; - name: DATABASE_PORT value: \u0026#34;{{ DATABASE_PORT }}\u0026#34; - name: LOG_LEVEL value: \u0026#34;{{ log_level }}\u0026#34; - name: SINGLE_THREAD_MODE value: \u0026#34;{{ single_thread_mode }}\u0026#34; - name: S3_ACCESS_KEY valueFrom: secretKeyRef: name: {{ s3_credentials }} key: access_key - name: S3_SECRET_KEY valueFrom: secretKeyRef: name: {{ s3_credentials }} key: secret_key - name: GIT_SSL_CAINFO value: /etc/cray/ca/certificate_authority.crt - name: S3_PROTOCOL value: \u0026#34;{{ S3_PROTOCOL }}\u0026#34; - name: S3_GATEWAY value: \u0026#34;{{ S3_GATEWAY }}\u0026#34; **- name: \u0026#34;NODE_STATE_CHECK_NUMBER_OF_RETRIES\u0026#34; value: \u0026#34;360\u0026#34;** Restart BOA.\nRestarting BOA will allow the new timeout values to take effect.\nncn-mw# kubectl scale deployment -n services cray-bos --replicas=0 \u0026amp;\u0026amp; kubectl scale deployment -n services cray-bos --replicas=1 "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/create_a_uai_with_additional_ports/",
	"title": "Create a UAI with Additional Ports",
	"tags": [],
	"description": "",
	"content": "Create a UAI with Additional Ports In legacy mode UAI creation, an option is available to expose UAI ports to the customer user network in addition to the the port used for SSH access. These ports are restricted to ports 80, 443, and 8888. This procedure allows a user or administrator to create a new UAI with these additional ports.\nPrerequisites The user must be logged into a host that has user access to the HPE Cray EX System API Gateway The user must have an installed initialized cray CLI and network access to the API Gateway The user must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The user must be logged in as to the HPE Cray EX System CLI (cray auth login command) The user must have a public SSH key configured on the host from which SSH connections to the UAI will take place The user must have access to a file containing the above public SSH key Limitations Only ports 80, 443, and 8888 can be exposed. Attempting to open any other ports will result in an error.\nProcedure Create a new UAI with the --ports option.\nvers\u0026gt; cray uas create --publickey PUBLIC_SSH_KEY_FILE --ports PORT_LIST When these ports are exposed in the UAI, they will be mapped from the port number on the externally visible IP address of the UAI to the port number used to reach the UAI pod. The mapping of these ports is displayed in the uai_portmap element of the returned output from cray uas create, and cray uas list. The mapping is shown as a dictionary where the key is the externally served port and the value is the internally routed port. Applications running on the UAI should listen on the internally routed port. Usually these will be the same value.\nvers\u0026gt; cray uas create --publickey ~/.ssh/id_rsa.pub --ports 80,443,8888 Example output:\nuai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@34.68.41.239\u0026#34; uai_host = \u0026#34;ncn-w002\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34; uai_ip = \u0026#34;34.68.41.239\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-42de2eeb\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; [uai_portmap] 80 = 80 443 = 443 8888 = 8888 Log in to the UAI with the connection string.\nlinux# ssh USERNAME@UAI_IP_ADDRESS -i ~/.ssh/id_rsa Top: User Access Service (UAS)\n"
},
{
	"uri": "/docs-csm/en-12/install/create_application_node_config_yaml/",
	"title": "Create Application Node Config YAML",
	"tags": [],
	"description": "",
	"content": "Create Application Node Config YAML This topic provides directions on constructing the application_node_config.yaml file. This file controls how the csi config init command finds and treats application nodes discovered in the hmn_connections.json file when generating configuration files for the system.\nPrerequisites Background Directions Prerequisites The application_node_config.yaml file can be constructed from information from one of the following sources:\nThe SHCD Excel spreadsheet for the system The hmn_connections.json file generated from the system\u0026rsquo;s SHCD Background SHCD and hmn_connections.json The HMN tab of the SHCD describes the air-cooled hardware present in the system and how these devices are connected to the Hardware Management Network (HMN). This information is required by CSM to perform hardware discovery and geolocation of air-cooled hardware in the system. The HMN tab may contain other hardware that is not managed by CSM, but is connected to the HMN.\nThe hmn_connections.json file is derived from the HMN tab of a system SHCD, and is one of the seed files required by Cray Site Init (CSI) command to generate configuration files required to install CSM. The hmn_connections.json file is almost a one-to-one copy of the right-hand table in the HMN tab of the SHCD. It is an array of JSON objects, and each object represents a row from the HMN tab. Any row that is not understood by CSI will be ignored, this includes any additional devices connected to the HMN that are not managed by CSM.\nFor a detailed mapping between the data in the SHCD and the equivalent information in the hmn_connections.json file, see Introduction to SHCD HMN Connections Rules and Application Nodes in SHCD HMN Connections Rules.\nWhat is a Source Name? The source name is the name of the device that is being connected to the HMN network. In the SHCD HMN tab, this is in a column with the header Source or the Source field in the element of the hmn_connections.json for this device. From this source name, the csi config init command can infer the type of hardware that is connected to the HMN network (Node BMC, PDU, HSN Switch, BMC, and more).\nExample SHCD row from HMN tab with column headers representing an application node with SourceName uan01 in cabinet x3000 in slot 19. Its BMC is connected to port 37 of the management leaf switch in x3000 in slot 14.\nSource (J20) Rack (K20) Location (L20) (M20) Parent (N20) (O20) Port (P20) Destination (Q20) Rack (R20) Location (S20) (T20) Port (U20) uan01 x3000 u19 - j3 sw-smn01 x3000 u14 - j37 Example hmn_connections.json row representing an application node with SourceName uan01 in cabinet x3000 in slot 19. Its BMC is connected to port 37 of the management leaf switch in x3000 in slot 14.\n{ \u0026#34;Source\u0026#34;: \u0026#34;uan01\u0026#34;, \u0026#34;SourceRack\u0026#34;: \u0026#34;x3000\u0026#34;, \u0026#34;SourceLocation\u0026#34;: \u0026#34;u19\u0026#34;, \u0026#34;DestinationRack\u0026#34;: \u0026#34;x3000\u0026#34;, \u0026#34;DestinationLocation\u0026#34;: \u0026#34;u14\u0026#34;, \u0026#34;DestinationPort\u0026#34;: \u0026#34;j37\u0026#34; } Directions Create a file called application_node_config.yaml with the following contents.\nThis is a base application node config file for CSI that does not add any additional prefixes, HSM SubRole mappings, or aliases.\n--- # Additional application node prefixes to match in the hmn_connections.json file prefixes: [] # Additional HSM SubRoles prefix_hsm_subroles: {} # Application Node aliases aliases: {} Identify application nodes present in hmn_connections.json or the HMN tab of the system\u0026rsquo;s SHCD. In general, everything in the HMN tab of the SHCD or hmn_connections.json file that starts with uan, gn, or ln, are considered application nodes and any node that does not follow the SHCD/HMN Connections Rules should also be considered an application node, unless it is a KVM.\nIf the hmn_connections.json file is not available, then use the HMN tab of SHCD spreadsheet. This table is equivalent to the example hmn_connections.json output below.\nSource (J20) Rack (K20) Location (L20) (M20) Parent (N20) (O20) Port (P20) Destination (Q20) Rack (R20) Location (S20) (T20) Port (U20) gateway01 x3000 u29 - j3 sw-smn01 x3000 u32 - j42 login02 x3000 u28 - j3 sw-smn01 x3000 u32 - j43 lnet01 x3000 u27 - j3 sw-smn01 x3000 u32 - j41 vn01 x3000 u25 - j3 sw-smn01 x3000 u32 - j40 uan01 x3000 u23 - j3 sw-smn01 x3000 u32 - j39 If the hmn_connections.json file is available, then the following command can be used to show the HMN rows that are application nodes.\nlinux# cat hmn_connections.json | jq -rc \u0026#39;.[] | select(.Source | test(\u0026#34;^((mn|wn|sn|nid|cn|cn\\\\-|pdu)\\\\d+|.*(cmc|rcm|kvm|door).*|x\\\\d+p\\\\d*|sw-.+|columbia$)\u0026#34;; \u0026#34;i\u0026#34;) | not)\u0026#39; Example hmn_connections.json output:\n{\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u29\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u32\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j42\u0026#34;}, {\u0026#34;Source\u0026#34;:\u0026#34;login02\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u28\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u32\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j43\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;lnet01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u27\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u32\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j41\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;vn01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u25\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u32\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j40\u0026#34;}, {\u0026#34;Source\u0026#34;:\u0026#34;uan01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u23\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u32\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j39\u0026#34;}, Add additional application node prefixes.\nThe prefixes field is an array of strings, that augments the list of source name prefixes that are treated as application nodes. By default, csi config init only looks for application nodes that have source names that start with uan, gn, and ln. If the system contains application nodes that fall outside of those source name prefixes, then additional prefixes must be added to application_node_config.yaml. These additional prefixes will be used in addition to the default prefixes.\nTo add an additional prefix, append a new string element to the prefixes array.\nNOTE: The command csi config init does a case insensitive check for whether a source name contains an application node prefix. For example, the prefix uan will match uan, Uan, and UAN.\nFrom the HMN example above, the following additional prefixes are required:\n# Additional application node prefixes to match in the hmn_connections.json file prefixes: - gateway - login - lnet - vn Add HSM SubRoles for application node prefixes.\nThe prefix_hsm_subroles field mapping application node prefix (string) to the applicable Hardware State Manager (HSM) SubRole (string) for the application nodes. All applications nodes have the HSM Role of Application, and the SubRole value can be used to label what type of the application node it is (such as UAN, Gateway, LNETRouter, and more).\nBy default, the csi config init command will use the following SubRoles for application nodes:\nPrefix HSM SubRole uan UAN ln UAN gn Gateway If there are no additional prefixes in the SHCD or no desire to use a different HSM SubRole than the default, then this prefix_hsm_subroles field does not need any data populated.\nTo add additional HSM SubRole for a given prefix, add a new mapping under the prefix_hsm_subroles field. Where the key is the application node prefix and the value is the HSM SubRole.\nValid HSM SubRoles values are: Worker, Master, Storage, UAN, Gateway, LNETRouter, Visualization, and UserDefined.\nFrom the HMN example above, the following additional prefix HSM SubRole mappings are required:\n# Additional HSM SubRoles prefix_hsm_subroles: login: UAN lnet: LNETRouter gateway: Gateway vn: Visualization Add application node aliases.\nThe aliases field is a map of component name (xname) strings to an array of alias strings.\nFor guidance on building application node component names (xnames), follow one of the following:\nBuilding component names (xnames) for nodes in a single application node chassis Building component names (xnames) for nodes in a dual application node chassis By default, the csi config init command does not set the ExtraProperties.Alias field for application nodes in the SLS input file.\nFor each application node, add its alias mapping under the aliases field. Where the key is the component name (xname) of the application node, and the value is an array of aliases (strings) which allows for one or more aliases to be specified for an application node.\nFrom the HMN example above, the following application node aliases are required:\n# Application Node aliases aliases: x3113c0s29b0n0: [\u0026#34;gateway01\u0026#34;] x3113c0s28b0n0: [\u0026#34;login02\u0026#34;] x3113c0s27b0n0: [\u0026#34;lnet01\u0026#34;] x3113c0s25b0n0: [\u0026#34;visualization01\u0026#34;, \u0026#34;vn02\u0026#34;] x3113c0s23b0n0: [\u0026#34;uan01\u0026#34;] The ordering of component names (xnames) under aliases does not matter.\nFinal information in the example application_node_config.yaml built from the HMN example above.\n--- # Additional application node prefixes to match in the hmn_connections.json file prefixes: - gateway - login - lnet - vn # Additional HSM SubRoles prefix_hsm_subroles: login: UAN lnet: LNETRouter gateway: Gateway vn: Visualization # Application Node aliases aliases: x3113c0s29b0n0: [\u0026#34;gateway01\u0026#34;] x3113c0s28b0n0: [\u0026#34;login02\u0026#34;] x3113c0s27b0n0: [\u0026#34;lnet01\u0026#34;] x3113c0s25b0n0: [\u0026#34;visualization01\u0026#34;, \u0026#34;vn02\u0026#34;] x3113c0s23b0n0: [\u0026#34;uan01\u0026#34;] "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/kafka_upgrade_failure/",
	"title": "Kafka Failure after CSM 1.2 Upgrade",
	"tags": [],
	"description": "",
	"content": "Kafka Failure after CSM 1.2 Upgrade Occasionally the cray-shared-kafka-kafka pods will be restarted before the cray-shared-kafka-zookeeper pods are ready. If this happens then the shared kafka cluster will start to fail.\nError Messages cray-shared-kafka-kafka-# pods 2022-05-20 19:43:02,242 INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn) [main-SendThread(localhost:2181)] 2022-05-20 19:43:02,245 WARN Session 0x1001ec4c0730001 for server localhost/127.0.0.1:2181, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn) [main-SendThread(localhost:2181)] java.io.IOException: Connection reset by peer cray-shared-kafka-zookeeper-# pods 2022.05.20 19:44:06 LOG3[1:139846453499648]: SSL_connect: 1408F10B: error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number cray-shared-kafka-zookeeper-# logs: io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: Client requested protocol TLSv1 is not enabled or supported in server context strimzi-cluster-operator pod Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listTopics 2022-05-20 19:43:47 INFO KafkaRoller:292 - Reconciliation #1395(timer) Kafka(services/cray-shared-kafka): Could not roll pod 1, giving up after 10 attempts. Total delay between attempts 127750ms io.strimzi.operator.cluster.operator.resource.KafkaRoller$ForceableProblem: An error while trying to determine rollability Solution Run the kafka-restart.sh script to fix this issue\nncn# /usr/share/doc/csm/upgrade/1.2/scripts/strimzi/kafka-restart.sh "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/identify_ceph_latency_issues/",
	"title": "Identify Ceph Latency Issues",
	"tags": [],
	"description": "",
	"content": "Identify Ceph Latency Issues Examine the output of the ceph -s command to get context for potential issues causing latency.\nTroubleshoot the underlying causes for the ceph -s command reporting slow PGs.\nPrerequisites This procedure requires admin privileges.\nProcedure View the status of Ceph.\nncn-m001# ceph -s Example output:\ncluster: id: 73084634-9534-434f-a28b-1d6f39cf1d3d health: HEALTH_WARN 1 filesystem is degraded 1 MDSs report slow metadata IOs Reduced data availability: 15 pgs inactive, 15 pgs peering 46 slow ops, oldest one blocked for 1395 sec, daemons [osd,2,osd,5,mon,ceph-1,mon,ceph-2,mon,ceph-3] have slow ops. services: mon: 3 daemons, quorum ceph-3,ceph-1,ceph-2 (age 38m) mgr: ceph-2(active, since 30m), standbys: ceph-3, ceph-1 mds: cephfs:1/1 {0=ceph-2=up:replay} 2 up:standby osd: 15 osds: 15 up (since 93s), 15 in (since 23m); 10 remapped pgs rgw: 1 daemon active (ceph-1.rgw0) data: pools: 9 pools, 192 pgs objects: 15.93k objects, 55 GiB usage: 175 GiB used, 27 TiB / 27 TiB avail pgs: 7.812% pgs not active 425/47793 objects misplaced (0.889%) 174 active+clean 8 peering 7 remapped+peering 2 active+remapped+backfill_wait 1 active+remapped+backfilling io: client: 7.0 KiB/s wr, 0 op/s rd, 1 op/s wr recovery: 12 MiB/s, 3 objects/s The output can provide a lot of context to potential issues causing latency. In the example output above, the following troubleshooting information can be observed:\nhealth - Shows latency and what daemons/OSDs are associated with it. mds - MDS is functional, but it is in replay because of the slow ops. osd - All OSDs are up and in. This could be related to a network issue or a single system issue if both OSDs are on the same box. client - Shows the amount of IO/Throughput that clients using Ceph are performing. If health is not set to HEALTH_OK and traffic is passing through, then Ceph is functioning and re-balancing data because of typical hardware/network issues. recovery - Shows recovery traffic as the system ensures all the copies of data are available to ensure data redundancy. Fixes Based on the output from ceph -s (using our example above) we can correlate some information to help determine our bottleneck.\nWhen reporting slow ops for OSDs, then it is good to find out if those OSDs are on the same node or different nodes.\nIf the OSDs are on the same node, then look at networking or other hardware-related issues on that node. If the OSDs are on different nodes, then investigate networking issues. As an initial step, restart the OSDs, if the slow ops go away and do not return, then we can investigate the logs for possible software bugs or memory issues. If the slow ops come right back, then there is an issue with replication between the 2 OSDs which tends to be network-related. When reporting slow ops for MONs, then it is typically an issue with the process.\nThe most common cause here is either an abrupt clock skew or a hung mon/mgr process. The recommended remediation is to do a rolling restart of the Ceph MON and MGR daemons. When reporting slow ops for MDS, then are multiple possible causes.\nIf listed in addition to OSDs, then the root cause for this is typically the OSDs, and the process above should be used, followed by restarting the MDS daemons. If it is only listing MDS, then restart the MDS daemons. If the problem persists, then examine the logs in order to determine the root cause. See Troubleshoot_Ceph_MDS_reporting_slow_requests_and_failure_on_client for additional steps to help identify MDS slow ops "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/change_ncn_image_root_password_and_ssh_keys_on_pit_node/",
	"title": "Set NCN Image Root Password, SSH Keys, and Timezone on PIT Node",
	"tags": [],
	"description": "",
	"content": "Set NCN Image Root Password, SSH Keys, and Timezone on PIT Node NOTE: This procedure is required during initial CSM installs before management nodes are first deployed.\nModify the NCN images by setting the root user password and adding SSH keys for the root user account. If desired, also change the timezone for the NCNs.\nThis page describes this procedure being performed on the PIT node during a first time installation of the CSM software. If wanting to perform this operation after the initial CSM install, then see Set NCN Image Root Password, SSH Keys, and Timezone.\nOverview SSH keys Script-generated keys Administrator-provided keys Password Use PIT node password Enter password and generate hash Timezone Examples Example 1: New keys, copy PIT password, keep UTC Example 2: Provide keys, prompt for password, change timezone Example 3: New keys, copy PIT password, keep UTC, no prompting Cleanup Overview Add SSH keys and the root password to the NCN SquashFS images. Optionally set their timezone, if a timezone other than UTC (the default) is desired. This is all done by running the ncn-image-modification.sh script, which is located in the scripts/operations/node_management directory of the CSM documentation. Set the path to the script:\npit# NCN_MOD_SCRIPT=$(rpm -ql docs-csm | grep ncn-image-modification[.]sh) ; echo \u0026#34;${NCN_MOD_SCRIPT}\u0026#34; This document provides common ways of using the script to accomplish this. However, specific environments may require deviations from these examples. In those cases, it may be helpful to view the complete script usage statement by running it with only the -h argument.\nThe Kubernetes NCN image location is specified with the -k argument to the script, and the storage NCN image location is specified with the -s argument to the script. Both images should be customized with a single call to the script to ensure that they receive matching customizations.\nThe new customized images are created in their original image\u0026rsquo;s directory. They have the same name as the original image, except with the secure- prefix added. The original image is moved into a subdirectory named old, for backup purposes.\nThere are several choices to be made during this process:\nSSH key files can be provided to the script, or the script can generate them itself. The hashed root password can be provided to the script, or the script can prompt for password entry when it is running. To use a non-default timezone, that must be passed into the script. SSH keys Script-generated keys To have the script generate the SSH keys automatically, it must be provided with the ssh-keygen options to use.\nTo view the complete list of supported ssh-keygen options, view the script usage statement by running it with the -h argument. If the -N option is not used to specify the passphrase, then the script will prompt for the passphrase when it generates the keys. Even specifying an empty passphrase will prevent being prompted to enter the passphrase during script execution. See Example 3. Administrator-provided keys To provide SSH keys to the script, specify the directory containing them with the -d argument.\nThe script assumes that public keys in that directory have the .pub file extension. The entire contents of this directory will be copied into the /root/.ssh directory in the images. After copying the directory contents, the script updates the /root/.ssh/authorized_keys file in the images with the new public keys. This is usually the desired behavior, but it can be overridden by specifying the -a argument. In that case, the script will not update the authorized_keys file after copying the directory contents. Password In order for the script to set root passwords in the images, the -p argument must be included when calling it. This is required for initial CSM installs.\nIf the SQUASHFS_ROOT_PW_HASH environment variable is exported, the script will use that as the new root password hash for the images. Otherwise, the script will prompt for the password to be entered during its execution.\nUse PIT node password If wanting to use the same root user password that is being used on the PIT node where this procedure is being run, then the following command can be used to set the SQUASHFS_ROOT_PW_HASH variable.\npit# export SQUASHFS_ROOT_PW_HASH=$(awk -F\u0026#39;:\u0026#39; /^root:/\u0026#39;{print $2}\u0026#39; \u0026lt; /etc/shadow) Enter password and generate hash The following script can be used to manually enter a new password, and then generate its hash.\nThis script uses read -s to prevent the password from being echoed to the screen or saved in the shell history. It unsets the plaintext password variables at the end, so that only the hash is preserved.\npit# read -r -s -p \u0026#34;Enter root password for NCN images: \u0026#34; PW1 ; echo ; if [[ -z ${PW1} ]]; then echo \u0026#34;ERROR: Password cannot be blank\u0026#34; else read -r -s -p \u0026#34;Enter again: \u0026#34; PW2 echo if [[ ${PW1} != ${PW2} ]]; then echo \u0026#34;ERROR: Passwords do not match\u0026#34; else export SQUASHFS_ROOT_PW_HASH=$(echo -n \u0026#34;${PW1}\u0026#34; | openssl passwd -6 -salt $(\u0026lt; /dev/urandom tr -dc ./A-Za-z0-9 | head -c4) --stdin) [[ -n ${SQUASHFS_ROOT_PW_HASH} ]] \u0026amp;\u0026amp; echo \u0026#34;Password hash set and exported\u0026#34; || echo \u0026#34;ERROR: Problem generating hash\u0026#34; fi fi ; unset PW1 PW2 Timezone The default timezone in the NCN images is UTC. This can optionally be changed by passing the -z argument to the script. Valid timezone options can be listed by running timedatectl list-timezones.\nExamples Example 1: New keys, copy PIT password, keep UTC This example has the script generate new SSH keys (prompting the administrator for the SSH key passphrase) and copies the root user password from the PIT node. It does not change the timezone from the UTC default.\npit# export SQUASHFS_ROOT_PW_HASH=$(awk -F\u0026#39;:\u0026#39; /^root:/\u0026#39;{print $2}\u0026#39; \u0026lt; /etc/shadow) pit# $NCN_MOD_SCRIPT -p \\ -t rsa \\ -k \u0026#34;${PITDATA}\u0026#34;/data/k8s/kubernetes-*.squashfs \\ -s \u0026#34;${PITDATA}\u0026#34;/data/ceph/storage-ceph-*.squashfs Example 2: Provide keys, prompt for password, change timezone This example uses existing SSH keys located in the /my/pre-existing/keys directory. The script prompts the administrator for the root user password during execution. It changes the timezone to America/Chicago.\npit# $NCN_MOD_SCRIPT -p \\ -d /my/pre-existing/keys \\ -z America/Chicago \\ -k \u0026#34;${PITDATA}\u0026#34;/data/k8s/kubernetes-*.squashfs \\ -s \u0026#34;${PITDATA}\u0026#34;/data/ceph/storage-ceph-*.squashfs Example 3: New keys, copy PIT password, keep UTC, no prompting This example has the script generate new SSH keys and copies the root user password from the PIT node. It does not change the timezone from the UTC default. It is identical to the first example except that a blank passphrase is provided, so that the script requires no input from the administrator while it is running.\npit# export SQUASHFS_ROOT_PW_HASH=$(awk -F\u0026#39;:\u0026#39; /^root:/\u0026#39;{print $2}\u0026#39; \u0026lt; /etc/shadow) pit# $NCN_MOD_SCRIPT -p \\ -t rsa \\ -N \u0026#34;\u0026#34; \\ -k \u0026#34;${PITDATA}\u0026#34;/data/k8s/kubernetes-*.squashfs \\ -s \u0026#34;${PITDATA}\u0026#34;/data/ceph/storage-ceph-*.squashfs Cleanup Remove backups of NCN images, if desired. These may be removed now, or after verifying that the nodes are able to boot successfully with the new images.\npit# cd \u0026#34;${PITDATA}\u0026#34;/data \u0026amp;\u0026amp; rm -rvf ceph/old k8s/old "
},
{
	"uri": "/docs-csm/en-12/operations/power_management/shut_down_and_power_off_compute_and_user_access_nodes/",
	"title": "Shut Down and Power Off Compute and User Access Nodes",
	"tags": [],
	"description": "",
	"content": "Shut Down and Power Off Compute and User Access Nodes Shut down and power off compute and user access nodes (UANs). This procedure powers off all compute nodes in the context of an entire system shutdown.\nPrerequisites The cray and sat commands must be initialized and authenticated with valid credentials for Keycloak. If these have not been prepared, then see Configure the Cray CLI and refer to the \u0026ldquo;SAT Authentication\u0026rdquo; section of the HPE Cray EX System Admin Toolkit (SAT) product stream documentation (S-8031) for instructions on how to acquire a SAT authentication token.\nProcedure List detailed information about the available boot orchestration service (BOS) session template names.\nIdentify the BOS session template names such as cos-2.0.x, uan-slurm, and choose the appropriate compute and UAN node templates for the shutdown.\nncn-mw# cray bos sessiontemplate list --format toml Example output excerpts:\n[[results]] name = \u0026#34;cos-2.0.x\u0026#34; description = \u0026#34;BOS session template for booting compute nodes, generated by the installation\u0026#34; [...] name = \u0026#34;slurm\u0026#34; description = \u0026#34;BOS session template for booting compute nodes, generated by the installation\u0026#34; [...] name = \u0026#34;uan-slurm\u0026#34; description = \u0026#34;Template for booting UANs with Slurm\u0026#34; To display more information about a session template, for example cos-2.0.x, use the describe option.\nncn-mw# cray bos sessiontemplate describe cos-2.0.x Use sat bootsys shutdown to shut down and power off UANs and compute nodes.\nAttention: Specify the required session templates for COS_SESSION_TEMPLATE and UAN_SESSION_TEMPLATE in the example.\nAn optional --loglevel debug can be used to provide more information as the system shuts down. If used, it must be added after sat but before bootsys.\nncn-mw# sat bootsys shutdown --stage bos-operations \\ --bos-templates COS_SESSION_TEMPLATE,UAN_SESSION_TEMPLATE Example output:\nStarted boot operation on BOS session templates: cos-2.0.x, uan. Waiting up to 600 seconds for sessions to complete. Waiting for BOA k8s job with id boa-a1a697fc-e040-4707-8a44-a6aef9e4d6ea to complete. Session template: uan. To monitor the progress of this job, run the following command in a separate window: \u0026#39;kubectl -n services logs -c boa -f --selector job-name=boa-a1a697fc-e040-4707-8a44-a6aef9e4d6ea\u0026#39; Waiting for BOA k8s job with id boa-79584ffe-104c-4766-b584-06c5a3a60996 to complete. Session template: cos-2.0.0. To monitor the progress of this job, run the following command in a separate window: \u0026#39;kubectl -n services logs -c boa -f --selector job-name=boa-79584ffe-104c-4766-b584-06c5a3a60996\u0026#39; [...] All BOS sessions completed. Use the Job ID strings (for example, the cos-2.0.0 session, boa-79584ffe-104c-4766- b584-06c5a3a60996) from the previous command to monitor the progress of the boot of the compute nodes.\nThe command to run is displayed in the output of the sat bootsys shutdown command.\nncn-mw# kubectl logs -n services -c boa -f \\ --selector job-name=boa-boa-79584ffe-104c-4766-b584-06c5a3a60996 Example output:\n2020-08-21 17:27:02,358 - DEBUG - cray.boa - BOA starting 2020-08-21 17:27:02,358 - DEBUG - cray.boa - Boot Agent Image: created. 2020-08-21 17:27:02,358 - INFO - cray.boa - Boot Session: boa-79584ffe-104c-4766-b584-06c5a3a60996 2020-08-21 17:27:02,371 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; 2020-08-21 17:27:02,373 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; 2020-08-21 17:27:02,395 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; 2020-08-21 17:27:02,437 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; 2020-08-21 17:27:02,519 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; 2020-08-21 17:27:02,681 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; 2020-08-21 17:27:03,003 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; 2020-08-21 17:27:03,645 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; The BOS shutdown session may or may not power off compute nodes depending on the session template being used.\nIn another shell window, use a similar command to monitor the UAN session.\nncn-mw# kubectl -n services logs -c boa -f --selector job-name=boa-a1a697fc-e040-4707-8a44-a6aef9e4d6ea Check the status of UAN and compute nodes to verify they are Off.\nThere may be delay in nodes reaching the Off state in the hardware state manager (HSM).\nncn-mw# sat status Example output:\n+----------------+------+----------+-------+------+---------+------+----------+-------------+----------+ | xname | Type | NID | State | Flag | Enabled | Arch | Class | Role | Net Type | +----------------+------+----------+-------+------+---------+------+----------+-------------+----------+ | x1000c0s0b0n0 | Node | 1001 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s0b0n1 | Node | 1002 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s0b1n0 | Node | 1003 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s0b1n1 | Node | 1004 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s1b0n0 | Node | 1005 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s1b0n1 | Node | 1006 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s1b1n0 | Node | 1007 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s1b1n1 | Node | 1008 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c1s0b0n0 | Node | 1033 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c1s0b0n1 | Node | 1034 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c1s0b1n0 | Node | 1035 | Off | OK | True | X86 | Mountain | Compute | Sling | Next steps Return to System Power Off Procedures and continue with next step.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/swap_a_compute_blade_with_a_different_system/",
	"title": "Swap a Compute Blade with a Different System",
	"tags": [],
	"description": "",
	"content": "Swap a Compute Blade with a Different System Swap an HPE Cray EX liquid-cooled compute blade between two systems.\nThe two systems in this example are: Source system - Cray EX TDS cabinet x9000 with a healthy EX425 blade (Windom dual-injection) in chassis 3, slot 0 Destination system - Cray EX cabinet x1005 with a defective EX425 blade (Windom dual-injection) in chassis 3, slot 0 Substitute the correct component names (xnames) or other parameters in the command examples that follow. All the nodes in the blade must be specified using a comma-separated list. For example, EX425 compute blades include two node cards, each with two logical nodes (4 nodes). Prerequisites The Slingshot fabric must be configured with the desired topology for both blades.\nThe System Layout Service (SLS) must have the desired HSN configuration.\nThe blade that is removed from the source system must be installed in the empty slot left by the blade removed from destination system, and vice-versa.\nCheck the status of the high-speed network (HSN) and record link status before the procedure.\nReview the following command examples.\nThe commands can be used to capture the required values from the HSM ethernetInterfaces table and write the values to a file. The file then can be used to automate subsequent commands in this procedure, for example:\nncn-mw# mkdir blade_swap_scripts; cd blade_swap_scripts ncn-mw# cat blade_query.sh Example output:\n#!/bin/bash BLADE=$1 OUTFILE=$2 BLADE_DOT=$BLADE. cray hsm inventory ethernetInterfaces list --format json | \\ jq -c --arg BLADE \u0026#34;$BLADE_DOT\u0026#34; \\ \u0026#39;map(select(.ComponentID|test($BLADE))) | map(select(.Description == \u0026#34;Node Maintenance Network\u0026#34;)) | .[] | {xname: .ComponentID, ID: .ID,MAC: .MACAddress, IP: .IPAddresses[0].IPAddress,Desc: .Description}\u0026#39; \u0026gt; $OUTFILE ncn-mw# ./blade_query.sh x1000c0s1 x1000c0s1.json ncn-mw# cat x1000c0s1.json Example output:\n{\u0026#34;xname\u0026#34;:\u0026#34;x9000c3s1b0n0\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a6836339\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:63:39\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.10\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} {\u0026#34;xname\u0026#34;:\u0026#34;x9000c3s1b0n1\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a683633a\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:63:3a\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.98\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} {\u0026#34;xname\u0026#34;:\u0026#34;x9000c3s1b1n0\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a68362e2\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:62:e2\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.123\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} {\u0026#34;xname\u0026#34;:\u0026#34;x9000c3s1b1n1\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a68362e3\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:62:e3\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.122\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} To delete an ethernetInterfaces entry using curl:\nncn-mw# for ID in $(cat x9000c3s1.json | jq -r \u0026#39;.ID\u0026#39;); do cray hsm inventory ethernetInterfaces delete $ID; done To insert an ethernetInterfaces entry using curl:\nThis requires an API token. See Retrieve an Authentication Token for more information.\nncn-mw# while read PAYLOAD ; do curl -H \u0026#34;Authorization: Bearer $MY_TOKEN\u0026#34; -L -X POST \u0026#39;https://api-gw-service-nmn.local/apis/smd/hsm/v2/Inventory/EthernetInterfaces\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ --data-raw \u0026#34;$(echo $PAYLOAD | jq -c \u0026#39;{ComponentID: .xname,Description: .Desc,MACAddress: .MAC,IPAddresses: [{IPAddress: .IP}]}\u0026#39;)\u0026#34; sleep 5 done \u0026lt; x9000c3s1.json The blades must have the coolant drained and filled during the swap to minimize cross-contamination of cooling systems.\nReview procedures in HPE Cray EX Coolant Service Procedures H-6199 Review the HPE Cray EX Hand Pump User Guide H-6200 Prepare the source system blade for removal Using the work load manager (WLM), drain running jobs from the affected nodes on the blade. Refer to the vendor documentation for the WLM for more information.\nUse Boot Orchestration Services (BOS) to shut down the affected nodes in the source blade (in this example, x9000c3s0).\nSpecify the appropriate component name (xname) and BOS template for the node type in the following command.\nncn-mw# BOS_TEMPLATE=cos-2.0.30-slurm-healthy-compute ncn-mw# cray bos session create --template-uuid $BOS_TEMPLATE --operation shutdown --limit x9000c3s0b0n0,x9000c3s0b0n1,x9000c3s0b1n0,x9000c3s0b1n1 Source: Disable the Redfish endpoints for the nodes Temporarily disable the Redfish endpoints for each compute node NodeBMC.\nncn-mw# cray hsm inventory redfishEndpoints update --enabled false x9000c3s0b0 ncn-mw# cray hsm inventory redfishEndpoints update --enabled false x9000c3s0b1 Source: Clear out the existing Redfish event subscriptions from the BMCs on the blade Set the environment variable SLOT to the blade\u0026rsquo;s location.\nncn-mw# SLOT=\u0026#34;x9000c3s0\u0026#34; Clear the Redfish event subscriptions.\nncn-mw# for BMC in $(cray hsm inventory redfishEndpoints list --type NodeBMC --format json | jq .RedfishEndpoints[].ID -r | grep $SLOT); do PASSWD=$(cray scsd bmc creds list --targets $BMC --format json | jq .Targets[].Password -r) SUBS=$(curl -sk -u root:$PASSWD https://${BMC}/redfish/v1/EventService/Subscriptions | jq -r \u0026#39;.Members[].\u0026#34;@odata.id\u0026#34;\u0026#39;) for SUB in $SUBS; do echo \u0026#34;Deleting event subscription: https://${BMC}${SUB}\u0026#34; curl -i -sk -u root:$PASSWD -X DELETE https://${BMC}${SUB} done done Each event subscription deleted that was deleted will have output like the following:\nDeleting event subscription: https://x9000c3s2b0/redfish/v1/EventService/Subscriptions/1 HTTP/2 204 access-control-allow-credentials: true access-control-allow-headers: X-Auth-Token access-control-allow-origin: * access-control-expose-headers: X-Auth-Token cache-control: no-cache, must-revalidate content-type: text/html; charset=UTF-8 date: Tue, 19 Jan 2038 03:14:07 GMT odata-version: 4.0 server: Cray Embedded Software Redfish Service Source: Clear the node controller settings Remove the system specific settings from each node controller on the blade.\nThe two commands look similar, but note that the component names (xnames) in the URLs differ slightly, targeting each node controller on the blade.\nncn-mw# curl -k -u root:PASSWORD -X POST -H \\ \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;ResetType\u0026#34;:\u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://x9000c3s0b0/redfish/v1/Managers/BMC/Actions/Manager.Reset ncn-mw# curl -k -u root:PASSWORD -X POST -H \\ \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;ResetType\u0026#34;:\u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://x9000c3s0b1/redfish/v1/Managers/BMC/Actions/Manager.Reset Use Ctrl-C to return to the prompt if command does not return.\nSource: Power off the chassis slot Suspend the hms-discovery cronjob.\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : true }}\u0026#39; Verify that the hms-discovery cronjob has stopped.\nThat is, verify that ACTIVE = 0 and SUSPEND = True.\nncn-mw# kubectl get cronjobs -n services hms-discovery Example output:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * True 0 117s 15d Power off the chassis slot.\nThis examples powers off slot 0, chassis 3, in cabinet 9000.\nncn-mw# cray capmc xname_off create --xnames x9000c3s0 --recursive true Source: Disable the chassis slot Disable the chassis slot.\nDisabling the slot prevents hms-discovery from automatically powering on the slot. This example disables slot 0, chassis 3, in cabinet 9000.\nncn-mw# cray hsm state components enabled update --enabled false x9000c3s0 Source: Record MAC and IP addresses for nodes IMPORTANT: Record the node management network (NMN) MAC and IP addresses for each node in the blade (labeled Node Maintenance Network). To prevent disruption in the data virtualization service (DVS), these addresses must be maintained in the HSM when the blade is swapped and discovered.\nThe hardware management network MAC and IP addresses are assigned algorithmically and must not be deleted from the HSM.\nQuery HSM to determine the ComponentID, MAC addresses, and IP addresses for each node in the blade.\nThe prerequisites show an example of how to gather HSM values and store them to a file.\nncn-mw# cray hsm inventory ethernetInterfaces list --component-id x9000c3s0b0n0 --format json Example output:\n[ { \u0026#34;ID\u0026#34;: \u0026#34;0040a6836339\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:63:39\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-04-09T21:51:04.662063Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x9000c3s0b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;IPAddresses\u0026#34;: [ { \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.10\u0026#34; } ] } ] Record the following values for the blade:\n\u0026#34;ComponentID\u0026#34;: \u0026#34;x9000c3s0b0n0\u0026#34; \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:63:39\u0026#34; \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.10\u0026#34; Repeat the command to record the component IDs, MAC addresses, and IP addresses for the Node Maintenance Network for the other nodes in the blade.\nDelete the NMN MAC and IP addresses of each node in the blade from the HSM.\nDo not delete the MAC and IP addresses for the node BMC.\nncn-mw# cray hsm inventory ethernetInterfaces delete 0040a6836339 Repeat the preceding command for each node in the blade.\nDelete the Redfish endpoints for each node.\nncn-mw# cray hsm inventory redfishEndpoints delete x9000c3s0b0 ncn-mw# cray hsm inventory redfishEndpoints delete x9000c3s0b1 Source: Remove the blade Remove the blade from the source system. Review the Remove a Compute Blade Using the Lift procedure in HPE Cray EX Hardware Replacement Procedures H-6173 for detailed instructions for replacing liquid-cooled blades (HPE Support). Drain the coolant from the blade and fill with fresh coolant to minimize cross-contamination of cooling systems. Review HPE Cray EX Coolant Service Procedures H-6199. If using the hand pump, review procedures in the HPE Cray EX Hand Pump User Guide H-6200 (HPE Support). Install the blade from the source system in a storage rack or leave it on the cart. Prepare the blade in the destination system for removal Use WLM to drain jobs from the affected nodes on the blade.\nUse BOS to shut down the affected nodes in the destination blade (in this example, x1005c3s0).\nncn-mw# BOS_TEMPLATE=cos-2.0.30-slurm-healthy-compute ncn-mw# cray bos session create --template-uuid $BOS_TEMPLATE --operation shutdown \\ --limit x1005c3s0b0n0,x1005c3s0b0n1,x1005c3s0b1n0,x1005c3s0b1n1 Destination: Disable the Redfish endpoints for the nodes When nodes are Off, temporarily disable endpoint discovery service (MEDS) for the compute nodes.\nDisabling chassis slot prevents hms-discovery from attempting to power them back on.\nncn-mw# cray hsm inventory redfishEndpoints update --enabled false x1005c3s0b0 ncn-mw# cray hsm inventory redfishEndpoints update --enabled false x1005c3s0b1 Destination: Clear out the existing Redfish event subscriptions from the BMCs on the blade Set the environment variable SLOT to the blade\u0026rsquo;s location.\nncn-mw# SLOT=\u0026#34;x1005c3s0\u0026#34; Clear the Redfish event subscriptions.\nncn-mw# for BMC in $(cray hsm inventory redfishEndpoints list --type NodeBMC --format json | jq .RedfishEndpoints[].ID -r | grep $SLOT); do PASSWD=$(cray scsd bmc creds list --targets $BMC --format json | jq .Targets[].Password -r) SUBS=$(curl -sk -u root:$PASSWD https://${BMC}/redfish/v1/EventService/Subscriptions | jq -r \u0026#39;.Members[].\u0026#34;@odata.id\u0026#34;\u0026#39;) for SUB in $SUBS; do echo \u0026#34;Deleting event subscription: https://${BMC}${SUB}\u0026#34; curl -i -sk -u root:$PASSWD -X DELETE https://${BMC}${SUB} done done Each event subscription deleted that was deleted will have output like the following:\nDeleting event subscription: https://x9000c3s2b0/redfish/v1/EventService/Subscriptions/1 HTTP/2 204 access-control-allow-credentials: true access-control-allow-headers: X-Auth-Token access-control-allow-origin: * access-control-expose-headers: X-Auth-Token cache-control: no-cache, must-revalidate content-type: text/html; charset=UTF-8 date: Tue, 19 Jan 2038 03:14:07 GMT odata-version: 4.0 server: Cray Embedded Software Redfish Service Destination: Clear the node controller settings Remove system-specific settings from each node controller on the blade.\nThe two commands look similar, but note that the component names (xnames) in the URLs differ slightly, targeting each node controller on the blade.\nncn-mw# curl -k -u root:PASSWORD -X POST -H \u0026#39;Content-Type: application/json\u0026#39; -d \\ \u0026#39;{\u0026#34;ResetType\u0026#34;: \u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://x1005c3s0b0/redfish/v1/Managers/BMC/Actions/Manager.Reset ncn-mw# curl -k -u root:PASSWORD -X POST -H \u0026#39;Content-Type: application/json\u0026#39; -d \\ \u0026#39;{\u0026#34;ResetType\u0026#34;: \u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://x1005c3s0b1/redfish/v1/Managers/BMC/Actions/Manager.Reset Destination: Power off the chassis slot Suspend the hms-discovery cronjob. ncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;suspend\u0026#34;: true }}\u0026#39; Destination: Verify that the hms-discovery cronjob has stopped.\nThat is, verify that ACTIVE = 0 and SUSPEND = True.\nncn-mw# kubectl get cronjobs -n services hms-discovery Example output:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * True 0 128s 15d Destination: Power off the chassis slot.\nThis example powers off slot 0 in chassis 3 of cabinet 1005.\nncn-mw# cray capmc xname_off create --xnames x1005c3s0 --recursive true Destination: Disable the chassis slot Disable the chassis slot.\nThis example disables slot 0, chassis 3, in cabinet 1005.\nncn-mw# cray hsm state components enabled update --enabled false x1005c3s0 Destination: Record the NIC MAC and IP addresses IMPORTANT: Record the component ID, MAC addresses, and IP addresses for each node in the blade in the destination system. To prevent disruption in the data virtualization service (DVS), these addresses must be maintained in the HSM when the replacement blade is swapped and discovered.\nThe hardware management network NIC MAC addresses for liquid-cooled blades are assigned algorithmically and must not be deleted from the HSM.\nQuery HSM to determine the component ID, MAC addresses, and IP addresses associated with nodes in the destination blade.\nThe prerequisites show an example of how to gather HSM values and store them to a file.\nncn-mw# cray hsm inventory ethernetInterfaces list \\ --component-id XNAME --format json Example output:\n[ { \u0026#34;ID\u0026#34;: \u0026#34;0040a6836399\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:63:99\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-04-09T21:51:04.662063Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1005c3s0b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;IPAddresses\u0026#34;: [ { \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.123\u0026#34; } ] } ] Record the following Node Maintenance Network values for each node in the blade.\n\u0026#34;ComponentID\u0026#34;: \u0026#34;x1005c3s0b0n0\u0026#34; \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:63:99\u0026#34; \u0026#34;IPAddress\u0026#34;: \u0026#34;10.10.0.123\u0026#34; Delete the node NIC MAC and IP addresses from the HSM ethernetInterfaces table for each node.\nncn-mw# cray hsm inventory ethernetInterfaces delete 0040a6836399 Repeat the preceding command for each node in the blade.\nDelete the Redfish endpoints for each node.\nncn-mw# cray hsm inventory redfishEndpoints delete x1005c3s0b0 ncn-mw# cray hsm inventory redfishEndpoints delete x1005c3s0b1 Destination: Swap the blade hardware Remove the blade from destination system install it in a storage cart.\nInstall the blade from the source system into the destination system.\nBring up the blade in the destination system Obtain an authentication token to access the API gateway.\nIn the example below, replace myuser, mypass, and shasta in the curl command with site-specific values. Note the value of access_token. Review Retrieve an Authentication Token for more information. The example is a script to secure a token and set it to the variable MY_TOKEN.\nncn-mw# MY_TOKEN=$(curl -s -d grant_type=password -d client_id=shasta -d \\ username=USERNAME -d password=PASSWORD \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) ncn-mw# echo $MY_TOKEN Stop Kea from automatically adding MAC address entries to the HSM ethernetInterfaces table.\nKea automatically adds entries to the HSM ethernetInterfaces table when a DHCP lease is provided (about every 5 minutes). To prevent Kea from automatically adding these entries, perform the following steps:\nCreate an eth_interfaces file that contains the interface IDs for the Node Maintenance Network entries for the destination system.\nWhen repeating this procedure for the source system, use the interface IDs for the source system.\nncn-mw# cat eth_interfaces Example output:\n0040a6836339 0040a683633a 0040a68362e2 0040a68362e3 Run the following commands in succession to remove the interfaces.\nDeleting the cray-dhcp-kea pod prevents the interfaces from being re-created.\nncn-mw# kubectl delete -n services pod $(kubectl get pods -n services | grep cray-dhcp-kea- | awk \u0026#39;{ print $2 }\u0026#39;) ncn-mw# for ETH in $(cat eth_interfaces); do cray hsm inventory ethernetInterfaces delete $ETH --format json ; done Add the MAC address, IP address, and Node Maintenance Network description to the interfaces.\nThe component ID and IP address must be the values recorded from the destination blade, and the MAC address must be the value recorded from the source blade.\n\u0026#34;ComponentID\u0026#34;: \u0026#34;x1005c3s0b0n0\u0026#34; \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:63:99\u0026#34; \u0026#34;IPAddress\u0026#34;: \u0026#34;10.10.0.123\u0026#34; ncn-mw# MAC=SOURCESYS_MAC_ADDRESS ncn-mw# IP_ADDRESS=DESTSYS_IP_ADDRESS ncn-mw# XNAME=DESTSYS_XNAME ncn-mw# curl -H \u0026#34;Authorization: Bearer ${MY_TOKEN}\u0026#34; -L -X POST \u0026#39;https://api-gw-service-nmn.local/apis/smd/hsm/v1/Inventory/EthernetInterfaces\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; --data-raw \u0026#34;{ \\\u0026#34;Description\\\u0026#34;: \\\u0026#34;Node Maintenance Network\\\u0026#34;, \\\u0026#34;MACAddress\\\u0026#34;: \\\u0026#34;$MAC\\\u0026#34;, \\\u0026#34;IPAddress\\\u0026#34;: \\\u0026#34;$IP_ADDRESS\\\u0026#34;, \\\u0026#34;ComponentID\\\u0026#34;: \\\u0026#34;$XNAME\\\u0026#34; }\u0026#34; When repeating this procedure for the source system, the component ID and IP address must be the values recorded from the source system, and the MAC address must be the value recorded from the blade in the destination system.\nncn-mw# MAC=DESTSYS_MAC_ADDRESS ncn-mw# IP_ADDRESS=SOURCESYS_IP_ADDRESS ncn-mw# XNAME=SOURCESYS_XNAME To change or correct a curl command that has been entered, use a PATCH request. For example:\nncn-mw# curl -k -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; -L -X PATCH \\ \u0026#39;https://api-gw-service-nmn.local/apis/smd/hsm/v1/Inventory/EthernetInterfaces/0040a68350a4\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; \\ --data-raw \u0026#39;{\u0026#34;MACAddress\u0026#34;:\u0026#34;xx:xx:xx:xx:xx:xx\u0026#34;,\u0026#34;IPAddress\u0026#34;:\u0026#34;10.xxx.xxx.xxx\u0026#34;,\u0026#34;ComponentID\u0026#34;:\u0026#34;XNAME\u0026#34;}\u0026#39; Restart Kea.\nncn-mw# kubectl delete pods -n services -l app.kubernetes.io/name=cray-dhcp-kea Repeat the preceding step for each node in the blade.\nEnable and power on the chassis slot Enable the chassis slot.\nThe example enables slot 0, chassis 3, in cabinet 1005.\nncn-mw# cray hsm state components enabled update --enabled true x1005c3s0 Power on the chassis slot.\nThe example powers on slot 0, chassis 3, in cabinet 1005.\nncn-mw# cray capmc xname_on create --xnames x1005c3s0 --recursive true Enable discovery Verify that the hms-discovery cronjob is not suspended in Kubernetes.\nThat is, verify that ACTIVE = 1 and SUSPEND = False.\nncn-mw# kubectl -n services patch cronjobs hms-discovery \\ -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : false }}\u0026#39; ncn-mw# kubectl get cronjobs.batch -n services hms-discovery Example output:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * False 1 41s 33d ncn-mw# kubectl get pods -Ao wide | grep hms-discovery ncn-mw# kubectl -n services logs hms-discovery-1600117560-5w95d \\ hms-discovery | grep \u0026#34;Mountain discovery finished\u0026#34; | jq \u0026#39;.discoveredXnames\u0026#39; Example output:\n[ \u0026#34;x1005c3s0b0\u0026#34; ] Wait for three minutes for the blade to power on and the node controllers (BMCs) to be discovered.\nVerify that discovery has completed To verify that the BMCs have been discovered by the HSM, run this command for each BMC in the blade.\nncn-mw# cray hsm inventory redfishEndpoints describe XNAME --format json Example output:\n{ \u0026#34;ID\u0026#34;: \u0026#34;x1005c3s0b0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;NodeBMC\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;x1005c3s0b0\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x1005c3s0b0\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MACAddr\u0026#34;: \u0026#34;02:03:E8:00:31:00\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2021-06-10T18:01:59.920850Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;1.2.0\u0026#34; } } When LastDiscoveryStatus displays as DiscoverOK, the node BMC has been successfully discovered. If the last discovery state is DiscoveryStarted, then the BMC is currently being inventoried by HSM. If the last discovery state is HTTPsGetFailed or ChildVerificationFailed, then an error has occurred during the discovery process. Optional: Force rediscovery of the components in the chassis.\nThis example shows cabinet 1005, chassis 3.\nncn-mw# cray hsm inventory discover create --xnames x1005c3b0 Optional: Verify that discovery has completed.\nThat is, verify that LastDiscoveryStatus = DiscoverOK.\nncn-mw# cray hsm inventory redfishEndpoints describe x1005c3 --format json Example output:\n{ \u0026#34;ID\u0026#34;: \u0026#34;x1005c3\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;ChassisBMC\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;x1005c3\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x1005c3\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MACAddr\u0026#34;: \u0026#34;02:03:ED:03:00:00\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2020-09-03T19:03:47.989621Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;1.2.0\u0026#34; } } Enable the nodes in the HSM database.\nncn-mw# cray hsm state components bulkEnabled update --enabled true \\ --component-ids x1005c3s0b0n0,x1005c3s0b0n1,x1005c3s0b1n0,x1005c3s0b1n1 Verify that the nodes are enabled in the HSM.\nncn-mw# cray hsm state components query create \\ --component-ids x1005c3s0b0n0,x1005c3s0b0n1,x1005c3s0b1n0,x1005c3s0b1n1 --format toml Partial example output:\n[[Components]] ID = x1005c3s0b0n0 Type = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;Off\u0026#34; [[Components]] ID = x1005c3s0b1n1 Type = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;Off\u0026#34; Power on and boot the nodes Use boot orchestration to power on and boot the nodes.\nSpecify the appropriate BOS template for the node type.\nncn-mw# BOS_TEMPLATE=cos-2.0.30-slurm-healthy-compute ncn-mw# cray bos session create --template-uuid $BOS_TEMPLATE \\ --operation reboot --limit x1005c3s0b0n0,x1005c3s0b0n1,x1005c3s0b1n0,x1005c3s0b1n1 Check firmware Verify that the correct firmware versions for node BIOS, node controller (nC), NIC mezzanine card (NMC), GPUs, and so on.\nIf necessary, update the firmware.\nReview the FAS Admin Procedures and Update Firmware with FAS procedure.\nncn-mw# cray fas actions create CUSTOM_DEVICE_PARAMETERS.json Check DVS There should be a cray-cps pod (the broker), three cray-cps-etcd pods and their waiter, and at least one cray-cps-cm-pm pod. Usually there are two cray-cps-cm-pm pods: one on ncn-w002, and one on ncn-w003 or another worker node.\nVerify that the cray-cps pods on worker nodes are Running.\nncn-mw# kubectl get pods -Ao wide | grep cps Example output:\nservices cray-cps-75cffc4b94-j9qzf 2/2 Running 0 42h 10.40.0.57 ncn-w001 services cray-cps-cm-pm-g6tjx 5/5 Running 21 41h 10.42.0.77 ncn-w003 services cray-cps-cm-pm-kss5k 5/5 Running 21 41h 10.39.0.80 ncn-w002 services cray-cps-etcd-knt45b8sjf 1/1 Running 0 42h 10.42.0.67 ncn-w003 services cray-cps-etcd-n76pmpbl5h 1/1 Running 0 42h 10.39.0.49 ncn-w002 services cray-cps-etcd-qwdn74rxmp 1/1 Running 0 42h 10.40.0.42 ncn-w001 services cray-cps-wait-for-etcd-jb95m 0/1 Completed SSH to each worker node running CPS/DVS, and run dmesg -T to ensure that there are no recurring \u0026quot;DVS: merge_one\u0026quot; error messages as shown.\nThe error messages indicate that DVS is detecting an IP address change for one of the client nodes.\nncn-w# dmesg -T | grep \u0026#34;DVS: merge_one\u0026#34; [Tue Jul 21 13:09:54 2020] DVS: merge_one#351: New node map entry does not match the existing entry [Tue Jul 21 13:09:54 2020] DVS: merge_one#353: nid: 8 -\u0026gt; 8 [Tue Jul 21 13:09:54 2020] DVS: merge_one#355: name: \u0026#39;x3000c0s19b1n0\u0026#39; -\u0026gt; \u0026#39;x3000c0s19b1n0\u0026#39; [Tue Jul 21 13:09:54 2020] DVS: merge_one#357: address: \u0026#39;10.252.0.26@tcp99\u0026#39; -\u0026gt; \u0026#39;10.252.0.33@tcp99\u0026#39; [Tue Jul 21 13:09:54 2020] DVS: merge_one#358: Ignoring. Make sure the Configuration Framework Service (CFS) finished successfully. Review HPE Cray EX DVS Administration Guide 1.4.1 S-8004.\nSSH to the node and check each DVS mount.\nnid# mount | grep dvs | head -1 Example output:\n/var/lib/cps-local/0dbb42538e05485de6f433a28c19e200 on /var/opt/cray/gpu/nvidia-squashfs-21.3 type dvs (ro,relatime,blksize=524288,statsfile=/sys/kernel/debug/dvs/mounts/1/stats,attrcache_timeout=14400,cache,nodatasync,noclosesync,retry,failover,userenv,noclusterfs,killprocess,noatomic,nodeferopens,no_distribute_create_ops,no_ro_cache,loadbalance,maxnodes=1,nnodes=6,nomagic,hash_on_nid,hash=modulo,nodefile=/sys/kernel/debug/dvs/mounts/1/nodenames,nodename=x3000c0s6b0n0:x3000c0s5b0n0:x3000c0s4b0n0:x3000c0s9b0n0:x3000c0s8b0n0:x3000c0s7b0n0) nid# ls /var/opt/cray/gpu/nvidia-squashfs-21.3 Example output:\nrootfs Check the HSN for the affected nodes Determine the pod name for the Slingshot fabric manager pod and check the status of the fabric.\nncn-mw# kubectl exec -it -n services \\ $(kubectl get pods --all-namespaces |grep slingshot | awk \u0026#39;{print $2}\u0026#39;) \\ -- fmn_status Check DNS Check for duplicate IP address entries in the State Management Database (SMD).\nDuplicate entries will cause DNS operations to fail.\nncn-mw# ssh uan01 Example output:\nssh: Could not resolve hostname uan01: Temporary failure in name resolution ncn-mw# ssh x3000c0s14b0n0 Example output:\nssh: Could not resolve hostname x3000c0s14b0n0: Temporary failure in name resolution ncn-mw# ssh x1000c1s1b0n1 Example output:\nssh: Could not resolve hostname x1000c1s1b0n1: Temporary failure in name resolution The Kea configuration error will display a message similar to the message below. This message indicates a duplicate IP address (10.100.0.105) in the SMD:\nConfig reload failed [{\u0026#39;result\u0026#39;: 1, \u0026#39;text\u0026#39;: \u0026#34;Config reload failed: configuration error using file \u0026#39;/usr/local/kea/cray-dhcp-kea-dhcp4.conf\u0026#39;: failed to add new host using the HW address \u0026#39;00:40:a6:83:50:a4 and DUID \u0026#39;(null)\u0026#39; to the IPv4 subnet id \u0026#39;0\u0026#39; for the address 10.100.0.105: There\u0026#39;s already a reservation for this address\u0026#34;}] Check for active DHCP leases.\nIf there are no DHCP leases, then there is a configuration error.\nThis requires an API token. See Retrieve an Authentication Token for more information.\nncn-mw# curl -H \u0026#34;Authorization: Bearer ${MY_TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api-gw-service-nmn.local/apis/dhcp-kea | jq Example output in the case of a configuration error:\n[ { \u0026#34;arguments\u0026#34;: { \u0026#34;leases\u0026#34;: [] }, \u0026#34;result\u0026#34;: 3, \u0026#34;text\u0026#34;: \u0026#34;0 IPv4 lease(s) found.\u0026#34; } ] Delete any duplicate entries.\nIf there are duplicate entries in the SMD as a result of the swap procedure (10.100.0.105 in this example), then delete the duplicate entry.\nShow the EthernetInterfaces for the duplicate IP address:\nncn-mw# cray hsm inventory ethernetInterfaces list --ip-address 10.100.0.105 --format json | jq Example output:\n[ { \u0026#34;ID\u0026#34;: \u0026#34;0040a68350a4\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:50:a4\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.105\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-08-24T20:24:23.214023Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1000c7s7b0n1\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;0040a683639a\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:63:9a\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.105\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-08-27T19:15:53.697459Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1000c7s7b0n1\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34; } ] Delete the older entry.\nncn-mw# cray hsm inventory ethernetInterfaces delete 0040a68350a4 Check DNS.\nncn-mw# nslookup 10.252.1.29 Example output:\n1.1.252.10.in-addr.arpa name = uan01. 1.1.252.10.in-addr.arpa name = uan01.local. 1.1.252.10.in-addr.arpa name = x3000c0s14b0n0. 1.1.252.10.in-addr.arpa name = x3000c0s14b0n0.local. 1.1.252.10.in-addr.arpa name = uan01-nmn. 1.1.252.10.in-addr.arpa name = uan01-nmn.local. ncn-mw# nslookup uan01 Example output:\nServer: 10.92.100.225 Address: 10.92.100.225#53 Name: uan01 Address: 10.252.1.29 ncn-mw# nslookup x3000c0s14b0n0 Example output:\nServer: 10.92.100.225 Address: 10.92.100.225#53 Name: x3000c0s14b0n0 Address: 10.252.1.29 Check SSH.\nncn-mw# ssh x3000c0s14b0n0 Example output:\nThe authenticity of host \u0026#39;x3000c0s14b0n0 (10.252.1.29)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:wttHXF5CaJcQGPTIq4zWp0whx3JTwT/tpx1dJNyyXkA. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;x3000c0s14b0n0\u0026#39; (ECDSA) to the list of known hosts. Last login: Tue Aug 31 10:45:49 2021 from 10.252.1.9 Bring up the blade in the source system To minimize cross-contamination of cooling systems, drain the coolant from the blade removed from destination system and fill with fresh coolant.\nReview the HPE Cray EX Coolant Service Procedures H-6199. If using the hand pump, review procedures in the HPE Cray EX Hand Pump User Guide H-6200 (HPE Support).\nInstall the blade from the destination system into source system.\nReview the Remove a Compute Blade Using the Lift procedure in HPE Cray EX Hardware Replacement Procedures H-6173 for detailed instructions for replacing liquid-cooled blades (HPE Support).\nPower on the nodes in the source system by repeating the steps starting in the Bring up the blade in the destination system section, and going up through the Check DNS section.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_tls_certificates_to_bmcs/",
	"title": "Add TLS Certificates to BMCs",
	"tags": [],
	"description": "",
	"content": "Add TLS Certificates to BMCs Use the System Configuration Service (SCSD) tool to create TLS certificates and store them in Vault secure storage. Once certificates are created, they are placed on to the target BMCs.\nPrerequisites Limitations Generate TLS certificates Regenerate TLS certificates Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Limitations TLS certificates can only be set for liquid-cooled BMCs. TLS certificate support for air-cooled BMCs is not supported in release 1.4.\nGenerate TLS certificates Use SCSD to generate TLS certificates.\nCreate a cert_create.json JSON file containing all cabinet level certificate creation information.\n{ \u0026#34;Domain\u0026#34;: \u0026#34;Cabinet\u0026#34;, \u0026#34;DomainIDs\u0026#34;: [ \u0026#34;x0\u0026#34;, \u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;, \u0026#34;x3\u0026#34;] } Generate the TLS certificates.\nncn-mw# cray scsd bmc createcerts create --format json cert_create.json Example output:\n{ \u0026#34;DomainIDs\u0026#34;: [ { \u0026#34;ID\u0026#34;: \u0026#34;x0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;x1\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;x2\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;x3\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; } ] } Apply the TLS certificates to the target BMCs.\nCreate a new cert_set.json JSON file to specify the endpoints.\n{ \u0026#34;Force\u0026#34;: false, \u0026#34;CertDomain\u0026#34;: \u0026#34;Cabinet\u0026#34;, \u0026#34;Targets\u0026#34;: [ \u0026#34;x0c0s0b0\u0026#34;,\u0026#34;x0c0s1b0\u0026#34;,\u0026#34;x0c0s2b0\u0026#34;, \u0026#34;x0c0s3b0\u0026#34; ] } Set the certificates on the target BMCs.\nncn-mw# cray scsd bmc setcerts create --format json cert_set.json Example output:\n{ \u0026#34;Targets\u0026#34;: [ { \u0026#34;ID\u0026#34;: \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;x0c0s1b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;x0c0s2b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;x0c0s3b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; } ] } Follow the Redeploying a Chart procedure with the following specifications:\nName of chart to be redeployed: cray-hms-smd\nBase name of manifest: sysmgmt\nWhen reaching the step to update customizations, perform the following step:\nOnly follow this step as part of the previously linked chart redeploy procedure.\nEnable the CA_URI variable in all Hardware Management Services (HMS) that use Redfish.\nThe customizations.yaml file needs an entry to specify the URI where the Certificate Authority (CA) bundle can be found.\nncn-mw# vi customizations.yaml Example excerpts from customizations.yaml:\nhms_ca_info: hms_svc_ca_uri: \u0026#34;/usr/local/cray-pki/certificate_authority.crt\u0026#34; cray-hms-reds: hms_ca_uri: \u0026#34;{{ hms_ca_info.hms_svc_ca_uri}}\u0026#34; cray-hms-capmc: hms_ca_uri: \u0026#34;{{ hms_ca_info.hms_svc_ca_uri}}\u0026#34; cray-hms-meds: hms_ca_uri: \u0026#34;{{ hms_ca_info.hms_svc_ca_uri}}\u0026#34; cray-hms-hmcollector: hms_ca_uri: \u0026#34;{{ hms_ca_info.hms_svc_ca_uri}}\u0026#34; cray-hms-smd: hms_ca_uri: \u0026#34;{{ hms_ca_info.hms_svc_ca_uri }}\u0026#34; cray-hms-firmware-action: hms_ca_uri: \u0026#34;{{ hms_ca_info.hms_svc_ca_uri}}\u0026#34; Setting hms_ca_uri to \u0026quot;vault://pki_common/ca_chain\u0026quot; specifies the use of the Vault PKI directly.\nWhen reaching the step to validate that the redeploy was successful, there are no additional steps to perform.\nMake sure to perform the entire linked procedure, including the step to save the updated customizations.\nRegenerate TLS certificates At any point the TLS certs can be regenerated and replaced on Redfish BMCs. The CA trust bundle can also be modified at any time. When this is to be done, the following steps are needed:\nModify the CA trust bundle.\nOnce the CA trust bundle is modified, each service will automatically pick up the new CA bundle data. There is no manual step.\nRegenerate the TLS cabinet-level certificates as done is the preceding step.\nPlace the TLS certificates onto the Redfish BMCs as in the preceding step.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/domain_name/",
	"title": "Domain name",
	"tags": [],
	"description": "",
	"content": "Domain name A domain name is a name to identify the person, group, or organization that controls the devices within an area. An example of a domain name could be us.cray.com\nRelevant Configuration\nCreating a domain name\nswitch(config)# ip map-hostname Show Commands to Validate Functionality\nswitch# show hosts Expected Results\nStep 1: You can configure the domain name Step 2: The output of all show commands is correct Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/mstp/",
	"title": "Configure Multiple Spanning Tree Protocol (MSTP)",
	"tags": [],
	"description": "",
	"content": "Configure Multiple Spanning Tree Protocol (MSTP) MSTP (802.1s) ensures that only one active path exists between any two nodes in a spanning-tree instance. A spanning-tree instance comprises a unique set of VLANs. MSTP instances significantly improve network resource utilization while maintaining a loop-free environment.\nConfiguration commands Enable MSTP (default mode for spanning-tree):\nswitch(config)# spanning-tree mode mst switch(conf-mstp)# name my-mstp-region switch(conf-mstp)# revision 0 Show commands to validate functionality:\nswitch# show spanning-tree mst Expected results Spanning-tree mode is configured Spanning-tree is enabled, if loops are detected ports should go blocked state Spanning-tree splits traffic domain between two DUTs Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/configure_snmp/",
	"title": "Configure SNMP",
	"tags": [],
	"description": "",
	"content": "Configure SNMP SNMP configuration is required for hardware discovery of the HPE Cray EX system.\nThese are examples only; verify SNMP credentials before applying this configuration.\nFor more information on SNMP credentials, see Change SNMP Credentials on Leaf-BMC Switches and Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials.\nDell SNMP sw-leaf-bmc-001# conf t snmp-server group cray-reds-group 3 noauth read cray-reds-view snmp-server user testuser cray-reds-group 3 auth md5 xxxxxxxx priv des xxxxxxx snmp-server view cray-reds-view 1.3.6.1.2 included Aruba SNMP sw-leaf-bmc-001# conf t snmp-server vrf default snmpv3 user testuser auth md5 auth-pass plaintext xxxxxx priv des priv-pass plaintext xxxxx "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/compute_uan_application_nodes/",
	"title": "Verify Computes/UANs/Application Nodes",
	"tags": [],
	"description": "",
	"content": "Verify Computes/UANs/Application Nodes If the computes make it past PXE and go into the PXE shell, verify DNS and connectivity.\niPXE\u0026gt; dhcp Configuring (net0 98:03:9b:a8:60:88).................. No configuration methods succeeded (http://ipxe.org/040ee186) Configuring (net1 b4:2e:99:be:1a:37)...... ok Procedure Verify DNS:\niPXE\u0026gt; show dns Example output:\nnet1.dhcp/dns:ipv4 = 10.92.100.225 Verify connectivity:\niPXE\u0026gt; nslookup address api-gw-service-nmn.local iPXE\u0026gt; echo ${address} 10.92.100.71 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/disaster_recovery_postgres/",
	"title": "Disaster Recovery for Postgres",
	"tags": [],
	"description": "",
	"content": "Disaster Recovery for Postgres In the event that the Postgres cluster has failed to the point that it must be recovered and there is no dump available to restore the data, a full service specific disaster recovery is needed.\nBelow are the service specific steps required to cleanup any existing resources, redeploy the resources, and repopulate the data.\nDisaster recovery procedures by service:\nRestore HSM (Hardware State Manger) Postgres without a Backup Restore SLS (System Layout Service) Postgres without a Backup Restore Spire Postgres without a Backup Restore Keycloak Postgres without a backup Restore console Postgres Restore Keycloak Postgres without a backup The following procedures are required to rebuild the automatically populated contents of Keycloak\u0026rsquo;s PostgreSQL database if the database has been lost and recreated.\nRe-run the keycloak-setup job.\nFetch the current job definition.\nncn-mw# kubectl get job -n services -l app.kubernetes.io/name=cray-keycloak -oyaml |\\ yq r - \u0026#39;items[0]\u0026#39; | yq d - \u0026#39;spec.selector\u0026#39; | \\ yq d - \u0026#39;spec.template.metadata.labels\u0026#39; \u0026gt; keycloak-setup.yaml There should be no output.\nRestart the keycloak-setup job.\nncn-mw# kubectl replace --force -f keycloak-setup.yaml The output should be similar to the following:\njob.batch \u0026#34;keycloak-setup-1\u0026#34; deleted job.batch/keycloak-setup-1 replaced Wait for the job to finish.\nncn-mw# kubectl wait --for=condition=complete -n services job -l app.kubernetes.io/name=cray-keycloak --timeout=-1s The output should be similar to the following:\njob.batch/keycloak-setup-1 condition met Re-run the keycloak-users-localize job.\nFetch the current job definition.\nncn-mw# kubectl get job -n services -l app.kubernetes.io/name=cray-keycloak-users-localize -oyaml |\\ yq r - \u0026#39;items[0]\u0026#39; | yq d - \u0026#39;spec.selector\u0026#39; | \\ yq d - \u0026#39;spec.template.metadata.labels\u0026#39; \u0026gt; keycloak-users-localize.yaml There should be no output.\nRestart the keycloak-users-localize job.\nncn-mw# kubectl replace --force -f keycloak-users-localize.yaml The output should be similar to the following:\njob.batch \u0026#34;keycloak-users-localize-1\u0026#34; deleted job.batch/keycloak-users-localize-1 replaced Wait for the job to finish.\nncn-mw# kubectl wait --for=condition=complete -n services job -l app.kubernetes.io/name=cray-keycloak-users-localize --timeout=-1s The output should be similar to the following:\njob.batch/keycloak-users-localize-1 condition met Restart the ingress oauth2-proxies.\nRestart the deployments.\nncn-mw# kubectl rollout restart -n services deployment/cray-oauth2-proxies-customer-access-ingress \u0026amp;\u0026amp; \\ kubectl rollout restart -n services deployment/cray-oauth2-proxies-customer-high-speed-ingress \u0026amp;\u0026amp; \\ kubectl rollout restart -n services deployment/cray-oauth2-proxies-customer-management-ingress Expected output:\ndeployment.apps/cray-oauth2-proxies-customer-access-ingress restarted deployment.apps/cray-oauth2-proxies-customer-high-speed-ingress restarted deployment.apps/cray-oauth2-proxies-customer-management-ingress restarted Wait for the restart to complete.\nncn-mw# kubectl rollout status -n services deployment/cray-oauth2-proxies-customer-access-ingress \u0026amp;\u0026amp; \\ kubectl rollout status -n services deployment/cray-oauth2-proxies-customer-high-speed-ingress \u0026amp;\u0026amp; \\ kubectl rollout status -n services deployment/cray-oauth2-proxies-customer-management-ingress Expected output:\ndeployment \u0026#34;cray-oauth2-proxies-customer-access-ingress\u0026#34; successfully rolled out deployment \u0026#34;cray-oauth2-proxies-customer-high-speed-ingress\u0026#34; successfully rolled out deployment \u0026#34;cray-oauth2-proxies-customer-management-ingress\u0026#34; successfully rolled out Any other changes made to Keycloak, such as local users that have been created, will have to be manually re-applied.\nRestore console Postgres Many times the PostgreSQL database used for the console services may be restored to health using the techniques described in the following documents:\nTroubleshoot Postgres Database Recover from Postgres WAL Event If the database is not able to be restored to health, follow the directions below to recover. There is nothing in the console services PostgreSQL database that needs to be backed up and restored. Once the database is healthy it will get rebuilt and populated by the console services from the current system. Recovery consists of uninstalling and reinstalling the Helm chart for the cray-console-data service.\nDetermine the version of cray-console-data that is deployed.\nncn-mw# helm history -n services cray-console-data Output similar to the following will be returned:\nREVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Thu Sep 2 19:56:24 2021 deployed cray-console-data-1.0.8 1.0.8 Install complete Note the version of the helm chart that is deployed.\nGet the correct Helm chart package to reinstall.\nCopy the chart from the local Nexus repository into the current directory:\nReplace the version in the following example with the version noted in the previous step.\nncn-mw# wget https://packages.local/repository/charts/cray-console-data-1.0.8.tgz Uninstall the current cray-console-data service.\nncn-mw# helm uninstall -n services cray-console-data Example output:\nrelease \u0026#34;cray-console-data\u0026#34; uninstalled Wait for all resources to be removed.\nWatch the deployed pods terminate.\nWatch the services from the cray-console-data Helm chart as they are terminated and removed:\nncn-mw# watch -n .2 \u0026#39;kubectl -n services get pods | grep cray-console-data\u0026#39; Output similar to the following will be returned:\ncray-console-data-764f9d46b5-vbs7w 2/2 Running 0 4d20h cray-console-data-postgres-0 3/3 Running 0 20d cray-console-data-postgres-1 3/3 Running 0 20d cray-console-data-postgres-2 3/3 Terminating 0 4d20h This may take several minutes to complete. When all of the services have terminated and nothing is displayed any longer, use ctrl-C to exit from the watch command.\nCheck that the data PVC instances have been removed.\nncn-mw# kubectl -n services get pvc | grep console-data-postgres There should be no PVC instances returned by this command. If there are, delete them manually with the following command:\nReplace the name of the PVC in the following example with the PVC to be deleted.\nncn-mw# kubectl -n services delete pvc pgdata-cray-console-data-postgres-0 Repeat until all of the `pgdata-cray-console-data-postgres-\u0026rsquo; instances are removed.\nInstall the Helm chart.\nInstall using the file downloaded previously:\nncn-mw# helm install -n services cray-console-data ./cray-console-data-1.0.8.tgz Example output:\nNAME: cray-console-data LAST DEPLOYED: Mon Oct 25 22:44:49 2021 NAMESPACE: services STATUS: deployed REVISION: 1 TEST SUITE: None Verify that all services restart correctly.\nWatch the services come back up again.\nncn-mw# watch -n .2 \u0026#39;kubectl -n services get pods | grep cray-console-data\u0026#39; After a little time, expected output should look similar to:\ncray-console-data-764f9d46b5-vbs7w 2/2 Running 0 5m cray-console-data-postgres-0 3/3 Running 0 4m cray-console-data-postgres-1 3/3 Running 0 3m cray-console-data-postgres-2 3/3 Running 0 2m It will take a few minutes after these services are back up and running for the console services to settle and rebuild the database.\nQuery cray-console-operator for a node location.\nAfter a few minutes, query cray-console-operator to find the pod a particular node is connected to.\nIn the following example, replace the cray-console-operator pod name with the actual name of the running pod, and replace the component name (xname) with an actual node xname on the system.\nncn-mw# kubectl -n services exec -it cray-console-operator-7fdc797f9f-xz8rt -- sh -c \u0026#39;/app/get-node x9000c3s3b0n1\u0026#39; Example output:\n{\u0026#34;podname\u0026#34;:\u0026#34;cray-console-node-0\u0026#34;} This confirms that the cray-console-data service is up and operational.\n"
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/manage_component_partitions/",
	"title": "Manage Component Partitions",
	"tags": [],
	"description": "",
	"content": "Manage Component Partitions The creation, deletion, and modification of partitions is enabled by the Hardware State Manager (HSM) APIs.\nThe following is an example partition that contains the optional tags field:\n{ \u0026#34;name\u0026#34; : \u0026#34;partition 1\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;partition 1\u0026#34;, \u0026#34;tags\u0026#34; : [ \u0026#34;tag2\u0026#34; ], \u0026#34;members\u0026#34; : { \u0026#34;ids\u0026#34; : [ \u0026#34;x0c0s0b0n0\u0026#34;, \u0026#34;x0c0s0b0n1\u0026#34;, \u0026#34;x0c0s0b1n0\u0026#34; ] }, } Troubleshooting: If the Cray CLI has not been initialized, the CLI commands will not work.\nCreate a New Partition Creating a partition is very similar to creating a group. Members can either be provided in an initial list, or the list can be initially empty and added to later. There is no exclusiveGroups field because partition memberships are always exclusive. The following are two different ways to create a partition.\nCreate a new partition with an empty members list and two optional tags:\nncn-m# cray hsm partitions create --name PARTITION_NAME \\ --tags TAG1,TAG2 \\ --description DESCRIPTION_OF_PARTITION_NAME Create a new partition with a pre-set members list:\nncn-m# cray hsm partitions create --name PARTITION_NAME \\ --description DESCRIPTION OF PARTITION_NAME \\ --members-ids MEMBER_ID,MEMBER_ID,MEMBER_ID,MEMBER_ID Create a new partition:\nncn-m# cray hsm partitions create -v --label PARTITION_LABEL Add a description of the partition:\nncn-m# cray hsm partitions update test_group --description \u0026#34;Description of partition\u0026#34; Add a new component to the partition:\nncn-m# cray hsm partitions members create --id XNAME PARTITION_LABEL Retrieve Partition Information Information about a partition is retrieved with the partition name.\nRetrieve all fields for a partition, including the members list:\nncn-m# cray hsm partitions describe PARTITION_NAME Delete a Partition Once a partition is deleted, the former members will not have a partition assigned to them and are ready to be assigned to a new partition.\nDelete a partition so all members are no longer in it:\nncn-m# cray hsm partitions delete PARTITION_NAME "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/create_a_cfs_session_with_dynamic_inventory/",
	"title": "Create a CFS Session with Dynamic Inventory",
	"tags": [],
	"description": "",
	"content": "Create a CFS Session with Dynamic Inventory A Configuration Framework Service (CFS) session using dynamic inventory is used to configure live nodes. To create a CFS session using the default dynamic inventory, simply provide a session name and the name of the configuration to apply:\nncn# cray cfs sessions create --name example \\ --configuration-name configurations-example Example output:\n{ \u0026#34;ansible\u0026#34;: { \u0026#34;config\u0026#34;: \u0026#34;cfs-default-ansible-cfg\u0026#34;, \u0026#34;limit\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;verbosity\u0026#34;: 0 }, \u0026#34;configuration\u0026#34;: { \u0026#34;limit\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;artifacts\u0026#34;: [], \u0026#34;session\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;succeeded\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;tags\u0026#34;: {}, \u0026#34;target\u0026#34;: { \u0026#34;definition\u0026#34;: \u0026#34;dynamic\u0026#34;, \u0026#34;groups\u0026#34;: null } } Add the --target-definition dynamic parameter to the create command to explicitly define the inventory type to be dynamic. This will enable CFS to provide the Ansible host groups via its dynamic inventory. The individual Ansible playbooks specified in the configuration layers will decide which hosts and/or groups will have configuration applied to them.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/create_a_session_template_to_boot_compute_nodes_with_cps/",
	"title": "Create a Session Template to Boot Compute Nodes with CPS",
	"tags": [],
	"description": "",
	"content": "Create a Session Template to Boot Compute Nodes with CPS When compute nodes are booted, the Content Projection Service (CPS) and Data Virtualization Service (DVS) project the root file system (rootfs) over the network to the compute nodes by default.\nAnother option when compute nodes are booted is to download their rootfs into RAM.\nThis page covers the appropriate contents for a BOS session template in order to use CPS and DVS.\nBoot set S3 parameters Boot set rootfs_provider parameters \u0026lt;transport\u0026gt; \u0026lt;api_gateway\u0026gt; \u0026lt;timeout\u0026gt; \u0026lt;etag\u0026gt; \u0026lt;interface\u0026gt;[,\u0026lt;interface\u0026gt;][,\u0026lt;interface\u0026gt;]... \u0026lt;ramroot\u0026gt; Example rootfs_provider_passthrough root= kernel parameter Example session template input file Creating a BOS session using the new template Boot set S3 parameters The session template boot set contains several Simple Storage Service (S3) parameters. These are listed below, along with the appropriate values to use.\ntype: Set to s3 path: Set to s3://\u0026lt;BUCKET_NAME\u0026gt;/\u0026lt;KEY_NAME\u0026gt; etag: Set to \u0026lt;etag\\\u0026gt; Boot set rootfs_provider parameters The Content Projection Service (CPS) is an optional provider for rootfs on compute nodes.\nThe rootfs_provider_passthrough boot set parameter is customized according to the following format:\nrootfs_provider_passthrough=\u0026lt;transport\u0026gt;:\u0026lt;api_gateway\u0026gt;:\u0026lt;timeout\u0026gt;:\u0026lt;interface\u0026gt;[,\u0026lt;interface\u0026gt;[,\u0026lt;interface\u0026gt;]...]:\u0026lt;ramroot\u0026gt; The following values need to be set in the boot set of the session template in order to make CPS the rootfs provider:\n\u0026quot;rootfs_provider\u0026quot;: Set to \u0026quot;cpss3\u0026quot; \u0026quot;rootfs_provider_passthrough\u0026quot;: Set to \u0026quot;dvs:api-gw-service-nmn.local:300:eth0\u0026quot; The variables used in this parameter represent the following:\n\u0026lt;transport\u0026gt; File system network transport. For example, nfs or dvs.\nCan be left as an empty string to use the default value dvs.\n\u0026lt;api_gateway\u0026gt; Name or address of the Kubernetes API gateway.\nCan be left as an empty string to use the default value api-gw-service-nmn.local.\n\u0026lt;timeout\u0026gt; The timeout, in seconds, for attempting to mount the netroot via CPS.\nCan be left as an empty string to use the default value of 300 seconds.\n\u0026lt;etag\u0026gt; Lists the syntax in use. BOS fills in the S3 path and etag values, so the user does not need to fill in any data.\n\u0026lt;interface\u0026gt;[,\u0026lt;interface\u0026gt;][,\u0026lt;interface\u0026gt;]... A comma-separated list of interfaces to support. A minimum of one interface must be specified.\nThe first interface specified must exist on the node or the module will exit with an error. Any other specified interface that is not found on the node will be ignored. The module will wait until all specified and existing interfaces are up before proceeding with boot. The first interface specified will be passed to the CPS mount command to identify the interface to be used for mounting.\n\u0026lt;ramroot\u0026gt; Indicates that the specified S3 path should be copied to RAM (tmpfs) and mounted locally instead of persisting as a remote file system mount.\nCan be left empty. Any string except \u0026quot;0\u0026quot; is interpreted as true.\nExample rootfs_provider_passthrough rootfs_provider_passthrough=dvs:api-gw-service-nmn.local:300:eth0 root= kernel parameter BOS will construct the root= kernel parameter, which will be used by the node when it boots, based on the rootfs_provider and rootfs_provider_passthrough values.\nFor CPS, BOS supplies a protocol craycps-s3, the S3 path to the rootfs, and the etag value (if it exists). The rest of the parameters are supplied from the rootfs_provider_passthrough values as specified above.\nBOS will construct it in the following format:\nroot=craycps-s3:s3-path:\u0026lt;etag\u0026gt;:\u0026lt;transport\u0026gt;:\u0026lt;api_gateway\u0026gt;:\u0026lt;timeout\u0026gt;:interface[,\u0026lt;interface\u0026gt;[,\u0026lt;interface\u0026gt;]...]:\u0026lt;ramroot\u0026gt; Example session template input file The following is an example of an input file to use with the Cray CLI:\n{ \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;description\u0026#34;: \u0026#34;Template for booting compute nodes, generated by the installation\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;computes\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gw-service-nmn.local quiet turbo_boost_limit=999\u0026#34;, \u0026#34;node_roles_groups\u0026#34;: [ \u0026#34;Compute\u0026#34; ], \u0026#34;etag\u0026#34;: \u0026#34;b0ace28163302e18b68cf04dd64f2e01\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/ef97d3c4-6f10-4d58-b4aa-7b70fcaf41ba/manifest.json\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;cfs_branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34; } Refer to Manage a Session Template for more information about creating a session template.\nOr use a bash script to setup a session template using the BOS API.\n#!/bin/bash function get_token () { ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d) curl -s -d grant_type=client_credentials -d client_id=admin-client -d client_secret=$ADMIN_SECRET \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | python3 -c \u0026#39;import sys, json; print(json.load(sys.stdin)[\u0026#34;access_token\u0026#34;])\u0026#39; } body=\u0026#39; { \u0026#34;name\u0026#34;: \u0026#34;st1\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;boot_set1\u0026#34;: { \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34;, \u0026#34;etag\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/ef97d3c4-6f10-4d58-b4aa-7b70fcaf41ba/manifest.json\u0026#34;, \u0026#34;node_roles_groups\u0026#34;: [\u0026#34;Compute\u0026#34;], \u0026#34;node_list\u0026#34;: [\u0026#34;\u0026#34;], \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cps\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gw-service-nmn.local quiet turbo_boost_limit=999\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34; } }, \u0026#34;cfs_branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/config-management.git\u0026#34;, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;partition\u0026#34;: \u0026#34;\u0026#34; }\u0026#39; curl -i -X POST -s https://api-gw-service-nmn.local/apis/bos/v1/sessiontemplate \\ -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;$body\u0026#34; Creating a BOS session using the new template The new CPS-based session template can be used when creating a BOS session. The following is an example of creating a reboot session using the CLI:\nncn-mw# cray bos session create --template-uuid cps_rootfs_template --operation reboot "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/create_and_use_default_uais_in_legacy_mode/",
	"title": "Create and Use Default UAIs in Legacy Mode",
	"tags": [],
	"description": "",
	"content": "Create and Use Default UAIs in Legacy Mode Create a UAI using the default UAI image or the default UAI class in legacy mode.\nProcedure Create a UAI with a command of the following form:\nuser\u0026gt; cray uas create --public-key \u0026#39;\u0026lt;path\u0026gt;\u0026#39; \u0026lt;path\u0026gt; is the path to a file containing an SSH public-key matched to the SSH private key belonging to the user.\nWatch the UAI and see when it is ready for logins.\nuser\u0026gt; cray uas list Log into the UAI using the ssh command.\nDelete the UAI when finished working with it.\nuser\u0026gt; cray uas delete --uai-list \u0026#39;\u0026lt;uai-list\u0026gt;\u0026#39; Example UAI Lifecycle In the following example, the user logs into the CLI using cray auth login with a user name and password matching that user\u0026rsquo;s credentials in Keycloak.\nvers\u0026gt; cray auth login Username: vers Password: Success! vers\u0026gt; cray uas list results = [] From there the user creates a UAI. The UAI starts out in a Pending or Waiting state as Kubernetes constructs its pod and starts its container running.\nvers\u0026gt; cray uas create --publickey ~/.ssh/id_rsa.pub uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@34.136.140.107\u0026#34; uai_host = \u0026#34;ncn-w002\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34; uai_ip = \u0026#34;34.136.140.107\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-01b26dd1\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; [uai_portmap] vers\u0026gt; cray uas list cray uas list [[results]] uai_age = \u0026#34;1m\u0026#34; uai_connect_string = \u0026#34;ssh vers@34.136.140.107\u0026#34; uai_host = \u0026#34;ncn-w002\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34; uai_ip = \u0026#34;34.136.140.107\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-01b26dd1\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; Using cray uas list, the user watches the UAI until it reaches a Running: Ready state. The UAI is now ready to accept SSH logins from the user, and the user then logs into the UAI to run a simple Slurm job, and logs out.\nvers\u0026gt; ssh vers@34.136.140.107 The authenticity of host \u0026#39;34.136.140.107 (34.136.140.107)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:5gU4SPiw8UvcX7s+xJfVMKULaUi3e0E3i+XA6AklEJA. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;34.136.140.107\u0026#39; (ECDSA) to the list of known hosts. vers@uai-vers-01b26dd1-45tpc:~\u0026gt; ps -afe UID PID PPID C STIME TTY TIME CMD root 1 0 0 14:50 ? 00:00:00 /bin/bash /usr/bin/uai-ssh.sh root 45 1 0 14:50 ? 00:00:00 su vers -c /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 46 45 0 14:50 ? 00:00:00 /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 107 46 0 14:53 ? 00:00:00 sshd: vers [priv] vers 110 107 0 14:53 ? 00:00:00 sshd: vers@pts/0 vers 111 110 0 14:53 pts/0 00:00:00 -bash vers 148 111 0 14:53 pts/0 00:00:00 ps -afe vers@uai-vers-01b26dd1-45tpc:~\u0026gt; exit logout Connection to 34.136.140.107 closed. Now finished with the UAI, the user deletes it with cray uas delete. If the user has more than one UAI to delete, the argument to the --uai-list option can be a comma-separated list of UAI names.\nvers\u0026gt; cray uas delete --uai-list uai-vers-01b26dd1 results = [ \u0026#34;Successfully deleted uai-vers-01b26dd1\u0026#34;,] Top: User Access Service (UAS)\nNext Topic: List Available UAI Images in Legacy Mode\n"
},
{
	"uri": "/docs-csm/en-12/install/create_cabinets_yaml/",
	"title": "Create Cabinets YAML",
	"tags": [],
	"description": "",
	"content": "Create Cabinets YAML This page provides directions on constructing the optional cabinets.yaml file. This file lists cabinet IDs for any systems with non-contiguous cabinet ID numbers and controls how the csi config init command treats cabinet IDs.\nThe following example file is manually created and follows this format. Each \u0026ldquo;type\u0026rdquo; of cabinet can have several fields: total_number of cabinets of this type, starting_id for this cabinet type, and a list of the IDs.\n--- cabinets: - type: hill total_number: 2 starting_id: 9000 - type: mountain total_number: 4 starting_id: 1000 cabinets: - id: 1000 nmn-vlan: 2000 hmn-vlan: 3000 - id: 1001 nmn-vlan: 2001 hmn-vlan: 3001 - id: 1002 - id: 1003 - type: river total_number: 4 starting_id: 3000 In this example file, there are two Hill cabinets that will be automatically numbered as 9000 and 9001. The Mountain cabinets appear in three groupings of four IDs. The River cabinets are non-contiguous in four separated IDs.\nA system with Hill cabinets can have one to four cabinet IDs. There is no limit on the number of Mountain or River cabinets.\nWhen the above cabinets.yaml file is used, csi will ignore any command-line argument to csi config init for starting-mountain-cabinet, starting-river-cabinet, starting-hill-cabinet, mountain-cabinets, river-cabinets, or hill-cabinets.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/kubernetes_node_rootfs_out_of_space/",
	"title": "Kubernetes Master or Worker node&#39;s root filesystem is out of space",
	"tags": [],
	"description": "",
	"content": "Kubernetes Master or Worker node\u0026rsquo;s root filesystem is out of space Description There is a known bug in Kubernetes 1.19.9 where movement of a pod with an attached volume may not complete in time and cause the kubelet service to stream error messages to the /var/log/messages log file. If this goes unchecked, it will fill up the root file system.\nFix Log into the node that has space issues.\nVerify that you have a large messages file in /var/log/.\nncn-m/w:/var/log # ls -lh messages-20211212 -rw-r----- 1 root root 67G Dec 13 12:24 messages-20211212 Remove the file.\nRestart kubelet to address the streaming log entries.\nncn-m/w# systemctl restart kubelet.service Restart the syslog service.\nncn-m/w# systemctl restart rsyslog Verify that the space issue is resolved\nncn-m/w# df -h / Filesystem Size Used Avail Use% Mounted on LiveOS_rootfs 280G 933M 279G 1% / "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/manage_ceph_services/",
	"title": "Manage Ceph Services",
	"tags": [],
	"description": "",
	"content": "Manage Ceph Services The following commands are required to start, stop, or restart Ceph services. Restarting Ceph services is helpful for troubleshoot issues with the utility storage platform.\nList Ceph Services ncn-s00(1/2/3)# ceph orch ps Example output:\nNAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID mds.cephfs.ncn-s001.zwptsg ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c bb08bcb2f034 mds.cephfs.ncn-s002.qyvoyv ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 32c3ff10be42 mds.cephfs.ncn-s003.vvsuvy ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c e172b979c747 mgr.ncn-s001 ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c ad887936d37f mgr.ncn-s002 ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 80902ae9010c mgr.ncn-s003 ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 28700bb4053e mon.ncn-s001 ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c af8d64f9df8a mon.ncn-s002 ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 698557732bf4 mon.ncn-s003 ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 27421ddd81bd osd.0 ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 97f4f922edc7 osd.1 ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 69d11a7ddecb osd.10 ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c aa054d15d4ab osd.11 ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c b3814f3348ed osd.2 ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 998c334e41c6 osd.3 ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c da2daa780fd0 osd.4 ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c b831003fdf32 osd.5 ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 6850b49c9bc1 osd.6 ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 6aeb8b274212 osd.7 ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 4eb2f577daba osd.8 ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c f0386c093874 osd.9 ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 50d7066f66a6 rgw.site1.zone1.ncn-s001.xtjggh ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 6a90ba6415e6 rgw.site1.zone1.ncn-s002.divvfs ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 825c5b5f33c5 rgw.site1.zone1.ncn-s003.spojqa ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c f95116a16e41 Ceph Monitor Service (ceph-mon) IMPORTANT: All of the below ceph orch commands should be run from ncn-s001/2/3 or ncn-m001/2/3.\nStart the ceph-mon service:\nNOTE: The mon process can have a container ID appended to the end of the host name. Please use the output from above to ensure the correct name is used.\nncn-s00(1/2/3)# ceph orch daemon start mon.\u0026lt;hostname\u0026gt; Stop the ceph-mon service:\nncn-s00(1/2/3)# ceph orch daemon stop mon.\u0026lt;hostname\u0026gt; Restart the ceph-mon service:\nncn-s00(1/2/3)# ceph orch daemon restart mon.\u0026lt;hostname\u0026gt; Ceph OSD Service (ceph-osd) Start the ceph-osd service:\nncn-s00(1/2/3)# ceph orch daemon start osd.\u0026lt;number\u0026gt; Stop the ceph-osd service:\nnncn-s00(1/2/3)# ceph orch daemon stop osd.\u0026lt;number\u0026gt; Restart the ceph-osd service:\nncn-s00(1/2/3)# ceph orch daemon restart osd.\u0026lt;number\u0026gt; Ceph Manager Service (ceph-mgr) Start the ceph-mgr service:\nncn-s00(1/2/3)# ceph orch daemon start mgr.\u0026lt;hostname\u0026gt; Stop the ceph-mgr service:\nncn-s00(1/2/3)# ceph orch daemon stop mgr.\u0026lt;hostname\u0026gt; Restart the ceph-mgr service:\nncn-s00(1/2/3)# ceph orch daemon restart mgr.\u0026lt;hostname\u0026gt; Ceph MDS Service (cephfs) Start the ceph-mgr service:\nncn-s00(1/2/3)# ceph orch daemon start mds.cephfs.\u0026lt;container id from ceph orch ps output\u0026gt; Stop the ceph-mgr service:\nncn-s00(1/2/3)# ceph orch daemon stop mds.cephfs.\u0026lt;container id from ceph orch ps output\u0026gt; Restart the ceph-mgr service:\nncn-s00(1/2/3)# ceph orch daemon restart mds.cephfs.\u0026lt;container id from ceph orch ps output\u0026gt; Ceph Rados-Gateway Service (ceph-radosgw) Start the rados-gateway:\nncn-s00(1/2/3)# ceph orch daemon start rgw.site1.zone1.\u0026lt;container id from ceph orch ls\u0026gt; Stop the rados-gateway:\nncn-s00(1/2/3)# ceph orch daemon stop rgw.site1.zone1.\u0026lt;container id from ceph orch ls\u0026gt; Restart the rados-gateway:\nncn-s00(1/2/3)# ceph orch daemon restart rgw.site1.zone1.\u0026lt;container id from ceph orch ls\u0026gt; Ceph Service Restart using CEPHADM IMPORTANT: The following commands need to run from the host where services are being started or stopped.\nGet the service system_unit:\nncn-s# cephadm ls Example output:\n{ \u0026#34;style\u0026#34;: \u0026#34;cephadm:v1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;mgr.ncn-s001\u0026#34;, \u0026#34;fsid\u0026#34;: \u0026#34;01a0d9d2-ea7f-43dc-af25-acdfa5242a48\u0026#34;, \u0026#34;systemd_unit\u0026#34;: \u0026#34;ceph-01a0d9d2-ea7f-43dc-af25-acdfa5242a48@mgr.ncn-s001\u0026#34;, \u0026#34;enabled\u0026#34;: false, \u0026#34;state\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;container_id\u0026#34;: \u0026#34;ad887936d37fd87999b140c366ef288443faf03e869219bdd282b5825be14d6e\u0026#34;, \u0026#34;container_image_name\u0026#34;: \u0026#34;registry.local/ceph/ceph:v15.2.8\u0026#34;, \u0026#34;container_image_id\u0026#34;: \u0026#34;5553b0cb212ca2aa220d33ba39d9c602c8412ce6c5febc57ef9cdc9c5844b185\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;15.2.8\u0026#34;, \u0026#34;started\u0026#34;: \u0026#34;2021-06-17T22:41:45.132838Z\u0026#34;, \u0026#34;created\u0026#34;: \u0026#34;2021-06-17T22:17:51.063202Z\u0026#34;, \u0026#34;deployed\u0026#34;: \u0026#34;2021-06-17T22:16:03.898845Z\u0026#34;, \u0026#34;configured\u0026#34;: \u0026#34;2021-06-17T22:41:07.807004Z\u0026#34; }, Restart the service:\nncn-s# systemctl restart ceph-01a0d9d2-ea7f-43dc-af25-acdfa5242a48@mgr.ncn-s001 Ceph Manager Modules Location: Ceph manager modules can be enabled or disabled from any ceph-mon nodes.\nEnable Ceph manager modules:\nncn-m001# ceph mgr MODULE_NAME enable MODULE Disable Ceph manager modules:\nncn-m001# ceph mgr MODULE_NAME disable MODULE Scale Ceph Services Ceph has the ability to deploy/scale/reconfigure/redeploy Ceph processes down and back up to restart the services.\nIMPORTANT: When scaling the Ceph manager daemon (mgr.hostname.), keep in mind that there must be a running manager daemon as it is what is controlling the orchestration processes.\nIMPORTANT: osd.all-available-devices cannot be scaled; this is the process to auto-discover available OSDs.\nIMPORTANT: The crash service cannot be scaled; this is the equivalent of a Kubernetes daemon set and runs on all nodes to collect crash data.\nThe following example shows scaling the mgr service down and back up.\nPrerequisites Two SSH sessions are required. One to do the work from and another that is running watch ceph -s to monitor the progress.\nProcedure List the services.\nncn-s001# ceph orch ls Example output:\nNAME RUNNING REFRESHED AGE PLACEMENT IMAGE NAME IMAGE ID crash 6/6 9s ago 4d * registry.local/ceph/ceph:v15.2.8 5553b0cb212c mds.cephfs 3/3 9s ago 4d ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c mgr 3/3 9s ago 4d ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c mon 3/3 9s ago 4d ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c osd.all-available-devices 6/6 9s ago 4d * registry.local/ceph/ceph:v15.2.8 5553b0cb212c rgw.site1.zone1 3/3 9s ago 4d ncn-s001;ncn-s002;ncn-s003;ncn-s004;ncn-s005;ncn-s006;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c (Optional) Limit the results.\nSyntax: ceph orch [\u0026lt;service_type\u0026gt;] [\u0026lt;service_name\u0026gt;] [--export] [plain|json|json-pretty|yaml] [--refresh]\nncn-s001# ceph orch ls mgr Example output:\nNAME RUNNING REFRESHED AGE PLACEMENT IMAGE NAME IMAGE ID mgr 3/3 17s ago 4d ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c The placement of the services is retrieved with this command.\nChoose the service to scale. (reminder the example will use the MGR service)\nIf scaling mds or mgr daemons, make sure to fail over the active mgr/mds daemon so there is always one running.\n# To get the active MDS ncn-s# ceph fs status -f json-pretty|jq -r \u0026#39;.mdsmap[]|select(.state==\u0026#34;active\u0026#34;)|.name\u0026#39; cephfs.ncn-s001.juehkw \u0026lt;-- current active MDS. note this will change when you fail it over so keep this command handy # To get the active MGR ncn-s# ceph mgr dump | jq -r .active_name ncn-s002.fumzfm \u0026lt;-- current active MGR. note this will change when you fail it over so keep this command handy Now, the service, current placement policy, and if applicable, the active MGR/MDS daemon are all known.\nScale the service.\nceph orch apply --placement=\u0026#34;1 \u0026lt;host where the active mgr is running\u0026gt;\u0026#34; For example:\nncn-s001# ceph orch apply mgr --placement=\u0026#34;1 ncn-s002\u0026#34; Scheduled mgr update... Watch the SSH session that is showing the Ceph status (ceph -s).\nncn-s001# ceph -s Example output:\ncluster: id: 11d5d552-cfac-11eb-ab69-fa163ec012bf health: HEALTH_OK services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 70s) mgr: ncn-s002.fumzfm(active, since 14s) mds: cephfs:1 {0=cephfs.ncn-s001.juehkw=up:active} 1 up:standby-replay 1 up:standby osd: 6 osds: 6 up (since 4d), 6 in (since 4d) rgw: 2 daemons active (site1.zone1.ncn-s005.hzfbkd, site1.zone1.ncn-s006.vjuwkf) task status: data: pools: 7 pools, 193 pgs objects: 256 objects, 7.5 MiB usage: 9.1 GiB used, 261 GiB / 270 GiB avail pgs: 193 active+clean io: client: 663 B/s rd, 1 op/s rd, 0 op/s wr The mgr service is now showing 1 active on the node we chose.\nScale the service back up to 3 mgrs.\nncn-s001# ceph orch apply mgr --placement=\u0026#34;3 ncn-s001 ncn-s002 ncn-s003\u0026#34; The returned output will be \u0026ldquo;Scheduled mgr update\u0026hellip;\u0026rdquo;.\nWhen the Ceph status output shows there are 3 running mgr daemons, scale the last daemon back down and up.\nIf it is the MDS or MGR daemons, then REMEMBER we have to fail over the active daemon.\nncn-s001# ceph mgr fail ncn-s002.fumzfm # This was our active MGR. In the Ceph status output, the will be an ACTIVE ceph mgr process change.\nmgr: ncn-s003.wtvbtz(active, since 2m), standbys: ncn-s001.cgbxdw Scale the service back to its original deployment size.\nncn-s001# ceph orch apply mgr --placement=\u0026#34;3 ncn-s001 ncn-s002 ncn-s003\u0026#34; The returned output will be \u0026ldquo;Scheduled mgr update\u0026hellip;\u0026rdquo;.\nMonitor the Ceph status to make sure all the daemons come back online.\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/change_root_passwords_for_compute_nodes/",
	"title": "Change Root Passwords for Compute Nodes",
	"tags": [],
	"description": "",
	"content": "Change Root Passwords for Compute Nodes Update the root password on the system for compute nodes.\nChanging the root password at least once is a recommended best practice for system security.\nPrerequisites The initial root password for compute nodes is not set. Use this procedure to initially set or later change the password.\nProcedure Get an encrypted value for the new password.\nUse the passwd command to update the password and get the encrypted hash of the new password.\nIn the following example, mypasswd-example is the new password set by the administrator, and demonstration is an example salt value, which is also configured by the administrator. Refer to the openssl man pages for more information.\nncn-mw# openssl passwd -6 -salt demonstration It will prompt for the new password to be entered (it will not be echoed to the screen as seen below):\nPassword: mypasswd-example Example command output:\n$6$demonstration$gbSD0NlKb2QTo7NRu/pUn4zTNjk5yhSysTS1tUruNIfbROX/a5H92T7CF8fovhORUkOtPrLUpGXmbqIEMmvrh/ Save the output to be used in the next step to configure the override of compute nodes\u0026rsquo; password.\nOverride the default passwords for compute nodes.\nThe output from the previous step is needed for this procedure. See Customize Configuration Values.\n"
},
{
	"uri": "/docs-csm/en-12/operations/power_management/shut_down_and_power_off_the_management_kubernetes_cluster/",
	"title": "Shut Down and Power Off the Management Kubernetes Cluster",
	"tags": [],
	"description": "",
	"content": "Shut Down and Power Off the Management Kubernetes Cluster Shut down management services and power off the HPE Cray EX management Kubernetes cluster.\nOverview Prerequisites Check health of the management cluster Shut down the Kubernetes management cluster Next step Overview Understand the following concepts before powering off the management non-compute nodes (NCNs) for the Kubernetes cluster and storage:\nThe etcd cluster provides storage for the state of the management Kubernetes cluster. The three node etcd cluster runs on the same nodes that are configured as Kubernetes Master nodes. The management cluster state must be frozen when powering off the Kubernetes cluster. When one member is unavailable, the two other members continue to provide full access to the data. When two members are down, the remaining member will switch to only providing read-only access to the data. Avoid Unnecessary Data Movement with Ceph - The Ceph cluster runs not only on the dedicated storage nodes, but also on the nodes configured as Kubernetes Master nodes. Specifically, the mon processes. If one of the storage nodes goes down, Ceph can rebalance the data onto the remaining nodes and object storage daemons (OSDs) to regain full protection. Avoid Spinning up Replacement Pods on Worker Nodes - Kubernetes keeps all pods running on the management cluster. The kubelet process on each node retrieves information from the etcd cluster about what pods must be running. If a node becomes unavailable for more than five minutes, Kubernetes creates replacement pods on other management nodes. High-Speed Network (HSN) - When the management cluster is shut down the HSN is also shut down. The sat bootsys command automates the shutdown of Ceph and the Kubernetes management cluster and performs these tasks:\nStops etcd and which freezes the state of the Kubernetes cluster on each management node. Stops and disables the kubelet on each management and worker node. Stops all containers on each management and worker node. Stop containerd on each management and worker node. Stops Ceph from rebalancing on the management node that is running a mon process. Prerequisites An authentication token is required to access the API gateway and to use the sat command. See the \u0026ldquo;SAT Authentication\u0026rdquo; section of the HPE Cray EX System Admin Toolkit (SAT) product stream documentation (S-8031) for instructions on how to acquire a SAT authentication token.\nCheck health of the management cluster To check the health and status of the management cluster before shutdown, see the \u0026ldquo;Platform Health Checks\u0026rdquo; section in Validate CSM Health.\nCheck the health and backup etcd clusters:\nDetermine what etcd clusters must be backed up and if they are healthy.\nReview Check the Health and Balance of etcd Clusters.\nBackup etcd clusters.\nSee Backups for etcd-operator Clusters.\nCheck the status of NCN no wipe settings.\nMake sure that metal.no-wipe=1. If a management NCN is set to metal.no-wipe=0, then review Check and Set the metal.no-wipe Setting on NCNs before proceeding.\nncn-m001# /opt/cray/platform-utils/ncnGetXnames.sh Example output:\n+++++ Get NCN Xnames +++++ === Can be executed on any worker or master ncn node. === === Executing on ncn-m001, Thu Mar 18 20:58:04 UTC 2021 === === NCN node xnames and metal.no-wipe status === === metal.no-wipe=1, expected setting - the client === === already has the right partitions and a bootable ROM. === === Requires CLI to be initialized === === NCN Master nodes: ncn-m001 ncn-m002 ncn-m003 === === NCN Worker nodes: ncn-w001 ncn-w002 ncn-w003 === === NCN Storage nodes: ncn-s001 ncn-s002 ncn-s003 === Thu Mar 18 20:58:06 UTC 2021 ncn-m001: x3000c0s1b0n0 - metal.no-wipe=1 ncn-m002: x3000c0s2b0n0 - metal.no-wipe=1 ncn-m003: x3000c0s3b0n0 - metal.no-wipe=1 ncn-w001: x3000c0s4b0n0 - metal.no-wipe=1 ncn-w002: x3000c0s5b0n0 - metal.no-wipe=1 ncn-w003: x3000c0s6b0n0 - metal.no-wipe=1 ncn-s001: x3000c0s7b0n0 - metal.no-wipe=1 ncn-s002: x3000c0s8b0n0 - metal.no-wipe=1 ncn-s003: x3000c0s9b0n0 - metal.no-wipe=1 Shut down the Kubernetes management cluster Shut down platform services.\nncn-m001# sat bootsys shutdown --stage platform-services Example output:\nThe following Non-compute Nodes (NCNs) will be included in this operation: managers: - ncn-m001 storage: - ncn-s001 - ncn-s002 - ncn-s003 workers: - ncn-w001 - ncn-w002 - ncn-w003 Are the above NCN groupings correct? [yes,no] yes Executing step: Create etcd snapshot on all Kubernetes manager NCNs. Executing step: Stop etcd on all Kubernetes manager NCNs. Executing step: Stop and disable kubelet on all Kubernetes NCNs. Executing step: Stop containers running under containerd on all Kubernetes NCNs. WARNING: One or more \u0026#34;crictl stop\u0026#34; commands timed out on ncn-w003 WARNING: One or more \u0026#34;crictl stop\u0026#34; commands timed out on ncn-w002 ERROR: Failed to stop 1 container(s) on ncn-w003. Execute \u0026#34;crictl ps -q\u0026#34; on the host to view running containers. ERROR: Failed to stop 2 container(s) on ncn-w002. Execute \u0026#34;crictl ps -q\u0026#34; on the host to view running containers. WARNING: One or more \u0026#34;crictl stop\u0026#34; commands timed out on ncn-w001 ERROR: Failed to stop 4 container(s) on ncn-w001. Execute \u0026#34;crictl ps -q\u0026#34; on the host to view running containers. WARNING: Non-fatal error in step \u0026#34;Stop containers running under containerd on all Kubernetes NCNs.\u0026#34; of platform services stop: Failed to stop containers on the following NCN (s): ncn-w001, ncn-w002, ncn-w003 Continue with platform services stop? [yes,no] no Aborting. In the preceding example, the commands to stop containers timed out on all the worker nodes and reported WARNING and ERROR messages. A summary of the issue displays and prompts the user to continue or stop. Respond no stop the shutdown. Then review the containers running on the nodes.\nncn-m001# for ncn in ncn-w00{1,2,3}; do echo \u0026#34;${ncn}\u0026#34;; ssh \u0026#34;${ncn}\u0026#34; \u0026#34;crictl ps\u0026#34;; echo; done Example output:\nncn-w001 CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 032d69162ad24 302d9780da639 54 minutes ago Running cray-dhcp-kea 0 e4d1c01818a5a 7ab8021279164 2ad3f16035f1f 3 hours ago Running log-forwarding 0 a5e89a366f5a3 ncn-w002 CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 1ca9d9fb81829 de444b360808f 4 hours ago Running cray-uas-mgr 0 902287a6d0393 ncn-w003 CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID Run the sat command again and enter yes at the prompt about the etcd snapshot not being created:\nncn-m001# sat bootsys shutdown --stage platform-services Example output:\nThe following Non-compute Nodes (NCNs) will be included in this operation: managers: - ncn-m001 storage: - ncn-s001 - ncn-s002 - ncn-s003 workers: - ncn-w001 - ncn-w002 - ncn-w003 Are the above NCN groupings correct? [yes,no] yes Executing step: Create etcd snapshot on all Kubernetes manager NCNs. WARNING: Failed to create etcd snapshot on ncn-m001: The etcd service is not active on ncn-m001 so a snapshot cannot be created. WARNING: Failed to create etcd snapshot on ncn-m002: The etcd service is not active on ncn-m002 so a snapshot cannot be created. WARNING: Failed to create etcd snapshot on ncn-m003: The etcd service is not active on ncn-m003 so a snapshot cannot be created. WARNING: Non-fatal error in step \u0026#34;Create etcd snapshot on all Kubernetes manager NCNs.\u0026#34; of platform services stop: Failed to create etcd snapshot on hosts: ncn-m001, ncn-m00 2, ncn-m003 Continue with platform services stop? [yes,no] yes Continuing. Executing step: Stop etcd on all Kubernetes manager NCNs. Executing step: Stop and disable kubelet on all Kubernetes NCNs. Executing step: Stop containers running under containerd on all Kubernetes NCNs. Executing step: Stop containerd on all Kubernetes NCNs. Executing step: Check health of Ceph cluster and freeze state. If the process continues to report errors due to Failed to stop containers, iterate on the above step. Each iteration should reduce the number of containers running. If necessary, containers can be manually stopped using crictl stop CONTAINER. If containers are stopped manually, re-run the above procedure to complete any final steps in the process.\nShut down and power off all management NCNs except ncn-m001.\nImportant: The default timeout for the next command is 300 seconds. If it is known that the nodes take longer than this amount of time for a graceful shutdown, then a different value can be set using --ncn-shutdown-timeout NCN_SHUTDOWN_TIMEOUT with a value other than 300 for NCN_SHUTDOWN_TIMEOUT. Once this timeout has been exceeded, the node will be forcefully powered down.\nncn-m001# sat bootsys shutdown --stage ncn-power Example output:\nProceed with shutdown of other management NCNs? [yes,no] yes Proceeding with shutdown of other management NCNs. IPMI username: root IPMI password: The following Non-compute Nodes (NCNs) will be included in this operation: managers: - ncn-m002 - ncn-m003 storage: - ncn-s001 - ncn-s002 - ncn-s003 workers: - ncn-w001 - ncn-w002 - ncn-w003 The following Non-compute Nodes (NCNs) will be excluded from this operation: managers: - ncn-m001 storage: [] workers: [] Are the above NCN groupings and exclusions correct? [yes,no] yes Monitor the consoles for each NCN.\nUse tail to monitor the log files in /var/log/cray/console_logs for each NCN.\nAlternatively, attach to the screen session (screen sessions real time, but not saved):\nncn-m001# screen -ls Example output:\nThere are screens on: 26745.SAT-console-ncn-m003-mgmt (Detached) 26706.SAT-console-ncn-m002-mgmt (Detached) 26666.SAT-console-ncn-s003-mgmt (Detached) 26627.SAT-console-ncn-s002-mgmt (Detached) 26589.SAT-console-ncn-s001-mgmt (Detached) 26552.SAT-console-ncn-w003-mgmt (Detached) 26514.SAT-console-ncn-w002-mgmt (Detached) 26444.SAT-console-ncn-w001-mgmt (Detached) Attach to one of the screen sessions.\nncn-m001# screen -x 26745.SAT-console-ncn-m003-mgmt Check the power off status of management nodes.\nNOTE: The read -s command silently reads the password in order to prevent it from being echoed to the screen or preserved in the shell history.\nncn-m001# USERNAME=root ncn-m001# read -r -s -p \u0026#34;NCN BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn-m001# export IPMI_PASSWORD ncn-m001# for ncn in $(grep -oP \u0026#39;ncn-\\w\\d+\u0026#39; /etc/hosts | grep -v ncn-m001 | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39; ); do echo -n \u0026#34;${ncn}: \u0026#34; ipmitool -U \u0026#34;${USERNAME}\u0026#34; -H \u0026#34;${ncn}-mgmt\u0026#34; -E -I lanplus chassis power status done From a remote system, activate the serial console for ncn-m001.\nremote$ USERNAME=root remote$ read -r -s -p \u0026#34;ncn-m01 BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD remote$ export IPMI_PASSWORD remote$ ipmitool -I lanplus -U ${USERNAME} -E -H NCN-M001_BMC_HOSTNAME sol activate Log in at the console login prompt:\nncn-m001 login: root Password: From the serial console of ncn-m001, shut down Linux.\nncn-m001# shutdown -h now Wait until the console indicates that the node has shut down.\nFrom a remote system that has access to the management plane, power off ncn-m001.\nremote$ ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN-M001_BMC_HOSTNAME chassis power status remote$ ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN-M001_BMC_HOSTNAME chassis power off remote$ ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H NCN-M001_BMC_HOSTNAME chassis power status CAUTION: The modular coolant distribution unit (MDCU) in a liquid-cooled HPE Cray EX2000 cabinet (also referred to as a Hill or TDS cabinet) typically receives power from its management cabinet PDUs. If the system includes an EX2000 cabinet, then do not power off the management cabinet PDUs; powering off the MDCU will cause an emergency power off (EPO) of the cabinet and may result in data loss or equipment damage.\n(Optional) If a liquid-cooled EX2000 cabinet is not receiving MCDU power from this management cabinet, then power off the PDU circuit breakers or disconnect the PDUs from facility power and follow lockout-tagout procedures for the site.\nNext step Return to System Power Off Procedures and continue with next step.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/tls_certificates_for_redfish_bmcs/",
	"title": "TLS Certificates for Redfish BMCs",
	"tags": [],
	"description": "",
	"content": "TLS Certificates for Redfish BMCs Redfish HTTP communications are capable of using TLS certificates and Certificate Authority (CA) trust bundles to improve security. Several Hardware Management Services (HMS) have been modified to enable the HTTP transports used for Redfish communications to use a CA trust bundle.\nThe following services communicate with Redfish BMCs:\nState Manager Daemon (SMD) Cray Advanced Platform Monitoring and Control (CAPMC) Firmware Action Service (FAS) HMS Collector River Endpoint Discovery Service (REDS) Mountain Endpoint Discovery Service (MEDS) Each Redfish BMC must have a TLS certificate in order to be useful. The certificates will come from the same PKI that issues the CA trust bundle. The Vault PKI is used to create the TLS certs. Services will get the CA trust bundle either directly from the Vault PKI, or it can be read in via a Kubernetes ConfigMap.\nTLS Certificate Creation TLS certificates are created by the System Configuration Service (SCSD) tool and are stored in Vault secure storage for later retrieval. Each certificate can be created at any level or domain from the individual BMC, all the way up to a cabinet-level. Any certificate created above the BMC domain will contain enough Subject Alternative Names (SANs) to cover that domain. The goal and intent is to create certificates at the cabinet domain, containing the SANs for every possible BMC in that cabinet.\nRefer to Add TLS Certificates to BMCs for the SCSD commands used to create and store the certificates.\nOnce the certificates are created, they can be placed on the target BMCs again using SCSD. Only liquid-cooled BMCs can have TLS certificates set in them.\nCA Bundle Usage By Services Services will use a CA trust bundle when creating secured/validated HTTP clients and transports for use in Redfish operations. Any services that communicate with other services must not use this same client/transport because these services are within the service mesh and do not use TLS certificates. Thus, most services will need different HTTP clients/transports for Redfish and for inter-service communications.\nThe CA trust bundle is placed into a file visible by each HMS service. The Helm chart for each service will specify where this file is located. In addition, there is an environment variable (CA_URI) that comes from a value in customizations.yaml and will direct the service to point to either the Vault PKI\u0026rsquo;s CA bundle or to the ConfigMap bundle.\nIf the CA_URI variable is an empty string, it means that the customizations.yaml has no special entry for it. In this event, each service is set up to not use a CA bundle for Redfish HTTP clients/transports. Thus, this implementation is very backward compatible.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_a_standard_rack_node/",
	"title": "Add a Standard Rack Node",
	"tags": [],
	"description": "",
	"content": "Add a Standard Rack Node These procedures are intended for trained technicians and support personnel only. Always follow ESD precautions when handling this equipment.\nThe example is this procedure adds a User Access Node (UAN) or compute node to an HPE Cray standard rack system. This example adds a node to rack number 3000 at U27.\nProcedures for updating the Hardware State Manager (HSM) or System Layout Service (SLS) are similar when adding additional compute nodes or User Application Nodes (UANs). The contents of the node object in the SLS are slightly different for each node type.\nRefer to the OEM documentation for information about the node architecture, installation, and cabling.\nFor this procedure, a new object must be created in the SLS and modifications will be required to the Slingshot HSN topology.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Procedure Retrieve an authentication token.\nncn-mw# TOKEN=$(curl -k -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Create a new node object in SLS.\nNew node objects require the following information:\nParent: component name (xname) of the new node\u0026rsquo;s BMC\nXname: xname of the new node\nRole: Compute or Application\nAliases: Array of aliases for the node. For compute nodes, this is in the form of nid0000\nNID: The Node ID integer for the node. This applies only to compute nodes\nSubRole: Such as UAN, Gateway, or other valid HSM SubRoles\nIf adding a compute node:\nncn-mw# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST --data \u0026#39;{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s27b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s27b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000001\u0026#34; ], \u0026#34;NID\u0026#34;: 1, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34; } }\u0026#39; https://api-gw-service-nmn.local/apis/sls/v1/hardware | jq If adding a UAN:\nncn-mw# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST --data \u0026#39;{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s27b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s27b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Aliases\u0026#34;: [ \u0026#34;uan04\u0026#34; ], \u0026#34;Role\u0026#34;: \u0026#34;Application\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;UAN\u0026#34; } }\u0026#39; https://api-gw-service-nmn.local/apis/sls/v1/hardware Create a new MgmtSwitchConnector object in SLS.\nThe MgmtSwitchConnector connector is used by the hms-discovery job to determine which management switch port is connected to the node\u0026rsquo;s BMC. The SLS requires the following information:\nThe management switch port that is connected to the new node\u0026rsquo;s BMC Xname: The component name (xname) for the MgmtSwitchConnector in the form of xXcCwWjJ X is the rack number C is the chassis (standard racks are always chassis 0) W is the rack U position of the management network leaf switch J is the switch port number NodeNics: The component name (xname) of the new node\u0026rsquo;s BMC; this field is an array in the payloads below, but should only contain one element VendorName: This field varies depending on the OEM for the management switch; for example, if the BMC is plugged into switch port 36, then the following vendor names could apply: Aruba leaf switches use this format: 1/1/36 Dell leaf switches use this format: ethernet1/1/36 ncn-mw# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST --data \u0026#39;{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j36\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s27b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/36\u0026#34; } }\u0026#39; https://api-gw-service-nmn.local/apis/sls/v1/hardware | jq Install the node hardware in the rack Install the new node hardware in the rack and connect power cables, HSN cables, and management network cables (if it has not already been installed).\nIf the node was added before modifying the SLS, then the node\u0026rsquo;s BMC should have been able to DHCP with Kea, and there will be an unknown MAC address in the HSM Ethernet interfaces table.\nRefer to the OEM documentation for the node for information about the hardware installation and cabling.\nPower on and boot compute node Power on the node to boot the BMC.\nWait for the hms-discovery cronjob to run, and for DNS to update.\nThe hms-discovery cronjob will attempt to correctly identity the new node by comparing node and BMC MAC addresses from the HSM Ethernet interfaces table with the connection information present in SLS.\nAfter roughly 5-10 minutes, the node\u0026rsquo;s BMC should be discovered by the HSM, and the node\u0026rsquo;s BMC can be resolved by using its component name (xname) in DNS.\nncn-mw# ping x3000c0s27b0 Verify that discovery has completed.\nncn-mw# cray hsm inventory redfishEndpoints describe x3000c0s27b0 --format toml Example output:\nID = \u0026#34;x3000c0s2b0\u0026#34; Type = \u0026#34;NodeBMC\u0026#34; Hostname = \u0026#34;\u0026#34; Domain = \u0026#34;\u0026#34; FQDN = \u0026#34;x3000c0s27b0\u0026#34; Enabled = true UUID = \u0026#34;990150e5-03bc-58b5-b986-cfd418d5778b\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34;\u0026#34; RediscoverOnUpdate = true [DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2021-10-20T21:19:32.332521Z\u0026#34; RedfishVersion = \u0026#34;1.6.0\u0026#34; LastDiscoveryStatus = \u0026#34;DiscoverOK\u0026#34; When LastDiscoveryStatus displays as DiscoverOK, the node BMC has been successfully discovered. If the last discovery state is DiscoveryStarted then the BMC is currently being inventoried by HSM. If the last discovery state is HTTPsGetFailed or ChildVerificationFailed, then an error has occurred during the discovery process. Verify that the nodes are enabled in the HSM.\nncn-mw# cray hsm state components describe x3000c0s27b0n0 --format toml Example output (truncated):\nType = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;Off\u0026#34; Verify that the node BMC has been discovered by the HSM.\nncn-mw# cray hsm inventory redfishEndpoints describe x3000c0s27b0 --format json Example output:\n{ \u0026#34;ID\u0026#34;: \u0026#34;x3000c0s27b0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;NodeBMC\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;x3000c0s27b0\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x3000c0s27b0\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;UUID\u0026#34;: \u0026#34;e005dd6e-debf-0010-e803-b42e99be1a2d\u0026#34;, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MACAddr\u0026#34;: \u0026#34;b42e99be1a2d\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2021-01-29T16:15:37.643327Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;1.7.0\u0026#34; } } When LastDiscoveryStatus displays as DiscoverOK, the node BMC has been successfully discovered. If the last discovery state is DiscoveryStarted then the BMC is currently being inventoried by HSM. If the last discovery state is HTTPsGetFailed or ChildVerificationFailed then an error occurred during the discovery process. Enable the nodes in the HSM database.\nIn this example, the nodes are x3000c0s27b1n0 - x3000c0s27b1n3.\nncn-mw# cray hsm state components bulkEnabled update --enabled true \\ --component-ids x3000c0s27b1n0,x3000c0s27b1n1,x3000c0s27b1n2,x3000c0s27b1n3 Verify that the correct firmware versions are present for node BIOS, BMC, HSN NICs, GPUs, and so on.\nIf necessary, update the firmware.\nncn-mw# cray fas actions create CUSTOM_DEVICE_PARAMETERS.json See Update Firmware with FAS.\nUse the Boot Orchestration Service (BOS) to power on and boot the nodes.\nUse the appropriate BOS template for the node type.\nncn-mw# cray bos session create --template-uuid cle-VERSION \\ --operation reboot --limit x3000c0s27b0n0,x3000c0s27b0n1,x3000c0s27b0n2,x3000c0s27b00n3 Verify that the chassis status LEDs indicate normal operation.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/duplicate_ip/",
	"title": "You are getting an IP address, but not the correct one. Duplicate IP address check",
	"tags": [],
	"description": "",
	"content": "You are getting an IP address, but not the correct one. Duplicate IP address check A sign of a duplicate IP address is seeing a DECLINE message from the client to the server.\n10.40.0.0.337 \u0026gt; 10.42.0.58.67: BOOTP/DHCP, Request from b4:2e:99:be:1a:d3, length 301, hops 1, xid 0x9d1210d, Flags [none] Gateway-IP 10.252.0.2 Client-Ethernet-Address b4:2e:99:be:1a:d3 Vendor-rfc1048 Extensions Magic Cookie 0x63825363 DHCP-Message Option 53, length 1: Decline Client-ID Option 61, length 19: hardware-type 255, 99:be:1a:d3:00:01:00:01:26:c8:55:c3:b4:2e:99:be:1a:d3 Server-ID Option 54, length 4: 10.42.0.58 Requested-IP Option 50, length 4: 10.252.0.26 Agent-Information Option 82, length 22: Circuit-ID SubOption 1, length 20: vlan2-ethernet1/1/12 To test for Duplicate IP addresses you can ping the suspected address while you turn off the node, if you continue to get responses, then you have a duplicate IP.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/ntp/",
	"title": "Network Time Protocol (NTP) Client",
	"tags": [],
	"description": "",
	"content": "Network Time Protocol (NTP) Client Summary of NTP from RFC-1305 Network Time Protocol (Version 3):\nNTP is used to synchronize timekeeping among a set of distributed time servers and clients \u0026hellip; It provides the protocol mechanisms to synchronize time in principle to precisions in the order of nanoseconds while preserving a non-ambiguous date well into the next century.\nThe Network Time Protocol (NTP) client is essential for syncing time on various clients in the system. This document shows how to view NTP status and configure NTP on a Dell switch.\nShow NTP status Specify a remote NTP server Configure source for NTP Expected results Show NTP status OS10(config)# do show ntp status Example output:\nsystem peer: 0.0.0.0 system peer mode: unspec leap indicator: 11 stratum: 16 precision: -22 root distance: 0.00000 s root dispersion: 1.28647 s reference ID: [73.78.73.84] reference time: 00000000.00000000 Mon, Jan 1 1900 0:00:00.000 system flags: monitor ntp kernel stats jitter: 0.000000 s stability: 0.000 ppm broadcastdelay: 0.000000 s authdelay: 0.000000 s Specify a remote NTP server Specify a remote NTP server to use for time synchronization:\nswitch(config)# ntp server \u0026lt;FQDN|IP-ADDR\u0026gt; Configure source for NTP switch(config)# ntp source interface Expected results The NTP client can be configured. The functionality can be validated using the show command. The system time of the switch matches that of the NTP server. Back to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/dhcp_decline/",
	"title": "Large Number of DHCP Declines During a Node Boot",
	"tags": [],
	"description": "",
	"content": "Large Number of DHCP Declines During a Node Boot If something similar to the following is in the logs, then this indicates an issue that an IP address being allocated is already being used. It is not able to get the IP address assigned to the device.\ndracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.56 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.57 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.58 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.59 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.60 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.51 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.53 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.54 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.61 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.62 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.63 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.64 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.65 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.66 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.67 Procedure Check by MAC address (no colons):\nThis requires an API token. See Retrieve an Authentication Token for more information.\nncn# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api_gw_service.local/apis/smd/hsm/v2/Inventory/EthernetInterfaces/18c04d13d73c Check by component name (xname):\nThis requires an API token. See Retrieve an Authentication Token for more information.\nncn# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api_gw_service.local/apis/smd/hsm/v2/Inventory/EthernetInterfaces?ComponentID=x3000c0s25b0n0 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/increase_kafka_pod_resource_limits/",
	"title": "Increase Kafka Pod Resource Limits",
	"tags": [],
	"description": "",
	"content": "Increase Kafka Pod Resource Limits For larger scale systems, the Kafka resource limits may need to be increased. See Increase Pod Resource Limits for details on how to increase limits.\nIncrease Kafka Resource Limits Example\nFor a 1500 compute node system, increasing the cpu count to 6 and memory limits to 128G should be adequate.\n"
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/manage_hms_locks/",
	"title": "Manage HMS Locks",
	"tags": [],
	"description": "",
	"content": "Manage HMS Locks This section describes how to check the status of a lock, disable reservations, and repair reservations. The disable and repair operations only affect the ability to make reservations on hardware devices.\nSome of the common scenarios an admin might encounter when working with the Hardware State Manager (HSM) Locking API are also described.\nCheck Lock Status Use the following command to verify if a component name (xname) is locked or not. The command will show if it is locked (admin), reserved (service command), or reservation disabled (either an EPO or an admin command).\nThe following shows how to interpret the output:\nLocked: Shows if the component name (xname) has been locked with the cray hsm locks lock create command. Reserved: Shows if the component name (xname) has been locked for a time-boxed event. Only service can reserve component names (xnames); administrators are not able to reserve component names (xnames). ReservationDisable: Shows if the ability to reserve a component name (xname) has been changed by an EPO or admin command. ncn-m001# cray hsm locks status create --component-ids x1003c5s2b1n1 Example output:\nNotFound = [] [[Components]] ID = \u0026#34;x1003c5s2b1n1\u0026#34; Locked = false Reserved = false ReservationDisabled = false Disable Reservations Disabling a lock prevents a service from being able to make a reservation on it, and it releases/ends any current reservations. Even though SMD removes the reservation when disabling a lock, it does not mean that the Firmware Action Service (FAS) is aware that it has lost the reservation. Additionally, if CAPMC has a reservation that is cancelled, disabled, or broken, it will do nothing to the existing CAPMC operation. There are no checks by CAPMC to make sure things are still reserved at any time during a power operation.\nThis is a way to stop new operations from happening, not a way to prevent currently executing operations.\nncn-m001# cray hsm locks disable create --component-ids x1003c5s2b1n1 Example output:\nFailure = [] [Counts] Total = 1 Success = 1 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x1003c5s2b1n1\u0026#34;,] The following is an example of a when a lock is disabled:\nncn-m001# cray hsm locks status create --component-ids x1003c5s2b1n1 Example output:\nNotFound = [] [[Components]] ID = \u0026#34;x1003c5s2b1n1\u0026#34; Locked = false Reserved = false ReservationDisabled = true Repair Reservations Locks must be manually repaired after disabling a component or performing a manual EPO. This prevents the system from automatically re-issuing reservations or giving out lock requests.\nncn-m001# cray hsm locks repair create --component-ids x1003c5s2b1n1 Example output:\nFailure = [] [Counts] Total = 1 Success = 1 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x1003c5s2b1n1\u0026#34;,] To verify if the lock was successfully repaired:\nncn-m001# cray hsm locks status create --component-ids x1003c5s2b1n1 Example output:\nNotFound = [] [[Components]] ID = \u0026#34;x1003c5s2b1n1\u0026#34; Locked = false Reserved = false ReservationDisabled = false Scenario: What Happens to a Lock if a disable is Issued? Before issuing a disable command, verify that a lock is already in effect:\nncn-m001# cray hsm locks lock create --component-ids x1003c5s2b1n1 Example output:\nFailure = [] [Counts] Total = 1 Success = 1 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x1003c5s2b1n1\u0026#34;,] ncn-m001# cray hsm locks status create --component-ids x1003c5s2b1n1 Example output:\nNotFound = [] [[Components]] ID = \u0026#34;x1003c5s2b1n1\u0026#34; Locked = true Reserved = false ReservationDisabled = false When attempting to disable, the lock will stay in effect, but the reservation ability will be disabled. For example:\nncn-m001# cray hsm locks disable create --component-ids x1003c5s2b1n1 Example output:\nFailure = [] [Counts] Total = 1 Success = 1 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x1003c5s2b1n1\u0026#34;,] ncn-m001# cray hsm locks status create --component-ids x1003c5s2b1n1 Example output:\nNotFound = [] [[Components]] ID = \u0026#34;x1003c5s2b1n1\u0026#34; Locked = true Reserved = false ReservationDisabled = true Scenario: Can a lock be Issued to a Currently Locked Component? A lock cannot be issued to a component that is already locked. The following example shows a component that is already locked, and the returned error message when trying to lock the component again.\nncn-m001# cray hsm locks status create --component-ids x1003c5s2b1n1 Example output:\nNotFound = [] [[Components]] ID = \u0026#34;x1003c5s2b1n1\u0026#34; Locked = true \u0026lt;\u0026lt;-- component name (xname) is locked Reserved = false ReservationDisabled = true ncn-m001# cray hsm locks lock create --component-ids x1003c5s2b1n1 Example output:\nUsage: cray hsm locks lock create [OPTIONS] Try \u0026#39;cray hsm locks lock create --help\u0026#39; for help. Error: Bad Request: Component is Locked "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/create_an_image_customization_cfs_session/",
	"title": "Create an Image Customization CFS Session",
	"tags": [],
	"description": "",
	"content": "Create an Image Customization CFS Session A configuration session that is meant to customize image roots tracked by the Image Management Service (IMS) can be created using the --target-definition image option. This option will instruct the Configuration Framework Service (CFS) to prepare the image IDs specified and assign them to the groups specified in Ansible inventory. IMS will then provide SSH connection information to each image root that CFS will use to configure Ansible.\nAlong with the --target-definition option, users must also provide the --target-group option. This option can be provided multiple times, and allows users to specify the Ansible inventory by creating multiple groups within the inventory and the image(s) that should be in each group. It is important to note that users provide the entire inventory when using image customization, and groups that are not specified will not be included, even if they appear in other CFS inventory types, such as dynamic inventory. For more information on what it means to provide the inventory, see Specifying Hosts and Groups.\nUsers can expect that staging the image and generating an inventory will be a longer process than creating a session with other target definitions (for example, inventories). Tearing down the configuration session will also require additional time while IMS packages up the image build artifacts and uploads them to the artifact repository.\nPrerequisites 1. Check if image is registered with IMS 2. Create CFS image customization session 3. Wait for CFS session to complete successfully 4. Retrieve the resultant image ID Prerequisites The Cray CLI must be configured on the node where the commands are being run. See Configure the Cray CLI. The image being customized must be registered in IMS. See Check if image is registered with IMS. 1. Check if image is registered with IMS In order to use the image target definition, an image must be registered with IMS.\nFor example, if the image ID is 5d64c8b2-4f0e-4b2e-b334-51daba16b7fb, then use jq along with the CLI --format json output option to determine if the image ID is known to IMS:\nncn-mw# cray ims images list --format json | jq -r \u0026#39;any(.[]; .id == \u0026#34;5d64c8b2-4f0e-4b2e-b334-51daba16b7fb\u0026#34;)\u0026#39; Example output:\ntrue 2. Create CFS image customization session To create a CFS session for image customization, provide a session name, the name of the configuration to apply, and the group/image ID mapping:\nWARNING: If a CFS session is created with an ID that is not known to IMS, then CFS will not fail and will instead wait for the image ID to become available in IMS.\nncn-mw# cray cfs sessions create --name example \\ --configuration-name configurations-example \\ --target-definition image --format json \\ --target-group Compute IMS_IMAGE_ID Example output:\n{ \u0026#34;ansible\u0026#34;: { \u0026#34;config\u0026#34;: \u0026#34;cfs-default-ansible-cfg\u0026#34;, \u0026#34;limit\u0026#34;: null, \u0026#34;verbosity\u0026#34;: 0 }, \u0026#34;configuration\u0026#34;: { \u0026#34;limit\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;artifacts\u0026#34;: [], \u0026#34;session\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;succeeded\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;tags\u0026#34;: {}, \u0026#34;target\u0026#34;: { \u0026#34;definition\u0026#34;: \u0026#34;image\u0026#34;, \u0026#34;groups\u0026#34;: [ { \u0026#34;members\u0026#34;: [ \u0026#34;\u0026lt;IMS IMAGE ID\u0026gt;\u0026#34; ], \u0026#34;name\u0026#34;: \u0026#34;Compute\u0026#34; } ] } } 3. Wait for CFS session to complete successfully See Track the Status of a Session.\n4. Retrieve the resultant image ID When an image customization CFS session is complete, use the CFS describe command to show the IMS image ID that results from the applied configuration:\nncn-mw# cray cfs sessions describe example --format json | jq .status.artifacts Example output:\n[ { \u0026#34;image_id\u0026#34;: \u0026#34;\u0026lt;IMS IMAGE ID\u0026gt;\u0026#34;, \u0026#34;result_id\u0026#34;: \u0026#34;\u0026lt;RESULTANT IMS IMAGE ID\u0026gt;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ims_customized_image\u0026#34; } ] This resultant image ID can be used to be further customized pre-boot, or if it is ready, in a Boot Orchestration Service (BOS) boot session template.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/edit_the_ipxe_embedded_boot_script/",
	"title": "Edit the iPXE Embedded Boot Script",
	"tags": [],
	"description": "",
	"content": "Edit the iPXE Embedded Boot Script Manually adjust the iPXE embedded boot script to change the order of network interfaces for DHCP request. Changing the order of network interfaces for DHCP requests helps improve boot time performance.\nPrerequisites This procedure requires administrative privileges.\nProcedure Edit the ConfigMap using one of the following options.\nNOTE: Save a backup of the ConfigMap before making any changes.\nThe following is an example of creating a backup:\nncn-mw# kubectl get configmap -n services cray-ipxe-bss-ipxe \\ -o yaml \u0026gt; /root/cray-ipxe-bss-ipxe-backup.yaml Administrators can add, remove, or reorder sections in the ConfigMap related to the interface being used.\nIn the following example, the net2 section is located before the net0 section. If an administrator wants net0 to be run first, they could move the net0 section to be located before the net2 section.\n:net2 dhcp net2 || goto net2_stop echo net2 IPv4 lease: ${ip} mac: ${net2/mac} chain --timeout 10000 https://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript?mac=${net2/mac} || echo Failed to retrieve next chain from Boot Script Service over net2 (https://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript?mac=${net2/mac} \u0026amp;\u0026amp; goto net2_stop :net2_stop ifclose net2 || echo No routes to drop. :net0 dhcp net0 || goto net0_stop echo net0 IPv4 lease: ${ip} mac: ${net0/mac} chain --timeout 10000 https://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript?mac=${net0/mac} || echo Failed to retrieve next chain from Boot Script Service over net0 (https://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript?mac=${net0/mac} \u0026amp;\u0026amp; goto net0_stop :net0_stop ifclose net0 || echo No routes to drop. Option 1: Edit the cray-ipxe-bss-ipxe ConfigMap directly.\nncn-mw# kubectl edit configmap -n services cray-ipxe-bss-ipxe Option 2: Edit the ConfigMap by saving the file, editing it, and reloading the ConfigMap.\nSave the file.\nncn-mw# kubectl get configmap -n services cray-ipxe-bss-ipxe -o yaml \u0026gt; /root/cray-ipxe-bss-ipxe.yaml Edit the cray-ipxe-bss-ipxe.yaml file.\nReload the ConfigMap.\nDeleting and recreating the ConfigMap will reload it.\nncn-mw# kubectl delete configmap -n services cray-ipxe-bss-ipxe ncn-mw# kubectl create -f /root/cray-ipxe-bss-ipxe.yaml Delete the iPXE pod to ensure the updated ConfigMap will be used.\nFind the pod ID.\nncn-mw# kubectl -n services get pods|grep cray-ipxe Example output:\ncray-ipxe-5dddfc65f-qfmrr 2/2 Running 2 39h Delete the pod.\nReplace CRAY-IPXE_POD_ID with the value returned in the previous step. In this example, the pod ID is cray-ipxe-5dddfc65f-qfmrr.\nncn-mw# kubectl -n services delete pod CRAY-IPXE_POD_ID Wait about 30 seconds for the iPXE binary to be regenerated, and then the nodes will pick up the new ipxe.efi binary.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/customize_end-user_uai_images/",
	"title": "Customize End-User UAI Images",
	"tags": [],
	"description": "",
	"content": "Customize End-User UAI Images The provided end-user UAI image is a basic UAI image that includes an up-to-date version of the SLES Linux distribution. It provides an entry point to using UAIs and an easy way for administrators to experiment with UAS configurations. To support building software to be run in compute nodes, or other HPC and Analytics workflows, it is necessary to create a custom end-user UAI image and use that.\nA custom end-user UAI image can be any container image set up with the end-user UAI entrypoint.sh script. Experimentation with the wide range of possible UAI images is beyond the scope of this document, but the example given here should offer a starting point for that kind of experimentation.\nThe example provided here covers the most common use-case, which is building a UAI image from the SquashFS image used on compute nodes on the host system to support application development, workload management and analytics workflows. Some of the steps are specific to that activity, others would be common to or similar to steps needed to create special purpose UAIs.\nPrerequisites The administrator must be logged into a Non-Compute Node (NCN) or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) See Configure the Cray CLI.\nNOTE: This procedure cannot be run from a PIT node or an external host, it must be run from a Kubernetes worker or master node.\nProcedure Choose a name for the custom image.\nThis example names the custom end-user UAI image called registry.local/cray/cray-uai-compute:latest, and places that name in an environment variable for convenience. Alter the name as appropriate for the image to be created:\nncn-mw# UAI_IMAGE_NAME=registry.local/cray/cray-uai-compute:latest Query the Boot Orchestration Service (BOS) for a session template ID.\nIdentify the session template name to use. A full list may be found with the following command:\nncn-mw# cray bos sessiontemplate list --format yaml Example output:\n- boot_sets: compute: boot_ordinal: 2 etag: d54782b3853a2d8713a597d80286b93e kernel_parameters: ip=dhcp quiet spire_join_token=${SPIRE_JOIN_TOKEN} network: nmn node_roles_groups: - Compute path: s3://boot-images/0c0d4081-2e8b-433f-b6f7-e1ef0b907be3/manifest.json rootfs_provider: cpss3 rootfs_provider_passthrough: dvs:api-gw-service-nmn.local:300:nmn0 type: s3 cfs: configuration: wlm-config-0.1.0 enable_cfs: true name: wlm-sessiontemplate-0.1.0 Alternatively, collect the session template name used during the Cray Operating System (COS) install. Refer to the \u0026ldquo;Boot COS\u0026rdquo; procedure in the COS product stream documentation. Near the end of that procedure, the step to create a BOS session to boot the compute nodes should contain the name.\nRecord the session template name.\nncn-mw# ST_NAME=wlm-sessiontemplate-0.1.0 Download a compute node SquashFS.\nUse the session template name to download a compute node SquashFS from a BOS session template name:\nncn-mw# ST_ID=$(cray bos sessiontemplate describe $ST_NAME --format json | jq -r \u0026#39;.boot_sets.compute.path\u0026#39; | awk -F/ \u0026#39;{print $4}\u0026#39;) ncn-mw# cray artifacts get boot-images $ST_ID/rootfs rootfs.squashfs Mount the SquashFS and create a tarball.\nCreate a directory and mount the SquashFS on the directory.\nncn-w001# mkdir -v mount ncn-mw# mount -v -o loop,ro rootfs.squashfs `pwd`/mount Create the tarball.\nIMPORTANT: 99-slingshot-network.conf is omitted from the tarball, because that prevents the UAI from running sshd as the UAI user with the su command.\nncn-mw# (cd `pwd`/mount; tar --xattrs --xattrs-include=\u0026#39;*\u0026#39; --exclude=\u0026#34;99-slingshot-network.conf\u0026#34; -cf \u0026#34;../$ST_ID.tar\u0026#34; .) 2\u0026gt; /dev/null This may take several minutes. Notice that this does not create a compressed tarball. Using an uncompressed format makes it possible to add files if needed once the tarball is made. It also makes the procedure run slightly faster. Warnings related to xattr can be ignored; the resulting tarball should still result in a functioning UAI container image.\nCheck that the tarball contains ./usr/bin/uai-ssh.sh.\nncn-mw# tar tf $ST_ID.tar | grep \u0026#39;[.]/usr/bin/uai-ssh[.]sh\u0026#39; Example output:\n./usr/bin/uai-ssh.sh If this script is not present, the easiest place to get a copy of the script is from a UAI built from the end-user UAI image provided with UAS. After getting a copy of the script, it can be appended to the tarball.\nCreate a directory for the script.\nncn-mw# mkdir -pv ./usr/bin Create a UAI.\nncn-mw# cray uas create --format toml --publickey ~/.ssh/id_rsa.pub Example output:\nuai_connect_string = \u0026#34;ssh vers@10.26.23.123\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.26.23.123\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-32079250\u0026#34; uai_status = \u0026#34;Pending\u0026#34; username = \u0026#34;vers\u0026#34; [uai_portmap] Copy the script from the UAI.\nncn-mw# scp vers@10.26.23.123:/usr/bin/uai-ssh.sh ./usr/bin/uai-ssh.sh Example output:\nThe authenticity of host \u0026#39;10.26.23.123 (10.26.23.123)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:voQUCKDG4C9FGkmUcHZVrYJBXVKVYqcJ4kmTpe4tvOA. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;10.26.23.123\u0026#39; (ECDSA) to the list of known hosts. uai-ssh.sh 100% 5035 3.0MB/s 00:00 Delete the UAI.\nncn-mw# cray uas delete --uai-list uai-vers-32079250 --format toml Example output:\nresults = [ \u0026#34;Successfully deleted uai-vers-32079250\u0026#34;,] Append the script to the tarball.\nncn-mw# tar rvf 0c0d4081-2e8b-433f-b6f7-e1ef0b907be3.tar ./usr/bin/uai-ssh.sh Create and push the container image.\nCreate a container image using Podman or Docker and push it to the site container registry. Any container-specific modifications may also be done here with a Dockerfile. The ENTRYPOINT layer must be /usr/bin/uai-ssh.sh as that starts sshd for the user in the UAI container started by UAS.\nncn-mw# UAI_IMAGE_NAME=registry.local/cray/cray-uai-compute:latest ncn-mw# podman import --change \u0026#34;ENTRYPOINT /usr/bin/uai-ssh.sh\u0026#34; $ST_ID.tar $UAI_IMAGE_NAME ncn-mw# PODMAN_USER=$(kubectl get secret -n nexus nexus-admin-credential -o json | jq -r \u0026#39;.data.username\u0026#39; | base64 -d) ncn-mw# PODMAN_PASSWD=$(kubectl get secret -n nexus nexus-admin-credential -o json | jq -r \u0026#39;.data.password\u0026#39; | base64 -d) ncn-mw# podman push --creds \u0026#34;$PODMAN_USER:$PODMAN_PASSWD\u0026#34; $UAI_IMAGE_NAME Register the new container image with UAS.\nncn-mw# cray uas admin config images create --imagename $UAI_IMAGE_NAME Cleanup the mount directory and tarball.\nncn-mw# umount -v mount; rmdir -v mount ncn-mw# rm -v $ST_ID.tar rootfs.squashfs # NOTE: The next step could be done as an `rm -rf` but, because the user # is `root` and the path is very similar to an important system # path a more cautious approach is taken. ncn-mw# rm -fv ./usr/bin/uai-ssh.sh \u0026amp;\u0026amp; rmdir ./usr/bin ./usr "
},
{
	"uri": "/docs-csm/en-12/install/create_hmn_connections_json/",
	"title": "Create HMN Connections JSON File",
	"tags": [],
	"description": "",
	"content": "Create HMN Connections JSON File Use this procedure to generate the hmn_connections.json from the system\u0026rsquo;s SHCD Excel document. This process is typically needed when generating the hmn_connections.json file for a new system, or regenerating it when a system\u0026rsquo;s SHCD file is changed (specifically the HMN tab). The hms-shcd-parser tool can be used to generate the hmn_connections.json file.\nThe SHCD/HMN Connections Rules document explains the expected naming conventions and rules for the HMN tab of the SHCD file, and for the hmn_connections.json file.\nPrerequisites SHCD Excel file for the system\nPodman is available\nPodman is available on the CSM LiveCD, and is installed onto an NCN when being used as an environment to create the CSM PIT in the Bootstrap PIT Node from LiveCD USB or Bootstrap Pit Node from LiveCD Remote ISO procedures.\nProcedure Inspect the HMN tab of the SHCD file.\nVerify that it does not have unexpected data in columns J through U in rows 20 or below. If any unexpected data is present in this region of the HMN tab, then it will end up in the generated hmn_connections.json. Therefore, it must be removed before generating the hmn_connections.json file. Unexpected data is anything other than HMN cabling information, such as another table placed below the HMN cabling information. Any data above row 20 will not interfere when generating hmn_connections.json.\nFor example, the following image shows an unexpected table present underneath HMN cabling information in rows 26 to 29. The HMN cabling information in this example is truncated for brevity.\nLoad the hms-shcd-parser container image from the CSM release distribution into Podman.\nThe CSM_PATH environment variable is expected to to be set from the Bootstrap PIT Node from LiveCD USB or Bootstrap PIT Node from LiveCD Remote ISO procedures.\nDetermine the version of the hms-shcd-parser container image:\nlinux# SHCD_PARSER_VERSION=$(realpath ${CSM_PATH}/docker/artifactory.algol60.net/csm-docker/stable/hms-shcd-parser* | egrep -o \u0026#39;[0-9]+\\.[0-9]+\\.[0-9]+$\u0026#39;) linux# echo $SHCD_PARSER_VERSION Load the hms-shcd-parser container image into Podman:\nlinux# ${CSM_PATH}/hack/load-container-image.sh artifactory.algol60.net/csm-docker/stable/hms-shcd-parser:$SHCD_PARSER_VERSION Copy the system\u0026rsquo;s SHCD file to the machine being used to prepare the hmn_connections.json file.\nSet a variable to point to the system\u0026rsquo;s SHCD Excel file.\nNOTE: Make sure to quote the SHCD file path if there is whitespace in the document\u0026rsquo;s path or filename.\nlinux# SHCD_FILE=\u0026#34;/path/to/systems/SHCD.xlsx\u0026#34; Generate the hmn_connections.json file from the SHCD file.\nThis will create the hmn_connections.json file in the current directory. If it already exists, it will be overwritten.\nlinux# podman run --rm -it --name hms-shcd-parser -v \u0026#34;$(realpath \u0026#34;$SHCD_FILE\u0026#34;)\u0026#34;:/input/shcd_file.xlsx -v \u0026#34;$(pwd)\u0026#34;:/output artifactory.algol60.net/csm-docker/stable/hms-shcd-parser:$SHCD_PARSER_VERSION "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/mellanox_lacp_individual/",
	"title": "Mellanox lacp-individual Limitations",
	"tags": [],
	"description": "",
	"content": "Mellanox lacp-individual Limitations Description In some failover/maintenance scenarios, administrators may want to shut down one port of the bond on an NCN. Because of the way Mellanox handles lacp-individual mode, the ports need to be shut down from the switch instead of the NCN.\nFix Shut down the port on the switch instead of the NCN.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/remove_ceph_node/",
	"title": "Shrink the Ceph Cluster",
	"tags": [],
	"description": "",
	"content": "Shrink the Ceph Cluster This procedure describes how to remove a Ceph node from the Ceph cluster. Once the node is removed, the cluster is also rebalanced to account for the changes. Use this procedure to reduce the size of a cluster.\nPrerequisites This procedure requires administrative privileges and three ssh sessions. One to monitor the cluster. One to perform cluster wide actions from a ceph-mon node. One to perform node only actions on the node being removed. IMPORTANT NOTES Permanent removal of ncn-s001, ncn-s002, or ncn-s003 is NOT SUPPORTED. They can only be rebuilt in place or replaced with new hardware. This is due to the Ceph mon and mgr processes running on them. Always ensure you have the free capacity to remove the node(s) prior to performing this task. When removing a node other than ncn-s001, ncn-s002, or ncn-s003, the SMF pools quotas must be adjusted accordingly. Removal of more than one node at a time is NOT SUPPORTED because the SMA telemetry pool only has 2 copies. Procedure Log in as root on ncn-s001, ncn-s002, ncn-s003, or a master node.\nMonitor the progress of the OSDs that have been added.\nncn# watch ceph -s View the status of each OSD and see where they reside.\nncn-s00(1/2/3)# ceph osd tree Example output:\nID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 31.43875 root default -3 10.47958 host ncn-s001 4 ssd 1.74660 osd.4 up 1.00000 1.00000 5 ssd 1.74660 osd.5 up 1.00000 1.00000 10 ssd 1.74660 osd.10 up 1.00000 1.00000 12 ssd 1.74660 osd.12 up 1.00000 1.00000 13 ssd 1.74660 osd.13 up 1.00000 1.00000 14 ssd 1.74660 osd.14 up 1.00000 1.00000 -5 6.98639 host ncn-s002 0 ssd 1.74660 osd.0 up 1.00000 1.00000 3 ssd 1.74660 osd.3 up 1.00000 1.00000 6 ssd 1.74660 osd.6 up 1.00000 1.00000 9 ssd 1.74660 osd.9 up 1.00000 1.00000 -7 6.98639 host ncn-s003 2 ssd 1.74660 osd.2 up 1.00000 1.00000 7 ssd 1.74660 osd.7 up 1.00000 1.00000 8 ssd 1.74660 osd.8 up 1.00000 1.00000 11 ssd 1.74660 osd.11 up 1.00000 1.00000 -9 6.98639 host ncn-s004 1 ssd 1.74660 osd.1 up 1.00000 1.00000 15 ssd 1.74660 osd.15 up 1.00000 1.00000 16 ssd 1.74660 osd.16 up 1.00000 1.00000 17 ssd 1.74660 osd.17 up 1.00000 1.00000 Set the NODE variable.\nncn# export NODE=\u0026lt;node being removed\u0026gt; Reweigh the OSD(s) on the node being removed to rebalance the cluster.\nChange the weight and CRUSH weight of the OSD being removed to 0.\nncn-s00(1/2/3)# for osd in $(ceph osd ls-tree $NODE); do ceph osd reweight osd.$osd 0; ceph osd crush reweight osd.$osd 0; done Watch the ceph -s output until the cluster status is HEALTH_OK and the Rebalancing has completed.\nRemove the OSD after the reweighing work is complete.\nncn-s00(1/2/3)# for osd in $(ceph osd ls-tree $NODE); do ceph osd down osd.$osd; ceph osd destroy osd.$osd --force; ceph osd purge osd.$osd --force; done Remove any weight 0 orphaned OSDs.\nIf orphaned OSDs from the host $NODE remain that have weight 0, then remove those OSDs.\nncn-s00(1/2/3)# ceph osd tree Example output:\nID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 24.45236 root default -3 10.47958 host ncn-s001 4 ssd 1.74660 osd.4 up 1.00000 1.00000 5 ssd 1.74660 osd.5 up 1.00000 1.00000 10 ssd 1.74660 osd.10 up 1.00000 1.00000 12 ssd 1.74660 osd.12 up 1.00000 1.00000 13 ssd 1.74660 osd.13 up 1.00000 1.00000 14 ssd 1.74660 osd.14 up 1.00000 1.00000 -5 6.98639 host ncn-s002 0 ssd 1.74660 osd.0 up 1.00000 1.00000 3 ssd 1.74660 osd.3 up 1.00000 1.00000 6 ssd 1.74660 osd.6 up 1.00000 1.00000 9 ssd 1.74660 osd.9 up 1.00000 1.00000 -7 6.98639 host ncn-s003 2 ssd 1.74660 osd.2 up 1.00000 1.00000 7 ssd 1.74660 osd.7 up 1.00000 1.00000 8 ssd 1.74660 osd.8 up 1.00000 1.00000 11 ssd 1.74660 osd.11 up 1.00000 1.00000 -9 0 host ncn-s004 1 0 osd.1 up 1.00000 1.00000 \u0026lt;--- orphan ncn-s# ceph osd down osd.1; ceph osd destroy osd.1 --force; ceph osd purge osd.1 --force Regenerate Rados-GW Load Balancer Configuration.\nUpdate the existing HAProxy configuration to remove the node from the configuration.\nncn-s00(1/2/3)# vi /etc/haproxy/haproxy.cfg This example removes node ncn-s004 from the backend rgw-backend.\n... backend rgw-backend option forwardfor balance static-rr option httpchk GET / server server-ncn-s001-rgw0 10.252.1.6:8080 check weight 100 server server-ncn-s002-rgw0 10.252.1.5:8080 check weight 100 server server-ncn-s003-rgw0 10.252.1.4:8080 check weight 100 server server-ncn-s004-rgw0 10.252.1.13:8080 check weight 100 \u0026lt;--- Line to remove ... Copy the HAproxy configuration from ncn-s001 to all the storage nodes. Adjust the command based on the number of storage nodes.\nncn-s001# pdcp -w ncn-s00[2-(end node number)] /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg Restart HAproxy on all the storage nodes, and stop HAproxy and KeepAlived on the node that is being removed.\nncn# pdsh -w ncn-s00[1-(end node number)] -f 2 \u0026#39;systemctl restart haproxy.service\u0026#39; ncn# pdsh -w $NODE \u0026#39;systemctl stop haproxy.service; systemctl stop keepalived.service\u0026#39; Redeploy the Rados Gateway containers to adjust the placement group.\nncn-ms# ceph orch apply rgw site1 zone1 --placement=\u0026#34;\u0026lt;num-daemons\u0026gt; \u0026lt;node1 node2 node3 node4 ... \u0026gt;\u0026#34; --port=8080 For example:\nncn-ms# ceph orch apply rgw site1 zone1 --placement=\u0026#34;3 ncn-s001 ncn-s002 ncn-s003\u0026#34; --port=8080 Verify that the Rados Gateway is running on the desired nodes.\nncn-ms# ceph orch ps --daemon_type rgw Example output:\nNAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID rgw.site1.zone1.ncn-s001.kvskqt ncn-s001 running (41m) 6m ago 41m 15.2.8 registry.local/ceph/ceph:v15.2.8 553b0cb212c 6e323878db46 rgw.site1.zone1.ncn-s002.tisuez ncn-s002 running (41m) 6m ago 41m 15.2.8 registry.local/ceph/ceph:v15.2.8 553b0cb212c 278830a273d3 rgw.site1.zone1.ncn-s003.nnwuqy ncn-s003 running (41m) 6m ago 41m 15.2.8 registry.local/ceph/ceph:v15.2.8 553b0cb212c a9706e6d7a69 Remove the node from the cluster.\nncn-s00(1/2/3)# ceph orch host rm $NODE Remove the Ceph configuration from the node.\nOn the node being removed\nncn-s# cephadm rm-cluster --fsid $(cephadm ls|jq -r .[1].fsid) --force Remove the node from the CRUSH map.\nncn-s00(1/2/3)# ceph osd crush rm $NODE In the output from ceph -s, verify that the status is HEALTH_OK.\nNOTE: If ncn-s001, ncn-s002, or ncn-s003 has been temporarily removed, HEALTH_WARN will be seen until the storage node is added back to the cluster.\nhealth: HEALTH_WARN 1 stray daemons(s) not managed by cephadm 1 stray host(s) with 1 daemon(s) not managed by cephadm 1/3 mons down, quorum ncn-s003,ncn-s002 Degraded data redundancy:... "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/change_snmp_credentials_on_leaf_bmc_switches/",
	"title": "Change SNMP Credentials on Leaf-BMC Switches",
	"tags": [],
	"description": "",
	"content": "Change SNMP Credentials on Leaf-BMC Switches This procedure changes the SNMP credentials on management leaf-BMC switches in the system. All SNMP credentials need to be the same as those found in the customizations.yaml sealed secret cray_reds_credentials.\nNOTE This procedure will not update the default SNMP credentials used when new leaf BMC switches are added to the system. To update the default SNMP credentials for new hardware, follow the Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials procedure.\nPrerequisites Procedure Troubleshooting Viewing SNMP credentials stored in Vault Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Procedure There are three steps involved. The first two steps involve running the leaf_switch_snmp_creds.sh script. This script can be used to check for undesirable SNMP user IDs and credentials, and also to set new ones. The default behavior is to check first and then set new credentials. The script can be run either interactively (no environment variables or command line options) or non-interactively (using environment variables on the command line). The following examples use the environment variable method.\nSet environment variables containing the new SNMP credentials.\nread -s is used to prevent the password from appearing in the command history.\nSet the SNMP authentication password environment variable.\nncn-mw# read -s SNMP_AUTH_PASS Set the SNMP privacy password environment variable.\nncn-mw# read -s SNMP_PRIV_PASS Set environment variable containing the switch admin user password for the management switches in the system.\nncn-mw# read -s SWITCH_ADMIN_PASSWORD Update SNMP credentials (desired SNMP user ID and authentication and privacy passwords) on leaf-BMC switches. The SNMP user IDs and passwords are not shown.\nAlso note that this will change the SNMP credentials in Vault. See below for details on how to do that.\nncn-mw# SNMPNEWUSER=testuser \\ SNMPAUTHPW=$SNMP_AUTH_PASS SNMPPRIVPW=$SNMP_PRIV_PASS \\ SNMPMGMTPW=$SWITCH_ADMIN_PASSWORD \\ /opt/cray/csm/scripts/hms_verification/leaf_switch_snmp_creds.sh Example output:\n==\u0026gt; Getting management network leaf switch info from SLS... ==\u0026gt; Fetching switch hostnames... =============================== Checking SNMP default creds on Dell leaf switch: sw-leaf-002 ==\u0026gt; SNMP user ID \u0026#39;testuser\u0026#39; found on switch sw-leaf-002. Setting SNMP default creds on Aruba leaf switch: sw-leaf-002 =============================== Checking SNMP default creds on Dell leaf switch: sw-leaf-001 ==\u0026gt; SNMP user ID \u0026#39;testuser\u0026#39; found on switch sw-leaf-001. Setting SNMP default creds on Dell leaf switch: sw-leaf-002 Set the Vault alias, if it is not already set.\nncn-mw# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-mw# alias vault=\u0026#39;kubectl -n vault exec -i cray-vault-0 -c vault -- env VAULT_TOKEN=\u0026#34;$VAULT_PASSWD\u0026#34; VAULT_ADDR=http://127.0.0.1:8200 VAULT_FORMAT=json vault\u0026#39; Update Vault with new SNMP credentials.\nEither update the credentials in Vault for a single leaf switch, or update Vault for all leaf switches to have same global default value.\nTo update Vault for all leaf switches in the system to the same password:\nncn-mw# for XNAME in $(cray sls search hardware list --type comptype_mgmt_switch --format json | jq -r .[].Xname); do echo \u0026#34;Updating SNMP creds for $XNAME\u0026#34; vault kv get secret/hms-creds/$XNAME | jq --arg SNMP_AUTH_PASS \u0026#34;$SNMP_AUTH_PASS\u0026#34; --arg SNMP_PRIV_PASS \u0026#34;$SNMP_PRIV_PASS\u0026#34; \u0026#39;.data | .SNMPAuthPass=$SNMP_AUTH_PASS | .SNMPPrivPass=$SNMP_PRIV_PASS\u0026#39; | vault kv put secret/hms-creds/$XNAME - done To update Vault for a single leaf switch:\nncn-mw# XNAME=x3000c0w22 vault kv get secret/hms-creds/$XNAME | jq --arg SNMP_AUTH_PASS \u0026#34;$SNMP_AUTH_PASS\u0026#34; --arg SNMP_PRIV_PASS \u0026#34;$SNMP_PRIV_PASS\u0026#34; \u0026#39;.data | .SNMPAuthPass=$SNMP_AUTH_PASS | .SNMPPrivPass=$SNMP_PRIV_PASS\u0026#39; | vault kv put secret/hms-creds/$XNAME - Restart the River Endpoint Discovery Service (REDS) to pick up the new SNMP credentials.\nncn-mw# kubectl -n services rollout restart deployment cray-reds ncn-mw# kubectl -n services rollout status deployment cray-reds Wait two minutes for REDS to initialize itself.\nVerify that REDS was able to communicate with the leaf-BMC switches with the updated credentials.\nDetermine the name of the REDS pods.\nncn-mw# kubectl -n services get pods -l app.kubernetes.io/name=cray-reds Example output:\nNAME READY STATUS RESTARTS AGE cray-reds-6b99b9d5dc-c5g2t 2/2 Running 0 3m21s Check the logs of the REDS pod for SNMP communication issues.\nIn the following command, replace CRAY_REDS_POD_NAME with the currently running pod for REDS:\nncn-mw# kubectl -n services logs CRAY_REDS_POD_NAME cray-reds | grep \u0026#34;Failed to get ifIndex\u0026lt;-\u0026gt;name map\u0026#34; If nothing is returned, then REDS is able to successfully communicate to the leaf-BMC switches in the system via SNMP.\nErrors like the following occur when SNMP credentials in Vault to not match what is configured on the leaf-BMC switch.\n2021/10/26 20:03:21 WARNING: Failed to get ifIndex\u0026lt;-\u0026gt;name map (1.3.6.1.2.1.31.1.1.1.1) for x3000c0w22: Received a report from the agent - UsmStatsWrongDigests(1.3.6.1.6.3.15.1.1.5.0) Troubleshooting If the credentials are not working, then check the Vault credentials as shown above.\nIf the leaf_switch_snmp_creds.sh script fails for any leaf-BMC switch, then validate and change the credentials manually using the procedures found in Aruba SNMP Users Guide or Dell SNMP Users Guide.\nViewing SNMP credentials stored in Vault If desired view the existing SNMP credentials stored in Vault for a leaf-bmc switch.\nSet the Vault alias, if it is not already set.\nncn-mw# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-mw# alias vault=\u0026#39;kubectl -n vault exec -i cray-vault-0 -c vault -- env VAULT_TOKEN=\u0026#34;$VAULT_PASSWD\u0026#34; VAULT_ADDR=http://127.0.0.1:8200 VAULT_FORMAT=json vault\u0026#39; List the switches in the system.\nncn-mw# cray sls search hardware list --type comptype_mgmt_switch --format json | jq \u0026#39;.[] | { Xname: .Xname, Aliases: .ExtraProperties.Aliases, Brand: .ExtraProperties.Brand}\u0026#39; -c Example output:\n{\u0026#34;Xname\u0026#34;:\u0026#34;x3000c0w37\u0026#34;,\u0026#34;Aliases\u0026#34;:[\u0026#34;sw-leaf-bmc-001\u0026#34;],\u0026#34;Brand\u0026#34;:\u0026#34;Aruba\u0026#34;} Query Vault for the expected sw-leaf-bmc credentials.\nncn-mw# vault kv get secret/hms-creds/x3000c0w37 Example output:\n{ \u0026#34;request_id\u0026#34;: \u0026#34;62070a95-8bbb-6834-d707-c17ca9b565e3\u0026#34;, \u0026#34;lease_id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;lease_duration\u0026#34;: 2764800, \u0026#34;renewable\u0026#34;: false, \u0026#34;data\u0026#34;: { \u0026#34;Password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;SNMPAuthPass\u0026#34;: \u0026#34;SNMP_AUTH_PASSWORD\u0026#34;, \u0026#34;SNMPPrivPass\u0026#34;: \u0026#34;SNMP_PRIV_PASSWORD\u0026#34;, \u0026#34;URL\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Username\u0026#34;: \u0026#34;testuser\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w37\u0026#34; }, \u0026#34;warnings\u0026#34;: null } "
},
{
	"uri": "/docs-csm/en-12/operations/power_management/standard_rack_node_power_management/",
	"title": "Standard Rack Node Power Management",
	"tags": [],
	"description": "",
	"content": "Standard Rack Node Power Management HPE Cray EX standard EIA rack node power management is supported by the server vendor BMC firmware. The BMC exposes the power control API for a node through the node\u0026rsquo;s Redfish Power schema.\nOut-of-band power management data is polled by a collector and published on a Kafka bus for entry into the Power Management Database (PMDB). Access to the data stored in the PMDB is available through the System Monitoring Application (SMA) Grafana instance.\nPower limiting of a node must be enabled and may require additional licenses to use. Refer to vendor documentation for instructions on how to enable power limiting and what licenses, if any, are needed.\nCAPMC only handles power limiting of one hardware type at a time. Each vendor and server model has their own power limiting capabilities. Therefore a different power limit request will be needed for each vendor and model that needs to have its power limited.\nRequirements Hardware State Manager (cray-hms-smd) \u0026gt;= v1.30.16 CAPMC (cray-hms-capmc) \u0026gt;= 1.31.0 Cray CLI \u0026gt;= 0.44.0 Deprecated Interfaces See the CAPMC Deprecation Notice for more information.\nget_node_energy (Deprecated) get_node_energy_stats (Deprecated) get_system_power (Deprecated) Redfish API The Redfish API for rack-mounted nodes is the node\u0026rsquo;s Power resource which is presented by the BMC. OEM properties may be used to augment the Power schema to provide additional power management capabilities.\nPower Limiting CAPMC power limiting controls for compute nodes can query component capabilities and manipulate the node power constraints. This functionality enables external software to establish an upper bound, or estimate a minimum bound, on the amount of power a system or a select subset of the system may consume.\nCAPMC API calls provide means for third party software to implement advanced power management strategies using JSON data structures.\nThe rack-mounted compute nodes support these power limiting and monitoring API calls:\nget_power_cap_capabilities get_power_cap set_power_cap In general, rack-mounted compute nodes do not allow for power limiting of any installed accelerators separately from the node limit.\nPower limit control will only be valid on a compute node when power limiting is enabled, the node is booted, and the node is in the Ready state as seen via the Hardware State Manager (HSM).\nCray CLI Examples for Standard Rack Compute Node Power Management Get Node Power Control and Limit Settings\nncn-m001# cray capmc get_power_cap create –-nids NID_LIST --format json Return the current power limit settings for a node and any accelerators that are installed. Valid settings are only returned if power limiting is enabled on the target nodes.\nncn-m001# cray capmc get_power_cap create --nids 4 { \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;nids\u0026#34;: [ { \u0026#34;nid\u0026#34;: 4, \u0026#34;controls\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Chassis Power Control\u0026#34;, \u0026#34;val\u0026#34;: 500 } ] } ] } Get Power Limiting Capabilities\nncn-m001# cray capmc get_power_cap_capabilities create –-nids NID_LIST --format json Return the min and max power limit settings for the node list and any accelerators that are installed.\nncn-m001# cray capmc get_power_cap_capabilities create --nids 4 --format json { \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;groups\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;3_AuthenticAMD_64c_244GiB_3200MHz_NoAccel\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;3_AuthenticAMD_64c_244GiB_3200MHz_NoAccel\u0026#34;, \u0026#34;host_limit_max\u0026#34;: 0, \u0026#34;host_limit_min\u0026#34;: 0, \u0026#34;static\u0026#34;: 0, \u0026#34;supply\u0026#34;: 900, \u0026#34;powerup\u0026#34;: 0, \u0026#34;nids\u0026#34;: [ 4 ], \u0026#34;controls\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Chassis Power Control\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;Chassis Power Control\u0026#34;, \u0026#34;max\u0026#34;: 900, \u0026#34;min\u0026#34;: 61 } ] } ] } Set Node Power Limit\nncn-m001# cray capmc set_power_cap create --nids NID_LIST --control CONTROL_NAME VALUE --format json Set the total power limit of the node by using the name of the node control. The power provided to the host CPU and memory is the total node power limit minus the power limits of each of the accelerators installed on the node.\nncn-m001# cray capmc set_power_cap create --nids 4 --control \u0026#34;Chassis Power Control\u0026#34; 600 { \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;nids\u0026#34;: [ { \u0026#34;nid\u0026#34;: 4, \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34; } ] } Multiple controls can be set at the same time on multiple nodes, but all target nodes must have the same set of controls available, otherwise the call will fail.\nncn-m001# cray capmc set_power_cap create \\ --nids [1-4] --control \u0026#34;Chassis Power Control\u0026#34; 600 { \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;nids\u0026#34;: [ { \u0026#34;nid\u0026#34;: 1, \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;nid\u0026#34;: 2, \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;nid\u0026#34;: 3, \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;nid\u0026#34;: 4, \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34; } ] } Remove Node Power Limit (Set to Default)\nncn-m001# cray capmc set_power_cap create --nids NID_LIST --control CONTROL_NAME 0 --format json Reset the power limit to the default maximum. Alternatively, using the max value returned from get_power_cap_capabilities may also be used. Multiple controls can be set at the same time on multiple nodes, but all target nodes must have the same set of controls available, otherwise the call will fail.\nncn-m001# cray capmc set_power_cap create --nids 4 --control \u0026#34;Node Power Limit\u0026#34; 0 { \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;nids\u0026#34;: [ { \u0026#34;nid\u0026#34;: 4, \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;\u0026#34; } ] } Enable and Disable Power Limiting Gigabyte Enable Power Limiting\nncn-m001# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; \\ -X POST https://${BMC}/redfish/v1/Chassis/Self/Power/Actions/LimitTrigger \\ --data \u0026#39;{\u0026#34;PowerLimitTrigger\u0026#34;: \u0026#34;Activate\u0026#34;}\u0026#39; Deactivate Node Power Limit\nncn-m001# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; \\ -X POST https://${BMC}/redfish/v1/Chassis/Self/Power/Actions/LimitTrigger \\ --data \u0026#39;{\u0026#34;PowerLimitTrigger\u0026#34;: \u0026#34;Deactivate\u0026#34;}\u0026#39; "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/troubleshoot_interfaces_with_ip_address_issues/",
	"title": "Troubleshoot Interfaces with IP Address Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Interfaces with IP Address Issues Correct NCNs that are failing to assigning a static IP address or detect a duplicate IP address.\nThe Wicked network manager tool will fail to bring an interface up if its assigned IP address already exists in the respective LAN. This can be detected by checking for signs of duplicate IP address messages in the log.\nPrerequisites An NCN has an interface that is failing to assign a static IP address or that has a duplicate IP address.\nProcedure Use one of the following options to correct NCNs that are failing to assigning a static IP address or to detect a duplicate IP address:\nCheck the logs in /var/log/ for reports of duplicate IP addresses.\nThis also will show which MAC address is being used for the IP address.\nncn# grep duplicate /var/log/* Example output:\nwarn:2020-08-04T19:22:02.434775+00:00 ncn-w001 wickedd[2188]: bond0: IPv4 duplicate address 10.1.1.1 detected (in use by 00:30:48:bb:e8:d2)! Add an IP address that is not found or commonly assigned on the respective network.\nEdit the /etc/sysconfig/network/ifcfg-FILENAME file.\nReload the interface.\nUse the following command to safely reload the interface:\nncn# wicked ifreload INTERFACE_NAME If that does not work, attempt to forcefully add it:\nncn# systemctl restart wickedd-nanny Add the duplicate IP address with the ip command.\nAdd the duplicate IP address.\nThe command below will bypass Wicked and will ignore the system preference:\nncn# ip a a IP_ADDRESS/MASK dev INTERFACE_NAME For example:\nncn# ip a a 10.1.1.1/16 dev bond0 View the bond.\nncn# ip a s bond0 Example output:\n8: bond0: \u0026lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP\u0026gt; mtu 9238 qdisc noqueue state UP group default qlen 1000 link/ether b8:59:9f:c7:11:12 brd ff:ff:ff:ff:ff:ff inet 10.1.1.1/16 brd 10.1.255.255 scope global bond0 valid_lft forever preferred_lft forever inet6 fe80::ba59:9fff:fec7:1112/64 scope link valid_lft forever preferred_lft forever Delete the IP address after the duplicate IP address is removed.\nncn# ip a d IP_ADDRESS/MASK dev bond0 For example:\nncn# ip a d 10.1.1.1/16 dev bond0 (Not recommended) Allow the duplicate IP address to exist.\nThis is not recommended because it is unstable and can make the work harder to correct down the line. The easiest way to deal with the duplicate is by adding another IP address, and then logging into the duplicate and nullifying it. This block will disable the safeguard for duplicate IP addresses.\nncn# sed -i \u0026#39;^CHECK_DUPLICATE_IP=.*/CHECK_DUPLICATE_IP=\u0026#34;no\u0026#34;/\u0026#39; /etc/sysconfig/network/config ncn# wicked ifup INTERFACE_NAME Notes Running wicked ifreload on a worker node can have the side effect of causing Slurm and UAI pods to lose their macvlan attachments.\nIn this case, restarts of those services (in the Kubernetes user namespace) can be performed by executing the following command:\nncn-mw# kubectl delete po -n user $(kubectl get po -n user | grep -v NAME | awk \u0026#39;\\{ print $1 }\u0026#39;) "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_additional_liquid-cooled_cabinets_to_a_system/",
	"title": "Add Additional Liquid-Cooled Cabinets to a System",
	"tags": [],
	"description": "",
	"content": "Add Additional Liquid-Cooled Cabinets to a System This top level procedure outlines the process for adding additional liquid-cooled cabinets to a currently installed system.\nPrerequisites The system\u0026rsquo;s SHCD file has been updated with the new cabinets and cabling changes. The new cabinets have been cabled to the system, and the system\u0026rsquo;s cabling has been validated to be correct. Follow the procedure Create a Backup of the SLS Postgres Database. Follow the procedure Create a Backup of the HSM Postgres Database Procedure Perform procedures in Add Liquid-Cooled Cabinets to SLS.\nPerform procedures in Updating Cabinet Routes on Management NCNs.\nReconfigure management network.\nValidate SHCD with CANU. Validate cabling with CANU. Generate new switch configurations with CANU and updated SLS. Review generated switch configurations. Apply switch configurations. Update CEC VLAN (if required). For more information on CANU, see the CANU v1.6.5 documentation.\nDISCLAIMER: This procedure is for standard mountain cabinet network configurations and does not account for any site customizations that have been made to the management network. Site administrators and support teams are responsible for knowing the customizations in effect in Shasta/CSM and configuring CANU to respect them when generating new network configurations.\nSee examples of using CANU custom switch configurations and examples of other CSM features that require custom configurations in the following documentation:\nManual Switch Configuration Example Custom Switch Configuration Example Verify that new hardware has been discovered.\nPerform the Hardware State Manager Discovery Validation procedure.\nAfter the management network has been reconfigured, it may take up to 10 minutes for the hardware in the new cabinets to become discovered.\nValidate BIOS and BMC firmware levels in the new nodes.\nPerform the procedures in Update Firmware with FAS. Perform updates as needed with FAS.\nSlingshot switches are updated with procedures from the Slingshot Operations Guide.\nConfigure BMC and controller parameters with SCSD\nThe System Configuration Service (SCSD) allows administrators to set various BMC and controller parameters for components in liquid-cooled cabinets. At this point SCSD should be used to set the SSH key in the node controllers (BMCs) to enable troubleshooting. If any of the nodes fail to power down or power up as part of the compute node booting process, it may be necessary to look at the logs on the BMC for node power down or node power up.\nSee Configure BMC and Controller Parameters with SCSD.\nContinue on to the Slingshot Operations Guide to bring up the additional cabinets.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/exec_banner/",
	"title": "Exec banners",
	"tags": [],
	"description": "",
	"content": "Exec banners Banners are custom messages displayed to users attempting to connect to the management interfaces. MOTD banners are displayed pre-login while exec banners are displayed post-login. Multiple lines of text can be stored using a custom delimiter to mark the end of message.\nRelevant Configuration\nCreate a banner.\nswitch(config)# banner motd Testing Show Commands to Validate Functionality.\nswitch# show banner Example Output\nufmapl [ mgmt-sa ] (config) # show banner Banners: MOTD: Mellanox UFM Appliance Login: Mellanox MLNX-OS UFM Appliance Management Expected Results:\nStep 1: You can create the banner Step 2: The output of the banner looks correct Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/physical_interfaces/",
	"title": "Configure Physical Interfaces",
	"tags": [],
	"description": "",
	"content": "Configure Physical Interfaces Ethernet port interfaces are enabled by default.\nConfiguration Commands Enable the interface:\nswitch(config)# interface ethernet 1/1/1 switch(conf-if-eth1/1/1)# no shutdown Disable the interface:\nswitch(config)# interface ethernet 1/1/1 switch(conf-if-eth1/1/1)# shutdown Show commands to validate functionality:\nswitch# show configuration Expected Results The switch recognizes the transceiver without errors Administrators can enter the interface context for the port and enable it Administrators can establish a link with a partner Administrators can pass traffic as expected Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/dns-client/",
	"title": "Configure Domain Name Service (DNS) Clients",
	"tags": [],
	"description": "",
	"content": "Configure Domain Name Service (DNS) Clients The Domain Name Service (DNS) translates domain and host names to and from IP addresses. A DNS client resolves hostnames to IP addresses by querying assigned DNS servers for the appropriate IP address.\nConfiguration Commands Configure the switch to resolve queries via a DNS server:\nswitch(config)# ip dns server-address IP-ADDR [vrf VRF] Configure a domain name:\nswitch(config)# ip dns domain-name NAME Show commands to validate functionality:\nswitch# show ip dns Expected Results Administrators can configure the DNS client The output is correct Administrators can ping the device Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/increase_pod_resource_limits/",
	"title": "Increase Pod Resource Limits",
	"tags": [],
	"description": "",
	"content": "Increase Pod Resource Limits Increase the appropriate resource limits for pods after determining if a pod is being CPU throttled or OOMKilled.\nReturn Kubernetes pods to a healthy state with resources available.\nPrerequisites The names of the pods hitting their resource limits are known. See Determine if Pods are Hitting Resource Limits. Procedure Determine the current limits of a pod.\nncn-w001# kubectl get po -n services POD_ID -o yaml Look for the following section returned in the output:\nresources: limits: cpu: \u0026#34;2\u0026#34; memory: 2Gi requests: cpu: 10m memory: 64Mi Determine which Kubernetes entity (etcdcluster, deployment, statefulset, etc) is creating the pod.\nThe Kubernetes entity can be found with either of the following options:\nFind the Kubernetes entity and grep for the pod in question.\nIn the following example, replace hbtd-etcd with the pod being used.\nncn-w001# kubectl get deployment,statefulset,etcdcluster,postgresql,daemonsets -A | grep hbtd-etcd Example output:\nservices etcdcluster.etcd.database.coreos.com/cray-hbtd-etcd 32d Describe the pod and look in the Labels section.\nThis section is helpful for tracking down which entity is creating the pod.\nncn-w001# kubectl describe pod -n services POD_ID Excerpt from example output:\nLabels: app=etcd etcd_cluster=cray-hbtd-etcd etcd_node=cray-hbtd-etcd-8r2scmpb58 Edit the entity.\nIn the example below, be sure to replace ENTITY_TYPE and ENTITY_NAME with the values determined in the previous step (in the example output for the following step, these would be etcdcluster and cray-hbtd-etcd, respectively).\nncn-w001# kubectl edit ENTITY_TYPE -n services ENTITY_NAME Increase the resource limits for the pod.\nresources: {} Replace the text above with the following section, increasing the limits values:\nresources: limits: cpu: \u0026#34;4\u0026#34; memory: 8Gi requests: cpu: 10m memory: 64Mi Run a rolling restart of the pods.\nncn-w001# kubectl get po -n services | grep ENTITY_NAME Example output:\ncray-hbtd-etcd-8r2scmpb58 1/1 Running 0 5d11h cray-hbtd-etcd-qvz4zzjzw2 1/1 Running 0 5d11h cray-hbtd-etcd-vzjzmbn6nr 1/1 Running 0 5d11h Kill the pods off one by one.\nWait for each replacement pod to come up and be in a Running state before proceeding to the next pod.\nncn-w001# kubectl -n services delete pod POD_ID Verify that all pods are now Running with a more recent age.\nncn-w001# kubectl get po -n services | grep ENTITY_NAME Example output:\ncray-hbtd-etcd-8r2scmpb58 1/1 Running 0 12s cray-hbtd-etcd-qvz4zzjzw2 1/1 Running 0 32s cray-hbtd-etcd-vzjzmbn6nr 1/1 Running 0 98s "
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/restore_hsm_postgres_from_backup/",
	"title": "Restore Hardware State Manager (HSM) Postgres Database from Backup",
	"tags": [],
	"description": "",
	"content": "Restore Hardware State Manager (HSM) Postgres Database from Backup This procedure can be used to restore the HSM Postgres database from a previously taken backup. This can be a manual backup created by the Create a Backup of the HSM Postgres Database procedure, or an automatic backup created by the cray-smd-postgresql-db-backup Kubernetes cronjob.\nPrerequisites Healthy System Layout Service (SLS). Recovered first if also affected.\nHealthy HSM Postgres Cluster.\nUse patronictl list on the HSM Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:\nncn# kubectl exec cray-smd-postgres-0 -n services -c postgres -it -- patronictl list Example output:\n+ Cluster: cray-smd-postgres (6975238790569058381) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres-0 | 10.44.0.40 | Leader | running | 1 | | | cray-smd-postgres-1 | 10.36.0.37 | | running | 1 | 0 | | cray-smd-postgres-2 | 10.42.0.42 | | running | 1 | 0 | +---------------------+------------+--------+---------+----+-----------+ Previously taken backup of the HSM Postgres cluster either a manual or automatic backup.\nCheck for any available automatic HSM Postgres backups:\nncn# cray artifacts list postgres-backup --format json | jq -r \u0026#39;.artifacts[].Key | select(contains(\u0026#34;smd\u0026#34;))\u0026#39; Example output:\ncray-smd-postgres-2021-07-11T23:10:08.manifest cray-smd-postgres-2021-07-11T23:10:08.psql Procedure Retrieve a previously taken HSM Postgres backup. This can be either a previously taken manual HSM backup or an automatic Postgres backup in the postgres-backup S3 bucket.\nFrom a previous manual backup:\nCopy over the folder or tarball containing the Postgres backup to be restored. If it is a tarball, extract it.\nSet the environment variable POSTGRES_SQL_FILE to point toward the .psql file in the backup folder:\nncn# export POSTGRES_SQL_FILE=/root/cray-smd-postgres-backup_2021-07-07_16-39-44/cray-smd-postgres-backup_2021-07-07_16-39-44.psql Set the environment variable POSTGRES_SECRET_MANIFEST to point toward the .manifest file in the backup folder:\nncn# export POSTGRES_SECRET_MANIFEST=/root/cray-smd-postgres-backup_2021-07-07_16-39-44/cray-smd-postgres-backup_2021-07-07_16-39-44.manifest From a previous automatic Postgres backup:\nCheck for available backups.\nncn# cray artifacts list postgres-backup --format json | jq -r \u0026#39;.artifacts[].Key | select(contains(\u0026#34;smd\u0026#34;))\u0026#39; Example output:\ncray-smd-postgres-2021-07-11T23:10:08.manifest cray-smd-postgres-2021-07-11T23:10:08.psql Set the following environment variables for the name of the files in the backup:\nncn# export POSTGRES_SECRET_MANIFEST_NAME=cray-smd-postgres-2021-07-11T23:10:08.manifest ncn# export POSTGRES_SQL_FILE_NAME=cray-smd-postgres-2021-07-11T23:10:08.psql Download the .psql file for the Postgres backup.\nncn# cray artifacts get postgres-backup \u0026#34;$POSTGRES_SQL_FILE_NAME\u0026#34; \u0026#34;$POSTGRES_SQL_FILE_NAME\u0026#34; Download the .manifest file for the HSM backup.\nncn# cray artifacts get postgres-backup \u0026#34;$POSTGRES_SECRET_MANIFEST_NAME\u0026#34; \u0026#34;$POSTGRES_SECRET_MANIFEST_NAME\u0026#34; Setup environment variables pointing to the full path of the .psql and .manifest files.\nncn# export POSTGRES_SQL_FILE=$(realpath \u0026#34;$POSTGRES_SQL_FILE_NAME\u0026#34;) ncn# export POSTGRES_SECRET_MANIFEST=$(realpath \u0026#34;$POSTGRES_SECRET_MANIFEST_NAME\u0026#34;) Verify the POSTGRES_SQL_FILE and POSTGRES_SECRET_MANIFEST environment variables are set correctly.\nncn# echo \u0026#34;$POSTGRES_SQL_FILE\u0026#34; /root/cray-smd-postgres-backup_2021-07-07_16-39-44/cray-smd-postgres-backup_2021-07-07_16-39-44.psql ncn# echo \u0026#34;$POSTGRES_SECRET_MANIFEST\u0026#34; /root/cray-smd-postgres-backup_2021-07-07_16-39-44/cray-smd-postgres-backup_2021-07-07_16-39-44.manifest Scale HSM to 0.\nncn# CLIENT=cray-smd ncn# POSTGRESQL=cray-smd-postgres ncn# NAMESPACE=services ncn# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=0 deployment.apps/cray-smd scaled ncn# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2; done Re-run the HSM loader job.\nncn# kubectl -n services get job cray-smd-init -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | kubectl replace --force -f - Wait for the job to complete:\nncn# kubectl wait -n services job cray-smd-init --for=condition=complete --timeout=5m Determine which Postgres member is the leader.\nncn# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- patronictl list Example output:\n+-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | Leader | running | 1 | | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | running | | 0 | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | running | | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ Create a variable for the identified leader:\nncn# POSTGRES_LEADER=cray-smd-postgres-0 Determine the database schema version of the currently running HSM database, and then verify that it matches the database schema version from the Postgres backup:\nDatabase schema of the currently running HSM Postgres instance.\nncn# kubectl exec $POSTGRES_LEADER -n services -c postgres -it -- bash -c \u0026#34;psql -U hmsdsuser -d hmsds -c \u0026#39;SELECT * FROM system\u0026#39;\u0026#34; Example output:\nid | schema_version | system_info ----+----------------+------------- 0 | 17 | {} (1 row) The output above shows the database schema is at version 17.\nDatabase schema version from the Postgres backup:\nncn# cat \u0026#34;$POSTGRES_SQL_FILE\u0026#34; | grep \u0026#34;COPY public.system\u0026#34; -A 2 COPY public.system (id, schema_version, dirty) FROM stdin; 0 17 f \\. The output above shows the database schema is at version 17.\nIf the database schema versions match, proceed to the next step. Otherwise, the Postgres backup taken is not applicable to the currently running instance of HSM.\nWARNING: If the database schema versions do not match the version of HSM deployed, they will need to be either upgraded/downgraded to a version with a compatible database schema version. Ideally, it will be to the same version of HSM that was used to create the Postgres backup.\nDelete and re-create the postgresql resource (which includes the PVCs).\nncn# CLIENT=cray-smd ncn# POSTGRESQL=cray-smd-postgres ncn# NAMESPACE=services ncn# kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; \u0026gt; postgres-cr.yaml ncn# kubectl delete -f postgres-cr.yaml postgresql.acid.zalan.do \u0026#34;cray-smd-postgres\u0026#34; deleted ncn# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2; done ncn# kubectl create -f postgres-cr.yaml postgresql.acid.zalan.do/cray-smd-postgres created ncn# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 3 ] ; do echo \u0026#34; waiting for pods to start running\u0026#34;; sleep 2; done Determine which Postgres member is the new leader.\nncn# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- patronictl list Example output:\n+-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | Leader | running | 1 | | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | running | | 0 | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | running | | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ Set a variable for the new leader:\nncn# POSTGRES_LEADER=cray-smd-postgres-0 Copy the dump taken above to the Postgres leader pod and restore the data.\nIf the dump exists in a different location, adjust this example as needed.\ncat ${POSTGRES_SQL_FILE} | kubectl exec ${POSTGRES_LEADER} -c postgres -n ${NAMESPACE} -it -- psql -U postgres Clear out of sync data from tables in Postgres.\nThe backup will have restored tables that may contain out of date information. To refresh this data, it must first be deleted.\nDelete the entries in the Ethernet Interfaces table. These will automatically get repopulated during rediscovery.\nncn# kubectl exec $POSTGRES_LEADER -n services -c postgres -it -- bash -c \u0026#34;psql -U hmsdsuser -d hmsds -c \u0026#39;DELETE FROM comp_eth_interfaces\u0026#39;\u0026#34; Restore the secrets.\nOnce the dump has been restored onto the newly built Postgres cluster, the Kubernetes secrets need to match with the Postgres cluster, otherwise the service will experience readiness and liveness probe failures because it will be unable to authenticate to the database.\nWith secrets manifest from an existing backup If the Postgres secrets were auto-backed up, then re-create the secrets in Kubernetes.\nDelete and re-create the four cray-smd-postgres secrets using the manifest set to POSTGRES_SECRET_MANIFEST in step 1 above.\nncn# kubectl delete secret postgres.cray-smd-postgres.credentials service-account.cray-smd-postgres.credentials hmsdsuser.cray-smd-postgres.credentials standby.cray-smd-postgres.credentials -n ${NAMESPACE} ncn# kubectl apply -f ${POSTGRES_SECRET_MANIFEST} Without the previous secrets from a backup If the Postgres secrets were not backed up, then update the secrets in Postgres.\nDetermine which Postgres member is the leader.\nncn# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- patronictl list Example output:\n+-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | Leader | running | 1 | | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | running | | 0 | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | running | | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ Set a variable for the leader:\nncn# POSTGRES_LEADER=cray-smd-postgres-0 Determine what secrets are associated with the Postgres credentials.\nncn# kubectl get secrets -n ${NAMESPACE} | grep \u0026#34;${POSTGRESQL}.credentials\u0026#34; Example output:\nservices hmsdsuser.cray-smd-postgres.credentials Opaque 2 31m services postgres.cray-smd-postgres.credentials Opaque 2 31m services service-account.cray-smd-postgres.credentials Opaque 2 31m services standby.cray-smd-postgres.credentials Opaque 2 31m For each secret above, get the username and password from Kubernetes and update the Postgres database with this information.\nFor example (hmsdsuser.cray-smd-postgres.credentials):\nncn# kubectl get secret hmsdsuser.cray-smd-postgres.credentials -n ${NAMESPACE} -ojsonpath=\u0026#39;{.data.username}\u0026#39; | base64 -d ncn# kubectl get secret hmsdsuser.cray-smd-postgres.credentials -n ${NAMESPACE} -ojsonpath=\u0026#39;{.data.password}\u0026#39;| base64 -d Exec into the leader pod to reset the user\u0026rsquo;s password:\nncn# kubectl exec ${POSTGRES_LEADER} -n ${NAMESPACE} -c postgres -it -- bash root@cray-smd-postgres-0:/home/postgres# /usr/bin/psql postgres postgres postgres=# ALTER USER hmsdsuser WITH PASSWORD \u0026#39;ABCXYZ\u0026#39;; ALTER ROLE postgres=# Continue the above process until all ${POSTGRESQL}.credentials secrets have been updated in the database.\nRestart the Postgres cluster.\nncn# kubectl delete pod \u0026#34;${POSTGRESQL}-0\u0026#34; \u0026#34;${POSTGRESQL}-1\u0026#34; \u0026#34;${POSTGRESQL}-2\u0026#34; -n ${NAMESPACE} ncn# while [ $(kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ]; do echo \u0026#34;waiting for ${POSTGRESQL} to start running\u0026#34;; sleep 2; done Scale the client service back to 3.\nncn# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=3 ncn# kubectl -n ${NAMESPACE} rollout status deployment ${CLIENT} Verify that the service is functional.\nncn# cray hsm service ready list Example output:\ncode = 0 message = \u0026#34;HSM is healthy\u0026#34; Get the number of node objects stored in HSM:\nncn# cray hsm state components list --type node --format json | jq .Components[].ID | wc -l Resync the component state and inventory.\nAfter restoring HSM\u0026rsquo;s Postgres from a back up, some of the transient data like component state and hardware inventory may be out of sync with reality. This involves kicking off an HSM rediscovery.\nncn# endpoints=$(cray hsm inventory redfishEndpoints list --format json | jq -r \u0026#39;.[]|.[]|.ID\u0026#39;) ncn# for e in $endpoints; do cray hsm inventory discover create --xnames ${e}; done Wait for discovery to complete. Discovery is complete after there are no redfishEndpoints left in the \u0026lsquo;DiscoveryStarted\u0026rsquo; state. A value of 0 will be returned.\nncn# cray hsm inventory redfishEndpoints list --format json | grep -c \u0026#34;DiscoveryStarted\u0026#34; Check for discovery errors.\nncn# cray hsm inventory redfishEndpoints list --format json | grep LastDiscoveryStatus | grep -v -c \u0026#34;DiscoverOK\u0026#34; If any of the RedfishEndpoint entries have a LastDiscoveryStatus other than DiscoverOK after discovery has completed, refer to the Troubleshoot Issues with Redfish Endpoint Discovery procedure for guidance.\nPerform this step only if the system has Intel management NCNs, otherwise for HPE or Gigabyte management NCNs skip this step. Due to known firmware issues on Intel BMCs they do not report the MAC addresses of the management NICs via Redfish, and when the BMC is discovered after restoring from a Postgres backup the management NIC MACs in HSM will have an empty component ID. The following script will correct any Ethernet Interfaces for a Intel management NCN without a component ID.\nncn# \\ UNKNOWN_NCN_MAC_ADDRESSES=$(cray hsm inventory ethernetInterfaces list --component-id \u0026#34;\u0026#34; --format json | jq \u0026#39;.[] | select(.Description == \u0026#34;- kea\u0026#34;) | .MACAddress\u0026#39; -r) for UNKNOWN_MAC_ADDRESS in $UNKNOWN_NCN_MAC_ADDRESSES; do XNAME=$(cray bss bootparameters list --format json | jq --arg MAC \u0026#34;${UNKNOWN_MAC_ADDRESS}\u0026#34; \u0026#39;.[] | select(.params != null) | select(.params | test($MAC)) | .hosts[]\u0026#39; -r) if [[ $(wc -l \u0026lt;\u0026lt;\u0026lt; $(printf $XNAME)) -ne 1 ]]; then echo \u0026#34;MAC Address ${UNKNOWN_MAC_ADDRESS} unexpected number matches found. Expected 1 match, but found: $(wc -l \u0026lt;\u0026lt;\u0026lt; $(printf $XNAME))\u0026#34; continue fi echo \u0026#34;MAC: ${UNKNOWN_MAC_ADDRESS} is ${XNAME}\u0026#34; EI_ID=$(echo \u0026#34;$UNKNOWN_MAC_ADDRESS\u0026#34; | sed \u0026#39;s/://g\u0026#39;) echo \u0026#34;Updating ${EI_ID} in HSM EthernetInterfaces with component ID ${XNAME}\u0026#34; cray hsm inventory ethernetInterfaces update ${EI_ID} --component-id ${XNAME} done "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/create_and_populate_a_vcs_configuration_repository/",
	"title": "Create and Populate a VCS Configuration Repository",
	"tags": [],
	"description": "",
	"content": "Create and Populate a VCS Configuration Repository Create a new repository in the VCS and populate it with content for site customizations in a custom Configuration Framework Service (CFS) configuration layer.\nPrerequisites The Version Control Service (VCS) login credentials for the crayvcs user are set up. See VCS Administrative User in Version Control Service (VCS) for more information. Procedure Create the empty repository in VCS.\nReplace the CRAYVCS_PASSWORD value in the following command before running it.\nncn-mw# curl -X POST https://api-gw-service-nmn.local/vcs/api/v1/org/cray/repos \\ -d \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;NEW_REPO\u0026#34;, \u0026#34;private\u0026#34;: true}\u0026#39; -u crayvcs:CRAYVCS_PASSWORD \\ -H \u0026#34;Content-Type: application/json\u0026#34; Clone the empty VCS repository.\nncn-mw# git clone https://api-gw-service-nmn.local/vcs/cray/NEW_REPO.git Change to the directory of the empty Git repository and populate it with content.\nncn-mw# cd NEW_REPO ncn-mw# cp -a ~/user/EXAMPLE-config-management/* . Add the new content, commit it, and push it to VCS.\nThe following command will move the content to the master branch of the repository.\nncn-mw# git add --all \u0026amp;\u0026amp; git commit -m \u0026#34;Initial config\u0026#34; \u0026amp;\u0026amp; git push Retrieve the Git hash for the CFS layer definition.\nncn-mw# git rev-parse --verify HEAD "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/healthy_compute_node_boot_process/",
	"title": "Healthy Compute Node Boot Process",
	"tags": [],
	"description": "",
	"content": "Healthy Compute Node Boot Process In order to investigate node boot-related issues, it is important to understand the flow of a healthy boot process and the associated components. This section outlines the normal flow of components that play a role in booting compute nodes, including DHCP, BSS, and TPTP.\nDHCP A healthy DHCP exchange between server and client looks like the following:\nTraffic Description Sender DHCP Discover A broadcast request from the client requesting an IP address. The request contains the client\u0026rsquo;s MAC address. Client DHCP Offer The server offers an IP address to the client. Server DHCP Request After testing the IP address to see that it is not in use, the client requests the proffered IP address. Client DHCP ACK The server acknowledges that the client owns the lease on the IP address. Server The following figure shows what a healthy DHCP discover process looks like via Wireshark, which is a packet analyzer:\nThe DHCP client uses port 68, whereas the DHCP server uses port 67. Unlike most Kubernetes pods, the DHCP pod is located on the host network.\nTFTP A healthy TFTP exchange between server and client looks like the following.\nTraffic Description Sender Read Request File: filename tsize=0 The client requests a file with a tsize equal to zero. Client Option Acknowledgement The server acknowledges the request and provides the file\u0026rsquo;s size and block transfer size. Server Error Code, Code: Option negotiation failed, Message: User aborted the transfer The client aborts the transfer once it determines the file size. Client Read Request File: filename The client requests the file again. Client Option Acknowledgement The server acknowledges the request and provides the block transfer size. Server Acknowledgement, Block: 0 The client acknowledges the server. Client Data Packet, Block: 1 The server sends the first data packet. Server Acknowledgement, Block: 1 The client acknowledges reception of block 1. Client The last two steps repeat until the file transfer is complete. The last block from the server will be labeled as (Last). The TFTP server listens on port 69. Kubernetes forwards port 69 on every node in the Kubernetes cluster to the TFTP pod.\nBoot Script Service (BSS) A healthy transaction with the Boot Script Service (BSS) looks similar to the following:\nncn-mw# cray bss bootscript list --mac a4:bf:01:3e:c0:a2 Example output (lines truncated because of extreme length):\n#!ipxe kernel --name kernel http://rgw.local:8080/boot-images/29c2cc23-a9d6-4e9a-ab1a-b5fa9270c975/kernel?X-Amz-A... initrd --name initrd http://rgw.local:8080/boot-images/29c2cc23-a9d6-4e9a-ab1a-b5fa9270c975/initrd?X-Amz-A... boot || goto boot_retry :boot_retry sleep 30 chain https://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript?mac=a4:bf:01:3e:f9:28\u0026amp;retry=1 "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/customize_the_broker_uai_image/",
	"title": "Customize the Broker UAI Image",
	"tags": [],
	"description": "",
	"content": "Customize the Broker UAI Image The Broker UAI image that comes with UAS is the image used to construct Broker UAIs.\nThe key pieces of the Broker UAI image are:\nAn entrypoint shell script that initializes the container and starts the SSH daemon running. An SSH configuration that forces logged in users into the switchboard command which creates / selects End-User UAIs and redirects connections. The primary way to customize the Broker UAI image is by defining volumes and connecting them to the Broker UAI class for a given broker. Some customizations may require action that cannot be covered simply by using volumes to override configuration. Those cases generally require changing the Broker UAI behavior in some way. Those cases can be covered either by volume mounting a customized entrypoint script, or volume mounting a customized SSH configuration. Both of these cases are shown in the following examples.\nCustomize the Broker UAI entrypoint Script The Broker UAI entrypoint script runs once every time the Broker UAI starts. It resides at /app/broker/entrypoint.sh in the Broker UAI image. The entrypoint script is the only file in that directory, so it can be overridden by creating a Kubernetes ConfigMap in the uas namespace containing the modified script and creating a volume using that ConfigMap with a mount point of /app/broker. There is critical content in the entrypoint script that should not be modified.\nThe following shows the contents of an unmodified script:\n#!/bin/bash # MIT License # # (C) Copyright [2020] Hewlett Packard Enterprise Development LP # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the \u0026#34;Software\u0026#34;), # to deal in the Software without restriction, including without limitation # the rights to use, copy, modify, merge, publish, distribute, sublicense, # and/or sell copies of the Software, and to permit persons to whom the # Software is furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED \u0026#34;AS IS\u0026#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL # THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR # OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, # ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR # OTHER DEALINGS IN THE SOFTWARE. echo \u0026#34;Configure PAM to use sssd...\u0026#34; pam-config -a --sss --mkhomedir echo \u0026#34;Generating broker host keys...\u0026#34; ssh-keygen -A echo \u0026#34;Checking for UAI_CREATION_CLASS...\u0026#34; if ! [ -z $UAI_CREATION_CLASS ]; then echo UAI_CREATION_CLASS=$UAI_CREATION_CLASS \u0026gt;\u0026gt; /etc/environment fi echo \u0026#34;Starting sshd...\u0026#34; /usr/sbin/sshd -f /etc/switchboard/sshd_config echo \u0026#34;Starting sssd...\u0026#34; sssd sleep infinity Starting at the top:\npam_config ... can be customized to set up PAM as needed. The configuration here assumes the broker is using SSSD to reach a directory server for authentication and that, if a home directory is not present for a user at login, one should be made on the broker. The ssh-keygen... part is needed to set up the SSH host key for the broker and should be left alone. The UAI_CREATION_CLASS code should be left alone, as it sets up information used by switchboard to create End-User UAIs. The /usr/sbin/sshd... part starts the SSH server on the broker and should be left alone. Configuration of SSH is covered in the next section and is done by replacing /etc/switchboard/sshd_config not by modifying this line. The sssd part assumes the broker is using SSSD to reach a directory server, it can be changed as needed. The sleep infinity prevents the script from exiting which keeps the Broker UAI running. It should not be removed or altered. As long as the basic flow and contents described here are honored, other changes to this script should work without compromising the Broker UAI\u0026rsquo;s function.\nThe following is an example of replacing the entrypoint script with a new entrypoint script that changes the SSSD invocation to explicitly specify the sssd.conf file path (the standard path is used here, but a different path might make customizing SSSD for a given site simpler under some set of circumstances):\nCreate a new entrypoint script.\nNOTE: A special \u0026ldquo;here document\u0026rdquo; form is used to prevent variable substitution in the file.\nncn-m001-pit# cat \u0026lt;\u0026lt;-\u0026#34;EOF\u0026#34; \u0026gt; entrypoint.sh #!/bin/bash # MIT License # # (C) Copyright [2020] Hewlett Packard Enterprise Development LP # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the \u0026#34;Software\u0026#34;), # to deal in the Software without restriction, including without limitation # the rights to use, copy, modify, merge, publish, distribute, sublicense, # and/or sell copies of the Software, and to permit persons to whom the # Software is furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED \u0026#34;AS IS\u0026#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL # THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR # OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, # ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR # OTHER DEALINGS IN THE SOFTWARE. echo \u0026#34;Configure PAM to use sssd...\u0026#34; pam-config -a --sss --mkhomedir echo \u0026#34;Generating broker host keys...\u0026#34; ssh-keygen -A echo \u0026#34;Checking for UAI_CREATION_CLASS...\u0026#34; if ! [ -z $UAI_CREATION_CLASS ]; then echo UAI_CREATION_CLASS=$UAI_CREATION_CLASS \u0026gt;\u0026gt; /etc/environment fi echo \u0026#34;Starting sshd...\u0026#34; /usr/sbin/sshd -f /etc/switchboard/sshd_config echo \u0026#34;Starting sssd...\u0026#34; # LOCAL MODIFICATION # change the normal SSSD invocation # sssd # to specify the config file path sssd --config /etc/sssd/sssd.conf # END OF LOCAL MODIFICATION sleep infinity EOF Create a new ConfigMap with the content from the script.\nncn-m001-pit# kubectl create configmap -n uas broker-entrypoint --from-file=entrypoint.sh Create a new volume.\nNOTE: The default_mode setting, which will set the mode on the file /app/broker/entrypoint.sh is decimal 493 here instead of octal 0755. The octal notation is not permitted in a JSON specification. Decimal numbers have to be used.\nncn-m001-pit# cray uas admin config volumes create --mount-path /app/broker --volume-description \u0026#39;{\u0026#34;config_map\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;broker-entrypoint\u0026#34;, \u0026#34;default_mode\u0026#34;: 493}}\u0026#39; --volumename broker-entrypoint Example output:\nmount_path = \u0026#34;/app/broker\u0026#34; volume_id = \u0026#34;2246bbb1-4006-4b11-ba57-6588a7b7c02f\u0026#34; volumename = \u0026#34;broker-entrypoint\u0026#34; [volume_description.config_map] default_mode = 493 name = \u0026#34;broker-entrypoint\u0026#34; List the UAI classes.\nncn-m001-pit# cray uas admin config classes list | grep -e class_id -e comment Example output:\nclass_id = \u0026#34;5eb523ba-a3b7-4a39-ba19-4cfe7d19d296\u0026#34; comment = \u0026#34;UAI Class to Create Non-Brokered End-User UAIs\u0026#34; class_id = \u0026#34;bdb4988b-c061-48fa-a005-34f8571b88b4\u0026#34; comment = \u0026#34;UAI Class to Create Brokered End-User UAIs\u0026#34; comment = \u0026#34;Resource Specification to use with Brokered End-User UAIs\u0026#34; class_id = \u0026#34;d764c880-41b8-41e8-bacc-f94f7c5b053d\u0026#34; comment = \u0026#34;UAI broker class\u0026#34; Describe the desired UAI class.\nncn-m001-pit# cray uas admin config classes describe d764c880-41b8-41e8-bacc-f94f7c5b053d --format yaml Example output:\nclass_id: d764c880-41b8-41e8-bacc-f94f7c5b053d comment: UAI broker class default: false image_id: 8f180ddc-37e5-4ead-b261-2b401914a79f namespace: uas opt_ports: [] priority_class_name: uai-priority public_ip: true replicas: 1 resource_config: resource_id: service_account: timeout: tolerations: uai_compute_network: false uai_creation_class: bdb4988b-c061-48fa-a005-34f8571b88b4 uai_image: default: false image_id: 8f180ddc-37e5-4ead-b261-2b401914a79f imagename: registry.local/cray/cray-uai-broker:1.2.4 volume_list: - 11a4a22a-9644-4529-9434-d296eef2dc48 - 1ec36af0-d5b6-4ad9-b3e8-755729765d76 - a3b149fd-c477-41f0-8f8d-bfcee87fdd0a volume_mounts: - mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: 11a4a22a-9644-4529-9434-d296eef2dc48 volumename: timezone - mount_path: /etc/sssd volume_description: secret: default_mode: 384 secret_name: broker-sssd-conf volume_id: 1ec36af0-d5b6-4ad9-b3e8-755729765d76 volumename: broker-sssd-config - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: a3b149fd-c477-41f0-8f8d-bfcee87fdd0a volumename: lustre Update the UAI class.\nncn-m001-pit# cray uas admin config classes update \\ --volume-list \u0026#39;11a4a22a-9644-4529-9434-d296eef2dc48,1ec36af0-d5b6-4ad9-b3e8-755729765d76,2246bbb1-4006-4b11-ba57-6588a7b7c02f,a3b149fd-c477-41f0-8f8d-bfcee87fdd0a\u0026#39; \\ d764c880-41b8-41e8-bacc-f94f7c5b053d --format yaml Example output:\nclass_id: d764c880-41b8-41e8-bacc-f94f7c5b053d comment: UAI broker class default: false image_id: 8f180ddc-37e5-4ead-b261-2b401914a79f namespace: uas opt_ports: [] priority_class_name: uai-priority public_ip: true replicas: 1 resource_config: resource_id: service_account: timeout: tolerations: uai_compute_network: false uai_creation_class: bdb4988b-c061-48fa-a005-34f8571b88b4 uai_image: default: false image_id: 8f180ddc-37e5-4ead-b261-2b401914a79f imagename: registry.local/cray/cray-uai-broker:1.2.4 volume_list: - 11a4a22a-9644-4529-9434-d296eef2dc48 - 1ec36af0-d5b6-4ad9-b3e8-755729765d76 - 2246bbb1-4006-4b11-ba57-6588a7b7c02f - a3b149fd-c477-41f0-8f8d-bfcee87fdd0a volume_mounts: - mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: 11a4a22a-9644-4529-9434-d296eef2dc48 volumename: timezone - mount_path: /etc/sssd volume_description: secret: default_mode: 384 secret_name: broker-sssd-conf volume_id: 1ec36af0-d5b6-4ad9-b3e8-755729765d76 volumename: broker-sssd-config - mount_path: /app/broker volume_description: config_map: default_mode: 493 name: broker-entrypoint volume_id: 2246bbb1-4006-4b11-ba57-6588a7b7c02f volumename: broker-entrypoint - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: a3b149fd-c477-41f0-8f8d-bfcee87fdd0a volumename: lustre After the Broker UAI class is updated, all that remains is to clear out any existing End-User UAIs (existing UAIs will not work with the new broker because the new broker will have a new key-pair shared with its UAIs) and the existing Broker UAI (if any) and create a new Broker UAI.\nNOTE: Clearing out existing UAIs will terminate any user activity on those UAIs, make sure that users are warned of the disruption.\nClear out the UAIs.\nncn-m001-pit# cray uas admin uais delete --class-id bdb4988b-c061-48fa-a005-34f8571b88b4 ncn-m001-pit# cray uas admin uais delete --class-id d764c880-41b8-41e8-bacc-f94f7c5b053d Output similar to results = [ \u0026quot;Successfully deleted uai-vers-e937b810\u0026quot;,] will be returned for each command.\nRestart the broker.\nncn-m001-pit# cray uas admin uais create --class-id d764c880-41b8-41e8-bacc-f94f7c5b053d --owner broker Example output:\nuai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh broker@34.136.140.107\u0026#34; uai_host = \u0026#34;ncn-w003\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-broker:1.2.4\u0026#34; uai_ip = \u0026#34;34.136.140.107\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-broker-f5bfb28c\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;broker\u0026#34; [uai_portmap] Customize the Broker UAI SSH Configuration The SSH configuration used on Broker UAIs resides in /etc/switchboard/sshd_config and contains the following:\nPort 30123 AuthorizedKeysFile .ssh/authorized_keys UsePAM yes X11Forwarding yes Subsystem sftp /usr/lib/ssh/sftp-server AcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES AcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT AcceptEnv LC_IDENTIFICATION LC_ALL AcceptEnv UAI_ONE_SHOT UseDNS no Match User !root,* PermitTTY yes ForceCommand /usr/bin/switchboard broker --class-id $UAI_CREATION_CLASS The important content here is as follows:\nPort 30123 tells SSHD to listen on a port that can be reached through port forwarding by the publicly visible Kubernetes service. The UseDNS no avoids any DNS issues resulting from the Broker UAI running in the Kubernetes network space. The permitTTY yes setting permits interactive UAI logins. The ForceCommand ... statement ensures that users are always sent on to End-User UAIs or drop out of the Broker UAI on failure, preventing users from directly accessing the Broker UAI. The AcceptEnv UAI_ONE_SHOT setting is not required, but it allows a user to set the UAI_ONE_SHOT variable which instructs the broker to delete any created End-User UAI after the user logs out. These should be left unchanged. The rest of the configuration can be customized as needed.\nThe following is an example that follows on from the previous section and configures SSH to provide a pre-login banner. Both a new banner file and a new sshd_config are placed in a Kubernetes ConfigMap and mounted over /etc/switchboard:\nCreate a new pre-login banner file.\nNOTE: A special \u0026ldquo;here document\u0026rdquo; form is used to prevent variable substitution in the file.\nncn-m001-pit# cat \u0026lt;\u0026lt;-\u0026#34;EOF\u0026#34; \u0026gt; banner Here is a banner that will be displayed before login on the Broker UAI EOF Create a new sshd_config.\nNOTE: A special \u0026ldquo;here document\u0026rdquo; form is used to prevent variable substitution in the file.\nncn-m001-pit# cat \u0026lt;\u0026lt;-\u0026#34;EOF\u0026#34; \u0026gt; sshd_config Port 30123 AuthorizedKeysFile .ssh/authorized_keys UsePAM yes X11Forwarding yes Subsystem sftp /usr/lib/ssh/sftp-server AcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES AcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT AcceptEnv LC_IDENTIFICATION LC_ALL AcceptEnv UAI_ONE_SHOT UseDNS no Banner /etc/switchboard/banner Match User !root,* PermitTTY yes ForceCommand /usr/bin/switchboard broker --class-id $UAI_CREATION_CLASS EOF Add the new banner file and sshd_config to a Kubernetes ConfigMap.\nncn-m001-pit# kubectl create configmap -n uas broker-sshd-conf --from-file sshd_config --from-file banner Mount the changes over /etc/switchboard.\nncn-m001-pit# cray uas admin config volumes create \\ --mount-path /etc/switchboard \\ --volume-description \u0026#39;{\u0026#34;config_map\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;broker-sshd-conf\u0026#34;, \u0026#34;default_mode\u0026#34;: 384}}\u0026#39; \\ --volumename broker-sshd-config Example output:\nmount_path = \u0026#34;/etc/switchboard\u0026#34; volume_id = \u0026#34;4577eddf-d81e-40c9-9c91-082f3193edd6\u0026#34; volumename = \u0026#34;broker-sshd-config\u0026#34; [volume_description.config_map] default_mode = 384 name = \u0026#34;broker-sshd-conf\u0026#34; Update the UAI class.\nncn-m001-pit# cray uas admin config classes update \\ --volume-list \u0026#39;4577eddf-d81e-40c9-9c91-082f3193edd6,11a4a22a-9644-4529-9434-d296eef2dc48,1ec36af0-d5b6-4ad9-b3e8-755729765d76,2246bbb1-4006-4b11-ba57-6588a7b7c02f,a3b149fd-c477-41f0-8f8d-bfcee87fdd0a\u0026#39; \\ d764c880-41b8-41e8-bacc-f94f7c5b053d --format yaml Example output:\nclass_id: d764c880-41b8-41e8-bacc-f94f7c5b053d comment: UAI broker class default: false image_id: 8f180ddc-37e5-4ead-b261-2b401914a79f namespace: uas opt_ports: [] priority_class_name: uai-priority public_ip: true replicas: 1 resource_config: resource_id: service_account: timeout: tolerations: uai_compute_network: false uai_creation_class: bdb4988b-c061-48fa-a005-34f8571b88b4 uai_image: default: false image_id: 8f180ddc-37e5-4ead-b261-2b401914a79f imagename: registry.local/cray/cray-uai-broker:1.2.4 volume_list: - 4577eddf-d81e-40c9-9c91-082f3193edd6 - 11a4a22a-9644-4529-9434-d296eef2dc48 - 1ec36af0-d5b6-4ad9-b3e8-755729765d76 - 2246bbb1-4006-4b11-ba57-6588a7b7c02f - a3b149fd-c477-41f0-8f8d-bfcee87fdd0a volume_mounts: - mount_path: /etc/switchboard volume_description: config_map: default_mode: 384 name: broker-sshd-conf volume_id: 4577eddf-d81e-40c9-9c91-082f3193edd6 volumename: broker-sshd-config - mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: 11a4a22a-9644-4529-9434-d296eef2dc48 volumename: timezone - mount_path: /etc/sssd volume_description: secret: default_mode: 384 secret_name: broker-sssd-conf volume_id: 1ec36af0-d5b6-4ad9-b3e8-755729765d76 volumename: broker-sssd-config - mount_path: /app/broker volume_description: config_map: default_mode: 493 name: broker-entrypoint volume_id: 2246bbb1-4006-4b11-ba57-6588a7b7c02f volumename: broker-entrypoint - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: a3b149fd-c477-41f0-8f8d-bfcee87fdd0a volumename: lustre Once the new configuration is installed, clean out the old UAIs and restart the broker.\nNOTE: Clearing out existing UAIs will terminate any user activity on those UAIs, make sure that users are warned of the disruption.\nClean out the old UAIs.\nncn-m001-pit# cray uas admin uais delete --class-id bdb4988b-c061-48fa-a005-34f8571b88b4 ncn-m001-pit# cray uas admin uais delete --class-id d764c880-41b8-41e8-bacc-f94f7c5b053d Output similar to results = [ \u0026quot;Successfully deleted uai-vers-e937b810\u0026quot;,] will be returned for each command.\nRestart the broker.\nncn-m001-pit# cray uas admin uais create --class-id d764c880-41b8-41e8-bacc-f94f7c5b053d --owner broker Example output:\nuai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh broker@104.197.32.33\u0026#34; uai_host = \u0026#34;ncn-w003\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-broker:1.2.4\u0026#34; uai_ip = \u0026#34;104.197.32.33\u0026#34; uai_msg = \u0026#34;PodInitializing\u0026#34; uai_name = \u0026#34;uai-broker-ed144660\u0026#34; uai_status = \u0026#34;Waiting\u0026#34; username = \u0026#34;broker\u0026#34; [uai_portmap] Connect to the broker to log in:\nvers\u0026gt; ssh vers@104.197.32.33 Here is a banner that will be displayed before login to SSH on Broker UAIs Password: Top: User Access Service (UAS)\nNext Topic: Customize End-User UAI Images\n"
},
{
	"uri": "/docs-csm/en-12/install/create_ncn_metadata_csv/",
	"title": "Create NCN Metadata CSV",
	"tags": [],
	"description": "",
	"content": "Create NCN Metadata CSV The information in the ncn_metadata.csv file identifies each of the management nodes, assigns the function as a master, worker, or storage node, and provides the MAC address information needed to identify the BMC and the NIC which will be used to boot the node.\nSome of the data in the ncn_metadata.csv can be found in the SHCD in the HMN tab. However, the hardest data to collect is the MAC addresses for the node\u0026rsquo;s BMC, the node\u0026rsquo;s bootable network interface, and the pair of network interfaces which will become the bonded interface bond0.\nTopics Introduction LACP Bonding PXE or BOOTSTRAP MAC Sample ncn_metadata.csv Collection of MAC Addresses Details Introduction Each of the management nodes is represented as a row in the ncn_metadata.csv file.\nFor example:\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Storage,94:40:c9:37:77:26,14:02:ec:d9:76:88,14:02:ec:d9:76:88,94:40:c9:5f:b6:92 For each management node, the component name (xname), role, and subrole can be extracted from the SHCD in the HMN tab. However, the rest of the MAC address information needs to be collected another way.\nCheck the description for component names while mapping names between the SHCD and the ncn_metadata.csv file. See Component Names (xnames).\nThere are two interesting parts to the NCN metadata file:\nThe MAC of the BMC The MAC(s) of the shasta-network interface(s) The \u0026ldquo;shasta-network interface\u0026rdquo; is the interfaces, one or more, that comprise the NCNs\u0026rsquo; LACP link-aggregation ports.\nLACP Bonding NCNs may have one or more bond interfaces, which may be comprised from one or more physical interfaces. The preferred default configuration is two physical network interfaces per bond. The number of bonds themselves depends on the systems network topology.\nFor example, systems with 4 network interfaces on a given node could configure either of these permutations (for redundancy minimums within Shasta cluster):\nOne bond with 4 interfaces (bond0) Two bonds with 2 interfaces each (bond0 and bond1) For more information, see NCN Networking page for NCNs.\nPXE or BOOTSTRAP MAC In general this refers to the interface to be used when the node attempts to PXE boot. This varies between vintages of systems; systems before \u0026ldquo;Spring 2020\u0026rdquo; often booted NCNs with onboard NICs, newer systems boot over their PCIe cards.\nIf the system is booting over PCIe then the \u0026ldquo;bootstrap MAC\u0026rdquo; and the \u0026ldquo;bond0 MAC0\u0026rdquo; will be identical. If the system is booting over onboard NICs then the \u0026ldquo;bootstrap MAC\u0026rdquo; and the \u0026ldquo;bond0 MAC0\u0026rdquo; will be different.\nOther Nomenclature\n\u0026ldquo;BOND MACS\u0026rdquo; are the MAC addresses for the physical interfaces that the node will use for the various VLANs. BOND0 MAC0 and BOND0 MAC1 should not be on the same physical network card to establish redundancy for failed chips. On the other hand, if any nodes\u0026rsquo; capacity prevents it from being redundant, then MAC1 and MAC0 will still produce a valid configuration if they do reside on the same physical chip/card. The BMC MAC is the exclusive, dedicated LAN for the onboard BMC. It should not be swapped with any other device. Sample ncn_metadata.csv The following are sample rows from a ncn_metadata.csv file:\nUse case: NCN with a single PCIe card (1 card with 2 ports):\nNotice how the MAC address for Bond0 MAC0 and Bond0 MAC1 are only off by 1, which indicates that they are on the same 2 port card.\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s6b0n0,Management,Worker,94:40:c9:37:77:b8,14:02:ec:da:bb:00,14:02:ec:da:bb:00,14:02:ec:da:bb:01 Use case: NCN with a dual PCIe cards (2 cards with 2 ports each for 4 ports total):\nNotice how the MAC address for Bond0 MAC0 and Bond0 MAC1 have a difference greater than 1, which indicates that they are on not on the same 2 port same card.\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Storage,94:40:c9:37:77:26,14:02:ec:d9:76:88,14:02:ec:d9:76:88,94:40:c9:5f:b6:92 Example ncn_metadata.csv file for a system that has been configured as follows:\nManagement NCNs are configured to boot over the PCIe NICs Master and Storage management NCNs have two 2 port PCIe cards Worker management NCNs have one 2 port PCIe card Because the NCNs have been configured to boot over their PCIe NICs, the Bootstrap MAC and Bond0 MAC0 columns have the same value.\nIMPORTANT: Mind the index for each group of nodes (3, 2, 1\u0026hellip;. ; not 1, 2, 3). If storage nodes are ncn-s001 x3000c0s7b0n0, ncn-s002 x3000c0s8b0n0, ncn-s003 x3000c0s9b0n0, then their portion of the file would be ordered x3000c0s9b0n0, x3000c0s8b0n0, x3000c0s7b0n0.\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Storage,94:40:c9:37:77:26,14:02:ec:d9:76:88,14:02:ec:d9:76:88,94:40:c9:5f:b6:92 x3000c0s8b0n0,Management,Storage,94:40:c9:37:87:5a,14:02:ec:d9:7b:c8,14:02:ec:d9:7b:c8,94:40:c9:5f:b6:5c x3000c0s7b0n0,Management,Storage,94:40:c9:37:0a:2a,14:02:ec:d9:7c:88,14:02:ec:d9:7c:88,94:40:c9:5f:9a:a8 x3000c0s6b0n0,Management,Worker,94:40:c9:37:77:b8,14:02:ec:da:bb:00,14:02:ec:da:bb:00,14:02:ec:da:bb:01 x3000c0s5b0n0,Management,Worker,94:40:c9:35:03:06,14:02:ec:d9:76:b8,14:02:ec:d9:76:b8,14:02:ec:d9:76:b9 x3000c0s4b0n0,Management,Worker,94:40:c9:37:67:60,14:02:ec:d9:7c:40,14:02:ec:d9:7c:40,14:02:ec:d9:7c:41 x3000c0s3b0n0,Management,Master,94:40:c9:37:04:84,14:02:ec:d9:79:e8,14:02:ec:d9:79:e8,94:40:c9:5f:b5:cc x3000c0s2b0n0,Management,Master,94:40:c9:37:f9:b4,14:02:ec:da:b8:18,14:02:ec:da:b8:18,94:40:c9:5f:a3:a8 x3000c0s1b0n0,Management,Master,94:40:c9:37:87:32,14:02:ec:da:b9:98,14:02:ec:da:b9:98,14:02:ec:da:b9:99 Collection of MAC Addresses Collect as much information as possible for the ncn_metadata.csv file before the PIT node is booted from the LiveCD and then get the rest later when directed. Having dummy MAC addresses, such as de:ad:be:ef:00:00, in the ncn_metadata.csv file is acceptable until the point during the install at which the management network switches have been configured and the PIT node can be used to collect the information. The correct MAC addresses are needed before attempting to boot the management nodes with their real image in Deploy Management Nodes\nIf the nodes are booted to Linux, then the data can be collected by ipmitool lan print for the BMC MAC, and the ip address command for the other NICs. This is rarely the case for a first time install. The PIT node examples of using these two commands could be extrapolated for other nodes which are booted to Linux. See the PIT node examples in Collecting BMC MAC Addresses and Collecting NCN MAC Addresses.\nIf the nodes are powered up and there is SSH access to the spine and leaf-bmc switches, it is possible to collect information from the spine and leaf-bmc switches.\nThe BMC MAC address can be collected from the switches using knowledge about the cabling of the HMN from the SHCD. See Collecting BMC MAC Addresses. The node MAC addresses cannot be collected until after the PIT node has booted from the LiveCD. At that point, a partial boot of the management nodes can be done to collect the remaining information from the conman console logs on the PIT node using the Procedure: iPXE Consoles If the nodes are powered up and there is no SSH access to the spine and leaf switches, it is possible to connect to the spine and leaf switches using the method described in Connect to Switch over USB-Serial Cable.\nThe BMC MAC address can be collected from the switches using knowledge about the cabling of the HMN from the SHCD. See Collecting BMC MAC Addresses. The node MAC addresses cannot be collected until after the PIT node has booted from the LiveCD. At that point, a partial boot of the management nodes can be done to collect the remaining information from the conman console logs on the PIT node using the Procedure: iPXE Consoles In all other cases, the full information needed for ncn_metadata.csv will not be available for collection until after the PIT node has been booted from the LiveCD. Having incorrect MAC addresses in the ncn_metadata.csv file as placeholders is acceptable until the point during the install at which the management network switches have been configured and the PIT node can be used to collect the information.\nAt that point in the installation workflow, the Collect MAC Addresses for NCNs procedure will be used. Unless the system does not use or does not have onboard NICs on the management nodes, then this topic may be necessary before constructing the ncn_metadata.csv file.\nSee Switch PXE Boot from Onboard NIC to PCIe for more information. "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/platform_ca_issues/",
	"title": "Common Platform CA Issues",
	"tags": [],
	"description": "",
	"content": "Common Platform CA Issues 1 NCN platform CA certificate does not match certificate in BSS During install, if the beginning steps are re-run after the NCNs are booted, then platform-ca files on those NCNs will no longer match the server\u0026rsquo;s CA certificate. This can be detected with a Goss test.\n1.1 Error messages (Caused by SSLError(SSLError(1, \u0026#39;[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)\u0026#39;),)) curl: (60) SSL certificate problem: self signed certificate in certificate chain More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. 1.2 Check Command:\nncn# goss -g /opt/cray/tests/install/ncn/tests/goss-platform-ca-certs-match-cloud-init.yaml v Example output:\nFailures/Skipped: Title: Validate that the local platform CA bundle matches the one in cloud-init Meta: desc: Validates that the local platform CA bundle matches the one in cloud-init sev: 0 Command: goss_platform_ca_certs_match_cloud_init: exit-status: Expected \u0026lt;int\u0026gt;: 1 to equal \u0026lt;int\u0026gt;: 0 Total Duration: 0.058s Count: 1, Failed: 1, Skipped: 0 1.3 Solution Run the following commands on any affected NCNs in order to update the platform-ca file.\nncn# curl http://10.92.100.71:8888/meta-data | jq -r \u0026#39;.Global.\u0026#34;ca-certs\u0026#34;.trusted[]\u0026#39; \u0026gt; /etc/pki/trust/anchors/platform-ca-certs.crt ncn# update-ca-certificates If the certificate issues are suspected to have caused problems with cfs-state-reporter, then restart the cfs-state-reporter service:\nncn# systemctl restart cfs-state-reporter 2 certifi has been updated and no longer respects the local ca-bundle SLES ships a modified version of the python3 certifi module. This module uses the local ca-bundle.pem file. If certifi is updated (usually due to a pip install), then the ca-bundle that certifi uses will revert to the one that is shipped with the module. This prevents any Python program that uses certifi, such as the ones that use the requests module, from being able to validate a server that uses the platform CA.\n2.1 Error message Error calling https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token: HTTPSConnectionPool(host=\u0026#39;api-gw-service-nmn.local\u0026#39;, port=443): Max retries exceeded with url: /keycloak/realms/shasta/protocol/openid-connect/token (Caused by SSLError(SSLError(1, \u0026#39;[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)\u0026#39;),)) 2.2 Check Command:\nncn# pip show certifi Example output:\nName: certifi Version: 2021.10.8 Summary: Python package for providing Mozilla\u0026#39;s CA Bundle. Home-page: https://certifiio.readthedocs.io/en/latest/ Author: Kenneth Reitz Author-email: me@kennethreitz.com License: MPL-2.0 Location: /root/.local/lib/python3.6/site-packages/certifi-2021.10.8-py3.6.egg Requires: Required-by: canu, kubernetes, requests 2.3 Solutions If certifi is installed in /root/.local/..., then uninstall it by running the following command:\nncn# pip uninstall certifi If certifi is installed in /usr/lib/python3.6/site-packages, then reinstall the certifi RPM that ships with SLES. If this is not possible, run the following commands to replace the ca-bundle that certifi uses with a link to the system\u0026rsquo;s ca-bundle.\nncn# CERTIFIDIR=\u0026#34;$(pip show certifi | grep Location | awk \u0026#39;{print $2}\u0026#39;)/certifi\u0026#34; ncn# mv \u0026#34;$CERTIFIDIR\u0026#34;/cacert.pem \u0026#34;$CERTIFIDIR\u0026#34;/cacert.pem.orig ncn# ln -s /var/lib/ca-certificates/ca-bundle.pem \u0026#34;$CERTIFIDIR\u0026#34;/cacert.pem If these issues are suspected to have caused problems with cfs-state-reporter, then restart the cfs-state-reporter service:\nncn# systemctl restart cfs-state-reporter 3 update-ca-certificates fails to add platform-ca to ca-bundle update-ca-certificates can occasionally fail to add the platform-ca-certs.crt file to the system\u0026rsquo;s ca-bundle.pem. This can cause the same error message as the previous issues. If the previous checks do not show any issues, then try the solution outlined below.\n3.1 Error messages (Caused by SSLError(SSLError(1, \u0026#39;[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)\u0026#39;),)) curl: (60) SSL certificate problem: self signed certificate in certificate chain More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. 3.2 Solution Run the following commands on the affected node to regenerate the ca-bundle.pem file with the platform-ca-certs.crt file included.\nncn# rm -v /var/lib/ca-certificates/ca-bundle.pem ncn# update-ca-certificates If these issues are suspected to have caused problems with cfs-state-reporter, then restart the cfs-state-reporter service:\nncn# systemctl restart cfs-state-reporter "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/restore_corrupt_nexus/",
	"title": "Restore Nexus Data After Data Corruption",
	"tags": [],
	"description": "",
	"content": "Restore Nexus Data After Data Corruption In rare cases, if a Ceph upgrade is not completed successfully and has issues, the eventual Ceph health can end up with a damaged mds (cephfs) daemon. Ceph reports this as follows running the ceph -s command:\nncn-s002# ceph -s Example output:\ncluster: id: 7ed70f4c-852e-494a-b9e7-5f722af6d6e7 health: HEALTH_ERR 1 filesystem is degraded 1 filesystem is offline 1 mds daemon damaged When Ceph is in this state, Nexus will likely not operate properly and can be recovered using the following procedure.\nProcedure The commands in this procedure should be run from a master NCN (unless otherwise noted).\nDetermine where the originally installed Nexus helm chart and manifest exists.\nDuring normal installs, the initial artifacts (docker images and helm charts) are available on ncn-m001 in /mnt/pitdata after it has been rebooted out of PIT mode. The administrator may need to re-mount this volume (first step below):\nRe-mount the volume.\nncn-m001# mount -vL PITDATA /mnt/pitdata Find the Nexus helm chart to use.\nncn-m001# ls /mnt/pitdata/csm-0.9.4/helm/*nexus* Expected output:\n/mnt/pitdata/csm-0.9.4/helm/cray-nexus-0.6.0.tgz \u0026lt;-- this is the helm chart to use Find the Nexus manifest to use.\nncn-m001# ls /mnt/pitdata/csm-0.9.4/manifests/nexus.yaml Expected output:\n/mnt/pitdata/csm-0.9.4/manifests/nexus.yaml \u0026lt;-- this is the manifest to use NOTE: Do not proceed with further steps until these two files are located, as they are necessary to re-install the helm chart after deletion.\nUninstall the Nexus helm chart.\nncn-m001# helm uninstall -n nexus cray-nexus Delete the backing Nexus PVC.\nncn-m001# kubectl -n nexus delete pvc nexus-data Re-install the cray-nexus helm chart (using the chart and manifest determined in step 1).\nncn-m001# loftsman ship --manifest-path /mnt/pitdata/csm-0.9.4/manifests/nexus.yaml --charts-path /mnt/pitdata/csm-0.9.4/helm/cray-nexus-0.6.0.tgz Re-populate Nexus with 0.9.x artifacts.\nDepending on where initial PIT data was determined in step 1, now locate the Nexus setup script:\nncn-m001# ls /mnt/pitdata/csm-0.9.4/lib/setup-nexus.sh /mnt/pitdata/csm-0.9.4/lib/setup-nexus.sh Re-populate the 0.9.x artifacts by running the setup-nexus.sh script:\nncn-m001# /mnt/pitdata/csm-0.9.4/lib/setup-nexus.sh Re-populate Nexus with 1.0 artifacts. This step is also necessary if the mds/cephfs corruption occurred when upgrading from 0.9.x to 1.0. Nexus must be populated with both versions of the artifacts in order to support both old/new docker images during upgrade.\nLocate the Nexus setup script, this is typically in /root/csm-1.0.* on ncn-m001:\nncn-m001# ls /root/csm-1.0.0-beta.50/lib/setup-nexus.sh /root/csm-1.0.0-beta.50/lib/setup-nexus.sh Re-populate the 1.0.x artifacts by running the setup-nexus.sh script:\nncn-m001# /root/csm-1.0.0-beta.50/lib/setup-nexus.sh Re-populate Nexus with any add-on products that had been installed on this system.\nIf any additional products were installed on this system after initial install, locate the install documentation for those products and re-run the appropriate portion of their install script(s) to load those artifacts back into Nexus. For example, if SLES repositories have been added to Nexus prior to this content restore, refer to the install documentation contained in that distribution\u0026rsquo;s tar file for instructions on how to again load that content into Nexus.\nScale up any deployments/statefulsets that may have been scaled down during upgrade (if applicable). These commands should be run from ncn-s001.\nSource the k8s-scale-utils.sh script in order to define the scale_up_cephfs_clients function:\nncn-m001# source /usr/share/doc/csm/upgrade/1.0/scripts/ceph/lib/k8s-scale-utils.sh Execute the scale_up_cephfs_clients function in order to scale up any cephfs clients that may still be scaled down:\nncn-m001# scale_up_cephfs_clients "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/change_the_keycloak_admin_password/",
	"title": "Change the Keycloak Admin Password",
	"tags": [],
	"description": "",
	"content": "Change the Keycloak Admin Password Update the default password for the admin Keycloak account using the Keycloak user interface (UI). After updating the password in Keycloak, encrypt it on the system and verify that the change was made successfully.\nSystem domain name Procedure System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system\u0026rsquo;s fully qualified domain name (FQDN).\nThe FQDN can be found by running the following command on any Kubernetes NCN.\nncn-mw# kubectl get secret site-init -n loftsman -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d | yq r - spec.network.dns.external Example output:\nsystem.hpc.amslabs.hpecorp.net Be sure to modify the example URLs on this page by replacing SYSTEM_DOMAIN_NAME with the actual value found using the above command.\nProcedure Log in to Keycloak with the default admin credentials.\nPoint a browser at https://auth.cmn.SYSTEM_DOMAIN_NAME/keycloak/admin, replacing SYSTEM_DOMAIN_NAME with the actual NCN\u0026rsquo;s DNS name. Use of the auth.cmn. sub-domain is required for administrative access to Keycloak.\nThe following is an example URL for a system: https://auth.cmn.system1.us.cray.com/keycloak/admin\nUse the following admin login credentials:\nUsername: admin The password can be obtained with the following command: ncn# kubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode Click the Admin drop-down menu in the upper-right corner of the page.\nSelect Manage Account.\nClick the Password tab on the left side of the page.\nEnter the existing password, new password and confirmation, and then click Save.\nFollow the Redeploying a Chart procedure with the following specifications:\nName of chart to be redeployed: cray-keycloak\nBase name of manifest: platform\nWhen reaching the step to update customizations, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nClone the CSM repository. ncn-w001# git clone https://github.com/Cray-HPE/csm.git ```bash ``` Change the password in the customizations.yaml file.\nIn the customizations.yaml file, set the values for the keycloak_master_admin_auth keys in the spec.kubernetes.sealed_secrets field. The value in the data element where the name is password needs to be changed to the new Keycloak master admin password. The section below will replace the existing sealed secret data in the customizations.yaml file.\nEncrypt the values after changing the customizations.yaml file.\nncn-w001# ./utils/secrets-seed-customizations.sh customizations.yaml If the above command complains that it cannot find certs/sealed_secrets.crt then you can run the following commands to create it:\nmkdir -pV certs \u0026amp;\u0026amp; ./utils/bin/linux/kubeseal --controller-name sealed-secrets --fetch-cert \u0026gt; certs/sealed_secrets.crt When reaching the step to validate that the redeploy was successful, perform the following step:\nOnly follow this step as part of the previously linked chart redeploy procedure.\nVerify that the Secret has been updated.\nGive the SealedSecret controller a few seconds to update the Secret, then run the following command to see the current value of the Secret:\nkubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode Make sure to perform the entire linked procedure, including the step to save the updated customizations.\n"
},
{
	"uri": "/docs-csm/en-12/operations/power_management/system_power_off_procedures/",
	"title": "System Power Off Procedures",
	"tags": [],
	"description": "",
	"content": "System Power Off Procedures The procedures in this section detail the high-level tasks required to power off an HPE Cray EX system.\nNote about Services Used During System Power Off The Cray Advanced Platform Monitoring and Control (CAPMC) service controls power to major components. CAPMC sequences the power off tasks in the correct order, but does not gracefully shut down software services. The Boot Orchestration Service (BOS) manages proper shutdown and power off tasks for compute nodes and User Access Nodes (UANs). The System Admin Toolkit (SAT) automates shutdown services by stage. Prepare the System for Power Off To make sure that the system is healthy before power off and all the information is collected, refer to Prepare the System for Power Off.\nShut Down Compute Nodes and UANs To shut down compute nodes and User Access Nodes (UANs), refer to Shut Down and Power Off Compute and User Access Nodes.\nSave Management Network Switch Settings To save management switch configuration settings, refer to Save Management Network Switch Configuration Settings.\nPower Off System Cabinets To power off standard rack and liquid-cooled cabinet PDUs, refer to Power Off Compute and IO Cabinets.\nShut Down the Management Kubernetes Cluster To shut down the management Kubernetes cluster, refer to Shut Down and Power Off the Management Kubernetes Cluster.\nPower Off the External Lustre File System To power off the external Lustre file system (ClusterStor), refer to Power Off the External Lustre File System.\nLockout Tagout Facility Power If facility power must be removed from a single cabinet or cabinet group for maintenance, follow proper lockout-tagout procedures for the site.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/troubleshoot_issues_with_redfish_endpoint_discovery/",
	"title": "Troubleshoot Issues with Redfish Endpoint Discovery",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Issues with Redfish Endpoint Discovery If a Redfish endpoint is in the HTTPsGetFailed status, then the endpoint does not need to be fully rediscovered. The error indicates an issue in the inventory process done by the Hardware State Manager (HSM). Restart the inventory process to fix this issue.\nUpdate the HSM inventory to resolve issues with discovering Redfish endpoints.\nError Symptom The following is an example of the HSM error:\nncn-m001# cray hsm inventory redfishEndpoints describe x3000c0s15b0 Example output:\nDomain = \u0026#34;\u0026#34; MACAddr = \u0026#34;b42e993b70ac\u0026#34; Enabled = true Hostname = \u0026#34;10.254.2.100\u0026#34; RediscoverOnUpdate = true FQDN = \u0026#34;10.254.2.100\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34;\u0026#34; Type = \u0026#34;NodeBMC\u0026#34; ID = \u0026#34;x3000c0s15b0\u0026#34; [DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2019-11-18T21:34:29.990441Z\u0026#34; RedfishVersion = \u0026#34;1.1.0\u0026#34; LastDiscoveryStatus = \u0026#34;HTTPsGetFailed\u0026#34; Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. Procedure Restart the HSM inventory process.\nncn-m001# cray hsm inventory discover create --xnames XNAME Verify that the Redfish endpoint has been rediscovered by HSM.\nncn-m001# cray hsm inventory redfishEndpoints describe XNAME Example output:\nDomain = \u0026#34;\u0026#34; MACAddr = \u0026#34;b42e993b70ac\u0026#34; Enabled = true Hostname = \u0026#34;10.254.2.100\u0026#34; RediscoverOnUpdate = true FQDN = \u0026#34;10.254.2.100\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34;\u0026#34; Type = \u0026#34;NodeBMC\u0026#34; ID = \u0026#34;x3000c0s15b0\u0026#34; [DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2019-11-18T21:34:29.990441Z\u0026#34; RedfishVersion = \u0026#34;1.1.0\u0026#34; LastDiscoveryStatus = \u0026#34;DiscoverOK\u0026#34; Troubleshooting can stop if the discovery status is DiscoverOK. Otherwise, proceed to the next step.\nRe-enable the RedfishEndpoint by setting both the Enabled and RediscoverOnUpdate fields to true.\nIf Enabled = false for the RedfishEndpoint, then it may indicate a network or firmware issue with the BMC.\nncn-m001# cray hsm inventory redfishEndpoints update BMC_XNAME \\ --enabled true --rediscover-on-update true Verify that the Redfish endpoint has been rediscovered by HSM.\nRe-enabling the RedfishEndpoint will cause a rediscovery to start. Troubleshooting can stop if the discovery status is DiscoverOK.\nncn-m001# cray hsm inventory redfishEndpoints describe BMC_XNAME Troubleshooting: If discovery is still failing, then use the curl command to manually contact the BMC using Redfish.\nncn-m001# curl -k -u USERNAME:PASSWORD https://BMC_XNAME/redfish/v1/ If there is no response from /redfish/v1/, then the BMC is either not powered or there is a network issue.\nContact other Redfish URLs on the BMC.\nIf the response is one of the following, then there is a firmware issue and a BMC restart or update may be needed:\nAn empty response:\nncn-m001# curl -ku USERNAME:PASSWORD https://BMC_XNAME/redfish/v1/Systems/Node0 | jq . Example output:\n% Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1330 100 1330 0 0 5708 0 --:--:-- --:--:-- --:--:-- 5708 {} A garbled response:\nncn-m001# curl -ku USERNAME:PASSWORD https://BMC_XNAME/redfish/v1/Managers/Self Example output:\n\u0026lt;pre style=\u0026#34;font-size:12px; font-family:monospace; color:#8B0000;\u0026#34;\u0026gt;[web.lua] Error in RequestHandler, thread: 0xb60670d8 is dead. ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼ .\u0026amp;#47;redfish-handler.lua:0: attempt to index a nil value stack traceback: .\u0026amp;#47;turbo\u0026amp;#47;httpserver.lua:251: in function \u0026amp;lt;.\u0026amp;#47;turbo\u0026amp;#47;httpserver.lua:212\u0026amp;gt; [C]: in function \u0026amp;#39;xpcall\u0026amp;#39; .\u0026amp;#47;turbo\u0026amp;#47;iostream.lua:553: in function \u0026amp;lt;.\u0026amp;#47;turbo\u0026amp;#47;iostream.lua:544\u0026amp;gt; [C]: in function \u0026amp;#39;xpcall\u0026amp;#39; .\u0026amp;#47;turbo\u0026amp;#47;ioloop.lua:573: in function \u0026amp;lt;.\u0026amp;#47;turbo\u0026amp;#47;ioloop.lua:572\u0026amp;gt; ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲\u0026lt;/pre\u0026gt; The URLs listed for the Systems do not include /Systems/ in the URL:\nncn-m001# curl -ku USERNAME:PASSWORD https://BMC_XNAME/redfish/v1/Systems | jq . Example output:\n% Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 421 100 421 0 0 1427 0 --:--:-- --:--:-- --:--:-- 1422 { \u0026#34;@odata.context\u0026#34;: \u0026#34;/redfish/v1/$metadata#ComputerSystemCollection.ComputerSystemCollection\u0026#34;, \u0026#34;@odata.etag\u0026#34;: \u0026#34;W/\\\u0026#34;1604517759\\\u0026#34;\u0026#34;, \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/Systems\u0026#34;, \u0026#34;@odata.type\u0026#34;: \u0026#34;#ComputerSystemCollection.ComputerSystemCollection\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Collection of Computer Systems\u0026#34;, \u0026#34;Members\u0026#34;: [ { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/Node0\u0026#34; }, { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/Node1\u0026#34; } ], \u0026#34;Members@odata.count\u0026#34;: 2, \u0026#34;Name\u0026#34;: \u0026#34;Systems Collection\u0026#34; } The URL for a system is missing the SystemID:\nncn-m001# curl -ku USERNAME:PASSWORD https://BMC_XNAME/redfish/v1/Systems | jq . Example output:\n% Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 421 100 421 0 0 1427 0 --:--:-- --:--:-- --:--:-- 1422 { \u0026#34;@odata.context\u0026#34;: \u0026#34;/redfish/v1/$metadata#ComputerSystemCollection.ComputerSystemCollection\u0026#34;, \u0026#34;@odata.etag\u0026#34;: \u0026#34;W/\\\u0026#34;1604517759\\\u0026#34;\u0026#34;, \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/Systems\u0026#34;, \u0026#34;@odata.type\u0026#34;: \u0026#34;#ComputerSystemCollection.ComputerSystemCollection\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Collection of Computer Systems\u0026#34;, \u0026#34;Members\u0026#34;: [ { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/Systems/\u0026#34; } ], \u0026#34;Members@odata.count\u0026#34;: 1, \u0026#34;Name\u0026#34;: \u0026#34;Systems Collection\u0026#34; } Troubleshooting: If there was no indication of a firmware issue, then there may be an issue with Vault or the credentials stored in Vault.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/adding_a_liquid-cooled_blade_to_a_system/",
	"title": "Adding a Liquid-cooled Blade to a System",
	"tags": [],
	"description": "",
	"content": "Adding a Liquid-cooled Blade to a System This procedure will add a liquid-cooled blades to an HPE Cray EX system.\nPrerequisites The Cray command line interface (CLI) tool) is initialized and configured on the system. See Configure the Cray CLI.\nKnowledge of whether Data Virtualization Service (DVS) is operating over the Node Management Network (NMN) or the High Speed Network (HSN).\nBlade is being added to an existing liquid-cooled cabinet in the system.\nThe Slingshot fabric must be configured with the desired topology for desired state of the blades in the system.\nThe System Layout Service (SLS) must have the desired HSN configuration.\nCheck the status of the high-speed network (HSN) and record link status before the procedure.\nReview the following command examples.\nThe following commands can be used to capture the required values from the Hardware State Manager (HSM) ethernetInterfaces table and write the values to a file, which can then be used to automate subsequent commands in this procedure.\nncn-mw# mkdir blade_swap_scripts; cd blade_swap_scripts ncn-mw# cat blade_query.sh Example output:\n#!/bin/bash BLADE=$1 OUTFILE=$2 BLADE_DOT=$BLADE. cray hsm inventory ethernetInterfaces list --format json | jq -c --arg BLADE \u0026#34;$BLADE_DOT\u0026#34; \\ \u0026#39;map(select(.ComponentID|test($BLADE))) | map(select(.Description == \u0026#34;Node Maintenance Network\u0026#34;)) | .[] | {xname: .ComponentID, ID: .ID,MAC: .MACAddress, IP: .IPAddresses[0].IPAddress,Desc: .Description}\u0026#39; \u0026gt; $OUTFILE ncn-mw# ./blade_query.sh x1000c0s1 x1000c0s1.json ncn-mw# cat x1000c0s1.json Example output:\n{\u0026#34;xname\u0026#34;:\u0026#34;x1000c0s1b0n0\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a6836339\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:63:39\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.10\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} {\u0026#34;xname\u0026#34;:\u0026#34;x1000c0s1b0n1\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a683633a\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:63:3a\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.98\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} {\u0026#34;xname\u0026#34;:\u0026#34;x1000c0s1b1n0\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a68362e2\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:62:e2\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.123\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} {\u0026#34;xname\u0026#34;:\u0026#34;x1000c0s1b1n1\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a68362e3\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:62:e3\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.122\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} To delete anethernetInterfaces entry using curl:\nncn-mw# for ID in $(cat x1000c0s1.json | jq -r \u0026#39;.ID\u0026#39;); do cray hsm inventory ethernetInterfaces delete $ID; done To insert an ethernetInterfaces entry using curl:\nncn-mw# while read PAYLOAD ; do curl -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; -L -X POST \u0026#39;https://api-gw-service-nmn.local/apis/smd/hsm/v1/Inventory/EthernetInterfaces\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ --data-raw \u0026#34;$(echo $PAYLOAD | jq -c \u0026#39;{ComponentID: .xname,Description: .Desc,MACAddress: .MAC,IPAddress: .IP}\u0026#39;)\u0026#34; sleep 5 done \u0026lt; x1000c0s1.json The blades must have the coolant drained and filled during the swap to minimize cross-contamination of cooling systems.\nReview procedures in HPE Cray EX Coolant Service Procedures H-6199. Review the HPE Cray EX Hand Pump User Guide H-6200. Procedure Suspend the hms-discovery cron job to disable it.\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : true }}\u0026#39; Verify that the hms-discovery cron job has stopped.\nncn-mw# kubectl get cronjobs -n services hms-discovery Example output. Note the ACTIVE = 0 and is SUSPEND = True in the output indicating the job has been suspended:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * True 0 117s 15d Determine if the destination chassis slot is populated.\nThis example is checking slot 0 in chassis 3 of cabinet x1005.\nncn-mw# cray hsm state components describe x1005c3s0 --format toml Example output:\nID = \u0026#34;x1005c3s0\u0026#34; Type = \u0026#34;ComputeModule\u0026#34; State = \u0026#34;Empty\u0026#34; Flag = \u0026#34;OK\u0026#34; Enabled = true NetType = \u0026#34;Sling\u0026#34; Arch = \u0026#34;X86\u0026#34; Class = \u0026#34;Mountain\u0026#34; If the state of the slot is On or Off, then the chassis slot is populated. If the state of the slot is Empty, then the chassis slot is not populated.\nVerify that the chassis slot is powered off.\nSkip this step if the chassis slot is unpopulated.\nncn-mw# cray capmc get_xname_status create --xnames x1005c3s0 --format toml Example output:\ne = 0 err_msg = \u0026#34;\u0026#34; off = [ \u0026#34;x1005c3s0\u0026#34;,] If the slot is powered on, then power the chassis slot off.\nncn-mw# cray capmc xname_off create --xnames x1005c3s0 --recursive true Install the the blade into the system into the desired location.\nObtain an authentication token to access the API gateway.\nncn-mw# export TOKEN=$(curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Preserve node component name (xname) to IP address mapping Skip this step if DVS is operating over the HSN, otherwise proceed with this step.\nWhen DVS is operating over the NMN and a blade is being replaced, the mapping of node component name (xname) to node IP address must be preserved. Kea automatically adds entries to the HSM ethernetInterfaces table when DHCP lease is provided (about every 5 minutes). To prevent from Kea from automatically adding MAC entries to the HSM ethernetInterfaces table, use the following commands:\nCreate an eth_interfaces file that contains the interface IDs for the Node Maintenance Network entries for the destination blade location. If there has not been a blade previously in the destination location there may not be any Ethernet Interfaces to delete from HSM. The blade_query.sh script from the perquisites section can help determine the IDs for the HSM Ethernet Interfaces associated with the blade if any. It is expected that if a blade has not been populated in the slot before that no HSM Ethernet Interfaces IDs would be found.\nncn-mw# cat eth_interfaces Example output:\n0040a6836339 0040a683633a 0040a68362e2 0040a68362e3 Run the following commands in succession to remove the interfaces if any. Delete the cray-dhcp-kea pod to prevent the interfaces from being re-created.\nncn-mw# kubectl get pods -Ao wide | grep kea ncn-mw# kubectl delete -n services pod CRAY_DHCP_KEA_PODNAME ncn-mw# for ETH in $(cat eth_interfaces); do cray hsm inventory ethernetInterfaces delete $ETH --format json ; done Skip this step if the destination blade location has not been previously populated with a blade.\nAdd the MAC address, IP address, and the Node Maintenance Network description to the interfaces. The component ID and IP address must be the values recorded from the blade previously in the destination location, and the MAC address must be the value recorded from the blade. These values were recorded if the blade was removed via the Removing a Liquid-cooled blade from a System procedure.\nValues recorded from the blade that was was previously in the slot.\nComponentID: \u0026#34;x1005c3s0b0n0\u0026#34; MACAddress: \u0026#34;00:40:a6:83:63:99\u0026#34; IPAddress: \u0026#34;10.10.0.123\u0026#34; ncn-mw# MAC=NEW_BLADE_MAC_ADDRESS ncn-mw# IP_ADDRESS=DESTLOCATION_IP_ADDRESS ncn-mw# XNAME=DESTLOCATION_XNAME ncn-mw# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -L -X POST \u0026#39;https://api-gw-service-nmn.local/apis/smd/hsm/v1/Inventory/EthernetInterfaces\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; --data-raw \u0026#34;{ \\\u0026#34;Description\\\u0026#34;: \\\u0026#34;Node Maintenance Network\\\u0026#34;, \\\u0026#34;MACAddress\\\u0026#34;: \\\u0026#34;$MAC\\\u0026#34;, \\\u0026#34;IPAddress\\\u0026#34;: \\\u0026#34;$IP_ADDRESS\\\u0026#34;, \\\u0026#34;ComponentID\\\u0026#34;: \\\u0026#34;$XNAME\\\u0026#34; }\u0026#34; Note: Kea may must be restarted when the curl command is issued.\nncn-mw# kubectl delete pods -n services -l app.kubernetes.io/name=cray-dhcp-kea To change or correct a curl command that has been entered, use a PATCH request, for example:\nncn-mw# curl -k -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; -L -X PATCH \\ \u0026#39;https://api-gw-service-nmn.local/apis/smd/hsm/v1/Inventory/EthernetInterfaces/0040a68350a4\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; \\ --data-raw \u0026#39;{\u0026#34;MACAddress\u0026#34;:\u0026#34;xx:xx:xx:xx:xx:xx\u0026#34;,\u0026#34;IPAddress\u0026#34;:\u0026#34;10.xxx.xxx.xxx\u0026#34;,\u0026#34;ComponentID\u0026#34;:\u0026#34;XNAME\u0026#34;}\u0026#39; Repeat the preceding command for each node in the blade.\nRe-enable hms-discovery cron job Rediscover the ChassisBMC (the example shows cabinet 1005, chassis 3).\nRediscovering the ChassisBMC will update HSM to become aware of the newly populated slot and allow Cray Advanced Platform Monitoring and Control (CAPMC) to perform power actions on the slot.\nncn-mw# cray hsm inventory discover create --xnames x1005c3b0 Verify that discovery of the ChassisBMC has completed.\nThat is, verify that LastDiscoveryStatus = DiscoverOK.\nncn-mw# cray hsm inventory redfishEndpoints describe x1005c3b0 --format json Example output:\n{ \u0026#34;ID\u0026#34;: \u0026#34;x1005c3b0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;ChassisBMC\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;x1005c3b0\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x1005c3b0\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MACAddr\u0026#34;: \u0026#34;02:03:ED:03:00:00\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2020-09-03T19:03:47.989621Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;1.2.0\u0026#34; } } Unsuspend the hms-discovery cronjob to re-enable the hms-discovery job.\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : false }}\u0026#39; Verify that the hms-discovery job has been unsuspended:\nncn-mw# kubectl get cronjobs.batch -n services hms-discovery Example output.\nACTIVE = 1 and SUSPEND = False in the output indicates that the job has been unsuspended:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * False 1 41s 33d Enable and power on the chassis slot Enable the chassis slot.\nThe example enables slot 0, chassis 3, in cabinet 1005.\nncn-mw# cray hsm state components enabled update --enabled true x1005c3s0 Power on the chassis slot.\nThe example powers on slot 0, chassis 3, in cabinet 1005.\nncn-mw# cray capmc xname_on create --xnames x1005c3s0 --recursive true Wait at least three minutes for the blade to power on and the node controllers (BMCs) to be discovered.\nVerify that discovery has completed Verify that the two node BMCs in the blade have been discovered by the HSM.\nRun this command for each BMC in the blade (x1005c3s0b0 and x1005c3s0b1 in this example):\nncn-mw# cray hsm inventory redfishEndpoints describe x1005c3s0b0 --format json Example output:\n{ \u0026#34;ID\u0026#34;: \u0026#34;x1005c3s0b0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;NodeBMC\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;x1005c3s0b0\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x1005c3s0b0\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MACAddr\u0026#34;: \u0026#34;02:03:E8:00:31:00\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2021-06-10T18:01:59.920850Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;1.2.0\u0026#34; } } When LastDiscoveryStatus displays as DiscoverOK, the node BMC has been successfully discovered. If the last discovery state is DiscoveryStarted then the BMC is currently being inventoried by HSM. If the last discovery state is HTTPsGetFailed or ChildVerificationFailed, then an error has occurred during the discovery process. Troubleshooting:\nIf the Redfish endpoint does not exist for a BMC, then verify the following:\nVerify that the node BMC is pingable:\nncn-mw# ping x1005c3s0b0 If the BMC is not pingable, then verify that the chassis slot has power.\nncn-mw# cray capmc get_xname_status create --xnames x1005c3s0 If the Redfish endpoint is in HTTPsGetFailed:\nVerify that the node BMC is pingable:\nncn-mw# ping x1005c3s0b0 If the BMC is pingable, then verify that the node BMC is configured with the expected credentials.\nncn-mw# curl -k -u root:password https://x1005c3s0b0/redfish/v1/Managers Clear out the existing Redfish event subscriptions from the BMCs on the blade.\nSet the environment variable SLOT corresponding to the blades location:\nncn-mw# SLOT=\u0026#34;x1005c3s0\u0026#34; Clear the Redfish event subscriptions:\nncn-mw# for BMC in $(cray hsm inventory redfishEndpoints list --type NodeBMC --format json | jq .RedfishEndpoints[].ID -r | grep $SLOT); do PASSWD=$(cray scsd bmc creds list --targets $BMC --format json | jq .Targets[].Password -r) SUBS=$(curl -sk -u root:$PASSWD https://${BMC}/redfish/v1/EventService/Subscriptions | jq -r \u0026#39;.Members[].\u0026#34;@odata.id\u0026#34;\u0026#39;) for SUB in $SUBS; do echo \u0026#34;Deleting event subscription: https://${BMC}${SUB}\u0026#34; curl -i -sk -u root:$PASSWD -X DELETE https://${BMC}${SUB} done done Each event subscription deleted that was deleted will have output like the following:\nDeleting event subscription: https://x1005c3s0b0/redfish/v1/EventService/Subscriptions/1 HTTP/2 204 access-control-allow-credentials: true access-control-allow-headers: X-Auth-Token access-control-allow-origin: * access-control-expose-headers: X-Auth-Token cache-control: no-cache, must-revalidate content-type: text/html; charset=UTF-8 date: Tue, 19 Jan 2038 03:14:07 GMT odata-version: 4.0 server: Cray Embedded Software Redfish Service Enable the nodes in the HSM database.\nFor a blade with four nodes per blade:\nncn-mw# cray hsm state components bulkEnabled update --enabled true --component-ids x1005c3s0b0n0,x1005c3s0b0n1,x1005c3s0b1n0,x1005c3s0b1n1 For a blade with two nodes per blade:\nncn-mw# cray hsm state components bulkEnabled update --enabled true --component-ids x1005c3s0b0n0,x1005c3s0b1n0 Verify that the nodes are enabled in the HSM.\nncn-mw# cray hsm state components query create --component-ids x1005c3s0b0n0,x1005c3s0b0n1,x1005c3s0b1n0,x1005c3s0b1n1 --format toml Partial example output:\n[[Components]] ID = x1005c3s0b0n0 Type = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;Off\u0026#34; [[Components]] ID = x1005c3s0b1n1 Type = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;Off\u0026#34; Power on and boot the nodes Use boot orchestration to power on and boot the nodes. Specify the appropriate Boot Orchestration Service (BOS) template for the node type.\nDetermine how the BOS session template references compute hosts.\nTypically, they are referenced by their Compute role. However, if they are referenced by component name (xname), then these new nodes should added to the BOS session template.\nncn-mw# BOS_TEMPLATE=cos-2.0.30-slurm-healthy-compute ncn-mw# cray bos sessiontemplate describe $BOS_TEMPLATE --format json|jq \u0026#39;.boot_sets[] | select(.node_list)\u0026#39; If this query returns empty, then skip to booting the nodes. If this query returns with data, then one or more boot sets within the BOS session template reference nodes explicitly by xname. Consider adding the new nodes to this list (sub-step 1) or adding them on the command line (sub-step 2). Add new nodes to the list.\nDump the current session template.\nncn-mw# cray bos sessiontemplate describe $BOS_TEMPLATE --format json \u0026gt; tmp.txt Edit the tmp.txt file, adding the new nodes to the node_list.\nCreate the session template.\nSet the name of the template.\nThe name of the session template is determined by the name provided to the --name option on the command line. Use the current value of $BOS_TEMPLATE if wanting to overwrite the existing session template. If wanting to use the current value, then skip this sub-step. Otherwise, provide a different name for BOS_TEMPLATE which will be used the --name option. The name specified in tmp.txt is overridden by the value provided to the --name option.\nncn-mw# BOS_TEMPLATE=\u0026#34;New-Session-Template-Name\u0026#34; Create the session template.\nncn-mw# cray bos sessiontemplate create --file tmp.txt --name $BOS_TEMPLATE Verify that the session template contains the additional nodes and the proper name.\nncn-mw# cray bos sessiontemplate describe $BOS_TEMPLATE --format json Boot the nodes.\nncn-mw# cray bos session create --template-uuid $BOS_TEMPLATE \\ --operation reboot --limit x1005c3s0b0n0,x1005c3s0b0n1,x1005c3s0b1n0,x1005c3s0b1n1 Check firmware Use Firmware Action Service (FAS) to verify that the correct firmware versions are present for node BIOS, node controller (nC), NIC Mezzanine Card (NMC), GPUs, and so on.\nReview FAS Admin Procedures to perform a dry run using FAS to verify firmware versions.\nIf necessary update firmware with FAS. See Update Firmware with FAS for more information.\nCheck DVS There should be a cray-cps pod (the broker), three cray-cps-etcd pods and their waiter, and at least one cray-cps-cm-pm pod. Usually there are two cray-cps-cm-pm pods, one on ncn-w002 and one on ncn-w003 and other worker nodes.\nVerify that the cray-cps pods on worker nodes are Running.\nncn-mw# kubectl get pods -Ao wide | grep cps Example output:\nservices cray-cps-75cffc4b94-j9qzf 2/2 Running 0 42h 10.40.0.57 ncn-w001 services cray-cps-cm-pm-g6tjx 5/5 Running 21 41h 10.42.0.77 ncn-w003 services cray-cps-cm-pm-kss5k 5/5 Running 21 41h 10.39.0.80 ncn-w002 services cray-cps-etcd-knt45b8sjf 1/1 Running 0 42h 10.42.0.67 ncn-w003 services cray-cps-etcd-n76pmpbl5h 1/1 Running 0 42h 10.39.0.49 ncn-w002 services cray-cps-etcd-qwdn74rxmp 1/1 Running 0 42h 10.40.0.42 ncn-w001 services cray-cps-wait-for-etcd-jb95m 0/1 Completed SSH to each worker node running Content Projection Service (CPS)/DVS, and ensure that there are no recurring \u0026quot;DVS: merge_one\u0026quot; error messages as shown.\nIf found, these error messages indicate that DVS is detecting an IP address change for one of the client nodes.\nncn-w# dmesg -T | grep \u0026#34;DVS: merge_one\u0026#34; Example output:\n[Tue Jul 21 13:09:54 2020] DVS: merge_one#351: New node map entry does not match the existing entry [Tue Jul 21 13:09:54 2020] DVS: merge_one#353: nid: 8 -\u0026gt; 8 [Tue Jul 21 13:09:54 2020] DVS: merge_one#355: name: \u0026#39;x3000c0s19b1n0\u0026#39; -\u0026gt; \u0026#39;x3000c0s19b1n0\u0026#39; [Tue Jul 21 13:09:54 2020] DVS: merge_one#357: address: \u0026#39;10.252.0.26@tcp99\u0026#39; -\u0026gt; \u0026#39;10.252.0.33@tcp99\u0026#39; [Tue Jul 21 13:09:54 2020] DVS: merge_one#358: Ignoring. SSH to the node and check each DVS mount.\nnid# mount | grep dvs | head -1 Example output:\n/var/lib/cps-local/0dbb42538e05485de6f433a28c19e200 on /var/opt/cray/gpu/nvidia-squashfs-21.3 type dvs (ro,relatime,blksize=524288,statsfile=/sys/kernel/debug/dvs/mounts/1/stats,attrcache_timeout=14400,cache,nodatasync,noclosesync,retry,failover,userenv,noclusterfs,killprocess,noatomic,nodeferopens,no_distribute_create_ops,no_ro_cache,loadbalance,maxnodes=1,nnodes=6,nomagic,hash_on_nid,hash=modulo,nodefile=/sys/kernel/debug/dvs/mounts/1/nodenames,nodename=x3000c0s6b0n0:x3000c0s5b0n0:x3000c0s4b0n0:x3000c0s9b0n0:x3000c0s8b0n0:x3000c0s7b0n0) Check the HSN for the affected nodes Determine the pod name for the Slingshot fabric manager pod and check the status of the fabric.\nncn-mw# kubectl exec -it -n services $(kubectl get pods --all-namespaces |grep slingshot | awk \u0026#39;{print $2}\u0026#39;) -- fmn_status Check for duplicate IP address entries Check for duplicate IP address entries in the Hardware State Management Database (HSM).\nDuplicate entries will cause DNS operations to fail.\nVerify that each node hostname resolves to one IP address.\nncn-mw# nslookup x1005c3s0b0n0 Example output with one IP address resolving:\nServer: 10.92.100.225 Address: 10.92.100.225#53 Name: x1005c3s0b0n0 Address: 10.100.0.26 Reload the KEA configuration.\nncn-mw# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;config-reload\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api-gw-service-nmn.local/apis/dhcp-kea |jq If there are no duplicate IP addresses within HSM, then the following response is expected:\n[ { \u0026#34;result\u0026#34;: 0, \u0026#34;text\u0026#34;: \u0026#34;Configuration successful.\u0026#34; } ] If there is a duplicate IP address in the HSM, then an error message similar to the message below will be returned.\n[{\u0026#39;result\u0026#39;: 1, \u0026#39;text\u0026#39;: \u0026#34;Config reload failed: configuration error using file \u0026#39;/usr/local/kea/cray-dhcp-kea-dhcp4.conf\u0026#39;: failed to add new host using the HW address \u0026#39;00:40:a6:83:50:a4 and DUID \u0026#39;(null)\u0026#39; to the IPv4 subnet id \u0026#39;0\u0026#39; for the address 10.100.0.105: There\u0026#39;s already a reservation for this address\u0026#34;}] Check for active DHCP leases.\nIf there are no DHCP leases, then there is a configuration error.\nncn-mw# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api-gw-service-nmn.local/apis/dhcp-kea | jq Example output with no active DHCP leases:\n[ { \u0026#34;arguments\u0026#34;: { \u0026#34;leases\u0026#34;: [] }, \u0026#34;result\u0026#34;: 3, \u0026#34;text\u0026#34;: \u0026#34;0 IPv4 lease(s) found.\u0026#34; } ] If there are duplicate entries in the HSM as a result of this procedure (10.100.0.105 in this example), then delete the duplicate entry.\nShow the EthernetInterfaces for the duplicate IP address:\nncn-mw# cray hsm inventory ethernetInterfaces list --ip-address 10.100.0.105 --format json | jq Example output for an IP address that is associated with two MAC addresses:\n[ { \u0026#34;ID\u0026#34;: \u0026#34;0040a68350a4\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:50:a4\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.105\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-08-24T20:24:23.214023Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1005c3s0b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;0040a683639a\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:63:9a\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.105\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-08-27T19:15:53.697459Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1005c3s0b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34; } ] Delete the older entry.\nncn-mw# cray hsm inventory ethernetInterfaces delete 0040a68350a4 Check DNS.\nncn-mw# nslookup 10.100.0.105 Example output:\n105.0.100.10.in-addr.arpa name = nid001032-nmn. 105.0.100.10.in-addr.arpa name = nid001032-nmn.local. 105.0.100.10.in-addr.arpa name = x1005c3s0b0n0. 105.0.100.10.in-addr.arpa name = x1005c3s0b0n0.local. Check SSH.\nncn-mw# ssh x1005c3s0b0n0 "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/hostname/",
	"title": "Hostname",
	"tags": [],
	"description": "",
	"content": "Hostname A hostname is a human-friendly name used to identify a device. An example of a hostname could be the name \u0026ldquo;Test.\u0026rdquo;\nRelevant Configuration\nCreating a hostname\nswitch(config)# hostname \u0026lt;NAME\u0026gt; Show Commands to Validate Functionality\nswitch# show hosts Expected Results\nStep 1: You can configure the hostname Step 2: The output of all show commands is correct Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/qos/",
	"title": "Configure QoS",
	"tags": [],
	"description": "",
	"content": "Configure QoS Network traffic is processed based on classification and policies that are created and applied to the traffic.\nQoS trust is by default disabled.\nConfiguration Commands Create a dot1p trust map:\nswitch(config)# trust dot1p-map dot1p-trust-map switch(config-tmap-dot1p-map)# Define the set of values to match the class:\nswitch(config-tmap-dot1p-map)# qos-group 3 dot1p 0-4 switch(config-tmap-dot1p-map)# qos-group 5 dot1p 5-7 Apply the map on a specific interface or on global level:\nswitch(conf-if-eth1/1/1)# trust-map dot1p dot1p-trust-map switch(config-sys-qos)# trust-map dot1p dot1p-trust-map Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/domain_name/",
	"title": "Configure Domain Names",
	"tags": [],
	"description": "",
	"content": "Configure Domain Names A domain name is a name to identify the person, group, or organization that controls the devices within an area. An example of a domain name could be us.cray.com.\nConfiguration Commands Create a domain name:\nswitch(config)# domain-name NAME Show commands to validate functionality:\nswitch# show domain-name Expected Results Administrators can configure the domain name The output of all show commands is correct Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "Kubernetes The system management components are broken down into a series of micro-services. Each service is independently deployable, fine-grained, and uses lightweight protocols. As a result, the system\u0026rsquo;s micro-services are modular, resilient, and can be updated independently. Services within this architecture communicate via REST APIs.\nAbout Kubernetes Kubernetes is a portable and extensible platform for managing containerized workloads and services. Kubernetes serves as a micro-services platform on the system that facilitates application deployment, scaling, and management. The system uses Kubernetes for container orchestration.\nResiliency The resiliency feature of Kubernetes ensures that the desired number of deployments of a micro-service are always running on one or more NCNs. In addition, Kubernetes ensures that if one NCN becomes unresponsive, the micro-services that were running on it are migrated to another NCN that is up and meets the requirements of the micro-services.\nKubernetes Components Kubernetes components can be divided into:\nMaster components - Kubernetes master components provide the cluster\u0026rsquo;s control plane. These components make global decisions about the cluster, such as scheduling, and responding to cluster events. Worker components - A Kubernetes worker is a node that provides services necessary to run application containers. It is managed by the Kubernetes master. Node components run on every node and keep pods running, while providing the Kubernetes runtime environment. An etcd cluster is used for storage and state management of the Kubernetes cluster.\n"
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/restore_hsm_postgres_without_a_backup/",
	"title": "Restore Hardware State Manager (HSM) Postgres without an Existing Backup",
	"tags": [],
	"description": "",
	"content": "Restore Hardware State Manager (HSM) Postgres without an Existing Backup This procedure is intended to repopulate HSM in the event when no Postgres backup exists.\nPrerequisite Healthy System Layout Service (SLS). Recovered first if also affected.\nHealthy HSM service.\nVerify all 3 HSM postgres replicas are up and running:\nncn# kubectl -n services get pods -l cluster-name=cray-smd-postgres Example output:\nNAME READY STATUS RESTARTS AGE cray-smd-postgres-0 3/3 Running 0 18d cray-smd-postgres-1 3/3 Running 0 18d cray-smd-postgres-2 3/3 Running 0 18d Procedure Re-run the HSM loader job.\nncn# kubectl -n services get job cray-smd-init -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | kubectl replace --force -f - Wait for the job to complete:\nncn# kubectl wait -n services job cray-smd-init --for=condition=complete --timeout=5m Verify that the service is functional.\nncn# cray hsm service ready Example output:\ncode = 0 message = \u0026#34;HSM is healthy\u0026#34; Get the number of node objects stored in HSM.\nncn# cray hsm state components list --type node --format json | jq .[].ID | wc -l Restart MEDS and REDS.\nTo repopulate HSM with components, restart MEDS and REDS so that they will add known RedfishEndpoints back in to HSM. This will also kick off HSM rediscovery to repopulate components and hardware inventory.\nncn# kubectl scale deployment cray-meds -n services --replicas=0 ncn# kubectl scale deployment cray-meds -n services --replicas=1 ncn# kubectl scale deployment cray-reds -n services --replicas=0 ncn# kubectl scale deployment cray-reds -n services --replicas=1 Wait for the RedfishEndpoints table to get repopulated and discovery to complete.\nncn# cray hsm inventory RedfishEndpoints list --format json | jq .[].ID | wc -l 100 ncn# cray hsm inventory redfishEndpoints list --format json | grep -c \u0026#34;DiscoveryStarted\u0026#34; 0 Check for Discovery Errors.\nncn# cray hsm inventory redfishEndpoints list --format json | grep LastDiscoveryStatus | grep -v -c \u0026#34;DiscoverOK\u0026#34; If any of the RedfishEndpoint entries have a LastDiscoveryStatus other than DiscoverOK after discovery has completed, refer to the Troubleshoot Issues with Redfish Endpoint Discovery procedure for guidance.\nRe-apply any component group or partition customizations.\nAny component groups or partitions created before HSM\u0026rsquo;s Postgres information was lost will need to be manually re-entered.\nManage Component Groups Manage Component Partitions "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/customize_configuration_values/",
	"title": "Customize Configuration Values",
	"tags": [],
	"description": "",
	"content": "Customize Configuration Values In general, most systems will require some customization from the default values provided by HPE Cray products. These changes cannot be made on the pristine product branches that are imported during product installation and upgrades. Changes can only be made in Git branches that are based on the pristine branches.\nChanging or overriding default values should be done in accordance with Ansible best practices and variable precedence in mind. For more information, see the following external links:\nAnsible best practices guide Ansible variable guide The following practices should also be followed:\nWhen it is necessary to add more functionality beyond an Ansible playbook provided by an HPE Cray product, include the product playbook in a new playbook instead of modifying it directly. Any modifications to a product playbook will result in a merge being required during a product upgrade. Do not modify default Ansible role variables, override all values using inventory (group_vars and host_vars directories). Cray products do not import any content to inventory locations, so merges of new product content will not cause conflicts if variables are located in inventory. Do not put any sensitive or secret information, such as passwords, in the Git repository. These values should be pulled during runtime from an external key management system. Handling sensitive information Passwords and other sensitive content should not be stored in a Version Control Service (VCS) repository. Instead, consider writing Ansible tasks/roles that pull the value in dynamically while the playbook is running from an external secrets management system.\nAt this time, the Ansible Vault is not supported by the Configuration Framework Service (CFS).\nExample: Override a role default value To override a value that is defined in an Ansible role in a configuration repository, set the value in the Ansible inventory. If the override pertains to an entire Ansible group of nodes, then create a file as follows:\nClone the configuration repository.\nCheckout the branch that will include the change, or create a new branch.\nCapture the variable name in the roles/[role name]/defaults/main.yml file.\nCreate a new directory and edit a file with the role name.\nncn# mkdir -p group_vars/all \u0026amp;\u0026amp; touch group_vars/all/[role name].yml Set the variable to a new value in the file.\nncn# echo \u0026#39;[variable name]: [new variable value]\u0026#39; \u0026gt;\u0026gt; group_vars/all/[role name].yml Stage the file in the Git branch, commit it, and promote the change.\nThis change will be applied to all nodes by using the group name all. To narrow the variable scope to a specific group (Compute for example), use the group name instead of all as follows:\nncn# mkdir -p group_vars/Compute \u0026amp;\u0026amp; touch group_vars/Compute/[role name].yml ncn# echo \u0026#39;[variable name]: [new variable value]\u0026#39; \u0026gt;\u0026gt; group_vars/Compute/[role name].yml To narrow the variable scope to a single node, create the file in the host_vars/[node xname]/[role name].yaml and override the value.\nThe name of the created file is largely inconsequential. The identified best practice is to include the name of the role in the created file for reasons of maintainability and discoverability. However, be aware that the file name matters when multiple files in the same directory contain the same variable with different values. In that case, use a single all.yml file rather than a directory, or use files names with import ordering in mind. See the external Ansible documentation for more information on how variables precedence is handled.\nTo override role variables for roles that exist across multiple repositories, consider using the CFS Additional Inventory feature. See Manage Multiple Inventories in a Single Location.\nExample: Add functionality to a provided playbook To add more functionality to a playbook provided by a configuration repository, it is considered best practice to leave the existing playbook unmodified (if possible), in order to avoid merge conflicts when new versions of the playbook are installed.\nFor instance, if a site.yml playbook needs to be extended with a custom site-custom.yml playbook, consider creating a new playbook that imports and runs them both. For example, the site-all.yml playbook.\n- import_playbook: site.yml - import_playbook: site-custom.yml See the external Ansible documentation on re-using playbooks.\nExample: Remove functionality from a provided playbook To remove a role from a provided playbook, the best practice is to determine if the role provides a variable to skip the roles tasks altogether. See the roles/[role name]/README file for a listing of the role variables.\nIf changes to role variables are not able to skip the role, then the role may be commented out in the playbook. However, note that an upgrade to the configuration will result in a merge conflict because of the changes made in the playbook, if the upgrade pristine branch is merged with the branch containing the commented playbook.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/kernel_boot_parameters/",
	"title": "Kernel Boot Parameters",
	"tags": [],
	"description": "",
	"content": "Kernel Boot Parameters The Image Management Service (IMS) extracts kernel boot parameters from the /boot/kernel-parameters file in the image, if that file exists, and stores them in S3. IMS already stores the other boot artifacts (kernel, initrd, and rootfs) in S3. When told to boot an image, the Boot Orchestration Service (BOS) will extract these parameters and deliver them to the Boot Script Service (BSS) so they can be used during the next boot of a node.\nThere are two benefits to having kernel boot parameters extracted from the image. First, these parameters can be tightly coupled to the image. Second, these parameters do not need to be specified in the BOS session template, making the template shorter, cleaner, and less error prone.\nThe kernel boot parameters obtained from the image can be overridden by specifying the same parameters in the BOS session template. BOS supplies these parameters to the kernel in a deliberate order that causes the parameters obtained from the image to be overridden by those obtained from the session template.\nThe following is a simplified kernel boot parameter ordering:\n\u0026lt;Image parameters\u0026gt; \u0026lt;Session template parameters\u0026gt; If there are competing values, the ones earlier in the boot string are superseded by the ones appearing later in the string.\nThe actual contents of the boot parameters are not as simple as previously described. For completeness, the following is the entire kernel boot parameter ordering:\n\u0026lt;Image parameters\u0026gt; \u0026lt;Session template parameters\u0026gt; \u0026lt;rootfs parameters\u0026gt; \u0026lt;rootfs passthrough parameters\u0026gt; \u0026lt;BOS session id\u0026gt; "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_uai/",
	"title": "Delete a UAI",
	"tags": [],
	"description": "",
	"content": "Delete a UAI There are two procedures described here. The first shows how an administrator can manually delete arbitrary UAIs or delete UAIs belonging to a given user or created using a given UAI Class. The second shows how an authorized user on can delete UAIs created in the legacy UIA creation mode.\nWhen a UAI is deleted, any running WLM sessions associated with the owner of the UAI are left intact and can be interacted with through future UAIs owned by the same user or from UANs.\nPrerequisites For administrative procedures:\nThe administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know or be able to find: the name(s) of the target UAI(s) or the user name of the owner of the targeted UAI(s) or the class-id of the targeted UAIs For Legacy Mode user procedures:\nThe user must be logged into a host that has user access to the HPE Cray EX System API Gateway The user must have an installed initialized cray CLI and network access to the API Gateway The user must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The user must be logged in as to the HPE Cray EX System CLI (cray auth login command) The user must know the name(s) of the target UAI(s) Procedures Delete UAIs as an administrator To delete a list of UAIs as an administrator use a command of the following form:\nncn-m001-pit# cray uas admin uais delete --uai-list UAI-NAMES UAI-NAMES is a comma-separated list of UAI Names of targeted UAIs.\nTo deleted all UAIs owned by a given user, use a command of the form:\nncn-m001-pit# cray uas admin uais delete --owner USERNAME USERNAME is the user name of the owner of the targeted UAIs.\nTo delete all UAIs of a given class, use a command of the form:\nncn-m001-pit# cray uas admin uais delete --class-id CLASS-ID CLASS-ID is the class ID of the class used to create the targeted UAIs.\nHere are some examples:\nDelete a list of UAIs by name:\nncn-m001-pit# cray uas admin uais delete --uai-list uai-vers-5f46dffb,uai-vers-e530f53a results = [ \u0026#34;Successfully deleted uai-vers-5f46dffb\u0026#34;, \u0026#34;Successfully deleted uai-vers-e530f53a\u0026#34;,] Delete all UAIs belonging to a named user (user name here is vers):\nncn-m001-pit# cray uas admin uais delete --owner vers results = [ \u0026#34;Successfully deleted uai-vers-5ef890be\u0026#34;, \u0026#34;Successfully deleted uai-vers-da65468d\u0026#34;,] Delete all UAIs belonging to a given UAI Class:\nncn-m001-pit# cray uas admin uais delete --class-id a630cbda-24b4-47eb-a1f7-be1c25965ead results = [ \u0026#34;Successfully deleted uai-vers-5ef890be\u0026#34;, \u0026#34;Successfully deleted uai-vers-da65468d\u0026#34;,] Delete UAIs as an Authorized User in Legacy Mode An authorized user in Legacy Mode can delete any UAI created by that user using a command of the form:\nvers\u0026gt; cray uas delete --uai-list UAI-NAMES To get a list of UAIs the user can delete:\nvers\u0026gt; cray uas list For example:\nvers\u0026gt; cray uas list [[results]] uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@104.155.164.238\u0026#34; uai_host = \u0026#34;ncn-w003\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34; uai_ip = \u0026#34;104.155.164.238\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-be3e219c\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; [[results]] uai_age = \u0026#34;1m\u0026#34; uai_connect_string = \u0026#34;ssh vers@34.70.243.171\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34; uai_ip = \u0026#34;34.70.243.171\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-ea57eb7b\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; To delete the UAI:\nvers\u0026gt; cray uas delete --uai-list uai-vers-be3e219c,uai-vers-ea57eb7b Output similar to the following is expected:\nresults = [ \u0026#34;Successfully deleted uai-vers-be3e219c\u0026#34;, \u0026#34;Successfully deleted uai-vers-ea57eb7b\u0026#34;,] Top: User Access Service (UAS)\nNext Topic: Common UAI Configurations\n"
},
{
	"uri": "/docs-csm/en-12/install/create_switch_metadata_csv/",
	"title": "Create Switch Metadata CSV",
	"tags": [],
	"description": "",
	"content": "Create Switch Metadata CSV This page provides directions on constructing the switch_metadata.csv file.\nPrerequisites The SHCD file for the system.\nCheck the description for component names while mapping names between the SHCD and the switch_metadata.csv file. See Component Names (xnames).\nOverview This file is manually created to include information about all spine, LeafBMC, CDU, and leaf switches in the system. None of the Slingshot switches for the HSN should be included in this file.\nThe file should have the following format, in ascending order by component name (xname):\nSwitch Xname,Type,Brand d0w1,CDU,Dell d0w2,CDU,Dell x3000c0w38,LeafBMC,Dell x3000c0w36,LeafBMC,Dell x3000c0h33s1,Spine,Mellanox x3000c0h34s1,Spine,Mellanox The above file would lead to this pairing between component name and hostname:\nHostname Component Name sw-spine-001 x3000c0h33s1 sw-spine-002 x3000c0h34s1 sw-LeafBMC-001 x3000c0w38 sw-LeafBMC-002 x3000c0w36 sw-cdu-001 d0w1 sw-cdu-002 d0w2 The hostnames are automatically generated in ascending order by switch type.\nSwitch brands The general guidelines for any abbreviations are that MLNX or MLX is for Mellanox and DL is for Dell. All other switches are HPE Aruba switches.\nThe brand name of the management switches can be determined from one of two places:\nDevice Diagrams tab The Device Diagrams or River Device Diagrams tab of the SHCD has pictures and diagrams of the components of the system including the management network switches. This will have a long name which shows the part number and the vendor name.\nRack Layout tab The Rack Layout or River Rack Layout tab shows the part number in the context of its location within the cabinet.\nPart Number Brand Aruba 8320 48P 1G/10GBASE-T and 6P 40G QSFP with X472 (JL481A) Aruba Aruba 8325-23C 32-port 100G QSFP+/QSFP28 (JL627A) Aruba CS-XGE40-MLNX-2100-16 Mellanox HPE Aruba 6300M - switch - 48 ports - managed - rack-mountable Aruba JL625A - Aruba 8325-48Y8C BF 6 F 2 PS Bdl Aruba XC-XGE-48P-DL2 Ethernet switch (Dell S3048-ON) Dell XC-XGT-48P-DL2 Ethernet switch (Dell S4048-ON) Dell Format Spine and leaf switches use the format xXcChHsS. LeafBMC switches use xXcCwW. CDU switches use dDwW.\nReference diagram for subsequent sections Diagram of a cabinet with side-by-side switches in SHCD.\nDirections Identify the switches in the SHCD.\nLook for the following:\nThe slot numbers for the LeafBMC switches (usually 48-port switches)\nIn the above diagram this is x3000u22 The slot numbers for the spine switches\nIn the above diagram these are x3000u23R and x3000u23L (two side-by-side switches) Newer side-by-side switches use slot numbers of s1 and s2 instead of R and L Each spine or leaf switch will follow this format: xXcChHsS:\nThis format also applies to CDU switches that are in a River cabinet that make connections to an adjacent Hill cabinet.\nxX : where X is the River cabinet identification number (the figure above is 3000). cC : where C is the chassis identification number. This should be 0. hH : where H is the slot number in the cabinet (height). sS : where S is the horizontal space number. Each LeafBMC switch will follow this format: xXcCwW:\nxX : where X is the River cabinet identification number (the figure above is 3000). cC : where C is the chassis identification number. This should be 0. wW : where W is the slot number in the cabinet (height). Each CDU switch will follow this format: dDwW:\nIf a CDU switch is in a River cabinet, then follow the naming convention in earlier \u0026ldquo;spine or leaf switch\u0026rdquo; step instead.\ndD : where D is the Coolant Distribution Unit (CDU) wW : where W is the management switch in a CDU Each line in the file must specify the type: leaf, CDU, LeafBMC, or Spine.\nEach line in the file must specify the brand: Dell, Mellanox, or Aruba.\nCreate the switch_metadata.csv file with this information.\nSee the example files in the next section for reference.\nExamples of switch_metadata.csv Example: Two Aruba CDU switches, two Aruba LeafBMC switches, four Aruba leaf switches, and two Aruba spine switches:\nSwitch Xname,Type,Brand d0w1,CDU,Aruba d0w2,CDU,Aruba x3000c0w31,LeafBMC,Aruba x3000c0w32,LeafBMC,Aruba x3000c0h33s1,leaf,Aruba x3000c0h34s1,leaf,Aruba x3000c0h35s1,leaf,Aruba x3000c0h36s1,leaf,Aruba x3000c0h37s1,Spine,Aruba x3000c0h38s1,Spine,Aruba Example: Two Dell CDU switches, two Dell LeafBMC switches, and two Mellanox spine switches:\nSwitch Xname,Type,Brand d0w1,CDU,Dell d0w2,CDU,Dell x3000c0w36,LeafBMC,Dell x3000c0w38,LeafBMC,Dell x3000c0h33s1,Spine,Mellanox x3000c0h34s1,Spine,Mellanox Example: Two Dell LeafBMC switches and two Mellanox switches in the same slot number:\nSwitch Xname,Type,Brand x3000c0w38,LeafBMC,Dell x3000c0w36,LeafBMC,Dell x3000c0h33s1,Spine,Mellanox x3000c0h33s2,Spine,Mellanox "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/spire_database_airgap_configuration/",
	"title": "Spire database connection pool configuration in an air-gapped environment",
	"tags": [],
	"description": "",
	"content": "Spire database connection pool configuration in an air-gapped environment Description Due to the way the resolver code works in certain versions of Alpine Linux, it may be necessary to reconfigure the spire-postgres-pooler to use the fully qualified domain name of the database in order to prevent DNS lookup errors.\nSymptoms The spire-server pods are logging query_wait_timeout errors.\ntime=\u0026#34;2022-11-15T09:39:38Z\u0026#34; level=error msg=\u0026#34;Fatal run error\u0026#34; error=\u0026#34;datastore-sql: pq: query_wait_timeout\u0026#34; time=\u0026#34;2022-11-15T09:39:38Z\u0026#34; level=error msg=\u0026#34;Server crashed\u0026#34; error=\u0026#34;datastore-sql: pq: query_wait_timeout\u0026#34; The spire-postgres-pooler pods are logging DNS lookup failure errors.\n2022-11-15 09:38:40.290 UTC [1] WARNING DNS lookup failed: spire-postgres: result=0 2022-11-15 09:38:56.211 UTC [1] WARNING DNS lookup failed: spire-postgres: result=0 2022-11-15 09:39:11.881 UTC [1] WARNING DNS lookup failed: spire-postgres: result=0 2022-11-15 09:39:27.879 UTC [1] WARNING DNS lookup failed: spire-postgres: result=0 2022-11-15 09:39:38.541 UTC [1] WARNING C-0x55729bbc56c0: spire/(nouser)@127.0.0.6:56151 pooler error: query_wait_timeout Solution Edit the spire-postgres-pooler deployment.\nncn-mw# kubectl -n spire edit deployment spire-postgres-pooler Update the PGHOST environment variable to use the fully qualified domain name.\nAn example of the deployment before being edited:\ncontainers: - env: - name: PGHOST value: spire-postgres Change PGHOST to:\ncontainers: - env: - name: PGHOST value: spire-postgres.spire.svc.cluster.local The spire-postgres-pooler pods will automatically restart to pick up the new value.\nIMPORTANT: This change will need to be reapplied if the spire Helm chart is re-installed.\nThis will be resolved in a future CSM release when the PostgreSQL operator is upgraded to a newer version.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/shrink_ceph_osds/",
	"title": "Shrink Ceph OSDs",
	"tags": [],
	"description": "",
	"content": "Shrink Ceph OSDs This procedure describes how to remove an OSD(s) from a Ceph cluster. Once the OSD is removed, the cluster is also rebalanced to account for the changes. Use this procedure to reduce the size of a cluster or to replace hardware.\nPrerequisites This procedure requires administrative privileges.\nProcedure Log in as root on the first master node (ncn-m001).\nMonitor the progress of the OSDs that have been added.\nncn-m001# ceph -s View the status of each OSD and see where they reside.\nncn-m001# ceph osd tree Reweigh the OSD(s) being removed to rebalance the cluster.\nThe first two substeps below can be skipped if there is a down drive and OSD.\nChange the weight of the OSD being removed to 0.\nThe OSD_ID value should be replaced with the ID of the OSD being removed. For example, if the ID is osd.1, the OSD_ID value would be 1 in the command below.\nncn-m001# ceph osd reweight osd.OSD_ID 0 Change the weight in the CRUSH map to 0.\nncn-m001# ceph osd crush reweight osd.OSD_ID 0 Prevent the removed OSD from getting marked up.\nncn-m001# ceph osd set noup Remove the OSD after the reweighing work is complete.\nTake down the OSD being removed.\nncn-m001# ceph osd down osd.OSD_ID Destroy the OSD.\nncn-m001# ceph osd destroy osd.OSD_ID Remove the OSD authentication key.\nncn-m001# ceph auth rm osd.OSD_ID Remove the OSD.\nncn-m001# ceph osd rm osd.OSD_ID Remove the OSD from the CRUSH map.\nncn-m001# ceph osd crush rm osd.OSD_ID Remove references to the OSDs on the storage node(s) they were located on.\nThe following commands must be run on the storage node(s) that held the OSDs being removed.\nncn-s001# umount /var/lib/ceph/osd/ceph-OSD_ID ncn-s001# rm -rf /var/lib/ceph/osd/ceph-OSD_ID Clear the flags that were set earlier in the procedure.\nncn-m001# ceph osd unset noup Monitor the cluster until the rebalancing is complete.\nncn-m001# ceph -s "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/change_the_ldap_server_ip_address_for_existing_ldap_server_content/",
	"title": "Change the LDAP Server IP Address for Existing LDAP Server Content",
	"tags": [],
	"description": "",
	"content": "Change the LDAP Server IP Address for Existing LDAP Server Content The IP address that Keycloak is using for the LDAP server can be changed. In the case where the new LDAP server has the same contents as the previous LDAP server, edit the LDAP user federation to switch Keycloak to use the new LDAP server.\nRefer to Change the LDAP Server IP Address for New LDAP Server Content if the LDAP server is being replaced by a different LDAP server that has different content.\nPrerequisites The contents of the new LDAP server are the same as the previous LDAP server. For example, it is a replica or was restored from a backup.\nProcedure Follow the steps in only one of the sections below:\nUse the Keycloak administration console UI Use the Keycloak REST API Use the Keycloak administration console UI Log in to the administration console.\nSee Access the Keycloak User Management UI for more information.\nClick on User Federation under the Configure header of the navigation panel on the left side of the page.\nClick on the LDAP provider in the User Federation table.\nThis will bring up a form to edit the LDAP user federation.\nChange the Connection URL value in the LDAP user federation form to use the new IP address.\nClick the Save button at the bottom of the form.\nClick the Synchronize all users button.\nThis may take a while depending on the number of users and groups in the LDAP server.\nWhen the synchronize process completes, the pop-up will show that the update was successful. There should be minimal or no changes because the contents of the servers are the same.\nUse the Keycloak REST API Create a function to get a token as a Keycloak master administrator.\nncn-mw# MASTER_USERNAME=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.user}\u0026#39; | base64 -d) ncn-mw# MASTER_PASSWORD=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d) ncn-mw# SITE_DOMAIN=\u0026#34;$(craysys metadata get site-domain)\u0026#34; ncn-mw# SYSTEM_NAME=\u0026#34;$(craysys metadata get system-name)\u0026#34; ncn-mw# AUTH_FQDN=\u0026#34;auth.cmn.${SYSTEM_NAME}.${SITE_DOMAIN}\u0026#34; ncn-mw# function get_master_token { curl -ks -d client_id=admin-cli -d username=\u0026#34;${MASTER_USERNAME}\u0026#34; -d password=\u0026#34;${MASTER_PASSWORD}\u0026#34; \\ -d grant_type=password \u0026#34;https://${AUTH_FQDN}/keycloak/realms/master/protocol/openid-connect/token\u0026#34; | \\ jq -r .access_token } Get the component ID for the LDAP user federation.\nncn-mw# COMPONENT_ID=$(curl -s -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; https://${AUTH_FQDN}/keycloak/admin/realms/shasta/components \\ | jq -r \u0026#39;.[] | select(.providerId==\u0026#34;ldap\u0026#34;).id\u0026#39;) ncn-mw# echo \u0026#34;${COMPONENT_ID}\u0026#34; Example output:\n57817383-e4a0-4717-905a-ea343c2b5722 Get the current representation of the LDAP user federation.\nncn-mw# curl -s -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \u0026#34;https://${AUTH_FQDN}/keycloak/admin/realms/shasta/components/${COMPONENT_ID}\u0026#34; \\ | jq . \u0026gt; keycloak_ldap.json Example of output written to the file:\n{ \u0026#34;id\u0026#34;: \u0026#34;57817383-e4a0-4717-905a-ea343c2b5722\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;shasta-user-federation-ldap\u0026#34;, \u0026#34;providerId\u0026#34;: \u0026#34;ldap\u0026#34;, \u0026#34;providerType\u0026#34;: \u0026#34;org.keycloak.storage.UserStorageProvider\u0026#34;, \u0026#34;parentId\u0026#34;: \u0026#34;09580343-fc55-4951-84ee-1c73b3a7ad29\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;pagination\u0026#34;: [ \u0026#34;true\u0026#34; ], \u0026#34;fullSyncPeriod\u0026#34;: [ \u0026#34;-1\u0026#34; ], \u0026#34;[...]\u0026#34;, \u0026#34;connectionUrl\u0026#34;: [ \u0026#34;ldap://10.248.0.59\u0026#34; ], \u0026#34;[...]\u0026#34; } Edit the keycloak_ldap.json file.\nSet the connectionUrl string to the new URL with the new IP address.\nncn-mw# vi keycloak_ldap.json Apply the updated keycloak_ldap.json file to the Keycloak server.\nThe output should show that the response code is HTTP/2 204.\nncn-mw# curl -i -XPUT -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d @keycloak_ldap.json \\ \u0026#34;https://${AUTH_FQDN}/keycloak/admin/realms/shasta/components/${COMPONENT_ID}\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/power_management/system_power_on_procedures/",
	"title": "System Power On Procedures",
	"tags": [],
	"description": "",
	"content": "System Power On Procedures The procedures in this section detail the high-level tasks required to power on an HPE Cray EX system.\nImportant: If an emergency power off (EPO) event occurred, then see Recover from a Liquid-Cooled Cabinet EPO Event for recovery procedures.\nIf user IDs or passwords are needed, then see step 1 of the Prepare the System for Power Off procedure.\nNote about services used during system power on The Cray Advanced Platform Monitoring and Control (CAPMC) service controls power to major components. CAPMC sequences the power on tasks in the correct order, but does not determine if the required software services are running on the components. The Boot Orchestration Service (BOS) manages and configures power on and boot tasks. The System Admin Toolkit (SAT) automates boot and shutdown services by stage. Power on cabinet circuit breakers and PDUs Always use the cabinet power-on sequence for the site.\nThe management cabinet is the first part of the system that must be powered on and booted. Management network and Slingshot fabric switches power on and boot when cabinet power is applied. After cabinets are powered on, wait at least 10 minutes for systems to initialize.\nAfter all the system cabinets are powered on, be sure that all management network and Slingshot network switches are powered on, and that there are no error LEDS or hardware failures.\nPower on the external Lustre file system To power on an external Lustre file system (ClusterStor), refer to Power On the External Lustre File System.\nPower on and boot the Kubernetes management cluster To power on the management cabinet and bring up the management Kubernetes cluster, refer to Power On and Start the Management Kubernetes Cluster.\nPower on compute and IO cabinets To power on all liquid-cooled cabinet CDUs and cabinet PDUs, refer to Power On Compute and IO Cabinets.\nPower on and boot compute nodes and user access nodes (UANs) To power on and boot compute nodes and UANs, refer to Power On and Boot Compute and User Access Nodes and make nodes available to users.\nRun system health checks After power on, refer to Validate CSM Health to check system health and status.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/troubleshoot_loss_of_console_connections_and_logs_on_gigabyte_nodes/",
	"title": "Troubleshoot Loss of Console Connections and Logs on Gigabyte Nodes",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Loss of Console Connections and Logs on Gigabyte Nodes Problem Gigabyte console log information will no longer be collected. If attempting to initiate a console session through Cray console services, there will be an error reported. This error will occur every time the node is rebooted unless this workaround is applied.\nPrerequisites Console log information is no longer being collected for Gigabyte nodes or ConMan is reporting an error.\nProcedure Deactivate the current console connection.\nEnter root user password for the BMC of the affected node.\nread -s is used to prevent the password from being displayed on the screen or preserved in the shell history.\nncn# USERNAME=root ncn# read -r -s -p \u0026#34;BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD Export the variable.\nncn# export IPMI_PASSWORD Deactivate the SOL session for the node.\nIn the following command, replace XNAME with the component name (xname) of the BMC of the affected node.\nncn# ipmitool -I lanplus -H XNAME -U \u0026#34;${USERNAME}\u0026#34; -E sol deactivate Manually open a console connection to the node using the Cray console services.\nThis is necessary to force the ConMan reconnection after closing the SOL session. See Log in to a Node Using ConMan.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/build_ncn_images_locally/",
	"title": "Build NCN Images Locally",
	"tags": [],
	"description": "",
	"content": "Build NCN Images Locally Build and test NCN images locally by using the following procedure. This procedure can be done on any x86 machine with the following prerequisites.\nNecessary software The listed software below will equip a local machine or build server to build for any medium (.squashfs, .vbox, .qcow2, .iso).\nmedia (.iso, .ovf, or .qcow2) (depending on the layer) packer qemu envsubst Media packer can intake any ISO, the sections below detail utilized base ISOs in CRAY HPCaaS.\nFor any ISO, copy it into the iso directory.\nSuSE Linux Enterprise The file name is either:\nSLE-15-SP2-Full-x86_64-GM-Media1.iso SLE-15-SP3-Full-x86_64-GM-Media1.iso Repositories Access is required to the appropriate SLES repositories in the form of official access, self-hosted access, or the provided Nexus access.\nTo build locally, an administrator must provide their own repositories file and the custom_repos_file variable must be passed with the packer command. The custom_repos_file variable is a filename that is placed into the custom folder of the project. The file must be formatted with the following fields: url name flags\n-var 'custom_repos_file=custom.repos' And example entry for a custom repository:\nhttps://myserver.net/sles-mirror/Products/SLE-Module-Basesystem/15-SP3/x86_64/product SUSE-SLE-Module-Basesystem-15-SP3-x86_64-Pool -g -p 99 suse/SLE-Module-Basesystem/15-SP3/x86_64/product\nBuild steps There are two Providers that can be built; VirtualBox and QEMU VirtualBox is best for local development. QEMU is best for pipeline and portability on Linux machines. Both outputs are capable of creating the kernel, initrd, and SquashFS required to boot nodes. Setup Install packer from a reputable source, like packer.io. If building QEMU images in MacOS, then specific QEMU options must be adjusted:\nMacOS requires HVF for acceleration MacOS uses Cocoa for output -var 'qemu_display=cocoa' -var 'qemu_accelerator=hvf' Notes Setup environment variables by copying scripts/environment.template to scripts/environment and modifying the values for the specific environment. Ensure that SLE-15-SP3-Full-x86_64-GM-Media1.iso is in the iso/ folder Check out csm-rpms repository and create a symlink to it in the root directory of the project. Execute source scripts/environment Execute ./scripts/setup.sh Render the autoinst template Quick start git clone \u0026lt;csm-rpms-repo\u0026gt; git clone https://github.com/Cray-HPE/node-image-build.git cd node-image-build ln -s ../csm-rpms/ csm-rpms cp scripts/environment.template scripts/environment vim scripts/environment source scripts/environment mkdir -p iso wget https://\u0026lt;somepath\u0026gt;/SLE-15-SP3-Full-x86_64-GM-Media1.iso -O iso/SLE-15-SP3-Full-x86_64-GM-Media1.iso ./scripts/setup.sh Base layer The base layer will install SLES 15 and prepare the image for the installation of Kubernetes and Ceph.\nExecute the following commands from the top level of the project\nTo build with QEMU, run the following command.\nRun packer build -only=qemu.sles15-base -var 'ssh_password=$SLES15_INITIAL_ROOT_PASSWORD' boxes/sles15-base/ To build with VirtualBox, run the following command.\nRun packer build -only=virtualbox-iso.sles15-base -var 'ssh_password=$SLES15_INITIAL_ROOT_PASSWORD' boxes/sles15-base/ In order to view the output of the build, disable headless mode:\nRun packer build -var 'ssh_password=$SLES15_INITIAL_ROOT_PASSWORD' -var 'headless=false' boxes/sles15-base/ Once the images are built, the output will be placed in the output-sles15-base directory in the root of the project.\nCommon layer The common layer starts from the output of the base layer. As such the base layer must be created before building common.\nTo build with QEMU, run the following command.\nRun packer build -only=qemu.ncn-common -var 'ssh_password=$SLES15_INITIAL_ROOT_PASSWORD' boxes/ncn-common/ To build with VirtualBox, run the following command.\nRun packer build -only=virtualbox-ovf.ncn-common -var 'ssh_password=$SLES15_INITIAL_ROOT_PASSWORD' boxes/ncn-common/ Once the image is built, the output will be placed in the output-ncn-common directory in the root of the project.\nNon-Compute Node image layer The ncn-node-images stage builds on top of the common layer to create functional images for Kubernetes and Ceph.\nTo build with QEMU, run the following command.\nRun packer build -only=qemu.* -var 'ssh_password=$SLES15_INITIAL_ROOT_PASSWORD' boxes/ncn-node-images/ To build with VirtualBox, run the following command.\nRun packer build -only=virtualbox-ovf.* -var 'ssh_password=$SLES15_INITIAL_ROOT_PASSWORD' boxes/ncn-node-images/ Once the images are built, the output will be placed in the output-sles15-images directory in the root of the project.\nArtifacts Each layer creates a certain set of artifacts that can be used in different ways.\nEach layer creates a VM disk image that can be directly booted and/or used to create the next layer\u0026rsquo;s image. Each layer after sles15-base creates a list of packages and repos. ncn-common creates kernel and initrd artifacts. ncn-node-images creates kernel, initrd, and SquashFS artifacts. Versioning The version of the build is passed with the packer build command as the artifact_version var: packer build -only=qemu.sles15-base -var \u0026#34;artifact_version=`git rev-parse --short HEAD`\u0026#34; -var \u0026#39;ssh_password=$SLES15_INITIAL_ROOT_PASSWORD\u0026#39; -var \u0026#39;headless=false\u0026#39; boxes/sles15-base/ If no version is passed to the builder then the version none is used when generating the archive. "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/igmp/",
	"title": "IGMP",
	"tags": [],
	"description": "",
	"content": "IGMP The Internet Group Multicast Protocol (IGMP) is a communications protocol used by hosts and adjacent routers on IP networks to establish multicast group memberships. The host joins a multicast-group by sending a join request message towards the network router, and responds to queries sent from the network router by dispatching a join report.\nRelevant Configuration\nEnable IGMP snooping globally. Run:\nswitch (config) # ip igmp snooping Enable IGMP snooping on a VLAN. Run:\nswitch (config) # vlan 2 switch (config vlan 2) # ip igmp snooping (Optional) Verify the IGMP snooping querier configuration. Run:\nswitch (config vlan 10)# show ip igmp snooping querier Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/remote_logging/",
	"title": "Configure Remote Logging",
	"tags": [],
	"description": "",
	"content": "Configure Remote Logging Configure remote logging to view log files from the switch on a remote server. This functionality is enabled by syslog.\nConfiguration Commands Configure logging:\nswitch(config)# logging server dell.com severity log-info Expected Results Administrators can configure remote logging Administrators can see the log files from the switch on the remote server Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/duplicate_ip/",
	"title": "Check for Duplicate IP Addresses",
	"tags": [],
	"description": "",
	"content": "Check for Duplicate IP Addresses It is common to get an IP address that is not the correct one. A sign of a duplicate IP address is seeing a DECLINE message from the client to the server.\nFor example:\n10.40.0.0.337 \u0026gt; 10.42.0.58.67: BOOTP/DHCP, Request from b4:2e:99:be:1a:d3, length 301, hops 1, xid 0x9d1210d, Flags [none] Gateway-IP 10.252.0.2 Client-Ethernet-Address b4:2e:99:be:1a:d3 Vendor-rfc1048 Extensions Magic Cookie 0x63825363 DHCP-Message Option 53, length 1: Decline Client-ID Option 61, length 19: hardware-type 255, 99:be:1a:d3:00:01:00:01:26:c8:55:c3:b4:2e:99:be:1a:d3 Server-ID Option 54, length 4: 10.42.0.58 Requested-IP Option 50, length 4: 10.252.0.26 Agent-Information Option 82, length 22: Circuit-ID SubOption 1, length 20: vlan2-ethernet1/1/12 To test for duplicate IP addresses, ping the suspected address while turning off the node. If there continues to be responses, then there is a duplicate IP address.\nBack to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/kubernetes_networking/",
	"title": "Kubernetes Networking",
	"tags": [],
	"description": "",
	"content": "Kubernetes Networking Every Kubernetes pod has an IP address in the pod network that is reachable within the cluster. The system uses the weave-net plugin for inter-node communication.\nAccess services from outside the cluster All services with a REST API must be accessed from outside the cluster using the Istio Ingress Gateway. This gateway can be accessed using a URL in the following formats:\nhttps://api.cmn.SYSTEM-NAME_DOMAIN-NAME https://api.can.SYSTEM-NAME_DOMAIN-NAME https://api.chn.SYSTEM-NAME_DOMAIN-NAME The API requests then get routed to the appropriate node running that service.\nAccess services from within the cluster All services running inside the cluster can access each other using their pod IP address or the service\u0026rsquo;s cluster IP address, along with the service\u0026rsquo;s exposed port. The exception to this is a service that has a Cray REST API. These services are configured such that they must be accessed through the API gateway service.\nNetwork policies Kubernetes supports network policies to limit access to pods. Therefore, services running inside the cluster generally cannot access each other using their pod IP address or the service\u0026rsquo;s cluster IP address. Any other services that must be accessed through a protocol other than REST, can do so using the cluster VIP and the service\u0026rsquo;s NodePort. Only services that are configured to expose a NodePort or ExternalIP can be accessed from outside the cluster.\nAs part of the SMS installation, the following network policies are configured on the system:\nkeycloak-database: Allows only keycloak to access the keycloak Postgres instance sma-zookeeper: Allows only Apache Kafka to access the SMA Zookeeper instance sma-postgres: Allows only Grafana to access the SMA Postgres instance hms-mariadb: Allows only SMD to access the MariaDB instance hms-badger: Allows only badger services to access the badger Postgres instance api-gateway-database: Allows only the API gateway to access the API gateway Postgres instance api-gateway-upstream: Allows only the API gateway to access the upstream services vcs-database: Allows only Gitea to access the VCS instance To learn more about Kubernetes, refer to https://kubernetes.io/.\n"
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/set_bmc_management_role/",
	"title": "Set BMC Management Roles",
	"tags": [],
	"description": "",
	"content": "Set BMC Management Roles The ability to ignore non-compute nodes (NCNs) is turned off by default. Management nodes and NCNs are also not locked by default. The administrator must lock the NCNs and their BMCs to prevent unwanted actions from affecting these nodes. To more easily identify the BMCs that are associated with the management nodes, they need to be marked with the Management role in the Hardware State Manager (HSM), just like their associated nodes.\nThis section only covers marking BMCs of management nodes with the Management role using HSM. For more information on locking or ignoring nodes, refer to the following sections:\nHardware State Manager (HSM) See Lock and Unlock Nodes. Firmware Action Service (FAS) See Ignore Node within FAS. Cray Advanced Platform Monitoring and Control (CAPMC) See Ignore Nodes with CAPMC. Topics Prerequisites When to set BMC management role How to set BMC management role Prerequisites The Cray Command Line Interface must be configured on the NCN where this procedure is being performed. See Configure the Cray CLI.\nWhen to set BMC management role The BMCs of NCNs should be marked with the Management role as early as possible in the install/upgrade cycle to prevent unintentionally taking down a critical node. The Management role on the BMCs cannot be set until after Kubernetes is running and the HSM service is operational.\nCheck whether HSM is running with the following command:\nncn-mw# kubectl -n services get pods | grep smd Example output:\ncray-smd-848bcc875c-6wqsh 2/2 Running 0 9d cray-smd-848bcc875c-hznqj 2/2 Running 0 9d cray-smd-848bcc875c-tp6gf 2/2 Running 0 6d22h cray-smd-init-2tnnq 0/2 Completed 0 9d cray-smd-postgres-0 2/2 Running 0 19d cray-smd-postgres-1 2/2 Running 0 6d21h cray-smd-postgres-2 2/2 Running 0 19d cray-smd-wait-for-postgres-4-7c78j 0/3 Completed 0 9d The cray-smd pods need to be in the Running state, except for cray-smd-init and cray-smd-wait-for-postgres which should be in Completed state.\nHow to set BMC management role Use the cray hsm state components bulkRole update command to perform setting roles on the BMC.\nHow to set BMC management roles on all BMCs of management nodes Get the list of BMCs of management nodes.\nncn-mw# BMCList=$(cray hsm state components list --role Management --type Node --format json | jq -r .Components[].ID | \\ sed \u0026#39;s/n[0-9]*//\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/.$//\u0026#39;) ncn-mw# echo ${BMCList} Example output:\nx3000c0s1b0,x3000c0s2b0,x3000c0s3b0,x3000c0s6b0,x3000c0s5b0,x3000c0s4b0,x3000c0s7b0,x3000c0s8b0,x3000c0s9b0 Set the Management role for those BMCs.\nncn-mw# cray hsm state components bulkRole update --role Management --component-ids \u0026#34;${BMCList}\u0026#34; This command gives no output when it completes successfully.\nHow to set BMC management roles on specific BMCs of management nodes Set the Management role for specific BMCs.\nncn-mw# cray hsm state components bulkRole update --role Management --component-ids x3000c0s8b0 This command gives no output when it completes successfully.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/delete_cfs_sessions/",
	"title": "Delete CFS Sessions",
	"tags": [],
	"description": "",
	"content": "Delete CFS Sessions Delete an existing Configuration Framework Service (CFS) configuration session with the CFS delete command.\nPrerequisites This requires that the Cray command line interface is configured. See Configure the Cray Command Line Interface.\nDelete single CFS session Use the session name to delete the session:\nncn# cray cfs sessions delete \u0026lt;session_name\u0026gt; No output is expected.\nDelete multiple CFS sessions To delete all completed CFS sessions, use the deleteall command.\nncn# cray cfs sessions deleteall This command can also filter the sessions to delete based on tags, name, status, age, and success or failure. By default, if no other filter is specified, this command only deletes completed sessions.\nDelete old CFS sessions automatically Completed CFS sessions can be automatically deleted based on age. See the Automatic Session Deletion with sessionTTL section.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/limit_the_scope_of_a_bos_session/",
	"title": "Limit the Scope of a BOS Session",
	"tags": [],
	"description": "",
	"content": "Limit the Scope of a BOS Session The Boot Orchestration Service (BOS) supports an optional limit parameter when creating a session. This parameter can be used to further limit the nodes that BOS runs against, and is applied to all boot sets.\nThe limit parameter takes a comma-separated list of nodes, groups, or roles in any combination. The BOS session will be limited to run against components that match both the boot set information and one or more of the nodes, groups, or roles listed in the limit.\nWhen specifying nodes, component names (xnames) must be used. The use of NIDs is not supported.\nThe table below describes the operations that can be used to further limit the scope of a BOS session. Components are treated as OR operations unless preceded by one of the operations listed in the following table.\nOperation Description \u0026amp; Added to the beginning of a group or role to specify an intersection of groups. ! Added to the beginning of a node, group, or role to exclude it. all When only trying to exclude a node or group, the limit must start with all. * Same as all The table below helps demonstrate the logic used with the limit parameter and includes examples of how to limit against different nodes, groups, and roles.\nDescription Pattern Targets All nodes all or * (or leave empty) All nodes One node node1 node1 Multiple nodes node1,node2 node1 and node2 Excluding a node all,!node1 All nodes except node1 One group group1 Nodes in group1 Multiple groups group1,group2 Nodes in group1 or group2 Excluding groups group1,!group2 Nodes in group1 but not in group2 Intersection of groups group1,\u0026amp;group2 Nodes in both group1 and group2 The limit parameter for BOS works similarly to the --ansible-limit parameter for CFS, as well as the limit parameter for Ansible. Some limitations do apply for those familiar with the Ansible syntax. BOS accepts only a comma-separated list, not colons, and does not support regular expressions in the patterns. For more information on what it means to provide a limit, see Specifying Hosts and Groups.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_uai_class/",
	"title": "Delete a UAI Class",
	"tags": [],
	"description": "",
	"content": "Delete a UAI Class Delete a UAI class. After deletion, the class will no longer be available for creation of UAIs. Existing UAIs are unaffected.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Class ID of the UAI Class to be deleted: List UAI Classes Procedure Delete a UAI Class by using a command of the following form:\ncray uas admin config classes delete UAI_CLASS_ID UAI_CLASS_ID is the UAI Class ID of the UAI class.\nDelete a UAI class.\nncn-m001-pit# cray uas admin config classes delete bb28a35a-6cbc-4c30-84b0-6050314af76b Top: User Access Service (UAS)\nNext Topic: UAI Management\n"
},
{
	"uri": "/docs-csm/en-12/install/csm_installation_failure/",
	"title": "CSM Services Install Fails Because of Missing Secret",
	"tags": [],
	"description": "",
	"content": "CSM Services Install Fails Because of Missing Secret When running the install script in the Install CSM Services procedure, it may fail due to a timing-related issue. This page documents how to determine if this problem was the cause of an install script failure, and the appropriate remediation steps to take if it is encountered.\nHow to determine if an install hit this issue Verify that the installation script output contains an error about a missing secret.\nExample snippet from the install script output:\nERROR Step: Set Management NCNs to use Unbound --- Checking Precondition + Getting admin-client-auth secret Error from server (NotFound): secrets \u0026#34;admin-client-auth\u0026#34; not found + Obtaining access token curl: (22) The requested URL returned error: 404 + Querying SLS curl: (22) The requested URL returned error: 503 Check the doc below for troubleshooting: If any management NCNs are missing from the output, take corrective action before proceeding. INFO Failed Pipeline/Step id: f0cd9574240989eb04118d308f26b3ea exit status 22 If the above secrets \u0026quot;admin-client-auth\u0026quot; not found error is observed, then proceed to the next step.\nVerify that keycloak-setup has this issue.\nLook for a keycloak-setup pod still in the Running state:\npit# kubectl get pods -n services | grep keycloak-setup Example output:\nkeycloak-setup-1-xj9s5 1/2 Running 0 32m Check the istio-proxy container logs for the keycloak-setup pod found in the previous step.\nIn the following command, substitute the name of the keycloak-setup pod found in the previous step.\npit # kubectl logs --namespace services -n services KEYCLOAK-SETUP-POD-NAME --container istio-proxy | grep \u0026#39;[[:space:]]503[[:space:]]\u0026#39; | grep SDS | tail -n2 If the output looks similar to the following, then proceed to the remediation steps.\n[2022-05-27T13:21:24.535Z] \u0026#34;POST /keycloak/realms/master/protocol/openid-connect/token HTTP/1.1\u0026#34; 503 UF,URX \u0026#34;TLS error: Secret is not supplied by SDS\u0026#34; 96 159 16 - \u0026#34;-\u0026#34; \u0026#34;python-requests/2.27.1\u0026#34; \u0026#34;19fe9b72-d887-4649-b934-9dc7bc76cc21\u0026#34; \u0026#34;keycloak.services:8080\u0026#34; \u0026#34;10.44.0.31:8080\u0026#34; outbound|8080||keycloak.services.svc.cluster.local - 10.28.81.125:8080 10.32.0.25:60032 - default [2022-05-27T13:21:34.573Z] \u0026#34;POST /keycloak/realms/master/protocol/openid-connect/token HTTP/1.1\u0026#34; 503 UF,URX \u0026#34;TLS error: Secret is not supplied by SDS\u0026#34; 96 159 61 - \u0026#34;-\u0026#34; \u0026#34;python-requests/2.27.1\u0026#34; \u0026#34;ef0255b4-260b-47b5-8077-3e17e9371baf\u0026#34; \u0026#34;keycloak.services:8080\u0026#34; \u0026#34;10.44.0.31:8080\u0026#34; outbound|8080||keycloak.services.svc.cluster.local - 10.28.81.125:8080 10.32.0.25:60964 - default Remediate the problem Delete the current keycloak-setup pod.\nIn the following command, substitute the name of the keycloak-setup pod found in the previous section.\npit # kubectl delete pod --namespace services KEYCLOAK-SETUP-POD-NAME Find the pod name of the new keycloak-setup pod by using the same kubectl get pods command from the previous section.\nEnsure that the new keycloak-setup pod completed setup:\nIn the following command, substitute the name of the new keycloak-setup pod found in the previous step.\npit # kubectl logs --namespace services -n services NEW-KEYCLOAK-SETUP-POD-NAME --container keycloak-setup | tail -n 3 Example output indicating that it has completed setup:\n2022-05-27 14:12:25,251 - INFO - keycloak_setup - Deleting \u0026#39;keycloak-gatekeeper-client\u0026#39; Secret in namespace \u0026#39;services\u0026#39;... 2022-05-27 14:12:25,264 - INFO - keycloak_setup - The \u0026#39;keycloak-gatekeeper-client\u0026#39; secret in namespace \u0026#39;services\u0026#39; already doesn\u0026#39;t exit. 2022-05-27 14:12:25,264 - INFO - keycloak_setup - Keycloak setup complete Once all Keycloak pods have successfully completed, then re-run the installation script and proceed with the installation.\npit # kubectl get pods --namespace services | grep keycloak | grep -Ev \u0026#39;(Completed|Running)\u0026#39; If this command gives no output, then installation may proceed.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/spire_database_lookup_error/",
	"title": "Spire Database Cluster DNS Lookup Failure",
	"tags": [],
	"description": "",
	"content": "Spire Database Cluster DNS Lookup Failure Description There is a known issue where if Unbound is configured to forward to an invalid or inaccessible site DNS server, the Spire server may be unable to resolve the hostname of its PostgreSQL cluster.\nSymptoms The spire-server pods may be in a CrashLoopBackOff state.\nAPI calls to services may fail with HTTP 503 errors.\nThe spire-server pods contain the following error in the logs.\ntime=\u0026#34;2022-06-13T15:43:49Z\u0026#34; level=error msg=\u0026#34;Fatal run error\u0026#34; error=\u0026#34;datastore-sql: dial tcp: lookup spire-postgres-pooler.spire.svc.cluster.local: Try again\u0026#34; Solution Check to see if cray-dns-unbound has a forward-addr configured.\nCommand:\nncn-mw# kubectl -n services get cm cray-dns-unbound -o yaml | grep forward-addr Example output:\nforward-addr: 172.30.84.40 Check to see if the forward-addr is accessible from the worker nodes.\nAttempt to ping it.\nncn-w# ping -c 1 172.30.84.40 Example output:\nPING 172.30.84.40 (172.30.84.40) 56(84) bytes of data. 64 bytes from 172.30.84.40: icmp_seq=1 ttl=58 time=0.175 ms --- 172.30.84.40 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.175/0.175/0.175/0.000 ms Attempt to resolve a hostname external to the system.\nncn-w# host www.google.com 172.30.84.40 Example output:\nUsing domain server: Name: 172.30.84.40 Address: 172.30.84.40#53 Aliases: www.google.com has address 209.85.234.106 www.google.com has address 209.85.234.103 www.google.com has address 209.85.234.147 www.google.com has address 209.85.234.105 www.google.com has address 209.85.234.99 www.google.com has address 209.85.234.104 www.google.com has IPv6 address 2607:f8b0:4001:c17::63 www.google.com has IPv6 address 2607:f8b0:4001:c17::93 www.google.com has IPv6 address 2607:f8b0:4001:c17::6a www.google.com has IPv6 address 2607:f8b0:4001:c17::67 If the above checks fail, then verify that the Customer Management Network (CMN) is working correctly. See Troubleshoot CMN issues for more information.\nIf it is not possible to restore access to the forward-addr, then reconfigure Unbound to point to a working DNS server, or temporarily remove the forwarder configuration. See \u0026ldquo;Change the Site DNS Server\u0026rdquo; in Manage the DNS Unbound Resolver for more information.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/troubleshoot_ceph-mon_processes_stopping_and_exceeding_max_restarts/",
	"title": "Troubleshoot Ceph-Mon Processes Stopping and Exceeding Max Restarts",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Ceph-Mon Processes Stopping and Exceeding Max Restarts Troubleshoot an issue where all of the ceph-mon processes stop and exceed their maximum amount of attempts at restarting. This bug corrupts the health of the Ceph cluster.\nReturn the Ceph cluster to a healthy state by resolving issues with ceph-mon processes.\nPrerequisites This procedure requires admin privileges.\nProcedure See Collect Information about the Ceph Cluster for more information on how to interpret the output of the Ceph commands used in this procedure.\nLog on to the manager nodes via ssh.\nThe commands in the next step will need to be run on each manager node.\nVerify the ceph-mon process is running as expected.\nCheck to see if the ceph-mon process is running on all of the manager nodes.\nThis command needs to be run on each manager node to determine where the issues are occurring. Make a note of which nodes do not have the ceph-mon process running.\nncn-m001# ps -ef |grep ceph-mon Example output:\nroot 24465 24175 0 10:04 pts/0 00:00:00 grep ceph-mon ceph 33480 1 0 Jan15 ? 00:11:36 /usr/bin/ceph-mon -f --cluster ceph --id ncn-m001 --setuser ceph --setgroup ceph \u0026lt;\u0026lt;-- If missing, it is not running Restart the ceph-mon process on any node where it was not running.\nThis is expected to crash again, but this is a good way to verify there is an issue.\nncn-s00(1/2/3)# systemctl daemon-reload ncn-s00(1/2/3)# ceph orch daemon restart mon.\u0026lt;hostname\u0026gt; Check the health of the Ceph cluster on one of the manager nodes.\nThis command will report a HEALTH_WARN status. There will be a message below this warning indicating that a ceph-mon node or multiple ceph-mon nodes are out of quorum.\nncn-s00(1/2/3)# ceph -s To watch nodes that drop out of quorum, run the following command:\nncn-s00(1/2/3)# ceph -ws Once it is clear that the ceph-mon processes keep crashing across all of the manager nodes, proceed to the next step. If only a single ceph-mon process on a manager node is having issues, then a different issue is occurring.\nRestart the ceph-mds services on all manager nodes.\nncn-s00(1/2/3)# ceph orch daemon restart mds.cephfs.\u0026lt;container id\u0026gt; Restart the ceph-mon process on all manager nodes.\nncn-m001# systemctl daemon-reload ncn-s00(1/2/3)# ceph orch daemon restart mon.\u0026lt;hostname\u0026gt; Monitor the cluster to ensure the ceph-mon processes are running on all manager nodes.\nThe health status should return to reporting as HEALTH_OK. Monitor the health of the cluster over the next 30 minutes to ensure the debugging was successful.\nncn-s00(1/2/3)# ceph -s "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/change_the_ldap_server_ip_address_for_new_ldap_server_content/",
	"title": "Change the LDAP Server IP Address for New LDAP Server Content",
	"tags": [],
	"description": "",
	"content": "Change the LDAP Server IP Address for New LDAP Server Content Delete the old LDAP user federation and create a new one. This procedure should only be done if the LDAP server is being replaced by a different LDAP server that has different contents.\nRefer to Change the LDAP Server IP Address for Existing LDAP Server Content if the new LDAP server content matches the previous LDAP server content.\nPrerequisites The LDAP server is being replaced by a different LDAP server that has different contents. For example, different users and groups.\nProcedure Remove the LDAP user federation from Keycloak.\nFollow the procedure in Remove the LDAP User Federation from Keycloak.\nRe-add the LDAP user federation in Keycloak.\nFollow the procedure in Add LDAP User Federation.\n"
},
{
	"uri": "/docs-csm/en-12/operations/power_management/user_access_to_compute_node_power_data/",
	"title": "User Access to Compute Node Power Data",
	"tags": [],
	"description": "",
	"content": "User Access to Compute Node Power Data Shasta Liquid Cooled AMD EPYC compute node power management data available to users.\nShasta Liquid Cooled compute blade power management counters (pm_counters) enable users access to energy usage over time for billing and job profiling.\nThe blade-level and node-level accumulated energy telemetry is point-in-time power data. Blade accumulated energy data is collected out-of-band and is made available via workload managers. Users have access to the data in-band at the node-level via a special sysfs files in /sys/cray/pm\\_counters on the node.\nTime-stamped energy data from each node can be captured for a specific job before, during, and after the job to generate a power profile about the job. This energy usage data can be used in conjunction with current energy costs to assign a monetary value to the job.\nThe node CPU vendor provides specific in-band and out-of-band interfaces for controlling power management. In-band interfaces are accessed from the node OS through /sys/cray/pm\\_counters. Out-of-band interfaces are accessed from a node BMC or Redfish API.\nNote that each node has a power supply that can support a fixed number of Watts. The combined power consumption of the CPU and the accelerator can never exceed this limit, thus, power to either the CPU or the accelerator must be capped so as not to exceed the total amount of power available.\npm_counters Access to compute node power and energy data is provided by a set of files located in /sys/cray/pm\\_counters/ on the node. All pm_counters are accompanied by a timestamp.\nFile Description power Point-in-time power (Watts). When accelerators are present, includes accel_power. See limitation below on data collection from accelerators. energy Accumulated energy, in joules. When accelerators are present, includes accel_energy. See limitation below on data collection from accelerators. cpu_power Point-in-time power (Watts) used by the CPU domain. cpu_energy The total energy (Joules) used by the CPU domain. cpu_temp Temperature reading (Celsius) of the CPU domain. memory_power Point-in-time power (Watts) used by the memory domain. memory_energy The total energy (Joules) used by the memory domain. accel_energy Accumulated accelerator energy (Joules). The data is non-zero only when an accelerator is present on the node. accel_power Accelerator point-in-time power (Watts). The data is non-zero only when an accelerator is present on the node. generation A counter that increments each time a power cap value is changed. startup Startup counter. freshness Free-running counter that increments at a rate of approximately 10Hz. version Version number for power management counter support. power_cap Current power cap limit in Watts; 0 indicates no capping. When accelerators are present, includes accel_power_cap. raw_scan_hz The power management scanning rate for all data in pm_counters. "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/update_compute_node_mellanox_hsn_nic_firmware/",
	"title": "Update Compute Node Mellanox HSN NIC Firmware",
	"tags": [],
	"description": "",
	"content": "Update Compute Node Mellanox HSN NIC Firmware This procedure updates liquid-cooled or standard rack compute node NIC mezzanine cards (NMC) firmware for Slingshot 10 Mellanox ConnectX-5 NICs. The deployed RPM on compute nodes contains the scripts and firmware images required to perform the firmware and configuration updates.\nAttention: The NIC firmware update is performed while the node is running the compute image (in-band). Use the CX-5 NIC firmware that is deployed with the compute node RPMs and not from some other repository.\nSee Update Firmware with FAS for information about automated firmware updates using Redfish.\nTime required 2-5 minutes for a firmware update and 1-3 minutes for a configuration update.\nProcedure SSH to the node as root.\nLoad the module.\nnid# module load cray-shasta-mlnx-firmware nid# module show cray-shasta-mlnx-firmware Example output:\n------------------------------------------------------------------- /opt/cray/modulefiles/cray-shasta-mlnx-firmware/1.0.5: module-whatis \u0026#34;This module adds cray-shasta-mlnx-firmware v1.0.5 to the environment\u0026#34; prepend-path PATH /opt/cray/cray-shasta-mlnx-firmware/1.0.5/sbin ------------------------------------------------------------------- List the contents of the firmware directories.\nnid# ls /opt/cray/cray-shasta-mlnx-firmware/1.0.5/share/firmware/* Example output:\napply_mlnx_configs generate_mlnx_configs update_mlnx_firmware nid# ls /opt/cray/cray-shasta-mlnx-firmware/1.0.5/share/firmware/ Example output:\nCRAY000000001/ MT_0000000011/ images/ nid# ls /opt/cray/cray-shasta-mlnx-firmware/1.0.5/share/firmware/ Example output:\nCRAY000000001/ MT_0000000011/ images/ nid# ls /opt/cray/cray-shasta-mlnx-firmware/1.0.5/share/firmware/* Example output:\n/opt/cray/cray-shasta-mlnx-firmware/1.0.5/share/firmware/CRAY000000001: config.xml fw-ConnectX5-rel-16_26_4012-Cray_Timms_mezz_100G_1P-UEFI-14.19.17-FlexBoot-3.5.805.bin /opt/cray/cray-shasta-mlnx-firmware/1.0.5/share/firmware/MT_0000000011: config.xml fw-ConnectX5-rel-16_26_4012-MCX515A-CCA_Ax-UEFI-14.19.17-FlexBoot-3.5.805.bin /opt/cray/cray-shasta-mlnx-firmware/1.0.5/share/firmware/images: CRAY000000001.bin MT_0000000011.bin Update the firmware on the node.\nnid# update_mlnx_firmware Apply the configuration settings.\nnid# apply_mlnx_configs Determine the prepend pathname.\nnid# module show cray-shasta-mlnx-firmware Example output:\n------------------------------------------------------------------- /opt/cray/modulefiles/cray-shasta-mlnx-firmware/1.0.5: module-whatis \u0026#34;This module adds cray-shasta-mlnx-firmware v1.0.5 to the environment\u0026#34; prepend-path PATH /opt/cray/cray-shasta-mlnx-firmware/1.0.5/sbin ------------------------------------------------------------------- Log in to ncn-m001 and use pdsh to update the firmware.\nncn-m001# pdsh -w NODE_LIST /opt/cray/cray-shasta-mlnx-firmware/1.0.5/sbin/update_mlnx_firmware Apply the configuration settings.\nncn-m001# pdsh -w NODE_LIST /opt/cray/cray-shasta-mlnx-firmware/1.0.5/sbin/apply_mlnx_configs Use the Boot Orchestration Service (BOS) to reboot all the affected nodes.\nncn-m001# cray bos session create --template-uuid SESSION_TEMPLATE --operation reboot "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/change_java_security_settings/",
	"title": "Change Java Security Settings",
	"tags": [],
	"description": "",
	"content": "Change Java Security Settings If Java will not allow a connection to an Intel node via SOL or iKVM, change Java security settings to add an exception for the node\u0026rsquo;s BMC IP address.\nThe Intel nodes ship with an insecure certificate, which causes an exception for Java when trying to connect via SOL or iKVM to these nodes. The workaround is to add the node\u0026rsquo;s BMC IP address to the Exception Site List in the Java Control Panel of the machine attempting to connect to the Intel node.\nTo add an IP address to the Exception Site List:\nJava Control Panel \u0026gt; Security \u0026gt; Edit Site List\nThe following figures show examples of the Security tab of the Java Control Panel on several different operating systems.\nLinux Java Control Panel Security Tab MacOS Java Control Panel Security Tab Windows Java Control Panel Security Tab "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/",
	"title": "Mellanox Installation and Configuration Guide",
	"tags": [],
	"description": "",
	"content": "Mellanox Installation and Configuration Guide This documentation helps network administrators and support personnel install install and manage Mellanox network devices in a CSM install.\nThe HPE Cray recommended way of configuring the network is by using the CANU tool. Therefore this guide will not go into detail on how to configure each switch manually using the CLI. Instead, it will give helpful examples of how to configure/use features generated by CANU, in order to provide administrators easy ways to customize their installation.\nAlso included in this guide are the current documented and supported network scenarios.\nNOTE: Not every configuration option is covered here; for any configuration outside of the scope of this document, refer to the official Mellanox user manual. See Mellanox.\nThis document is intended for network administrators and support personnel.\nNOTE: The display and command lines illustrated in this document are examples and might not exactly match any particular environment. The switch and accessory drawings in this document are for illustration only, and may not exactly match installed products.\nCANU See CSM Automatic Network Utility (CANU)\nExamples of Network topologies Very Large Large Medium Small Network Design Explained What is Spine-Leaf Architecture? How does a spine-leaf architecture differ from traditional network designs? Why are spine-leaf architectures becoming more popular? What is MLAG? Management Network Overview Network Types – Naming and Segment Function Network Traffic Pattern System Management Network Functions Key Features Used in the Management Network Configuration Key Feature List Typical Configuration of MLAG Between Switches Typical Configuration of MLAG Link Connecting to NCN How to Connect Management Network to a Campus Network Connect the Management Network to a Campus Network Scenario A: Network Connection via Management Network Scenario B: Network Connection via High-Speed Network Example of How to Configure Scenario A or B Managing Switches from the CLI Device Management Management Interface Network Time Protocol (NTP) Client Domain Name System (DNS) Client Exec Banners Hostname Domain Name Secure Shell (SSH) Remote Logging Web User Interface (Web UI) SNMPv2c Community SNMPv3 Users System Images Layer One Features Physical Interfaces Cable Diagnostics Layer Two Features Link Layer Discovery Protocol (LLDP) Virtual Local Access Networks (VLANs) Native VLAN VLAN Trunking 802.1Q Link Aggregation Group (LAG) MLAG Switch Configuration Multi-Chassis Link Aggregation Group (MCLAG) Multiple Spanning Tree Protocol (MSTP) Layer Three Features Routed Interfaces VLAN Interface Address Resolution Protocol (ARP) Static MAC Static Routing Loopback Interface Open Shortest Path First (OSPF) v2 BGP Basics Multicast IGMP PIM-SM Bootstrap Router (BSR) and Rendezvous-Point (RP) Security Access Control Lists (ACLs) IP filter Performing Upgrade on Mellanox Switch upgrade Backing Up Switch Configuration Backing up switch configuration Troubleshooting DHCP Confirm the status of the cray-dhcp-kea pods/services Check current DHCP leases Check HSM Check Kea DHPC logs TCPDUMP Check BGP and MetalLB Getting incorrect IP address. Duplicate IP address check Large number of DHCP declines during a node boot DNS PXE Boot NCNs on install Rebooting NCN and PXE fails Verify BGP Verify route to TFTP Check DHCP lease is getting allocated Verify DHCP traffic on workers Verify switches are forwarding DHCP traffic Computes/UANs/Application Nodes "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/reset/",
	"title": "Reset Dell Switch Configuration",
	"tags": [],
	"description": "",
	"content": "Reset Dell Switch Configuration How to reset Dell switch configuration:\nswitch(config)# delete startup-config Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/exec_banner/",
	"title": "Configure Exec Banners",
	"tags": [],
	"description": "",
	"content": "Configure Exec Banners Exec banners are custom messages displayed to users attempting to connect to the management interfaces. Multiple lines of text can be stored using a custom delimiter to mark the end of message.\nConfiguration Commands Create a banner:\nswitch(config)# banner \u0026lt;motd|exec\u0026gt; DELIM Show commands to validate functionality:\nswitch# show banner \u0026lt;motd|exec\u0026gt; Example Output switch(config)# banner exec $ Enter a new banner, when you are done enter a new line containing only your chosen delimiter. (banner-motd)# This is an example of a custom banner (banner-motd)# that spans multiple lines. (banner-motd)# $ switch(config)# do show banner exec Expected Results Administrators can create the Exec banner The output of the Exec banner looks correct Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/kubernetes_storage/",
	"title": "Kubernetes Storage",
	"tags": [],
	"description": "",
	"content": "Kubernetes Storage Data belonging to micro-services in the management cluster is managed through persistent storage, which provides reliable and resilient data protection for containers running in the Kubernetes cluster.\nThe backing storage for this service is currently provided by JBOD disks that are spread across several nodes of the management cluster. These node disks are managed by Ceph, and are exposed to containers in the form of persistent volumes.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/enable_ansible_profiling/",
	"title": "Enable Ansible Profiling",
	"tags": [],
	"description": "",
	"content": "Enable Ansible Profiling Ansible tasks and playbooks can be profiled in order to determine execution times and single out poor performance in runtime. The default Configuration Framework Service (CFS) ansible.cfg in the cfs-default-ansible-cfg ConfigMap does not enable these profiling tools. If profiling tools are desired, modify the default Ansible configuration file to enable them.\nProcedure Edit the cfs-default-ansible-cfg ConfigMap.\nncn# kubectl edit cm cfs-default-ansible-cfg -n services Uncomment the indicated line by removing the # character from the beginning of the line.\n#callback_whitelist = cfs_aggregator, timer, profile_tasks, profile_roles Comment out the indicated line by adding a # character to the beginning of the line.\ncallback_whitelist = cfs_aggregator New sessions will be created with profiling information available in the Ansible logs of the session pods. Alternatively, if editing the default ansible.cfg file that CFS uses is not desired, then another option is to create a new Ansible configuration to enable profiling and then direct CFS to use it. See Use a Custom ansible.cfg File for more information.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/limitations_for_gigabyte_bmc_hardware/",
	"title": "BOS Limitations for Gigabyte BMC Hardware",
	"tags": [],
	"description": "",
	"content": "BOS Limitations for Gigabyte BMC Hardware NOTE This section is for Boot Orchestration Service (BOS) v1 only. BOS v2 does not use Cray Advanced Platform Monitoring and Control (CAPMC).\nSpecial steps need to be taken when using BOS to boot, reboot, or shutdown Gigabyte hardware. Gigabyte hardware treats power off and power on requests as successful, regardless of if actually successfully completed. The power on/off requests are ignored by CAPMC if they are received within a short period of time, which is typically around 60 seconds per operation.\nThe work around for customers with Gigabyte BMC hardware is to manually serialize power off events. This is done to prevent frequent power actions from being attempted and ignored by CAPMC. From a boot orchestration perspective, this can be effectively worked around by issuing CAPMC power off commands before issuing BOS reboot commands.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_uai_image_registration/",
	"title": "Delete a UAI Image Registration",
	"tags": [],
	"description": "",
	"content": "Delete a UAI Image Registration Unregister a UAI image from UAS.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the name of the UAI Image Registration to be deleted: List Registered UAI Images Procedure Deleting a UAI image from UAS removes the UAI image registration from UAS. This procedure does not delete the actual UAI image artifact, nor does it affect UAIs currently created using the UAI Image.\nDelete a UAS image registration by using a command of the following form:\nncn-m001-pit# cray uas admin config images delete IMAGE_ID IMAGE_ID is the image ID of the UAI image registration being removed from UAS.\nFor example:\nncn-m001-pit# cray uas admin config images delete 8fdf5d4a-c190-24c1-2b96-74ab98c7ec07 Top: User Access Service (UAS)\nNext Topic: Volumes\n"
},
{
	"uri": "/docs-csm/en-12/install/deploy_final_ncn/",
	"title": "Deploy Final NCN",
	"tags": [],
	"description": "",
	"content": "Deploy Final NCN The following procedure contains information for rebooting and deploying the management node that is currently hosting the LiveCD. At the end of this procedure, the LiveCD will no longer be active. The node it was using will join the Kubernetes cluster as the final of three master nodes, forming a quorum.\nIMPORTANT: While the node is rebooting, it will only be available through Serial-Over-LAN (SOL) and local terminals. This procedure entails deactivating the LiveCD, meaning the LiveCD and all of its resources will be unavailable.\nRequired services Notice of danger Hand-off Reboot Enable NCN disk wiping safeguard Clean up chrony configurations Configure DNS and NTP on each BMC Next topic 1. Required services These services must be healthy before the reboot of the LiveCD can take place. If the health checks performed earlier in the install completed successfully (Validate CSM Health), then the following platform services will be healthy and ready for reboot of the LiveCD:\nUtility Storage (Ceph) cray-bss cray-dhcp-kea cray-dns-unbound cray-ipxe cray-sls cray-tftp 2. Notice of danger An administrator is strongly encouraged to be mindful of pitfalls during this segment of the CSM install. The steps below do contain warnings themselves, but overall there are risks:\nSSH will cease to work when the LiveCD reboots; the serial console will need to be used. Rebooting a remote ISO will dump all running changes on the PIT node; USB devices are accessible after the install. The NCN will never wipe a USB device during installation. Prior to shutting down the PIT node, learning the CMN IP addresses of the other NCNs will be helpful if troubleshooting is required. This procedure entails deactivating the LiveCD, meaning the LiveCD and all of its resources will be unavailable.\n3. Hand-off The steps in this section load hand-off data before a later procedure reboots the LiveCD node.\nStart a new typescript.\nExit the current typescript, if one is active.\npit# exit Start a new typescript on the PIT node.\npit# mkdir -pv /var/www/ephemeral/prep/admin \u0026amp;\u0026amp; pushd /var/www/ephemeral/prep/admin \u0026amp;\u0026amp; script -af csm-livecd-reboot.$(date +%Y-%m-%d).txt pit# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Upload SLS file.\nNOTE: The environment variable SYSTEM_NAME must be set.\npit# csi upload-sls-file --sls-file /var/www/ephemeral/prep/${SYSTEM_NAME}/sls_input_file.json Expected output looks similar to the following:\n2021/02/02 14:05:15 Retrieving S3 credentials ( sls-s3-credentials ) for SLS 2021/02/02 14:05:15 Uploading SLS file: /var/www/ephemeral/prep/eniac/sls_input_file.json 2021/02/02 14:05:15 Successfully uploaded SLS Input File. Get a token to use for authenticated communication with the gateway.\nNOTE: api-gw-service-nmn.local is legacy, and will be replaced with api-gw-service.nmn.\npit# export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Validate that CSM_RELEASE and CSM_PATH variables are set.\nThese variables were set and added to /etc/environment during the earlier Bootstrap PIT Node step of the install. CSM_PATH should be the fully-qualified path to the expanded CSM release tarball on the PIT node.\npit# echo \u0026#34;CSM_RELEASE=${CSM_RELEASE} CSM_PATH=${CSM_PATH}\u0026#34; Upload NCN boot artifacts into S3.\nRun the following command.\npit# artdir=/var/www/ephemeral/data \u0026amp;\u0026amp; k8sdir=$artdir/k8s \u0026amp;\u0026amp; cephdir=$artdir/ceph \u0026amp;\u0026amp; csi handoff ncn-images \\ --k8s-kernel-path $k8sdir/*.kernel \\ --k8s-initrd-path $k8sdir/initrd.img*.xz \\ --k8s-squashfs-path $k8sdir/secure-*.squashfs \\ --ceph-kernel-path $cephdir/*.kernel \\ --ceph-initrd-path $cephdir/initrd.img*.xz \\ --ceph-squashfs-path $cephdir/secure-*.squashfs The end of the command output contains a block similar to this:\nRun the following commands so that the versions of the images that were just uploaded can be used in other steps: export KUBERNETES_VERSION=x.y.z export CEPH_VERSION=x.y.z Run the export commands listed at the end of the output from the previous step.\nUpload the data.json file to BSS, our cloud-init data source.\nIf any changes have been made to this file (for example, as a result of any customizations or workarounds), then use the path to the modified file instead.\nThis step will prompt for the root password of the NCNs.\npit# csi handoff bss-metadata --data-file /var/www/ephemeral/configs/data.json || echo \u0026#34;ERROR: csi handoff bss-metadata failed\u0026#34; Patch the metadata for the Ceph nodes to have the correct run commands.\npit# python3 /usr/share/doc/csm/scripts/patch-ceph-runcmd.py Ensure that the DNS server value is correctly set to point toward Unbound at 10.92.100.225 (NMN) and 10.94.100.225 (HMN).\npit# csi handoff bss-update-cloud-init --set meta-data.dns-server=\u0026#34;10.92.100.225 10.94.100.225\u0026#34; --limit Global Preserve logs and configuration files if desired (optional).\nAfter the PIT node is redeployed, all files on its local drives will be lost. It is recommended to retain some of the log files and configuration files, because they may be useful if issues are encountered during the remainder of the install.\nThe following commands create a tar archive of these files, storing it in a directory that will be backed up in the next step.\npit# mkdir -pv /var/www/ephemeral/prep/logs \u0026amp;\u0026amp; ls -d \\ /etc/dnsmasq.d \\ /etc/os-release \\ /etc/sysconfig/network \\ /opt/cray/tests/cmsdev.log \\ /opt/cray/tests/install/logs \\ /opt/cray/tests/logs \\ /root/.canu \\ /root/.config/cray/logs \\ /root/csm*.{log,txt} \\ /tmp/*.log \\ /usr/share/doc/csm/install/scripts/csm_services/yapl.log \\ /var/log/conman \\ /var/log/zypper.log 2\u0026gt;/dev/null | sed \u0026#39;s_^/__\u0026#39; | xargs tar -C / -czvf /var/www/ephemeral/prep/logs/pit-backup-$(date +%Y-%m-%d_%H-%M-%S).tgz Backup the bootstrap information from ncn-m001.\nNOTE: This preserves information that should always be kept together in order to fresh-install the system again.\nLog in and set up passwordless SSH to the PIT node.\nCopying only the public keys from ncn-m002 and ncn-m003 to the PIT node. Do not set up passwordless SSH from the PIT node or the key will have to be securely tracked or expunged if using a USB installation).\nThe ssh commands below may prompt for the NCN root password.\npit# ssh ncn-m002 cat /root/.ssh/id_rsa.pub \u0026gt;\u0026gt; /root/.ssh/authorized_keys \u0026amp;\u0026amp; ssh ncn-m003 cat /root/.ssh/id_rsa.pub \u0026gt;\u0026gt; /root/.ssh/authorized_keys \u0026amp;\u0026amp; chmod 600 /root/.ssh/authorized_keys Back up files from the PIT to ncn-m002.\npit# ssh ncn-m002 \\ \u0026#34;mkdir -pv /metal/bootstrap rsync -e \u0026#39;ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#39; -rltD -P --delete pit.nmn:/var/www/ephemeral/prep /metal/bootstrap/ rsync -e \u0026#39;ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#39; -rltD -P --delete pit.nmn:${CSM_PATH}/cray-pre-install-toolkit*.iso /metal/bootstrap/\u0026#34; Back up files from the PIT to ncn-m003.\npit# ssh ncn-m003 \\ \u0026#34;mkdir -pv /metal/bootstrap rsync -e \u0026#39;ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#39; -rltD -P --delete pit.nmn:/var/www/ephemeral/prep /metal/bootstrap/ rsync -e \u0026#39;ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#39; -rltD -P --delete pit.nmn:${CSM_PATH}/cray-pre-install-toolkit*.iso /metal/bootstrap/\u0026#34; Set the PIT node to PXE boot.\nList IPv4 boot options using efibootmgr.\npit# efibootmgr | grep -Ei \u0026#34;ip(v4|4)\u0026#34; Set and trim the boot order on the PIT node.\nThis only needs to be done for the PIT node, not for any of the other NCNs. See Setting boot order and Trimming boot order.\nTell the PIT node to PXE boot on the next boot.\nUse efibootmgr to set the next boot device to the first PXE boot option. This step assumes the boot order was set up in the previous step.\npit# efibootmgr -n $(efibootmgr | grep -Ei \u0026#34;ip(v4|4)\u0026#34; | awk \u0026#39;{print $1}\u0026#39; | head -n 1 | tr -d Boot*) | grep -i bootnext Example output:\nBootNext: 0014 Collect a backdoor login. Fetch the CMN IP address for ncn-m002 for a backdoor during the reboot of ncn-m001.\nGet the IP address.\npit# ssh ncn-m002 \u0026#39;ip a show bond0.cmn0 | grep inet\u0026#39; Expected output will look similar to the following (exact values may differ):\ninet 10.102.11.13/24 brd 10.102.11.255 scope global bond0.cmn0 inet6 fe80::1602:ecff:fed9:7820/64 scope link Log in from another external machine to verify SSH is up and running for this session.\nexternal# ssh root@10.102.11.13 Keep this terminal active as it will enable kubectl commands during the bring-up of the new NCN. If the reboot successfully deploys the LiveCD, then this terminal can be exited.\nPOINT OF NO RETURN: The next step will wipe the underlying nodes disks clean. It will ignore USB devices. RemoteISOs are at risk here; even though a backup has been performed of the PIT node, it is not possible to boot back to the same state. This is the last step before rebooting the node.\nWipe the disks on the PIT node.\nWARNING: Risk of USER ERROR! Do not assume to wipe the first three disks (for example, sda, sdb, and sdc); they are not pinned to any physical disk layout. Choosing the wrong ones may result in wiping the USB device. USB devices can only be wiped by operators at this point in the install. USB devices are never wiped by the CSM installer.\nSelect disks to wipe (SATA/NVME/SAS).\npit# md_disks=\u0026#34;$(lsblk -l -o SIZE,NAME,TYPE,TRAN | grep -E \u0026#39;(sata|nvme|sas)\u0026#39; | sort -h | awk \u0026#39;{print \u0026#34;/dev/\u0026#34; $2}\u0026#39;)\u0026#34; Run a sanity check by printing disks into typescript or console.\npit# echo $md_disks Expected output looks similar to the following:\n/dev/sda /dev/sdb /dev/sdc Wipe. This is irreversible.\npit# wipefs --all --force $md_disks If any disks had labels present, output looks similar to the following:\n/dev/sda: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa /dev/sdb: 6 bytes were erased at offset 0x00000000 (crypto_LUKS): 4c 55 4b 53 ba be /dev/sdb: 6 bytes were erased at offset 0x00004000 (crypto_LUKS): 53 4b 55 4c ba be /dev/sdc: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sdc: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sdc: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa If there was any wiping done, output should appear similar to the output above. If this is re-run, there may be no output or an ignorable error.\nQuit the typescript session and copy the typescript file off of ncn-m001.\nStop the typescript session:\npit# exit Back up the completed typescript file by re-running the rsync commands in the Backup Bootstrap Information section.\n(Optional) Setup ConMan or serial console, if not already on, from any laptop or other system with network connectivity to the cluster.\nexternal# script -a boot.livecd.$(date +%Y-%m-%d).txt external# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; external# SYSTEM_NAME=eniac external# USERNAME=root external# export IPMI_PASSWORD=changeme external# ipmitool -I lanplus -U $USERNAME -E -H ${SYSTEM_NAME}-ncn-m001-mgmt chassis power status external# ipmitool -I lanplus -U $USERNAME -E -H ${SYSTEM_NAME}-ncn-m001-mgmt sol activate 4. Reboot Reboot the LiveCD.\npit# reboot Wait for the node to boot, acquire its hostname (ncn-m001), and run cloud-init.\nIf all of that happens successfully, then skip the rest of this step and proceed to the next step. Otherwise, use the following information to remediate the problems.\nNOTES:\nIf the node has PXE boot issues, such as getting PXE errors or not pulling the ipxe.efi binary, see PXE boot troubleshooting. If ncn-m001 did not run all the cloud-init scripts, then the following commands need to be run (but only in that circumstance). ncn-m001# cloud-init clean ; cloud-init init ; cloud-init modules -m init ; \\ cloud-init modules -m config ; cloud-init modules -m final Once cloud-init has completed successfully, log in and start a typescript (the IP address used here is the one noted for ncn-m002 in an earlier step).\nexternal# ssh root@10.102.11.13 ncn-m002# pushd /metal/bootstrap/prep/admin ncn-m002# script -af csm-verify.$(date +%Y-%m-%d).txt ncn-m002# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; ncn-m002# ssh ncn-m001 Run kubectl get nodes to see the full Kubernetes cluster.\nncn-m001# kubectl get nodes Expected output looks similar to the following:\nNAME STATUS ROLES AGE VERSION ncn-m001 Ready control-plane,master 27s v1.20.13 ncn-m002 Ready control-plane,master 4h v1.20.13 ncn-m003 Ready control-plane,master 4h v1.20.13 ncn-w001 Ready \u0026lt;none\u0026gt; 4h v1.20.13 ncn-w002 Ready \u0026lt;none\u0026gt; 4h v1.20.13 ncn-w003 Ready \u0026lt;none\u0026gt; 4h v1.20.13 Restore and verify the site link.\nRestore networking files from the manual backup taken during the Backup the bootstrap information step.\nNOTE Do NOT change any default NCN hostname; otherwise, unexpected deployment or upgrade errors may happen.\nncn-m001# SYSTEM_NAME=eniac ncn-m001# rsync ncn-m002:/metal/bootstrap/prep/${SYSTEM_NAME}/pit-files/ifcfg-lan0 /etc/sysconfig/network/ \u0026amp;\u0026amp; \\ wicked ifreload lan0 \u0026amp;\u0026amp; \\ wicked ifstatus lan0 Expected output looks similar to:\nlan0 up link: #32, state up, mtu 1500 type: bridge, hwaddr 90:e2:ba:0f:11:c2 config: compat:suse:/etc/sysconfig/network/ifcfg-lan0 leases: ipv4 static granted addr: ipv4 172.30.53.88/20 [static] Verify that the site link (lan0) and the VLANs have IP addresses.\nExamine the output to ensure that each interface has been assigned an IPv4 address.\nncn-m001# for INT in lan0 bond0.nmn0 bond0.hmn0 bond0.can0 bond0.cmn0 ; do ip a show $INT || echo \u0026#34;ERROR: Command failed: ip a show $INT\u0026#34; done Verify that the default route is via the CMN.\nncn-m001# ip r show default Verify that there is not a metal bootstrap IP address.\nncn-m001# ip a show bond0 Verify zypper repositories are empty and all remote SUSE repositories are disabled.\nIf the rm command fails because the files do not exist, this is not an error and should be ignored.\nncn-m001# rm -v /etc/zypp/repos.d/* \u0026amp;\u0026amp; zypper ms --remote --disable Download and install/upgrade the documentation RPM.\nSee Check for Latest Documentation\nExit the typescript and move the backup to ncn-m001.\nThis is required to facilitate reinstallations, because it pulls the preparation data back over to the documented area (ncn-m001).\nncn-m001# exit ncn-m002# exit # typescript exited ncn-m002# rsync -rltDv -P /metal/bootstrap ncn-m001:/metal/ \u0026amp;\u0026amp; rm -rfv /metal/bootstrap ncn-m002# exit SSH back into ncn-m001 or restart a local console.\nResume the typescript.\nncn-m001# script -af /metal/bootstrap/prep/admin/csm-verify.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Apply the boot order workaround.\nncn-m001# /usr/share/doc/csm/scripts/workarounds/boot-order/run.sh Apply the kdump workaround.\nkdump assists in taking a dump of the NCN if it encounters a kernel panic. kdump does not work properly in CSM 1.2. Until this workaround is applied, kdump may not produce a proper dump. Earlier in the install, this workaround was applied to all of the NCNs except for ncn-m001, because it was the PIT node. Running it now applies the fix to ncn-m001 as well.\nncn-m001# /usr/share/doc/csm/scripts/workarounds/kdump/run.sh Example output:\nUploading hotfix files to ncn-m001:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-m002:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-m003:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s001:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s002:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s003:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s004:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w001:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w002:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w003:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w004:/srv/cray/scripts/common/ ... Done Running updated create-kdump-artifacts.sh script on [11] NCNs ... Done The following NCNs contain the kdump patch: ncn-m001 ncn-m002 ncn-m003 ncn-s001 ncn-s002 ncn-s003 ncn-s004 ncn-w001 ncn-w002 ncn-w003 ncn-w004 This workaround has completed. 5. Enable NCN disk wiping safeguard The next steps require csi from the installation media. csi will not be provided on an NCN otherwise because it is used for Cray installation and bootstrap.\nObtain access to CSI.\nNOTE The App. Version will report incorrectly in CSM 1.2.0. Please refer to the Git Version field for this information.\nncn-m001# mkdir -pv /mnt/livecd /mnt/rootfs /mnt/sqfs \u0026amp;\u0026amp; \\ mount -v /metal/bootstrap/cray-pre-install-toolkit-*.iso /mnt/livecd/ \u0026amp;\u0026amp; \\ mount -v /mnt/livecd/LiveOS/squashfs.img /mnt/sqfs/ \u0026amp;\u0026amp; \\ mount -v /mnt/sqfs/LiveOS/rootfs.img /mnt/rootfs/ \u0026amp;\u0026amp; \\ cp -pv /mnt/rootfs/usr/bin/csi /tmp/csi \u0026amp;\u0026amp; \\ /tmp/csi version \u0026amp;\u0026amp; \\ umount -vl /mnt/sqfs /mnt/rootfs /mnt/livecd NOTE /tmp/csi will delete itself on the next reboot. The /tmp directory is tmpfs and runs in memory; it will not persist on restarts.\nAuthenticate with the cluster.\nncn-m001# export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Set the wipe safeguard to allow safe reboots on all NCNs.\nncn-m001# /tmp/csi handoff bss-update-param --set metal.no-wipe=1 6. Clean up chrony configurations This step requires the exported TOKEN variable from the previous section: Enable NCN disk wiping safeguard. If still using the same shell session, there is no need to export it again.\nncn-m001# /srv/cray/scripts/common/chrony/csm_ntp.py Successful output can appear as:\n... BSS query failed. Checking local cache... Chrony configuration created Problematic config found: /etc/chrony.d/cray.conf.dist Problematic config found: /etc/chrony.d/pool.conf Restarted chronyd ... or\n... Chrony configuration created Restarted chronyd ... 7. Configure DNS and NTP on each BMC NOTE: Only follow this section if the NCNs are HPE hardware. If the system uses Gigabyte or Intel hardware, skip this section.\nConfigure DNS and NTP on the BMC for each management node except ncn-m001. However, the commands in this section are all run on ncn-m001.\nValidate that the system is HPE hardware.\nncn-m001# ipmitool mc info | grep \u0026#34;Hewlett Packard Enterprise\u0026#34; || echo \u0026#34;Not HPE hardware -- SKIP these steps\u0026#34; Set environment variables.\nSet the IPMI_PASSWORD and USERNAME variables to the BMC credentials for the NCNs.\nUsing read -s for this prevents the credentials from being echoed to the screen or saved in the shell history.\nncn-m001# read -s IPMI_PASSWORD ncn-m001# read -s USERNAME ncn-m001# export IPMI_PASSWORD USERNAME Set BMCS variable to list of the BMCs for all master, worker, and storage nodes, except ncn-m001-mgmt:\nncn-m001# BMCS=$(grep -Eo \u0026#34;[[:space:]]ncn-[msw][0-9][0-9][0-9]-mgmt([.]|[[:space:]]|$)\u0026#34; /etc/hosts | sed \u0026#39;s/^.*\\(ncn-[msw][0-9][0-9][0-9]-mgmt\\).*$/\\1/\u0026#39; | sort -u | grep -v \u0026#34;^ncn-m001-mgmt$\u0026#34;) ; echo $BMCS Expected output looks similar to the following:\nncn-m002-mgmt ncn-m003-mgmt ncn-s001-mgmt ncn-s002-mgmt ncn-s003-mgmt ncn-w001-mgmt ncn-w002-mgmt ncn-w003-mgmt Get the DNS server IP address for the HMN.\nncn-m001# HMN_DNS=$(kubectl get services -n services -o wide | awk /cray-dns-unbound-udp-hmn/\u0026#39;{printf \u0026#34;%s%s\u0026#34;, sep, $4; sep=\u0026#34;,\u0026#34;} END{print \u0026#34;\u0026#34;}\u0026#39;); echo ${HMN_DNS} Example output for a single DNS server:\n10.94.100.225 Example output for multiple DNS servers:\n10.94.100.225,10.94.100.224,10.94.100.223 Run the following to loop through all of the BMCs (except ncn-m001-mgmt) and apply the desired settings.\nncn-m001# for BMC in $BMCS ; do echo \u0026#34;$BMC: Disabling DHCP and configure NTP on the BMC using data from unbound service\u0026#34; /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh ilo -H $BMC -S -n echo echo \u0026#34;$BMC: Configuring DNS on the BMC using data from unbound\u0026#34; /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh ilo -H $BMC -D $HMN_DNS -d echo echo \u0026#34;$BMC: Showing settings\u0026#34; /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh ilo -H $BMC -s echo done ; echo \u0026#34;Configuration completed on all NCN BMCs\u0026#34; 8. Next topic After completing this procedure, proceed to Configure Administrative Access.\n"
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/wait_for_unbound_hang/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "#Wait_for_unbound or cray-dns-unbound-manager hangs\nRun the following command:\nkubectl get jobs -n services | grep cray-dns-unbound-manager services cray-dns-unbound-manager-1635352560 0/1 26h 26h services cray-dns-unbound-manager-1635448680 1/1 35s 8m37s services cray-dns-unbound-manager-1635448860 1/1 51s 5m36s services cray-dns-unbound-manager-1635449040 1/1 61s 2m35s If you see one of the jobs show 0/1 for more than 10 minutes and there are other runs with 1/1. That means that job is hung. You can delete the job with:\nkubectl delete jobs -n services $job_with_0/1 Alternative is copy and paste following code block:\nunbound_manager_jobs=$(kubectl get jobs -n services |awk \u0026#39;{ print $1 }\u0026#39;|grep unbound-manager) for job in $unbound_manager_jobs; do job_entry=$(kubectl get jobs -n services $job|sed 1d) echo $job_entry job_id=$(echo $job_entry| awk \u0026#39;{ print $1 }\u0026#39;) echo $job_id job_status=$(echo $job_entry| awk \u0026#39;{ print $2 }\u0026#39;) echo $job_status if [[ \u0026#34;$job_status\u0026#34; -eq \u0026#34;0/1\u0026#34; ]];then echo \u0026#34;deleting stale job\u0026#34; kubectl delete jobs -n services $job_id echo \u0026#34;kubectl delete jobs -n services $job_id\u0026#34; fi done "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/troubleshoot_ceph_fs_client_connectivity_issues/",
	"title": "Troubleshoot Ceph MDS Client Connectivity Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Ceph MDS Client Connectivity Issues Use this procedure to diagnose and fix clients not logging into Ceph FS.\nNOTE: This section does not diagnose nor fix network issues. Please ensure that all networking is functional before proceeding.\nIMPORTANT: The following commands can be run from ncn-m001/2/3 or ncn-s001/2/3.\nProcedure Identify if clients are not logged into Ceph FS.\nceph fs status Example output:\ncephfs - 0 clients \u0026lt;---- This indicates we have no clients connected ====== RANK STATE MDS ACTIVITY DNS INOS 0 active cephfs.ncn-s001.abiiiw Reqs: 0 /s 0 0 0-s standby-replay cephfs.ncn-s002.kyayma Evts: 38 /s 35.5k 3220 POOL TYPE USED AVAIL cephfs_metadata metadata 2403M 11.1T cephfs_data data 2641G 11.1T STANDBY MDS cephfs.ncn-s003.sjatdm Fail over the MDS to trigger clients logins.\nceph mds fail 0 NOTE: \u0026ldquo;0\u0026rdquo; refers to the active rank in our above output.\nVerify clients have reconnected.\nceph fs status Example output:\ncephfs - 24 clients \u0026lt;---- Shows our clients have reconnected ====== RANK STATE MDS ACTIVITY DNS INOS 0 active cephfs.ncn-s002.kyayma Reqs: 1 /s 52.8k 20.3k 0-s standby-replay cephfs.ncn-s003.sjatdm Evts: 0 /s 0 0 POOL TYPE USED AVAIL cephfs_metadata metadata 2404M 11.1T cephfs_data data 2641G 11.1T STANDBY MDS cephfs.ncn-s001.abiiiw MDS version: ceph version 15.2.8 (bdf3eebcd22d7d0b3dd4d5501bee5bac354d5b55) octopus (stable) "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/configure_keycloak_for_ldapad_authentication/",
	"title": "Configure Keycloak for LDAP/AD authentication",
	"tags": [],
	"description": "",
	"content": "Configure Keycloak for LDAP/AD authentication Keycloak enables users to be in an LDAP or Active Directory (AD) server. This allows users to get their tokens using their regular username and password, and use those tokens to perform operations on the system\u0026rsquo;s REST API.\nConfiguring Keycloak can be done using the admin GUI or through Keycloak\u0026rsquo;s web API.\nFor more information on setting up LDAP federation, see the Keycloak administrative documentation in a section titled https://www.keycloak.org/docs/latest/server_admin/index.html#_ldap.\nUsers are expected to have the following attributes:\nuidNumber gidNumber homeDirectory loginShell These attributes are added to the users by adding a \u0026ldquo;User Attribute Mapper\u0026rdquo; to the LDAP User Federation object. For each of these, there should be a User Attribute Mapper that maps the \u0026ldquo;LDAP Attribute\u0026rdquo; in the LDAP Directory to the \u0026ldquo;User Model Attribute,\u0026rdquo; which will be uidNumber, gidNumber, and more.\nThe shasta client that is created during the install maps these specific user model attributes into the JWT token so that it is available to the REST APIs.\n"
},
{
	"uri": "/docs-csm/en-12/operations/power_management/worker_node_cos_power_up_configuration/",
	"title": "Worker Node COS Power Up Configuration",
	"tags": [],
	"description": "",
	"content": "Worker Node COS Power Up Configuration This procedure is only to be used as part of a full system power up procedure when there are no DVS clients using CPS/DVS from the worker nodes.\nImportant: Some systems may have a failure to mount Lustre on the worker nodes during the COS 2.3 layer of configuration if their connection to the ClusterStor is via cables to Slingshot switches in the liquid-cooled cabinets which have not been powered up at this point in the power on procedure. This affects worker nodes which have Mellanox HSN NICs. Worker nodes with Cassini HSN NICs are unaffected. This may include systems which have Arista switches.\nNormally, CFS could be restarted for these worker nodes after the Slingshot switches in the liquid-cooled cabinets have been powered up. however there is a known problem with Slingshot 1.7.3a and earlier versions of the Slingshot Host Software (SHS) which require a special procedure in the COS 2.3 layer to address this problem.\nPrerequisites All compute nodes and application nodes are powered off. All Slingshot switches in liquid-cooled cabinets are powered on No worker node has any DVS mounts to CPS or other dedicated DVS server. No worker node has any Lustre filesystems mounted. CFS has failed post-boot configuration, NCN personalization, on the worker nodes. Procedure Check whether CFS has failed NCN personalization on the worker nodes.\nIf a node has its Configuration Status set to configured, then that node has completed all configuration layers for post-boot CFS.\nIf any nodes have Configuration Status set to pending, then there should be a CFS session in progress which includes that node.\nIf any nodes have Configuration Status set to failed with Error Count set to 3, then the node was unable complete a layer of configuration.\nncn-m# sat status --filter role=management --filter enabled=true --fields \\ xname,aliases,role,subrole,\u0026#34;desired config\u0026#34;,\u0026#34;configuration status\u0026#34;,\u0026#34;error count\u0026#34; Example output:\n+----------------+----------+------------+---------+---------------------+----------------------+-------------+ | xname | Aliases | Role | SubRole | Desired Config | Configuration Status | Error Count | +----------------+----------+------------+---------+---------------------+----------------------+-------------+ | x3000c0s7b0n0 | ncn-w001 | Management | Worker | ncn-personalization | failed | 3 | | x3000c0s9b0n0 | ncn-w002 | Management | Worker | ncn-personalization | failed | 3 | | x3000c0s11b0n0 | ncn-w003 | Management | Worker | ncn-personalization | failed | 3 | | x3000c0s13b0n0 | ncn-w004 | Management | Worker | ncn-personalization | pending | 2 | | x3000c0s25b0n0 | ncn-w005 | Management | Worker | ncn-personalization | pending | 2 | +----------------+----------+------------+---------+---------------------+----------------------+-------------+ This example shows three worker nodes which have failed with an error count of 3 and two which are pending, but have already failed twice.\nIf some nodes are not fully configured, then find any CFS sessions in progress.\nncn-mw# kubectl -n services --sort-by=.metadata.creationTimestamp get pods | grep cfs Example output:\ncfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk 7/9 Error 0 21m cfs-157af6d5-b63d-48ba-9eb9-b33af9a8325d-tfj8x 3/9 Not Ready 0 11m CFS sessions which are in Not Ready status are still in progress. CFS sessions with status Error had a failure in one of the layers.\nInspect all layers of Ansible configuration to find a failed layer.\nncn-mw# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-0 ncn-mw# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-1 ncn-mw# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-2 ncn-mw# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-3 ncn-mw# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-4 ncn-mw# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-5 ncn-mw# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-6 ncn-mw# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-7 ncn-mw# kubectl logs -f -n services cfs-51a7665d-l63d-41ab-e93e-796d5cb7b823-czkhk ansible-8 If the slingshot-host-software has completed and the COS layer has run, but fails to mount Lustre filesystems, then several roles have already been run to load DVS, Lnet, and Lustre kernel modules. These need to be unloaded before NCN personalization can be run again. This is due to a flaw in the slingshot-host-software which restarts the openibd service for worker nodes which have Mellanox NICs.\nOtherwise, do not continue in this procedure.\nCheck out cos-config-management from VCS.\nImportant: The rest of this procedure is only needed when the Lustre filesystems failed to mount as checked in the previous step.\nCreate a branch using the imported branch from the installation to customize COS.\nThe imported branch will be reported in the cray-product-catalog and can be used as a base branch. The imported branch from the installation should not be modified. It is recommended that a branch is created from the imported branch to customize COS configuration content as necessary. The following steps create an integration branch to accomplish this.\nObtain the import_branch from the cray-product-catalog.\nSet the COS_RELEASE to the version of COS 2.3 which has been installed.\nncn-mw# COS_RELEASE=2.3.101 ncn-mw# kubectl get cm cray-product-catalog -n services -o yaml \\ | yq r - \u0026#39;data.cos\u0026#39; | yq r - \\\u0026#34;${COS_RELEASE}\\\u0026#34; Example output:\n2.3.XX: configuration: clone_url: https://vcs.DOMAIN_NAME.dev.cray.com/vcs/cray/cos-config-management.git commit: 215eab2c316fb75662ace6aaade8b8c2ab2d08ee import_branch: cray/cos/2.3.XX import_date: 2021-02-21 23:01:16.100251 ssh_url: git@vcs.DOMAIN_NAME.dev.cray.com:cray/cos-config-management.git images: {} recipes: cray-shasta-compute-sles15sp3.x86_64-1.4.64: id: 5149788d-4e5d-493d-b259-f56156a58b0d Store the import branch for later use.\nncn-mw# IMPORT_BRANCH=cray/cos/${COS_RELEASE} Obtain VCS credentials from a Kubernetes secret.\nThe credentials are required for cloning a branch from VCS.\nObtain the username.\nThe output of the following command is the username:\nncn-mw# kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_username}} | base64 --decode \u0026amp;\u0026amp; echo Obtain the password.\nThe output of the following command is the password:\nncn-mw# kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode \u0026amp;\u0026amp; echo Clone the COS configuration repository.\nWhen prompted for the username and password, enter the values obtained in the previous steps.\nncn-mw# git clone https://api-gw-service-nmn.local/vcs/cray/cos-config-management.git Change directory and create a new branch.\nncn-mw# cd cos-config-management/ ncn-mw# git checkout -b $IMPORT_BRANCH origin/$IMPORT_BRANCH Example output:\nBranch \u0026#39;cray/cos/\u0026lt;X\u0026gt;.\u0026lt;Y\u0026gt;.\u0026lt;Z\u0026gt;\u0026#39; set up to track remote branch \u0026#39;cray/cos/\u0026lt;X\u0026gt;.\u0026lt;Y\u0026gt;.\u0026lt;Z\u0026gt;\u0026#39; from \u0026#39;origin\u0026#39;. Switched to a new branch \u0026#39;cray/cos/\u0026lt;X\u0026gt;.\u0026lt;Y\u0026gt;.\u0026lt;Z\u0026gt;\u0026#39; If the integration branch exists, then run the following command:\nncn-mw# git checkout -b integration origin/integration Example output:\nBranch \u0026#39;integration\u0026#39; set up to track remote branch \u0026#39;integration\u0026#39; from \u0026#39;origin\u0026#39;. Switched to a new branch \u0026#39;integration\u0026#39; Merge the import branch into the integration branch.\nncn-mw# git merge $IMPORT_BRANCH Create a new ncn-powerup.yml playbook.\nCopy the ncn-upgrade.yml playbook to ncn-powerup.yml. Edit the file with two changes.\nChange serial parameter from 1 node to 100% Comment all roles after the ones with names ending in uninstall, unmount, and unload. See the example below. ncn-mw# cp -pv ncn-upgrade.yml ncn-powerup.yml ncn-mw# vi ncn-powerup.yml ncn-mw# cat ncn-powerup.yml Example output\n#!/usr/bin/env ansible-playbook # Copyright 2021-2022 Hewlett Packard Enterprise Development LP --- - hosts: Management_Worker serial: 100% any_errors_fatal: true remote_user: root roles: - configure_fs_unload - cray_dvs_unmount - cray_dvs_unload - cray_lnet_unload - cray_dvs_uninstall - cray_lnet_uninstall # - cos-services-install # - cos-services-restart # - cray_lnet_install # - cray_dvs_install # - cray_lnet_load # - cray_dvs_load # - lustre_config # - configure_fs Commit the new ncn-powerup.yml to the cos-config-management VCS repository.\nncn-mw# git add ncn-powerup.yml ncn-mw# git commit -m \u0026#34;Patched with ncn-powerup.yml playbook\u0026#34; ncn-mw# git push origin integration Identify the commit hash for this branch.\nThis will be used later when creating the CFS configuration layer. The following command will display the commit hash.\nncn-mw# git rev-parse --verify HEAD Store the commit hash for later use.\nncn-mw# COS_CONFIG_COMMIT_HASH=\u0026lt;commit hash output\u0026gt; Create and run a CFS configuration which has only a COS layer with this ncn-powerup.yml playbook in it.\nCreate a JSON file with the configuration contents.\nncn-mw# vi ncn-powerup.json ncn-mw# cat ncn-powerup.json Example output:\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;:\u0026#34;https://api-gw-service-nmn.local/vcs/cray/cos-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;:\u0026#34;\u0026lt;COS_CONFIG_COMMIT_HASH\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cos-integration-2.3.101\u0026#34;, \u0026#34;playbook\u0026#34;:\u0026#34;ncn-powerup.yml\u0026#34; } ] } Create a CFS configuration from this file.\nncn-mw# cray cfs configurations update ncn-powerup --file ncn-powerup.json --format json Run a CFS session with the new configuration.\nncn-mw# cray cfs sessions create --name ncn-powerup --configuration-name ncn-powerup Watch the CFS session run on the worker nodes.\nncn-mw# kubectl -n services --sort-by=.metadata.creationTimestamp get pods | grep cfs ncn-mw# kubectl logs -f -n services POD ansible-0 Continue only when there are no errors in the Ansible log.\nClear the error counts on all nodes so that CFS batcher can run NCN personalization on all worker nodes.\nThis will have the SHS openibd restart, see that all of the COS steps have never been done, and then load Lnet, DVS, and Lustre.\nncn-mw# cray cfs components update --enabled true --state \u0026#39;[]\u0026#39; --error-count 0 --format json $XNAME Watch the CFS NCN personalization run on the worker nodes to ensure that the configuration completes with no further errors.\nncn-mw# kubectl -n services --sort-by=.metadata.creationTimestamp get pods | grep cfs ncn-mw# kubectl logs -f -n services POD ansible-0 "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/update_the_gigabyte_node_bios_time/",
	"title": "Update the Gigabyte Node BIOS Time",
	"tags": [],
	"description": "",
	"content": "Update the Gigabyte Node BIOS Time Check and set the time for Gigabyte compute nodes.\nIf the console log indicates the time between the rest of the system and the compute nodes is off by several hours, then it prevents the spire-agent from getting a valid certificate, which causes the node boot to drop into the dracut emergency shell.\nProcedure Retrieve the cray-console-operator pod ID.\nncn-mw# CONPOD=$(kubectl get pods -n services -o wide|grep cray-console-operator|awk \u0026#39;{print $1}\u0026#39;); echo ${CONPOD} Example output:\ncray-console-operator-79bf95964-qpcpp The following steps should be repeated for each Gigabyte node which needs to have its BIOS time reset.\nSet the XNAME variable to the component name (xname) of the node whose console you wish to open.\nncn-mw# XNAME=x1001c0s24b1n0 Find the cray-console-node pod that is connected to that node.\nncn-mw# NODEPOD=$(kubectl -n services exec \u0026#34;${CONPOD}\u0026#34; -c cray-console-operator -- \\ sh -c \u0026#34;/app/get-node ${XNAME}\u0026#34; | jq .podname | sed \u0026#39;s/\u0026#34;//g\u0026#39;) ; echo ${NODEPOD} Example output:\ncray-console-node-1 Connect to the node\u0026rsquo;s console using ConMan on the identified cray-console-node pod.\nncn-mw# kubectl exec -it -n services \u0026#34;${NODEPOD}\u0026#34; -- conman -j \u0026#34;${XNAME}\u0026#34; Example output:\n\u0026lt;ConMan\u0026gt; Connection to console [x1001c0s24b1] opened. In another terminal, boot the node to BIOS.\nSet the BMC variable to the component name (xname) of the BMC for the node.\nThis value will be different for each node.\nncn# BMC=x1001c0s24b1 Boot the node to BIOS.\nread -s is used to prevent the password from being written to the screen or the shell history.\nncn# USERNAME=root ncn# read -r -s -p \u0026#34;$BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026#34;${BMC}\u0026#34; chassis bootdev bios ncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026#34;${BMC}\u0026#34; chassis power off ncn# sleep 10 ncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026#34;${BMC}\u0026#34; chassis power on Update the System Date field to match the time on the system.\nUse the terminal which is watching the console for this step. As the node powers on, it will complete POST (Power On Self Test) and then display the BIOS menu.\nThe System Date field is located under the Main tab in the navigation bar.\nEnter the F10 key followed by the Enter key to save the BIOS time.\nExit the connection to the console by entering \u0026amp;..\nRepeat the above steps for other nodes which need their BIOS time reset.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/change_settings_for_hms_collector_polling_of_air_cooled_nodes/",
	"title": "Change Settings for HMS Collector Polling of Air-Cooled Nodes",
	"tags": [],
	"description": "",
	"content": "Change Settings for HMS Collector Polling of Air-Cooled Nodes The cray-hms-hmcollector service polls all air-cooled hardware to gather the necessary telemetry information for use by other services, such as the Cray Advanced Platform Monitoring and Control (CAPMC) service. This polling occurs every 10 seconds on a continual basis. Instabilities with the AMI Redfish implementation in the Gigabyte BMCs require a less significant approach when gathering power and temperature telemetry data. If the BMCs are overloaded, they can become unresponsive, return incorrect data, or encounter other errors.\nAll of these issues prevent other services, such as CAPMC and the Firmware Action Service (FAS), from successfully acting on the BMCs. Recovery from this state requires a BMC reset and sometimes a hard power cycle by unplugging the server and plugging it back in.\nCollecting telemetry data while trying to boot air-cooled compute nodes increases the burden on the BMCs and increases the likelihood of BMC issues. The most likely time to encounter BMCs in a bad state is when trying to boot air-cooled compute nodes and User Access Nodes (UANs) using the Boot Orchestration Service, or when trying to do a firmware or BIOS update on the nodes. Check the service logs of CAPMC and FAS for error information returned from the BMCs.\nRecommendations for polling The following are the best practices for using the HMS Collector polling:\nDo not query the power state of air-cooled nodes using CAPMC more than two or three times a minute.\nThis is done via the CAPMC get_xname_status command.\nThe Cray CLI must be configured on the node where this command is run. See Configure the Cray CLI.\nncn# cray capmc get_xname_status create --xnames LIST_OF_NODES Polling of air-cooled nodes should be disabled by default. Before nodes are booted, verify that cray-hms-hmcollector polling is disabled.\nTo check if polling is disabled:\nncn-mw# kubectl get deployments.apps -n services cray-hms-hmcollector -o json | \\ jq \u0026#39;.spec.template.spec.containers[].env[]|select(.name==\u0026#34;POLLING_ENABLED\u0026#34;)\u0026#39; To disable polling, if it is not already disabled:\nncn-mw# kubectl edit deployment -n services cray-hms-hmcollector Change the value for the POLLING_ENABLED environment variable to false in the spec: section. Save and quit the editor for the changes to take effect. The cray-hms-hmcollector pod will automatically restart.\nOnly enable telemetry polling when needed, such as when running jobs.\nncn-mw# kubectl edit deployment -n services cray-hms-hmcollector Change the value for the POLLING_ENABLED environment variable to true in the spec: section. Save and quit the editor for the changes to take effect. The cray-hms-hmcollector pod will automatically restart.\nIf BMCs are encountering issues at a high rate, then increase the polling interval. Do not set the polling interval to less than the default of 10 seconds.\nncn-mw# kubectl edit deployment -n services cray-hms-hmcollector Change the value for the POLLING_INTERVAL environment variable to the selected rate in seconds. This value is located in the spec: section. Save and quit the editor for the changes to take effect. The cray-hms-hmcollector pod will automatically restart.\nReset BMCs in a bad state Even with the polling recommendations above, it is still possible for the BMCs to end up in a bad state, necessitating a reset.\nTo restart the BMCs:\nread -s is used to prevent the password from being written to the screen or the shell history.\nncn# USERNAME=root ncn# read -r -s -p \u0026#34;BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -H BMC_HOSTNAME -U \u0026#34;${USERNAME}\u0026#34; -E -I lanplus mc reset cold If the reset does not recover the BMCs, then use the following steps to shut down the nodes, unplug the servers, and plug them back in:\nShut down the nodes.\nFor each server with a BMC in a bad state:\nncn# ipmitool -H BMC_HOSTNAME -U \u0026#34;${USERNAME}\u0026#34; -E -I lanplus chassis power soft Wait 30 seconds after shutting down the nodes before proceeding.\nUnplug the server Power Supply Units (PSUs) and wait 30 seconds.\nPlug both server PSUs back in.\nWait at least two minutes before proceeding.\nVerify that the BMCs are available again.\nncn# ping -c 1 BMC_HOSTNAME Check the power of the nodes.\nncn# cray capmc get_xname_status create --xnames LIST_OF_NODES After these steps, the nodes should be ready to be booted again with the Boot Orchestration Service (BOS).\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/ip_filter/",
	"title": "Ip filter",
	"tags": [],
	"description": "",
	"content": "Ip filter There are two types of malicious traffic that can be received from external sources to the data center:\nTraffic that target the switch\u0026rsquo;s CPU, either inband or out of band (e.g. via mgmt0) targeted one of the IP interfaces of the switch (loopback, router IP). To protect or filter those traffic threats use the ip filter set of commands. Traffic that target the data center servers transferred via the switch. To protect or filter this traffic use the switch\u0026rsquo;s ACL set of commands. Relevant Configuration\nEnable IP filter globally.\nswitch (config) # ip filter enable Set the default input or output policy rule. The default is to accept all. The default rule will be applied if no other rule will match.\nFor example, drop all traffic other than a specific set of flows, or accept all traffic except a specific set of flows.\nswitch (config) # ip filter chain input policy drop switch (config) # ip filter chain output policy accept Set IP filtering rules for input or output traffic. For example, block (drop) UDP source port 100.\nswitch (config) # ip filter chain input rule set 2 target drop protocol udp source-port 100 Show Commands to Validate Functionality\nswitch (config) # show ip filter Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/snmp-community/",
	"title": "Configure SNMPv2c Community",
	"tags": [],
	"description": "",
	"content": "Configure SNMPv2c Community The switch supports SNMPv2c community-based security for read-only access.\nConfiguration Commands Configure an SNMPv2c community name:\nswitch(config)# snmp-server community community-name Show commands to validate functionality:\nswitch# show snmp community Expected Results Administrators can configure the community name Administrators can bind the SNMP server to the default VRF Administrators can connect from the workstation using the community name Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/hostname/",
	"title": "Configure Hostnames",
	"tags": [],
	"description": "",
	"content": "Configure Hostnames A hostname is a human-friendly name used to identify a device. An example of a hostname could be the name \u0026ldquo;Test.\u0026rdquo;\nConfiguration Commands Create a hostname:\nswitch(config)# hostname \u0026lt;NAME\u0026gt; Show commands to validate functionality:\nswitch# show hostname Example Output switch(config)# hostname switch-test switch-test# show hostname switch-test Expected Results Administrators can configure the hostname The output of all show commands is correct Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/limit_kubernetes_api_audit_log_maxbackups/",
	"title": "Configure Kubernetes API Audit Log Maximum Backups",
	"tags": [],
	"description": "",
	"content": "Configure Kubernetes API Audit Log Maximum Backups If Kubernetes API Auditing was enabled at install or upgrade, via the CSI option --k8s-api-auditing-enabled true or the system_config.yaml option k8s-api-auditing-enabled: true, apply this procedure to running Kubernetes Master Nodes.\nPrerequisites This procedure requires administrative privileges and assumes that the device being used has:\nkubectl is installed Access to the site admin network Procedure SSH as root to the first Kubernetes Master Node, canonically ncn-m001.\nVerify Kubernetes API Auditing is enabled.\nYou should see both of the following settings in kube-apiserver.yaml.\nncn-m# egrep \u0026#39;audit-log-path|audit-policy-file\u0026#39; /etc/kubernetes/manifests/kube-apiserver.yaml - --audit-log-path=/var/log/audit/kl8s/apiserver/audit.log - --audit-policy-file=/etc/kubernetes/audit/audit-policy.yaml Verify all Kubernetes API Server Pods are Running. You should have one for each master node.\nncn-m# kubectl get pod -n kube-system -l component=kube-apiserver -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-apiserver-ncn-m001 1/1 Running 0 44m 10.252.1.4 ncn-m001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-apiserver-ncn-m002 1/1 Running 0 2m1s 10.252.1.5 ncn-m002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-apiserver-ncn-m003 1/1 Running 0 3d20h 10.252.1.6 ncn-m003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; If Kubernetes API Auditing is enabled, add --audit-log-maxbackup=100 command line option to the Kubernetes API Server.\nMake a backup of the /etc/kubernetes/manifests/kube-apiserver.yaml. Ensure the backup is to a directory other than /etc/kubernetes/manifests/.\nncn-m# cp -a /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/ Edit the /etc/kubernetes/manifests/kube-apiserver.yaml file, adding --audit-log-maxbackup=100 as an option after --audit-policy-file.\nncn-m# grep -n \u0026#34;\\-\\-audit\u0026#34; /etc/kubernetes/manifests/kube-apiserver.yaml 46: - --audit-log-path=/var/log/audit/kl8s/apiserver/audit.log 47: - --audit-policy-file=/etc/kubernetes/audit/audit-policy.yaml 48: - --audit-log-maxbackup=100 Wait for the Kubernetes API Server Pod on the node to restart. Do not proceed until the pod is in a running state and is ready.\nMonitor the node and pod age using:\nncn-m# kubectl get pod -n kube-system -l component=kube-apiserver -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-apiserver-ncn-m001 1/1 Running 0 44m 10.252.1.4 ncn-m001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-apiserver-ncn-m002 1/1 Running 0 2m1s 10.252.1.5 ncn-m002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-apiserver-ncn-m003 1/1 Running 0 3d20h 10.252.1.6 ncn-m003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Repeat steps 2-5 for all other master nodes.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/git_operations/",
	"title": "Git Operations",
	"tags": [],
	"description": "",
	"content": "Git Operations Use the git command to manage repository content in the Version Control Service (VCS).\nOnce a repository is cloned, the git command line tool is available to interact with a repository from VCS. The git command is used for making commits, creating new branches, and pushing new branches, tags, and commits to the remote repository stored in VCS.\nWhen pushing changes to the VCS server using the crayvcs user, input the password retrieved from the Kubernetes secret as the credentials. See the \u0026ldquo;VCS Administrative User\u0026rdquo; heading in Version Control Service (VCS) for more information.\nncn# git push Enter the appropriate credentials when prompted:\nUsername for \u0026#39;https://api-gw-service-nmn.local\u0026#39;: crayvcs Password for \u0026#39;https://crayvcs@api-gw-service-nmn.local\u0026#39;: \u0026lt;input password here\u0026gt; For more information on how to use the Git command line tools, refer to the external Git User Manual.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/log_file_locations_and_ports_used_in_compute_node_boot_troubleshooting/",
	"title": "Log File Locations and Ports Used in Compute Node Boot Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Log File Locations and Ports Used in Compute Node Boot Troubleshooting This section includes the port IDs and log file locations of components associated with the node boot process.\nLog File Locations The log file locations for ConMan, DHCP, and TFTP.\nConMan logs are located within the conman pod at /var/log/conman.log.\nDHCP:\nncn-m001# kubectl logs DHCP_POD_ID TFTP:\nncn-m001# kubectl logs -n services TFTP_POD_ID Port IDs The following table includes the port IDs for DHCP and TFTP.\nComponent Port DHCP server 67 DHCP client 68 TFTP server 69 "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_uai_resource_specification/",
	"title": "Delete a UAI Resource Specification",
	"tags": [],
	"description": "",
	"content": "Delete a UAI Resource Specification Delete a specific UAI resource specification using the resource_id of that specification. Once deleted, UAIs will no longer be able to use that specification for creation. Existing UAIs are not affected by the change.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Resource ID of the resource specification to be deleted: List Resource Specifications Procedure To delete a particular resource specification, use a command of the following form:\nncn-m001-pit# cray uas admin config resources delete RESOURCE_ID Remove a UAI resource specification from UAS.\nncn-m001-pit# cray uas admin config resources delete 7c78f5cf-ccf3-4d69-ae0b-a75648e5cddb Top: User Access Service (UAS)\nNext Topic: UAI Classes\n"
},
{
	"uri": "/docs-csm/en-12/install/deploy_management_nodes/",
	"title": "Deploy Management Nodes",
	"tags": [],
	"description": "",
	"content": "Deploy Management Nodes The following procedure deploys Linux and Kubernetes software to the management NCNs. Deployment of the nodes starts with booting the storage nodes followed by the master nodes and worker nodes together.\nAfter the operating system boots on each node, there are some configuration actions which take place. Watching the console or the console log for certain nodes can help to understand what happens and when. When the process completes for all nodes, the Ceph storage is initialized and the Kubernetes cluster is created and ready for a workload. The PIT node will join Kubernetes after it is rebooted later in Deploy Final NCN.\nTiming of deployments The timing of each set of boots varies based on hardware. Nodes from some manufacturers will POST faster than others or vary based on BIOS setting. After powering on a set of nodes, an administrator can expect a healthy boot session to take about 60 minutes depending on the number of storage and worker nodes.\nTopics Prepare for management node deployment Tokens and IPMI password Ensure time is accurate before Deploying NCNs Update management node firmware Deploy management nodes Deploy workflow Deploy Check for unused drives on utility storage nodes Configure after management node deployment LiveCD cluster authentication Install tests and test server on NCNs Clean up chrony configurations Validate management node deployment Important checkpoint Next topic LVM check troubleshooting Manual LVM check procedure LVM check failure recovery 1. Prepare for management node deployment Preparation of the environment must be done before attempting to deploy the management nodes.\n1.1 Tokens and IPMI password Define shell environment variables that will simplify later commands to deploy management nodes.\nSet IPMI_PASSWORD to the root password for the NCN BMCs.\nread -s is used to prevent the password from being written to the screen or the shell history.\npit# read -s IPMI_PASSWORD pit# export IPMI_PASSWORD Set the remaining helper variables.\nThese values do not need to be altered from what is shown.\npit# mtoken=\u0026#39;ncn-m(?!001)\\w+-mgmt\u0026#39; ; stoken=\u0026#39;ncn-s\\w+-mgmt\u0026#39; ; wtoken=\u0026#39;ncn-w\\w+-mgmt\u0026#39; ; export USERNAME=root Throughout the guide, simple one-liners can be used to query status of expected nodes. If the shell or environment is terminated, these environment variables should be re-exported.\nExamples:\nCheck power status of all NCNs.\npit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power status Power off all NCNs.\npit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power off 1.2 Ensure time is accurate before deploying NCNs NOTE: Optionally, in order to use a timezone other than UTC, instead of step 1 below, follow this procedure for setting a local timezone. Then proceed to step 2.\nEnsure that the PIT node has the correct current time.\nThe time can be inaccurate if the system has been powered off for a long time, or, for example, the CMOS was cleared on a Gigabyte node. See Clear Gigabyte CMOS.\nThis step should not be skipped.\nCheck the time on the PIT node to see whether it matches the current time:\npit# date \u0026#34;+%Y-%m-%d %H:%M:%S.%6N%z\u0026#34; If the time is inaccurate, set the time manually.\npit# timedatectl set-time \u0026#34;2019-11-15 00:00:00\u0026#34; Run the NTP script:\npit# /root/bin/configure-ntp.sh This ensures that the PIT is configured with an accurate date/time, which will be propagated to the NCNs during boot.\nIf the error Failed to set time: NTP unit is active is observed, then stop chrony first.\npit# systemctl stop chronyd Then run the commands above to complete the process.\nEnsure that the current time is set in BIOS for all management NCNs.\nEach NCN is booted to the BIOS menu, the date and time are checked, and set to the current UTC time if needed.\nNOTE: Some steps in this procedure depend on USERNAME and IPMI_PASSWORD being set. This is done in Tokens and IPMI Password.\nRepeat the following process for each NCN.\nSet the bmc variable to the name of the BMC of the NCN being checked.\nImportant: Be sure to change the below example to the appropriate NCN.\npit# bmc=ncn-w001-mgmt Start an IPMI console session to the NCN.\npit# conman -j $bmc Using another terminal to watch the console, boot the node to BIOS.\npit# ipmitool -I lanplus -U $USERNAME -E -H $bmc chassis bootdev bios \u0026amp;\u0026amp; ipmitool -I lanplus -U $USERNAME -E -H $bmc chassis power off \u0026amp;\u0026amp; sleep 10 \u0026amp;\u0026amp; ipmitool -I lanplus -U $USERNAME -E -H $bmc chassis power on For HPE NCNs, the above process will boot the nodes to their BIOS; however, the BIOS menu is unavailable through conman because the node is booted into a graphical BIOS menu.\nIn order to access the serial version of the BIOS menu, perform the ipmitool steps above to boot the node. Then, in conman, press ESC+9 key combination when the following messages are shown on the console. That key combination will open a menu that can be used to enter the BIOS using conman.\nFor access via BIOS Serial Console: Press \u0026#39;ESC+9\u0026#39; for System Utilities Press \u0026#39;ESC+0\u0026#39; for Intelligent Provisioning Press \u0026#39;ESC+!\u0026#39; for One-Time Boot Menu Press \u0026#39;ESC+@\u0026#39; for Network Boot For HPE NCNs, the date configuration menu is at the following path: System Configuration -\u0026gt; BIOS/Platform Configuration (RBSU) -\u0026gt; Date and Time.\nAlternatively, for HPE NCNs, log in to the BMC\u0026rsquo;s web interface and access the HTML5 console for the node, in order to interact with the graphical BIOS. From the administrator\u0026rsquo;s own machine, create an SSH tunnel (-L creates the tunnel; -N prevents a shell and stubs the connection):\nlinux# bmc=ncn-w001-mgmt # Change this to be the appropriate node linux# ssh -L 9443:$bmc:443 -N root@eniac-ncn-m001 Opening a web browser to https://localhost:9443 will give access to the BMC\u0026rsquo;s web interface.\nWhen the node boots, the conman session can be used to see the BIOS menu, in order to check and set the time to current UTC time. The process varies depending on the vendor of the NCN.\nAfter the correct time has been verified, power off the NCN.\npit# ipmitool -I lanplus -U $USERNAME -E -H $bmc chassis power off Repeat the above process for each NCN.\n2. Update management node firmware All firmware can be found in the HFP package provided with the Shasta release.\nThe management nodes are expected to have certain minimum firmware installed for BMC, node BIOS, and PCIe cards. Where possible, the firmware should be updated prior to install. It is good to meet the minimum NCN firmware requirement before starting.\nNote: When the PIT node is booted from the LiveCD, it is not possible to use the Firmware Action Service (FAS) to update the the firmware because that service has not yet been installed. However, at this point, it would be possible to use the HPE Cray EX HPC Firmware Pack (HFP) product on the PIT node to learn about the firmware versions available in HFP.\nIf the firmware is not updated at this point in the installation workflow, then it can be done with FAS after CSM and HFP have both been installed and configured. However, at that point a rolling reboot procedure for the management nodes will be needed, after the firmware has been updated.\nSee the HPE Cray EX System Software Getting Started Guide (S-8000) 22.07 on the HPE Customer Support Center for information about the HPE Cray EX HPC Firmware Pack (HFP) product. In the HFP documentation there is information about the recommended firmware packages to be installed. See \u0026ldquo;Product Details\u0026rdquo; in the HPE Cray EX HPC Firmware Pack Installation Guide.\nSome of the component types have manual procedures to check firmware versions and update firmware. See Upgrading Firmware Without FAS in the HPE Cray EX HPC Firmware Pack Installation Guide. It will be possible to extract the files from the product tarball, but the install.sh script from that product will be unable to load the firmware versions into the Firmware Action Services (FAS) because the management nodes are not booted and running Kubernetes and FAS cannot be used until Kubernetes is running.\nIf booted into the PIT node, the firmware can be found with HFP package provided with the Shasta release.\n(optional) Check these BIOS settings on management nodes NCN BIOS.\nThis is optional, the BIOS settings (or lack thereof) do not prevent deployment. The NCN installation will work with the CMOS\u0026rsquo; default BIOS. There may be settings that facilitate the speed of deployment, but they may be tuned at a later time.\nNOTE: The BIOS tuning will be automated, further reducing this step.\nThe firmware on the management nodes should be checked for compliance with the minimum required version and updated, if necessary, at this point.\nWARNING: Gigabyte NCNs running BIOS version C20 can become unusable when Shasta 1.5 is installed. This is a result of a bug in the Gigabyte firmware. This bug has not been observed in BIOS version C17.\nA key symptom of this bug is that the NCN will not PXE boot and will instead fall through to the boot menu, despite being configure to PXE boot. This behavior will persist until the failing node\u0026rsquo;s CMOS is cleared.\nSee Clear Gigabyte CMOS. 3. Deploy management nodes Deployment of the nodes starts with booting the storage nodes first. Then, the master nodes and worker nodes should be booted together. After the operating system boots on each node, there are some configuration actions which take place. Watching the console or the console log for certain nodes can help to understand what happens and when. When the process is complete for all nodes, the Ceph storage will have been initialized and the Kubernetes cluster will be created ready for a workload.\n3.1 Deploy workflow The configuration workflow described here is intended to help understand the expected path for booting and configuring. The actual steps to be performed are in the Deploy section.\nStart watching the consoles for ncn-s001 and at least one other storage node Boot all storage nodes at the same time The first storage node (ncn-s001) will boot; it then starts a loop as ceph-ansible configuration waits for all other storage nodes to boot. The other storage nodes boot and become passive. They will be fully configured when ceph-ansible runs to completion on ncn-s001. Once ncn-s001 notices that all other storage nodes have booted, ceph-ansible will begin Ceph configuration. This takes several minutes. Once ceph-ansible has finished on ncn-s001, then ncn-s001 waits for ncn-m002 to create /etc/kubernetes/admin.conf. Start watching the consoles for ncn-m002, ncn-m003, and at least one worker node. Boot master nodes (ncn-m002 and ncn-m003) and all worker nodes at the same time. The worker nodes will boot and wait for ncn-m002 to create the /etc/cray/kubernetes/join-command-control-plane file so that they can join Kubernetes. The third master node (ncn-m003) boots and waits for ncn-m002 to create the /etc/cray/kubernetes/join-command-control-plane file so that it can join Kubernetes The second master node (ncn-m002) boots and runs kubernetes-cloudinit.sh, which will create /etc/kubernetes/admin.conf and /etc/cray/kubernetes/join-command-control-plan. It then waits for the storage node to create etcd-backup-s3-credentials. Once ncn-s001 notices that ncn-m002 has created /etc/kubernetes/admin.conf, then ncn-s001 waits for any worker node to become available. As each worker node notices that ncn-m002 has created /etc/cray/kubernetes/join-command-control-plane, they will join the Kubernetes cluster. Once ncn-s001 notices that a worker node has done this, it moves forward with the creation of ConfigMaps and running the post-Ceph playbooks (S3, OSD pools, quotas, and so on.) Once ncn-s001 creates etcd-backup-s3-credentials during the ceph-rgw-users role (one of the last roles after Ceph has been set up), then ncn-m001 notices this and proceeds. NOTE: If several hours have elapsed between storage and master nodes booting, or if there were issues PXE booting master nodes, the cloud-init script on ncn-s001 may not complete successfully. This can cause the /var/log/cloud-init-output.log on master node(s) to continue to output the following message:\n[ 1328.351558] cloud-init[8472]: Waiting for storage node to create etcd-backup-s3-credentials secret... In this case, the following script is safe to be executed again on ncn-s001:\nncn-s001# /srv/cray/scripts/common/storage-ceph-cloudinit.sh After this script finishes, the secrets will be created and the cloud-init script on the master node(s) should complete.\n3.2 Deploy NOTE: Some scripts in this section depend on IPMI_PASSWORD being set. This is done in Tokens and IPMI Password.\nSet the default root password and SSH keys and optionally change the timezone.\nThe management nodes images do not contain a default password or default SSH keys.\nIt is required to set the default root password and SSH keys in the images used to boot the management nodes. Follow the NCN image customization steps in Change NCN Image Root Password and SSH Keys on PIT Node\nCreate boot directories for any NCN in DNS.\nThis will create folders for each host in /var/www, allowing each host to have its own unique set of artifacts: kernel, initrd, SquashFS, and script.ipxe bootscript.\nPatch the set-sqfs-links.sh script to include the blacklisting of an undesired kernel module.\npit# sed -i -E \u0026#39;s:rd.luks=0 /:rd.luks=0 module_blacklist=rpcrdma \\/:g\u0026#39; /root/bin/set-sqfs-links.sh Invoke the script.\npit# /root/bin/set-sqfs-links.sh Every NCN except for ncn-m001 should be included in the output from this script. If that is not the case, then verify that all NCN BMCs are set to use DHCP. See Set node BMCs to DHCP. After that is done, re-run the set-sqfs-links.sh script.\nCustomize boot scripts for any out-of-baseline NCNs\nWorker nodes with more than two small disks need to make adjustments to prevent bare-metal etcd creation. For a brief overview of what is expected, see disk plan of record / baseline. Run the BIOS baseline script to apply configurations to BMCs.\nThe script will apply helper configurations to facilitate more deterministic network booting on any NCN port. This runs against any server vendor, but some settings are not applied for certain vendors.\nNOTE: This script will enable DCMI/IPMI on Hewlett-Packard Enterprise servers equipped with ILO. If ipmitool is not working at this time, it will after running this script.\npit# /root/bin/bios-baseline.sh Set each node to always UEFI Network Boot, and ensure they are powered off\npit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} chassis bootdev pxe options=persistent pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} chassis bootdev pxe options=efiboot pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power off NOTE: The NCN boot order is further explained in NCN Boot Workflow.\nValidate that the LiveCD is ready for installing NCNs.\nObserve the output of the checks and note any failures, then remediate them.\nSpecify the admin user password for the management switches in the system.\nread -s is used to prevent the password from being written to the screen or the shell history.\npit# read -s SW_ADMIN_PASSWORD pit# export SW_ADMIN_PASSWORD Run the LiveCD preflight checks.\npit# csi pit validate --livecd-preflight Note: Ignore any errors about not being able resolve arti.dev.cray.com.\nPrint the available consoles.\npit# conman -q Expected output looks similar to the following:\nncn-m001-mgmt ncn-m002-mgmt ncn-m003-mgmt ncn-s001-mgmt ncn-s002-mgmt ncn-s003-mgmt ncn-w001-mgmt ncn-w002-mgmt ncn-w003-mgmt NOTE: All console logs are located at /var/log/conman/console*\nBoot the Storage Nodes\nBoot all the storage nodes. ncn-s001 will start 1 minute after the other storage nodes.\npit# grep -oP $stoken /etc/dnsmasq.d/statics.conf | grep -v \u0026#34;ncn-s001-\u0026#34; | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power on; \\ sleep 60; ipmitool -I lanplus -U $USERNAME -E -H ncn-s001-mgmt power on Observe the installation through the console of ncn-s001-mgmt.\npit# conman -j ncn-s001-mgmt From there an administrator can witness console output for the cloud-init scripts.\nNOTE: Watch the storage node consoles carefully for error messages. If any are seen, consult Ceph-CSI Troubleshooting.\nNOTE: If the nodes have PXE boot issues (for example, getting PXE errors, or not pulling the ipxe.efi binary), see PXE boot troubleshooting.\nWait for storage nodes before booting Kubernetes master nodes and worker nodes.\nNOTE: Once all storage nodes are up and the message ...sleeping 5 seconds until /etc/kubernetes/admin.conf appears on ncn-s001\u0026rsquo;s console, it is safe to proceed with booting the Kubernetes master nodes and worker nodes\npit# grep -oP \u0026#34;($mtoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power on Stop watching the console from ncn-s001.\nType the ampersand character and then the period character to exit from the conman session on ncn-s001.\n\u0026amp;. pit# Wait. Observe the installation through ncn-m002-mgmt\u0026rsquo;s console:\nPrint the console name:\npit# conman -q | grep m002 Expected output looks similar to the following:\nncn-m002-mgmt Then join the console:\npit# conman -j ncn-m002-mgmt NOTE: If the nodes have PXE boot issues (e.g. getting PXE errors, not pulling the ipxe.efi binary) see PXE boot troubleshooting\nNOTE: If one of the master nodes seems hung waiting for the storage nodes to create a secret, check the storage node consoles for error messages. If any are found, consult CEPH CSI Troubleshooting\nWait for the deployment to finish.\nRefer to timing of deployments. It should not take more than 60 minutes for the kubectl get nodes command to return output indicating that all the master nodes and worker nodes (excluding from the PIT node) booted from the LiveCD and are Ready.\nWhen the following command prompts for a password, enter the root password for ncn-m002.\npit# ssh ncn-m002 kubectl get nodes -o wide Expected output looks similar to the following:\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ncn-m002 Ready control-plane,master 2h v1.20.13 10.252.1.5 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-m003 Ready control-plane,master 2h v1.20.13 10.252.1.6 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-w001 Ready \u0026lt;none\u0026gt; 2h v1.20.13 10.252.1.7 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-w002 Ready \u0026lt;none\u0026gt; 2h v1.20.13 10.252.1.8 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-w003 Ready \u0026lt;none\u0026gt; 2h v1.20.13 10.252.1.9 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 Stop watching the console of ncn-m002.\nType the ampersand character and then the period character to exit from the conman session on ncn-m002.\n\u0026amp;. pit# Enable passwordless SSH for the PIT node.\nCopy SSH files from ncn-m002 to the PIT node.\nWhen the following command prompts for a password, enter the root password for ncn-m002.\npit# rsync -av ncn-m002:.ssh/ /root/.ssh/ Expected output looks similar to the following:\nPassword: receiving incremental file list ./ authorized_keys id_rsa id_rsa.pub known_hosts sent 145 bytes received 13,107 bytes 3,786.29 bytes/sec total size is 12,806 speedup is 0.97 Make a list of all of the NCNs (including ncn-m001).\npit# NCNS=$(grep -oP \u0026#34;ncn-[msw][0-9]{3}\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) ; echo \u0026#34;${NCNS}\u0026#34; Expected output looks similar to the following:\nncn-m001,ncn-m002,ncn-m003,ncn-s001,ncn-s002,ncn-s003,ncn-w001,ncn-w002,ncn-w003, Verify that passwordless SSH is now working from the PIT node to the other NCNs.\nThe following command should not prompt for a password.\npit# PDSH_SSH_ARGS_APPEND=\u0026#39;-o StrictHostKeyChecking=no\u0026#39; pdsh -Sw \u0026#34;${NCNS}\u0026#34; date \u0026amp;\u0026amp; echo SUCCESS || echo ERROR Expected output looks similar to the following:\nncn-w001: Warning: Permanently added \u0026#39;ncn-w001,10.252.1.7\u0026#39; (ECDSA) to the list of known hosts. ncn-w003: Warning: Permanently added \u0026#39;ncn-w003,10.252.1.9\u0026#39; (ECDSA) to the list of known hosts. ncn-m003: Warning: Permanently added \u0026#39;ncn-m003,10.252.1.6\u0026#39; (ECDSA) to the list of known hosts. ncn-s002: Warning: Permanently added \u0026#39;ncn-s002,10.252.1.11\u0026#39; (ECDSA) to the list of known hosts. ncn-m001: Warning: Permanently added \u0026#39;ncn-m001,10.252.1.4\u0026#39; (ECDSA) to the list of known hosts. ncn-w002: Warning: Permanently added \u0026#39;ncn-w002,10.252.1.8\u0026#39; (ECDSA) to the list of known hosts. ncn-m002: Warning: Permanently added \u0026#39;ncn-m002,10.252.1.5\u0026#39; (ECDSA) to the list of known hosts. ncn-s003: Warning: Permanently added \u0026#39;ncn-s003,10.252.1.12\u0026#39; (ECDSA) to the list of known hosts. ncn-s001: Warning: Permanently added \u0026#39;ncn-s001,10.252.1.10\u0026#39; (ECDSA) to the list of known hosts. ncn-s003: Thu 28 Apr 2022 02:43:21 PM UTC ncn-s001: Thu 28 Apr 2022 02:43:21 PM UTC ncn-s002: Thu 28 Apr 2022 02:43:21 PM UTC ncn-m001: Thu 28 Apr 2022 02:43:21 PM UTC ncn-m003: Thu 28 Apr 2022 02:43:21 PM UTC ncn-m002: Thu 28 Apr 2022 02:43:21 PM UTC ncn-w001: Thu 28 Apr 2022 02:43:22 PM UTC ncn-w002: Thu 28 Apr 2022 02:43:22 PM UTC ncn-w003: Thu 28 Apr 2022 02:43:22 PM UTC SUCCESS Validate that the expected LVM labels are present on disks on the master and worker nodes.\npit# /usr/share/doc/csm/install/scripts/check_lvm.sh Expected output looks similar to the following:\nWhen prompted, please enter the NCN password for ncn-m002 Warning: Permanently added \u0026#39;ncn-m002,10.252.1.11\u0026#39; (ECDSA) to the list of known hosts. Password: Checking ncn-m002... ncn-m002: OK Checking ncn-m003... Warning: Permanently added \u0026#39;ncn-m003,10.252.1.10\u0026#39; (ECDSA) to the list of known hosts. Warning: Permanently added \u0026#39;ncn-m003,10.252.1.10\u0026#39; (ECDSA) to the list of known hosts. ncn-m003: OK Checking ncn-w001... Warning: Permanently added \u0026#39;ncn-w001,10.252.1.9\u0026#39; (ECDSA) to the list of known hosts. Warning: Permanently added \u0026#39;ncn-w001,10.252.1.9\u0026#39; (ECDSA) to the list of known hosts. ncn-w001: OK Checking ncn-w002... Warning: Permanently added \u0026#39;ncn-w002,10.252.1.8\u0026#39; (ECDSA) to the list of known hosts. Warning: Permanently added \u0026#39;ncn-w002,10.252.1.8\u0026#39; (ECDSA) to the list of known hosts. ncn-w002: OK Checking ncn-w003... Warning: Permanently added \u0026#39;ncn-w003,10.252.1.7\u0026#39; (ECDSA) to the list of known hosts. Warning: Permanently added \u0026#39;ncn-w003,10.252.1.7\u0026#39; (ECDSA) to the list of known hosts. ncn-w003: OK SUCCESS: LVM checks passed on all master and worker NCNs If the check fails for any nodes, then the problem must be resolved before continuing. See LVM Check Failure Recovery.\nApply the boot order workaround.\npit# /usr/share/doc/csm/scripts/workarounds/boot-order/run.sh Apply the kdump workaround.\nkdump assists in taking a dump of the NCN if it encounters a kernel panic. kdump does not work properly in CSM 1.2. Until this workaround is applied, kdump may not produce a proper dump. Running this script applies the workaround on all of the NCNs that were just deployed.\npit# /usr/share/doc/csm/scripts/workarounds/kdump/run.sh Example output:\nUploading hotfix files to ncn-m001:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-m002:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-m003:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s001:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s002:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s003:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-s004:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w001:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w002:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w003:/srv/cray/scripts/common/ ... Done Uploading hotfix files to ncn-w004:/srv/cray/scripts/common/ ... Done Running updated create-kdump-artifacts.sh script on [11] NCNs ... Done The following NCNs contain the kdump patch: ncn-m001 ncn-m002 ncn-m003 ncn-s001 ncn-s002 ncn-s003 ncn-s004 ncn-w001 ncn-w002 ncn-w003 ncn-w004 This workaround has completed. 3.3 Check for unused drives on utility storage nodes IMPORTANT: Do the following if NCNs are Gigabyte hardware. It is suggested (but optional) for HPE NCNs.\nIMPORTANT: Estimate the expected number of OSDs using the following table and using this equation:\ntotal_osds = (number of utility storage/Ceph nodes) * (OSD count from table below for the appropriate hardware)\nHardware Manufacturer OSD Drive Count (not including OS drives) GigaByte 12 HPE 8 Option 1 If there are OSDs on each node (ceph osd tree can show this), then all the nodes are in Ceph. That means the orchestrator can be used to look for the devices.\nGet the number of OSDs in the cluster.\nncn-s# ceph -f json-pretty osd stat |jq .num_osds 24 IMPORTANT: If the returned number of OSDs is equal to total_osds calculated, then skip the following steps. If not, then proceed with the below additional checks and remediation steps.\nCompare the number of OSDs to the output (which should resemble the example below). The number of drives will depend on the server hardware.\nNOTE: If the Ceph cluster is large and has a lot of nodes, a node may be specified after the below command to limit the results.\nncn-s# ceph orch device ls Hostname Path Type Serial Size Health Ident Fault Available ncn-s001 /dev/sda ssd PHYF015500M71P9DGN 1920G Unknown N/A N/A No ncn-s001 /dev/sdb ssd PHYF016500TZ1P9DGN 1920G Unknown N/A N/A No ncn-s001 /dev/sdc ssd PHYF016402EB1P9DGN 1920G Unknown N/A N/A No ncn-s001 /dev/sdd ssd PHYF016504831P9DGN 1920G Unknown N/A N/A No ncn-s001 /dev/sde ssd PHYF016500TV1P9DGN 1920G Unknown N/A N/A No ncn-s001 /dev/sdf ssd PHYF016501131P9DGN 1920G Unknown N/A N/A No ncn-s001 /dev/sdi ssd PHYF016500YB1P9DGN 1920G Unknown N/A N/A No ncn-s001 /dev/sdj ssd PHYF016500WN1P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sda ssd PHYF0155006W1P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sdb ssd PHYF0155006Z1P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sdc ssd PHYF015500L61P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sdd ssd PHYF015502631P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sde ssd PHYF0153000G1P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sdf ssd PHYF016401T41P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sdi ssd PHYF016504C21P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sdj ssd PHYF015500GQ1P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sda ssd PHYF016402FP1P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sdb ssd PHYF016401TE1P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sdc ssd PHYF015500N51P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sdd ssd PHYF0165010Z1P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sde ssd PHYF016500YR1P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sdf ssd PHYF016500X01P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sdi ssd PHYF0165011H1P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sdj ssd PHYF016500TQ1P9DGN 1920G Unknown N/A N/A No If there are devices that show Available as Yes and they are not being automatically added, that device may need to be zapped.\nIMPORTANT: Prior to zapping any device, ensure that it is not being used.\nCheck to see if the number of devices is less than the number of listed drives in the output from step 1.\nncn-s# ceph orch device ls|grep dev|wc -l Example output:\n24 If the numbers are equal, but less than the total_osds calculated, then the ceph-mgr daemon may need to be failed in order to get a fresh inventory.\nncn-s# ceph mgr fail $(ceph mgr dump | jq -r .active_name) Wait 5 minutes and then re-check ceph orch device ls. See if the drives are still showing as Available. If so, then proceed to the next step.\nssh to the host and look at lsblk output and check against the device from the above ceph orch device ls\nncn-s# lsblk Example output:\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 4.2G 1 loop / run/ rootfsbase loop1 7:1 0 30G 0 loop └─live-overlay-pool 254:8 0 300G 0 dm loop2 7:2 0 300G 0 loop └─live-overlay-pool 254:8 0 300G 0 dm sda 8:0 0 1.8T 0 disk └─ceph--0a476f53--8b38--450d--8779--4e587402f8a8-osd--data--b620b7ef--184a--46d7--9a99--771239e7a323 254:7 0 1.8T 0 lvm If it has an LVM volume like above, then it may be in use. In that case, do the option 2 check below to make sure that the drive can be wiped. Option 2 Log into each ncn-s node and check for unused drives.\nncn-s# cephadm shell -- ceph-volume inventory IMPORTANT: The cephadm command may output this warning WARNING: The same type, major and minor should not be used for multiple devices.. Ignore this warning.\nThe field available would be True if Ceph sees the drive as empty and can be used. For example:\nDevice Path Size rotates available Model name /dev/sda 447.13 GB False False SAMSUNG MZ7LH480 /dev/sdb 447.13 GB False False SAMSUNG MZ7LH480 /dev/sdc 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sdd 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sde 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sdf 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sdg 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sdh 3.49 TB False False SAMSUNG MZ7LH3T8 Alternatively, just dump the paths of available drives:\nncn-s# cephadm shell -- ceph-volume inventory --format json-pretty | jq -r \u0026#39;.[]|select(.available==true)|.path\u0026#39; Wipe and add drives Wipe the drive ONLY after confirming that the drive is not being used by the current Ceph cluster using options 1, 2, or both.\nThe following example wipes drive /dev/sdc on ncn-s002. Replace these values with the appropriate ones for the situation.\nncn-s# ceph orch device zap ncn-s002 /dev/sdc --force Add unused drives.\nncn-s# cephadm shell -- ceph-volume lvm create --data /dev/sd\u0026lt;drive to add\u0026gt; --bluestore More information can be found at the cephadm reference page.\n4. Configure after management node deployment After the management nodes have been deployed, configuration can be applied to the booted nodes.\n4.1 LiveCD cluster authentication The LiveCD needs to authenticate with the cluster to facilitate the rest of the CSM installation.\nDetermine which master node is the first master node.\nMost often the first master node will be ncn-m002.\nRun the following commands on the PIT node to extract the value of the first-master-hostname field from the /var/www/ephemeral/configs/data.json file:\npit# FM=$(cat /var/www/ephemeral/configs/data.json | jq -r \u0026#39;.\u0026#34;Global\u0026#34;.\u0026#34;meta-data\u0026#34;.\u0026#34;first-master-hostname\u0026#34;\u0026#39;) pit# echo $FM Copy the Kubernetes configuration file from that node to the LiveCD to be able to use kubectl as cluster administrator.\nRun the following commands on the PIT node:\npit# mkdir -v ~/.kube pit# scp ${FM}.nmn:/etc/kubernetes/admin.conf ~/.kube/config Validate that kubectl commands run successfully from the PIT node.\npit# kubectl get nodes -o wide Expected output looks similar to the following:\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ncn-m002 Ready control-plane,master 2h v1.20.13 10.252.1.5 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-m003 Ready control-plane,master 2h v1.20.13 10.252.1.6 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-w001 Ready \u0026lt;none\u0026gt; 2h v1.20.13 10.252.1.7 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-w002 Ready \u0026lt;none\u0026gt; 2h v1.20.13 10.252.1.8 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 ncn-w003 Ready \u0026lt;none\u0026gt; 2h v1.20.13 10.252.1.9 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP3 5.3.18-59.19-default containerd://1.5.7 4.2 Install tests and test server on NCNs Run the following commands on the PIT node.\npit# pushd /var/www/ephemeral \u0026amp;\u0026amp; ${CSM_RELEASE}/lib/install-goss-tests.sh \u0026amp;\u0026amp; popd 4.3 Clean up chrony configurations Run the following command without editing the value of the TOKEN variable.\npit# for i in $(grep -oP \u0026#39;ncn-\\w\\d+\u0026#39; /etc/dnsmasq.d/statics.conf | sort -u | grep -v ncn-m001); do ssh $i \u0026#34;TOKEN=token /srv/cray/scripts/common/chrony/csm_ntp.py\u0026#34;; done Successful output can appear as:\nIf BSS is unreachable, local cache is checked and the configuration is still deployed:\n... BSS query failed. Checking local cache... Chrony configuration created Problematic config found: /etc/chrony.d/cray.conf.dist Problematic config found: /etc/chrony.d/pool.conf Restarted chronyd ... 5. Validate management node deployment The following csi pit validate commands will run a series of remote tests on the other nodes to validate they are healthy and configured correctly.\nObserve the output of the checks. If there are any failures, remediate them.\nCheck the storage nodes.\npit# csi pit validate --ceph | tee csi-pit-validate-ceph.log Once that command has finished, the following will extract the test totals reported for each node:\npit# grep \u0026#34;Total Test\u0026#34; csi-pit-validate-ceph.log Example output for a system with three storage nodes:\nTotal Tests: 8, Total Passed: 8, Total Failed: 0, Total Execution Time: 74.3782 seconds Total Tests: 3, Total Passed: 3, Total Failed: 0, Total Execution Time: 0.6091 seconds Total Tests: 3, Total Passed: 3, Total Failed: 0, Total Execution Time: 0.6260 seconds If these total lines report any failed tests, then look through the full output of the test in csi-pit-validate-ceph.log to see which node had the failed test and what the details are for that test.\nNote: See Utility Storage and Ceph CSI Troubleshooting in order to help resolve any failed tests.\nCheck the master and worker nodes.\nNote: Throughout the output of the csi pit validate command are test totals for each node where the tests run. Be sure to check all of them and not just the final one. A grep command is provided to help with this.\npit# csi pit validate --k8s | tee csi-pit-validate-k8s.log Once that command has finished, the following will extract the test totals reported for each node:\npit# grep \u0026#34;Total Test\u0026#34; csi-pit-validate-k8s.log Example output for a system with five master and worker nodes (excluding the PIT node):\nTotal Tests: 16, Total Passed: 16, Total Failed: 0, Total Execution Time: 0.3072 seconds Total Tests: 16, Total Passed: 16, Total Failed: 0, Total Execution Time: 0.2727 seconds Total Tests: 12, Total Passed: 12, Total Failed: 0, Total Execution Time: 0.2841 seconds Total Tests: 12, Total Passed: 12, Total Failed: 0, Total Execution Time: 0.3622 seconds Total Tests: 12, Total Passed: 12, Total Failed: 0, Total Execution Time: 0.2353 seconds If these total lines report any failed tests, then look through the full output of the test in csi-pit-validate-k8s.log to see which node had the failed test and what the details are for that test.\nWARNING: Notes on specific failures:\nIf any of the FS Label tests fail (they have names like Master Node ETCDLVM FS Label or Worker Node CONLIB FS Label), then run manual tests on the node which reported the failure. See Manual LVM Check Procedure. If the manual tests fail, then the problem must be resolved before continuing to the next step. See LVM Check Failure Recovery. If the Weave Health test fails, run weave --local status connections on the node where the test failed. If messages similar to IP allocation was seeded by different peers are seen, then weave appears to be split-brained. At this point, it is necessary to wipe the NCNs and start the PXE boot again: Wipe the NCNs using the \u0026lsquo;Basic Wipe\u0026rsquo; section of Wipe NCN Disks for Reinstallation. Return to the \u0026lsquo;Boot the Storage Nodes\u0026rsquo; step of Deploy Management Nodes section above. Verify that all the pods in the kube-system namespace are Running or Completed.\nRun the following command on any Kubernetes master or worker node, or the PIT node:\nncn-mw/pit# kubectl get pods -o wide -n kube-system | grep -Ev \u0026#39;(Running|Completed)\u0026#39; If any pods are listed by this command, it means they are not in the Running or Completed state. Do not proceed before investigating this.\nImportant checkpoint Before proceeding, be aware that this is the last point where the other NCNs can be rebuilt without also having to rebuild the PIT node. Therefore, take time to double check both the cluster and the validation test results\nNext topic After completing the deployment of the management nodes, the next step is to install the CSM services.\nSee Install CSM Services.\nLVM check troubleshooting This section gives information on troubleshooting and remediating issues with the LVM check performed during the Deploy Management Nodes procedure. If that check passed, this section can be ignored.\nManual LVM check procedure If needed, the LVM checks can be performed manually on the master and worker nodes.\nManual check on master nodes:\nncn-m# blkid -L ETCDLVM Example output:\n/dev/sdc Manual check on worker nodes:\nncn-w# blkid -L CONLIB /dev/sdb2 ncn-w# blkid -L CONRUN /dev/sdb1 ncn-w# blkid -L K8SLET /dev/sdb3 The manual checks are considered successful if all of the blkid commands report a disk device (such as /dev/sdc \u0026ndash; the particular device is unimportant). If any of the lsblk commands return no output, then the check is a failure. Any failures must be resolved before continuing. See the following section for details on how to do so.\nLVM check failure recovery If there are LVM check failures, then the problem must be resolved before continuing with the install.\nIf any master node has the problem, then wipe and redeploy all of the NCNs before continuing the installation:\nWipe each worker node using the \u0026lsquo;Basic Wipe\u0026rsquo; section of Wipe NCN Disks for Reinstallation. Wipe each master node (except ncn-m001 because it is the PIT node) using the \u0026lsquo;Basic Wipe\u0026rsquo; section of Wipe NCN Disks for Reinstallation. Wipe each storage node using the \u0026lsquo;Full Wipe\u0026rsquo; section of Wipe NCN Disks for Reinstallation. Return to the Set each node to always UEFI Network Boot, and ensure they are powered off step of the Deploy Management Nodes section above. If only worker nodes have the problem, then wipe and redeploy the affected worker nodes before continuing the installation:\nWipe each affected worker node using the \u0026lsquo;Basic Wipe\u0026rsquo; section of Wipe NCN Disks for Reinstallation. Power off each affected worker node. Return to the Boot the Master and Worker Nodes step of the Deploy Management Nodes section above. Note: The ipmitool command will give errors trying to power on the unaffected nodes, because they are already powered on \u0026ndash; this is expected and not a problem. "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/troubleshoot_ceph_mds_reporting_slow_requests_and_failure_on_client/",
	"title": "Troubleshooting Ceph MDS Reporting Slow Requests and Failure on Client",
	"tags": [],
	"description": "",
	"content": "Troubleshooting Ceph MDS Reporting Slow Requests and Failure on Client Use this procedure to troubleshoot Ceph MDS reporting slow requests after following the Identify Ceph Latency Issues procedure.\nIMPORTANT: This procedure includes a mix of commands that need to be run on the host(s) running the MDS daemon(s) and other commands that can be run from any of the ceph-mon nodes.\nNOTICE: These steps are based off upstream Ceph documentation.\nPrerequisites The Identify Ceph Latency Issues procedure has been completed. This issue has been encountered and this page is being used as a reference for commands. The correct version of the documentation for the cluster running is being used. Procedure Identify the active MDS.\nncn-s00(1/2/3)# ceph fs status -f json-pretty|jq -r \u0026#39;.mdsmap[]|select(.state==\u0026#34;active\u0026#34;)|.name\u0026#39; cephfs.ncn-s003.ihwkop ssh to the host running the active MDS.\nEnter into a cephadm shell.\nncn-s003# cephadm shell Example output:\nInferring fsid 7350865a-0b21-11ec-b9fa-fa163e06c459 Inferring config /var/lib/ceph/7350865a-0b21-11ec-b9fa-fa163e06c459/mon.ncn-s003/config Using recent ceph image arti.dev.cray.com/third-party-docker-stable-local/ceph/ ceph@sha256:70536e31b29a4241999ec4fd13d93e5860a5ffdc5467911e57e6bf04dfe68337 [ceph: root@ncn-s003 /]# NOTE: Messages such as \u0026ldquo;WARNING: The same type, major and minor should not be used for multiple devices\u0026rdquo; can be ignored. There is an upstream bug to address this issue.\nDump in-flight ops.\n[ceph: root@ncn-s003 /]# ceph daemon mds.cephfs.ncn-s003.ihwkop dump_ops_in_flight Example output:\n{ \u0026#34;ops\u0026#34;: [], \u0026#34;num_ops\u0026#34;: 0 } NOTE: The example above is about how to run the command. Recreating the exact scenario to provide a full example is not easily done. This will be updated when the information is available.\nGeneral Steps from Upstream Identify the stuck commands and examine why they are stuck.\nUsually the last \u0026ldquo;event\u0026rdquo; will have been an attempt to gather locks, or sending the operation off to the MDS log.\nIf it is waiting on the OSDs, fix them.\nIf operations are stuck on a specific inode, you probably have a client holding caps which prevent others from using it, either because the client is trying to flush out dirty data or because you have encountered a bug in CephFS\u0026rsquo; distributed file lock code (the file \u0026ldquo;capabilities\u0026rdquo; [\u0026ldquo;caps\u0026rdquo;] system).\nIf it is a result of a bug in the capabilities code, restarting the MDS is likely to resolve the problem. If there are no slow requests reported on the MDS, and it is not reporting that clients are misbehaving, either the client has a problem or its requests are not reaching the MDS.\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/configure_the_rsa_plugin_in_keycloak/",
	"title": "Configure the RSA Plugin in Keycloak",
	"tags": [],
	"description": "",
	"content": "Configure the RSA Plugin in Keycloak Use Keycloak to configure a plugin that enables RSA token authentication.\nPrerequisites Procedure Verification Prerequisites Access to the Keycloak UI is needed.\nProcedure Verify the Shasta domain is being used.\nThis is indicated in the dropdown in the upper left of the UI.\nClick on Authentication under the Configure header of the navigation area on the left side of the page.\nClick on the Flows tab.\nClick the dropdown button in the table header and switch to Browser.\nClick the Copy button in the table header.\nEnter RSA - Browser for the New Name type.\nClick the Add execution button in the table header.\nSwitch the Provider to RSA and click Save.\nUpdate the Requirement field.\nSet the table values to the following:\nField Requirement RSA - Browser Forms REQUIRED Username Password Form REQUIRED RSA - Browser - Conditional OTP CONDITIONAL Condition - User Configured DISABLED OTP Form DISABLED RSA REQUIRED Click the Actions dropdown on the RSA line of the table, then select Config.\nEnter the different configuration options:\nConfiguration Field Value Alias Enter the desired alias. For example, RSA could be used. RSA URL The base URL of the RSA API service. For example, https://rsa.mycompany.com:5555/ RSA Verify Endpoint /mfa/v1_1/authn/initialize Keycloak Client ID The authentication agent. For example, rsa.mycompany.com. The value is from Access \u0026gt; Authentication Agents \u0026gt; Manage Existing in the RSA Console. RSA Authentication Manager Client Key The key for the RSA API. Set the Shared username if applicable.\nIf the usernames are the same in Keycloak and RSA, then this can be set to ON. This means that the browser flow will not ask for the username for the RSA validation.\nClick Save.\nReturn to the Flows tab on the Authentication page.\nClick the dropdown button in the table header and switch to Direct Grant.\nClick the Copy button in the table header.\nEnter RSA - CLI for the New Name type.\nClick the Add execution button in the table header.\nSwitch the Provider to RSA - CLI and click Save.\nUpdate the Requirement field.\nSet the table values to the following:\nField Requirement RSA - CLI REQUIRED RSA - CLI Direct Grant - Conditional OTP DISABLED Click Save.\nSwitch to the Bindings tab in the Authentication page.\nChange Browser Flow to RSA - Browser.\nChange Direct Grant Flow to RSA - CLI.\nClick Save.\nVerification After this is set up, verify that it is working:\nPoint a browser at the following URL: http://auth.cmn.SYSTEM_DOMAIN_NAME/keycloak/realms/shasta/account\nThe browser will be directed to the user login page. The first screen will ask for the username and password in Keycloak. After logging in this way, the next page will ask for the RSA username and token code.\nGet a token using the direct grant flow.\nReplace USER with a user in Keycloak, PWD_NAME with the user\u0026rsquo;s password, RSA_USER with the user in RSA, and TOKEN_CODE with the token code:\nncn-mw# curl -i -d grant_type=password -d client_id=shasta -d username=USER \\ -d password=PWD_NAME -d rsa_username=RSA_USER -d rsa_otp=TOKEN_CODE \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token "
},
{
	"uri": "/docs-csm/en-12/operations/power_management/power_management/",
	"title": "Power Management",
	"tags": [],
	"description": "",
	"content": "Power Management HPE Cray System Management (CSM) software manages and controls power out-of-band through Redfish APIs. Note that power management features are \u0026ldquo;asynchronous,\u0026rdquo; in that the client must determine whether the component status has changed after a power management API call returns.\nIn-band power management features are not supported in v1.4.\nHPE supports Slurm as a workload manager which reports job energy usage and records it in the ITDB for system accounting purposes.\nsma-postgres-cluster A time-series PostgreSQL database (sma-postgres-cluster) contains the power telemetry data and tracks job start/end times, job-id, application-id, user-id, and application node allocation data. This data is made available through the System Monitoring Framework (SMF). Power monitoring and job data in sma-postgres-cluster enable out-of-band power profiling on the management nodes. Slurm interfaces with the PostgreSQL database through a plug-in.\nHPE Cray EX EX Systems Cabinet-level power/energy data from compute blades, switch blades, and chassis rectifiers is collected by each Chassis Management Module (CMM) and provided on system management network. This power telemetry can be monitored by the SMF. Cabinet-level power data is collected and forwarded to the management nodes. The management nodes store the telemetry in the power management database.\nPM Counters The blade-level and node-level accumulated energy telemetry is point-in-time power data. Blade accumulated energy data is collected out-of-band and is made available via workload managers. Users have access to the data in-band at the node-level via a special sysfs files in /sys/cray/pm_counters on the node.\nHPE Cray EX Standard Rack Systems Rack systems support 2 intelligent power distribution units (iPDUs). The power/energy telemetry, temperature, and humidity measurements (supported by optional probes), are accessible through the iPDU HTTP interface.\nNode-level accumulated energy data is point-in-time power and accumulated energy data collected via Redfish through the server BMC.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/updating_cabinet_routes_on_management_ncns/",
	"title": "Updating Cabinet Routes on Management NCNs",
	"tags": [],
	"description": "",
	"content": "Updating Cabinet Routes on Management NCNs This procedure will use configuration from System Layout Service (SLS) to set up the proper routing for all air-cooled and liquid-cooled cabinets present in the system on each of the Management NCNs.\nPrerequisites Passwordless SSH to all of the management NCNs is configured.\nEnsure that Cray Site Init (CSI) is installed and available on ncn-m001.\nncn-m001# csi version If the csi command is not available, then install it:\nEnsure the csm-sle-15sp2 RPM repository has been added to ncn-m001.\nncn-m001# zypper lr csm-sle-15sp2 Expected output:\nAlias : csm-sle-15sp2 Name : CSM SLE 15 SP2 Packages (added by Ansible) URI : https://packages.local/repository/csm-sle-15sp2 Enabled : Yes GPG Check : ( p) Yes Priority : 99 (default priority) Autorefresh : On Keep Packages : Off Type : rpm-md GPG Key URI : Path Prefix : Parent Service : Keywords : --- Repo Info Path : /etc/zypp/repos.d/csm-sle-15sp2.repo MD Cache Path : /var/cache/zypp/raw/csm-sle-15sp2 If the csm-sle-15sp2 repository is not present, then add it.\nncn-m001# zypper addrepo -fG https://packages.local/repository/csm-sle-15sp2 csm-sle-15sp2 Install Cray Site Init.\nncn-m001# zypper install cray-site-init Procedure Get an API token.\nncn-m001# export TOKEN=$(curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Add cabinet routes to each of the management NCNs using data from SLS.\nncn-m001# /usr/share/doc/csm/scripts/operations/node_management/update-ncn-cabinet-routes.sh If the following message appears, then the route being added is already present on the NCN and can be safely ignored.\nRTNETLINK answers: File exists Create payload to update the cloud-init user data for management NCNs in BSS to contain the updated cabinet route information.\nncn-m001# cat \u0026lt;\u0026lt;EOF \u0026gt;write-files-user-data.json { \u0026#34;user-data\u0026#34;: { \u0026#34;write_files\u0026#34;: [{ \u0026#34;content\u0026#34;: $(jq -n --rawfile file /etc/sysconfig/network/ifroute-bond0.nmn0 \u0026#39;$file\u0026#39;), \u0026#34;owner\u0026#34;: \u0026#34;root:root\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/etc/sysconfig/network/ifroute-bond0.nmn0\u0026#34;, \u0026#34;permissions\u0026#34;: \u0026#34;0644\u0026#34; }, { \u0026#34;content\u0026#34;: $(jq -n --rawfile file /etc/sysconfig/network/ifroute-bond0.hmn0 \u0026#39;$file\u0026#39;), \u0026#34;owner\u0026#34;: \u0026#34;root:root\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/etc/sysconfig/network/ifroute-bond0.hmn0\u0026#34;, \u0026#34;permissions\u0026#34;: \u0026#34;0644\u0026#34; } ] } } EOF Update BSS cloud-init user data for the management NCNs.\nncn-m001# ncn_xnames=$(curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \u0026#34;https://api-gw-service-nmn.local/apis/sls/v1/search/hardware?extra_properties.Role=Management\u0026#34; | jq -r \u0026#39;.[] | .Xname\u0026#39; | sort) ncn-m001# for ncn in $ncn_xnames; do echo \u0026#34;Updating BSS for $ncn\u0026#34; csi handoff bss-update-cloud-init --user-data=write-files-user-data.json --limit=${ncn} done "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/change_settings_in_the_bond/",
	"title": "Change Settings in the Bond",
	"tags": [],
	"description": "",
	"content": "Change Settings in the Bond iPXE is used to setup udev rules for interface names and bond members. The configuration of these are dynamic on boot until node customization runs (cloud-init) and sets up the conventional /etc/sysconfig/network/ifcfg-bond0 and other neighboring files.\nThe initial settings of the bonds can be changed directly in the LiveCD or with the Boot Script Service (BSS). When cabling is different than normal, there is flexibility for customizing bond links.\nCustomize the settings of the bonds.\nFor more information, refer to the following lines of the script.ipxe file:\n# Assign bonds. iseq ${dual-bond} 1 \u0026amp;\u0026amp; set net-udev-params ${net-udev-params} ${net-hsn-udev-params} ${net-lan-udev-params} ${net-mgmt-udev-params} ${net-sun-udev-params} || set net-udev-params ${net-udev-params} ${net-hsn-udev-params} ${net-lan-udev-params} ${net-mgmt-udev-params} ${net-mgmt-single-bond-udev-params} iseq ${dual-bond} 1 \u0026amp;\u0026amp; echo Dual-Bond mode: Enabled (mgmt and sun NICs) || echo Dual-Bond mode: Disabled (mgmt NICs only) These are available on the PIT node at /var/www/ncn-*/script.ipxe, and the master script is available at /var/www/boot/script.ipxe. Administrators may apply this to each NCN or the master script. If applied to the master script, then the following procedure will apply that master script to all NCNs:\nread -s is used to prevent the password from being written to the screen or the shell history.\npit# read -s IPMI_PASSWORD pit# export IPMI_PASSWORD pit# /root/bin/set-sqfs-links.sh Use one of the following options depending on the number of bonds formed:\nWhen one bond is formed:\nbond=bond0:mgmt0,mgmt2:mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3:9000 || set net-bond-params bond=bond0:mgmt0,mgmt1:mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3:9000 hwprobe=+200:*:*:bond0 When two bonds are formed:\nbond=bond0:mgmt0,mgmt2:mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3:9000 hwprobe=+200:*:*:bond0 bond=bond1:mgmt1,mgmt3:mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3:9000 hwprobe=+200:*:*:bond1 ip=bond1:auto6 "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/key_features/",
	"title": "Key features used in the management network configuration",
	"tags": [],
	"description": "",
	"content": "Key features used in the management network configuration This section is to list and define the key features used in the management network configuration.\nFeature list:\nFeature Notes Description MLAG Layer 2 Redundancy, Allows the NCNs to be bonded so if one link fails they can continue to operate. MLAG Layer 3 Redundancy, Allows one Spine switch/default gateway to fail and continue to work Vlan Segregates layer 2 broadcast domains, need to separate NMN/HMN/compute traffic. MSTP Layer 2 loop prevention mechanism at edge IP routing IP routing / static routes OSPF Routing protocol used to peer from Leaf switches to Spines BGP Routing protocol used to peer with MetalLB Prefix-Lists Lists to match components of an IP route Route-Maps Defines which route are redistributed NTP Network Time Protocol ACLs Access Control Lists Max MTU - 9198 Max Transmission Unit/Maximum Frame size SNMP Allows for device polling from the NCNs to map out interfaces VRF Virtual routing and forwarding, used to segregate traffic between networks Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/snmpv3_users/",
	"title": "Dell SNMPv3 Users",
	"tags": [],
	"description": "",
	"content": "Dell SNMPv3 Users SNMPv3 supports cryptographic security by a combination of authenticating and encrypting the SNMP protocol packets over the network. Read-only access is currently supported. The admin user can add or remove SNMPv3 users.\nConfiguration commands Configure a new SNMPv3 user (minimum 8 characters for passwords):\nswitch(config)# snmp-server user \u0026lt;USER\u0026gt; cray-reds-group 3 auth md5 \u0026lt;A-PASS\u0026gt; priv des \u0026lt;P-PASS\u0026gt; NOTE: Removal an SNMPv3 user us not possible on Dell equipment.\nShow commands to validate functionality:\nswitch# show snmp user Example output switch(config)# show snmp vrf SNMP enabled VRF ---------------------------- default switch(config)# show snmp user User name : testuser Group : cray-reds-group Version : 3 Authentication Protocol : MD5 Privacy Protocol : DES Expected results Administrators can configure the new user Administrators can connect to the server from the workstation Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/igmp/",
	"title": "Configure Internet Group Multicast Protocol (IGMP)",
	"tags": [],
	"description": "",
	"content": "Configure Internet Group Multicast Protocol (IGMP) The Internet Group Multicast Protocol (IGMP) is a communications protocol used by hosts and adjacent routers on IP networks to establish multicast group memberships. The host joins a multicast-group by sending a join request message towards the network router, and responds to queries sent from the network router by dispatching a join report.\nGeneral notes:\nIn ArubaOS-CX igmp snooping is disabled by default IGMP v3 is used by default, supported configuration allows v2 and v3 Configuration Commands switch(config)# interface vlan 1 switch(config-if-vlan)# igmp Expected Results show ip igmp-snooping vlan 1 should show IGMP enabled on the VLAN, but no IGMP Querier set.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/pod_resource_limits/",
	"title": "Pod Resource Limits",
	"tags": [],
	"description": "",
	"content": "Pod Resource Limits Kubernetes uses resource requests and Quality of Service (QoS) for scheduling pods. Resource requests can be provided explicitly for pods and containers, whereas pod QoS is implicit, based on the resource requests and limits of the containers in the pod. There are three types of QoS:\nGuaranteed: All containers in a pod have explicit memory and CPU resource requests and limits. For each resource, the limit equals the request. Burstable: Does not meet Guaranteed requirements, but some of the containers have explicit memory and CPU resource requests or limits. BestEffort: None of the containers specify any resources. Kubernetes will best be able to schedule pods when there are resources associated with each container in each pod.\nResource Limits For systems, all containers should have explicit resource requests and limits. Most pods should fall into the Burstable category. Containers that have resource requests equal to the resource limits should be reserved for very well behaved containers, and will usually be simple, single-function containers. One example of this could be an init container that is waiting for another resource to become available.\nResource limits are set by default at the namespace level, but pods within that namespace can increase or decrease their limits depending on the nature of the workload.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/manage_multiple_inventories_in_a_single_location/",
	"title": "Manage Multiple Inventories in a Single Location",
	"tags": [],
	"description": "",
	"content": "Manage Multiple Inventories in a Single Location Many configuration layers may be present in a single configuration for larger systems that configure multiple Cray products. When values for each of these layers need to be customized, it can be tedious to override values in each of the respective repositories. The CFS additionalInventoryUrl option allows for static inventory files to be automatically added to the hosts directory of each configuration layer before it is applied by Ansible. It is then possible to add this additional Ansible inventory information to all configuration sessions so it can be used simultaneously with other inventory types, including the CFS dynamic inventory type, across all configuration layers in a session.\nThe additionalInventoryUrl option is optional and is set on a global CFS level. If provided, it must be set to the URL of a Git repository containing inventory files in the base directory of the repository. For ordering purposes, any inventory generated by CFS will also be placed in this directory with the name 01-cfs-generated.yaml. For more information, see the Dynamic Inventory and Host Groups section in Ansible Inventory.\nThe following is an example of an inventory repository:\n02-static-inventory.ini 03-my-dynamic-inventory.py group_vars/... host_vars/... CFS will provide the following inventory to Ansible when running a configuration session:\nhosts/01-cfs-generated.yaml hosts/02-static-inventory.ini hosts/03-my-dynamic-inventory.py hosts/group_vars/... hosts/host_vars/... CFS will clone the additional inventory Git repository and use the default branch (usually master) to populate the hosts directory. Only one inventory repository can be specified, and it will apply to all CFS sessions.\nUse the following command to set the additionalInventoryUrl value:\nncn-mw# cray cfs options update --additional-inventory-url https://api-gw-service-nmn.local/vcs/cray/inventory.git Use the following command to unset the additionalInventoryUrl value:\nncn-mw# cray cfs options update --additional-inventory-url \u0026#34;\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/manage_a_bos_session/",
	"title": "Manage a BOS Session",
	"tags": [],
	"description": "",
	"content": "Manage a BOS Session Once there is a Boot Orchestration Service (BOS) session template created, users can perform operations on nodes, such as boot, reboot, configure, and shutdown. Managing sessions through the Cray CLI can be accomplished using the cray bos session commands.\nCreate a new session List all sessions Show details for a session Delete a session Create a new session Creating a new BOS session requires the following command-line options:\n--template-uuid: Use this option to specify the name value returned in the cray bos sessiontemplate list command. --operation: Use this option to indicate if a boot, reboot, configure, or shutdown action is being taken. The following is a boot operation:\nncn-mw# cray bos session create --template-uuid TEMPLATE_UUID --operation boot --format toml Example output:\noperation = \u0026#34;boot\u0026#34; templateUuid = \u0026#34;TEMPLATE_UUID\u0026#34; [[links]] href = \u0026#34;foo-c7faa704-3f98-4c91-bdfb-e377a184ab4f\u0026#34; jobId = \u0026#34;boa-a939bd32-9d27-433f-afc2-735e77ec8e58\u0026#34; rel = \u0026#34;session\u0026#34; type = \u0026#34;GET\u0026#34; It is important to periodically delete completed BOS v1 sessions. If too many BOS v1 sessions exist, it can lead to hangs when trying to list them. This limitation does not exist in BOS v2. For more information, see:\nHang Listing BOS Sessions Delete a session List all sessions List all BOS sessions with the following command:\nncn-mw# cray bos session list --format toml Example output:\nresults = [ \u0026#34;fc469e41-6419-4367-a571-d5fd92893398\u0026#34;, \u0026#34;st3-d6730dd5-f0f8-4229-b224-24df005cae52\u0026#34;,] Troubleshooting: There is a known limitation of BOS v1 that listing sessions will hang if too many sessions exist. For more information, see Hang Listing BOS Sessions.\nShow details for a session Get details for a BOS session using the session ID returned in the cray bos session list command output.\nncn-mw# cray bos session describe BOS_SESSION_ID --format toml Example output:\ncomputes = \u0026#34;boot_finished\u0026#34; boa_finish = \u0026#34;2019-12-13 17:07:23.501674\u0026#34; bos_launch = \u0026#34;2019-12-13 17:02:24.000324\u0026#34; operation = \u0026#34;reboot\u0026#34; session_template_id = \u0026#34;cle-1.1.0\u0026#34; boa_launch = \u0026#34;2019-12-13 17:02:29.703310\u0026#34; stage = \u0026#34;Done\u0026#34; Troubleshooting: There is a known issue in BOS v1 where some sessions cannot be described using the cray bos session describe command. The issue with the describe action results in a 404 error, despite the session existing in the output of cray bos session list command.\nDelete a session It is important to periodically delete completed BOS sessions. If too many BOS sessions exist, it can lead to hangs when trying to list them. For more information, see Hang Listing BOS Sessions.\nDelete a specific BOS session:\nncn-mw# cray bos session delete BOS_SESSION_ID "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_volume_configuration/",
	"title": "Delete a Volume Configuration",
	"tags": [],
	"description": "",
	"content": "Delete a Volume Configuration Delete an existing volume configuration. This procedure does not delete the underlying object referred to by the UAS volume configuration.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Volume ID of the UAS volume to be deleted: List Volumes Registered in UAS Procedure Delete the target volume configuration.\nTo delete a UAS Volume, use a command of the following form:\nncn-m001-pit# cray uas admin config volumes delete \u0026lt;volume-id\u0026gt; For example:\nncn-m001-pit# cray uas admin config volumes delete a0066f48-9867-4155-9268-d001a4430f5c If wanted, perform List Volumes Registered in UAS to confirm that the UAS volume has been deleted.\nTop: User Access Service (UAS)\nNext Topic: Resource Specifications\n"
},
{
	"uri": "/docs-csm/en-12/install/",
	"title": "Install CSM",
	"tags": [],
	"description": "",
	"content": "Install CSM Note: If installing CSM 1.2.0, then in order to avoid performing a patch update later, instead install the latest released CSM 1.2.x version.\nAbstract Installation of the CSM product stream has many steps in multiple procedures which should be done in a specific order. Information about the HPE Cray EX system and the site is used to prepare the configuration payload. The initial node used to bootstrap the installation process is called the PIT node because the Pre-Install Toolkit (PIT) is installed there.\nOnce the management network switches have been configured, the other management nodes can be deployed with an operating system and the software to create a Kubernetes cluster utilizing Ceph storage. The CSM services provide essential software infrastructure including the API gateway and many micro-services with REST APIs for managing the system. Once administrative access has been configured, the installation of CSM software can be validated with health checks before doing operational tasks like the checking and updating of firmware on system components or the preparation of compute nodes.\nOnce the CSM installation has completed, other product streams for the HPE Cray EX system can be installed.\nA major feature of CSM 1.2 is the Bifurcated CAN (BICAN). The BICAN is designed to separate administrative network traffic from user network traffic. More information can be found on the BICAN Technical Summary. Review the BICAN summary before continuing with the CSM install. For detailed BICAN documentation, see BICAN Technical Details.\nTroubleshooting installation problems The installation of the Cray System Management (CSM) product requires knowledge of the various nodes and switches for the HPE Cray EX system.\nFor additional installation-specific troubleshooting information, see Troubleshooting Installation Problems. Some topics also have supplementary troubleshooting sections listed in the CSM Operations index.\nOverview The topics in this chapter need to be done as part of an ordered procedure so are shown here with numbered topics.\nValidate SHCD Prepare configuration payload Prepare management nodes Bootstrap PIT node Configure management network switches Collect MAC addresses for NCNs Deploy management nodes Install CSM services Validate CSM health Deploy final NCN Configure administrative access Validate CSM health Configure Prometheus alert notifications Update firmware with FAS Prepare compute nodes Apply security hardening Next topic Procedure Validate SHCD\nThe cabling should be validated between the nodes and the management network switches. The information in the Shasta Cabling Diagram (SHCD) can be used to confirm the cables which physically connect components of the system. Having the data in the SHCD which matches the physical cabling will be needed later in both Prepare configuration payload and Configure management network switches.\nSee Validate SHCD.\nNote: If a reinstall or fresh install of this software release is being done on this system and the management network cabling has already been validated, then skip this step and move to Prepare configuration payload. Prepare configuration payload\nInformation gathered from a site survey is needed to feed into the CSM installation process, such as system name, system size, site network information for the Customer Access Network (CAN), site DNS configuration, site NTP configuration, network information for the node used to bootstrap the installation. Much of the information about the system hardware is encapsulated in the SHCD (Shasta Cabling Diagram), which is a spreadsheet prepared by HPE Cray Manufacturing to assemble the components of the system and connect appropriately labeled cables.\nSee Prepare Configuration Payload Prepare management nodes\nSome preparation of the management nodes might be needed before starting an install or reinstall. The preparation includes checking and updating the firmware on the PIT node, quiescing the compute nodes and application nodes, scaling back DHCP on the management nodes, wiping the storage on the management nodes, powering off the management nodes, and possibly powering off the PIT node.\nSee Prepare Management Nodes. Bootstrap PIT node\nThe Pre-Install Toolkit (PIT) node needs to be bootstrapped from the LiveCD. There are two media available to bootstrap the PIT node \u0026ndash; the RemoteISO or a bootable USB device. The recommended media is the RemoteISO, because it does not require any physical media to prepare. However, remotely mounting an ISO on a BMC does not work smoothly for nodes from all vendors. It is recommended to try the RemoteISO first.\nUse one of these procedures to bootstrap the PIT node from the LiveCD:\nBootstrap PIT Node from LiveCD Remote ISO (recommended) Gigabyte BMCs should not use the RemoteISO method. Intel BMCs should not use the RemoteISO method. Bootstrap PIT Node from LiveCD USB (fallback) Using the LiveCD USB method requires a USB 3.0 device with at least 1TB of space to create a bootable LiveCD. Configure management network switches\nNow that the PIT node has been booted with the LiveCD environment and Cray Site Init (CSI) has generated the switch IP addresses, the management network switches can be configured.\nSee Management Network User Guide.\nNote: If a reinstall of this software release is being done on this system and the management network switches have already been configured, then skip this step and move to Collect MAC addresses for NCNs. Collect MAC addresses for NCNs\nNow that the PIT node has been booted with the LiveCD and the management network switches have been configured, the actual MAC address for the management nodes can be collected. This process will include repetition of some of the steps done up to this point because csi config init will need to be run with the proper MAC addresses.\nSee Collect MAC Addresses for NCNs.\nNote: If a reinstall of this software release is being done on this system and the ncn_metadata.csv file already had valid MAC addresses for both BMC and node interfaces before csi config init was run, then this topic could be skipped and instead move to Deploy management nodes.\nNote: If a first time install of this software release is being done on this system and the ncn_metadata.csv file already had valid MAC addresses for both BMC and node interfaces before csi config init was run, then this topic could be skipped and instead move to Deploy management nodes. Deploy management nodes\nNow that the PIT node has been booted with the LiveCD and the management network switches have been configured, the other management nodes can be deployed. This procedure will boot all of the management nodes, initialize Ceph storage on the storage nodes and start the Kubernetes cluster on all of the worker nodes and the master nodes, except for the PIT node. The PIT node will join Kubernetes after it is rebooted later in Deploy final NCN.\nSee Deploy Management Nodes. Install CSM services\nDeployment of management nodes is complete with initialized Ceph storage and a running Kubernetes cluster on all worker and master nodes, except the PIT node. The Nexus repository will be populated with artifacts, containerized CSM services will be installed, and a few other configuration steps will be taken.\nSee Install CSM Services. Validate CSM health\nValidate the health of the management nodes and all CSM services. The reason to do it now is that if there are any problems detected with the core infrastructure or the nodes, it is easy to rewind the installation to Deploy management nodes, because the PIT node has not yet been rebooted. In addition, rebooting the PIT node and deploying the final NCN successfully requires several CSM services to be working properly, so validating this is important.\nSee Validate CSM Health. Deploy final NCN\nNow that all CSM services have been installed and the CSM health checks completed, with the possible exception of the User Access Service (UAS)/User Access Instance (UAI) tests, the PIT node can be rebooted to leave the LiveCD environment and assume its intended role as one the Kubernetes master nodes.\nSee Deploy Final NCN. Configure administrative access\nNow that all of the CSM services have been installed and the final NCN has been deployed, administrative access can be prepared. This may include:\nConfiguring Keycloak with a local Keycloak account or confirming that Keycloak is properly federating LDAP or another Identity Provider (IdP) Initializing the Cray CLI (cray) for administrative commands Locking the management nodes from accidental actions such as firmware updates by Firmware Action Service (FAS) or power actions by Cray Advanced Platform Monitoring and Control (CAPMC) Configuring the CSM layer of configuration by Configuration Framework Service (CFS) in NCN personalization Configuring the node BMCs (node controllers) for nodes in liquid-cooled cabinets See Configure Administrative Access. Validate CSM health\nNow that all management nodes have joined the Kubernetes cluster, CSM services have been installed, and administrative access has been enabled, the health of the management nodes and all CSM services should be validated. There are no exceptions to running the tests \u0026ndash; all tests should be run now.\nThis CSM health validation can also be run at other points during the system lifecycle, such as when replacing a management node, checking the health after a management node has rebooted because of a crash, as part of doing a full system power down or power up, or after other types of system maintenance.\nSee Validate CSM Health. Configure Prometheus alert notifications\nNow that CSM has been installed and health has been validated, if the system management health monitoring tools and specifically, Prometheus, are found to be useful, email notifications can be configured for specific alerts defined in Prometheus. Prometheus upstream documentation can be leveraged for an Alert Notification Template Reference as well as Notification Template Examples. Currently supported notification types include Slack, Pager Duty, email, or a custom integration via a generic webhook interface.\nSee Configure Prometheus Email Alert Notifications for an example configuration of an email alert notification for the Postgres replication alerts that are defined on the system. Update firmware with FAS\nNow that all management nodes and CSM services have been validated as healthy, the firmware on other components in the system can be checked and updated. The Firmware Action Service (FAS) communicates with many devices on the system. FAS can be used to update the firmware for all of the devices it communicates with at once, or specific devices can be targeted for a firmware update.\nIMPORTANT: Before FAS can be used to update firmware, refer to the HPE Cray EX System Software Getting Started Guide (S-8000) 22.07 for more information about how to install the HPE Cray EX HPC Firmware Pack (HFP) product. The installation of HFP will inform FAS of the newest firmware available. Once FAS is aware that new firmware is available, then see Update Firmware with FAS. Prepare compute nodes\nAfter completion of the firmware update with FAS, compute nodes can be prepared. Some compute node types have special preparation steps, but most compute nodes are ready to be used now.\nThese compute node types require preparation:\nHPE Apollo 6500 XL645D Gen10 Plus Gigabyte See Prepare Compute Nodes Apply security hardening\nAfter preparing compute nodes, and prior to the installation of other product streams, review the security hardening guide.\nSee Security Hardening Next topic\nAfter completion of the firmware update with FAS and the preparation of compute nodes, the CSM product stream has been fully installed and configured. Refer to the HPE Cray EX System Software Getting Started Guide (S-8000) 22.07 on the HPE Customer Support Center for more information on other product streams to be installed and configured after CSM.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/troubleshoot_ceph_osds_reporting_full/",
	"title": "Troubleshoot Ceph OSDs Reporting Full",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Ceph OSDs Reporting Full Use this procedure to examine the Ceph cluster and troubleshoot issues where Ceph runs out of space and the Kubernetes cluster cannot write data. The OSDs need to be reweighed to move data from the drive and get it back under the warning threshold.\nWhen a single OSD for a pool fills up, the pool will go into read-only mode to protect the data. This can occur if the data distribution is unbalanced or if more storage nodes are needed.\nReturn the Ceph cluster to a healthy state after it reports a full OSD.\nPrerequisites The commands in this procedure need to be run on a ceph-mon node.\nProcedure View the status of the Ceph cluster.\nncn-m001# ceph -s Example output:\ncluster: id: 64e553c3-e7d9-4636-81a4-56f26c1b20e1 health: HEALTH_ERR 1 full osd(s) 13 pool(s) full services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby osd: 18 osds: 18 up (since 20h), 18 in (since 9d) rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) data: pools: 13 pools, 288 pgs objects: 2.98M objects, 11 TiB usage: 32 TiB used, 24 TiB / 56 TiB avail pgs: 288 active+clean io: client: 379 KiB/s rd, 2.2 KiB/s wr, 13 op/s rd, 1 op/s wr View the Ceph health detail.\nThe OSD_NEARFULL list can have multiple results. Take a note of the returned results to compare with the output of the ceph osd df output.\nncn-m001# ceph health detail Example output:\nHEALTH_ERR 1 nearfull osd(s); 13 pool(s) nearfull; Degraded data redundancy (low space): 3 pgs backfill_toofull OSD_NEARFULL 1 nearfull osd(s) osd.9 is near full \u0026lt;\u0026lt;-- Note this value View the storage utilization of the cluster and pools.\nncn-m001# ceph df Example output:\nRAW STORAGE: CLASS SIZE AVAIL USED RAW USED %RAW USED ssd 56 TiB 24 TiB 32 TiB 32 TiB 57.15 TOTAL 56 TiB 24 TiB 32 TiB 32 TiB 57.15 POOLS: POOL ID STORED OBJECTS USED %USED MAX AVAIL cephfs_data 1 39 MiB 387 121 MiB 0.10 39 GiB cephfs_metadata 2 257 MiB 123 770 MiB 0.64 39 GiB .rgw.root 3 3.7 KiB 8 400 KiB 0 39 GiB defaults.rgw.buckets.data 4 0 B 0 0 B 0 39 GiB default.rgw.control 5 0 B 8 0 B 0 39 GiB defaults.rgw.buckets.index 6 0 B 0 0 B 0 39 GiB default.rgw.meta 7 22 KiB 114 4.4 MiB 0 39 GiB default.rgw.log 8 0 B 207 0 B 0 39 GiB kube 9 220 GiB 61.88k 661 GiB 84.93 39 GiB smf 10 10 TiB 2.86M 31 TiB 99.63 39 GiB default.rgw.buckets.index 11 5.9 MiB 14 5.9 MiB 0 39 GiB default.rgw.buckets.data 12 145 GiB 48.11k 436 GiB 78.81 39 GiB default.rgw.buckets.non-ec 13 305 KiB 34 1.9 MiB 0 39 GiB View the utilization of the OSDs to see if data is not balanced across them.\nIn the example below, the OSD.9 value is showing that it is 95.17 percent full.\nncn-m001# ceph osd df Example output:\nID CLASS WEIGHT REWEIGHT SIZE RAW USE DATA OMAP META AVAIL %USE VAR PGS STATUS 1 ssd 3.49219 1.00000 3.5 TiB 2.1 TiB 2.1 TiB 6.3 MiB 3.9 GiB 1.4 TiB 60.81 1.06 57 up 4 ssd 3.49219 1.00000 3.5 TiB 2.0 TiB 2.0 TiB 133 KiB 3.7 GiB 1.5 TiB 57.58 1.01 56 up 5 ssd 3.49219 1.00000 3.5 TiB 2.1 TiB 2.1 TiB 195 KiB 3.5 GiB 1.4 TiB 61.33 1.07 49 up 6 ssd 3.49219 1.00000 3.5 TiB 1.2 TiB 1.2 TiB 321 KiB 2.4 GiB 2.3 TiB 33.90 0.59 40 up 7 ssd 3.49219 1.00000 3.5 TiB 1.5 TiB 1.5 TiB 1012 KiB 2.9 GiB 2.0 TiB 43.03 0.75 39 up 8 ssd 3.49219 1.00000 3.5 TiB 1.7 TiB 1.7 TiB 194 KiB 4.0 GiB 1.8 TiB 47.96 0.84 47 up 0 ssd 3.49219 1.00000 3.5 TiB 2.8 TiB 2.8 TiB 485 KiB 5.2 GiB 696 GiB 80.53 1.41 75 up **9 ssd 3.49219 1.00000 3.5 TiB 3.3 TiB 3.3 TiB 642 KiB 6.1 GiB 173 GiB 95.17 1.67 67 up** 10 ssd 3.49219 1.00000 3.5 TiB 1.7 TiB 1.7 TiB 6.7 MiB 3.1 GiB 1.8 TiB 47.74 0.84 68 up 11 ssd 3.49219 1.00000 3.5 TiB 2.8 TiB 2.8 TiB 1.1 MiB 5.4 GiB 675 GiB 81.14 1.42 78 up 2 ssd 3.49219 1.00000 3.5 TiB 2.2 TiB 2.2 TiB 27 KiB 4.0 GiB 1.3 TiB 62.14 1.09 40 up 3 ssd 3.49219 1.00000 3.5 TiB 2.3 TiB 2.3 TiB 445 KiB 4.4 GiB 1.2 TiB 65.90 1.15 55 up 12 ssd 3.49219 1.00000 3.5 TiB 541 GiB 540 GiB 1006 KiB 1.3 GiB 3.0 TiB 15.14 0.27 48 up 13 ssd 3.49219 1.00000 3.5 TiB 2.6 TiB 2.6 TiB 176 KiB 4.9 GiB 895 GiB 74.96 1.31 56 up 14 ssd 3.49219 1.00000 3.5 TiB 1.8 TiB 1.8 TiB 6.4 MiB 3.3 GiB 1.7 TiB 52.03 0.91 48 up 15 ssd 3.49219 1.00000 3.5 TiB 1.2 TiB 1.2 TiB 179 KiB 2.5 GiB 2.3 TiB 34.44 0.60 41 up Use the ceph osd reweight command on the OSD to move data from the drive and get it back under the warning threshold of 85 percent.\nThis command tells Ceph that the drive can now only hold 80 percent of the usable space (CRUSH weight).\nncn-m001# ceph osd reweight osd.9 0.80 Confirm the reweight command made the change.\nIn this example, the new reweight is .79999 and the use is now at 80 percent.\nncn-m001# ceph osd df Example output:\nID CLASS WEIGHT REWEIGHT SIZE RAW USE DATA OMAP META AVAIL %USE VAR PGS STATUS 1 ssd 3.49219 1.00000 3.5 TiB 2.1 TiB 2.1 TiB 7.1 MiB 4.7 GiB 1.4 TiB 60.91 1.07 57 up 4 ssd 3.49219 1.00000 3.5 TiB 2.0 TiB 2.0 TiB 137 KiB 3.7 GiB 1.5 TiB 57.65 1.01 56 up 5 ssd 3.49219 1.00000 3.5 TiB 2.1 TiB 2.1 TiB 207 KiB 4.0 GiB 1.3 TiB 61.42 1.07 49 up 6 ssd 3.49219 1.00000 3.5 TiB 1.2 TiB 1.2 TiB 293 KiB 2.4 GiB 2.3 TiB 33.94 0.59 40 up 7 ssd 3.49219 1.00000 3.5 TiB 1.5 TiB 1.5 TiB 1012 KiB 2.9 GiB 2.0 TiB 43.08 0.75 39 up 8 ssd 3.49219 1.00000 3.5 TiB 1.7 TiB 1.7 TiB 198 KiB 3.1 GiB 1.8 TiB 48.00 0.84 47 up 0 ssd 3.49219 1.00000 3.5 TiB 3.0 TiB 3.0 TiB 497 KiB 5.5 GiB 522 GiB 85.40 1.49 80 up **9 ssd 3.49219 0.79999 3.5 TiB 2.8 TiB 2.8 TiB 650 KiB 6.1 GiB 687 GiB 80.80 1.41 51 up** 10 ssd 3.49219 1.00000 3.5 TiB 2.0 TiB 2.0 TiB 7.2 MiB 3.6 GiB 1.5 TiB 57.35 1.00 75 up 11 ssd 3.49219 1.00000 3.5 TiB 2.8 TiB 2.8 TiB 1.1 MiB 5.3 GiB 664 GiB 81.43 1.42 82 up 2 ssd 3.49219 1.00000 3.5 TiB 2.2 TiB 2.2 TiB 31 KiB 4.0 GiB 1.3 TiB 62.22 1.09 40 up 3 ssd 3.49219 1.00000 3.5 TiB 2.3 TiB 2.3 TiB 457 KiB 4.2 GiB 1.2 TiB 65.98 1.15 55 up 12 ssd 3.49219 1.00000 3.5 TiB 542 GiB 541 GiB 990 KiB 1.3 GiB 3.0 TiB 15.16 0.27 48 up 13 ssd 3.49219 1.00000 3.5 TiB 2.6 TiB 2.6 TiB 196 KiB 4.9 GiB 892 GiB 75.05 1.31 56 up 14 ssd 3.49219 1.00000 3.5 TiB 1.8 TiB 1.8 TiB 7.1 MiB 3.3 GiB 1.7 TiB 52.10 0.91 48 up 15 ssd 3.49219 1.00000 3.5 TiB 1.2 TiB 1.2 TiB 171 KiB 2.7 GiB 2.3 TiB 34.48 0.60 41 up TOTAL 56 TiB 32 TiB 32 TiB 27 MiB 62 GiB 24 TiB 57.19 MIN/MAX VAR: 0.27/1.49 STDDEV: 18.51 Monitor the Ceph cluster during recovery.\nncn-m001# ceph -s "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/create_internal_groups_in_the_keycloak_shasta_realm/",
	"title": "Create Internal Groups in the Keycloak Shasta Realm",
	"tags": [],
	"description": "",
	"content": "Create Internal Groups in the Keycloak Shasta Realm Manually create a group in the Keycloak Shasta realm. New groups can be created with the Keycloak UI. In CSM, Keycloak groups must have the cn and gidNumber attributes, otherwise the keycloak-users-localize tool will fail to export the groups.\nNew Keycloak groups can be used to group users for authentication.\nPrerequisites This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process. The password can be obtained with the following command:\nncn-mw# kubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode Procedure Log in to the administration console.\nSee Access the Keycloak User Management UI for more information.\nClick the Groups text in the Manage section in the navigation area on the left side of the screen.\nClick the New button in the groups table header.\nProvide a unique name for the new group and click the Save button.\nNavigate to the Attributes tab.\nAdd the cn attribute.\nSet the Key to cn.\nSet the Value to the name of the group.\nClick the Add button on the row.\nAdd the gidNumber attribute.\nSet the Key to gidNumber.\nSet the Value to the gidNumber of the group.\nClick the Add button on the row.\nClick the Save button at the bottom of the page.\nOnce the groups are added to Keycloak, add users to the groups and follow the instructions in Re-Sync Keycloak Users to Compute Nodes in order to update the groups on the compute nodes.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/use_the_physical_kvm/",
	"title": "Use the Physical KVM",
	"tags": [],
	"description": "",
	"content": "Use the Physical KVM For those who prefer to stand in front of the system and use a physically connected keyboard, mouse, and monitor, Cray provides a rack-mount-extendable KVM unit installed in rack unit slot 23 (RU23) of the management cabinet. It is connected to the first non-compute node (NCN) by default.\nTo use it, pull it out and raise the lid.\nTo bring up the main menu (shown in following figure), press Prnt Scrn.\nEach node in the system (except ClusterStor) appears in the main menu associated with a port. The first NCN is port 01, the other three NCNs are ports 02–04, and the compute nodes are 05–08.\nTo move to any node in the system, use the arrow keys and press Enter. The login screen for that node will appear.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/check_and_set_the_metalno-wipe_setting_on_ncns/",
	"title": "Check and Set the metal.no-wipe Setting on NCNs",
	"tags": [],
	"description": "",
	"content": "Check and Set the metal.no-wipe Setting on NCNs Configure the metal.no-wipe setting on non-compute nodes (NCNs) to preserve data on the nodes before doing an NCN reboot.\nRun the ncnGetXnames.shscript to view the metal.no-wipe settings for each NCN. The component name (xname) and metal.no-wipe settings are also dumped out when executing the /opt/cray/platform-utils/ncnHealthChecks.sh script.\nPrerequisites This procedure requires administrative privileges.\nProcedure Run the ncnGetXnames.sh script.\nThe output will end with a listing of all of the NCNs, their component names (xnames), and what the metal.no-wipe setting is for each.\nncn# /opt/cray/platform-utils/ncnGetXnames.sh Example of the NCN listing at the end of the output:\nncn-m001: x3000c0s1b0n0 - metal.no-wipe=1 ncn-m002: x3000c0s2b0n0 - metal.no-wipe=1 ncn-m003: x3000c0s3b0n0 - metal.no-wipe=1 ncn-w001: x3000c0s4b0n0 - metal.no-wipe=1 ncn-w002: x3000c0s5b0n0 - metal.no-wipe=1 ncn-w003: x3000c0s6b0n0 - metal.no-wipe=1 ncn-s001: x3000c0s7b0n0 - metal.no-wipe=1 ncn-s002: x3000c0s8b0n0 - metal.no-wipe=1 ncn-s003: x3000c0s9b0n0 - metal.no-wipe=1 The metal.no-wipe setting must be set to 1 (that is, metal.no-wipe=1) if doing a reboot of an NCN, in order to preserve the current data on it. If it is not set to 1 when the NCN is rebooted, then the NCN will be completely wiped and will subsequently have to be rebuilt. If the metal.no-wipe status for one or more NCNs is not returned, then re-run the ncnGetXnames.sh script.\nReset the metal.no-wipe settings for any NCN where it is set to 0.\nThis step can be skipped if the metal.no-wipe is already set to 1 for any NCNs being rebooted.\nGenerate an API token and export it.\nncn# export TOKEN=$(curl -k -s -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; \\ | base64 -d` https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Update the metal.no-wipe settings.\nncn# /tmp/csi handoff bss-update-param --set metal.no-wipe=1 Run the ncnGetXnames.sh script again to verify the metal.no-wipe settings have been updated as expected.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/lag/",
	"title": "Link aggregation group (LAG)",
	"tags": [],
	"description": "",
	"content": "Link aggregation group (LAG) Link Aggregation allows you to assign multiple physical links to one logical link that functions as a single, higher-speed link providing dramatically increased bandwidth.\nRelevant Configuration\nCreate and configure the LAG interface\nswitch (config) # interface port-channel 1 switch (config interface port-channel 1) # Exit port-channel context\nswitch (config interface port-channel 1) # exit switch (config) # Associate member links with the LAG interface switch(config)# interface IFACE\nswitch (config interface ethernet 1/4) # channel-group 1 mode on switch (config interface ethernet 1/4) # To enable LACP in LAG\nswitch (config interfaces ethernet 1/7)# lacp rate fast Show Commands to Validate Functionality\nswitch# show interface port-channel Expected Results\nStep 1: You can create and configure a LAG Step 2: You can add ports to a LAG Step 3: You can configure a LAG interface Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/ssh/",
	"title": "Configure Secure Shell (SSH)",
	"tags": [],
	"description": "",
	"content": "Configure Secure Shell (SSH) SSH server enables an SSH client to make a secure and encrypted connection to a switch. Currently, switches support SSH version 2.0 only. The user authentication mechanisms supported for SSH are public key authentication and password authentication (RADIUS, TACACS+, or locally stored password). Secure File Transfer Protocol (SFTP) provides file transfer. SSH Server and sftp-client via the copy command are supported for managing the router.\nConfiguration Commands The SSH server is enabled by default.\nTo disable the SSH server:\n# no ip ssh server enable. Expected Results Administrators can create the user account Administrators can generate working SSH keys The output of the show commands is correct Administrators can successfully connect to the switch via an SSH client using SSH 2.0 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/",
	"title": "Aruba Installation and Configuration Guide",
	"tags": [],
	"description": "",
	"content": "Aruba Installation and Configuration Guide This documentation helps network administrators and support personnel install and manage Aruba network devices in a CSM install.\nThe HPE Cray recommended way of configuring the network is by using the CANU tool. Therefore this guide will not go into detail on how to configure each switch manually using the CLI. Instead, it will give helpful examples of how to configure/use features generated by CANU, in order to provide administrators easy ways to customize their installation.\nAlso included in this guide are the current documented and supported network scenarios.\nNOTE: Not every configuration option is covered here; for any configuration outside of the scope of this document, refer to the official Aruba user manual. See Aruba Networks.\nThis document is intended for network administrators and support personnel.\nNOTE: The display and command lines illustrated in this document are examples and might not exactly match any particular environment. The switch and accessory drawings in this document are for illustration only, and may not exactly match installed products.\nCANU See CSM Automatic Network Utility (CANU)\nExamples of Network Topologies Very Large Large Medium Small Network Design Explained What is Spine-Leaf Architecture? How Does a Spine-Leaf Architecture Differ From Traditional Network Designs? Why are Spine-Leaf Architectures Becoming More Popular? What is VSX? What are the benefits of VSX? VSX summary Built in High Availability VSX: MCLAG link HA VSX: ISL HA VSX: Split VSX: Member Power Failure Management Network Overview Network Types – Naming and Segment Function Network Traffic Pattern System Management Network Functions Key Features Used in the Management Network Configuration Key Feature List Typical Configuration of VSX Typical Configuration of MCLAG Link Typical Edge Port Configuration How to Connect Management Network to a Campus Network Connect the Management Network to a Campus Network Scenario A: Network Connection via Management Network Scenario B: Network Connection via High-Speed Network Example of How to Configure Scenario A or B Manage Switches from CLI Device Management Management Interface Network Time Protocol (NTP) Client Domain Name System (DNS) Client Message-Of-The-Day (MOTD) Exec Banners Hostname Domain Name Secure Shell (SSH) Remote Logging Web User Interface (Web UI) Simple Network Management Protocol (SNMP) Agent SNMPv2c Community SNMP Traps SNMPv3 Users Bluetooth Capabilities Layer One Features Physical Interfaces Redundant Power Supplies Locator LED Cable Diagnostics Layer Two Features Unidirectional Link Detection (UDLD) Link Layer Discovery Protocol (LLDP) Virtual Local Access Networks (VLANs) Native VLAN VLAN Trunking 802.1Q Link Aggregation Group (LAG) Virtual Switching Extension (VSX) Multi-Chassis Link Aggregation Group (MCLAG) VSX Sync Virtual Switching Framework (VSF) 6300 Only Multiple Spanning Tree Protocol (MSTP) Layer Three Features Routed Interfaces VLAN Interface Address Resolution Protocol (ARP) Static Routing Loopback Interface Open Shortest Path First (OSPF) v2 BGP Basics Multicast IGMP MSDP PIM-SM Bootstrap Router (BSR) and Rendezvous-Point (RP) Security Access Control Lists (ACLs) TACACS RADIUS Port Security 802.1X MAC Authentication Quality of Service Initial Prioritization) Classifier Policies Queuing and Scheduling Perform a VSX Upgrade on Aruba Switches VSX Upgrade Switch Replacement in the VSX Cluster Switch replacement in the VSX Cluster Back Up Switch Configuration Backing up switch configuration Erase All zeroize Erase all zeroize Troubleshooting Port Mirroring on Aruba CX Port Mirroring DHCP Confirm the status of the cray-dhcp-kea pods/services Check current DHCP leases Check HSM Check Kea DHPC logs TCPDUMP Check BGP and MetalLB Getting incorrect IP address. Duplicate IP address check Large number of DHCP declines during a node boot DNS PXE Boot NCNs on install Rebooting NCN and PXE fails Verify BGP Verify route to TFTP Test TFTP traffic (Aruba Only) Check DHCP lease is getting allocated Verify DHCP traffic on workers Verify switches are forwarding DHCP traffic Computes/UANs/Application Nodes "
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/rebalance_healthy_etcd_clusters/",
	"title": "Rebalance Healthy etcd Clusters",
	"tags": [],
	"description": "",
	"content": "Rebalance Healthy etcd Clusters Rebalance the etcd clusters. The clusters need to be in a healthy state, and there needs to be the same number of pods running on each worker node for the etcd clusters to be balanced.\nRestoring the balance of etcd clusters will help with the storage of Kubernetes cluster data.\nPrerequisites etcd clusters are in a healthy state. etcd clusters do not have the same number of pods on each worker node. Procedure Check to see if clusters have two or more pods on the same worker node.\nThe following is an example of an unhealthy cluster. Two of the pods are on ncn-w001 and only one pod is on ncn-w003.\nncn-w001# kubectl get pods -o wide -A -l app=etcd Example output:\nNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES services cray-bos-etcd-cqjr66ldlr 1/1 Running 0 5d10h 10.39.1.55 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services cray-bos-etcd-hsb2zfzxqv 1/1 Running 0 5d10h 10.36.0.13 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services cray-bos-etcd-v9sfkxcpzc 1/1 Running 0 3d 10.39.2.58 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Confirm that the clusters are healthy.\nRefer to Check the Health and Balance of etcd Clusters.\nDelete one of the pods that is on the same node as another in the cluster.\nncn-w001# kubectl -A delete pod POD_NAME Check the health of the pods.\nRefer to Check the Health and Balance of etcd Clusters.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/ncn_worker_image_customization/",
	"title": "NCN Worker Image Customization",
	"tags": [],
	"description": "",
	"content": "NCN Worker Image Customization NOTE: Some of the documentation linked from this page mentions use of the Boot Orchestration Service (BOS). The use of BOS is only relevant for booting compute nodes and can be ignored when working with NCN images.\nThis document describes the configuration of a Kubernetes NCN image. The same steps could be used to modify a Ceph image.\nIdentify the NCN image to be modified.\nThis example assumes that the administrator wants to modify the Kubernetes image that is currently in use by Kubernetes NCNs. However, the steps are the same for any Management NCN SquashFS image.\nIf the image to be modified is the image currently booted on an NCN, the value for ARTIFACT_VERSION can be found by looking at the boot parameters for the NCNs, or from /proc/cmdline on a booted NCN. The version has the form of X.Y.Z.\nObtain the NCN image\u0026rsquo;s associated artifacts (SquashFS, kernel, and initrd).\nThese example commands show how to download these artifacts from S3, which is where the NCN image artifacts are stored.\nncn-mw# ARTIFACT_VERSION=\u0026lt;artifact-version\u0026gt; ncn-mw# cray artifacts get ncn-images k8s/\u0026#34;${ARTIFACT_VERSION}\u0026#34;/filesystem.squashfs ./\u0026#34;${ARTIFACT_VERSION}\u0026#34;-filesystem.squashfs ncn-mw# cray artifacts get ncn-images k8s/\u0026#34;${ARTIFACT_VERSION}\u0026#34;/kernel ./\u0026#34;${ARTIFACT_VERSION}\u0026#34;-kernel ncn-mw# cray artifacts get ncn-images k8s/\u0026#34;${ARTIFACT_VERSION}\u0026#34;/initrd ./\u0026#34;${ARTIFACT_VERSION}\u0026#34;-initrd ncn-mw# export IMS_ROOTFS_FILENAME=\u0026#34;${ARTIFACT_VERSION}\u0026#34;-filesystem.squashfs ncn-mw# export IMS_KERNEL_FILENAME=\u0026#34;${ARTIFACT_VERSION}\u0026#34;-kernel ncn-mw# export IMS_INITRD_FILENAME=\u0026#34;${ARTIFACT_VERSION}\u0026#34;-initrd Import the NCN image into IMS.\nPerform the Import External Image to IMS procedure, except skip the following sections:\nSet helper variables Skip this section because the variables have already been set above, in the previous step. Upload artifacts to S3 Skip this section because the artifacts are already in S3. Clone the csm-config-management repository.\nncn-mw# VCS_USER=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_username}} | base64 --decode) ncn-mw# VCS_PASS=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode) ncn-mw# git clone \u0026#34;https://${VCS_USER}:${VCS_PASS}@api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34; A Git commit hash from this repository is needed in the following step.\nCreate a CFS Configuration.\nNOTE: A new initrd must be generated at the end of the CFS session. Create a new Ansible playbook named ncn-initrd.yml at the root of the csm-config-management repository. The new playbook should have the following contents:\n- hosts: Management_Worker any_errors_fatal: true remote_user: root tasks: - name: Create NCN initrd command: /srv/cray/scripts/common/create-ims-initrd.sh when: cray_cfs_image|default(false)|bool This playbook should be the final layer in the CFS configuration created in the upcoming steps.\nNOTE: If desired, the default SSH host keys can be removed with a CFS Ansible task:\n- name: Remove SSH host keys command: rm -f /etc/ssh/*key* warn: false New host keys will be generated by sshd when the NCN boots for the first time.\nCreate an Image Customization CFS Session.\nIn this section, use the following values for the target definition and target group for the cray cfs session create command invocation:\n--target-definition image --target-group Management_Worker Download the resultant NCN artifacts.\nNOTE: $IMS_RESULTANT_IMAGE_ID in the following commands is the result_id returned in the output of the last command in the previous step, which is repeated here for reference:\nncn-mw# cray cfs sessions describe example --format json | jq .status.artifacts Download the resultant NCN artifacts:\nncn-mw# cray artifacts get boot-images \u0026#34;${IMS_RESULTANT_IMAGE_ID}/rootfs\u0026#34; \u0026#34;kubernetes-${ARTIFACT_VERSION}-1.squashfs\u0026#34; ncn-mw# cray artifacts get boot-images \u0026#34;${IMS_RESULTANT_IMAGE_ID}/initrd\u0026#34; \u0026#34;initrd.img-${ARTIFACT_VERSION}-1.xz\u0026#34; ncn-mw# cray artifacts get boot-images \u0026#34;${IMS_RESULTANT_IMAGE_ID}/kernel\u0026#34; \u0026#34;${ARTIFACT_VERSION}-1.kernel\u0026#34; Upload NCN boot artifacts into S3.\nThis steps assumes that the docs-csm RPM is installed. See Check for latest documentation.\nncn-mw# /usr/share/doc/csm/scripts/ceph-upload-file-public-read.py --bucket-name ncn-images --key-name \u0026#34;k8s/${ARTIFACT_VERSION}-1/filesystem.squashfs\u0026#34; --file-name \u0026#34;kubernetes-${ARTIFACT_VERSION}-1.squashfs\u0026#34; ncn-mw# /usr/share/doc/csm/scripts/ceph-upload-file-public-read.py --bucket-name ncn-images --key-name \u0026#34;k8s/${ARTIFACT_VERSION}-1/initrd\u0026#34; --file-name \u0026#34;initrd.img-${ARTIFACT_VERSION}-1.xz\u0026#34; ncn-mw# /usr/share/doc/csm/scripts/ceph-upload-file-public-read.py --bucket-name ncn-images --key-name \u0026#34;k8s/${ARTIFACT_VERSION}-1/kernel\u0026#34; --file-name \u0026#34;${ARTIFACT_VERSION}-1.kernel\u0026#34; Update NCN boot parameters.\nGet the existing metal.server setting for the component name (xname) of the node of interest.\nncn-mw# XNAME=\u0026lt;node-xname\u0026gt; ncn-mw# METAL_SERVER=$(cray bss bootparameters list --hosts \u0026#34;${XNAME}\u0026#34; --format json | jq \u0026#39;.[] |.\u0026#34;params\u0026#34;\u0026#39; \\ | awk -F \u0026#39;metal.server=\u0026#39; \u0026#39;{print $2}\u0026#39; \\ | awk -F \u0026#39; \u0026#39; \u0026#39;{print $1}\u0026#39;) Verify that the variable was set correctly.\nncn-mw# echo \u0026#34;${METAL_SERVER}\u0026#34; Update the kernel, initrd, and metal server to point to the new artifacts.\nncn-mw# S3_ARTIFACT_PATH=\u0026#34;ncn-images/k8s/${ARTIFACT_VERSION}-1\u0026#34; ncn-mw# NEW_METAL_SERVER=\u0026#34;http://rgw-vip.nmn/${S3_ARTIFACT_PATH}\u0026#34; ncn-mw# PARAMS=$(cray bss bootparameters list --hosts \u0026#34;${XNAME}\u0026#34; --format json | jq \u0026#39;.[] |.\u0026#34;params\u0026#34;\u0026#39; | \\ sed \u0026#34;/metal.server/ s|${METAL_SERVER}|${NEW_METAL_SERVER}|\u0026#34; | \\ sed \u0026#34;s/metal.no-wipe=1/metal.no-wipe=0/\u0026#34; | \\ tr -d \\\u0026#34;) Verify that the value of $NEW_METAL_SERVER was set correctly within the boot parameters.\nncn-mw# echo \u0026#34;${PARAMS}\u0026#34; Update the boot parameters in BSS.\nncn-mw# cray bss bootparameters update --hosts \u0026#34;${XNAME}\u0026#34; \\ --kernel \u0026#34;s3://${S3_ARTIFACT_PATH}/kernel\u0026#34; \\ --initrd \u0026#34;s3://${S3_ARTIFACT_PATH}/initrd\u0026#34; \\ --params \u0026#34;${PARAMS}\u0026#34; Prepare for reboot.\nNOTE: If the worker node image is being customized as part of a Cray EX initial install or upgrade involving multiple products, then refer to the HPE Cray EX System Software Getting Started Guide (S-8000) for details on when to reboot the worker nodes to the new image.\nFailover any Postgres leader that is running on the worker node being rebooted.\nncn-mw# /usr/share/doc/csm/upgrade/1.2/scripts/k8s/failover-leader.sh \u0026lt;node to be rebooted\u0026gt; Cordon and drain the node.\nncn-mw# kubectl drain --ignore-daemonsets=true --delete-local-data=true \u0026lt;node to be rebooted\u0026gt; There may be pods that cannot be gracefully evicted because of Pod Disruption Budgets (PDB). This will result in messages like the following:\nerror when evicting pod \u0026#34;\u0026lt;pod\u0026gt;\u0026#34; (will retry after 5s): Cannot evict pod as it would violate the pod\u0026#39;s disruption budget. In this case, there are some options. First, if the service is scalable, then increase the scale to start up another pod on another node, and then the drain will be able to delete it. However, it will probably be necessary to force the deletion of the pod:\nncn-mw# kubectl delete pod [-n \u0026lt;namespace\u0026gt;] --force --grace-period=0 \u0026lt;pod\u0026gt; This will delete the offending pod, and Kubernetes should schedule a replacement on another node. Then rerun the kubectl drain command, and it should report that the node is drained.\nncn-mw# kubectl drain --ignore-daemonsets=true --delete-local-data=true \u0026lt;node to be rebooted\u0026gt; SSH to the node and wipe the disks.\nncn-w# vgremove -f --select \u0026#39;vg_name=~ceph*\u0026#39; ncn-w# vgremove -f --select \u0026#39;vg_name=~metal*\u0026#39; ncn-w# sgdisk --zap-all /dev/sd* ncn-w# wipefs --all --force /dev/sd* /dev/disk/by-label/* Reboot the NCN.\nncn-w# shutdown -r now IMPORTANT: If the node does not shut down after 5 minutes, then proceed with the power reset below.\nPower off the node.\nread -s is used to prevent the password from being written to the screen or the shell history.\nIn the example commands below, be sure to replace \u0026lt;node\u0026gt; with the name of the node being rebooted. For example, ncn-w002.\nncn-mw# USERNAME=root ncn-mw# read -r -s -p \u0026#34;NCN BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn-mw# export IPMI_PASSWORD ncn-mw# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power off ncn-mw# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power status Ensure that the power is reporting as off. It may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nPower on the node.\nIn the example commands below, be sure to replace \u0026lt;node\u0026gt; with the name of the node being rebooted. For example, ncn-w002.\nncn-mw# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power on ncn-mw# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power status Ensure that the power is reporting as on. It may take 5-10 seconds for this to update.\nSet metal.no-wipe=1.\nncn-mw# PARAMS=$(cray bss bootparameters list --hosts \u0026#34;${XNAME}\u0026#34; --format json | jq \u0026#39;.[] |.\u0026#34;params\u0026#34;\u0026#39; | \\ sed \u0026#34;s/metal.no-wipe=0/metal.no-wipe=1/\u0026#34; | \\ tr -d \\\u0026#34;) ncn-mw# cray bss bootparameters update --hosts \u0026#34;${XNAME}\u0026#34; \\ --params \u0026#34;${PARAMS}\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/manage_a_session_template/",
	"title": "Manage a Session Template",
	"tags": [],
	"description": "",
	"content": "Manage a Session Template A session template must be created before starting a session with the Boot Orchestration Service (BOS).\nThis page shows Cray CLI commands for managing BOS session templates. To find the API versions of any commands listed, add -vvv to the end of the CLI command, and the CLI will print the underlying call to the API in the output.\nSession template framework Create a session template [Create with the Cray CLI](#create with-the-cray-cli) Create with a Bash script Resulting template List all session templates View a session template Delete a session template Session template framework When creating a new BOS session template, it can be helpful to start with a framework and then edit it as needed. Use the following command to retrieve the BOS session template framework:\nncn-mw# cray bos sessiontemplatetemplate list --format json Example output:\n{ \u0026#34;boot_sets\u0026#34;: { \u0026#34;name_your_boot_set\u0026#34;: { \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;etag\u0026#34;: \u0026#34;your_boot_image_etag\u0026#34;, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;your-kernel-parameters\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;xname1\u0026#34;, \u0026#34;xname2\u0026#34;, \u0026#34;xname3\u0026#34; ], \u0026#34;path\u0026#34;: \u0026#34;your-boot-path\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;your-rootfs-provider\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;your-rootfs-provider-passthrough\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;your-boot-type\u0026#34; } }, \u0026#34;cfs\u0026#34;: { \u0026#34;configuration\u0026#34;: \u0026#34;desired-cfs-config\u0026#34; }, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;name-your-template\u0026#34; } Create a session template Create with the Cray CLI The following command takes a JSON input file that contains the information required to create a new BOS session template. It reads it in and creates a BOS session template using the BOS API.\nncn-mw# cray bos sessiontemplate create --file INPUT_FILE --name NEW_TEMPLATE_NAME The following is an example of an input file:\n{ \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;cle-1.2.0\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;boot_set1\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gwservice-nmn.local quiet turbo_boost_limit=999\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x3000c0s19b1n0\u0026#34; ], \u0026#34;etag\u0026#34;: \u0026#34;90b2466ae8081c9a604fd6121f4c08b7\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/06901f40-f2a6-4a64-bc26-772a5cc9d321/manifest.json\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;partition\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cfs_branch\u0026#34;: \u0026#34;master\u0026#34; } Create with a Bash script A BOS session template can also be generated with a shell script, which directly uses the BOS API. The following is an example script for creating a session template. The get_token function retrieves a token that validates the request to the API gateway. The values in the body section of the script can be customized when creating a new session template.\n#!/bin/bash # Up to date as of 2020-02-05 ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d) TOKEN=$(curl -s -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=$ADMIN_SECRET \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | python -c \u0026#39;import sys, json; print json.load(sys.stdin)[\u0026#34;access_token\u0026#34;]\u0026#39;) kernel_parameters=\u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g \\ intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless \\ numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y \\ rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gw-service-nmn.local quiet turbo_boost_limit=999\u0026#34; body=\u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;cle-1.2.0\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;boot_set1\u0026#34;: { \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/06901f40-f2a6-4a64-bc26-772a5cc9d321/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34;, \u0026#34;etag\u0026#34;: \u0026#34;90b2466ae8081c9a604fd6121f4c08b7\u0026#34;, \u0026#34;node_list\u0026#34;: [\u0026#34;x3000c0s19b1n0\u0026#34;], \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$kernel_parameters\u0026#34;\u0026#39;\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34; }}, \u0026#34;cfs_branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;partition\u0026#34;: \u0026#34;\u0026#34; }\u0026#39; curl -i -X POST -s https://api-gw-service-nmn.local/apis/bos/v1/sessiontemplate \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;$body\u0026#34; Resulting template Either option will generate the following session template:\n{ \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;cle-1.2.0\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;boot_set1\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchr \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x3000c0s19b1n0\u0026#34; ], \u0026#34;etag\u0026#34;: \u0026#34;90b2466ae8081c9a604fd6121f4c08b7\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/06901f40-f2a6-4a64-bc26-772a5cc9d321/manifest.json\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;partition\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cfs_branch\u0026#34;: \u0026#34;master\u0026#34; } List all session templates List all session templates:\nncn-mw# cray bos sessiontemplate list --format json Example output:\n[ { \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;description\u0026#34;: \u0026#34;Template for booting compute nodes, generated by the installation\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;computes\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gw-service-nmn.local quiet turbo_boost_limit=999\u0026#34;, \u0026#34;node_roles_groups\u0026#34;: [ \u0026#34;Compute\u0026#34; ], \u0026#34;etag\u0026#34;: \u0026#34;b0ace28163302e18b68cf04dd64f2e01\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/ef97d3c4-6f10-4d58-b4aa-7b70fcaf41ba/manifest.json\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;name\u0026#34;: \u0026#34;cle-1.2.0\u0026#34;, \u0026#34;cfs_branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34; } ] View a session template ncn-mw# cray bos sessiontemplate describe \u0026lt;SESSION_TEMPLATE_NAME\u0026gt; --format json Example output:\n{ \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;description\u0026#34;: \u0026#34;Template for booting compute nodes, generated by the installation\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;computes\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gw-service-nmn.local quiet turbo_boost_limit=999\u0026#34;, \u0026#34;node_roles_groups\u0026#34;: [ \u0026#34;Compute\u0026#34; ], \u0026#34;etag\u0026#34;: \u0026#34;b0ace28163302e18b68cf04dd64f2e01\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/ef97d3c4-6f10-4d58-b4aa-7b70fcaf41ba/manifest.json\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;cfs_branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cle-1.2.0\u0026#34; } Delete a session template Remove an existing session template:\nncn-mw# cray bos sessiontemplate delete \u0026lt;SESSION_TEMPLATE_NAME\u0026gt; "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/elements_of_a_uai/",
	"title": "Elements of a UAI",
	"tags": [],
	"description": "",
	"content": "Elements of a UAI All UAIs can have the following attributes associated with them:\nA required container image An optional set of volumes An optional resource specification An optional collection of other configuration items This topic explains each of these attributes.\nUAI container image The container image for a UAI (UAI image) defines and provides the basic environment available to the user. This environment includes, among other things:\nThe operating system (including version) Pre-installed packages A site can customize UAI images and add those images to UAS, allowing them to be used for UAI creation. Any number of UAI images can be configured in UAS, though only one will be used by any given UAI.\nUAS provides two UAI images by default. These images enable HPE Cray EX administrators to set up UAIs and run many common tasks. The first image is a standard End-User UAI image that has the software necessary to support a basic Linux login experience. This is primarily intended to give administrators a way to get started with UAIs and experiment with their configuration. The second image is a Broker UAI image. Broker UAIs present a single SSH endpoint that every user of a given class of UAIs logs into. The Broker UAI then locates or creates a suitable End-User UAI and redirects the SSH session to that End-User UAI.\nUAI Volumes The volumes defined for a UAI provide external access to data provided by the host node. Anything that can be defined as a volume in a Kubernetes pod specification can be configured in UAS as a volume and used within a UAI. Examples include:\nKubernetes ConfigMaps and Secrets External file systems used for persistent storage or external data access Host node files and directories When UAIs are created they mount a list of volumes inside their containers to give them access to various data provided either by Kubernetes resources or through Kubernetes by the host node where the UAI runs. Which volumes are in that list depends on how the UAI is created:\nUAIs created without using a UAI class mount all volumes configured in UAS UAIs created using a UAI Class mount only the volumes listed in the UAI Class and configured in UAS The following are some example use cases for UAI volumes:\nConnecting UAIs to configuration files like /etc/localtime maintained by the host node Connect End-User UAIs to Slurm or PBS Professional Workload Manager configuration shared through Kubernetes Connecting End-User UAIs to Programming Environment libraries and tools hosted on the UAI host nodes Connecting End-User UAIs to Lustre or other external storage for user data Connecting Broker UAIs to a directory service or SSH configuration to authenticate and redirect user sessions Every UAS volume includes the following values in its registration information:\nmount_path: Specifies where in the UAI the volume will be mounted. volume_description: A dictionary with one entry, whose key identifies the kind of Kubernetes volume is described (for example, host_path, configmap, secret). The value associated with that key is another dictionary containing the Kubernetes volume description itself. volumename: A required string chosen by the creator of the volume. This may describe or name the volume. It is used inside the UAI pod specification to identify the volume that is mounted in a given location in a container. A volumename is unique within any given UAI, but not necessarily within UAS. These are useful when searching for a volume if they are unique across the UAS configuration. volume_id: Used to identify the UAS volume when examining, updating, or deleting a volume and when linking a volume to a UAI class. The volume_id is generated by and unique within UAS. Refer to Kubernetes Documentation describing Volumes for more information about Kubernetes volumes.\nResource Specifications A resource request tells Kubernetes the minimum amount of a given host node resource to give to each UAI. A resource limit sets the maximum amount of a given host node resource that Kubernetes can give to any UAI. Kubernetes uses resource limits and requests to manage the system resources available to pods. Because UAIs run as pods under Kubernetes, UAS takes advantage of Kubernetes to manage the system resources available to UAIs. In UAS, resource specifications contain that configuration. A UAI that is assigned a resource specification will use the resource requests and limits found there instead of the default resource limits or requests on the Kubernetes namespace containing the UAI. This way, resource specifications can be used to fine-tune resources assigned to UAIs of different classes.\nUAI resource specifications have three configurable parameters:\nA set of limits which is a JSON string describing Kubernetes resource limits A set of requests which is a JSON string describing Kubernetes resource requests An optional comment which is a free-form string containing any information an administrator might find useful about the resource specification Resource specifications also contain a resource-id that is used for examining, updating, or deleting the resource specification as well as linking the resource specification into a UAI class. Resource-ids are generated by UAS and unique to each resource specification.\nResource specifications configured in UAS contain resource requests, limits, or both, that can be associated with a UAI. Any resource request or limit that can be set up on a Kubernetes pod can be set up as a resource specification under UAS.\nOther Configuration Items There are also smaller configuration items that control things such as:\nWhether the UAI can talk to compute nodes over the high-speed network (needed for workload management) Whether the UAI presents a public facing or private facing IP address for SSH Kubernetes scheduling priority Timeout values for limiting the lifespan of active or idle UAIs These items are configured in UAI Classes so only UAIs created from UAI Classes can have these settings.\nUAI Configuration and UAI Classes All of the above described UAI configuration and more can be encapsulated into a UAI Class, which can then be used to create UAIs with greater precision and efficiency. UAI Classes are especially important when using Broker UAIs because Broker UAIs use a UAI Creation Class configured into the Broker UAI\u0026rsquo;s class to determine what kind of End-User UAI to create when a user logs into the Broker.\nTop: User Access Service (UAS)\nNext Topic: UAI Host Nodes\n"
},
{
	"uri": "/docs-csm/en-12/install/install_csm_services/",
	"title": "Install CSM Services",
	"tags": [],
	"description": "",
	"content": "Install CSM Services This procedure will install CSM applications and services into the CSM Kubernetes cluster.\nNOTE: Check the information in Known issues before starting this procedure to be warned about possible problems.\nInstall CSM services Create base BSS global boot parameters Wait for everything to settle Next topic Known issues Deploy CSM Applications and Services known issues Error releasing chart known issues Setup Nexus known issues 1. Install CSM services NOTE: During this step, only on systems with only three worker nodes (typically Testing and Development Systems (TDS)), the customizations.yaml file will be automatically edited to lower pod CPU requests for some services, in order to better facilitate scheduling on smaller systems. See the file /var/www/ephemeral/${CSM_RELEASE}/tds_cpu_requests.yaml for these settings. This file can be modified with different values (prior to executing the yapl command below), if other settings are desired in the customizations.yaml file for this system. For more information about modifying customizations.yaml and tuning for specific systems, see Post-Install Customizations.\nInstall YAPL.\npit# rpm -Uvh /var/www/ephemeral/${CSM_RELEASE}/rpm/cray/csm/sle-15sp2/x86_64/yapl-*.x86_64.rpm Install CSM services using YAPL.\npit# pushd /usr/share/doc/csm/install/scripts/csm_services \u0026amp;\u0026amp; \\ yapl -f install.yaml execute pit# popd NOTES:\nThis command may take up to 90 minutes to complete. If any errors are encountered, then potential fixes should be displayed where the error occurred. If the installation fails with a missing secret error message, then see CSM Services Install Fails Because of Missing Secret. Output is redirected to /usr/share/doc/csm/install/scripts/csm_services/yapl.log . To show the output in the terminal, append the --console-output execute argument to the yapl command. The yapl command can safely be rerun. By default, it will skip any steps which were previously completed successfully. To force it to rerun all steps regardless of what was previously completed, append the --no-cache argument to the yapl command. The order of the yapl command arguments is important. The syntax is yapl -f install.yaml [--console-output] execute [--no-cache]. 2. Create base BSS global boot parameters Wait for BSS to be ready.\npit# kubectl -n services rollout status deployment cray-bss Retrieve an API token.\npit# export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Create empty boot parameters.\npit# curl -i -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X PUT \\ https://api-gw-service-nmn.local/apis/bss/boot/v1/bootparameters \\ --data \u0026#39;{\u0026#34;hosts\u0026#34;:[\u0026#34;Global\u0026#34;]}\u0026#39; Example of successful output:\nHTTP/2 200 content-type: application/json; charset=UTF-8 date: Mon, 27 Jun 2022 17:08:55 GMT content-length: 0 x-envoy-upstream-service-time: 7 server: istio-envoy Restart the spire-update-bss job.\npit# SPIRE_JOB=$(kubectl -n spire get jobs -l app.kubernetes.io/name=spire-update-bss -o name) pit# kubectl -n spire get \u0026#34;${SPIRE_JOB}\u0026#34; -o json | jq \u0026#39;del(.spec.selector)\u0026#39; \\ | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; \\ | kubectl replace --force -f - Wait for the spire-update-bss job to complete.\npit# kubectl -n spire wait \u0026#34;${SPIRE_JOB}\u0026#34; --for=condition=complete --timeout=5m 3. Wait for everything to settle Wait at least 15 minutes to let the various Kubernetes resources initialize and start before proceeding with the rest of the install. Because there are a number of dependencies between them, some services are not expected to work immediately after the install script completes.\n4. Next topic The next step is to validate CSM health before redeploying the final NCN.\nSee Validate CSM health before final NCN deployment.\nKnown issues Deploy CSM Applications and Services known issues The following error may occur during the Deploy CSM Applications and Services step:\n+ csi upload-sls-file --sls-file /var/www/ephemeral/prep/eniac/sls_input_file.json 2021/10/05 18:42:58 Retrieving S3 credentials ( sls-s3-credentials ) for SLS 2021/10/05 18:42:58 Unable to SLS S3 secret from k8s:secrets \u0026#34;sls-s3-credentials\u0026#34; not found Verify that the sls-s3-credentials secret exists in the default namespace:\npit# kubectl get secret sls-s3-credentials Example output:\nNAME TYPE DATA AGE sls-s3-credentials Opaque 7 28d Check for running sonar-sync jobs. If there are no sonar-sync jobs, then wait for one to complete. The sonar-sync CronJob is responsible for copying the sls-s3-credentials secret from the default namespace to the services namespace.\npit# kubectl -n services get pods -l cronjob-name=sonar-sync Example output:\nNAME READY STATUS RESTARTS AGE sonar-sync-1634322840-4fckz 0/1 Completed 0 73s sonar-sync-1634322900-pnvl6 1/1 Running 0 13s Verify that the sls-s3-credentials secret now exists in the services namespace.\npit# kubectl -n services get secret sls-s3-credentials Example output:\nNAME TYPE DATA AGE sls-s3-credentials Opaque 7 20s Running the yapl command again is expected to succeed.\nError releasing chart known issues Some chart installation errors may occur during the Deploy CSM Applications and Services step:\nExample output (Constraint kind not recognized):\nERR Error releasing chart gatekeeper-constraints v0.5.0: Shell error: Release \u0026#34;gatekeeper-constraints\u0026#34; does not exist. Installing it now. Error: admission webhook \u0026#34;validation.gatekeeper.sh\u0026#34; denied the request: Constraint kind K8sPSPFSGroup is not recognized chart=gatekeeper-constraints command=ship namespace=gatekeeper-system version=0.5.0 Another example output (connection refused):\nERR Error releasing chart cray-metallb v1.1.1: Shell error: Release \u0026#34;cray-metallb\u0026#34; does not exist. Installing it now. Error: Internal error occurred: failed calling webhook \u0026#34;prometheusrulemutate.monitoring.coreos.com\u0026#34;: Post \u0026#34;https://cray-sysmgmt-health-promet-operator.sysmgmt-health.svc:443/admission-prometheusrules/mutate?timeout=30s\u0026#34;: dial tcp 10.17.87.159:443: connect: connection refused chart=cray-metallb command=ship namespace=metallb-system version=1.1.1 As most chart release errors are timing or transitory issues, running the yapl command again (or a few times) is expected to succeed.\nSetup Nexus known issues Known potential issues along with suggested fixes are listed in Troubleshoot Nexus.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/troubleshoot_ceph_services_not_starting/",
	"title": "Troubleshoot Ceph Services Not Starting After a Server Crash",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Ceph Services Not Starting After a Server Crash Issue There is a known issue where the Ceph container images will not start after a power failure or server component failure that causes the server to crash and not boot back up\nThere will be a message similar to the following in the journalctl logs for the Ceph services on the machine that crashed:\nceph daemons will not start due to: Error: readlink /var/lib/containers/storage/overlay/l/CXMD7IEI4LUKBJKX5BPVGZLY3Y: no such file or directory\nWhen the issue materializes, then it is highly likely the Ceph container images have been corrupted.\nFix Remove the corrupted images.\nfor i in $(podman images|grep -v REPO|awk {\u0026#39;print $1\u0026#34;:\u0026#34;$2\u0026#39;}); do podman image rm $i; done Reload the images.\n/srv/cray/scripts/common/pre-load-images.sh Validate that the services are starting.\nncn-s00(1/2/3)# ceph orch ps Example output:\nNAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID alertmanager.ncn-s001 ncn-s001 running (95m) 2m ago 97m 0.20.0 registry.local/prometheus/alertmanager:v0.20.0 0881eb8f169f a3fbad5afe50 crash.ncn-s001 ncn-s001 running (97m) 2m ago 97m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c ddc724e9a18e crash.ncn-s002 ncn-s002 running (97m) 2m ago 97m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 3925895be42d crash.ncn-s003 ncn-s003 running (97m) 2m ago 97m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c b9eb9f3582f7 grafana.ncn-s001 ncn-s001 running (97m) 2m ago 97m 6.6.2 registry.local/ceph/ceph-grafana:6.6.2 a0dce381714a 269fd70c881f mds.cephfs.ncn-s001.dkpjnt ncn-s001 running (95m) 2m ago 95m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 600c4a5513e5 mds.cephfs.ncn-s002.nyirpe ncn-s002 running (95m) 2m ago 95m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 6c9295a5a795 mds.cephfs.ncn-s003.gqxuoc ncn-s003 running (95m) 2m ago 95m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c c92990c970f4 mgr.ncn-s001.lhjrhi ncn-s001 running (98m) 2m ago 98m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c e85dbd963f0d mgr.ncn-s002.hvqjgu ncn-s002 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c a9ba72dfde66 mgr.ncn-s003.zqoych ncn-s003 running (97m) 2m ago 97m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c a33f6f1a265c mon.ncn-s001 ncn-s001 running (98m) 2m ago 99m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 53245f1e60b7 mon.ncn-s002 ncn-s002 running (97m) 2m ago 97m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c cdbda41fc32e mon.ncn-s003 ncn-s003 running (97m) 2m ago 97m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 24578b34f6cd node-exporter.ncn-s001 ncn-s001 running (97m) 2m ago 97m 0.18.1 registry.local/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 79617e2d92ed node-exporter.ncn-s002 ncn-s002 running (97m) 2m ago 97m 0.18.1 registry.local/prometheus/node-exporter:v0.18.1 e5a616e4b9cf d5a93a7ab603 node-exporter.ncn-s003 ncn-s003 running (96m) 2m ago 96m 0.18.1 registry.local/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 8ba07c965a83 osd.0 ncn-s003 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 9dd55acc0475 osd.1 ncn-s001 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 08548417e7ea [...] At this point, the processes are starting/running on the node that crashed; this may take a few minutes.\nIf after five mins the services are still reporting down, then fail-over the ceph mgr daemon and re-check the daemons:\nceph mgr fail $(ceph mgr dump | jq -r .active_name) "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/create_internal_user_accounts_in_the_keycloak_shasta_realm/",
	"title": "Create Internal User Accounts in the Keycloak Shasta Realm",
	"tags": [],
	"description": "",
	"content": "Create Internal User Accounts in the Keycloak Shasta Realm The following manual procedure can be used to create a user in the Keycloak Shasta realm. New accounts can be created with the Keycloak UI.\nNew administrator and user accounts are authenticated with Keycloak. Authenticated accounts are needed to use the Cray CLI.\nPrerequisites This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process. The password can be obtained with the following command:\nncn-mw# kubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode Procedure Log in to the administration console.\nSee Access the Keycloak User Management UI for more information.\nClick the Add User button.\nEnter the user name and other attributes as required.\nClick the Save button.\nIn the Credentials tab, enter a password for the user and change the temporary option from ON to OFF.\nClick the Reset Password button.\nClick the red Change Password button on the Change Password page.\nRemove Update Password from the Required User Actions and on the user Details tab.\nThis step allows the user to authenticate and get a token without first needing to change the administrator-supplied password. It does not prevent the user from changing the password. It is also acceptable to leave this setting, which means a password reset in Keycloak will be required before making a token request with this user account.\nClick the Save button.\nCreate a user and group ID for this user.\nThe User Access Service (UAS) requires these attributes. In the Attributes tab, performing the following steps for both the uid and gid attributes:\nAdd the attribute name to the Key column and its value to the Value column.\nClick the Add button.\nClick the Save button at the bottom once both the uid and gid attributes have been added.\nOptionally add other attributes.\nOther attributes can be added as needed by site-specific applications.\nUser accounts need the following attributes defined in order to create a User Access Instance (UAI):\ngidNumber homeDirectory loginShell uidNumber Click on the Role Mappings tab to grant the user authority.\nClick the Client Roles button.\nSelect Shasta.\nSet the assigned role to either admin or user.\nVerify that the user account has been created in the Shasta realm.\nThis can be verified by performing one or more of the following checks:\nEnsure that the new user is listed under Manage Users on the Administration Console page. Retrieve a token for the user. Log in to the Keycloak Shasta realm as the new user. This verifies the account\u0026rsquo;s validity and allows the user to reset their password. This functionality is supported for internal Keycloak accounts only. Verify that the new local Keycloak account can authenticate to the Cray CLI.\nNOTE: Authorization with the Cray CLI is local to a host. The first time the CLI is used on a host where it has not been used before, it is first necessary to authenticate on that host. There is no provided mechanism to distribute CLI authorization across hosts.\nFor additional information, see Configure the Cray CLI.\nlinux# cray auth login --username USERNAME "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/verify_node_removal/",
	"title": "Verify Node Removal",
	"tags": [],
	"description": "",
	"content": "Verify Node Removal Use this procedure to verify that a node has been successfully removed from the system.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. This procedure requires the component name (xname) of the removed node to be known. Procedure Ensure that the Redfish endpoint of the removed node\u0026rsquo;s BMC has been disabled.\nncn-m001# cray hsm inventory redfishEndpoints describe x3000c0s19b4 --format toml In the following example output, the Enabled field is false, indicating that the Redfish endpoint has been disabled.\nDomain = \u0026#34;\u0026#34; MACAddr = \u0026#34;a4bf012b71a9\u0026#34; UUID = \u0026#34;61b3843b-9d33-4986-ba03-1b8acd0bfd9c\u0026#34; IPAddress = \u0026#34;10.254.2.13\u0026#34; RediscoverOnUpdate = true Hostname = \u0026#34;10.254.2.13\u0026#34; Enabled = false FQDN = \u0026#34;10.254.2.13\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34;\u0026#34; Type = \u0026#34;NodeBMC\u0026#34; ID = \u0026#34;x3000c0s19b4\u0026#34; [DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2020-04-03T12:37:48.833692Z\u0026#34; RedfishVersion = \u0026#34;1.1.0\u0026#34; LastDiscoveryStatus = \u0026#34;DiscoverOK\u0026#34; Ensure that the nodes have been disabled.\nncn-m001# cray hsm state components describe x3000c0s19b4n0 --format toml In the following example output, the Enabled field is false, indicating that the node has been disabled.\nID = \u0026#34;x3000c0s19b4n0\u0026#34; Type = \u0026#34;Node\u0026#34; State = \u0026#34;Off\u0026#34; Flag = \u0026#34;OK\u0026#34; Enabled = false Role = \u0026#34;Compute\u0026#34; NID = 1164 NetType = \u0026#34;Sling\u0026#34; Arch = \u0026#34;X86\u0026#34; Class = \u0026#34;River\u0026#34; If a River node will not be replaced, update SLS to omit it.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/check_the_bmc_failover_mode/",
	"title": "Check the BMC Failover Mode",
	"tags": [],
	"description": "",
	"content": "Check the BMC Failover Mode Gigabyte BMCs must have their failover mode disabled to prevent incorrect network assignment.\nIf Gigabyte BMC failover mode is not disabled, then some BMCs may receive incorrect IP addresses. Specifically, a BMC may request an IP address on the wrong subnet and be unable to re-acquire a new IP address on the correct subnet. If this occurs, administrators should ensure that the impacted BMC has its failover feature disabled.\nCheck the failover setting on a Gigabyte BMC.\nread -s is used to prevent the password from being written to the screen or the shell history.\nncn# USERNAME=root ncn# read -r -s -p \u0026#34;BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H BMC_HOSTNAME_OR_IP raw 0x0c 0x02 0x01 210 0 0 Example output:\n11 00 00 The output can be interpreted as follows:\n11 00 01 - failover mode is enabled. 11 00 00 – failover mode is disabled (this is the desired state). Note: On Gigabyte BMCs, the default setting is for failover mode to be enabled. Therefore, if a Gigabyte BMC is reset to defaults for any reason, or upgraded, then failover mode must be disabled again in order to switch the BMC to manual mode.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/large/",
	"title": "Large",
	"tags": [],
	"description": "",
	"content": "Large Back to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/system_images/",
	"title": "Configure System Images",
	"tags": [],
	"description": "",
	"content": "Configure System Images Dell switches support active and standby images.\nConfiguration Commands Copy an image from a local server:\nSwitch(config)# image download ftp://admin@1.1.1.1:/image.bin Install image:\nSwitch(config)# image install file-url Show commands to validate functionality:\nSwitch(config)# show boot detail Expected Results Administrators can upload an image to the switch Administrators can boot into the uploaded image Administrators can see they are running the uploaded image Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/initial_prioritization/",
	"title": "Initial Prioritization",
	"tags": [],
	"description": "",
	"content": "Initial Prioritization For most switches, the local-priority has eight levels (0-7). Zero is the lowest priority. The allowed maximum will vary per product family. Local priority is used to determine which queue a packet will use. There are multiple options to configure the local-priority:\nqos cos-map: Maps Class of Services (CoS) values from VLAN tags in incoming packets to specific local priorities qos dscp-map: Maps the DSCP from incoming packets to specific local priorities qos trust: Assumes incoming packets are marked correctly, and takes the local-priority from either the CoS or Differentiated Service Code-Points (DSCP) field of the packet, or ignores any values set on incoming packets and places the packets into the default local-priority queue if the none option is given Configuration Commands Map incoming 802.1p values to a local priority:\nswitch(config)# qos cos-map \u0026lt;0-7\u0026gt; local-priority VALUE [color COLOR] [name NAME] Map incoming DSCP to a local priority:\nswitch(config)# qos dscp-map \u0026lt;0-63\u0026gt; local-priority VALUE [color COLOR] [name NAME] Configure QoS trust:\nswitch(config)# qos trust [none|cos|dscp] switch(config-if)# qos trust [none|cos|dscp] Show commands to validate functionality:\nswitch# show qos [cos-map|dscp-map|trust] Expected Results Administrators can enable QoS trust to CoS on an interface Administrators can map incoming 802.1p values to local priorities The output of all show commands is correct Example Output switch(config)# qos dscp-map 46 local-priority 7 color green name VOICE switch # show qos cos-map code_point local_priority color name ---------- -------------- ------- ---- 0 1 green Best_Effort 1 0 green Background 2 2 green Excellent_Effort 3 3 green Critical_Applications 4 4 green Video 5 5 green Voice 6 6 green Internetwork_Control 7 7 green Network_Control Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/rebuild_unhealthy_etcd_clusters/",
	"title": "Rebuild Unhealthy etcd Clusters",
	"tags": [],
	"description": "",
	"content": "Rebuild Unhealthy etcd Clusters Rebuild any cluster that does not have healthy pods by deleting and redeploying unhealthy pods. This procedure includes examples for rebuilding etcd clusters in the services namespace. This procedure must be used for each unhealthy cluster, and not just those used in the following examples.\nThis process also applies when etcd is not visible when running the kubectl get pods command.\nThe commands in this procedure can be run on any Kubernetes master or worker node on the system.\nA special procedure is also included for the Content Projection Service (CPS), because the process for rebuilding its cluster is slightly different.\nPrerequisites Rebuild procedure Automated script for clusters in the services namespace Example command and output Next step Manual procedure for clusters in the services namespace Post-rebuild steps Final checks Prerequisites An etcd cluster has pods that are not healthy, or the etcd cluster has no pods. See Check the Health and Balance of etcd Clusters for more information.\nRebuild procedure Etcd clusters other than the Content Projection Service (CPS) are rebuilt using an automated script or a manual procedure. The Content Projection Service (CPS) cluster can only be rebuilt using the manual procedure.\nAutomated script for clusters in the services namespace Manual procedure for clusters in the services namespace Regardless of which method is chosen, after completing the rebuild, the last step is to perform the Final checks.\nAutomated script for clusters in the services namespace The automated script will restore the cluster from a backup if it finds a backup created within the last 7 days. If it does not discover a backup within the last 7 days, it will ask the user if they would like to rebuild the cluster.\nThe automated script follows the same steps as the manual procedure. If the automated script fails at any step, then continue rebuilding the cluster using the manual procedure.\nRebuild/restore a single cluster\nncn-mw# /opt/cray/platform-utils/etcd_restore_rebuild_util/etcd_restore_rebuild.sh -s cray-bos-etcd Rebuild/restore multiple clusters\nncn-mw# /opt/cray/platform-utils/etcd_restore_rebuild_util/etcd_restore_rebuild.sh -m cray-bos-etcd,cray-uas-mgr-etcd Rebuild/restore all clusters\nncn-mw# /opt/cray/platform-utils/etcd_restore_rebuild_util/etcd_restore_rebuild.sh -a Example command and output ncn-mw# /opt/cray/platform-utils/etcd_restore_rebuild_util/etcd_restore_rebuild.sh -s cray-bss-etcd Example output:\nThe following etcd clusters will be restored/rebuilt: cray-bss-etcd You will be accepting responsibility for any missing data if there is a restore/rebuild over a running etcd k/v. HPE assumes no responsibility. Proceed restoring/rebuilding? (yes/no) yes Proceeding: restoring/rebuilding etcd clusters. The following etcd clusters did not have backups so they will need to be rebuilt: cray-bss-etcd Would you like to proceed rebuilding all of these etcd clusters? (yes/no) yes ----- Rebuilding cray-bss-etcd ----- Deployment and etcd cluster objects captured in yaml file yaml files edited deployment.apps \u0026#34;cray-bss\u0026#34; deleted etcdcluster.etcd.database.coreos.com \u0026#34;cray-bss-etcd\u0026#34; deleted Waiting for pods to terminate. etcdcluster.etcd.database.coreos.com/cray-bss-etcd created Waiting for pods to be \u0026#39;Running\u0026#39;. - Waiting for 3 cray-bss-etcd pods to be running: No resources found in services namespace. - 0/3 Running - 1/3 Running - 2/3 Running - 3/3 Running Checking endpoint health. cray-bss-etcd-qj4ds8j9k6 - Endpoint reached successfully cray-bss-etcd-s8ck74hf96 - Endpoint reached successfully cray-bss-etcd-vc2xznnbpj - Endpoint reached successfully deployment.apps/cray-bss created 2022-07-31-05:04:27 SUCCESSFUL REBUILD of the cray-bss-etcd cluster completed. etcdbackup.etcd.database.coreos.com \u0026#34;cray-bss-etcd-cluster-periodic-backup\u0026#34; deleted Next step After the script completes, perform the Final checks.\nManual procedure for clusters in the services namespace The following examples use the cray-bos etcd cluster, but these steps must be repeated for every unhealthy service.\nCreate YAML files for the deployment and the etcd cluster objects.\nncn-mw# kubectl -n services get deployment cray-bos -o yaml \u0026gt; /root/etcd/cray-bos.yaml ncn-mw# kubectl -n services get etcd cray-bos-etcd -o yaml \u0026gt; /root/etcd/cray-bos-etcd.yaml Only two files must be retrieved in most cases. There is a third file needed if rebuilding clusters for CPS. CPS must be unmounted before running the commands to rebuild its etcd cluster.\nncn-mw# kubectl -n services get deployment cray-cps -o yaml \u0026gt; /root/etcd/cray-cps.yaml ncn-mw# kubectl -n services get daemonset cray-cps-cm-pm -o yaml \u0026gt; /root/etcd/cray-cps-cm-pm.yaml ncn-mw# kubectl -n services get etcd cray-cps-etcd -o yaml \u0026gt; /root/etcd/cray-cps-etcd.yaml Edit each YAML file.\nRemove the entire lines for creationTimestamp, generation, resourceVersion, and uid. Remove the status line, as well as every line after it. For example:\ncreationTimestamp: \u0026#34;2019-11-26T16:54:23Z\u0026#34; generation: 1 resourceVersion: \u0026#34;5340297\u0026#34; uid: 65f4912e-106d-11ea-88b0-b42e993e060a status: availableReplicas: 1 conditions: - lastTransitionTime: \u0026#34;2019-11-26T16:54:23Z\u0026#34; lastUpdateTime: \u0026#34;2019-11-26T16:57:36Z\u0026#34; message: ReplicaSet \u0026#34;cray-bos-6f4475d59b\u0026#34; has successfully progressed. reason: NewReplicaSetAvailable status: \u0026#34;True\u0026#34; type: Progressing - lastTransitionTime: \u0026#34;2019-11-29T03:25:29Z\u0026#34; lastUpdateTime: \u0026#34;2019-11-29T03:25:29Z\u0026#34; message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \u0026#34;True\u0026#34; type: Available observedGeneration: 1 readyReplicas: 1 replicas: 1 updatedReplicas: 1 Delete the deployment and the etcd cluster objects.\nWait for the pods to terminate before proceeding to the next step.\nncn-mw# kubectl delete -f /root/etcd/cray-bos.yaml ncn-mw# kubectl delete -f /root/etcd/cray-bos-etcd.yaml If rebuilding CPS, the etcd cluster, deployment, and daemonset must be removed:\nncn-mw# kubectl delete -f /root/etcd/cray-cps.yaml ncn-mw# kubectl delete -f /root/etcd/cray-cps-cm-pm.yaml ncn-mw# kubectl delete -f /root/etcd/cray-cps-etcd.yaml Apply the etcd cluster file.\nncn-mw# kubectl apply -f /root/etcd/cray-bos-etcd.yaml Wait for all three pods to go into the Running state before proceeding to the next step. Use the following command to monitor the status of the pods:\nncn-mw# kubectl get pods -n services | grep bos-etcd Example output:\ncray-bos-etcd-hwcw4429b9 1/1 Running 1 7d18h cray-bos-etcd-mdnl28vq9c 1/1 Running 0 36h cray-bos-etcd-w5vv7j4ghh 1/1 Running 0 18h Apply the deployment file.\nncn-mw# kubectl apply -f /root/etcd/cray-bos.yaml If rebuilding CPS, the etcd cluster file, deployment file, and daemonset file must be reapplied:\nncn-mw# kubectl apply -f /root/etcd/cray-cps.yaml ncn-mw# kubectl apply -f /root/etcd/cray-cps-cm-pm.yaml ncn-mw# kubectl apply -f /root/etcd/cray-cps-etcd.yaml Proceed to Post-rebuild steps in order to finish rebuilding the cluster.\nPost-rebuild steps Update the IP address that interacts with the rebuilt cluster.\nAfter recreating the etcd cluster, the IP address needed to interact with the cluster changes, which requires recreating the etcd backup. The IP address is created automatically via a cronjob that runs at the top of each hour.\nDetermine the periodic backup name for the cluster.\nThe following example is for the bos cluster:\nncn-mw# kubectl get etcdbackup -n services | grep bos.*periodic Example output:\ncray-bos-etcd-cluster-periodic-backup Delete the etcd backup definition.\nA new backup will be created that points to the new IP address.\nIn the following command, substitute the backup name obtained in the previous step.\nncn-mw# kubectl delete etcdbackup -n services cray-bos-etcd-cluster-periodic-backup Proceed to the next section and perform the Final checks.\nFinal checks Whether the rebuild was done manually or with the automated script, after completing the procedure, perform the following checks.\nCheck if the rebuilt cluster\u0026rsquo;s data needs to be repopulated.\nSee Repopulate Data in etcd Clusters When Rebuilding Them.\nRun the etcd cluster health check.\nEnsure that the clusters are healthy and have the correct number of pods. See Check the Health and Balance of etcd Clusters.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/set_limits_for_a_configuration_session/",
	"title": "Set Limits for a Configuration Session",
	"tags": [],
	"description": "",
	"content": "Set Limits for a Configuration Session The configuration layers and session hosts can be limited when running a Configuration Framework Service (CFS) session.\nLimit CFS session hosts Subsets of nodes can be targeted in the inventory when running CFS sessions, which is useful specifically when running a session with dynamic inventory. Use the CFS --ansible-limit option when creating a session to apply the limits. The option directly corresponds to the --limit option offered by ansible-playbook, and can be used to specify hosts, groups, or combinations of them with patterns. CFS passes the value of this option directly to the ansible-playbook command for each configuration layer in the session. For more information, see the Ansible documentation on Patterns: targeting hosts and groups.\nIMPORTANT: The --limit option is useful for temporarily limiting the scope of targets for a configuration session. For example, it could be used to target a subset of the Compute group that has been separated for development use. However it should not be used to limit an Ansible playbook to target only the nodes that the playbook is intended to use. If a playbook should only be run on a specific group, target the proper groups with the hosts: section of the Ansible playbook.\nSee Using Ansible Limits for more information about limiting hosts and groups in playbooks.\nUse the following command to create a CFS session to run on all hosts in the Compute group, but not a previously defined dev group:\nncn-mw# cray cfs sessions create --name example --configuration-name configurations-example \\ --ansible-limit \u0026#39;Compute:!dev\u0026#39; --format json Example output:\n{ \u0026#34;ansible\u0026#34;: { \u0026#34;config\u0026#34;: \u0026#34;cfs-default-ansible-cfg\u0026#34;, \u0026#34;limit\u0026#34;: \u0026#34;Compute:!dev\u0026#34;, \u0026#34;verbosity\u0026#34;: 0 }, \u0026#34;configuration\u0026#34;: { \u0026#34;limit\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;artifacts\u0026#34;: [], \u0026#34;session\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;succeeded\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;tags\u0026#34;: {}, \u0026#34;target\u0026#34;: { \u0026#34;definition\u0026#34;: \u0026#34;dynamic\u0026#34;, \u0026#34;groups\u0026#34;: null } } Limit CFS session configuration layers It is possible to limit the session to only specific layers of the configuration that is specified. This is useful when re-applying configuration of a specific layer and applying the other layers is not necessary or desired. This option may also reduce the number of configurations that need to be created and stored by CFS because sessions can specify layers from a master configuration layer list.\nUse the --configuration-limit option when creating a CFS session to apply configuration layer limits. Multiple layers to limit the session are specified as a comma-separated list either by name (if layers were given names when created) or by zero-based index as defined in the configuration submitted to CFS.\nUse the following command to create a CFS session to run only on example-layer1, and then example-layer5 of a previously created configurations-example configuration:\nWARNING: If the configuration\u0026rsquo;s layers do not have names, then indices must be specified. Do not mix layer names and layer indices when using limits.\nncn-mw# cray cfs sessions create --name example --configuration-name configurations-example \\ --configuration-limit \u0026#39;example-layer1,example-layer5\u0026#39; --format json Example output:\n{ \u0026#34;ansible\u0026#34;: { \u0026#34;config\u0026#34;: \u0026#34;cfs-default-ansible-cfg\u0026#34;, \u0026#34;limit\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;verbosity\u0026#34;: 0 }, \u0026#34;configuration\u0026#34;: { \u0026#34;limit\u0026#34;: \u0026#34;example-layer1,example-layer5\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;artifacts\u0026#34;: [], \u0026#34;session\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;succeeded\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;tags\u0026#34;: {}, \u0026#34;target\u0026#34;: { \u0026#34;definition\u0026#34;: \u0026#34;dynamic\u0026#34;, \u0026#34;groups\u0026#34;: null } } "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/node_boot_root_cause_analysis/",
	"title": "Node Boot Root Cause Analysis",
	"tags": [],
	"description": "",
	"content": "Node Boot Root Cause Analysis The first step in debugging compute node boot-related issues is to determine the underlying cause, and the stage that the issue was encountered at.\nThe ConMan tool collects compute node logs. To learn more about ConMan, refer to ConMan.\nA node\u0026rsquo;s console data can be accessed through its log file, as described in Access Compute Node Logs). This information can also be accessed by connecting to the node\u0026rsquo;s console with ipmitool. Refer to online documentation to learn more about using ipmitool.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/end_user_uais/",
	"title": "End-User UAIs",
	"tags": [],
	"description": "",
	"content": "End-User UAIs UAIs used for interactive logins are called End-User UAIs. End-User UAIs can be seen as lightweight User Access Nodes (UANs), but there are important differences between UAIs and UANs.\nEnd-User UAIs are not dedicated hardware like UANs. They are implemented as containers orchestrated by Kubernetes, which makes them subject to Kubernetes scheduling and resource management rules. One key element of Kubernetes orchestration is impermanence. While End-User UAIs are often long running, Kubernetes can reschedule or recreate them as needed to meet resource and node availability constraints. UAIs can also be removed administratively.\nEnd-User UAIs can also be configured with soft and hard timeout values. Reaching a soft timeout causes the UAI to be removed automatically when it is or becomes idle \u0026ndash; defined as having no logged in user sessions. Reaching a hard timeout causes the UAI to be removed immediately regardless of logged in user sessions.\nWhen any of these things cause a UAI to terminate, a new UAI may be created, but that new UAI reverts to its initial state, discarding any internal changes that might have been made in its previous incarnation. State residing on external storage is, of course, preserved and available in the new End-User UAI.\nAn administratively removed End-User UAI or an End-User UAI terminated by a timeout may or may not ever be re-created. An End-User UAI that is preempted because of resource pressure or other Kubernetes scheduling reasons may become unavailable for an extended time until the pressure is relieved, but will usually return to service once the underlying issue is resolved.\nThe impermanence of End-User UAIs makes them suitable for tasks that are immediate and interactive over relatively short time frames, such as building and testing software or launching workloads. This impermanence makes them unsuitable for unattended activities like executing cron jobs or continuous monitoring of workload progress from a logged-in shell. These kinds of activities are more suited to UANs, which are more permanent and, unless they are re-installed, retain modified state through reboots and other interruptions.\nAnother way End-User UAIs differ from UANs is that any given End-User UAI is restricted to serving a single user. This protects users from interfering with each other within UAIs and means that any user who wants to use a UAI has to arrange for the UAI to be created and assigned. The Brokered UAI Management mode simplifies this process by providing automatic creation of and connection to UAIs using SSH logins. Once a user has an End-User UAI assigned, the user may initiate any number of SSH sessions to that UAI (or, in the case of Broker UAIs the broker serving that UAI), but no other user will be recognized by the UAI when attempting to connect. In case of Broker UAIs each unique user will be assigned a unique End-User UAI upon successful login. Multiple sessions of the same user will be will be forwarded by the Broker UAI to the same End-User UAI.\nTop: User Access Service (UAS)\nNext Topic: Special Purpose UAIs\n"
},
{
	"uri": "/docs-csm/en-12/install/prepare_compute_nodes/",
	"title": "Prepare Compute Nodes",
	"tags": [],
	"description": "",
	"content": "Prepare Compute Nodes Configure HPE Apollo 6500 XL645d Gen10 Plus Compute Nodes Gather information Configure the iLO to use VLAN 4 Configure the switch port for the iLO to use VLAN 4 Clear bad MAC and IP address out of KEA Clear bad ID out of HSM Update the BIOS Time on Gigabyte Compute Nodes Next topic Configure HPE Apollo 6500 XL645d Gen10 Plus Compute Nodes The HPE Apollo 6500 XL645d Gen10 Plus compute node uses a NIC/shared iLO network port. The NIC is also referred to as the Embedded LOM (LAN On Motherboard) and is available to the booted OS. This shared port is plugged into a port on the TOR Ethernet switch designated for the Hardware Management Network (HMN) causing the NIC to get an IP address assigned to it from the wrong pool. To prevent this from happening, the iLO VLAN tag needs to be configured for VLAN 4 and the switch port the NIC/shared iLO is plugged into needs to be configured to allow only VLAN 4 traffic. This prevents the NIC from communicating over the switch and it will no longer DHCP an IP address.\nThis procedure needs to be done for each of the HPE Apollo 6500 XL645d servers that will be managed by CSM software. River compute nodes always index their BMCs from 1. For example, the compute BMCs in servers with more than one node will have component names (xnames) as follows: x3000c0s30b1, x3000c0s30b2, x3000c0s30b3, and so on. The node indicator is always a 0. For example, x3000c0s30b1n0 or x3000c0s30b4n0.\n1. Gather information Gather information.\nThe following is an example using x3000c0s30b1n0 as the target compute node component name (xname):\nncn# XNAME=x3000c0s30b1n0 ncn# cray hsm inventory ethernetInterfaces list --component-id \\ ${XNAME} --format json | jq \u0026#39;.[]|select((.IPAddresses|length)\u0026gt;0)\u0026#39; Example output:\n{ \u0026#34;ID\u0026#34;: \u0026#34;6805cabbc182\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;68:05:ca:bb:c1:82\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-04-19T22:15:00.523621Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x3000c0s30b1n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;IPAddresses\u0026#34;: [ { \u0026#34;IPAddress\u0026#34;: \u0026#34;10.252.1.21\u0026#34; } ] }, { \u0026#34;ID\u0026#34;: \u0026#34;9440c938f7b4\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;94:40:c9:38:f7:b4\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-05-07T18:37:59.239924Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x3000c0s30b1n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;IPAddresses\u0026#34;: [ { \u0026#34;IPAddress\u0026#34;: \u0026#34;10.254.1.38\u0026#34; } ] } The second entry is the indication that the NIC is receiving incorrect IP addresses. The 10.254.x.y address is for the HMN and should not be associated with the node itself (x3000c0s30b1n0).\nMake a note of the ID, MACAddress, and IPAddress of the entry that has the 10.254 address listed.\nncn# ID=\u0026#34;9440c938f7b4\u0026#34; ncn# MAC=\u0026#34;94:40:c9:38:f7:b4\u0026#34; ncn# IPADDR=\u0026#34;10.254.1.38\u0026#34; These will be used later to clean up KEA and Hardware State Manager (HSM). There may not be a 10.254 address associated with the node \u0026ndash; that is OK and will enable skipping of several steps later on.\n2. Configure iLO Configure the iLO to use VLAN 4.\nConnect to BMC web user interface and log in with standard root credentials.\nFrom the administrator\u0026rsquo;s own machine, create an SSH tunnel (-L creates the tunnel, and -N prevents a shell and stubs the connection):\nlinux# BMC=x3000c0s30b1 linux# ssh -L 9443:$BMC:443 -N root@example-ncn-m001 Opening a web browser to https://localhost:9443 will give access to the BMC\u0026rsquo;s web interface.\nLogin with the root credentials.\nClick on iLO Shared Network Port on left menu.\nMake a note of the MAC Address under the Information section; that will be needed later.\nncn# ILOMAC=\u0026#34;\u0026lt;MAC Address\u0026gt;\u0026#34; For example\nncn# ILOMAC=\u0026#34;94:40:c9:38:08:c7\u0026#34; Click on General on the top menu.\nUnder NIC Settings move slider to Enable VLAN.\nIn the VLAN Tag box, enter 4.\nClick Apply.\nClick Reset iLO when it appears.\nClick Yes, Reset when it appears on the right.\nAfter accepting the BMC restart, connection to the BMC will be lost until the switch port reconfiguration is performed.\n3. Configure switch port Configure the switch port for the iLO to use VLAN 4.\nFind the port and the switch the iLO is plugged into using the SHCD.\nSSH to the switch and log in with standard admin credentials. Refer to /etc/hosts for exact hostname.\nVerify the MAC address on the port.\nExample using port number 46.\nsw-leaf-001# show mac-address-table | include 1/1/46 Example output:\n94:40:c9:38:08:c7 4 dynamic 1/1/46 Make sure the MAC address shown for that port matches the ILOMAC address noted in step 2.3 from the Information section of the WebUI.\nNOTE: If the MAC address is not correct, double check the server cabling and SHCD for the correct port then start this section over. Do not move on until the ILOMAC address has been found on the switch at the expected port.\nConfigure the port if the MAC address is correct.\nExample using port number 46.\nConfigure the port.\nsw-leaf-001# configure t sw-leaf-001(config)# int 1/1/46 sw-leaf-001(config-if)# vlan trunk allowed 4 sw-leaf-001(config-if)# write mem Example output:\nCopying configuration: [Success] Exit configure mode.\nsw-leaf-001(config-if)# exit sw-leaf-001(config)# exit Verify the settings.\nsw-leaf-001# show running-config interface 1/1/46 Example output:\ninterface 1/1/46 no shutdown mtu 9198 description dl645d no routing vlan trunk native 1 vlan trunk allowed 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit After a few minutes the switch will be configured and access to the WebUI will be regained.\n4. Cleanup Kea Clear bad MAC address and IP address out of KEA.\nNOTE: Skip this section if there was no bad MAC address and IP address found in section 1.\nRetrieve an API token.\nncn# TOKEN=$(curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Remove the entry from KEA that is associated with the MAC address and IP address gathered previously.\nncn# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \\ \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;command\u0026#34;: \u0026#34;lease4-del\u0026#34;, \\ \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ], \u0026#34;arguments\u0026#34;: {\u0026#34;hw-address\u0026#34;: \u0026#34;\u0026#39;${MAC}\u0026#39;\u0026#34;, \\ \u0026#34;ip-address\u0026#34;: \u0026#34;\u0026#39;${IPADDR}\u0026#39;\u0026#34;}}\u0026#39; https://api-gw-service-nmn.local/apis/dhcp-kea Expected results:\n[ { \u0026#34;result\u0026#34;: 0, \u0026#34;text\u0026#34;: \u0026#34;IPv4 lease deleted.\u0026#34; } ] 5. Cleanup HSM Clear bad ID out of HSM.\nNOTE: Skip this section if there was no bad ID found in section 1.\nTell HSM to delete the bad ID out of the Ethernet Interfaces table.\nncn# cray hsm inventory ethernetInterfaces delete $ID --format json Expected results:\n{ \u0026#34;code\u0026#34;: 0, \u0026#34;message\u0026#34;: \u0026#34;deleted 1 entry\u0026#34; } Everything is now configured and the CSM software will automatically discover the node after several minutes. After it has been discovered, the node is ready to be booted.\n6. Update the BIOS time on Gigabyte compute nodes The BIOS time for Gigabyte compute nodes must be synchronized with the rest of the system. See Update the Gigabyte Node BIOS Time.\nNext topic After completing the preparation for compute nodes, the CSM product stream has been fully installed and configured. Check the next topic.\nSee Next topic for more information on other product streams to be installed and configured after CSM.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/troubleshoot_failure_to_get_ceph_health/",
	"title": "Troubleshoot Failure to Get Ceph Health",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Failure to Get Ceph Health Inspect Ceph commands that are failing by looking into the Ceph monitor logs (ceph-mon). For example, the monitoring logs can help determine any issues causing the ceph -s command to hang.\nTroubleshoot Ceph commands failing to run and determine how to make them operational again. These commands need to be operational to obtain critical information about the Ceph cluster on the system.\nPrerequisites This procedure requires admin privileges.\nProcedure Verify the node being used is running ceph-mon.\nVerify ceph-mon processes are running on the first three NCN storage nodes.\nSee Manage_Ceph_Service for more information. If more than three storage nodes exist, check the output of ceph orch ps for more information.\nCheck ceph-mon logs to see if the cluster is out of quorum.\nVerify the issue is resolved by rerunning the Ceph command that failed.\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/create_a_backup_of_the_keycloak_postgres_database/",
	"title": "Create a Backup of the Keycloak Postgres Database",
	"tags": [],
	"description": "",
	"content": "Create a Backup of the Keycloak Postgres Database Perform a manual backup of the contents of the Keycloak Postgres database. This backup can be used to restore the contents of the Keycloak Postgres database at a later point in time using the Restore Keycloak Postgres from Backup procedure.\nPrerequisites Healthy Keycloak Postgres Cluster.\nUse patronictl list on the Keycloak Postgres cluster to determine the current state of the cluster and note which member is the Leader. A healthy cluster will look similar to the following:\nncn-mw# kubectl exec keycloak-postgres-0 -n services -c postgres -it -- patronictl list Example output:\n+ Cluster: keycloak-postgres (7062401252302942285) -----+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +---------------------+--------------+--------+---------+----+-----------+ | keycloak-postgres-0 | 10.32.55.217 | Leader | running | 13 | | | keycloak-postgres-1 | 10.44.43.65 | | running | 13 | 0 | | keycloak-postgres-2 | 10.33.92.236 | | running | 13 | 0 | +---------------------+--------------+--------+---------+----+-----------+ Healthy Keycloak Service.\nVerify all 3 Keycloak replicas are up and running:\nncn-mw# kubectl -n services get pods -l cluster-name=keycloak-postgres Example output:\nNAME READY STATUS RESTARTS AGE keycloak-postgres-0 3/3 Running 0 12d keycloak-postgres-1 3/3 Running 0 12d keycloak-postgres-2 3/3 Running 0 12d Procedure Set the Keycloak variables including the Leader which for this case is the member keycloak-postgres-0.\nncn-mw# CLIENT=cray-keycloak ncn-mw# POSTGRESQL=keycloak-postgres ncn-mw# NAMESPACE=services ncn-mw# POSTGRES_LEADER=keycloak-postgres-0 Scale the client service down.\nncn-mw# kubectl scale statefulset ${CLIENT} -n ${NAMESPACE} --replicas=0 # Wait for the pods to terminate ncn-mw# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/instance=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2 done Create a dump of the Keycloak Postgres database.\nncn-mw# kubectl exec -it ${POSTGRES_LEADER} -n ${NAMESPACE} -c postgres -- pg_dumpall -c -U postgres \u0026gt; \u0026#34;${POSTGRESQL}-dumpall.sql\u0026#34; Copy the ${POSTGRESQL}-dumpall.sql file off of the cluster, and store it in a secure location.\nScale the client service back up.\nncn-mw# kubectl scale statefulset ${CLIENT} -n ${NAMESPACE} --replicas=3 "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/view_bios_logs_for_liquid_cooled_nodes/",
	"title": "View BIOS Logs for Liquid-Cooled Nodes",
	"tags": [],
	"description": "",
	"content": "View BIOS Logs for Liquid-Cooled Nodes SSH to a liquid-cooled node and view the BIOS logs. The BIOS logs for liquid-cooled node controllers (nC) are stored in the /var/log/n0/current and /var/log/n1/current directories.\nThe BIOS logs for liquid-cooled nodes are helpful for troubleshooting boot-related issues.\nPrerequisites This procedure requires administrative privileges.\nProcedure Log in to the node.\nSSH into the node controller for the host component name (xname). For example, if the host xname (as defined in /etc/hosts) is x5000c1s0b0n0, then the node controller would be x5000c1s0b0.\nncn# ssh XNAME Confirm that the hostname is correct for the node being used.\nhostname Example output:\nx1000c2s5b0 View the logs for n0.\nn0 is node 0 on the BMC.\ntail /var/log/n0/current View the logs for n1.\nn1 is node 1 on the BMC.\ntail /var/log/n1/current "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/clear_space_in_root_file_system_on_worker_nodes/",
	"title": "Clear Space in Root File System on Worker Nodes",
	"tags": [],
	"description": "",
	"content": "Clear Space in Root File System on Worker Nodes The disk space on an NCN worker node can fill up if any services are consuming a large portion of the root file system on the node. This procedure shows how to safely clear some space on worker nodes to return them to an appropriate storage threshold.\nPrerequisites An NCN worker node has a full disk.\nProcedure Check to see if Docker is running.\nncn-w001# syctemctl status docker Example output:\n● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor pres\u0026gt; Active: **active** (running) since Wed 2020-06-10 11:03:49 CDT; 2 months 2 days \u0026gt; Docs: http://docs.docker.com Main PID: 3062 (dockerd) Tasks: 145 CGroup: /system.slice/docker.service ├─3062 /usr/bin/dockerd --add-runtime oci=/usr/sbin/docker-runc ├─3248 docker-containerd --config /var/run/docker/containerd/contain\u0026gt; ├─5557 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port \u0026gt; └─5576 docker-containerd-shim -namespace moby -workdir /var/lib/dock\u0026gt; [...] If Docker is active, proceed to the next step to check its usage.\nView the file space usage for Docker.\nncn-w001# du -sh /var/lib/docker Example output:\n178G /var/lib/docker If the output indicates usage is over 100GB, proceed to the next step to prune Docker.\nPrune the Docker images.\nThe until=24 option in the command below preserves data less than one day old.\nncn-w001# docker image prune -a --filter until=24h Check the usage again with the du -sh /var/lib/docker command.\nPrune the Docker volumes.\nThe until=24 option in the command below preserves data less than one day old.\nncn-w001# docker volume prune -a --filter until=24h Check the usage again with the du -sh /var/lib/docker command.\nCheck the usage of /var/log/cray.\nAnother potentially large consumer of space is /var/log/cray when certain debug flags are enabled.\nncn-w001# du -sh /var/log/cray Example output:\n76M /var/log/cray If the usage is over 20GB, examine the logging and determine if any of the older log information needs to be kept. Candidates for clean up include old imfile-state files, as well as old forwarding-queue files. Reduce the quantity of any additional logging as soon as possible to prevent the disk from filling up again.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/lldp/",
	"title": "Link layer discovery protocol (LLDP)",
	"tags": [],
	"description": "",
	"content": "Link layer discovery protocol (LLDP) LLDP is used to advertise the device\u0026rsquo;s identity and abilities and read other devices connected to the same network. Note: LLDP is enabled by default.\nRelevant Configuration\nEnable lldp\nswitch(config)# lldp Enable lldp on interface\nswitch (config interface ethernet 1/1) # lldp receive switch (config interface ethernet 1/1) # lldp transmit Show Commands to Validate Functionality\nswitch# show lldp local Expected Results\nStep 1: Link status between the peer devices is UP Step 2: LLDP is enabled Step 3: Local device LLDP Information is displayed Step 4: Remote device LLDP information is displayed Step 5: LLDP statistics are displayed Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/upgrade/",
	"title": "Perform an Upgrade on Dell Switches",
	"tags": [],
	"description": "",
	"content": "Perform an Upgrade on Dell Switches How to perform an upgrade on the Dell switches.\nConfiguration Commands Download the new software image:\nswitch# image download file-url View the current software download status:\nswitch# show image status Install the software image:\nswitch# image install image-url View the status of the current software install:\nswitch# show image status Change the next boot partition to the standby partition:\nswitch# boot system standby Reload the new software image:\nswitch# reload Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/intro/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction The intent for the this documentation is to help install and manage Aruba CX network devices in an HPE Cray EX system installation.\nThe HPE Cray recommended way of configuring the network is with the CANU tool. Thus, this guide will not go to into a level of detail of how to configure each switch with the CLI in the topology. However, it will provide examples of how to configure and use features generated by CANU to provide administrators with an easy way to customize their installation.\nThis guide also includes the current documented and supported network scenarios.\nNOTE: Not every configuration option is covered here, and for any configuration outside of the scope of this document, refer to the official Aruba user manuals.\nAruba documentation and software can be found from:\nhttps://asp.arubanetworks.com/\nThis document is intended for network administrators and support personnel.\nThe display and command line illustrated in this document are examples and might not exactly match your particular environment. The switch and accessory drawings in this document are for illustration only, and may not exactly match your installed products.\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/recover_from_postgres_wal_event/",
	"title": "Recover from Postgres WAL Event",
	"tags": [],
	"description": "",
	"content": "Recover from Postgres WAL Event A WAL event can occur because of lag, network communication, or bandwidth issues. This can cause the PVC hosted by Ceph and mounted inside the container on /home/postgres/pgdata to fill and the database to stop running. If no database dump exists, then the disk space issue needs to be fixed so that a dump can be taken. Then the dump can be restored to a newly created postgresql cluster. If a dump already exists, then skip to Rebuild the cluster and Restore the data.\nIf no database dump exists and neither option results in a successful dump, then service-specific Disaster Recovery for Postgres is required.\nThe recovery workflow:\nAttempt to recover to a running database\nOption 1: Clear logs and/or WAL files Option 2: Resize the Postgres PVCs Dump the data\nRebuild the cluster and restore the data\n1. Attempt to recover to a running database A running database is needed to be able to dump the current data.\nThe following example is based on cray-smd-postgres.\nConfirm that the database is down (no endpoint exists) and that the disk is full on one or more postgresql cluster member.\nncn-mw# POSTGRESQL=cray-smd-postgres ncn-mw# NAMESPACE=services ncn-mw# kubectl get endpoints ${POSTGRESQL} -n ${NAMESPACE} Expected output looks similar to:\nNAME ENDPOINTS AGE cray-smd-postgres 3d22h ncn-mw# for i in {0..2}; do echo \u0026#34;${POSTGRESQL}-${i}:\u0026#34; ; kubectl exec ${POSTGRESQL}-${i} -n ${NAMESPACE} -c postgres -- df -h pgdata done Expected output looks similar to:\ncray-smd-postgres-0: Filesystem Size Used Avail Use% Mounted on /dev/rbd8 30G 28G 1.6G 95% /home/postgres/pgdata cray-smd-postgres-1: Filesystem Size Used Avail Use% Mounted on /dev/rbd15 30G 30G 0 100% /home/postgres/pgdata cray-smd-postgres-2: Filesystem Size Used Avail Use% Mounted on /dev/rbd6 30G 383M 30G 2% /home/postgres/pgdata If the database is down and the disk is full because of replication issues, there are two ways to attempt to get back to a running database: either delete files or resize the Postgres PVCs until the database is able to start running again.\nOption 1 : Clear logs and/or WAL files The following example is based on cray-smd-postgres.\nClear files from /home/postgres/pgdata/pgroot/pg_log/ until the database is running again and allowing connections.\nFor example, if the disk space is at 100%, then copy the logs off (optional), exec into the pod, and then clear the logs to recover some disk space.\nCopy off the logs. (Optional)\nncn-mw# kubectl cp \u0026#34;${POSTGRESQL}-1\u0026#34;:/home/postgres/pgdata/pgroot/pg_log /tmp -c postgres -n ${NAMESPACE} Open an interactive shell to the container.\nncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-1\u0026#34; -n ${NAMESPACE} -c postgres -it -- bash Clear the logs.\nroot@cray-smd-postgres-1:/home/postgres# for i in {0..7}; do \u0026gt; /home/postgres/pgdata/pgroot/pg_log/postgresql-$i.csv; done Restart the Postgres cluster and postgres-operator.\nncn-mw# kubectl delete pod -n ${NAMESPACE} \u0026#34;${POSTGRESQL}-0\u0026#34; \u0026#34;${POSTGRESQL}-1\u0026#34; \u0026#34;${POSTGRESQL}-2\u0026#34; \u0026amp;\u0026amp; \\ kubectl delete pod -n services -l app.kubernetes.io/name=postgres-operator Check if the database is running. If it is running, then proceed to Dump the data.\nncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-1\u0026#34; -n ${NAMESPACE} -c postgres -it -- psql -U postgres Example of output indicating that the database is running (type \\q to exit the shell):\npsql (12.2 (Ubuntu 12.2-1.pgdg18.04+1), server 11.7 (Ubuntu 11.7-1.pgdg18.04+1)) Type \u0026#34;help\u0026#34; for help. postgres=# If the database is still not running, then delete files from /home/postgres/pgdata/pgroot/data/pg_wal/.\nCAUTION: This method could result in unintended consequences for the Postgres database and long service downtime; do not use unless there is a known Disaster Recovery for Postgres procedure for repopulating the Postgres cluster.\nncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-1\u0026#34; -n ${NAMESPACE} -c postgres -it -- bash root@cray-smd-postgres-1:/home/postgres# rm pgdata/pgroot/data/pg_wal/0* Restart the Postgres cluster and postgres-operator.\nncn-mw# kubectl delete pod -n ${NAMESPACE} \u0026#34;${POSTGRESQL}-0\u0026#34; \u0026#34;${POSTGRESQL}-1\u0026#34; \u0026#34;${POSTGRESQL}-2\u0026#34; \u0026amp;\u0026amp; \\ kubectl delete pod -n services -l app.kubernetes.io/name=postgres-operator Check if the database is running using the same method as in the earlier step.\nIf the database is still not running, then try recovering using the other option listed in this document.\nOption 2 : Resize the Postgres PVCs The following example is based on cray-smd-postgres, where the postgresql cray-smd-postgres resource and the pgdata-cray-smd-postgres PVCs will be resized from 100Gi to 120Gi.\nDetermine the current size of the Postgres PVCs and set PGRESIZE to the desired new size (it must be larger than the current size).\nGet the name of the postgresql resource.\nncn-mw# kubectl get postgresql -A | grep \u0026#34;smd\\|NAME\u0026#34; Expected output:\nNAMESPACE NAME TEAM VERSION PODS VOLUME CPU-REQUEST MEMORY-REQUEST AGE STATUS services cray-smd-postgres cray-smd 11 3 100Gi 4 8Gi 18h Running List the PVCs associated with it.\nncn-mw# kubectl get pvc -A | grep \u0026#34;cray-smd-postgres\\|NAME\u0026#34; Expected output:\nNAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE services pgdata-cray-smd-postgres-0 Bound pvc-c86859f4-a57f-4694-a66a-8120e96a1ab4 100Gi RWO k8s-block-replicated 18h services pgdata-cray-smd-postgres-1 Bound pvc-300f52e4-f88d-47ef-9a1e-e598fd919047 100Gi RWO k8s-block-replicated 18h services pgdata-cray-smd-postgres-2 Bound pvc-f33879f3-0e99-4299-b796-210fbb693a2f 100Gi RWO k8s-block-replicated 18h Set variables based on the output of these commands.\nncn-mw# PGRESIZE=120Gi ncn-mw# POSTGRESQL=cray-smd-postgres ncn-mw# PGDATA=pgdata-cray-smd-postgres ncn-mw# NAMESPACE=services Edit numberOfInstances in the postgresql resource from 3 to 1.\nncn-mw# kubectl patch postgresql ${POSTGRESQL} -n ${NAMESPACE} --type=json -p=\u0026#39;[{\u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/numberOfInstances\u0026#34;, \u0026#34;value\u0026#34; : 1}]\u0026#39; Expected output:\npostgresql.acid.zalan.do/cray-smd-postgres patched Wait for 2 of the 3 postgresql pods to terminate.\nncn-mw# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 1 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2 done Delete the PVCs from the non-running Postgres pods.\nncn-mw# kubectl delete pvc \u0026#34;${PGDATA}-1\u0026#34; \u0026#34;${PGDATA}-2\u0026#34; -n ${NAMESPACE} Expected output:\npersistentvolumeclaim \u0026#34;pgdata-cray-smd-postgres-1\u0026#34; deleted persistentvolumeclaim \u0026#34;pgdata-cray-smd-postgres-2\u0026#34; deleted Resize the remaining Postgres PVC resources.requests.storage to $PGRESIZE.\nncn-mw# kubectl patch -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;resources\u0026#34;: {\u0026#34;requests\u0026#34;: {\u0026#34;storage\u0026#34;: \u0026#34;\u0026#39;${PGRESIZE}\u0026#39;\u0026#34;}}}}\u0026#39; \u0026#34;pvc/${PGDATA}-0\u0026#34; -n ${NAMESPACE} Expected output:\npersistentvolumeclaim/pgdata-cray-smd-postgres-0 patched Wait for the PVC to resize.\nncn-mw# while [ -z \u0026#39;$(kubectl describe pvc \u0026#34;${PGDATA}-0\u0026#34; -n ${NAMESPACE} | grep FileSystemResizeSuccessful\u0026#39; ] ; do echo \u0026#34; waiting for PVC to resize\u0026#34;; sleep 2 done Update the postgresql resource spec.volume.size to $PGRESIZE.\nncn-mw# kubectl get \u0026#34;postgresql/${POSTGRESQL}\u0026#34; -n ${NAMESPACE} -o json | jq \u0026#39;.spec.volume = {\u0026#34;size\u0026#34;: \u0026#34;\u0026#39;${PGRESIZE}\u0026#39;\u0026#34;}\u0026#39; | kubectl apply -f - Expected output:\npostgresql.acid.zalan.do/cray-smd-postgres configured Restart the existing postgresql pod.\nncn-mw# kubectl delete pod \u0026#34;${POSTGRESQL}-0\u0026#34; -n services Expected output:\npod \u0026#34;cray-smd-postgres-0\u0026#34; deleted Perform verifications.\nVerify that the single instance pod is Running with 3/3 Ready.\nncn-mw# kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} Expected output:\nNAME READY STATUS RESTARTS AGE cray-smd-postgres-0 3/3 Running 0 14s Verify that patronictl reports the member is running.\nncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- patronictl list Expected output:\n+-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.44.0.38 | Leader | running | 2 | | +-------------------+---------------------+------------+--------+---------+----+-----------+ Verify that the postgresql resource is Running with new volume size ($PGRESIZE).\nncn-mw# kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} Expected output:\nNAME TEAM VERSION PODS VOLUME CPU-REQUEST MEMORY-REQUEST AGE STATUS cray-smd-postgres cray-smd 11 1 120Gi 500m 100Mi 11m Running Verify that the database is running.\nncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n services -c postgres -it -- psql -U postgres Example of output indicating that the database is running (type \\q to exit the shell):\npsql (12.2 (Ubuntu 12.2-1.pgdg18.04+1), server 11.7 (Ubuntu 11.7-1.pgdg18.04+1)) Type \u0026#34;help\u0026#34; for help. postgres=# Scale numberOfInstances in postgresql resource from 1 back to 3.\nncn-mw# kubectl patch postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/numberOfInstances\u0026#34;, \u0026#34;value\u0026#34; : 3}]\u0026#39; Expected output:\npostgresql.acid.zalan.do/cray-smd-postgres patched Logs may indicate WAL error such as the following, but a dump can be taken at this point.\nncn-mw# kubectl logs \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres | grep -i error ncn-mw# kubectl logs \u0026#34;${POSTGRESQL}-1\u0026#34; -n ${NAMESPACE} -c postgres | grep -i error ncn-mw# kubectl logs \u0026#34;${POSTGRESQL}-2\u0026#34; -n ${NAMESPACE} -c postgres | grep -i error Example of possible output if there are WAL errors:\nerror: could not get write-ahead log end position from server: ERROR: invalid segment number Update the customizations.yaml file with the same changes.\nIn order to persist any Postgres PVC storage volume size changes, it is necessary that this change also be made to the customer-managed customizations.yaml file.\nFor more information, see Postgres PVC resize.\n2. Dump the data If the recovery was successful such that the database is now running, then continue with the following steps to dump the data.\nSet helper variables for the client service and postgresql resource in question.\nncn-mw# CLIENT=cray-smd ncn-mw# POSTGRESQL=cray-smd-postgres ncn-mw# NAMESPACE=services Record the number of replicas the client service is using. and then scale it to 0.\nThe following example is based on cray-smd. The cray-smd client service is deployed as a deployment with 3 replicas. Other services may differ in type (for example, statefulset) or number of replicas.\nRecord the number of replicas.\nncn-mw# CLIENT_REPLICAS=$(kubectl get deployment -n ${NAMESPACE} ${CLIENT} -o jsonpath=\u0026#39;{.spec.replicas}\u0026#39;); echo ${CLIENT_REPLICAS} Expected output:\n3 Scale it to 0.\nncn-mw# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=0 Expected output:\ndeployment.apps/cray-smd scaled Wait for the running pods to terminate.\nncn-mw# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2 done Dump all the data.\nDetermine which Postgres member is the leader.\nncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- patronictl list Expected output:\n+-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | Leader | running | 1 | | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | running | | 0 | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | running | | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ Dump the data to a local file.\nncn-mw# POSTGRES_LEADER=cray-smd-postgres-0 ncn-mw# kubectl exec -it ${POSTGRES_LEADER} -n ${NAMESPACE} -c postgres -- pg_dumpall -c -U postgres \u0026gt; \u0026#34;${POSTGRESQL}-dumpall.sql\u0026#34; ncn-mw# ls \u0026#34;${POSTGRESQL}-dumpall.sql\u0026#34; Expected output:\ncray-smd-postgres-dumpall.sql 3. Rebuild the cluster and restore the data If recovery was successful such that a dump could be taken or a dump already exists, then continue with the following steps to rebuild the postgresql cluster and restore the data.\nThe following example restores the dump to the cray-smd-postgres cluster.\nIf the client service is not yet scaled to 0, follow the steps above to scale the client service to 0.\nDelete and re-create the postgresql resource (which includes the PVCs).\nSet helper variables.\nncn-mw# CLIENT=cray-smd ncn-mw# POSTGRESQL=cray-smd-postgres ncn-mw# NAMESPACE=services Save the postgresql resource definition to a file.\nncn-mw# kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; \u0026gt; postgres-cr.yaml Delete the current postgresql resource.\nncn-mw# kubectl delete -f postgres-cr.yaml Expected output:\npostgresql.acid.zalan.do \u0026#34;cray-smd-postgres\u0026#34; deleted Wait for the pods to terminate.\nncn-mw# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2 done Create a new postgresql resource.\nncn-mw# kubectl create -f postgres-cr.yaml Expected output:\npostgresql.acid.zalan.do/cray-smd-postgres created Wait for the pods to start running.\nncn-mw# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 3 ] ; do echo \u0026#34; waiting for pods to start running\u0026#34;; sleep 2 done Determine which Postgres member is the leader.\nncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- patronictl list Example output:\n+-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | Leader | running | 1 | | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | running | | 0 | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | running | | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ Record the name of the leader.\nncn-mw# POSTGRES_LEADER=cray-smd-postgres-0 Copy the dump taken earlier to the Postgres leader pod and restore the data.\nIf the dump exists in a different location, adjust this example as needed.\nncn-mw# kubectl cp \u0026#34;./${POSTGRESQL}-dumpall.sql\u0026#34; \u0026#34;${POSTGRES_LEADER}:/home/postgres/${POSTGRESQL}-dumpall.sql\u0026#34; -c postgres -n ${NAMESPACE} ncn-mw# kubectl exec ${POSTGRES_LEADER} -c postgres -n ${NAMESPACE} -it -- psql -U postgres \u0026lt; ${POSTGRESQL}-dumpall.sql Restore the secrets.\nOnce the dump has been restored onto the newly built postgresql cluster, the current Kubernetes secrets need to be updated in the postgresql cluster, otherwise the service will experience readiness and liveness probe failures because it will be unable to authenticate to the database.\nDetermine what secrets are associated with the postgresql credentials.\nncn-mw# kubectl get secrets -n ${NAMESPACE} | grep \u0026#34;${POSTGRESQL}.credentials\u0026#34; Example output:\nservices hmsdsuser.cray-smd-postgres.credentials Opaque 2 31m services postgres.cray-smd-postgres.credentials Opaque 2 31m services service-account.cray-smd-postgres.credentials Opaque 2 31m services standby.cray-smd-postgres.credentials Opaque 2 31m For each secret above, get the username and password from Kubernetes and update the Postgres database with this information.\nThe following example uses the hmsdsuser.cray-smd-postgres.credentials secret.\nGet the username.\nncn-mw# kubectl get secret hmsdsuser.cray-smd-postgres.credentials -n ${NAMESPACE} -ojsonpath=\u0026#39;{.data.username}\u0026#39; | base64 -d Possible output:\nhmsdsuser Get the password.\nncn-mw# kubectl get secret hmsdsuser.cray-smd-postgres.credentials -n ${NAMESPACE} -ojsonpath=\u0026#39;{.data.password}\u0026#39;| base64 -d Possible output:\nABCXYZ Open an interactive Postgres console in the leader container.\nncn-mw# kubectl exec ${POSTGRES_LEADER} -n ${NAMESPACE} -c postgres -it -- /usr/bin/psql postgres postgres Update the password for the user.\npostgres=# ALTER USER hmsdsuser WITH PASSWORD \u0026#39;ABCXYZ\u0026#39;; On success, output resembles the following:\nALTER ROLE If this is the last user being updated, enter \\q to exit the console.\nContinue the above process until all ${POSTGRESQL}.credentials secrets have been updated in the database.\nRestart the postgresql cluster and wait for it to start running.\nncn-mw# kubectl delete pod \u0026#34;${POSTGRESQL}-0\u0026#34; \u0026#34;${POSTGRESQL}-1\u0026#34; \u0026#34;${POSTGRESQL}-2\u0026#34; -n ${NAMESPACE} ncn-mw# while [ $(kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json |jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ]; do echo \u0026#34;waiting for ${POSTGRESQL} to start running\u0026#34; sleep 2 done Scale the client service back to the original number of replicas and wait for it to start running.\nThe number of replicas was saved in the CLIENT_REPLICAS variable.\nScale it back up.\nThe following example is based on cray-smd. The cray-smd client service is deployed as a deployment. Other services may differ in type (for example, statefulset).\nncn-mw# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=${CLIENT_REPLICAS} Wait for all of the pods to start running.\nncn-mw# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != ${CLIENT_REPLICAS} ] ; do echo \u0026#34; waiting for pods to start running\u0026#34; ; sleep 2 done "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/set_the_ansible-cfg_for_a_session/",
	"title": "Set the ansible.cfg for a Session",
	"tags": [],
	"description": "",
	"content": "Set the ansible.cfg for a Session View and update the Ansible configuration used by the Configuration Framework Service (CFS).\nAnsible configuration is available through the ansible.cfg file. See the Configuring Ansible external documentation for more information about what values can be set.\nCFS provides a default ansible.cfg file in the cfs-default-ansible-cfg Kubernetes ConfigMap in the services namespace.\nTo view the ansible.cfg file:\nncn-mw# kubectl get cm -n services cfs-default-ansible-cfg -o json | jq -r \u0026#39;.data.\u0026#34;ansible.cfg\u0026#34;\u0026#39; WARNING: Much of the configuration in this file is required by CFS to function properly. Particularly the cfs_aggregator callback plug-in, which is used for reporting configuration state to the CFS APIs, and the cfs_* strategy plug-ins. Exercise extreme caution when making changes to this ConfigMap\u0026rsquo;s contents. See Ansible Execution Environments for more information.\nThe default ansible.cfg file ConfigMap can be changed to a custom ConfigMap (within the services Kubernetes namespace) by updating it in the CFS service options. This will update all CFS sessions to use this file for ansible.cfg.\nTo use a different ansible.cfg on a per-session basis, use the --ansible-config option when creating a CFS session. See Use a Custom ansible.cfg File for more information.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/redeploy_the_ipxe_and_tftp_services/",
	"title": "Redeploy the iPXE and TFTP Services",
	"tags": [],
	"description": "",
	"content": "Redeploy the iPXE and TFTP Services Redeploy the iPXE and TFTP services if a pod with a ceph-fs Process Virtualization Service (PVS) on a Kubernetes worker node is causing a HEALTH_WARN error.\nResolve issues with ceph-fs and ceph-mds by restarting the iPXE and TFTP services. The Ceph cluster will return to a healthy state after this procedure.\nPrerequisites This procedure requires administrative privileges.\nProcedure Find the iPXE and TFTP deployments.\nncn-m001# kubectl get deployments -n services|egrep \u0026#39;tftp|ipxe\u0026#39; Example output:\ncray-ipxe 1/1 1 1 22m cray-tftp 3/3 3 3 28m Delete the deployments for the iPXE and TFTP services.\nncn-m001# kubectl -n services delete deployment cray-tftp ncn-m001# kubectl -n services delete deployment cray-ipxe Check the status of Ceph.\nCeph commands need to be run on ncn-m001. If a health warning is shown after checking the status, the ceph-mds daemons will need to be restarted on the manager nodes.\nCheck the health of the Ceph cluster.\nncn-m001# ceph -s Example output:\ncluster: id: bac74735-d804-49f3-b920-cd615b18316b health: HEALTH_WARN 1 filesystem is degraded services: mon: 3 daemons, quorum ncn-m001,ncn-m002,ncn-m003 (age 13d) mgr: ncn-m001(active, since 24h), standbys: ncn-m002, ncn-m003 mds: cephfs:1/1 {0=ncn-m002=up:reconnect} 2 up:standby osd: 60 osds: 60 up (since 4d), 60 in (since 4d) rgw: 5 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0, ncn-s004.rgw0, ncn-s005.rgw0) data: pools: 13 pools, 1664 pgs objects: 2.47M objects, 9.3 TiB usage: 26 TiB used, 78 TiB / 105 TiB avail pgs: 1664 active+clean io: client: 990 MiB/s rd, 111 MiB/s wr, 2.76k op/s rd, 1.03k op/s wr Obtain more information on the health of the cluster.\nncn-m001# ceph health detail Example output:\nHEALTH_WARN 1 filesystem is degraded FS_DEGRADED 1 filesystem is degraded fs cephfs is degraded Show the status of all CephFS components.\nncn-m001# ceph fs status Example output:\ncephfs - 9 clients ====== +------+-----------+----------+----------+-------+-------+ | Rank | State | MDS | Activity | dns | inos | +------+-----------+----------+----------+-------+-------+ | 0 | reconnect | ncn-m002 | | 11.0k | 74 | +------+-----------+----------+----------+-------+-------+ +-----------------+----------+-------+-------+ | Pool | type | used | avail | +-----------------+----------+-------+-------+ | cephfs_metadata | metadata | 780M | 20.7T | | cephfs_data | data | 150M | 20.7T | +-----------------+----------+-------+-------+ +-------------+ | Standby MDS | +-------------+ | ncn-m003 | | ncn-m001 | Restart the ceph-mds service.\nThis step should only be done if a health warning is shown in the previous substeps.\nncn-m001# for i in 1 2 3 ; do ansible ncn-m00$i -m shell -a \u0026#34;systemctl restart ceph-mds@ncn-m00$i\u0026#34;; done Failover the ceph-mds daemon.\nThis step should only be done if a health warning still exists after restarting the ceph-mds service.\nncn-m001# ceph mds fail ncn-m002 The initial output will display the following:\ncephfs - 0 clients ====== +------+--------+----------+----------+-------+-------+ | Rank | State | MDS | Activity | dns | inos | +------+--------+----------+----------+-------+-------+ | 0 | **rejoin** | ncn-m003 | | 0 | 0 | +------+--------+----------+----------+-------+-------+ +-----------------+----------+-------+-------+ | Pool | type | used | avail | +-----------------+----------+-------+-------+ | cephfs_metadata | metadata | 781M | 20.7T | | cephfs_data | data | 117M | 20.7T | +-----------------+----------+-------+-------+ +-------------+ | Standby MDS | +-------------+ | ncn-m002 | | ncn-m001 | +-------------+ The rejoin status should turn to active:\ncephfs - 7 clients ====== +------+--------+----------+---------------+-------+-------+ | Rank | State | MDS | Activity | dns | inos | +------+--------+----------+---------------+-------+-------+ | 0 | **active** | ncn-m003 | Reqs: 0 /s | 11.1k | 193 | +------+--------+----------+---------------+-------+-------+ +-----------------+----------+-------+-------+ | Pool | type | used | avail | +-----------------+----------+-------+-------+ | cephfs_metadata | metadata | 781M | 20.7T | | cephfs_data | data | 117M | 20.7T | +-----------------+----------+-------+-------+ +-------------+ | Standby MDS | +-------------+ | ncn-m002 | | ncn-m001 | +-------------+ Ensure that the service is deleted along with the associated PVC.\nThe output for the command below should empty. If an output is displayed, such as in the example below, then the resources have not been deleted.\nncn-m001# kubectl get pvc -n services|grep tftp Example of resources not being deleted in returned output:\ncray-tftp-shared-pvc Bound pvc-315d08b0-4d00-11ea-ad9d-b42e993b7096 5Gi RWX ceph-cephfs-external 29m Optional: Use the following command to delete the associated PVC.\nncn-m001# kubectl -n services delete pvc PVC_NAME Deploy the TFTP service.\nWait for the TFTP pods to come online and verify that the PVC was created.\nncn-m001# loftsman helm upgrade cray-tftp loftsman/cray-tftp Deploy the iPXE service.\nThis may take a couple of minutes and the pod may initially show up in error state. Wait a couple minutes and it will go to running state.\nncn-m001# loftsman helm upgrade cms-ipxe loftsman/cms-ipxe Log into the iPXE pod and verify the iPXE file was created.\nThis may take another couple of minutes while it is creating the files.\nFind the iPXE pod ID.\nncn-m001# kubectl get pods -n services --no-headers -o wide | grep cray-ipxe | awk \u0026#39;{print $1}\u0026#39; Log into the pod using the iPXE pod ID.\nncn-m001# kubectl exec -n services -it IPXE_POD_ID /bin/sh To see the containers in the pod:\nncn-m001# kubectl describe pod/CRAY-IPXE_POD_NAME -n services Log into the TFTP pods and verify it is seeing the correct file size.\nFind the TFTP pod ID.\nncn-m001# kubectl get pods -n services --no-headers -o wide | grep cray-tftp | awk \u0026#39;{print $1}\u0026#39; Example output:\ncray-tftp-7dc77f9cdc-bn6ml cray-tftp-7dc77f9cdc-ffgnh cray-tftp-7dc77f9cdc-mr6zd cray-tftp-modprobe-42648 cray-tftp-modprobe-4kmqg cray-tftp-modprobe-4sqsk cray-tftp-modprobe-hlfcc cray-tftp-modprobe-r6bvb cray-tftp-modprobe-v2txr Log into the pod using the TFTP pod ID.\nncn-m001# kubectl exec -n services -it TFTP_POD_ID /bin/sh Change to the /var/lib/tftpboot directory.\npod# cd /var/lib/tftpboot Check the ipxe.efi file size on the TFTP servers.\nIf there are any issues, the file will have a size of 0 bytes.\npod# ls -l Example output:\ntotal 1919 -rw-r--r-- 1 root root 980768 May 15 16:49 debug.efi -rw-r--r-- 1 root root 983776 May 15 16:50 ipxe.efi "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/examine_a_uai_using_a_direct_administrative_command/",
	"title": "Examine a UAI Using a Direct Administrative Command",
	"tags": [],
	"description": "",
	"content": "Examine a UAI Using a Direct Administrative Command Print out information about a UAI.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway. The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host. The HPE Cray EX System CLI must be configured (initialized with cray init command) to reach the HPE Cray EX System API Gateway. The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command). The administrator must know the UAI Name of the target UAI; See List UAIs. Procedure Print out information about a UAI.\nTo examine an existing UAI use a command of the following form:\nlinux# cray uas admin uais describe \u0026lt;uai-name\u0026gt; For example:\nncn-m001-pit# cray uas admin uais describe uai-broker-07624d65 Example output:\nuai_age = \u0026#34;5h33m\u0026#34; uai_connect_string = \u0026#34;ssh broker@34.136.140.107\u0026#34; uai_host = \u0026#34;ncn-w003\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-broker:1.2.4\u0026#34; uai_ip = \u0026#34;34.136.140.107\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-broker-07624d65\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;broker\u0026#34; [uai_portmap] Top: User Access Service (UAS)\nNext Topic: Deleting a UAI\n"
},
{
	"uri": "/docs-csm/en-12/install/prepare_configuration_payload/",
	"title": "Prepare Configuration Payload",
	"tags": [],
	"description": "",
	"content": "Prepare Configuration Payload The configuration payload consists of the information which must be known about the HPE Cray EX system so it can be passed to the csi (Cray Site Init) program during the CSM installation process.\nInformation gathered from a site survey is needed to feed into the CSM installation process, such as system name, system size, site network information for the CAN, site DNS configuration, site NTP configuration, network information for the node used to bootstrap the installation. More detailed component level information about the system hardware is encapsulated in the SHCD (Shasta Cabling Diagram), which is a spreadsheet prepared by HPE Cray Manufacturing to assemble the components of the system and connect appropriately labeled cables.\nHow the configuration payload is prepared depends on whether this is a first time installation of CSM software on this system or the CSM software is being reinstalled. The reinstall scenario has the advantage of being able to use the configuration payload from the previous CSM installation and an additional configuration file which that installation generated. The first time install scenario requires passing command-line arguments to CSI, as well as the creation of a number of Configuration Payload Files.\nTopics Command Line Configuration Payload Configuration Payload Files First Time Install Reinstall Next Topic Command Line Configuration Payload The information from a site survey can be given to the csi command as command line arguments. The information and options shown below are to explain what data is needed. It will not be used until moving to the Bootstrap PIT Node procedure.\nThe air-cooled cabinet is known to csi as a river cabinet. The liquid-cooled cabinets are either mountain or hill (if a TDS system).\nFor more description of these settings and the default values, see Default IP Address Ranges and the other topics in CSM Overview. There are additional options not shown on this page that can be seen by running csi config init --help.\nCSI option Information --bootstrap-ncn-bmc-user root Administrative account for the management node BMCs --bootstrap-ncn-bmc-pass changeme Password for bootstrap-ncn-bmc-user account --system-name eniac Name of the HPE Cray EX system --mountain-cabinets 4 Number of Mountain cabinets, but this could also be in cabinets.yaml --starting-mountain-cabinet 1000 Starting Mountain cabinet ID --hill-cabinets 0 Number of Hill cabinets, but this could also be in cabinets.yaml --river-cabinets 1 Number of River cabinets, but this could also be in cabinets.yaml --can-cidr 10.103.11.0/24 IP subnet for the CAN assigned to this system --can-gateway 10.103.11.1 Virtual IP address for the CAN (on the spine switches) --can-static-pool 10.103.11.112/28 MetalLB static pool on CAN --can-dynamic-pool 10.103.11.128/25 MetalLB dynamic pool on CAN --cmn-cidr 10.103.12.0/24 IP subnet for the CMN assigned to this system --cmn-external-dns 10.103.12.113 IP address on CMN for this system\u0026rsquo;s DNS server --cmn-gateway 10.103.12.1 Virtual IP address for the CMN (on the spine switches) --cmn-static-pool 10.103.12.112/28 MetalLB static pool on CMN --cmn-dynamic-pool 10.103.12.128/25 MetalLB dynamic pool on CMN --hmn-cidr 10.254.0.0/17 Override the default cabinet IPv4 subnet for River HMN --nmn-cidr 10.252.0.0/17 Override the default cabinet IPv4 subnet for River NMN --hmn-mtn-cidr 10.104.0.0/17 Override the default cabinet IPv4 subnet for Mountain HMN --nmn-mtn-cidr 10.100.0.0/17 Override the default cabinet IPv4 subnet for Mountain NMN --ntp-pools time.nist.gov External NTP pool(s) for this system to use --site-domain dev.cray.com Domain name for this system --site-ip 172.30.53.79/20 IP address and netmask for the PIT node lan0 connection --site-gw 172.30.48.1 Gateway for the PIT node to use --site-nic p1p2 NIC on the PIT node to become lan0 --site-dns 172.30.84.40 Site DNS servers to be used by the PIT node --install-ncn-bond-members p1p1,p10p1 NICs on each management node to become bond0 --application-node-config-yaml application_node_config.yaml Name of application_node_config.yaml --cabinets-yaml cabinets.yaml Name of cabinets.yaml --primary-server-name primary Desired name for the primary DNS server --secondary-servers \u0026quot;\u0026quot; Comma-separated list of FQDN/IP for all DNS servers to be notified on DNS zone update --notify-zones \u0026quot;\u0026quot; A comma-separated list of DNS zones to transfer --k8s-api-auditing-enabled Enable Kubernetes API audit logging --ncn-mgmt-node-auditing-enabled Enable host audit logging This is a long list of options. It can be helpful to create a Bash script file to call the csi command with all of these options, and then edit that file to adjust the values for the particular system being installed.\nThe bootstrap-ncn-bmc-user and bootstrap-ncn-bmc-pass must match what is used for the BMC account and its password for the management nodes.\nSet site parameters (site-domain, site-ip, site-gw, site-nic, site-dns) for the information which connects ncn-m001 (the PIT node) to the site. The site-nic is the interface on this node connected to the site.\nThere are other interfaces possible, but the install-ncn-bond-members are typically:\np1p1,p10p1 for HPE nodes p1p1,p1p2 for Gigabyte nodes p801p1,p801p2 for Intel nodes The starting cabinet number for each type of cabinet (for example, starting-mountain-cabinet) has a default that can be overridden. See the csi config init --help output for more information.\nAn override to default cabinet IPv4 subnets can be made with the hmn-mtn-cidr and nmn-mtn-cidr parameters.\nSeveral parameters (can-gateway, can-cidr, can-static-pool, can-dynamic-pool) describe the Customer Access Network (CAN).\nThe can-gateway is the common gateway IP address used for both spine switches and commonly referred to as the Virtual IP address for the CAN. The can-cidr is the IP subnet for the CAN assigned to this system. The can-static-pool and can-dynamic-pool are the MetalLB address static and dynamic pools for the CAN. Several parameters (cmn-gateway, cmn-cidr, cmn-static-pool, cmn-dynamic-pool) describe the Customer Management Network (CMN).\nThe cmn-gateway is the common gateway IP address used for both spine switches and commonly referred to as the Virtual IP address for the CMN. The cmn-cidr is the IP subnet for the CMN assigned to this system. The cmn-static-pool and cmn-dynamic-pool are the MetalLB address static and dynamic pools for the CMN. The cmn-external-dns is the static IP address assigned to the DNS instance running in the cluster to which requests the cluster subdomain will be forwarded. The cmn-external-dns IP address must be within the cnn-static-pool range. Set ntp-pool to a reachable NTP server.\nThe application_node_config.yaml file is required. It is used to describe the mapping between prefixes in hmn_connections.csv and HSM subroles. This file also defines aliases for application nodes.\nSee Create Application Node YAML. For systems that use non-sequential cabinet ID numbers, use cabinets-yaml to include the cabinets.yaml file.\nThis file can include information about the starting ID for each cabinet type and number of cabinets which have separate command line options. It also is a way to specify explicitly the ID of every cabinet in the system. See Create Cabinets YAML. The PowerDNS zone transfer arguments primary-server-name, secondary-servers, and notify-zones are optional unless zone transfer is being configured.\nSee the PowerDNS Configuration Guide. Use --k8s-api-auditing-enabled=true to enable Kubernetes API audit logging, and use --ncn-mgmt-node-auditing-enabled=true to enable host audit logging. See Audit Logs for more information.\nConfiguration Payload Files A few configuration files are needed for the installation of CSM. These are all provided to the csi command during the installation process.\nFilename Source Information cabinets.yaml SHCD The number and type of air-cooled and liquid-cooled cabinets. cabinet IDs, and VLAN numbers application_node_config.yaml SHCD The number and type of application nodes with mapping from the name in the SHCD to the desired hostname hmn_connections.json SHCD The network topology for HMN of the entire system ncn_metadata.csv SHCD, other The number of master, worker, and storage nodes and MAC address information for BMC and bootable NICs switch_metadata.csv SHCD Inventory of all spine, leaf, CDU, and leaf-bmc switches Although some information in these files can be populated from site survey information, the SHCD prepared by HPE Cray Manufacturing is the best source of data for hmn_connections.json. The ncn_metadata.csv does require collection of MAC addresses from the management nodes because that information is not present in the SHCD.\ncabinets.yaml The cabinets.yaml file describes the type of cabinets in the system, the number of each type of cabinet, and the starting cabinet ID for every cabinet in the system. This file can be used to indicate that a system has non-contiguous cabinet ID numbers or non-standard VLAN numbers.\nThe component names (xnames) used in the other files should fit within the cabinet IDs defined by the starting cabinet ID for River cabinets (modified by the number of cabinets). It is OK for management nodes not to be in x3000 (as the first River cabinet), but they must be in one of the River cabinets. For example, x3000 with two cabinets would mean x3000 or x3001 should have all management nodes.\nSee Create Cabinets YAML for instructions about creating this file.\napplication_node_config.yaml The application_node_config.yaml file controls how the csi config init command finds and treats application nodes discovered in the hmn_connections.json file when building the SLS Input file.\nDifferent node prefixes in the SHCD can be identified as Application nodes. Each node prefix can be mapped to a specific HSM sub role. These sub roles can then be used as the targets of Ansible plays run by CFS to configure these nodes. The component name (xname) for each Application node can be assigned one or more hostname aliases.\nSee Create Application Node YAML for instructions about creating this file.\nhmn_connections.json The hmn_connections.json file is extracted from the HMN tab of the SHCD spreadsheet. The CSM release includes the hms-shcd-parser container; this container can do the extraction on the PIT node booted from the LiveCD (RemoteISO or USB device) or on a Linux system. Although some information in these files can be populated from site survey information, the SHCD prepared by HPE Cray Manufacturing is the best source of data for hmn_connections.json.\nNo action is required to create this file at this point, and it will be created when the PIT node is bootstrapped.\nncn_metadata.csv The information in the ncn_metadata.csv file identifies each of the management nodes, assigns the function as a master, worker, or storage node, and provides the MAC address information needed to identify the BMC and the NIC which will be used to boot the node.\nFor each management node, the component name (xname), role, and subrole can be extracted from the SHCD. However, the rest of the MAC address information needs to be collected another way. Collect as much information as possible before the PIT node is booted from the LiveCD and then get the rest later when directed. See the scenarios which enable partial data collection below in First Time Install.\nSee Create NCN Metadata CSV for instructions about creating this file.\nswitch_metadata.csv The switch_metadata.csv file is manually created to include information about all spine, leaf, CDU, and leaf-bmc switches in the system. None of the Slingshot switches for the HSN should be included in this file.\nSee Create Switch Metadata CSV for instructions about creating this file.\nFirst Time Install The process to install for the first time must collect the information needed to create these files.\nCollect data for cabinets.yaml.\nSee Create Cabinets YAML for instructions about creating this file.\nCollect data for application_node_config.yaml.\nSee Create Application Node YAML for instructions about creating this file.\nCollect data for ncn_metadata.csv.\nSee Create NCN Metadata CSV for instructions about creating this file.\nCollect data for switch_metadata.csv.\nSee Create Switch Metadata CSV for instructions about creating this file.\nReinstall The process to reinstall must have the configuration payload files available.\nCollect Payload for Reinstall.\nThese files from a previous install are needed to do a reinstall.\napplication_node_config.yaml (if used previously) cabinets.yaml (if used previously) hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml If the system_config.yaml is not available, then a reinstall cannot be done. Switch to the install process and generate any of the other files for the Configuration Payload Files which are missing.\nThe command line options used to call csi config init are not needed.\nWhen doing a reinstall, all of the command line options which had been given to csi config init during the previous installation will be found inside the system_config.yaml file. This simplifies the reinstall process.\nWhen you are ready to bootstrap the LiveCD, it will indicate when to run this command without any extra command line options. It will expect to find all of the above files in the current working directory.\nlinux# csi config init Next Topic After completing this procedure the next step is to prepare the management nodes. See Prepare Management Nodes\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/troubleshoot_insufficient_standby_mds_daemons_available/",
	"title": "Troubleshoot Insufficient Standby MDS Daemons Available",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Insufficient Standby MDS Daemons Available Procedure Log into a node running ceph-mon. Typically this will be ncn-s001/2/3.\nCheck the ceph health.\nceph health detail Example Output:\nHEALTH_WARN insufficient standby MDS daemons available [WRN] MDS_INSUFFICIENT_STANDBY: insufficient standby MDS daemons available have 0; want 1 more This output explicitly states that you need at least 1 more to clear the alert.\nDetermine which MDS daemons are down.\nceph orch ps --daemon_type mds Example Output:\nNAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID mds.cephfs.ncn-s001.lhoocr ncn-s001 stopped 4m ago 18h \u0026lt;unknown\u0026gt; registry.local/ceph/ceph:v15.2.8 \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; mds.cephfs.ncn-s002.nywheq ncn-s002 stopped 4m ago 18h \u0026lt;unknown\u0026gt; registry.local/ceph/ceph:v15.2.8 \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; mds.cephfs.ncn-s003.jdufcg ncn-s003 running (18h) 4m ago 18h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 4df61111d738 IMPORTANT: Depending on the configuration and the number of MDS daemons, the number of MDS daemons in a stopped or error state may vary.\nStart the stopped MDS daemon(s).\nceph orch daemon start \u0026lt;MDS daemon name\u0026gt; Repeat for each stopped MDS daemon.\nCheck the status of the cluster.\nceph health detail Expected Output:\nHEALTH_OK IMPORTANT: If the daemon is not starting using the method above, please refer to Manage Ceph Services\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/create_a_service_account_in_keycloak/",
	"title": "Create a Service Account in Keycloak",
	"tags": [],
	"description": "",
	"content": "Create a Service Account in Keycloak Set up a Keycloak service account using the Keycloak administration console or the Keycloak REST API. A service account can be used to get a long-lived token that is used by automation tools.\nIn Keycloak, service accounts are associated with a client. See Service Accounts for more information from the Keycloak documentation.\nProcedure Follow the steps in only one of the following sections, depending on if it is preferred to use the Keycloak REST API or the Keycloak administration console UI.\nUse the Keycloak administration console UI Log in to the administration console.\nSee Access the Keycloak User Management UI for more information.\nClick Clients under the Configure header of the navigation panel on the left side of the page.\nClick the Create button at the top-right of the Clients table.\nEnter a Client ID for the new client.\nThe Client Protocol must be openid-connect and the Root URL can be left blank.\nClick the Save button.\nCustomize the new client.\nOnce the client is created, a new screen is displayed with more details for the client.\nChange the Access Type to confidential.\nChange Stand Flow Enabled to OFF.\nChange Direct Access Grants Enabled to OFF.\nChange Service Accounts Enabled to ON.\nClick the Save button.\nAssign a role to the client for authorization.\nSwitch to the Mappers tab for the new client.\nClick the Create button at the top-right of the Mappers table.\nA new form is displayed that asks for details for the mapper.\nEnter a name.\nIn the image above, the example name is admin-role.\nChange the Mapper Type to Hardcoded Role.\nSet the Role to shasta.admin.\nClick the Save button.\nUse the Keycloak REST API Create the get_master_token function to get a token as a Keycloak master administrator.\nncn-mw# MASTER_USERNAME=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.user}\u0026#39; | base64 -d) ncn-mw# MASTER_PASSWORD=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d) ncn-mw# SITE_DOMAIN=\u0026#34;$(craysys metadata get site-domain)\u0026#34; ncn-mw# SYSTEM_NAME=\u0026#34;$(craysys metadata get system-name)\u0026#34; ncn-mw# AUTH_FQDN=\u0026#34;auth.cmn.${SYSTEM_NAME}.${SITE_DOMAIN}\u0026#34; ncn-mw# function get_master_token { curl -ks -d client_id=admin-cli -d username=\u0026#34;${MASTER_USERNAME}\u0026#34; -d password=\u0026#34;${MASTER_PASSWORD}\u0026#34; \\ -d grant_type=password \u0026#34;https://${AUTH_FQDN}/keycloak/realms/master/protocol/openid-connect/token\u0026#34; | \\ jq -r .access_token } Create the client by doing a POST call for a JSON object.\nThe clientId should be changed to the name for the new service account.\nncn-mw# curl -is -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39; { \u0026#34;clientId\u0026#34;: \u0026#34;my-test-client\u0026#34;, \u0026#34;standardFlowEnabled\u0026#34;: false, \u0026#34;implicitFlowEnabled\u0026#34;: false, \u0026#34;directAccessGrantsEnabled\u0026#34;: false, \u0026#34;serviceAccountsEnabled\u0026#34;: true, \u0026#34;publicClient\u0026#34;: false, \u0026#34;protocolMappers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;admin-role\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;openid-connect\u0026#34;, \u0026#34;protocolMapper\u0026#34;: \u0026#34;oidc-hardcoded-role-mapper\u0026#34;, \u0026#34;consentRequired\u0026#34;: false, \u0026#34;config\u0026#34;: { \u0026#34;role\u0026#34;: \u0026#34;shasta.admin\u0026#34; } } ] } \u0026#39; \\ \u0026#34;https://${AUTH_FQDN}/keycloak/admin/realms/shasta/clients\u0026#34; Output similar to the following is expected:\nHTTP/2 201 location: https://auth.cmn.system1.us.cray.com/keycloak/admin/realms/shasta/clients/bd8084d2-08bf-45cb-ab94-ee81e39921be content-length: 0 "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/customize_disk_hardware/",
	"title": "Customize PCIe Hardware",
	"tags": [],
	"description": "",
	"content": "Customize PCIe Hardware This page will assist an admin with changing the kernel parameters for NCNs that have extra disks.\nNOTE: If a system\u0026rsquo;s hardware is Plan of Record (PoR), then this page is not needed.\nFor any procedure below, it is assumed that the extra disks are going to be utilized. If they are undesired, then the only action item to do is to yank/remove/pull the disks from the NCN.\nProcedure Masters \u0026amp; Workers Add the kernel parameter metal.disks=X where X is the number of extra disks on the NCN. This parameter may be added to the PIT\u0026rsquo;s boot scripts or to Boot Script Service (BSS).\nFor the PIT:\nEdit the scripts for the master(s) and/or worker(s) in /var/www/ncn-* with this kernel parameter (the snippet below uses workers with 1 extra disk as an example):\npit# for script in /var/www/ncn-w*/script.ipxe; do sed -i \u0026#39;s/append/append metal.disks=3/\u0026#39; $script; done Update BSS with the kernel parameter (the snippet below requires the xname of the NCN to limit the operation too):\nncn-m001# csi handoff bss-update-param --limit x3000c0s3b0n0 --kernel metal.disk=3 Storage There is nothing to do here. If there are extra disks they will be consumed by the CEPH installer.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/configuration_of_ncn_bonding/",
	"title": "Configuration of NCN Bonding",
	"tags": [],
	"description": "",
	"content": "Configuration of NCN Bonding Non-compute nodes (NCNs) have network interface controllers (NICs) connected to the management network that are configured in a redundant manner via Link Aggregation Control Protocol (LACP) link aggregation. The link aggregation configuration can be modified by editing and applying various configuration files either through Ansible or the interfaces directly.\nThe bond configuration exists across three files on an NCN. These files may vary depending on the NCN in use:\nFile Description ifcfg-bond0 Physical configuration and member interfaces. ifroute-bond0 Routing, which is critical for NCN PXE to work. ifrule-bond0 Routing table selecting, which is critical for NCN PXE to work. The following is an example of the contents of the ifcfg-bond0 file:\nBONDING_MASTER=\u0026#39;yes\u0026#39; BONDING_MODULE_OPTS=\u0026#39;mode=802.3ad miimon=100 lacp_rate=fast xmit_hash_policy=layer2+3\u0026#39; BONDING_SLAVE0=\u0026#39;p1p1\u0026#39; BONDING_SLAVE1=\u0026#39;p1p2\u0026#39; MTU=\u0026#39;9238\u0026#39; STARTMODE=\u0026#39;auto\u0026#39; BOOTPROTO=\u0026#39;static\u0026#39; PREFIXLEN=\u0026#39;16\u0026#39; IPADDR=\u0026#39;10.1.1.1/16\u0026#39; ZONE=\u0026#39;Do not assign ZONE\u0026#39; The bond is configured with modules that can be changed at the network administrator\u0026rsquo;s discretion and coordination.\nIt may be useful to only adjust the XMIT value to layer2; the current setting is chosen as a default to match existing settings for compute nodes from previous releases. This can be weighed out if problems arise across the NCNs over the bond and dual-spine or Multi-Chassis Link Aggregation (MLAG).\nWicked NetworkManager Wicked is the SUSE NetworkManager and Daemon wrapper for handling interfaces\u0026rsquo; processes and applying their configuration. For more information, see the SUSE Wicked external documentation.\nFor administrators familiar with Ubuntu Linux, it has an analogue to Wicked called NetPlan. The benefit is that it removes tedious, low-level configurations. However, Wicked and NetPlan each have their own web of configuration. The examples below are useful ways Wicked can be used to debug and triage interfaces.\nTo view a system-wide interface network configuration:\nncn-w001# wicked ifstatus all Use the following command to view information about a specific interface. In this example, bond0.cmn0 on ncn-w001 is used.\nncn-w001# wicked ifstatus --verbose bond0.cmn0 Example output:\nbond0.cmn0 up link: #4603, state up, mtu 1500 type: vlan bond0[7], hwaddr b8:59:9f:c7:11:12 control: none config: compat:suse:/etc/sysconfig/network/ifcfg-bond0.cmn0, uuid: 5cce4d33-8d99-50a2-b6c0-b4b3d101c557 leases: ipv4 static granted addr: ipv6 fe80::ba59:9fff:fec7:1112/64 scope link addr: ipv4 10.102.3.4/24 brd 10.102.3.4 scope universe label bond0.cmn0 [static] route: ipv4 0.0.0.0/0 via 10.102.3.20 dev bond0.cmn0 type unicast table 3 scope universe protocol boot route: ipv4 10.102.3.0/24 type unicast table main scope link protocol kernel pref-src 10.102.3.4 route: ipv6 fe80::/64 type unicast table main scope universe protocol kernel priority 256 To view information about the bond:\nncn-w001# wicked ifstatus bond0 Example output:\nbond0 device-not-running link: #9, state up, mtu 9238 type: bond, mode ieee802-3ad, hwaddr b8:59:9f:4a:f6:30 config: compat:suse:/etc/sysconfig/network/ifcfg-bond0 leases: ipv4 static failed "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/loopback/",
	"title": "Loopback interface",
	"tags": [],
	"description": "",
	"content": "Loopback interface You can think of loopbacks as internal virtual interfaces. Loopback interfaces are not bound to a physical port and are used for device management and routing protocols.\nRelevant Configuration\nCreate a loopback interface. Run:\nswitch (config)# interface loopback 2 switch (config interface loopback 2)# Configure an IP address on the loopback interface. Run:\nswitch (config interface loopback 2)# ip address 20.20.20.20 /32 Show Commands to Validate Functionality\nswitch# show interfaces loopback 2 Expected Results\nStep 1: You can create a loopback interface Step 2: You can give a loopback interface an IP address Step 3: You can validate the configuration using the show commands. Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/vlan/",
	"title": "Configure Virtual Local Access Networks (VLANs)",
	"tags": [],
	"description": "",
	"content": "Configure Virtual Local Access Networks (VLANs) VLANs allow for the logical grouping of switch interfaces, enabling communication as if all connected devices were on the same isolated network.\nConfiguration Commands Create VLAN:\nswitch(config)# interface vlan \u0026lt;VLAN\u0026gt; Show commands to validate functionality:\nswitch# show vlan [VLAN] Expected Results Administrators can create a VLAN Administrators can assign a VLAN to the physical interface Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/key_features/",
	"title": "Key Features Used in the Management Network Configuration",
	"tags": [],
	"description": "",
	"content": "Key Features Used in the Management Network Configuration The following is a list of key features used in the management network configuration.\nFeature List Feature Notes Description VSX MLAG Layer 2 Redundancy, Allows the NCNs to be bonded so if one link fails they can continue to operate. VSX Layer 3 Redundancy, Allows one Spine switch/default gateway to fail and continue to work Lacp fallback Allows for LACP links to come up individually without LACP PDUs, used for PXE booting the NCNs. Vlan Segregates layer 2 broadcast domains, need to separate NMN/HMN/compute traffic. MSTP Layer 2 loop prevention mechanism at edge IP routing IP routing / static routes OSPF Routing protocol used to peer from Leaf switches to Spines BGP Routing protocol used to peer with MetalLB Prefix-Lists Lists to match components of an IP route Route-Maps Defines which route are redistributed NTP Network Time Protocol ACLs Access Control Lists Max MTU - 9198 Max Transmission Unit/Maximum Frame size SNMP Allows for device polling from the NCNs to map out interfaces VRF Virtual routing and forwarding, used to segregate traffic between networks Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/repopulate_data_in_etcd_clusters_when_rebuilding_them/",
	"title": "Repopulate Data in etcd Clusters When Rebuilding Them",
	"tags": [],
	"description": "",
	"content": "Repopulate Data in etcd Clusters When Rebuilding Them When an etcd cluster is not healthy, it needs to be rebuilt. During that process, the pods that rely on etcd clusters lose data. That data needs to be repopulated in order for the cluster to go back to a healthy state.\nApplicable services Prerequisites Procedures BOS BSS CPS CRUS FAS HMNFD MEDS REDS Applicable services The following services need their data repopulated in the etcd cluster:\nBoot Orchestration Service (BOS) Boot Script Service (BSS) Content Projection Service (CPS) Compute Rolling Upgrade Service (CRUS) External DNS Firmware Action Service (FAS) HMS Notification Fanout Daemon (HMNFD) Mountain Endpoint Discovery Service (MEDS) River Endpoint Discovery Service (REDS) Prerequisites An etcd cluster was rebuilt. See Rebuild Unhealthy etcd Clusters.\nProcedures BOS BSS CPS CRUS FAS HMNFD MEDS REDS BOS Reconstruct boot session templates for impacted product streams to repopulate data.\nBoot preparation information for other product streams can be found in the following locations:\nUANs: Refer to the UAN product stream repository and search for the \u0026ldquo;PREPARE UAN BOOT SESSION TEMPLATES\u0026rdquo; header in the \u0026ldquo;Install and Configure UANs\u0026rdquo; procedure. Cray Operating System (COS): Refer to the \u0026ldquo;Create a Boot Session Template\u0026rdquo; header in the \u0026ldquo;Boot COS\u0026rdquo; procedure in the COS product stream documentation. BSS Data is repopulated in BSS when the REDS init job is run.\nGet the current REDS job.\nncn-mw# kubectl get -o json -n services job/cray-reds-init | jq \u0026#39;del(.spec.template.metadata.labels[\u0026#34;controller-uid\u0026#34;], .spec.selector)\u0026#39; \u0026gt; cray-reds-init.json Delete the reds-client-init job.\nncn-mw# kubectl delete -n services -f cray-reds-init.json Restart the reds-client-init job.\nncn-mw# kubectl apply -n services -f cray-reds-init.json CPS Repopulate clusters for CPS.\nIf there are no clients using CPS when the etcd cluster is rebuilt, then nothing needs to be done other than to rebuild the cluster and make sure all of the components are up and running. See Rebuild Unhealthy etcd Clusters for more information. If any clients have already mounted content provided by CPS, that content should be unmounted before rebuilding the etcd cluster, and then re-mounted after the etcd cluster is rebuilt. Compute nodes that use CPS to access their root file system must be shut down to unmount, and then booted to perform the re-mount. CRUS Note: CRUS is deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. It will be replaced with BOS V2, which will provide similar functionality. See Deprecated features.\nView the progress of existing CRUS sessions.\nList the existing CRUS sessions to find the upgrade_id for the desired session.\nncn# cray crus session list --format toml Example output:\n[[results]] api_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-nodes\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [ \u0026#34;Quiesce requested in step 0: moving to QUIESCING\u0026#34;, \u0026#34;All nodes quiesced in step 0: moving to QUIESCED\u0026#34;, \u0026#34;Began the boot session for step 0: moving to BOOTING\u0026#34;,] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;e0131663-dbee-47c2-aa5c-13fe9b110242\u0026#34; \u0026lt;\u0026lt;-- Note this value upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;upgrading-nodes\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; Describe the CRUS session to see if the session failed or is stuck.\nIf the session continued and appears to be in a healthy state, proceed to the BSS section.\nncn# cray crus session describe CRUS_UPGRADE_ID --format toml Example output:\napi_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-nodes\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [ \u0026#34;Quiesce requested in step 0: moving to QUIESCING\u0026#34;, \u0026#34;All nodes quiesced in step 0: moving to QUIESCED\u0026#34;, \u0026#34;Began the boot session for step 0: moving to BOOTING\u0026#34;,] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;e0131663-dbee-47c2-aa5c-13fe9b110242\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;upgrading-nodes\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; Find the name of the running CRUS pod.\nncn# kubectl get pods -n services | grep cray-crus Example output:\ncray-crus-549cb9cb5d-jtpqg 3/4 Running 528 25h Restart the CRUS pod.\nDeleting the pod will restart CRUS and start the discovery process for any data recovered in etcd.\nncn# kubectl delete pods -n services POD_NAME FAS Reload the firmware images from Nexus.\nRefer to the Load Firmware from Nexus section in FAS Admin Procedures for more information.\nWhen the etcd cluster is rebuilt, all historic data for firmware actions and all recorded snapshots will be lost. Image data will be reloaded from Nexus. Any images that were loaded into FAS outside of Nexus will need to be reloaded using the Load Firmware from RPM or ZIP file section in FAS Admin Procedures. After images are reloaded, any running actions at time of failure will need to be recreated.\nHMNFD Resubscribe the compute nodes and any NCNs that use the ORCA daemon for their State Change Notifications (SCN).\nResubscribe all compute nodes.\nncn-m# TMPFILE=$(mktemp) ncn-m# sat status --no-borders --no-headings | grep Ready | grep Compute | awk \u0026#39;{printf(\u0026#34;nid%06d-nmn\\n\u0026#34;,$4);}\u0026#39; \u0026gt; \u0026#34;${TMPFILE}\u0026#34; ncn-m# pdsh -w ^\u0026#34;${TMPFILE}\u0026#34; \u0026#34;systemctl restart cray-orca\u0026#34; ncn-m# rm -rf \u0026#34;${TMPFILE}\u0026#34; Resubscribe the NCNs.\nNOTE: Modify the -w arguments in the following commands to reflect the number of worker and storage nodes in the system.\nncn-m# pdsh -w ncn-w00[1-4]-can.local \u0026#34;systemctl restart cray-orca\u0026#34; ncn-m# pdsh -w ncn-s00[1-4]-can.local \u0026#34;systemctl restart cray-orca\u0026#34; MEDS Restart MEDS.\nncn-mw# kubectl -n services delete pods --selector=\u0026#39;app.kubernetes.io/name=cray-meds\u0026#39; REDS Restart REDS.\nncn-mw# kubectl -n services delete pods --selector=\u0026#39;app.kubernetes.io/name=cray-reds\u0026#39; "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/specifying_hosts_and_groups/",
	"title": "Specifying Hosts and Groups",
	"tags": [],
	"description": "",
	"content": "Specifying Hosts and Groups When using the Configuration Framework Service (CFS), there are many steps where users may need to specify the hosts that CFS should configure. This can be done by specifying individual hosts, or groups of hosts. There are several places where a user may need to provide this information, particularly groups, and depending on where this information is provided, the behavior can change greatly.\nInventories CFS has multiple options for generating inventories, but regardless of which option is used, the information is then converted into an Ansible inventory/hosts file. The inventory is the list of components that Ansible can run against. Anything not in the inventory is unknown to Ansible. Components in an inventory can be placed into groups so that they can be easily referenced together either in the Ansible code or when providing a limit to CFS.\nFor more information on Ansible inventory, see the official Ansible Inventory Documentation.\nHosts Within Ansible plays, it is possible to target different hosts and groups. These hosts and groups must exist in the inventory when Ansible is run. Combined with the inventory, this will determine which hosts have tasks run. For more information, see the Ansible Hosts Documentation.\nLimits The limit parameter is a way of restricting a run to a smaller set of hosts. Users can specify hosts or groups, or combinations of the two, but no new information can be added at this point. Hosts or groups that do not appear in the inventory will still not be configured, and likewise it is not possible to change the behavior of any parts of the play. Ansible will target the same groups for each task that it would have before, but now with a more limited inventory.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/session_templates/",
	"title": "BOS Session Templates",
	"tags": [],
	"description": "",
	"content": "BOS Session Templates Session templates in the Boot Orchestration Service (BOS) are a reusable collection of boot, configuration, and component information. After creation they can be combined with a boot operation to create a BOS session that will apply the desired changes to the specified components. Session templates can be created via the API by providing JSON data or via the CLI by writing the JSON data to a file, which can then be referenced using the --file parameter.\nSession template structure Boot sets Boot artifacts Specifying nodes Node list Node groups Node roles groups rootfs providers root kernel parameter example Session template structure The following is an example BOS session template:\n{ \u0026#34;name\u0026#34;: \u0026#34;session-template-example\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;session template example\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;boot_set1\u0026#34;: { \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gw-service-nmn.local quiet turbo_boost_limit=999\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x3000c0s19b1n0\u0026#34; ], \u0026#34;etag\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/e06530f1-fde2-4ca5-9148-7e84f4857d17/manifest_sans_boot_parameters.json\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;66666666:dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;boot_set2\u0026#34;: { \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gw-service-nmn.local quiet turbo_boost_limit=999\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x3000c0s21b1n0\u0026#34;, \u0026#34;x3000c0s22b1n0\u0026#34; ], \u0026#34;etag\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/f17631a1-fed1-5cb5-0aa8-7aaaf4123411/manifest.json\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;66666666:dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;cfs\u0026#34;: { \u0026#34;configuration\u0026#34;: \u0026#34;example-configuration\u0026#34; }, \u0026#34;enable_cfs\u0026#34;: true, } The description field is an optional text description of the template. The node_list field (under boot_sets) is a list of individual node component names (xnames). The etag field is used to identify the version of the manifest.json file in S3. The path field is the path to the manifest.json file in S3. The type field is the type of storage where the boot image resides. The configuration field (under cfs) is the name of the Configuration Framework Service (CFS) configuration to apply. The enable_cfs field indicates whether or not CFS should be invoked. Boot sets BOS session templates contain one or more boot sets, which each contain information on the kernel parameters that nodes should boot with, as well as information on the nodes the boot set should apply to. Optionally, with BOS v2, configuration information can also be overwritten on a per boot set basis.\nBoot artifacts Boot sets specify a set of parameters that point to a manifest.json file stored in the Simple Storage Service (S3). This file is created by the Image Management Service (IMS) and contains links to all of the boot artifacts. The following S3 parameters are used to specify this file:\ntype: This is the type of storage used. Currently, the only allowable value is s3. path: This is the path to the manifest.json file in S3. The path will follow the s3://\u0026lt;BUCKET_NAME\u0026gt;/\u0026lt;KEY_NAME\u0026gt; format. etag: This entity tag helps identify the version of the manifest.json file. Currently, it issues a warning if the manifest\u0026rsquo;s etag does not match. This can be an empty string, but cannot be left blank. This boot artifact information from the files stored in S3 is then written to the Boot Script Service (BSS) where it is retrieved when these nodes boot.\nSpecifying nodes Each boot set also specifies a set of nodes to be applied to. There are three different ways to specify the nodes. The node_list, node_groups, or node_role values can each be specified as a comma-separated list.\nNode list node_list maps to a list of nodes identified by component names (xnames). NIDs are not supported.\nFor example:\n\u0026#34;node_list\u0026#34;: [\u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;x3000c0s19b1n1\u0026#34;, \u0026#34;x3000c0s19b2n0\u0026#34;] Node groups node_groups maps to a list of groups defined by the Hardware State Manager (HSM). Each group may contain zero or more nodes. Groups can be arbitrarily defined by users.\nFor example:\n\u0026#34;node_groups\u0026#34;: [\u0026#34;green\u0026#34;, \u0026#34;white\u0026#34;, \u0026#34;pink\u0026#34;] To retrieve the current list of HSM groups, run following command:\nncn-mw# cray hsm groups list --format json | jq .[].label For more information on HSM groups, see Manage Component Groups.\nNode roles groups node_roles_groups is a list of groups based on a node\u0026rsquo;s designated role. Each node\u0026rsquo;s role is specified in the HSM database. node_roles_groups also supports node sub-roles, which are specified as a combination of the node role and sub-role (for example, Application_UAN).\nFor example:\n\u0026#34;node_roles_groups\u0026#34;: [\u0026#34;Compute\u0026#34;] The following roles are defined in the HSM database:\nCompute Service System Application Storage Management See HSM Roles and Subroles for more information.\nrootfs providers The rootfs is the root file system.\nrootfs_provider identifies the mechanism that provides the root file system for the node.\nIn the case of the Cray Operating System (COS) image, the rootfs_provider is HPE\u0026rsquo;s Content Projection Service (CPS), which uses HPE\u0026rsquo;s Data Virtualization Service (DVS) to deliver the content. CPS projects the root file system onto the nodes as a SquashFS image. This is provided via an overlay file system which is set up in dracut.\nrootfs_provider_passthrough is a string that is passed through to the provider of the rootfs. This string can contain additional information that the provider will act upon.\nBoth the rootfs_provider and rootfs_provider_passthrough parameters are used to construct the value of the kernel boot parameter root that BOS sends to the node.\nBOS constructs the kernel boot parameter root per the following syntax.\nroot=\u0026lt;Protocol\u0026gt;:\u0026lt;Root FS location\u0026gt;:\u0026lt;Etag\u0026gt;:\u0026lt;RootFS-provider-passthrough parameters\u0026gt; BOS fills in the protocol based on the value provided in rootfs_provider. If BOS does not know the rootfs_provider, then it omits the protocol field. BOS finds the rootfs_provider and etag values in the manifest file in the session template in the boot set. The rootfs_provider_passthrough parameters are appended to the root parameter without modification. They are \u0026ldquo;passed through\u0026rdquo;, as the name implies.\nCurrently, the only rootfs provider that BOS recognizes is cpss3. For more information on cpss3, see Create a Session Template to Boot Compute Nodes with CPS.\nroot kernel parameter example root=craycps-s3:s3://boot-images/b9caaf66-c0b4-4231-aba7-a45f6282b21d/rootfs:f040d70bd6fabaf91838fe4e484563cf-211:dvs:api-gw-service-nmn.local:300:nmn0 The following table explains the different pieces in the preceding example.\nField Example Value Explanation Protocol craycps-s3 The protocol used to mount the root file system, using CPS in this example. rootfs_provider location s3://boot-images/b9caaf66-c0b4-4231-aba7-a45f6282b21d/rootfs The rootfs_provider location is a SquashFS image stored in S3. etag f040d70bd6fabaf91838fe4e484563cf-211 The Etag (entity tag) is the identifier of the SquashFS image in S3. rootfs_provider passthrough parameters dvs:api-gw-service-nmn.local:300:nmn0 These are additional parameters passed through to CPS in this example, which it uses to properly mount the file system. The rootfs_provider_passthrough parameters are explained in the following table.\nParameter Example Explanation Transport dvs Use DVS to project the SquashFS image down to the node. Gateway api-gw-service-nmn.local This is the URL that identifies the gateway where the DVS servers are located. Time-out 300 The number of seconds to wait to establish a contact. Interface nmn0 The IP interface on the node to use to contact the DVS server; This interface must be up to continue booting. Regarding the interface to use for contacting DVS, the possible values are:\nnmn0 \u0026ndash; Ensures that the nmn0 interface is up nmn0,hsn0 \u0026ndash; Ensures that both the nmn0 and hsn0 interfaces are up. This is required for booting over the High Speed Network (HSN). hsn0 \u0026ndash; Ensures that the hsn0 interface is up. The DVS configuration files determine which interface to use (NMN or HSN). However, the CPS dracut ensures the that requested interfaces are up.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/legacy_mode_user-driven_uai_management/",
	"title": "Legacy Mode User-Driven UAI Management",
	"tags": [],
	"description": "",
	"content": "Legacy Mode User-Driven UAI Management In the legacy mode, users create and manage their own UAIs through the Cray CLI. A user may create, list, and delete only UAIs owned by the user. The user may not create a UAI for another user, nor may the user see or delete UAIs owned by another user. Once created, the information describing the UAI gives the user the information needed to reach the UAI using SSH and log into it.\nThe following diagram illustrates a system running with UAIs created in the legacy mode by four users, each of whom has created at least one End-User UAI. Notice that the example user Pat has created two End-User UAIs:\nIn the simplest UAS configuration, there is some number of UAI images available for use in legacy mode and there is a set of volumes defined. In this configuration, when a UAI is created, the user may specify the UAI image to use as an option when creating the UAI, or may allow a default UAI image, if one is assigned, to be used. Every volume defined at the time the UAI is created will be mounted unconditionally in every newly created UAI if this approach is used. This can lead to problems with conflicting volume mount points (see Troubleshoot Duplicate Mount Paths in a UAI) and unresolvable volumes (see Troubleshoot UAI Stuck in ContainerCreating) in some configurations of UAS. Unless UAI classes are used to make UAIs, care must be taken to ensure all volumes have unique mount-path settings and are accessible in the user Kubernetes namespace.\nThe Benefits of Using UAI Classes with Legacy Mode A slightly more sophisticated configuration approach defines a default UAI Class that is always used by legacy mode UAI creation. When this approach is taken, the user can no longer specify the image to use, as it will be supplied by the UAI class, and the volumes mounted in any UAI created in legacy mode will be based on the specified UAI class. As long as volumes do not conflict within the list of volumes in a given UAI class, there is no need to avoid duplicate mount-path settings in the global list of volumes when this approach is used. All other configuration in the default UAI Class will also be applied to all new legacy mode UAIs, so, for example, a site can place timeouts or resource specifications on UAIs by defining them in the default UAI Class.\nThe UAI Classes section provides information on what might go in an End-User UAI Class and what should specifically go in the Non-Brokered UAI Class used in legacy mode.\nTop: User Access Service (UAS)\nNext Topic: Configure A Default UAI Class for Legacy Mode\n"
},
{
	"uri": "/docs-csm/en-12/install/prepare_management_nodes/",
	"title": "Prepare Management Nodes",
	"tags": [],
	"description": "",
	"content": "Prepare Management Nodes The procedures described on this page must be completed before any node is booted with the Cray Pre-Install Toolkit (PIT), which is performed in a later document. When the PIT node is referenced during these procedures, it means the node that will be booted as the PIT node.\nQuiesce compute and application nodes Disable DHCP service Wipe disks on booted nodes Set IPMI credentials Power off booted nodes Set node BMCs to DHCP Wipe USB device on PIT node Power off PIT node Configure DNS Check disk space Quiesce compute nodes and application nodes Skip this section if compute nodes and application nodes are not booted.\nThe compute nodes and application nodes depend on the management nodes to provide services for their runtime environment. For example:\nContent Projection Service (CPS) to project the operating system image, the CPE image, or the Analytics image cray-dns-unbound (internal system DNS) cray-kea (DHCP leases) Access to the API gateway for node heartbeats While the reinstall process happens, these nodes would not be able to function normally. As part of the reinstall, they will be rebooted with new boot images and configuration.\nSee Shut Down and Power Off Compute and User Access Nodes.\nDisable DHCP service Skip this section if none of the management nodes are booted.\nIf doing a reinstall and any of the management nodes are booted, then the DHCP service will need to be disabled before powering off management nodes.\nRuntime DHCP services interfere with the LiveCD\u0026rsquo;s bootstrap nature to provide DHCP leases to BMCs. To remove edge cases, disable the run-time cray-dhcp-kea pod.\nScale the deployment from either the LiveCD or any Kubernetes node:\nncn# kubectl scale -n services --replicas=0 deployment cray-dhcp-kea Wipe disks on booted nodes Skip this section if none of the management nodes are booted.\nIf any of the management nodes are booted with Linux, then they have data from previous installations on them which must be wiped.\nREQUIRED If the above is true, then for each management node (excluding ncn-m001), log in and do a \u0026ldquo;full wipe\u0026rdquo; of the node\u0026rsquo;s disks.\nSee full wipe from Wipe NCN Disks for Reinstallation.\nSet IPMI credentials The upcoming procedures use ipmitool. Set IPMI credentials for the BMCs of the NCNs.\nread -s is used in order to prevent the credentials from being displayed on the screen or recorded in the shell history.\nlinux# USERNAME=root linux# read -s IPMI_PASSWORD linux# export IPMI_PASSWORD Power off booted nodes Skip this section if none of the management nodes are booted.\nPower each NCN off using ipmitool from ncn-m001 (or the booted LiveCD, if reinstalling an incomplete install).\nShut down from LiveCD (pit) Power off NCNs.\npit# conman -q | grep mgmt | grep -v m001 | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power off Check the power status to confirm that the nodes have powered off.\npit# conman -q | grep mgmt | grep -v m001 | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power status Shut down from ncn-m001 Power off NCNs.\nncn-m001# grep ncn /etc/hosts | grep mgmt | grep -v m001 | sort -u | awk \u0026#39;{print $2}\u0026#39; | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power off Check the power status to confirm that the nodes have powered off.\nncn-m001# grep ncn /etc/hosts | grep mgmt | grep -v m001 | sort -u | awk \u0026#39;{print $2}\u0026#39; | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power status Set node BMCs to DHCP Set the BMCs on the management nodes to DHCP. During the install of the management nodes their BMCs get set to static IP addresses. The installation expects these BMCs to be set back to DHCP before proceeding.\nThese steps require that the Set IPMI credentials steps have been performed.\nSet the LAN variable based on NCN hardware type.\nIf NCNs are Intel, set it to 3.\nlinux# LAN=3 For non-Intel nodes, set it to 1.\nlinux# LAN=1 Collect BMC hostnames or IP addresses.\nFrom the LiveCD (pit):\nThis collects BMC IP addresses using the old statics.conf on the system, in case CSI changes IP addresses:\npit# BMCS=$(grep mgmt /etc/dnsmasq.d/statics.conf | grep -v m001 | awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; | grep -Eo \u0026#34;([0-9]{1,3}[.]){3}[0-9]{1,3}\u0026#34; | sort -u | tr \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39;) ; echo $BMCS From ncn-m001:\nCollect BMC hostnames from /etc/hosts:\nncn-m001# BMCS=$(grep -wEo \u0026#34;ncn-[msw][0-9]{3}-mgmt\u0026#34; /etc/hosts | grep -v \u0026#34;m001\u0026#34; | sort -u | tr \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39;) ; echo $BMCS Set the BMCs to DHCP.\nlinux# for h in $BMCS ; do echo \u0026#34;Setting $h to DHCP\u0026#34; ipmitool -U $USERNAME -I lanplus -H $h -E lan set $LAN ipsrc dhcp done Verify that the BMCs have been set to DHCP:\nlinux# for h in $BMCS ; do printf \u0026#34;$h: \u0026#34; ipmitool -U $USERNAME -I lanplus -H $h -E lan print $LAN | grep Source done Perform a cold reset of any BMCs which are still reachable.\nlinux# for h in $BMCS ; do printf \u0026#34;$h: \u0026#34; if ping -c 3 $h \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then printf \u0026#34;Still reachable. Issuing cold reset... \u0026#34; ipmitool -U $USERNAME -I lanplus -H $h -E mc reset cold else echo \u0026#34;Not reachable (DHCP setting appears to be successful)\u0026#34; fi done As long as every BMC is either not reachable or receives a cold reset, this step is successful.\nWipe USB device on PIT node Skip this section if intending to boot the PIT node from a USB device for the install.\nIf the PIT node has previously been booted (either from a USB device or a remote ISO), then it should be wiped in order to avoid problems stemming from leftover LiveCD disk labels.\nWipe LiveCD disk labels with the following command:\nncn-m001# wipefs --all --force /dev/disk/by-label/cow /dev/disk/by-label/PITDATA /dev/disk/by-label/BOOT /dev/disk/by-label/CRAYLIVE Power off PIT node Skip this step if planning to use this node as a staging area to create the USB LiveCD.\nShut down the LiveCD or ncn-m001 node.\nlinux# poweroff Configure DNS If ncn-m001 is being used to prepare the USB LiveCD, remove the Kubernetes IP addresses from /etc/resolv.conf and add a valid external DNS server.\nCheck disk space If ncn-m001 is being used to prepare the USB LiveCD, ensure there is enough free disk space for the CSM tar archive to be downloaded and unpacked.\nNext topic The next step is to bootstrap the PIT node.\nSee Bootstrap PIT Node.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/troubleshoot_large_object_map_objects_in_ceph_health/",
	"title": "Troubleshoot Large Object Map Objects in Ceph Health",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Large Object Map Objects in Ceph Health Troubleshoot an issue where Ceph reports a HEALTH_WARN of 1 large omap objects. Adjust the omap object key threshold or number of placement groups (PG) to resolve this issue.\nPrerequisites Ceph health is reporting a HEALTH_WARN for large Object Map (omap) objects.\nncn-m001# ceph -s Example output:\ncluster: id: 464f8ee0-667d-49ac-a82b-43ba8d377f81 health: HEALTH_WARN 1 large omap objects clock skew detected on mon.ncn-m002, mon.ncn-m003 services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby osd: 18 osds: 18 up (since 20h), 18 in (since 9d) rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) data: pools: 11 pools, 600 pgs objects: 1.82M objects, 2.6 TiB usage: 6.1 TiB used, 57 TiB / 63 TiB avail pgs: 600 active+clean io: client: 162 KiB/s rd, 9.0 MiB/s wr, 2 op/s rd, 890 op/s wr Procedure Adjust the number of omap objects.\nUse one of the options below to resolve the issue:\nUse the ceph config command.\nIn the example below, the omap object key threshold is set to 350000, but it can be set to a higher number if desired.\nncn-m001# ceph config set client.osd osd_deep_scrub_large_omap_object_key_threshold 350000 In the example below, the rgw_usage_max_user_shards is set to 16 from 1. This can be set to a maximum of 32.\nceph config set client.radosgw rgw_usage_max_user_shards 16 Increase the number of PGs for the Ceph pool.\nGet the current threshold and PG numbers.\nncn-m001# ceph osd pool autoscale-status Example output:\nPOOL SIZE TARGET SIZE RATE RAW CAPACITY RATIO TARGET RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE cephfs_data 477.8M 3.0 64368G 0.0000 1.0 4 on cephfs_metadata 781.9M 3.0 64368G 0.0000 4.0 16 on .rgw.root 384.0k 3.0 64368G 0.0000 1.0 4 on default.rgw.buckets.data 4509G 3.0 64368G 0.2102 0.2000 1.0 128 on default.rgw.control 0 3.0 64368G 0.0000 1.0 4 on default.rgw.buckets.index 1199M 3.0 64368G 0.0001 0.1800 1.0 128 on default.rgw.meta 4898k 3.0 64368G 0.0000 1.0 4 on default.rgw.log 0 3.0 64368G 0.0000 1.0 4 on kube 307.0G 3.0 64368G 0.0143 0.1000 1.0 48 on smf 1414G 2.0 64368G 0.0440 0.3000 1.0 256 on default.rgw.buckets.non-ec 0 3.0 64368G 0.0000 1.0 4 on Adjust the target_size_ratio value to increase the PGs for the pool.\nThis number should be increased a tenth or smaller at a time. Check the autoscale-status between each adjustment. When there is a change to the New PG NUM, stop adjusting the number.\nIn the example below, the target_size_ratio is set to 0.2.\nncn-m001# ceph osd pool set POOL_NAME target_size_ratio 0.2 Check to see if the change is taking effect.\nncn-m001# ceph osd pool autoscale-status Watch the status of the Ceph health.\nVerify the recovery traffic is taking place on the keys. The -w option can be used to watch the cluster.\nncn-m001# ceph -s "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/default_keycloak_realms_accounts_and_clients/",
	"title": "Default Keycloak Realms, Accounts, and Clients",
	"tags": [],
	"description": "",
	"content": "Default Keycloak Realms, Accounts, and Clients This page details the default Keycloak realms, accounts, and clients that are created when the system software is installed.\nDefault realms Default accounts Default clients Private clients Public clients Default realms Master Shasta Default accounts Username: admin\nThe password can be obtained with the following command:\nncn-mw# kubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode The password for the admin account can be changed. See Change the Keycloak admin Password.\nDefault clients Users authenticate to Keycloak on behalf of a client. Keycloak clients own configurations, such as the mapping of Keycloak user information to data available to either the userinfo endpoint, or in the JWT token. Keycloak clients also own resources, such as URIs.\nPrivate clients admin-client The admin-client client represents a service account that is used during the install to register the services with the API gateway. The secret for this account is generated during the software installation process. gatekeeper The gatekeeper client is used by the keycloak-gatekeeper to authenticate web UIs using OAUTH. system-compute-client The system-compute-client client is used by the Cray Operating System (COS) for compute nodes and some NCN services for boot orchestration and management. system-pxe-client The system-pxe-client client is used by the cray-ipxe service to communicate with cray-bss to prepare boot scripts and other boot-related content. Public clients shasta The shasta client is meant to be a generic client that can be used to access any Cray micro-service. The software install process creates the shasta client in the Shasta realm. The shasta client is public and has mappers set up so that the uidNumber, gidNumber, homeDirectory, and loginShell user attributes are included in the userinfo response. The shasta client has two roles created for authorization: admin and user. "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/customize_pcie_hardware/",
	"title": "Customize PCIe Hardware",
	"tags": [],
	"description": "",
	"content": "Customize PCIe Hardware This page will assist an administrator with changing the NCN udev rules for varying PCIe hardware.\nNOTE: If a system\u0026rsquo;s hardware is Plan of Record (PoR), then this page is not needed.\nProcedure Identify the hardware configuration by PXE booting a node.\nPrevent the network boots from completing by removing the links generated by set-sqfs-links.sh.\npit# rm /var/www/ncn-*/{initrd.img.xz,kernel,filesystem.squashfs} The NCNs will fetch the iPXE binary and then pause; this pause prevents the NCN from continuing to boot, providing an opportunity to collect information from it.\nGo through each NCN and PXE boot it.\nReplace username and IPMI_PASSWORD with the present values for the system\u0026rsquo;s BMCs.\nread -s is used to prevent the password from being echoed to the screen or preserved in the shell history.\npit# username=root pit# read -r -s -p \u0026#34;NCN BMC ${username} password: \u0026#34; IPMI_PASSWORD pit# export IPMI_PASSWORD pit# mtoken=\u0026#39;ncn-m(?!001)\\w+-mgmt\u0026#39; ; stoken=\u0026#39;ncn-s\\w+-mgmt\u0026#39; ; wtoken=\u0026#39;ncn-w\\w+-mgmt\u0026#39; pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U \u0026#34;${username}\u0026#34; -E -H {} power off pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U \u0026#34;${username}\u0026#34; -E -H {} power on Each node will attempt to PXE boot; successful network boots will dump their PCI-SIG to the console. This data can be cross-referenced with the NCN networking page; the directions for this come next.\nCollect information from the nodes.\nCollect the PIT\u0026rsquo;s device IDs:\npit# lid Example output:\nem1 8086:37D2 em2 8086:37D2 p801p1 15B3:1013 p801p2 15B3:1013 Collect the other NCNs\u0026rsquo; PCI Device and PCI Vendor IDs:\npit# for file in /var/log/conman/console*ncn*; do echo ${file} grep -Eoh \u0026#39;(net[0-9] MAC .*)\u0026#39; \u0026#34;${file}\u0026#34; | sort -u | grep PCI \u0026amp;\u0026amp; echo ----- done Example output:\n/var/log/conman/console.ncn-m001-mgmt /var/log/conman/console.ncn-m002-mgmt net0 MAC b8:59:9f:f9:1c:8e PCI.DeviceID 1013 PCI.VendorID 15b3 net1 MAC b8:59:9f:f9:1c:8f PCI.DeviceID 1013 PCI.VendorID 15b3 ----- /var/log/conman/console.ncn-m003-mgmt net0 MAC a4:bf:01:6f:6a:fe PCI.DeviceID 37d2 PCI.VendorID 8086 net1 MAC a4:bf:01:6f:6a:ff PCI.DeviceID 37d2 PCI.VendorID 8086 net2 MAC b8:59:9f:fe:49:9c PCI.DeviceID 1013 PCI.VendorID 15b3 net3 MAC b8:59:9f:fe:49:9d PCI.DeviceID 1013 PCI.VendorID 15b3 ----- /var/log/conman/console.ncn-s001-mgmt net0 MAC b8:59:9f:4a:f6:58 PCI.DeviceID 1013 PCI.VendorID 15b3 net1 MAC b8:59:9f:4a:f6:59 PCI.DeviceID 1013 PCI.VendorID 15b3 ----- /var/log/conman/console.ncn-s002-mgmt net0 MAC b8:59:9f:fe:49:ec PCI.DeviceID 1013 PCI.VendorID 15b3 net1 MAC b8:59:9f:fe:49:ed PCI.DeviceID 1013 PCI.VendorID 15b3 ----- /var/log/conman/console.ncn-s003-mgmt net0 MAC a4:bf:01:48:1f:6c PCI.DeviceID 37d2 PCI.VendorID 8086 net1 MAC a4:bf:01:48:1f:6d PCI.DeviceID 37d2 PCI.VendorID 8086 net2 MAC b8:59:9f:f9:1c:ba PCI.DeviceID 1013 PCI.VendorID 15b3 net3 MAC b8:59:9f:f9:1c:bb PCI.DeviceID 1013 PCI.VendorID 15b3 ----- /var/log/conman/console.ncn-s004-mgmt net0 MAC b8:59:9f:2b:31:1a PCI.DeviceID 1013 PCI.VendorID 15b3 net1 MAC b8:59:9f:2b:31:1b PCI.DeviceID 1013 PCI.VendorID 15b3 ----- /var/log/conman/console.ncn-w001-mgmt net0 MAC 50:6b:4b:23:a7:90 PCI.DeviceID 1017 PCI.VendorID 15b3 net1 MAC b8:59:9f:fe:49:d8 PCI.DeviceID 1013 PCI.VendorID 15b3 net2 MAC b8:59:9f:fe:49:d9 PCI.DeviceID 1013 PCI.VendorID 15b3 ----- /var/log/conman/console.ncn-w002-mgmt net0 MAC 50:6b:4b:23:a7:98 PCI.DeviceID 1017 PCI.VendorID 15b3 net1 MAC b8:59:9f:fe:49:f0 PCI.DeviceID 1013 PCI.VendorID 15b3 net2 MAC b8:59:9f:fe:49:f1 PCI.DeviceID 1013 PCI.VendorID 15b3 ----- /var/log/conman/console.ncn-w003-mgmt net0 MAC b8:59:9f:d9:9e:2c PCI.DeviceID 1013 PCI.VendorID 15b3 net1 MAC b8:59:9f:d9:9e:2d PCI.DeviceID 1013 PCI.VendorID 15b3 ----- Use the information returned in the previous step to compare the PCI.DeviceID and PCI.VendorID values to what is in NCN Networking Vendor and Bus ID Identification.\nSince PoR systems are handled with defaults, at this point one should notice differing PCI.DeviceID values than the bold entries in the Vendor and Bus ID table.\nAfter identifying which cards should be used for the management NICs, follow either of the below options, depending on the scope of the PCIe card change:\nIf all NCNs have the same change (e.g. all NCNs use ConnectX-5s for their management NICs), then update the main /var/www/boot/script.ipxe file and re-run set-sqfs-links.sh.\nReplace the default Vendor ID with the desired Intel Vendor ID.\npit# sed -i \u0026#39;s/mgmt_vid0 .*/mgmt_vid0 8086/g\u0026#39; /var/www/boot/script.ipxe Restore the initrd.img.xz, kernel, and filesystem.squashfs links to the boot directories.\npit# set-sqfs-links.sh If only a subset of the NCNs have differing cards, re-run set-sqfs-links.sh and then update just that subset of boot scripts:\nThe below example sets Intel as the management NICs, meaning onboards or Intel PCIe cards would be used for management interfaces. Additionally the example is applying this change to only a subset of nodes; specifically it is applying it to worker nodes only.\nRestore the initrd.img.xz, kernel, and filesystem.squashfs links to the boot directories.\npit# set-sqfs-links.sh Replace the default Vendor ID with the desired Intel Vendor ID.\npit# sed -i \u0026#39;s/mgmt_vid0 .*/mgmt_vid0 8086/g\u0026#39; /var/www/ncn-w*/script.ipxe Now the boot scripts are set up for booting differing PCIe cards or onboard NICs.\nEnsure that the management NICs do not get labeled as HSN NICs.\nIn some cases the cards used for HSN NICs are used for management interfaces (for example, the system\u0026rsquo;s storage and master nodes use ConnectX-5s). In this case, this procedure will ensure that they are properly labeled.\nNote: The HSN NICs key off of the Device ID, not the Vendor ID.\nRestore the initrd.img.xz, kernel, and filesystem.squashfs links to the boot directories.\npit# set-sqfs-links.sh Replace the default Vendor ID with the desired Intel Vendor ID.\npit# sed -i \u0026#39;s/hsn_did0 .*/hsn_did0 0000/g\u0026#39; /var/www/ncn-m*/script.ipxe pit# sed -i \u0026#39;s/hsn_did0 .*/hsn_did0 0000/g\u0026#39; /var/www/ncn-s*/script.ipxe At this point the system is primed to boot custom PCIe or onboard NICs, and the boot files removed in step 1 are now restored.\nSave these scripts off the system for easy re-install at a later date.\npit# SYSTEM_NAME=eniac pit# tar -czvf \u0026#34;${SYSTEM_NAME}-boot-scripts.tar.gz\u0026#34; /var/www/ncn-*/script.ipxe "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/configure_ntp_on_ncns/",
	"title": "Configure NTP on NCNs",
	"tags": [],
	"description": "",
	"content": "Configure NTP on NCNs The management nodes serve Network Time Protocol (NTP) at stratum 10, except for ncn-m001, which serves at stratum 8 (or lower if an upstream NTP server is set). All management nodes peer with each other.\nUntil an upstream NTP server is configured, the time on the NCNs may not match the current time at the site, but they will stay in sync with each other.\nTopics Fix BSS metadata Fix broken configurations Fix BSS metadata If nodes are missing metadata for NTP, then the data must be generated using csi and the system\u0026rsquo;s system_config.yaml file.\nThe csi tool is not available on ncn-m001 after the CSM install is completed. However, if the install recovery data is still available on ncn-m001 or ncn-m003, then the csi tool can be retrieved from the saved PIT ISO file. To do this, see the step used to obtain access to CSI in the Enable NCN Disk Wiping Safeguard procedure.\nIf the seed data from system_config.yaml is not available, then open a support ticket to help generate the NTP data.\nThe following steps are structured to be executed on one node at a time. However, step #3 will generate all relevant files for each node. If multiple nodes are missing NTP data in BSS, then apply this fix to each node.\nUpdate system_config.yaml to have the correct NTP settings:\nntp-servers: - ncn-m001 - example.upstream.ntp.server ntp-timezone: UTC Generate new configurations:\nncn# csi config init Change directory to the newly created system/basecamp directory and execute the upgrade_ntp_timezone_metadata.sh script.\nncn# cd system/basecamp \u0026amp;\u0026amp; /usr/share/doc/csm/upgrade/scripts/upgrade_ntp_timezone_metadata.sh Find the relevant file for the node with missing metadata (such as upgrade-metadata-000000000000.json) based on the MAC address of the node.\nFind the component name (xname) for the node that needs to be fixed:\nRun this command on the node that needs to be fixed in order to determine its xname.\nncn# cat /etc/cray/xname From ncn-m001, update BSS:\nncn-m001# csi handoff bss-update-cloud-init --user-data=\u0026#34;upgrade-metadata-000000000000.json\u0026#34; --limit=\u0026lt;xname\u0026gt;` Continue with the upgrade.\nSet a token as described in Identify Nodes and Update Metadata\nWhen the upgrade is completed, run this script on ncn-m001 in order to ensure the time is set correctly on all NCNs:\nncn-m001# for i in $(grep -oP \u0026#39;ncn-\\w\\d+\u0026#39; /etc/hosts | sort -u); do ssh $i \u0026#34;TOKEN=$TOKEN /srv/cray/scripts/common/chrony/csm_ntp.py\u0026#34;; done Fix broken configuration Clock sync is performed in increments instead of all at once, so it may take some time for the clocks to sync. Before executing any commands, give the nodes some time to update. Sync typically happens within a few seconds, but on occasion could take up to 30 or more minutes. Periodically running chronyc tracking will show clock statistics and can be used to determine if the clocks are gradually syncing.\nOn each affected NCN run the following:\nSet a token as described in Identify Nodes and Update Metadata.\nExport the token.\nncn# export TOKEN Run the script:\nncn# /srv/cray/scripts/common/chrony/csm_ntp.py "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/management_interface/",
	"title": "Management interface",
	"tags": [],
	"description": "",
	"content": "Management interface The management interface can be used to gain remote management access to the switch. The management interface is accessible using the mgmt VRF and is separate from the data plane interfaces, which are in the default VRF. Mellanox switches support out-of-band (OOB) dedicated interfaces (e.g. mgmt0, mgmt1) and in-band dedicated interfaces.\nEnter configuration mode.\nswitch \u0026gt; enable switch# configure terminal Disable setting IP addresses using the DHCP using the following command in configuration mode:\nswitch (config) # no interface mgmt0 dhcp Define the interface IP address statically using the following command in configuration mode:\nswitch (config) # interface mgmt0 ip address \u0026lt;IP address\u0026gt; \u0026lt;netmask\u0026gt; Show interface information.\nswitch# show interface mgmt "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/vlan_interface/",
	"title": "Configure VLAN Interface",
	"tags": [],
	"description": "",
	"content": "Configure VLAN Interface The switch also supports classic L3 VLAN interfaces.\nConfiguration Commands Configure the VLAN:\nswitch(config)# vlan VLAN The default mode of any VLAN is L2 only. To enable L3 functionality, run no shutdown on the VLAN:\nswitch(config)# interface vlan 2 switch(conf-if-vl-2)# no shutdown Show commands to validate functionality:\nswitch# show interface vlan Expected Results Administrators can configure the VLAN Administrators can enable the interface and associate it with the VLAN Administrators can create an IP-enabled VLAN interface, and it is up Administrators validate the configuration is correct Administrators can ping from the switch to the client and from the client to the switch Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/lag/",
	"title": "Link Aggregation Group (LAG)",
	"tags": [],
	"description": "",
	"content": "Link Aggregation Group (LAG) Link Aggregation allows you to assign multiple physical links to one logical link that functions as a single, higher-speed link providing dramatically increased bandwidth.\nConfiguration Commands Create and configure the LAG interface:\nswitch(config)# interface lag LAG switch(config-lag-if)# no shutdown switch(config-lag-if)# lacp mode active Associate member links with the LAG interface:\nswitch(config)# interface IFACE switch(config-if)# no shutdown switch(config-if)# lag LAG Show commands to validate functionality:\nswitch# show lacp \u0026lt;interfaces|aggregates|configuration\u0026gt; Example Output switch# show interface lag1 Aggregate-name lag1 Aggregated-interfaces : 1/1/1 1/1/4 Aggregation-key : 1 Aggregate mode : active Speed 0 Mb/s qos trust none qos queue-profile default qos schedule-profile default RX TX 409 input packets 0 input error 0 CRC/FCS 530 output packets 0 input error 0 collision 47808 bytes 0 dropped 56975 bytes 0 dropped switch# show lacp interfaces State abbreviations : A - Active P - Passive S - Short-timeout L - Long-timeout N - InSync O - OutofSync C - Collecting D - Distributing X - State m/c expired E - Default neighbor state Actor details of all interfaces: ------------------------------------------------------------------------------ Intf Aggregate Port Port State System-id System Aggr name id Priority Priority Key ------------------------------------------------------------------------------ 1/1/1lag1 59 1 ALFOE 70:72:cf:4d:bb:53 65534 1 1/1/4lag1 41 1 ALFOE 70:72:cf:4d:bb:53 65534 1 Partner details of all interfaces: ------------------------------------------------------------------------------ Intf Aggregate Partner Port State System-id System Aggr name Port-id Priority Priority Key ------------------------------------------------------------------------------ 1/1/1lag1 0 65534 PLFOEX 00:00:00:00:00:00 65534 0 1/1/4lag1 0 65534 PLFOEX 00:00:00:00:00:00 65534 0 switch# show lacp aggregates Aggregate-name : lag1 Aggregated-interfaces : 1/1/1 1/1/4 Heartbeat rate : slow Aggregate mode : active F - Aggregable I - Individual Expected Results Administrators can create and configure a LAG Administrators can add ports to a LAG Administrators can configure a LAG interface Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/report_the_endpoint_status_for_etcd_clusters/",
	"title": "Report the Endpoint Status for etcd Clusters",
	"tags": [],
	"description": "",
	"content": "Report the Endpoint Status for etcd Clusters Report etcd cluster end point status. The report includes a cluster\u0026rsquo;s endpoint, database size, and leader status.\nThis procedure provides the ability to view the etcd cluster endpoint status.\nPrerequisites This procedure requires root privileges. The etcd clusters are in a healthy state. Procedure Report the endpoint status for all etcd clusters in a namespace.\nThe following example is for the services namespace.\nncn-mw# for pod in $(kubectl get pods -l app=etcd -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do echo \u0026#34;### ${pod} Endpoint Status: ###\u0026#34; kubectl -n services exec ${pod} -c etcd -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl endpoint status -w table\u0026#34; done Example output:\n### cray-bos-etcd-7cxq6qrhz5 Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | e57d42e2a85763bb | 3.3.22 | 139 kB | true | 26 | 78360 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ### cray-bos-etcd-b9m4k5qfrd Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | 355baed6cb6e3022 | 3.3.22 | 139 kB | false | 26 | 78360 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ### cray-bos-etcd-tnpv8x6cxv Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | 78949579ff08b422 | 3.3.22 | 139 kB | false | 26 | 78360 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ### cray-bss-etcd-q4k54rbbfj Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | cbeb570f568c6ca6 | 3.3.22 | 70 kB | true | 29 | 41321 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ### cray-bss-etcd-r75mlv6ffd Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | 7343edb17d8e6fd4 | 3.3.22 | 70 kB | false | 29 | 41321 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ### cray-bss-etcd-xprv5ht5d4 Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | d7404ad66483bd37 | 3.3.22 | 70 kB | false | 29 | 41321 | +----------------+------------------+---------+---------+-----------+-----------+------------+ [...] Report the endpoint status for a singe etcd cluster in a namespace.\nThe following example is for the services namespace.\nncn-mw# for pod in $(kubectl get pods -l etcd_cluster=cray-bos-etcd -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do echo \u0026#34;### ${pod} Endpoint Status: ###\u0026#34; kubectl -n services exec ${pod} -c etcd -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl endpoint status -w table\u0026#34; done Example output:\n### cray-bos-etcd-7cxq6qrhz5 Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | e57d42e2a85763bb | 3.3.22 | 139 kB | true | 26 | 78333 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ### cray-bos-etcd-b9m4k5qfrd Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | 355baed6cb6e3022 | 3.3.22 | 139 kB | false | 26 | 78333 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ### cray-bos-etcd-tnpv8x6cxv Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | 78949579ff08b422 | 3.3.22 | 139 kB | false | 26 | 78333 | +----------------+------------------+---------+---------+-----------+-----------+------------+ "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/target_ansible_tasks_for_image_customization/",
	"title": "Target Ansible Tasks for Image Customization",
	"tags": [],
	"description": "",
	"content": "Target Ansible Tasks for Image Customization The Configuration Framework Service (CFS) enables Ansible playbooks to run against both running nodes (node personalization) and images prior to boot(image customization). See Configuration Management Use Cases for more information about image customization and when it should be used.\nIdeally image customization playbooks should be separate from node personalization playbooks. This reduces the likelihood that Ansible content will run in both modes, and reduces the need for conditional checks in the playbooks.\nFor situations when both image customization and node personalization need to be part of the same playbook, CFS provides the cray_cfs_image variable to distinguish between node personalization and image customization. When this variable is set to true, it indicates that the CFS session is an image customization and the playbook is targeting an image.\nUsing cray_cfs_image cray_cfs_image can be set with Ansible playbook conditionals to selectively run individual tasks with when: cray_cfs_image, or to ignore individual tasks with when: not cray_cfs_image.\nConditionals can also be applied to entire roles if desired (see the external apply Ansible conditionals to roles). In instances where the same playbook may be run in both modes, it is best practice to include a conditional on all parts of the playbook. This is best done by placing the conditional on an include_* statement. See Write Ansible Code for CFS: Reduce wasted time for more information on optimizing conditionals.\nIt is also best practice to include a default in Ansible roles for playbook and role portability because CFS injects this variable at runtime. This can be done in the defaults section of the role, or where the variable is called. For example:\nwhen: \u0026#34;{{ cray_cfs_image | default(false) }}\u0026#34; If a default is not provided, any playbooks or roles will not be runnable outside of the CFS Ansible Execution Environment (AEE) without the user specifying cray_cfs_image in the vars files or with the Ansible extra-vars options.\nCFS automatically sets this variable in the hosts/01-cfs-generated.yaml file for all sessions. When the session target is image customization, it sets cray_cfs_image to true; otherwise, it is false.\nImage customization limitations When running Ansible against an IMS-hosted image root during an image customization CFS session, there are no special requirements for the paths when copying or syncing files. The image root directories will appears as if Ansible is connecting to a regular, live node. However, the image is not a running node, so actions that require a running system, such as starting/reloading a service, will not work properly and will cause the Ansible play to fail. Actions like these should be done only during live-node configuration modes such as node personalization.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/sessions/",
	"title": "BOS Sessions",
	"tags": [],
	"description": "",
	"content": "BOS Sessions The Boot Orchestration Service (BOS) creates a session when it is asked to perform an operation on a session template. Sessions provide a way to track the status of many nodes at once as they perform the same operation with the same session template information. When creating a session, both the operation and session template are required parameters.\nBOA functionality BOS v1 session limitations The v1 version of BOS supports these operations:\nBoot - Boot a designated collection of nodes. Shutdown - Shutdown a designated collection of nodes. Reboot - Reboot a designated collection of nodes. Configure - Configure a designated collection of booted nodes. BOS sessions can be used to boot compute nodes with customized image roots.\nA session requires two parameters, a session template ID and an operation to perform on that template. The BOS API\u0026rsquo;s session endpoint can display a list of all of the sessions that have been created, including previous and currently running sessions. The endpoint can also display the details of a given session when the specific session ID is provided as a parameter. Sessions can also be deleted through the API.\nBOS supports a RESTful API. This API can be interacted with directly using tools like cURL. It can also be interacted with through the Cray Command Line Interface (CLI). See Manage a BOS Session for more information.\nBOA functionality The Boot Orchestration Agent (BOA) implements each session and sees it through to completion. A BOA is a Kubernetes job. It runs once to completion. If there are transient failures, BOA will exit and Kubernetes will reschedule it so that it can re-execute its session.\nBOA moves nodes towards the requested state, but if a node fails during any of the intermediate steps, it takes note of it. BOA will then provide a command in the output of the BOA log that can be used to retry the action. This behavior impacts all BOS operations.\nFor example, if there is a 6,000 node system and 3 nodes fail to power off during a BOS operation. then BOA will continue and attempt to re-provision the remaining 5,997 nodes. After the command is finished, it will provide information about what the administrator needs to do in order to retry the operation on the 3 nodes that failed.\nBOS v1 session limitations The following limitations currently exist with BOS sessions:\nNo checking is done to prevent the launch of multiple sessions with overlapping lists of nodes. Concurrently running sessions may conflict with each other. The boot ordinal and shutdown ordinal are not honored. The partition parameter is not honored. All nodes proceed at the same pace. BOA will not move on to the next step of the boot process until all components have succeeded or failed the current step. The Configuration Framework Service (CFS) has its own limitations. Refer to the Configuration Management documentation for more information. "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/list_available_uai_classes/",
	"title": "List Available UAI Classes",
	"tags": [],
	"description": "",
	"content": "List Available UAI Classes View all the details of every available UAI class. Use this information to select a class to apply to one or more UAIs.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) Procedure List all available UAI classes.\nTo list available UAI classes, use the following command:\nncn-m001-pit# cray uas admin config classes list The cray uas admin config classes list command supports the same --format options as the cray uas admin config volumes list command. See List Volumes Registered in UAS for details.\nFor example:\nncn-m001-pit# cray uas admin config classes list --format json \u0026lt;output not shown\u0026gt; See UAI Classes and Elements of a UAI for more details on the output.\nTop: User Access Service (UAS)\nNext Topic: Create a UAI Class\n"
},
{
	"uri": "/docs-csm/en-12/install/prepare_site_init/",
	"title": "Prepare site-init",
	"tags": [],
	"description": "",
	"content": "Prepare site-init These procedures guide administrators through setting up the site-init directory which contains important customizations for various products.\nBackground Create and Initialize site-init Directory Create Baseline System Customizations Setup LDAP configuration End of LDAP configuration Customer-Specific Customizations Version Control site-init files Push to a remote repository 1. Background The shasta-cfg directory included in the CSM release tarball includes relatively static, installation-centric artifacts, such as:\nCluster-wide network configuration settings required by Helm charts deployed by product stream Loftsman manifests Sealed Secrets Sealed Secret Generate Blocks \u0026ndash; a form of plain-text input that renders to a Sealed Secret Helm chart value overrides that are merged into Loftsman manifests by product stream installers 2. Create and initialize site-init directory Set the SITE_INIT variable.\nImportant: All procedures on this page assume that SITE_INIT variable has been set.\npit# SITE_INIT=\u0026#34;${PITDATA}/prep/site-init\u0026#34; Create the site-init directory.\npit# mkdir -pv \u0026#34;${SITE_INIT}\u0026#34; Initialize site-init from CSM.\npit# \u0026#34;${CSM_PATH}/shasta-cfg/meta/init.sh\u0026#34; \u0026#34;${SITE_INIT}\u0026#34; 3. Create Baseline System Customizations The following steps update ${SITE_INIT}/customizations.yaml with system-specific customizations.\nChange into the site-init directory\npit# cd \u0026#34;${SITE_INIT}\u0026#34; Merge the system-specific settings generated by CSI into customizations.yaml.\npit# yq merge -xP -i \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; \u0026lt;(yq prefix -P \u0026#34;${PITDATA}/prep/${SYSTEM_NAME}/customizations.yaml\u0026#34; spec) Set the cluster name.\npit# yq write -i \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; spec.wlm.cluster_name \u0026#34;${SYSTEM_NAME}\u0026#34; Make a backup copy of ${SITE_INIT}/customizations.yaml.\npit# cp -pv \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; \u0026#34;${SITE_INIT}/customizations.yaml.prepassword\u0026#34; Review the configuration to generate these sealed secrets in customizations.yaml in the site-init directory:\nspec.kubernetes.sealed_secrets.cray_reds_credentials spec.kubernetes.sealed_secrets.cray_meds_credentials spec.kubernetes.sealed_secrets.cray_hms_rts_credentials Replace the Username and Password references in match the existing settings of your system hardware components. NOTE\nThe cray_reds_credentials are used by the River Endpoint Discovery Service (REDS) for River components. The cray_meds_credentials are used by the Mountain Endpoint Discovery Service (MEDS) for the liquid-cooled components in an Olympus (Mountain) cabinet. The cray_hms_rts_credentials are used by the Redfish Translation Service (RTS) for any hardware components which are not managed by Redfish, such as a ServerTech PDU in a River Cabinet. See the Decrypt Sealed Secrets for Review section of Manage Sealed Secrets, if needing to examine credentials from prior installations.\npit# vim \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; Review the changes that you made.\npit# diff ${SITE_INIT}/customizations.yaml ${SITE_INIT}/customizations.yaml.prepassword Validate that REDS/MEDS/RTS credentials are correct.\nFor all credentials, make sure that Username and Password values are correct.\nValidate REDS credentials:\nNOTE These credentials are used by the REDS and HMS discovery services, targeting River Redfish BMC endpoints and management switches\nFor vault_redfish_defaults, the only entry used is:\n{\u0026#34;Cray\u0026#34;: {\u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;XXXX\u0026#34;} Ensure the Cray key exists. This key is not used in any of the other credential specifications.\npit# yq read \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; \u0026#39;spec.kubernetes.sealed_secrets.cray_reds_credentials.generate.data[*].args.value\u0026#39; | jq Validate MEDS credentials:\nThese credentials are used by the MEDS service, targeting Redfish BMC endpoints.\npit# yq read \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; \u0026#39;spec.kubernetes.sealed_secrets.cray_meds_credentials.generate.data[0].args.value\u0026#39; | jq Validate RTS credentials:\nThese credentials are used by the Redfish Translation Service, targeting River Redfish BMC endpoints and PDU controllers.\npit# yq read \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; \u0026#39;spec.kubernetes.sealed_secrets.cray_hms_rts_credentials.generate.data[*].args.value\u0026#39; | jq To customize the PKI Certificate Authority (CA) used by the platform, see Certificate Authority.\nIMPORTANT The CA may not be modified after install.\nSetup LDAP configuration NOTE Skip past LDAP configuration to here if there is no LDAP configuration at this time. If LDAP should be enabled later, follow Add LDAP User Federation after installation.\nSet environment variables for the LDAP server and its port.\nIn the example below, the LDAP server has the hostname dcldap2.hpc.amslabs.hpecorp.net and is using the port 636.\npit# LDAP=dcldap2.hpc.amslabs.hpecorp.net pit# PORT=636 Load the openjdk container image.\nNOTE Requires a properly configured Docker or Podman environment.\npit# \u0026#34;${CSM_PATH}/hack/load-container-image.sh\u0026#34; artifactory.algol60.net/csm-docker/stable/docker.io/library/openjdk:11-jre-slim Get the issuer certificate.\nRetrieve the issuer certificate for the LDAP server at port 636. Use openssl s_client to connect and show the certificate chain returned by the LDAP host:\npit# openssl s_client -showcerts -connect \u0026#34;${LDAP}:${PORT}\u0026#34; \u0026lt;/dev/null Enter the issuer\u0026rsquo;s certificate into cacert.pem.\nEither manually extract (i.e., cut/paste) the issuer\u0026rsquo;s certificate into cacert.pem, or try the following commands to create it automatically.\nNOTE The following commands were verified using OpenSSL version 1.1.1d and use the -nameopt RFC2253 option to ensure consistent formatting of distinguished names. Unfortunately, older versions of OpenSSL may not support -nameopt on the s_client command or may use a different default format. However, the issuer certificate can be manually extracted from the output of the above openssl s_client example, if the following commands are unsuccessful.\nObserve the issuer\u0026rsquo;s DN.\npit# openssl s_client -showcerts -nameopt RFC2253 -connect \u0026#34;${LDAP}:${PORT}\u0026#34; \u0026lt;/dev/null 2\u0026gt;/dev/null | grep issuer= | sed -e \u0026#39;s/^issuer=//\u0026#39; Expected output includes a line similar to this:\nemailAddress=dcops@hpe.com,CN=Data Center,OU=HPC/MCS,O=HPE,ST=WI,C=US Extract the issuer\u0026rsquo;s certificate.\nNOTE The issuer DN is properly escaped as part of the awk pattern below. It must be changed to match the value for emailAddress, CN, OU, etc. for your LDAP. If the value you are using is different, be sure to escape it properly!\npit# openssl s_client -showcerts -nameopt RFC2253 -connect \u0026#34;${LDAP}:${PORT}\u0026#34; \u0026lt;/dev/null 2\u0026gt;/dev/null | awk \u0026#39;/s:emailAddress=dcops@hpe.com,CN=Data Center,OU=HPC\\/MCS,O=HPE,ST=WI,C=US/,/END CERTIFICATE/\u0026#39; | awk \u0026#39;/BEGIN CERTIFICATE/,/END CERTIFICATE/\u0026#39; \u0026gt; cacert.pem Create certs.jks.\nNOTE The alias used in this command for cray-data-center-ca should be changed to match your LDAP.\npit# podman run --rm -v \u0026#34;$(pwd):/data\u0026#34; \\ artifactory.algol60.net/csm-docker/stable/docker.io/library/openjdk:11-jre-slim keytool \\ -importcert -trustcacerts -file /data/cacert.pem -alias cray-data-center-ca \\ -keystore /data/certs.jks -storepass password -noprompt Create certs.jks.b64 by base-64 encoding certs.jks.\npit# base64 certs.jks \u0026gt; certs.jks.b64 Inject and encrypt certs.jks.b64 into customizations.yaml.\npit# cat \u0026lt;\u0026lt;EOF | yq w - \u0026#39;data.\u0026#34;certs.jks\u0026#34;\u0026#39; \u0026#34;$(\u0026lt;certs.jks.b64)\u0026#34; | \\ yq r -j - | ${SITE_INIT}/utils/secrets-encrypt.sh | \\ yq w -f - -i ${SITE_INIT}/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray-keycloak\u0026#39; { \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;keycloak-certs\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: {} } EOF Update the keycloak_users_localize sealed secret with the appropriate value for ldap_connection_url.\nSet ldap_connection_url in customizations.yaml.\npit# yq write -i \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; \\ \u0026#39;spec.kubernetes.sealed_secrets.keycloak_users_localize.generate.data.(args.name==ldap_connection_url).args.value\u0026#39; \\ \u0026#34;ldaps://${LDAP}\u0026#34; Review the keycloak_users_localize sealed secret.\npit# yq read \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; spec.kubernetes.sealed_secrets.keycloak_users_localize Configure the ldapSearchBase and localRoleAssignments settings for the cray-keycloak-users-localize chart in customizations.yaml.\nNOTE There may be one or more groups in LDAP for admins and one or more for users. Each admin group needs to be assigned to role admin and set to both shasta and cray clients in Keycloak. Each user group needs to be assigned to role user and set to both shasta and cray clients in Keycloak.\nSet ldapSearchBase in customizations.yaml.\nNOTE This example sets ldapSearchBase to dc=dcldap,dc=dit\npit# yq write -i \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; spec.kubernetes.services.cray-keycloak-users-localize.ldapSearchBase \u0026#39;dc=dcldap,dc=dit\u0026#39; Set localRoleAssignments in customizations.yaml.\nNOTE This example sets localRoleAssignments for the LDAP groups employee, craydev, and shasta_admins to be the admin role, and the LDAP group shasta_users to be the user role.\npit# yq write -s - -i \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; \u0026lt;\u0026lt;EOF - command: update path: spec.kubernetes.services.cray-keycloak-users-localize.localRoleAssignments value: - {\u0026#34;group\u0026#34;: \u0026#34;employee\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;employee\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;craydev\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;craydev\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_admins\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_admins\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_users\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_users\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} EOF Review the cray-keycloak-users-localize values.\npit# yq read \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; spec.kubernetes.services.cray-keycloak-users-localize End of LDAP configuration Configure the Unbound DNS resolver (if needed).\nImportant If access to a site DNS server is required and this DNS server was specified to csi using the site-dns option (either on the command line or in the system_config.yaml file), then no further action is required and this step should be skipped.\nThe default configuration is as follows:\ncray-dns-unbound: domain_name: \u0026#39;{{ network.dns.external }}\u0026#39; forwardZones: - name: \u0026#34;.\u0026#34; forwardIps: - \u0026#34;{{ network.netstaticips.system_to_site_lookups }}\u0026#34; The configured site DNS server can be verified by inspecting the value set for system_to_site_lookups.\npit# yq r ${SITE_INIT}/customizations.yaml spec.network.netstaticips.system_to_site_lookups Possible output:\n172.30.84.40 If there is no requirement to resolve external hostnames (including other services on the site network) or no upstream DNS server, then the cray-dns-unbound service should be configured to forward to the cray-dns-powerdns service.\nUpdate the forwardZones configuration for the cray-dns-unbound service to point to the cray-dns-powerdns service and only answer mtl network queries from static records.\npit# yq write -s - -i ${SITE_INIT}/customizations.yaml \u0026lt;\u0026lt;EOF - command: update path: spec.kubernetes.services.cray-dns-unbound.forwardZones value: - name: \u0026#34;.\u0026#34; forwardIps: - \u0026#34;10.92.100.85\u0026#34; - command: update path: spec.kubernetes.services.cray-dns-unbound.localZones value: - name: \u0026#34;mtl.\u0026#34; localType: static EOF Review the cray-dns-unbound values.\nIMPORTANT Do not remove the domain_name entry, it is required for Unbound to forward requests to PowerDNS correctly.\npit# yq read \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; spec.kubernetes.services.cray-dns-unbound Expected output:\ndomain_name: \u0026#39;{{ network.dns.external }}\u0026#39; forwardZones: - name: \u0026#34;.\u0026#34; forwardIps: - \u0026#34;10.92.100.85\u0026#34; localZones: - name: \u0026#34;mtl.\u0026#34; localType: static See the following documentation regarding known issues when operating with no upstream DNS server.\nSpire Database Cluster DNS Lookup Failure Spire database connection pool configuration in an air-gapped environment (Optional) Configure PowerDNS zone transfer and DNSSEC. See the PowerDNS Configuration Guide for more information.\nIf zone transfer is to be configured, then review customizations.yaml and ensure that the primary_server, secondary_servers, and notify_zones values are set correctly.\nIf DNSSEC is to be used, then add the desired keys into the dnssec SealedSecret.\n(Optional) Configure Prometheus SNMP Exporter.\nThe Prometheus SNMP exporter needs to be configured with a list of management network switches to scrape metrics from in order to populate the System Health Service Grafana dashboards.\nSee Prometheus SNMP Exporter for more information.\nLoad the zeromq container image required by Sealed Secret Generators.\nNOTE Requires a properly configured Docker or Podman environment.\npit# \u0026#34;${CSM_PATH}/hack/load-container-image.sh\u0026#34; artifactory.algol60.net/csm-docker/stable/docker.io/zeromq/zeromq:v4.0.5 Re-encrypt existing secrets.\npit# \u0026#34;${SITE_INIT}/utils/secrets-reencrypt.sh\u0026#34; \\ \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; \\ \u0026#34;${SITE_INIT}/certs/sealed_secrets.key\u0026#34; \\ \u0026#34;${SITE_INIT}/certs/sealed_secrets.crt\u0026#34; It is not an error if this script gives no output.\nGenerate secrets.\npit# \u0026#34;${SITE_INIT}/utils/secrets-seed-customizations.sh\u0026#34; \u0026#34;${SITE_INIT}/customizations.yaml\u0026#34; Leave the site-init directory.\npit# cd \u0026#34;${PITDATA}\u0026#34; site-init is now prepared. Resume Initialize the LiveCD.\n4. Customer-specific customizations Customer-specific customizations are any changes on top of the baseline configuration to satisfy customer-specific requirements. It is recommended that customer-specific customizations be tracked on branches separate from the mainline in order to make them easier to manage.\nApply any customer-specific customizations by merging the corresponding branches into master branch of site-init.\nWhen considering merges, and especially when resolving conflicts, carefully examine differences to ensure all changes are relevant. For example, when applying a customer-specific customization used in a prior version, be sure the change still makes sense. It is common for options to change as new features are introduced and bugs are fixed.\n5. Version Control site-init Files Setup site-init as a Git repository in order to manage the baseline configuration during initial system installation.\nInitialize site-init as a Git repository.\npit# cd ${SITE_INIT} pit# git init . (Optional) Exclude sealed secret private keys from Git.\nWARNING If production system or operational security is a concern, do NOT store the sealed secret private key in Git; instead, store the sealed secret key outside of Git in a secure offline system. In order to ensure that these sensitive keys are not accidentally committed, configure .gitignore to ignore files under the certs directory:\npit# echo \u0026#34;certs/\u0026#34; \u0026gt;\u0026gt; .gitignore Stage site-init files to be committed.\npit# git add -A Review what will be committed.\npit# git status Commit the baseline configuration.\npit# git commit -m \u0026#34;Baseline configuration for $(${CSM_PATH}/lib/version.sh)\u0026#34; 5.1 Push to a Remote Repository It is strongly recommended that the site-init repository be maintained off-cluster. Add a remote repository and push the baseline configuration on master branch to a corresponding remote branch.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/troubleshoot_pods_multi-attach_error/",
	"title": "Troubleshoot Pods Failing to Restart on Other Worker Nodes",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Pods Failing to Restart on Other Worker Nodes Troubleshoot an issue where pods cannot restart on another worker node because of the \u0026ldquo;Volume is already exclusively attached to one node and can\u0026rsquo;t be attached to another\u0026rdquo; error. Kubernetes does not currently support \u0026ldquo;readwritemany\u0026rdquo; access mode for Rados Block Device (RBD) devices, which causes an issue where devices fail to unmap correctly.\nThe issue occurs when unmounting the mounts tied to the RBD devices, which causes the rbd-task (watcher) to not stop for the RBD device.\nWARNING: If this process is followed and there are mount points that cannot be unmounted without using the force option, then a process may still be writing to them. If mount points are forcefully unmounted, there is a high probability of data loss or corruption.\nPrerequisites This procedure requires administrative privileges.\nProcedure Force delete the pod.\nThis may not be successful, but it is important to try before proceeding.\nncn-w001# kubectl delete pod -n NAMESPACE POD_NAME --force --grace-period=0 Log in to a manager node and proceed if the previous step did not fix the issue.\nDescribe the pod experiencing issues.\nThe returned Persistent Volume Claim (PVC) information will be needed in future steps.\nncn-m001# kubectl -n services describe pod POD_ID Example output:\n[...] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 23s default-scheduler Successfully assigned services/cray-ims-6578bf7874-twwp7 to ncn-w002 Warning FailedAttachVolume 23s attachdetach-controller Multi-Attach error for volume \u0026#34;**pvc-6ac68e32-de91-4e21-ac9f-c743b3ecb776**\u0026#34; Volume is already exclusively attached to one node and can\u0026#39;t be attached to another In this example, pvc-6ac68e32-de91-4e21-ac9f-c743b3ecb776 is the PVC information required for the next step.\nRetrieve the Ceph volume.\nncn-m001# kubectl describe -n NAMESPACE pv PVC_NAME Example output:\nName: pvc-6ac68e32-de91-4e21-ac9f-c743b3ecb776 Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/provisioned-by: ceph.com/rbd rbdProvisionerIdentity: ceph.com/rbd Finalizers: [kubernetes.io/pv-protection] StorageClass: ceph-rbd-external Status: Bound Claim: services/cray-ims-data-claim Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: RBD (a Rados Block Device mount on the host that shares a pod\u0026#39;s lifetime) CephMonitors: [10.252.0.10 10.252.0.11 10.252.0.12] RBDImage: kubernetes-dynamic-pvc-3ce9ec37-846b-11ea-acae-86f521872f4c \u0026lt;\u0026lt;-- Ceph image name FSType: RBDPool: kube \u0026lt;\u0026lt;-- Ceph pool RadosUser: kube Keyring: /etc/ceph/keyring SecretRef: \u0026amp;SecretReference{Name:ceph-rbd-kube,Namespace:ceph-rbd,} ReadOnly: false Events: \u0026lt;none\u0026gt; Find the worker node that has the RBD locked.\nFind the RBD status.\nTake a note of the returned IP address.\nncn-m001# rbd status CEPH_POOL_NAME/CEPH_IMAGE_NAME For example:\nncn-m001# rbd status kube/kubernetes-dynamic-pvc-3ce9ec37-846b-11ea-acae-86f521872f4c Watchers: watcher=**10.252.0.4**:0/3520479722 client.689192 cookie=18446462598732840976 Use the returned IP address to get the host name attached to it.\nTake note of the returned host name.\nncn-m001# grep IP_ADDRESS /etc/hosts Example output:\n10.252.0.4 ncn-w001.local ncn-w001 ncn-w001-nmn.local x3000c0s7b0n0 ncn-w001-nmn sms01-nmn.local sms04-nmn sms.local sms-nmn sms-nmn.local mgmt-plane-cmn mgmt-plane-cmn.local mgmt-plane-nmn.local bis.local bis time-nmn time-nmn.local #-label-10.252.0.4 SSH to the host name returned in the previous step.\nncn-m001# ssh HOST_NAME Unmap the device.\nFind the RBD number.\nUse the CEPH_IMAGE_NAME value returned in step 4.\nncn-m001# rbd showmapped|grep CEPH_IMAGE_NAME Example output:\n16 kube kubernetes-dynamic-pvc-3ce9ec37-846b-11ea-acae-86f521872f4c - /dev/**rbd16** Take note of the returned RBD number, which will be used in the next step.\nVerify it is not in use by an unstopped container.\nncn-m001# mount|grep RBD_NUMBER If no mount points are returned, proceed to the next step. If mount points are returned, run the following command:\nncn-m001# unmount MOUNT_POINT Troubleshooting: If that still does not succeed, use the unmount -f option.\nWARNING: If mount points are forcefully unmounted, there is a chance for data loss or corruption.\nUnmap the device.\nncn-m001# rbd unmap -o force /dev/RBD_NUMBER Check the status of the pod.\nncn-m001# kubectl get pod -n NAMESPACE POD_NAME Troubleshooting: If the pod status has not changes, try deleting the pod to restart it.\nncn-m001# kubectl delete pod -n NAMESPACE POD_NAME "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/delete_internal_user_accounts_from_the_keycloak_shasta_realm/",
	"title": "Delete Internal User Accounts in the Keycloak Shasta Realm",
	"tags": [],
	"description": "",
	"content": "Delete Internal User Accounts in the Keycloak Shasta Realm Manually delete a user account in the Keycloak Shasta realm. User accounts are maintained via the Keycloak user management UI.\nRemoving an account from Keycloak is a good way to revoke admin or user privileges.\nPrerequisites This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process. The password can be obtained with the following command:\nncn-mw# kubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode Procedure Access the Keycloak user management interface.\nSee Access the Keycloak User Management UI.\nNavigate to the Users tab.\nSearch for the username or ID of the account that is being deleted.\nClick the Delete button in the Actions column of the table to remove the desired account.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/disable_nodes/",
	"title": "Disable Nodes",
	"tags": [],
	"description": "",
	"content": "Disable Nodes Use the Hardware State Manager (HSM) Cray CLI commands to disable nodes on the system.\nDisabling nodes that are not configured correctly allows the system to successfully boot.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Procedure Disable one or more nodes with HSM.\nncn-m001# cray hsm state components bulkEnabled update --enabled false --component-ids XNAME_LIST Verify the desired nodes are disabled.\nncn-m001# cray hsm state components query create --component-ids XNAME_LIST --format toml Example output:\n[[Components]] Type = \u0026#34;Node\u0026#34; Enabled = false State = \u0026#34;On\u0026#34; NID = 1003 Flag = \u0026#34;OK\u0026#34; Role = \u0026#34;Compute\u0026#34; NetType = \u0026#34;Sling\u0026#34; Arch = \u0026#34;X86\u0026#34; ID = \u0026#34;x5000c1s0b1n1\u0026#34; [[Components]] Type = \u0026#34;Node\u0026#34; Enabled = false State = \u0026#34;On\u0026#34; NID = 1004 Flag = \u0026#34;OK\u0026#34; Role = \u0026#34;Compute\u0026#34; NetType = \u0026#34;Sling\u0026#34; Arch = \u0026#34;X86\u0026#34; ID = \u0026#34;x5000c1s0b1n2\u0026#34; After changing the state of nodes, be cautious when powering them on/off. The preferred method for safely powering them on/off is via the Boot Orchestration Service (BOS). The Cray Advanced Platform Monitoring and Control (CAPMC) service is used to directly control the power for nodes, regardless of the state in HSM. CAPMC does not check if a node is disabled in HSM.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/management_network_configuration_example/",
	"title": "Example of how to configure Scenario A or B",
	"tags": [],
	"description": "",
	"content": "Example of how to configure Scenario A or B Create the CAN VRF\nAruba\nswitch#config\nswitch(config)#vrf CAN\nMove interfaces into CAN VRF\nIf you have existing CAN interface configuration it will be deleted once you move the interface into the new VRF. You will have to re-apply it. NOTE: These are example configs only, most implementations of Bi-CAN will be different. Aruba\nAruba Primary Config\ninterface vlan 7 vsx-sync active-gateways vrf attach CAN description CAN ip mtu 9198 ip address 128.55.176.2/23 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 128.55.176.1 ip ospf 2 area 0.0.0.210 Aruba Secondary Config\ninterface vlan 7 vsx-sync active-gateways vrf attach CAN description CAN ip mtu 9198 ip address 128.55.176.3/23 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 128.55.176.1 ip ospf 2 area 0.0.0.210 Create BGP process in CAN VRF\nA new BGP process will need to be running in the CAN VRF, this will peer with the CAN IP addresses on the NCN-Workers. These are example configs only, the neighbors below are the IP addresses of the CAN interface on the Workers. Aruba Config\nrouter bgp 65533 vrf CAN maximum-paths 8 neighbor 128.55.176.3 remote-as 65533 neighbor 128.55.176.25 remote-as 65534 neighbor 128.55.176.25 passive neighbor 128.55.176.26 remote-as 65534 neighbor 128.55.176.26 passive neighbor 128.55.176.27 remote-as 65534 neighbor 128.55.176.27 passive Setup Customer Edge Router\nThe customer Edge router has to be certified by the Slingshot team. The configuration for this is going to be unique for most customers. Below is an example configuration of a single Arista switch with a static LAG to a single Slingshot switch. Arista LAG Config\ninterface Ethernet24/1 mtu 9214 flowcontrol send on flowcontrol receive on speed forced 100gfull error-correction encoding reed-solomon channel-group 1 mode on interface Ethernet25/1 mtu 9214 flowcontrol send on flowcontrol receive on speed forced 100gfull error-correction encoding reed-solomon channel-group 1 mode on interface Port-Channel1 mtu 9214 switchport access vlan 2 switchport trunk native vlan 2 switchport mode trunk We are using VLAN 2 for the HSN network. VLAN 2 config\ninterface Vlan2 ip address 10.101.10.1/24 The following is the Arista BGP config for peering over the HSN. The BGP neighbor IP addresses used are HSN IP addresses of Worker Nodes. Example HSN IP\nncn-w001:~ # ip a show hsn0 8: hsn0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9000 qdisc mq state UP group default qlen 1000 link/ether 02:00:00:00:00:0d brd ff:ff:ff:ff:ff:ff inet 10.101.10.10/24 scope global hsn0 valid_lft forever preferred_lft forever inet6 fe80::ff:fe00:d/64 scope link valid_lft forever preferred_lft forever We are creating a prefix list and route-map to only accept routes from the HSN network. Arista BGP Config\nip prefix-list HSN seq 10 permit 10.101.10.0/24 ge 24 route-map HSN permit 5 match ip address prefix-list HSN router bgp 65534 maximum-paths 32 neighbor 10.101.10.10 remote-as 65533 neighbor 10.101.10.10 transport connection-mode passive neighbor 10.101.10.10 route-map HSN in neighbor 10.101.10.11 remote-as 65533 neighbor 10.101.10.11 transport connection-mode passive neighbor 10.101.10.11 route-map HSN in neighbor 10.101.10.12 remote-as 65533 neighbor 10.101.10.12 transport connection-mode passive neighbor 10.101.10.12 route-map HSN in Configure MetalLB\nWe will need to configure MetalLB to peer with the new CAN VRF interfaces and the new HSN interface on the customer edge router. apiVersion: v1 data: config: | peers: - peer-address: 10.252.0.2 peer-asn: 65533 my-asn: 65533 - peer-address: 10.252.0.3 peer-asn: 65533 my-asn: 65533 - peer-address: 10.101.8.2 peer-asn: 65533 my-asn: 65536 - peer-address: 10.101.8.3 peer-asn: 65533 my-asn: 65536 - peer-address: 10.101.10.1 peer-asn: 65534 my-asn: 65533 address-pools: - name: customer-access protocol: bgp addresses: - 10.101.8.128/25 - name: customer-access-static protocol: bgp addresses: - 10.101.8.112/28 - name: customer-high-speed protocol: bgp addresses: - 10.101.10.128/25 - name: customer-high-speed-static protocol: bgp addresses: - 10.101.10.112/28 - name: hardware-management protocol: bgp addresses: - 10.94.100.0/24 - name: node-management protocol: bgp addresses: - 10.92.100.0/24 Verify BGP and Routes\nOnce MetalLB is configured the BGP peers on the Customer Edge router and the CAN VRF should be established. Arista Edge Router\nsw-edge01(config-router-bgp)#show ip bgp summary BGP summary information for VRF default Router identifier 192.168.50.50, local AS number 65534 Neighbor Status Codes: m - Under maintenance Neighbor V AS MsgRcvd MsgSent InQ OutQ Up/Down State PfxRcd PfxAcc 10.101.10.10 4 65533 23 12 0 0 00:03:49 Estab 14 14 10.101.10.11 4 65533 25 11 0 0 00:03:49 Estab 16 16 10.101.10.12 4 65533 23 11 0 0 00:03:49 Estab 14 14 The Arista routing table should now include the external IP addresses exposed by metalLB. The onsite network team will be responsible for distributing these routes to the rest of their network. sw-edge01(config)#show ip route B E 10.101.8.113/32 [200/0] via 10.101.10.10, Vlan2 via 10.101.10.11, Vlan2 via 10.101.10.12, Vlan2 B E 10.101.8.128/32 [200/0] via 10.101.10.10, Vlan2 via 10.101.10.11, Vlan2 via 10.101.10.12, Vlan2 B E 10.101.8.129/32 [200/0] via 10.101.10.10, Vlan2 via 10.101.10.11, Vlan2 via 10.101.10.12, Vlan2 B E 10.101.8.130/32 [200/0] via 10.101.10.10, Vlan2 via 10.101.10.11, Vlan2 via 10.101.10.12, Vlan2 O 10.101.8.0/24 [110/20] via 192.168.75.3, Ethernet1/1 via 192.168.75.1, Ethernet2/1 Example of how BGP routes would look like in the switch located in Highspeed network.\nsw-spine-001 [standalone: master] # show ip bgp vrf CAN summary VRF name : CAN BGP router identifier : 192.168.75.1 local AS number : 65533 BGP table version : 665 Main routing table version: 665 IPV4 Prefixes : 44 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.101.8.8 4 65536 24725 27717 665 0 0 0:11:52:43 ESTABLISHED/14 10.101.8.9 4 65536 24836 27692 665 0 0 0:08:44:20 ESTABLISHED/16 10.101.8.10 4 65536 24704 27741 665 0 0 0:08:44:18 ESTABLISHED/14 Configure default routes on Workers\nThe default route will need to change on the workers so they send their traffic out the HSN interface. ncn-w001# ip route replace default via 10.101.10.1 dev hsn0\nTo make it persistent we will need to create an ifcfg file for hsn0 and remove the old vlan7 default route. ncn-w001# mv /etc/sysconfig/network/ifroute-bond0.cmn0 /etc/sysconfig/network/ifroute-bond0.cmn0.old\nncn-w001# echo \u0026ldquo;default 10.101.10.1 - -\u0026rdquo; \u0026gt; /etc/sysconfig/network/ifroute-hsn0\nVerify the routing table and external connectivity. ncn-w001# ip route default via 10.101.10.1 dev hsn0 ncn-w001# ping 8.8.8.8 -c 1 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=110 time=13.6 ms Verify External Connectivity\nYou should now have external connectivity from outside the Shasta system to the external services offered by MetalLB over the HSN. Verify you are going over the HSN by a traceroute. NCN-m001 ~ % traceroute 10.101.8.113 traceroute to 10.101.8.113 (10.101.8.113), 64 hops max, 52 byte packets 1 172.30.252.234 (172.30.252.234) 37.652 ms 37.930 ms 36.574 ms 2 10.103.255.228 (10.103.255.228) 37.684 ms 37.180 ms 36.765 ms 3 10.103.255.249 (10.103.255.249) 36.531 ms 38.350 ms 39.593 ms 4 172.30.254.219 (172.30.254.219) 38.543 ms 38.699 ms 40.811 ms 5 172.30.254.212 (172.30.254.212) 37.931 ms 37.347 ms 40.404 ms 6 172.30.254.243 (172.30.254.243) 47.029 ms 39.014 ms 38.292 ms 7 172.30.254.134 (172.30.254.134) 42.197 ms 37.267 ms 38.522 ms 8 172.30.254.130 (172.30.254.130) 39.562 ms 38.094 ms 39.500 ms 9 10.101.15.254 (10.101.15.254) 37.616 ms 37.741 ms 37.529 ms 10 10.101.15.178 (10.101.15.178) 39.465 ms 37.052 ms 36.734 ms 11 10.101.8.113 (10.101.8.113) 39.937 ms 38.565 ms 36.524 ms You can also listen on all the HSN interfaces for ping/traceroute while you ping the external facing iP, in this example 10.101.8.113. ncn-m001# nodes=$(kubectl get nodes| awk \u0026#39;{print $1}\u0026#39; | grep ncn-w | awk -vORS=, \u0026#39;{print $1}\u0026#39;); pdsh -w ${nodes} \u0026#34;tcpdump -envli hsn0 icmp\u0026#34; ncn-w002: tcpdump: listening on hsn0, link-type EN10MB (Ethernet), capture size 262144 bytes ncn-w003: tcpdump: listening on hsn0, link-type EN10MB (Ethernet), capture size 262144 bytes ncn-w001: tcpdump: listening on hsn0, link-type EN10MB (Ethernet), capture size 262144 bytes ncn-w003: 04:59:35.826691 98:5d:82:71:ba:2d \u0026gt; 02:00:00:00:00:1e, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 54, id 951, offset 0, flags [none], proto ICMP (1), length 84) ncn-w003: 172.25.64.129 \u0026gt; 10.101.8.113: ICMP echo request, id 37368, seq 0, length 64 ncn-w003: 04:59:36.825591 98:5d:82:71:ba:2d \u0026gt; 02:00:00:00:00:1e, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 54, id 33996, offset 0, flags [none], proto ICMP (1), length 84) Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/dell/vlan_trunking_8021q/",
	"title": "VLAN Trunking 802.1Q",
	"tags": [],
	"description": "",
	"content": "VLAN Trunking 802.1Q A trunk port carries packets on one or more specified VLANs. Packet that ingress on a trunk port are in the VLAN specified in its 802.1Q header, or native VLAN if the packet has no 802.1Q header. A packet that egresses through a trunk port will have an 802.1Q header if it has a nonzero VLAN ID. Any packet that ingresses on a trunk port tagged with a VLAN that the port does not trunk is dropped.\nConfiguration Commands Configure an interface as a trunk port:\nswitch(config-if)# switchport mode trunk Add the allowed VLANs:\nswitch(config-if)#switchport trunk allowed vlan add 1,50,100 Assign a native VLAN:\nswitch(config-if)# switchport trunk native vlan-id 1 Show commands to validate functionality:\nswitch# show interfaces switchport Expected Results Administrators can create and enable multiple VLAN interfaces Administrators can assign the trunk VLAN interfaces Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/lldp/",
	"title": "Link Layer Discovery Protocol (LLDP)",
	"tags": [],
	"description": "",
	"content": "Link Layer Discovery Protocol (LLDP) LLDP is used to advertise the device\u0026rsquo;s identity and abilities and read other devices connected to the same network.\nNOTE: LLDP is enabled by default.\nConfiguration Commands Enable an interface to receive or transmit LLDP packets:\nswitch(config-if)# lldp \u0026lt;receive|transmit\u0026gt; Show commands to validate functionality:\nswitch# show lldp [local-device|neighbor-info|statistics] Example Output switch# show lldp configuration LLDP Global Configuration: LLDP Enabled :Yes LLDP Transmit Interval :30 LLDP Hold time Multiplier :4 LLDP Transmit Delay Interval:2 LLDP Reinit time Interval :2 Optional TLVs configured: Management Address Port description Port VLAN-ID System capabilities System description System name LLDP Port Configuration: Port Tx-Enabled Rx-Enabled 1/1/1 Yes Yes ... switch# show lldp local-device Global Data --------------- Chassis-id 60 Total Packets transmitted : 198 Total Packets received : 170 Total Packet received and discarded : 0 Total TLVs unrecognized : 0 LLDP Port Statistics: Port-ID Tx-Packets Rx-packets Rx-discarded TLVs-Unknown 1/1/1 70 43 0 0 1/1/3 70 70 0 0 Expected Results Link status between the peer devices is UP LLDP is enabled Local device LLDP Information is displayed Remote device LLDP information is displayed LLDP statistics are displayed Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/restore_bare-metal_etcd_clusters_from_an_s3_snapshot/",
	"title": "Restore Bare-Metal etcd Clusters from an S3 Snapshot",
	"tags": [],
	"description": "",
	"content": "Restore Bare-Metal etcd Clusters from an S3 Snapshot The etcd cluster that serves Kubernetes on master nodes is backed up every 10 minutes. These backups are pushed to Ceph Rados Gateway (S3).\nRestoring the etcd cluster from backup is only meant to be used in a catastrophic scenario, in which the Kubernetes cluster and master nodes are being rebuilt. This procedure shows how to restore the bare-metal etcd cluster from a Simple Storage Service (S3) snapshot.\nThe etcd cluster needs to be restored from a backup when the Kubernetes cluster and master nodes are being rebuilt.\nPrerequisites The Kubernetes cluster on master nodes is being rebuilt.\nProcedure Preparation Restore member directory Restart the cluster 1. Preparation This procedure can be run on any master NCN.\nSelect a snapshot to restore a backup.\nList the available backups.\nncn-m# cd /opt/cray/platform-utils/s3 \u0026amp;\u0026amp; ./list-objects.py --bucket-name etcd-backup Example output:\nbare-metal/etcd-backup-2020-02-04-18-00-10.tar.gz bare-metal/etcd-backup-2020-02-04-18-10-06.tar.gz bare-metal/etcd-backup-2020-02-04-18-20-02.tar.gz bare-metal/etcd-backup-2020-02-04-18-30-10.tar.gz bare-metal/etcd-backup-2020-02-04-18-40-06.tar.gz bare-metal/etcd-backup-2020-02-04-18-50-03.tar.gz Set the BACKUP_NAME variable to the file name of the desired backup from the list.\nOmit the bare-metal/ prefix shown in the output of the previous command, as well as the .tar.gz suffix.\nFor example:\nncn-m# BACKUP_NAME=etcd-backup-2020-02-04-18-50-03 Download the snapshot and copy it to all NCN master nodes.\nRetrieve the backup from S3 and uncompress it.\nncn-m# mkdir /tmp/etcd_restore ncn-m# cd /opt/cray/platform-utils/s3 ncn-m# ./download-file.py --bucket-name etcd-backup \\ --key-name \u0026#34;bare-metal/${BACKUP_NAME}.tar.gz\u0026#34; \\ --file-name \u0026#34;/tmp/etcd_restore/${BACKUP_NAME}.tar.gz\u0026#34; ncn-m# cd /tmp/etcd_restore ncn-m# gunzip \u0026#34;${BACKUP_NAME}.tar.gz\u0026#34; ncn-m# tar -xvf \u0026#34;${BACKUP_NAME}.tar\u0026#34; ncn-m# mv -v \u0026#34;${BACKUP_NAME}/etcd-dump.bin\u0026#34; /tmp Push the file to the other NCN master nodes.\nIf not running these steps on ncn-m001, adjust the NCN names in the following command accordingly.\nncn-m# scp /tmp/etcd-dump.bin ncn-m002:/tmp ncn-m# scp /tmp/etcd-dump.bin ncn-m003:/tmp 2. Restore member directory The following procedure must be performed on all master nodes, one at a time. The order does not matter.\nCreate a new temporary /tmp/etcd_restore directory, if it does not already exist.\nncn-m# mkdir -pv /tmp/etcd_restore Change to the /tmp/etcd_restore directory.\nncn-m# cd /tmp/etcd_restore Retrieve values from the kubeadmcfg.yaml file.\nThese values will be saved in variables and used in the following step.\nRetrieve the node name.\nThe value should be the name of the master node where this command is being run (for example, ncn-m002).\nncn-m# NODE_NAME=$(yq r /etc/kubernetes/kubeadmcfg.yaml \u0026#39;etcd.local.extraArgs.name\u0026#39;) ; echo \u0026#34;${NODE_NAME}\u0026#34; Retrieve the initial cluster.\nncn-m# INIT_CLUSTER=$(yq r /etc/kubernetes/kubeadmcfg.yaml \u0026#39;etcd.local.extraArgs.initial-cluster\u0026#39;); echo \u0026#34;${INIT_CLUSTER}\u0026#34; Example output:\nncn-m001=https://10.252.1.10:2380,ncn-m002=https://10.252.1.9:2380,ncn-m003=https://10.252.1.8:2380 Retrieve the initial advertise peer URLs.\nncn-m# INIT_URLS=$(yq r /etc/kubernetes/kubeadmcfg.yaml \u0026#39;etcd.local.extraArgs.initial-advertise-peer-urls\u0026#39;); echo \u0026#34;${INIT_URLS}\u0026#34; Example output:\nhttps://10.252.1.10:2380 Restore the member directory.\nncn-m# ETCDCTL_API=3 etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \\ --cert /etc/kubernetes/pki/etcd/server.crt \\ --key /etc/kubernetes/pki/etcd/server.key \\ --name \u0026#34;${NODE_NAME}\u0026#34; \\ --initial-cluster \u0026#34;${INIT_CLUSTER}\u0026#34; \\ --initial-cluster-token tkn \\ --initial-advertise-peer-urls \u0026#34;${INIT_URLS}\u0026#34; \\ snapshot restore /tmp/etcd-dump.bin Repeat the steps in this section on the next master node, until they have been performed on every master node.\n3. Restart the cluster Stop the cluster.\nRun the following command on each master node.\nIf the etcd cluster is not currently running, this step can be skipped.\nncn-m# systemctl stop etcd Start the restored etcd cluster on every master node.\nDo the following steps on each master node.\nSet a variable with the node name of the current master node.\nncn-m# NODE_NAME=ncn-mxxx Run the following commands.\nncn-m# rm -rvf /var/lib/etcd/member \u0026amp;\u0026amp; cd /tmp/etcd_restore \u0026amp;\u0026amp; mv -v ${NODE_NAME}.etcd/member/ /var/lib/etcd/ \u0026amp;\u0026amp; systemctl start etcd Confirm the membership of the cluster.\nThis command can be run on any master node.\nncn-m# ETCDCTL_API=3 etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \\ --cert /etc/kubernetes/pki/etcd/server.crt \\ --key /etc/kubernetes/pki/etcd/server.key member list Example output:\n448a8d056377359a, started, ncn-m001, https://10.252.1.7:2380, https://10.252.1.7:2379,https://127.0.0.1:2379, false 986f6ff2a30b01cb, started, ncn-m002, https://10.252.1.8:2380, https://10.252.1.8:2379,https://127.0.0.1:2379, false d5a8e497e2788510, started, ncn-m003, https://10.252.1.9:2380, https://10.252.1.9:2379,https://127.0.0.1:2379, false "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/track_the_status_of_a_session/",
	"title": "Track the Status of a Session",
	"tags": [],
	"description": "",
	"content": "Track the Status of a Session A configuration session can be a long-running process, and depends on many system factors, as well as the number of configuration layers and Ansible tasks that are run in each layer. The Configuration Framework Service (CFS) provides the session status through the session metadata to allow for tracking progress and session state.\nPrerequisites View session status Troubleshooting Prerequisites A configuration session exists in CFS. The Cray CLI must be configured on the node where the commands are being run. See Configure the Cray CLI. View session status To view the session status of a session named example, use the following command:\nncn-mw# cray cfs sessions describe example --format json Example output:\n{ \u0026#34;ansible\u0026#34;: { \u0026#34;config\u0026#34;: \u0026#34;cfs-default-ansible-cfg\u0026#34;, \u0026#34;limit\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;verbosity\u0026#34;: 0 }, \u0026#34;configuration\u0026#34;: { \u0026#34;limit\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;artifacts\u0026#34;: [], \u0026#34;session\u0026#34;: { \u0026#34;completionTime\u0026#34;: \u0026#34;2020-07-28T03:26:30\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;cfs-8c8d628b-ebac-4946-a8b7-f1f167b35b0d\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-07-28T03:26:00\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;complete\u0026#34;, \u0026#34;succeeded\u0026#34;: \u0026#34;true\u0026#34; } }, \u0026#34;tags\u0026#34;: {}, \u0026#34;target\u0026#34;: { \u0026#34;definition\u0026#34;: \u0026#34;dynamic\u0026#34;, \u0026#34;groups\u0026#34;: null } } The jq tool, along with the --format json output option of the CLI, are helpful for filtering the session data to view just the session status:\nncn-mw# cray cfs sessions describe example --format json | jq .status.session Example output:\n{ \u0026#34;completionTime\u0026#34;: \u0026#34;2020-07-28T03:26:30\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;cfs-8c8d628b-ebac-4946-a8b7-f1f167b35b0d\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-07-28T03:26:00\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;complete\u0026#34;, \u0026#34;succeeded\u0026#34;: \u0026#34;true\u0026#34; } The status section of the cray cfs session describe command output will not be populated until the CFS session Kubernetes job has started.\nThe .status.session mapping shows the overall status of the configuration session. The .succeeded key within this mapping is a string with values of either \u0026quot;true\u0026quot;, \u0026quot;false\u0026quot;, \u0026quot;unknown\u0026quot;, or \u0026quot;none\u0026quot;.\n\u0026quot;none\u0026quot; occurs if the session has not yet completed, and \u0026quot;unknown\u0026quot; occurs when the session is deleted mid-run, there is an error creating the session and it never starts, or any similar case where checking the session status would fail to find the underlying Kubernetes job running the CFS session.\nValues of .status can be \u0026quot;pending\u0026quot;, \u0026quot;running\u0026quot;, or \u0026quot;complete\u0026quot;.\nTroubleshooting If a session is not starting, then see Troubleshoot CFS Sessions Failing to Start.\nIf a session is starting but not completing, then see Troubleshoot CFS Session Failing to Complete.\nIf a session completed but did not succeed, then see Troubleshoot Ansible Play Failures in CFS Sessions.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/stage_changes_without_bos/",
	"title": "Stage Changes Without BOS",
	"tags": [],
	"description": "",
	"content": "Stage Changes Without BOS Sometimes there is a need to stages changes to take place on a reboot, without immediately rebooting a node. When this is called for, users can bypass BOS, and set boot artifacts or configuration that will only take place when a node is later booted, whether that occurs manually, or triggered by a task manager.\nStage Boot Artifacts For information on staging boot artifacts, see the section Upload Node Boot Information to Boot Script Service (BSS).\nStage a Configuration Disable CFS for all nodes receiving the staged configuration. Nodes will automatically re-enable configuration when they are rebooted and will be configured with any staged changes.\nncn-m001# cray cfs components update \u0026lt;xname\u0026gt; --enabled false Either set the new desired configuration, or update the existing configuration.\nIf an entirely new configuration is being used, or if no configuration was previously set for a component, update the configuration name with the following:\nncn-m001# cray cfs components update \u0026lt;xname\u0026gt; --configuration-name \u0026lt;configuration_name\u0026gt; If all nodes that share a configuration are being staged with an update, updating the shared configuration will stage the change for all relevant nodes. Be aware that if this step is taken and not all nodes that use the configuration are disabled in CFS, the configuration will automatically and immediately apply to all enabled nodes that are using it.\nncn-m001# cray cfs configurations update \u0026lt;configuration_name\u0026gt; --file \u0026lt;file_path\u0026gt; Users also have the option of specifying branches rather than commits in configurations. If this feature is used, the configuration can also be updated by telling CFS to update the commits for all layers of a configuration that specify branches. Like with updating the configuration from a file, this will automatically start configuration on any enabled nodes that are using this configuration. For information on using branches, see the section (Use Branches in Configuration Layers)\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/list_available_uai_images_in_legacy_mode/",
	"title": "List Available UAI Images in Legacy Mode",
	"tags": [],
	"description": "",
	"content": "List Available UAI Images in Legacy Mode A user can list the UAI images available for creating a UAI with a command of the form:\nuser\u0026gt; cray uas images list For example:\nvers\u0026gt; cray uas images list default_image = \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34; image_list = [ \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34;, \u0026#34;registry.local/cray/cray-uai-sanity-test:1.2.4\u0026#34;, \u0026#34;registry.local/cray/cray-uai-broker:1.2.4\u0026#34;,] Top: User Access Service (UAS)\nNext Topic: Create UAIs From Specific UAI Images in Legacy Mode\n"
},
{
	"uri": "/docs-csm/en-12/install/pxe_boot_troubleshooting/",
	"title": "PXE Boot Troubleshooting",
	"tags": [],
	"description": "",
	"content": "PXE Boot Troubleshooting This page is designed to cover various issues that arise when trying to PXE boot nodes in an HPE Cray EX system.\nConfiguration required for PXE booting Switch configuration Aruba configuration Mellanox configuration Next steps Node iPXE retries and NIC order Restart BSS Restart Kea Missing BSS data In order for PXE booting to work successfully, the management network switches need to be configured correctly.\nConfiguration required for PXE booting To successfully PXE boot nodes, the following is required:\nThe ip helper-address must be configured on VLANs 1, 2, 4, and 7. This will be where the layer 3 gateway exists (spine or leaf). The virtual-IP/VSX/MAGP IP address must be configured on VLANs 1, 2, 4, and 7. spine01 and spine02 need an active gateway on VLAN 1; this can be identified from MTL.yaml generated by CSI. spine01 and spine02 need an ip helper-address on VLAN 1 pointing to 10.92.100.222. Switch configuration Aruba configuration Check the configuration for interface vlan x.\nThis configuration will be the same on BOTH Switches (except the ip address). There will be an active-gateway and ip helper-address configured.\nsw-spine-001(config)# int vlan 1,2,4,7 sw-spine-001(config-if-vlan-\u0026lt;1,2,4,7\u0026gt;)# show run current-context Example output:\ninterface vlan 1 ip mtu 9198 ip address 10.1.0.2/16 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 10.1.0.1 ip helper-address 10.92.100.222 interface vlan 2 vsx-sync active-gateways ip mtu 9198 ip address 10.252.0.2/17 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.252.0.1 ip helper-address 10.92.100.222 ip ospf 1 area 0.0.0.0 interface vlan 4 vsx-sync active-gateways ip mtu 9198 ip address 10.254.0.2/17 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.254.0.1 ip helper-address 10.94.100.222 ip ospf 1 area 0.0.0.0 interface vlan 7 ip mtu 9198 ip address 10.103.11.1/24 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.103.11.111 ip helper-address 10.92.100.222 If any of this configuration is missing, then update it on BOTH switches.\nsw-spine-002# conf t sw-spine-002(config)# int vlan 1 sw-spine-002(config-if-vlan)# ip helper-address 10.92.100.222 sw-spine-002(config-if-vlan)# active-gateway ip mac 12:01:00:00:01:00 sw-spine-002(config-if-vlan)# active-gateway ip 10.1.0.1 sw-spine-002# conf t sw-spine-002(config)# int vlan 2 sw-spine-002(config-if-vlan)# ip helper-address 10.92.100.222 sw-spine-002(config-if-vlan)# active-gateway ip mac 12:01:00:00:01:00 sw-spine-002(config-if-vlan)# active-gateway ip 10.252.0.1 sw-spine-002# conf t sw-spine-002(config)# int vlan 4 sw-spine-002(config-if-vlan)# ip helper-address 10.94.100.222 sw-spine-002(config-if-vlan)# active-gateway ip mac 12:01:00:00:01:00 sw-spine-002# conf t sw-spine-002(config)# int vlan 7 sw-spine-002(config-if-vlan)# ip helper-address 10.92.100.222 sw-spine-002(config-if-vlan)# active-gateway ip mac 12:01:00:00:01:00 sw-spine-002(config-if-vlan)# active-gateway ip xxxxxxx sw-spine-002(config-if-vlan)# write mem Mellanox configuration Check the configuration for interface vlan 1.\nThis configuration will be the same on BOTH Switches (except the ip address). magp and ip dhcp relay will be configured.\nsw-spine-001 [standalone: master] # show run int vlan 1 Example output:\ninterface vlan 1 interface vlan 1 ip address 10.1.0.2/16 primary interface vlan 1 ip dhcp relay instance 2 downstream interface vlan 1 magp 1 interface vlan 1 magp 1 ip virtual-router address 10.1.0.1 interface vlan 1 magp 1 ip virtual-router mac-address 00:00:5E:00:01:01 If this configuration is missing, then add it to BOTH switches.\nsw-spine-001 [standalone: master] # conf t sw-spine-001 [standalone: master] (config) # interface vlan 1 magp 1 sw-spine-001 [standalone: master] (config interface vlan 1 magp 1) # ip virtual-router address 10.1.0.1 sw-spine-001 [standalone: master] (config interface vlan 1 magp 1) # ip virtual-router mac-address 00:00:5E:00:01:01 sw-spine-001 [standalone: master] # conf t sw-spine-001 [standalone: master] (config) # ip dhcp relay instance 2 vrf default sw-spine-001 [standalone: master] (config) # ip dhcp relay instance 2 address 10.92.100.222 sw-spine-001 [standalone: master] (config) # interface vlan 2 ip dhcp relay instance 2 downstream Verify the VLAN 1 MAGP configuration.\nsw-spine-001 [standalone: master] # show magp 1 Example output:\nMAGP 1: Interface vlan: 1 Admin state : Enabled State : Master Virtual IP : 10.1.0.1 Virtual MAC : 00:00:5E:00:01:01 Verify the DHCP relay configuration.\nsw-spine-001 [standalone: master] (config) # show ip dhcp relay instance 2 Example output:\nVRF Name: default DHCP Servers: 10.92.100.222 DHCP relay agent options: always-on : Disabled Information Option: Disabled UDP port : 67 Auto-helper : Disabled ------------------------------------------- Interface Label Mode ------------------------------------------- vlan1 N/A downstream vlan2 N/A downstream vlan7 N/A downstream Verify that the route to the TFTP server and the route for the ingress gateway are available.\nsw-spine-001 [standalone: master] # show ip route 10.92.100.60 Example output:\nFlags: F: Failed to install in H/W B: BFD protected (static route) i: BFD session initializing (static route) x: protecting BFD session failed (static route) c: consistent hashing p: partial programming in H/W VRF Name default: ------------------------------------------------------------------------------------------------------ Destination Mask Flag Gateway Interface Source AD/M ------------------------------------------------------------------------------------------------------ default 0.0.0.0 c 10.101.15.161 eth1/12 static 1/1 10.92.100.60 255.255.255.255 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 c 10.252.0.7 vlan2 bgp 200/0 sw-spine-001 [standalone: master] # show ip route 10.92.100.71 Example output:\nFlags: F: Failed to install in H/W B: BFD protected (static route) i: BFD session initializing (static route) x: protecting BFD session failed (static route) c: consistent hashing p: partial programming in H/W VRF Name default: ------------------------------------------------------------------------------------------------------ Destination Mask Flag Gateway Interface Source AD/M ------------------------------------------------------------------------------------------------------ default 0.0.0.0 c 10.101.15.161 eth1/12 static 1/1 10.92.100.71 255.255.255.255 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 c 10.252.0.7 vlan2 bgp 200/0 Next steps If the configuration looks good and PXE boot is still not working, then there are some other things to try.\nNode iPXE retries and NIC order In some environments, during the Deploy Final NCN reboot step, ncn-m001 may loop through all of its NICs and still fail to PXE boot, even after the third chain attempt. The NIC boot ordering used by default is designed to be optimal for multiple types of hardware and cabling, but it may need to be edited for specific environments in order to reduce the boot time of ncn-m001.\nIf the boot issues described above are observed, then follow the steps in the Edit the iPXE Embedded Boot Script procedure, adjusting the NIC boot order such that net0, or others, come before net2. If that does not resolve the issue, then return to this page.\nRestart BSS Restart the Boot Script Service (BSS) if the following output is returned on the console during an NCN PXE boot attempt (specifically the 404 Not Found error at the bottom):\nhttps://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript...X509 chain 0x6d35c548 added X509 0x6d360d68 \u0026#34;eniac.dev.cray.com\u0026#34; X509 chain 0x6d35c548 added X509 0x6d3d62e0 \u0026#34;Platform CA - L1 (a0b073c8-5c9c-4f89-b8a2-a44adce3cbdf)\u0026#34; X509 chain 0x6d35c548 added X509 0x6d3d6420 \u0026#34;Platform CA (a0b073c8-5c9c-4f89-b8a2-a44adce3cbdf)\u0026#34; EFITIME is 2021-02-26 21:55:04 HTTP 0x6d35da88 status 404 Not Found Roll out a restart of the BSS deployment.\nncn-mw# kubectl -n services rollout restart deployment cray-bss Example output:\ndeployment.apps/cray-bss restarted Wait for this command to return (it will block showing status as the pods are refreshed):\nncn-mw# # kubectl -n services rollout status deployment cray-bss Example output:\nWaiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 old replicas are pending termination... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 old replicas are pending termination... deployment \u0026#34;cray-bss\u0026#34; successfully rolled out Reboot the NCN that failed to PXE boot.\nRestart KEA In some cases, rebooting the Kea pod has resolved PXE issues.\nGet the Kea pod.\nncn-mw# kubectl get pods -n services | grep kea Example output:\ncray-dhcp-kea-6bd8cfc9c5-m6bgw 3/3 Running 0 20h Delete the Kea pod.\nncn-mw# kubectl delete pods -n services cray-dhcp-kea-6bd8cfc9c5-m6bgw Missing BSS data If the PXE boot is giving 404 errors, this could be because the necessary information is not in BSS. The information is uploaded into BSS with the csi handoff bss-metadata and csi handoff bss-update-cloud-init commands in the Deploy Final NCN Handoff Data procedure. If these commands failed or were skipped accidentally, then this will cause the ncn-m001 PXE boot to fail.\nIn that case, then use the following recovery procedure.\nReboot to the PIT.\nIf using a USB PIT, follow this procedure:\nReboot the PIT node, watching the console as it boots.\nManually stop it at the boot menu.\nSelect the USB device for the boot.\nOnce booted, log in and mount the data partition.\npit# mount -vL PITDATA If using a remote ISO PIT, follow the Bootstrap LiveCD Remote ISO procedure up through (and including) the Set Up The Site Link step.\nSet variables for the system name, the CAN IP address for ncn-m002, the Kubernetes version, and the Ceph version.\nThe CAN IP address for ncn-m002 is obtained at this step of the Deploy Final NCN procedure.\nThe Kubernetes and Ceph versions are from the output of the csi handoff ncn-images command in the Deploy Final NCN procedure. If needed, the typescript file from that procedure should be on ncn-m002 and ncn-m003 in the /metal/bootstrap/prep/admin directory.\nSubstitute the correct values for the system in use in the following commands:\npit# SYSTEM_NAME=eniac pit# CAN_IP_NCN_M002=a.b.c.d pit# export KUBERNETES_VERSION=m.n.o pit# export CEPH_VERSION=x.y.z If using a remote ISO PIT, run the following commands to finish configuring the network and copy files.\nSkip these steps if using a USB PIT.\nRun the following command to copy files from ncn-m002 to the PIT node.\npit# scp -p ${CAN_IP_NCN_M002}:/metal/bootstrap/prep/${SYSTEM_NAME}/pit-files/* /etc/sysconfig/network/ Apply the network changes.\npit# wicked ifreload all pit# systemctl restart wickedd-nanny \u0026amp;\u0026amp; sleep 5 Copy data.json from ncn-m002 to the PIT node.\npit# mkdir -p /var/www/ephemeral/configs pit# scp ${CAN_IP_NCN_M002}:/metal/bootstrap/prep/${SYSTEM_NAME}/basecamp/data.json /var/www/ephemeral/configs Copy Kubernetes configuration file from ncn-m002.\npit# mkdir -pv ~/.kube pit# scp ${CAN_IP_NCN_M002}:/etc/kubernetes/admin.conf ~/.kube/config Set DNS to use unbound.\npit# echo \u0026#34;nameserver 10.92.100.225\u0026#34; \u0026gt; /etc/resolv.conf Export an API token.\npit# export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Re-run the BSS handoff commands from the Deploy Final NCN procedure.\nWARNING: These commands should never be run from a node other than the PIT node or ncn-m001\npit# csi handoff bss-metadata --data-file /var/www/ephemeral/configs/data.json || echo \u0026#34;ERROR: csi handoff bss-metadata failed\u0026#34; pit# csi handoff bss-update-cloud-init --set meta-data.dns-server=10.92.100.225 --limit Global Perform the Restart BSS and the Restart Kea procedures.\nReboot the PIT node.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/troubleshoot_rgw_health_check_fail/",
	"title": "Troubleshoot if RGW Health Check Fails",
	"tags": [],
	"description": "",
	"content": "Troubleshoot if RGW Health Check Fails Use this procedure to determine why the RGW health check failed and what needs to be fixed.\nProcedure In the goss test output, look at the value of x in Expected \\\u0026lt; int \\\u0026gt;: x (possible values are 1, 2, 3, 4, 5). Based on the value, navigate to the corresponding numbered item below for troubleshooting this issue.\n(Optional) Manually run the rgw health check script to see descriptive output.\nncn-m001# GOSS_BASE=/opt/cray/tests/install/ncn /opt/cray/tests/install/ncn/scripts/rgw_health_check.sh A value of 1 is returned if unable to connect to rgw-vip. This happens if any of the following three commands fail.\nncn-m001# curl -i -s -S -k https://rgw-vip.nmn ncn-m001# curl -i -s -S http://rgw-vip.nmn ncn-m001# curl -i -s -S http://rgw-vip.hmn Log into a storage node and look at the version and status of Ceph.\nncn-s# ceph --version ncn-s# ceph -s A value of 2 is returned if a storage node is not able to be reached. In this case, run the rgw_health_check.sh as stated in the optional step above. Find which storage nodes are not able to be reached, and run the following checks on those nodes.\nCheck if HAProxy is running on the node.\nncn-s# systemctl status haproxy If HAProxy is not running, restart it and check the status again.\nncn-s# systemctl restart haproxy ncn-s# systemctl status haproxy Check if keepalived is running on the node.\nncn-s# systemctl status keepalived.service If keepalived is not running, restart it and check the status again.\nncn-s# systemctl restart keepalived.service ncn-s# systemctl status keepalived.service Check if the ceph-rgw daemon is running.\nncn-s# ceph -s | grep rgw If the ceph-rgw daemon is not running on 3 storage nodes, restart the daemon and watch it come up within a few seconds.\nncn-s# ceph orch ps | grep rgw #use this to wach the daemon start ncn-s# ceph orch daemon restart \u0026lt;name\u0026gt; A value of 3 is returned if a craysys command fails. This implies \u0026lsquo;cloud-init\u0026rsquo; is not healthy. Run the command below to determine the health.\nncn-s# cloud-init query -a If the command above fails, reinitialize \u0026lsquo;cloud-init\u0026rsquo; using the following command.\nncn-s# cloud-init init If a value of 4 or 5 is returned, then rgw-vip and the storage nodes are reachable. The error occurred when attempting to create a bucket, upload an object to a bucket, or download an object from a bucket. This implies Ceph may be unhealthy. Check Ceph status with the following command.\nncn-s# ceph -s If Ceph reports any status other than \u0026ldquo;HEALTH_OK\u0026rdquo;, refer to Utility Storage for general Ceph troubleshooting.\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/get_a_long-lived_token_for_a_service_account/",
	"title": "Get a Long-Lived Token for a Service Account",
	"tags": [],
	"description": "",
	"content": "Get a Long-Lived Token for a Service Account Set up a long-lived offline token for a service account using the Keycloak REST API. Keycloak implements the OpenID Connect protocol, so this is a standard procedure for any OpenID Connect server.\nRefer to Offline Access in the official Keycloak documentation for more information.\nPrerequisites A client or service account has been created. See Create a Service Account in Keycloak. The CLIENT_SECRET variable has been set up. See Retrieve the Client Secret for Service Accounts. Get a long-lived token for a service account Replace the my-test-client value in the command below with the ID of the target client. The scope option should be set to offline_access. Get a long-lived token for a service account with the following command:\nncn-mw# curl -s -d grant_type=client_credentials -d client_id=\u0026#34;my-test-client\u0026#34; -d client_secret=\u0026#34;${CLIENT_SECRET}\u0026#34; -d scope=offline_access \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq Example output:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;longAsciiStringTruncated\u0026#34;, \u0026#34;expires_in\u0026#34;: 31536000, \u0026#34;refresh_expires_in\u0026#34;: 0, \u0026#34;refresh_token\u0026#34;: \u0026#34;anotherLongAsciiStringTruncated\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34;, \u0026#34;not-before-policy\u0026#34;: 0, \u0026#34;session_state\u0026#34;: \u0026#34;80a96e21-0942-447e-b1d7-21da55d3ff4a\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;profile offline_access email\u0026#34; } Two things are important in the returned response compared to when requesting an \u0026ldquo;online\u0026rdquo; token:\nThe refresh_expires_in value is 0. The refresh token will not expire and become invalid by itself. The refresh tokens can be revoked via administrative action in Keycloak. The refresh_token value can be used to get a fresh token any time and will be needed if the access token expires (which will happen in 31,536,000 seconds after the access token was issued). Refresh a long-lived token for a service account Replace the my-test-client value in the command below with the ID of the target client. Replace the REFRESH_TOKEN value with the string returned in the previous section. The grant_type option is set to refresh_token. Refresh a long-lived token for a service account with the following command:\nncn-mw# curl -s -d grant_type=refresh_token -d client_id=\u0026#34;my-test-client\u0026#34; -d client_secret=\u0026#34;${CLIENT_SECRET}\u0026#34; -d refresh_token=\u0026#34;REFRESH_TOKEN\u0026#34; \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq Example output:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;longAsciiStringTruncated\u0026#34;, \u0026#34;expires_in\u0026#34;: 31536000, \u0026#34;refresh_expires_in\u0026#34;: 0, \u0026#34;refresh_token\u0026#34;: \u0026#34;anotherLongAsciiStringTruncated\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34;, \u0026#34;not-before-policy\u0026#34;: 0, \u0026#34;session_state\u0026#34;: \u0026#34;80a96e21-0942-447e-b1d7-21da55d3ff4a\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;profile offline_access email\u0026#34; } "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/dump_a_non-compute_node/",
	"title": "Dump a Non-Compute Node",
	"tags": [],
	"description": "",
	"content": "Dump a Non-Compute Node Trigger a non-compute node (NCN) memory dump and send the dump for analysis. This procedure is helpful for debugging NCN crashes.\nPrerequisites An NCN has crashed or an administrator has triggered a node crash.\nProcedure Force a dump on an NCN.\nncn-m001# echo c \u0026gt; /proc/sysrq-trigger Wait for the node to reboot.\nThe NCN dump is stored in /var/crash on the local disk after the node is rebooted.\nCollect the dump data using the System Diagnostic Utility (SDU).\nRefer to the \u0026ldquo;Run a Triage Collection with SDU\u0026rdquo; procedure in the SDU product stream documentation for more information about collecting dump data.\nThe --start_time command option can be customized. For example, -1 day, -2 hours, or a date/time string can be used. For more information on the SDU command options, use the sdu --help command.\nncn-m001# sdu --scenario triage --start_time DATE_OR_TIME_STRING --end_time DATE_OR_TIME_STRING --plugin ncn.gather.nodes --plugin ncn.gather.kernel.dumps Refer to the SUSE documentation for more information on memory dumps or crash dumps.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/management_network_function_in_detail/",
	"title": "Management network functions in detail",
	"tags": [],
	"description": "",
	"content": "Management network functions in detail Edge: Any interactions with the customer network or Internet Customer jobs (Customer Access Network (CAN)) User-facing cloud APIs User Access Instances (UAIs) Customer administration (Customer Management Network (CMN)) Administrative access to the system by customer administrators Access from the system to external services: Customer/Internet DNS LDAP authentication System installation and upgrade media (e.g. Nexus) System: Access by the machine to external (customer and/or Internet) resources (e.g. Internal DNS lookups may resolve to an external DNS). Internal: Node-to-node communication inside the system Administrative Hardware (Hardware Management Network (HMN)) Direct BMC/iLOM access Hardware discovery Firmware updates Cloud control plane (Node Management Network (NMN)) Job control plane (NMN) Services Traditional network services (e.g. TFTP, DHCP, DNS) Cloud API and control Cloud-based system services Jobs Traditional User Access Node (UAN) New UAI Storage Ceph (IP-based storage) "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/external_user_guides/",
	"title": "External User Guides",
	"tags": [],
	"description": "",
	"content": "External User Guides Refer to the following vendor-specific user guides for more information on Aruba, Dell, and Mellanox switches.\nAruba https://asp.arubanetworks.com/downloads Dell https://www.dell.com/support/manuals/en-us/force10-s4048-on/smartfabric-os-user-guide-10-5-1 Mellanox https://docs.nvidia.com/networking/spaces/viewspace.action?key=Onyxv393210 "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/locator_led/",
	"title": "Locator LED",
	"tags": [],
	"description": "",
	"content": "Locator LED The Locator LED is an LED in the front of the chassis that you can turn on or make flash. This is a handy feature when guiding someone to your switch during a \u0026ldquo;remote hands\u0026rdquo; situation, such as asking data center engineer to run a cable to your switch.\nConfiguration commands Enable LED:\nswitch# led locator \u0026lt;flashing|off|on\u0026gt; Show commands to validate functionality:\nswitch# show environment led Example output switch# show environment led Name State Status ----------------------------------- locator off ok switch# led locator flashing switch# show system led Name State Status ----------------------------------- locator flashing ok switch# led locator on switch# show system led Name State Status ----------------------------------- locator on ok switch# led locator off switch# show system led Name State Status ----------------------------------- locator off ok Expected results The Locator LED should be in the off state The Locator LED is now flashing The show command shows the LED is in the flashing state The Locator LED is lit a solid color and it does not flash The show command shows the LED is in the on state The LED is no longer lit The show command shows the LED is in the off state Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/restore_postgres/",
	"title": "Restore Postgres",
	"tags": [],
	"description": "",
	"content": "Restore Postgres Below are the service-specific steps required to restore data to a Postgres cluster.\nRestore Postgres procedures by service:\nSpire Restore without backup Keycloak Restore Postgres for Keycloak VCS Restore Postgres for VCS HSM Restore from backup Restore without backup SLS Restore from backup Restore without backup Restore Postgres for Keycloak In the event that the Keycloak Postgres cluster must be rebuilt and the data restored, then the following procedures are recommended.\nRestore Postgres for Keycloak: Prerequisites A dump of the database exists. The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Restore Postgres for Keycloak: Procedure Copy the database dump to an accessible location.\nIf a manual dump of the database was taken, then check that the dump file exists in a location off the Postgres cluster. It will be needed in the steps below. If the database is being automatically backed up, then the most recent version of the dump and the secrets should exist in the postgres-backup S3 bucket. These will be needed in the steps below. List the files in the postgres-backup S3 bucket and if the files exist, download the dump and secrets out of the S3 bucket. The cray artifacts CLI can be used list and download the files. Note that the .psql file contains the database dump and the .manifest file contains the secrets. Setup the CRAY_CREDENTIALS environment variable to permit simple CLI operations needed while restoring the Keycloak database. See Authenticate an Account with the Command Line.\nList the available backups.\nncn-mw# cray artifacts list postgres-backup --format json | jq -r \u0026#39;.artifacts[].Key | select(contains(\u0026#34;keycloak\u0026#34;))\u0026#39; Example output:\nkeycloak-postgres-2022-09-14T02:10:05.manifest keycloak-postgres-2022-09-14T02:10:05.psql Set the environment variables to the name of the backup files.\nIn order to avoid a kubectl cp bug, the dump file will be downloaded with a slightly altered name (: characters replaced with - characters).\nncn-mw# MANIFEST=keycloak-postgres-2022-09-14T02:10:05.manifest ncn-mw# DUMPFILE_SRC=keycloak-postgres-2022-09-14T02:10:05.psql ncn-mw# DUMPFILE=${DUMPFILE_SRC//:/-} Download the backup files.\nncn-mw# cray artifacts get postgres-backup \u0026#34;${DUMPFILE_SRC}\u0026#34; \u0026#34;${DUMPFILE}\u0026#34; ncn-mw# cray artifacts get postgres-backup \u0026#34;${MANIFEST}\u0026#34; \u0026#34;${MANIFEST}\u0026#34; Unset the CRAY_CREDENTIALS environment variable:\nunset CRAY_CREDENTIALS rm /tmp/setup-token.json Set helper variables.\nncn-mw# CLIENT=cray-keycloak ncn-mw# NAMESPACE=services ncn-mw# POSTGRESQL=keycloak-postgres Scale the Keycloak service to 0.\nncn-mw# kubectl scale statefulset \u0026#34;${CLIENT}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --replicas=0 Wait for the pods to terminate.\nncn-mw# while kubectl get pods -n \u0026#34;${NAMESPACE}\u0026#34; -l app.kubernetes.io/instance=\u0026#34;${CLIENT}\u0026#34; | grep -qv NAME ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2 done Delete the Keycloak Postgres cluster.\nncn-mw# kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; \u0026gt; postgres-cr.json ncn-mw# kubectl delete -f postgres-cr.json Wait for the pods to terminate.\nncn-mw# while kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; | grep -qv NAME ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2 done Create a new single instance Keycloak Postgres cluster.\nncn-mw# cp postgres-cr.json postgres-orig-cr.json ncn-mw# jq \u0026#39;.spec.numberOfInstances = 1\u0026#39; postgres-orig-cr.json \u0026gt; postgres-cr.json ncn-mw# kubectl create -f postgres-cr.json Wait for the pod to start.\nncn-mw# while ! kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; | grep -qv NAME ; do echo \u0026#34; waiting for pod to start\u0026#34;; sleep 2 done Wait for the Postgres cluster to start running.\nncn-mw# while [ $(kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ] do echo \u0026#34; waiting for postgresql to start running\u0026#34;; sleep 2 done Copy the database dump file to the Postgres member.\nncn-mw# kubectl cp \u0026#34;./${DUMPFILE}\u0026#34; \u0026#34;${POSTGRESQL}-0:/home/postgres/${DUMPFILE}\u0026#34; -c postgres -n \u0026#34;${NAMESPACE}\u0026#34; Restore the data.\nncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -c postgres -n \u0026#34;${NAMESPACE}\u0026#34; -it -- psql -U postgres \u0026lt; \u0026#34;${DUMPFILE}\u0026#34; Errors such as ... already exists can be ignored; the restore can be considered successful when it completes.\nEither update or re-create the keycloak-postgres secrets.\nUpdate the secrets in Postgres.\nIf a manual dump was done, and the secrets were not saved, then the secrets in the newly created Postgres cluster will need to be updated.\nFrom the three keycloak-postgres secrets, collect the password for each Postgres username: postgres, service_account, and standby.\nncn-mw# for secret in postgres.keycloak-postgres.credentials service-account.keycloak-postgres.credentials \\ standby.keycloak-postgres.credentials do echo -n \u0026#34;secret ${secret} username \u0026amp; password: \u0026#34; echo -n \u0026#34;`kubectl get secret \u0026#34;${secret}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -ojsonpath=\u0026#39;{.data.username}\u0026#39; | base64 -d` \u0026#34; echo `kubectl get secret \u0026#34;${secret}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -ojsonpath=\u0026#39;{.data.password}\u0026#39;| base64 -d` done Example output:\nsecret postgres.keycloak-postgres.credentials username \u0026amp; password: postgres ABCXYZ secret service-account.keycloak-postgres.credentials username \u0026amp; password: service_account ABC123 secret standby.keycloak-postgres.credentials username \u0026amp; password: standby 123456 kubectl exec into the Postgres pod.\nncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -c postgres -it -- bash Open a Postgres console.\npod# /usr/bin/psql postgres postgres Update the password for each user to match the values found in the secrets.\nUpdate the password for the postgres user.\npostgres# ALTER USER postgres WITH PASSWORD \u0026#39;ABCXYZ\u0026#39;; Example of successful output:\nALTER ROLE Update the password for the service_account user.\npostgres# ALTER USER service_account WITH PASSWORD \u0026#39;ABC123\u0026#39;; Example of successful output:\nALTER ROLE Update the password for the standby user.\npostgres# ALTER USER standby WITH PASSWORD \u0026#39;123456\u0026#39;; Example of successful output:\nALTER ROLE Exit the Postgres console with the \\q command.\nExit the Postgres pod with the exit command.\nRe-create secrets in Kubernetes.\nIf the Postgres secrets were automatically backed up, then re-create the secrets in Kubernetes.\nDelete and re-create the three keycloak-postgres secrets using the manifest that was copied from S3 earlier.\nncn-mw# kubectl delete secret postgres.keycloak-postgres.credentials \\ service-account.keycloak-postgres.credentials standby.keycloak-postgres.credentials -n \u0026#34;${NAMESPACE}\u0026#34; ncn-mw# kubectl apply -f \u0026#34;${MANIFEST}\u0026#34; Restart the Postgres cluster.\nncn-mw# kubectl delete pod -n \u0026#34;${NAMESPACE}\u0026#34; \u0026#34;${POSTGRESQL}-0\u0026#34; Wait for the postgresql pod to start.\nncn-mw# while ! kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; | grep -qv NAME ; do echo \u0026#34; waiting for pods to start\u0026#34;; sleep 2 done Scale the Postgres cluster back to 3 instances.\nncn-mw# kubectl patch postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --type=\u0026#39;json\u0026#39; \\ -p=\u0026#39;[{\u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/numberOfInstances\u0026#34;, \u0026#34;value\u0026#34; : 3}]\u0026#39; Wait for the postgresql cluster to start running.\nThis may take a few minutes to complete.\nncn-mw# while [ $(kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ] do echo \u0026#34; waiting for postgresql to start running\u0026#34;; sleep 2 done Scale the Keycloak service back to 3 replicas.\nncn-mw# kubectl scale statefulset \u0026#34;${CLIENT}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --replicas=3 Wait for the Keycloak pods to start.\nncn-mw# while [ $(kubectl get pods -n \u0026#34;${NAMESPACE}\u0026#34; -l app.kubernetes.io/instance=\u0026#34;${CLIENT}\u0026#34; | grep -cv NAME) != 3 ] do echo \u0026#34; waiting for pods to start\u0026#34;; sleep 2 done Wait for all Keycloak pods to be ready.\nIf there are pods that do not show that both containers are ready (READY is 2/2), then wait a few seconds and re-run the command until all containers are ready.\nncn-mw# kubectl get pods -n \u0026#34;${NAMESPACE}\u0026#34; -l app.kubernetes.io/instance=\u0026#34;${CLIENT}\u0026#34; Example output:\nNAME READY STATUS RESTARTS AGE cray-keycloak-0 2/2 Running 0 35s cray-keycloak-1 2/2 Running 0 35s cray-keycloak-2 2/2 Running 0 35s Run the keycloak-setup job to restore the Kubernetes client secrets.\nRun the job.\nncn-mw# kubectl get job -n \u0026#34;${NAMESPACE}\u0026#34; -l app.kubernetes.io/instance=cray-keycloak -o json \u0026gt; keycloak-setup.json ncn-mw# cat keycloak-setup.json | jq \u0026#39;.items[0]\u0026#39; | jq \u0026#39;del(.metadata.creationTimestamp)\u0026#39; | jq \u0026#39;del(.metadata.managedFields)\u0026#39; | jq \u0026#39;del(.metadata.resourceVersion)\u0026#39; | jq \u0026#39;del(.metadata.selfLink)\u0026#39; | jq \u0026#39;del(.metadata.uid)\u0026#39; | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; | kubectl replace --force -f - Wait for job to complete.\nCheck the status of the keycloak-setup job. If the COMPLETIONS value is not 1/1, then wait a few seconds and run the command again until the COMPLETIONS value is 1/1.\nncn-mw# kubectl get jobs -n \u0026#34;${NAMESPACE}\u0026#34; -l app.kubernetes.io/instance=cray-keycloak Example output:\nNAME COMPLETIONS DURATION AGE keycloak-setup-2 1/1 59s 91s Run the keycloak-users-localize job to restore the users and groups in S3 and the Kubernetes ConfigMap.\nRun the job.\nncn-mw# kubectl get job -n \u0026#34;${NAMESPACE}\u0026#34; -l app.kubernetes.io/instance=cray-keycloak-users-localize \\ -o json \u0026gt; cray-keycloak-users-localize.json ncn-mw# cat cray-keycloak-users-localize.json | jq \u0026#39;.items[0]\u0026#39; | jq \u0026#39;del(.metadata.creationTimestamp)\u0026#39; | jq \u0026#39;del(.metadata.managedFields)\u0026#39; | jq \u0026#39;del(.metadata.resourceVersion)\u0026#39; | jq \u0026#39;del(.metadata.selfLink)\u0026#39; | jq \u0026#39;del(.metadata.uid)\u0026#39; | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; | kubectl replace --force -f - Wait for the job to complete.\nCheck the status of the cray-keycloak-users-localize job. If the COMPLETIONS value is not 1/1, then wait a few minutes and run the command again until the COMPLETIONS value is 1/1.\nncn-mw# kubectl get jobs -n \u0026#34;${NAMESPACE}\u0026#34; -l app.kubernetes.io/instance=cray-keycloak-users-localize Example output:\nNAME COMPLETIONS DURATION AGE keycloak-users-localize-2 1/1 45s 49s Restart the ingress oauth2-proxies.\nIssue the restarts.\nncn-mw# kubectl rollout restart -n \u0026#34;${NAMESPACE}\u0026#34; deployment/cray-oauth2-proxies-customer-access-ingress \u0026amp;\u0026amp; kubectl rollout restart -n \u0026#34;${NAMESPACE}\u0026#34; deployment/cray-oauth2-proxies-customer-high-speed-ingress \u0026amp;\u0026amp; kubectl rollout restart -n \u0026#34;${NAMESPACE}\u0026#34; deployment/cray-oauth2-proxies-customer-management-ingress Wait for the restarts to complete.\nncn-mw# kubectl rollout status -n \u0026#34;${NAMESPACE}\u0026#34; deployment/cray-oauth2-proxies-customer-access-ingress \u0026amp;\u0026amp; kubectl rollout status -n \u0026#34;${NAMESPACE}\u0026#34; deployment/cray-oauth2-proxies-customer-high-speed-ingress \u0026amp;\u0026amp; kubectl rollout status -n \u0026#34;${NAMESPACE}\u0026#34; deployment/cray-oauth2-proxies-customer-management-ingress Verify that the service is working.\nThe following should return an access_token for an existing user. Replace the \u0026lt;username\u0026gt; and \u0026lt;password\u0026gt; as appropriate.\nncn-mw# curl -s -k -d grant_type=password -d client_id=shasta -d username=\u0026lt;username\u0026gt; -d password=\u0026lt;password\u0026gt; \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token Restore Postgres for VCS In the event that the VCS Postgres cluster must be rebuilt and the data restored, then the following procedures are recommended.\nRestore Postgres for VCS: Prerequisites A dump of the database exists. A backup of the VCS PVC exists. See Restore PVC data. The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Restore Postgres for VCS: Procedure Copy the database dump to an accessible location.\nIf a manual dump of the database was taken, then check that the dump file exists in a location off the Postgres cluster. It will be needed in the steps below. If the database is being automatically backed up, then the most recent version of the dump and the secrets should exist in the postgres-backup S3 bucket. These will be needed in the steps below. List the files in the postgres-backup S3 bucket and if the files exist, download the dump and secrets out of the S3 bucket. The cray artifacts CLI can be used list and download the files. Note that the .psql file contains the database dump and the .manifest file contains the secrets. List the available backups.\nncn-mw# cray artifacts list postgres-backup --format json | jq -r \u0026#39;.artifacts[].Key | select(contains(\u0026#34;vcs\u0026#34;))\u0026#39; Example output:\ngitea-vcs-postgres-2022-09-14T01:10:04.manifest gitea-vcs-postgres-2022-09-14T01:10:04.psql Set the environment variables to the name of the backup files.\nIn order to avoid a kubectl cp bug, the dump file will be downloaded with a slightly altered name (: characters replaced with - characters).\nncn-mw# MANIFEST=gitea-vcs-postgres-2022-09-14T01:10:04.manifest ncn-mw# DUMPFILE_SRC=gitea-vcs-postgres-2022-09-14T01:10:04.psql ncn-mw# DUMPFILE=${DUMPFILE_SRC//:/-} Download the backup files.\nncn-mw# cray artifacts get postgres-backup \u0026#34;${DUMPFILE_SRC}\u0026#34; \u0026#34;${DUMPFILE}\u0026#34; ncn-mw# cray artifacts get postgres-backup \u0026#34;${MANIFEST}\u0026#34; \u0026#34;${MANIFEST}\u0026#34; Set helper variables.\nncn-mw# SERVICE=gitea-vcs ncn-mw# SERVICELABEL=vcs ncn-mw# NAMESPACE=services ncn-mw# POSTGRESQL=gitea-vcs-postgres Scale the VCS service to 0.\nncn-mw# kubectl scale deployment ${SERVICE} -n \u0026#34;${NAMESPACE}\u0026#34; --replicas=0 Wait for the pods to terminate.\nncn-mw# while kubectl get pods -n \u0026#34;${NAMESPACE}\u0026#34; -l app.kubernetes.io/name=\u0026#34;${SERVICELABEL}\u0026#34; | grep -qv NAME ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2 done Delete the VCS Postgres cluster.\nncn-mw# kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; \u0026gt; postgres-cr.json ncn-mw# kubectl delete -f postgres-cr.json Wait for the pods to terminate.\nncn-mw# while kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; | grep -qv NAME ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2 done Create a new single instance VCS Postgres cluster.\nncn-mw# cp postgres-cr.json postgres-orig-cr.json ncn-mw# jq \u0026#39;.spec.numberOfInstances = 1\u0026#39; postgres-orig-cr.json \u0026gt; postgres-cr.json ncn-mw# kubectl create -f postgres-cr.json Wait for the pod to start.\nncn-mw# while ! kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; | grep -qv NAME ; do echo \u0026#34; waiting for pod to start\u0026#34;; sleep 2 done Wait for the Postgres cluster to start running.\nncn-mw# while [ $(kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ] do echo \u0026#34; waiting for postgresql to start running\u0026#34;; sleep 2 done Copy the database dump file to the Postgres member.\nncn-mw# kubectl cp \u0026#34;./${DUMPFILE}\u0026#34; \u0026#34;${POSTGRESQL}-0:/home/postgres/${DUMPFILE}\u0026#34; -c postgres -n services Restore the data.\nncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -c postgres -n services -it -- psql -U postgres \u0026lt; \u0026#34;${DUMPFILE}\u0026#34; Errors such as ... already exists can be ignored; the restore can be considered successful when it completes.\nEither update or re-create the gitea-vcs-postgres secrets.\nUpdate the secrets in Postgres.\nIf a manual dump was done, and the secrets were not saved, then the secrets in the newly created Postgres cluster will need to be updated.\nFrom the three gitea-vcs-postgres secrets, collect the password for each Postgres username: postgres, service_account, and standby.\nncn-mw# for secret in postgres.gitea-vcs-postgres.credentials service-account.gitea-vcs-postgres.credentials \\ standby.gitea-vcs-postgres.credentials do echo -n \u0026#34;secret ${secret} username \u0026amp; password: \u0026#34; echo -n \u0026#34;`kubectl get secret \u0026#34;${secret}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -ojsonpath=\u0026#39;{.data.username}\u0026#39; | base64 -d` \u0026#34; echo `kubectl get secret \u0026#34;${secret}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -ojsonpath=\u0026#39;{.data.password}\u0026#39;| base64 -d` done Example output:\nsecret postgres.gitea-vcs-postgres.credentials username \u0026amp; password: postgres ABCXYZ secret service-account.gitea-vcs-postgres.credentials username \u0026amp; password: service_account ABC123 secret standby.gitea-vcs-postgres.credentials username \u0026amp; password: standby 123456 kubectl exec into the Postgres pod.\nncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -c postgres -it -- bash Open a Postgres console.\npod# /usr/bin/psql postgres postgres Update the password for each user to match the values found in the secrets.\nUpdate the password for the postgres user.\npostgres# ALTER USER postgres WITH PASSWORD \u0026#39;ABCXYZ\u0026#39;; Example of successful output:\nALTER ROLE Update the password for the service_account user.\npostgres# ALTER USER service_account WITH PASSWORD \u0026#39;ABC123\u0026#39;; Example of successful output:\nALTER ROLE Update the password for the standby user.\npostgres# ALTER USER standby WITH PASSWORD \u0026#39;123456\u0026#39;; Example of successful output:\nALTER ROLE Exit the Postgres console with the \\q command.\nExit the Postgres pod with the exit command.\nRe-create secrets in Kubernetes.\nIf the Postgres secrets were auto-backed up, then re-create the secrets in Kubernetes.\nDelete and re-create the three gitea-vcs-postgres secrets using the manifest that was copied from S3 earlier.\nncn-mw# kubectl delete secret postgres.gitea-vcs-postgres.credentials \\ service-account.gitea-vcs-postgres.credentials standby.gitea-vcs-postgres.credentials -n services ncn-mw# kubectl apply -f \u0026#34;${MANIFEST}\u0026#34; Restart the Postgres cluster.\nncn-mw# kubectl delete pod -n \u0026#34;${NAMESPACE}\u0026#34; \u0026#34;${POSTGRESQL}-0\u0026#34; Wait for the postgresql pod to start.\nncn-mw# while ! kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; | grep -qv NAME ; do echo \u0026#34; waiting for pods to start\u0026#34;; sleep 2 done Scale the Postgres cluster back to 3 instances.\nncn-mw# kubectl patch postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --type=\u0026#39;json\u0026#39; \\ -p=\u0026#39;[{\u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/numberOfInstances\u0026#34;, \u0026#34;value\u0026#34; : 3}]\u0026#39; Wait for the postgresql cluster to start running.\nncn-mw# while [ $(kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ] do echo \u0026#34; waiting for postgresql to start running\u0026#34;; sleep 2 done Scale the Gitea service back up.\nncn-mw# kubectl scale deployment ${SERVICE} -n \u0026#34;${NAMESPACE}\u0026#34; --replicas=1 Wait for the Gitea pods to start.\nncn-mw# while ! kubectl get pods -n \u0026#34;${NAMESPACE}\u0026#34; -l app.kubernetes.io/name=\u0026#34;${SERVICELABEL}\u0026#34; | grep -qv NAME ; do echo \u0026#34; waiting for pods to start\u0026#34;; sleep 2 done "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/troubleshoot_ansible_play_failures_in_cfs_sessions/",
	"title": "Troubleshoot Ansible Play Failures in CFS Sessions",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Ansible Play Failures in CFS Sessions View the Kubernetes logs for a Configuration Framework Service (CFS) pod in an error state to determine whether the error resulted from the CFS infrastructure or from an Ansible play that was run by a specific configuration layer in a CFS session.\nUse this procedure to obtain important triage information for Ansible plays being called by CFS.\nPrerequisites A failed configuration session exists in CFS. Procedure Find the CFS pod that is in an error state.\nList all CFS pods in error state.\nncn-mw# kubectl get pods -n services | grep -E \u0026#34;^cfs-.*[[:space:]]Error[[:space:]]\u0026#34; Example output:\ncfs-e8e48c2a-448f-4e6b-86fa-dae534b1702e-pnxmn 0/3 Error 0 25h Set CFS_POD_NAME to the name of the pod to be investigated.\nUse the pod name identified in the previous substep.\nncn-mw# CFS_POD_NAME=cfs-e8e48c2a-448f-4e6b-86fa-dae534b1702e-pnxmn Check to see what containers are in the pod.\nncn-mw# kubectl logs -n services \u0026#34;${CFS_POD_NAME}\u0026#34; Example output:\nError from server (BadRequest): a container name must be specified for pod cfs-e8e48c2a-448f-4e6b-86fa-dae534b1702e-pnxmn, choose one of: [inventory ansible-0 istio-proxy] or one of the init containers: [git-clone-0 istio-init] Issues rarely occur in the istio-init and istio-proxy containers. These containers can be ignored for now.\nCheck the git-clone-0, inventory, and ansible-0 containers, in that order.\nIf there are additional Ansible pods, examine those as well, in ascending order.\nCheck the git-clone-0 container.\nncn-mw# kubectl logs -n services \u0026#34;${CFS_POD_NAME}\u0026#34; git-clone-0 Check the inventory container.\nncn-mw# kubectl logs -n services \u0026#34;${CFS_POD_NAME}\u0026#34; inventory Example output:\n% Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (7) Failed to connect to localhost port 15000: Connection refused Waiting for Sidecar % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 HTTP/1.1 200 OK content-type: text/html; charset=UTF-8 cache-control: no-cache, max-age=0 x-content-type-options: nosniff date: Thu, 05 Dec 2019 15:00:11 GMT server: envoy transfer-encoding: chunked Sidecar available 2019-12-05 15:00:12,160 - INFO - cray.cfs.inventory - Starting CFS Inventory version=0.4.3, namespace=services 2019-12-05 15:00:12,171 - INFO - cray.cfs.inventory - Inventory target=dynamic for cfsession=boa-2878e4c0-39c2-4df0-989e-053bb1edee0c 2019-12-05 15:00:12,227 - INFO - cray.cfs.inventory.dynamic - Dynamic inventory found a total of 2 groups 2019-12-05 15:00:12,227 - INFO - cray.cfs.inventory - Writing out the inventory to /inventory/hosts Check the ansible-0 container.\nLook towards the end of the Ansible log in the PLAY RECAP section to see if any targets failed. If a target failed, then look above in the log at the immediately preceding play. In the example below, the ncmp_hsn_cns role has an issue when being run against the compute nodes.\nncn-mw# kubectl logs -n services \u0026#34;${CFS_POD_NAME}\u0026#34; ansible-0 Example output:\nWaiting for Inventory Waiting for Inventory Inventory available % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 [...] TASK [ncmp_hsn_cns : SLES Compute Nodes (HSN): Create/Update ifcfg-hsnx File(s)] *** fatal: [x3000c0s19b1n0]: FAILED! =\u0026gt; {\u0026#34;msg\u0026#34;: \u0026#34;\u0026#39;interfaces\u0026#39; is undefined\u0026#34;} fatal: [x3000c0s19b2n0]: FAILED! =\u0026gt; {\u0026#34;msg\u0026#34;: \u0026#34;\u0026#39;interfaces\u0026#39; is undefined\u0026#34;} fatal: [x3000c0s19b3n0]: FAILED! =\u0026gt; {\u0026#34;msg\u0026#34;: \u0026#34;\u0026#39;interfaces\u0026#39; is undefined\u0026#34;} fatal: [x3000c0s19b4n0]: FAILED! =\u0026gt; {\u0026#34;msg\u0026#34;: \u0026#34;\u0026#39;interfaces\u0026#39; is undefined\u0026#34;} NO MORE HOSTS LEFT ************************************************************* PLAY RECAP ********************************************************************* x3000c0s19b1n0 : ok=28 changed=20 unreachable=0 failed=1 skipped=77 rescued=0 ignored=1 x3000c0s19b2n0 : ok=27 changed=19 unreachable=0 failed=1 skipped=63 rescued=0 ignored=1 x3000c0s19b3n0 : ok=27 changed=19 unreachable=0 failed=1 skipped=63 rescued=0 ignored=1 x3000c0s19b4n0 : ok=27 changed=19 unreachable=0 failed=1 skipped=63 rescued=0 ignored=1 Run the Ansible play again once the underlying issue has been resolved.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/tools_for_resolving_boot_issues/",
	"title": "Tools for Resolving Compute Node Boot Issues",
	"tags": [],
	"description": "",
	"content": "Tools for Resolving Compute Node Boot Issues A number of tools can be used to analyze and debug issues encountered during the compute node boot process. The underlying issue and symptoms dictate the type of tool required.\nnmap Use nmap to send out DHCP discover requests to test DHCP. nmap can be installed using the following command:\nncn# zypper install nmap To reach the DHCP server, the request generally needs to be sent over the Node Management network (NMN) from a non-compute node (NCN).\nIn the following example, nmap is used to send a broadcast request over the eth1 interface:\nncn# nmap --script broadcast-dhcp-discover -e eth1 Wireshark Use Wireshark to display network traffic. It has powerful display filters that help find information that can be used for debugging. To learn more, visit the Wireshark home page.\ntcpdump Use tcpdump to capture network traffic, such as DHCP or TFTP requests. It can be installed using the following mechanisms:\nInstall tcpdump inside an Alpine-based pod:\nalpine_pod# apk add --no-cache tcpdump Install tcpdump on an NCN or some other node that is running SUSE:\nsuse# zypper install tcpdump Invoking tcpdump without any arguments will write all of its output to stdout. This is reasonable for some tasks, but the volume of traffic that tcpdump can capture is large, so it is often better to write the output to a file.\nUse the following command to send tcpdump output to stdout:\nlinux# tcpdump Use the following command to send tcpdump output to a file (/tmp/tcpdump.output in the following example):\nlinux# tcpdump -w /tmp/tcpdump.output Use either tcpdump or Wireshark to read from the tcpdump file. Here is how to read the file using tcpdump:\nlinux# tcpdump -r /tmp/tcpdump.output Filtering the traffic using tcpdump filters is not recommended because when a TFTP server answers a client, it will usually use an ephemeral port that the user may not be able to identify and will not be captured by tcpdump. It is better to capture everything with tcpdump and then filter with Wireshark when looking at the resulting output. Filtering on DHCP traffic can be performed because that uses ports 67 and 68 specifically.\nTFTP client Use a TFTP client to send TFTP requests to the TFTP server. This will test that the server is functional. TFTP requests can be sent from the NCN, remote node, or laptop, as long as it targets the NMN.\nInstall the TFTP client using the following command:\nncn# zypper install atftp The atftp TFTP client can be used to request files from the TFTP server. The TFTP server is on the NMN and listens on port 69. The TFTP server sends the ipxe.efi file as the response in this example.\nRequest the files:\nncn# atftp tftp\u0026gt; connect 10.100.160.2 69 tftp\u0026gt; get ipxe.efi test-ipxe.efi tftp\u0026gt; quit List the files:\nncn# ls -l test-ipxe.efi Example output:\n-rw-r--r-- 1 root root 951904 Sep 11 10:44 test-ipxe.efi Serial Over LAN (SOL) sessions There are two tools that can be used to access a BMC\u0026rsquo;s console via SOL:\nipmitool\nipmitool is a utility for controlling IPMI-enabled devices.\nUse the following command to access a node\u0026rsquo;s SOL via ipmitool:\nread -s is used to prevent the password from being written to the screen or the shell history.\nncn# USERNAME=root ncn# read -r -s -p \u0026#34;BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node_management_network_IP_address_of_node\u0026gt; sol activate Example:\nncn# ipmitool -I lanplus -U \u0026#34;${USERNAME}\u0026#34; -E -H 10.100.165.2 sol activate ConMan\nThe ConMan tool is used to collect logs from nodes. It is also used to attach to the node\u0026rsquo;s SOL console. For more information, refer to ConMan and Access Compute Node Logs.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/list_registered_uai_images/",
	"title": "List Registered UAI Images",
	"tags": [],
	"description": "",
	"content": "List Registered UAI Images Administrators can use the cray uas admin config images list command to see the list of registered images. This command also displays the UAS registration information about each image.\nWhile Registering a UAI image name with UAS is necessary for UAIs to use the image, simply registering the image is not sufficient. The registered image must also be created and stored appropriately in its container registry. The basic HPE supplied UAI image is both installed and registered at UAS installation or upgrade time by the update-uas Kubernetes job when the update-uas Helm chart is deployed, upgraded or downgraded. Custom images are created, installed and registered as part of the Customize End-User UAI Images procedure.\nThis procedure describes how to list the currently registered UAI images.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) Procedure Obtain the list of UAI images that are currently registered with UAS.\nncn-m001-pit# cray uas admin config images list Example output:\n[[results]] default = true image_id = \u0026#34;08a04462-195a-4e66-aa31-08076072c9b3\u0026#34; imagename = \u0026#34;registry.local/cray/cray-uas-sles15:latest\u0026#34; [[results]] default = false image_id = \u0026#34;f8d5f4da-c910-421c-92b6-794ab8cc7e70\u0026#34; imagename = \u0026#34;registry.local/cray/cray-uai-broker:latest\u0026#34; [[results]] default = false image_id = \u0026#34;8fdf5d4a-c190-24c1-2b96-74ab98c7ec07\u0026#34; imagename = \u0026#34;registry.local/cray/custom-end-user-uai:latest\u0026#34; The output shown above shows three image registrations. Each has an imagename indicating the name of the image from the image registry to be used to construct a UAI.\nNOTE: Simply registering a UAI image name does not make the image available. The image must also be created and stored in the container registry. See Customize End-User UAI Images.\nThere is also a default flag. If this flag is true, the image will be used, in the absence of a default UAI Class, whenever a UAI is created without specifying an image or UAI Class as part of the creation. Finally, there is an image_id, which identifies this image registration for later inspection, update, or deletion and for linking the image to a UAI Class.\nTop: User Access Service (UAS)\nNext Topic: Register a UAI Image\n"
},
{
	"uri": "/docs-csm/en-12/install/reinstall_livecd/",
	"title": "Reinstall LiveCD",
	"tags": [],
	"description": "",
	"content": "Reinstall LiveCD Setup a re-install of LiveCD on a node using the previous configuration.\nBackup to the data partition:\npit# mkdir -pv /var/www/ephemeral/backup pit# pushd /var/www/ephemeral/backup pit# tar -czvf \u0026#34;dnsmasq-data-$(date \u0026#39;+%Y-%m-%d_%H-%M-%S\u0026#39;).tar.gz\u0026#34; /etc/dnsmasq.* pit# tar -czvf \u0026#34;network-data-$(date \u0026#39;+%Y-%m-%d_%H-%M-%S\u0026#39;).tar.gz\u0026#34; /etc/sysconfig/network/* pit# cp -pv /etc/hosts ./ pit# popd pit# umount -v /var/www/ephemeral Unplug the USB device.\nThe USB device should now contain all the information already loaded, as well as the backups of the initialized files.\nPlug the device into a new machine, or make a backup on the booted NCN. Make a snapshot of the USB device.\nmylinuxpc\u0026gt; mount -v /dev/disk/by-label/PITDATA /mnt mylinuxpc\u0026gt; tar -czvf --exclude *.squashfs \\ \u0026#34;install-data-$(date \u0026#39;+%Y-%m-%d_%H-%M-%S\u0026#39;).tar.gz\u0026#34; /mnt/ mylinuxpc\u0026gt; umount -v /dev/disk/by-label/PITDATA Follow the directions in Bootstrap PIT Node from LiveCD USB, and then return here and move onto the next step.\nThe new tar.gz file can be stored anywhere, and can be used to reinitialize the LiveCD.\nDelete the existing content on the USB device and create a new LiveCD on that same USB device.\nOnce the install-data partition is created, it can be remounted and can be used to restore the backup.\nmylinuxpc\u0026gt; mount -v /dev/disk/by-label/PITDATA /mnt mylinuxpc\u0026gt; tar -xzvf $(ls -ltR *.tar.gz | head -n 1) mylinuxpc\u0026gt; ls -R /mnt The tarball should have extracted everything into the install-data partition.\nRetrieve the SquashFS artifacts.\nThe artifacts can be retrieved at the following locations:\n/mnt/var/www/ephemeral/k8s/ /mnt/var/www/ephemeral/ceph/ Attach the USB to a Cray non-compute node (NCN) and reboot into the USB device.\nOnce booted into the USB device, restore network configuration and dnsmasq, and ensure the pods are started.\nSTOP AND INSPECT ANY FAILURE IN ANY OF THESE COMMANDS\npit# tar -xzvf /var/www/ephemeral/backup/dnsmasq*.tar.gz pit# tar -xzvf /var/www/ephemeral/backup/network*.tar.gz pit# systemctl restart wicked wickedd-nanny pit# systemctl restart dnsmasq pit# systemctl start basecamp nexus The LiveCD is now re-installed with the previous configuration.\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/troubleshoot_s3fs_mounts/",
	"title": "Troubleshoot S3FS Mount Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot S3FS Mount Issues The following procedure includes steps to troubleshoot issues with S3FS mount points on worker and master NCNs. Beginning in the CSM 1.2 release, S3FS is deployed as tool to reduce space usage on NCNs. Below is a list of the mount points on masters and workers:\nMaster Node Mount Points Master nodes should host the following three mount points:\n/var/opt/cray/config-data (config-data S3 bucket) /var/lib/admin-tools (admin-tools S3 bucket) /var/opt/cray/sdu/collection-mount (sds S3 bucket) Worker Node Mount Points Worker nodes should host the following mount point:\n/var/lib/cps-local/boot-images (boot-images S3 bucket) Note: If this mount is missing, the cray-cps-cm-pm pods may be unhealthy (in the CrashLoopBackoff state). Proceed to Step 1 to resolve the issue.\nStep 1: Verify Mounts are Present Verify Mount Points on Master Nodes Run the following command on master nodes to ensure the mounts are present:\nncn-m: # mount | grep \u0026#39;s3fs on\u0026#39; s3fs on /var/opt/cray/config-data type fuse.s3fs (rw,nosuid,nodev,relatime,user_id=0,group_id=0) s3fs on /var/lib/admin-tools type fuse.s3fs (rw,relatime,user_id=0,group_id=0,allow_other) s3fs on /var/opt/cray/sdu/collection-mount type fuse.s3fs (rw,relatime,user_id=0,group_id=0,allow_other) If the output is missing one or more of the mounts, proceed to Step 2\nVerify Mount Point on Worker Nodes Run the following command on worker nodes to ensure the mount is present:\nncn-w: # mount | grep \u0026#39;s3fs on\u0026#39; s3fs on /var/lib/cps-local/boot-images type fuse.s3fs (rw,nosuid,nodev,relatime,user_id=0,group_id=0) If the output is missing the mount, proceed to Step 2\nStep 2: Verify /etc/fstab Contains the Mounts Master Nodes /etc/fstab Entries Ensure the /etc/fstab contains the following content:\nncn-m: # grep fuse.s3fs /etc/fstab sds /var/opt/cray/sdu/collection-mount fuse.s3fs _netdev,allow_other,passwd_file=/root/.sds.s3fs,url=http://rgw-vip.nmn,use_path_request_style,use_cache=/var/lib/s3fs_cache,check_cache_dir_exist,use_xattr,uid=2370,gid=2370,umask=0007,allow_other 0 0 admin-tools /var/lib/admin-tools fuse.s3fs _netdev,allow_other,passwd_file=/root/.admin-tools.s3fs,url=http://rgw-vip.nmn,use_path_request_style,use_cache=/var/lib/s3fs_cache,check_cache_dir_exist,use_xattr 0 0 config-data /var/opt/cray/config-data fuse.s3fs _netdev,allow_other,passwd_file=/root/.config-data.s3fs,url=http://rgw-vip.nmn,use_path_request_style,use_xattr 0 0 If the three entries above are not present in /etc/fstab, add the above content and proceed to Step 3.\nWorker Nodes /etc/fstab Entry Ensure the /etc/fstab contains the following content:\nncn-w: # grep fuse.s3fs /etc/fstab boot-images /var/lib/cps-local/boot-images fuse.s3fs _netdev,allow_other,passwd_file=/root/.ims.s3fs,url=http://rgw-vip.nmn,use_path_request_style,use_cache=/var/lib/s3fs_cache,check_cache_dir_exist,use_xattr 0 0 If the above line is not present in /etc/fstab, add the above content and proceed to Step 3.\nStep 3: Attempt to Remount the Mount Point This step is the same for master and worker nodes. Run the following command to mount the directories specified in the /etc/fstab file:\nncn-mw: # mount -a If the above command fails, then the error likely indicates that there is an issue communicating with Ceph\u0026rsquo;s Radosgw endpoint (rgw-vip). In this case the Troubleshoot an Unresponsive S3 Endpoint procedure should be followed to ensure the endpoint is healthy.\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/hashicorp_vault/",
	"title": "HashiCorp Vault",
	"tags": [],
	"description": "",
	"content": "HashiCorp Vault Overview Storage model Unseal keys Administrative access Kubernetes service account access Check the status of Vault clusters Overview A deployment of HashiCorp Vault, managed via the Bitnami Bank-vaults operator, stores private and public Certificate Authority (CA) material, and serves APIs through a PKI engine instance. This instance also serves as a general secrets engine for the system.\nKubernetes service account authorization is utilized to authenticate access to Vault. The configuration of Vault, as deployed on the system, can be viewed with the following command:\nncn-mw# kubectl get vault -n vault cray-vault -o yaml A Kubernetes operator manages the deployment of Vault, based on this definition. The operator is deployed to the vault namespace. The resulting instance is deployed to the vault namespace.\nIMPORTANT: Changing the cray-vault custom resource definition or modifying data directly in Vault is not supported unless directed by customer support.\nFor more information, refer to the following resources:\nBank-vaults external documentation Vault external documentation Storage model In previous releases, Vault used etcd as a high-availability (HA) storage back-end. Currently, Vault uses HashiCorp\u0026rsquo;s Raft implementation. Raft is now configured to run natively inside the Vault StatefulSet instead of as an independent deployment.\nUnseal keys Vault requires unseal keys for start-up. If the unseal keys are not present, or are incorrect, Vault (by design) will not start. Unseal keys are stored in the cray-vault-unseal-keys Kubernetes Secret on a system, which is inside the vault namespace.\nAdministrative access Administrative access to Vault can be accomplished through the use of the unseal secret. The use of administrative access should be limited to situations where it is truly necessary. Otherwise, Kubernetes service account access should be used.\nTo obtain and use the root token:\nncn-mw# VAULT_TOKEN=$(kubectl get secrets cray-vault-unseal-keys -n vault -o jsonpath={.data.vault-root} | base64 -d) ncn-mw# kubectl exec -it -n vault -c vault cray-vault-0 -- sh -c \u0026#34;VAULT_ADDR=http://localhost:8200 VAULT_TOKEN=$VAULT_TOKEN vault secrets list\u0026#34; Kubernetes service account access Vault is configured to allow service account access from the services namespace (among others). This access is tied to a role, which is also subject to specific access policies.\nTo obtain and use the service account token:\nncn-mw# SA_SECRET=$(kubectl -n services get serviceaccounts default -o jsonpath=\u0026#39;{.secrets[0].name}\u0026#39;) ncn-mw# SA_JWT=$(kubectl -n services get secret $SA_SECRET -o jsonpath=\u0026#39;{.data.token}\u0026#39; | base64 --decode) ncn-mw# VAULT_TOKEN=$(kubectl exec -it -n vault -c vault cray-vault-0 -- sh -c \\ \u0026#34;export VAULT_ADDR=http://localhost:8200; vault write auth/kubernetes/login role=services jwt=$SA_JWT -format=json\u0026#34; \\ | jq \u0026#34;.auth.client_token\u0026#34; | sed -e \u0026#39;s/\u0026#34;//g\u0026#39;) ncn-mw# kubectl exec -it -n vault -c vault cray-vault-0 -- sh -c \u0026#34;VAULT_ADDR=http://localhost:8200 VAULT_TOKEN=$VAULT_TOKEN vault kv list secret/\u0026#34; Service account tokens will eventually expire.\nCheck the status of Vault clusters Check the status of Vault clusters with the following command:\nncn-mw# for n in $(seq 0 2); do echo \u0026#34;======= Vault status from cray-vault-${n} ======\u0026#34;; kubectl exec -it -n vault -c vault cray-vault-${n} -- sh -c \u0026#34;VAULT_ADDR=http://localhost:8200 vault status\u0026#34;; done Example output:\n======= Vault status from cray-vault-0 ====== Key Value --- ----- Seal Type shamir Initialized true Sealed false Total Shares 5 Threshold 3 Version 1.5.5 Cluster Name vault-cluster-e19b13b8 Cluster ID 3ea3b6a2-f3f8-fda3-d997-454795dc2be5 HA Enabled true HA Cluster https://cray-vault-1:8201 HA Mode standby Active Node Address http://cray-vault.vault:8200 Raft Committed Index 521 Raft Applied Index 521 ======= Vault status from cray-vault-1 ====== Key Value --- ----- Seal Type shamir Initialized true Sealed false Total Shares 5 Threshold 3 Version 1.5.5 Cluster Name vault-cluster-e19b13b8 Cluster ID 3ea3b6a2-f3f8-fda3-d997-454795dc2be5 HA Enabled true HA Cluster https://cray-vault-1:8201 HA Mode active Raft Committed Index 521 Raft Applied Index 521 ======= Vault status from cray-vault-2 ====== Key Value --- ----- Seal Type shamir Initialized true Sealed false Total Shares 5 Threshold 3 Version 1.5.5 Cluster Name vault-cluster-e19b13b8 Cluster ID 3ea3b6a2-f3f8-fda3-d997-454795dc2be5 HA Enabled true HA Cluster https://cray-vault-1:8201 HA Mode standby Active Node Address http://cray-vault.vault:8200 Raft Committed Index 521 Raft Applied Index 521 Healthy clusters will have one Vault pod in active HA mode, and two Vault pods in standby HA mode. All instances should also be unsealed and initialized.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/enable_nodes/",
	"title": "Enable Nodes",
	"tags": [],
	"description": "",
	"content": "Enable Nodes Use the Hardware State Manager (HSM) Cray CLI commands to enable nodes on the system.\nEnabling nodes that are available provides an accurate system configuration and node map.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Procedure Enable one or more nodes with HSM.\nncn-m001# cray hsm state components bulkEnabled update --enabled true --component-ids XNAME_LIST Verify the desired nodes are enabled.\nncn-m001# cray hsm state components query create --component-ids XNAME_LIST --format toml Example output:\n[[Components]] Type = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;On\u0026#34; NID = 1003 Flag = \u0026#34;OK\u0026#34; Role = \u0026#34;Compute\u0026#34; NetType = \u0026#34;Sling\u0026#34; Arch = \u0026#34;X86\u0026#34; ID = \u0026#34;x5000c1s0b1n1\u0026#34; [[Components]] Type = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;On\u0026#34; NID = 1004 Flag = \u0026#34;OK\u0026#34; Role = \u0026#34;Compute\u0026#34; NetType = \u0026#34;Sling\u0026#34; Arch = \u0026#34;X86\u0026#34; ID = \u0026#34;x5000c1s0b1n2\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/medium/",
	"title": "Medium",
	"tags": [],
	"description": "",
	"content": "Medium Back to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/loopback/",
	"title": "Loopback Interface",
	"tags": [],
	"description": "",
	"content": "Loopback Interface Loopbacks are essentially internal virtual interfaces. Loopback interfaces are not bound to a physical port and are used for device management and routing protocols.\nConfiguration Commands switch(config)# interface loopback LOOPBACK switch(config-loopback-if)# ip address IP-ADDR/\u0026lt;SUBNET|PREFIX\u0026gt; Example Output switch(config)# interface loopback 1 switch(config-loopback-if)# ip address 99.99.99.1/32 switch(config-loopback-if)# end switch# show run interface loopback1 interface loopback1 no shutdown ip address 99.99.99.1/32 exit switch# show ip interface loopback1 Interface loopback1 is up Admin state is up Hardware: Loopback IPv4 address 99.99.99.1/32 Expected Results Administrators can create a loopback interface Administrators can give a loopback interface an IP address Administrators can validate the configuration using the show commands. Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/restore_an_etcd_cluster_from_a_backup/",
	"title": "Restore an etcd Cluster from a Backup",
	"tags": [],
	"description": "",
	"content": "Restore an etcd Cluster from a Backup Use an existing backup of a healthy etcd cluster to restore an unhealthy cluster to a healthy state.\nThe commands in this procedure can be run on any master node (ncn-mXXX) or worker node (ncn-wXXX) on the system.\nNOTE: Etcd Clusters can be restored using the automation script or the manual procedure below. The automation script follows the same steps as the manual procedure. If the automation script fails to get the date from backups, follow the manual procedure.\nPrerequisites A backup of a healthy etcd cluster has been created.\nRestore with Automation Script The automated script will restore the cluster from the most recent backup if it finds a backup created within the last 7 days. If it does not discover a backup within the last 7 days, it will ask the user if they would like to rebuild the cluster.\nncn-w001# cd /opt/cray/platform-utils/etcd_restore_rebuild_util # rebuild/restore a single cluster ncn-w001:/opt/cray/platform-utils/etcd_restore_rebuild_util # ./etcd_restore_rebuild.sh -s cray-bos-etcd # rebuild/restore multiple clusters ncn-w001:/opt/cray/platform-utils/etcd_restore_rebuild_util # ./etcd_restore_rebuild.sh -m cray-bos-etcd,cray-uas-mgr-etcd # rebuild/restore all clusters ncn-w001:/opt/cray/platform-utils/etcd_restore_rebuild_util # ./etcd_restore_rebuild.sh -a An example using the automation script is below.\nncn-m001:/opt/cray/platform-utils/etcd_restore_rebuild_util # ./etcd_restore_rebuild.sh -s cray-externaldns-etcd The following etcd clusters will be restored/rebuilt: cray-externaldns-etcd You will be accepting responsibility for any missing data if there is a restore/rebuild over a running etcd k/v. HPE assumes no responsibility. Proceed restoring/rebuilding? (yes/no) yes Proceeding: restoring/rebuilding etcd clusters. ----- Restoring from cray-externaldns/etcd.backup_v8362_2021-08-18-20:00:09 etcdrestore.etcd.database.coreos.com/cray-externaldns-etcd created - 3/3 Running Successfully restored cray-externaldns-etcd etcdrestore.etcd.database.coreos.com \u0026#34;cray-externaldns-etcd\u0026#34; deleted Restore with Manual Procedure List the backups for the desired etcd cluster.\nThe example below uses the Boot Orchestration Service (BOS).\nncn-w001# kubectl exec -it -n operators \\ $(kubectl get pod -n operators | grep etcd-backup-restore | head -1 | awk \u0026#39;{print $1}\u0026#39;) \\ -c boto3 -- list_backups cray-bos Example output:\ncray-bos/etcd.backup_v108497_2020-03-20-23:42:37 cray-bos/etcd.backup_v125815_2020-03-21-23:42:37 cray-bos/etcd.backup_v143095_2020-03-22-23:42:38 cray-bos/etcd.backup_v160489_2020-03-23-23:42:37 cray-bos/etcd.backup_v176621_2020-03-24-23:42:37 cray-bos/etcd.backup_v277935_2020-03-30-23:52:54 cray-bos/etcd.backup_v86767_2020-03-19-18:00:05 Restore the cluster using a backup.\nReplace etcd.backup\\_v277935\\_2020-03-30-23:52:54 in the command below with the name of the backup being used.\nncn-w001# kubectl exec -it -n operators \\ $(kubectl get pod -n operators | grep etcd-backup-restore | head -1 | awk \u0026#39;{print $1}\u0026#39;) \\ -c util -- restore_from_backup cray-bos etcd.backup_v277935_2020-03-30-23:52:54 Example output:\netcdrestore.etcd.database.coreos.com/cray-bos-etcd created Restart the pods for the etcd cluster.\nWatch the pods come back online.\nThis may take a couple minutes.\nncn-w001# kubectl -n services get pod | grep SERVICE_NAME Example output:\ncray-bos-etcd-498jn7th6p 1/1 Running 0 4h1m cray-bos-etcd-dj7d894227 1/1 Running 0 3h59m cray-bos-etcd-tk4pr4kgqk 1/1 Running 0 4 Delete the EtcdRestore custom resource.\nThis step will make it possible for future restores to occur. Replace the etcdrestore.etcd.database.coreos.com/cray-bos-etcd value with the name returned in step 2.\nncn-w001# kubectl -n services delete etcdrestore.etcd.database.coreos.com/cray-bos-etcd Example output:\netcdrestore.etcd.database.coreos.com \u0026#34;cray-bos-etcd\u0026#34; deleted Verify that the cray-bos-etcd-client service was created.\nncn# kubectl get service -n services cray-bos-etcd-client Example of output showing that the service was created:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cray-bos-etcd-client ClusterIP 10.28.248.232 \u0026lt;none\u0026gt; 2379/TCP 2m If the etcd-client service was not created, then repeat the procedure to restore the cluster again.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/troubleshoot_cfs_session_failing_to_complete/",
	"title": "Troubleshoot CFS Session Failing to Complete",
	"tags": [],
	"description": "",
	"content": "Troubleshoot CFS Session Failing to Complete Troubleshoot issues where Configuration Framework Service (CFS) sessions/pods fail and Ansible hangs. These issues can be resolved by modifying Ansible to produce less output.\nPrerequisites A CFS session or pod is failing to complete, and the Ansible logs are not showing progress or completion.\nThe following is an example of the error causing Ansible to hang:\nPLAY [Compute] ***************************************************************** META: ran handlers META: ran handlers META: ran handlers PLAY [Compute] ***************************************************************** Using module file /usr/lib/python3.6/site-packages/ansible/modules/system/setup.py Pipelining is enabled. \u0026lt;x5000c3s3b0n1\u0026gt; ESTABLISH SSH CONNECTION FOR USER: root \u0026lt;x5000c3s3b0n1\u0026gt; SSH: EXEC ssh -vvv -o ServerAliveInterval=30 -o ControlMaster=auto -o ControlPersist=60s -o StrictHostKeyChecking=no -o \u0026#39;IdentityFile=\u0026#34;/secret/key\u0026#34;\u0026#39; -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o \u0026#39;User=\u0026#34;root\u0026#34;\u0026#39; -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/f6f378183d x5000c3s3b0n1 \u0026#39;/bin/sh -c \u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;/usr/bin/python3 \u0026amp;\u0026amp; sleep 0\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#39; Reduce Ansible output The steps in this procedure are independent from each other and are used to troubleshoot different underlying problems that both present a similar symptom.\nAny of the following steps can be taken to help reduce the output generated by Ansible:\nReduce the verbosity when running Ansible commands.\nIf the session was created with a higher value of --ansible-verbosity (three or higher), Ansible can generate a lot of output that can cause the pod to hang. Reducing the verbosity by one or more may resolve this issue.\nUpdate the Ansible configuration to produce less output.\nSee Enable Ansible Profiling for an example of modifying the configuration.\nAdjust the use of flags used when running Ansible commands.\nThe display_ok_hosts and display_skipped_hosts are examples of settings that can be disabled to reduce output. See the Ansible documentation for more information on what flags can be used.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/troubleshoot_booting_nodes_with_hardware_issues/",
	"title": "Troubleshoot Booting Nodes with Hardware Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Booting Nodes with Hardware Issues NOTE This section is for Boot Orchestration Service (BOS) v1 only. Bad components will not impact the booting of other components in BOS v2.\nThis document explains how to identify a node with hardware issues.\nIf a node included in a BOS session template is having hardware issues, then it can prevent the node from powering back up correctly. The entire BOS session will fail with a timeout error waiting for the node to become ready.\nThe following example shows log output from a node with hardware issues, resulting in a failed BOS session:\nncn-mw# kubectl logs -n services BOS_POD_ID Example output excerpt:\n2020-10-03 17:47:30,053 - ERROR - cray.boa.smd.wait_for_nodes - Number of retries: 361 exceeded allowed amount: 360; 2 nodes were not in the state: Ready 2020-10-03 17:47:30,054 - DEBUG - cray.boa.smd.wait_for_nodes - These nodes were not in the state: Ready x1003c0s1b1n1 x1001c0s2b1n1 Disabling nodes that have underlying hardware issues preventing them from booting will help resolve this issue. This can be done using the Hardware State Manager (HSM). This method does not return the node with hardware issues to a healthy state, but it does enable a BOS session that was encountering issues to complete successfully. For more information, see Disable Nodes.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/list_uai_resource_specifications/",
	"title": "List UAI Resource Specifications",
	"tags": [],
	"description": "",
	"content": "List UAI Resource Specifications Obtain a list of all the UAI resource specifications registered with UAS.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) Procedure List all the resource specifications registered in UAS.\nThe resource specifications returned by the following command are available for UAIs to use:\nncn-m001-pit# cray uas admin config resources list Example output:\n[[results]] comment = \u0026#34;Resource Specification to use with Brokered End-User UAIs\u0026#34; limit = \u0026#34;{\\\u0026#34;cpu\\\u0026#34;: \\\u0026#34;300m\\\u0026#34;, \\\u0026#34;memory\\\u0026#34;: \\\u0026#34;1Gi\\\u0026#34;}\u0026#34; request = \u0026#34;{\\\u0026#34;cpu\\\u0026#34;: \\\u0026#34;300m\\\u0026#34;, \\\u0026#34;memory\\\u0026#34;: \\\u0026#34;1Gi\\\u0026#34;}\u0026#34; resource_id = \u0026#34;f26ee12c-6215-4ad1-a15e-efe4232f45e6\u0026#34; The following are the configurable parts of a resource specification:\nlimit - A JSON string describing a Kubernetes resource limit request - A JSON string describing a Kubernetes resource request comment - An optional free form string containing any information an administrator might find useful about the resource specification resource-id - Used for examining, updating or deleting the resource specification as well as linking the resource specification into a UAI class Refer to Elements of a UAI for more information.\nTop: User Access Service (UAS)\nNext Topic: Create a UAI Resource Specification\n"
},
{
	"uri": "/docs-csm/en-12/install/reset_root_password_on_livecd/",
	"title": "Reset root Password on LiveCD",
	"tags": [],
	"description": "",
	"content": "Reset root Password on LiveCD It may become desirable to clear the password on the LiveCD.\nThe root password is preserved within the COW partition at cow:rw/etc/shadow. This is the modified copy of the /etc/shadow file used by the operating system.\nIf a site/user needs to reset/clear the password for root, they can mount their USB on another machine and remove this file from the COW partition. When next booting from the USB it will reinitialize to an empty password for root, and again at next login it will require the password to be changed.\nClear the password (macOS or Linux):\nmypc:~ \u0026gt; mount -vL cow /mnt mypc:~ \u0026gt; sudo rm -fv /mnt/rw/etc/shadow mypc:~ \u0026gt; umount -v /mnt "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/troubleshoot_system_clock_skew/",
	"title": "Troubleshoot System Clock Skew",
	"tags": [],
	"description": "",
	"content": "Troubleshoot System Clock Skew Resynchronize system clocks after Ceph reports a clock skew.\nSystems use chronyd to synchronize their system clocks. If systems are not able to communicate, then the clocks can drift, causing clock skew. Clock skew can also be caused by an individual or an automated task manually changing the clocks. In this case, chronyd may require a series of steps (time adjustments) to resynchronize the clocks.\nMajor time jumps where the clock is set back in time will require a full restart of all Ceph services.\nClock skew can cause issues with Kubernetes operations, etcd, node responsiveness, and more.\nPrerequisites This procedure requires admin privileges.\nProcedure Verify that the system is impacted by clock skew.\nCeph provides block storage and requires a clock skew of less than 0.05 seconds to report back healthy.\nncn# ceph -s Example output:\ncluster: id: b6d509e6-772e-4785-a421-e4a138b1780c health: HEALTH_WARN clock skew detected on mon.ncn-m002, mon.ncn-m003 services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby osd: 18 osds: 18 up (since 20h), 18 in (since 9d) rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) data: pools: 10 pools, 224 pgs objects: 19.41k objects, 59 GiB usage: 167 GiB used, 274 GiB / 441 GiB avail pgs: 224 active+clean io: client: 919 KiB/s wr, 0 op/s rd, 16 op/s wr IMPORTANT: If you see this message in the Ceph logs unable to obtain rotating service keys; retrying, it also indicates clock skew. You may have to run xzgrep skew *.xz to see the skew if your logs have rolled over.\nView the Ceph health details.\nView the Ceph logs.\nIf looking back to earlier logs, use the xzgrep command for the ceph.log or the ceph-mon*.log. There are cases where the MGR and OSD logs are not in the ceph-mon logs. This indicates that the skew was very drastic and sudden, causing the ceph-mon process to panic and not log the issue.\nncn-s# grep skew /var/log/ceph/*.log View the system time.\nncn-s# ansible ceph_all -m shell -a date Sync the clocks to fix the issue.\nncn-s# systemctl restart chronyd.service Wait a bit after running the command and the Ceph alert will clear. Restart the Ceph mon service on that node if the alert does not clear.\nCheck Ceph health to verify the clock skew issue is resolved.\nIt may take up to 15 minutes for this warning to resolve.\nncn# ceph -s Example output:\ncluster: id: 5f3b4031-d6c0-4118-94c0-bffd90b534eb health: HEALTH_OK services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby osd: 18 osds: 18 up (since 20h), 18 in (since 9d) rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) data: pools: 11 pools, 240 pgs objects: 3.12k objects, 11 GiB usage: 45 GiB used, 39 GiB / 84 GiB avail pgs: 240 active+clean If clocks are in sync and Ceph is still reporting skew, refer to Manage Ceph Services on restarting services.\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/keycloak_operations/",
	"title": "Keycloak Operations",
	"tags": [],
	"description": "",
	"content": "Keycloak Operations A service may need to access Keycloak to perform various tasks. These typical uses for a service to access Keycloak include creating a new service account, creating a new user, etc. These operations require Keycloak administrative access. As part of the System Management Services (SMS) installation process, Keycloak is initialized with a Master realm. An administrative client and user are created within this realm. The system installation process adds the information needed for the Keycloak administrator\u0026rsquo;s authentication into a Kubernetes secret that can be accessed by any pod. Using this information and the Keycloak REST API, a service can create an account in the Shasta realm. The Keycloak master administrative authentication information is located in the keycloak-master-admin-auth secret, which includes the following fields:\nclient-id - Client ID for administrative operations\nuser - Username for the Keycloak Master admin.\npassword - Password for the Keycloak Master admin.\ninternal_token_url - URL that can be used to get a token, such as https://istio-ingressgateway.istio-system.svc.cluster.local/keycloak/realms/master/protocol/openid-connect/token.\nThe pod in the following example gets a Keycloak Master admin token and makes a request to create a client with a user ID attribute mapper.\nncn# kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: kc-admin-example namespace: services spec: containers: - name: kc-admin-example image: alpine command: - sh - -c - \u0026gt;- apk update \u0026amp;\u0026amp; apk add --no-cache curl jq \u0026amp;\u0026amp; echo endpoint: \\$(cat /mnt/auth/internal_token_url) \u0026amp;\u0026amp; echo client_id: \\$(cat /mnt/auth/client-id) \u0026amp;\u0026amp; echo user: \\$(cat /mnt/auth/user) \u0026amp;\u0026amp; TOKEN=\\$(curl -s --cacert /mnt/shasta-ca/certificate_authority.crt -d grant_type=password -d client_id=\\$(cat /mnt/auth/client-id) -d username=\\$(cat /mnt/auth/user) -d password=\\$(cat /mnt/auth/password) \\$(cat /mnt/auth/internal_token_url) | jq -r .access_token) \u0026amp;\u0026amp; echo \u0026#34;=== Making request with token \\$(echo \\$TOKEN | head -c10)... ===\u0026#34; \u0026amp;\u0026amp; curl -is --cacert /mnt/shasta-ca/certificate_authority.crt -H \u0026#34;Authorization: Bearer \\$TOKEN\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;clientId\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;publicClient\u0026#34;: true, \u0026#34;standardFlowEnabled\u0026#34;: false, \u0026#34;implicitFlowEnabled\u0026#34;: false, \u0026#34;directAccessGrantsEnabled\u0026#34;: true, \u0026#34;protocolMappers\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;uid-user-attribute-mapper\u0026#34;, \u0026#34;protocolMapper\u0026#34;: \u0026#34;oidc-usermodel-attribute-mapper\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;openid-connect\u0026#34;, \u0026#34;config\u0026#34;: {\u0026#34;user.attribute\u0026#34;: \u0026#34;uid\u0026#34;, \u0026#34;claim.name\u0026#34;: \u0026#34;uid\u0026#34;, \u0026#34;access.token.claim\u0026#34;: false, \u0026#34;userinfo.token.claim\u0026#34;: true}}]}\u0026#39; https://istio-ingressgateway.istio-system.svc.cluster.local/keycloak/admin/realms/shasta/clients volumeMounts: - name: ca-vol mountPath: /mnt/shasta-ca - name: auth-vol mountPath: \u0026#39;/mnt/auth\u0026#39; readOnly: true volumes: - name: ca-vol configMap: name: cray-configmap-ca-public-key - name: auth-vol secret: secretName: keycloak-master-admin-auth restartPolicy: Never EOF ncn# kubectl logs -n services kc-admin-example Example output:\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.8/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz v3.8.1-115-ge3ed6b4e31 [http://dl-cdn.alpinelinux.org/alpine/v3.8/main] v3.8.1-112-g45bdd0edfb [http://dl-cdn.alpinelinux.org/alpine/v3.8/community] OK: 9546 distinct packages available fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz (1/7) Installing ca-certificates (20171114-r3) (2/7) Installing nghttp2-libs (1.32.0-r0) (3/7) Installing libssh2 (1.8.0-r3) (4/7) Installing libcurl (7.61.1-r1) (5/7) Installing curl (7.61.1-r1) (6/7) Installing oniguruma (6.8.2-r0) (7/7) Installing jq (1.6_rc1-r1) Executing busybox-1.28.4-r1.trigger Executing ca-certificates-20171114-r3.trigger OK: 7 MiB in 20 packages endpoint: https://istio-ingressgateway.istio-system.svc.cluster.local/keycloak/realms/master/protocol/openid-connect/token client_id: admin-cli user: admin === Making request with token eyJhbGciOi... === HTTP/1.1 201 Created Content-Length: 0 Connection: keep-alive Location: https://istio-ingressgateway.istio-system.svc.cluster.local/keycloak/admin/realms/shasta/clients/070c8537-6c46-43a4-b0bb-209b3c4b94c6 Date: Fri, 30 Nov 2018 20:07:39 GMT X-Kong-Upstream-Latency: 27 X-Kong-Proxy-Latency: 1 Via: kong/0.14.1 The new example client is now visible in the Keycloak administrative web application.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/enable_passwordless_connections_to_liquid_cooled_node_bmcs/",
	"title": "Enable Passwordless Connections to Liquid Cooled Node BMCs",
	"tags": [],
	"description": "",
	"content": "Enable Passwordless Connections to Liquid Cooled Node BMCs Set the passwordless SSH keys for the root account and/or console of all liquid-cooled Baseboard Management Controllers (BMCs) on the system. This procedure will not work on BMCs for air-cooled hardware.\nWarning: If administrator uses SCSD to update the SSHConsoleKey value outside of ConMan, it will disrupt the ConMan connection to the console and collection of console logs. Refer to ConMan for more information.\nSetting up SSH keys enables administrators to view recent console messages and interact with the console device for nodes.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. This procedure requires administrative privileges. Procedure Save the public SSH key for the root user.\nncn-w001# export SSH_PUBLIC_KEY=$(cat /root/.ssh/id_rsa.pub | sed \u0026#39;s/[[:space:]]*$//\u0026#39;) Enable passwordless SSH to the root user of the BMCs.\nSkip this step if passwordless SSH to the root user is not desired.\nncn-w001# export SCSD_SSH_KEY=$SSH_PUBLIC_KEY Enable passwordless SSH to the consoles on the BMCs.\nSkip this step if passwordless SSH to the consoles is not desired.\nncn-w001# export SCSD_SSH_CONSOLE_KEY=$SSH_PUBLIC_KEY Generate a System Configuration Service configuration using the scsd tool.\nThe administrator must be authenticated to the Cray CLI before proceeding.\nncn-w001# cat \u0026gt; scsd_cfg.json \u0026lt;\u0026lt;DATA { \u0026#34;Force\u0026#34;:false, \u0026#34;Targets\u0026#34;: $(cray hsm inventory redfishEndpoints list --format=json | jq \u0026#39;[.RedfishEndpoints[] | .ID]\u0026#39; | sed \u0026#39;s/^/ /\u0026#39;), \u0026#34;Params\u0026#34;:{ \u0026#34;SSHKey\u0026#34;:\u0026#34;$(echo $SCSD_SSH_KEY)\u0026#34;, \u0026#34;SSHConsoleKey\u0026#34;:\u0026#34;$(echo $SCSD_SSH_CONSOLE_KEY)\u0026#34; } } DATA Inspect the generated scsd_cfg.json file.\nEnsure that the following are true before running the command below:\nThe component name (xname) list looks valid/appropriate. The SSHKey and SSHConsoleKey settings match the desired public key. Load the configuration from the file to the System Configuration Service.\nncn-w001# cray scsd bmc loadcfg create scsd_cfg.json Check the output to verify all hardware has been set with the correct keys. Passwordless SSH to the root user and/or the consoles should now function as expected.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/mlag/",
	"title": "Multi-chassis interface",
	"tags": [],
	"description": "",
	"content": "Multi-chassis interface Multi-Chassis Link Aggregation Group (MCLAG) is a link aggregation technique where two or more links across two switches are aggregated together to form a trunk.\nCreating an MLAG interface:\nCreate an MLAG interface for the host. Run:\nswitch (config)# interface mlag-port-channel 1 switch (config interface mlag-port-channel 1)# The MPO interfaces should be configured in the same sequence on both switches of MLAG cluster.\nExample:\nOn Switch 1:\ninterface mlag-port-channel 1-10 interface mlag-port-channel 30-40 On Switch 2:\ninterface mlag-port-channel 1-10 interface mlag-port-channel 30-40 Bind an Ethernet port to the MLAG interface:\nswitch (config interface ethernet 1/1)# mlag-channel-group 1 mode on Create and enable the MLAG interface:\nswitch (config interface mlag-port-channel 1)# no shutdown Enabling MLAG:\nEnable MLAG:\nswitch (config mlag)# no shutdown When running MLAG as L2/L3 border point, MAGP VIP must be deployed as the default GW for MPOs.\nVerifying MLAG Configuration\nExamine MLAG configuration and status. Run show mlag on the switch:\nSwitch 1 [my-vip: master] (config)# show mlag Admin status: Enabled Operational status: Up Reload-delay: 1 sec Keepalive-interval: 30 sec Upgrade-timeout: 60 min System-mac: 00:00:5e:00:01:5d MLAG Ports Configuration Summary: Configured: 1 Disabled: 0 Enabled: 1 MLAG Ports Status Summary: Inactive: 0 Active-partial: 0 Active-full: 1 MLAG IPLs Summary: ID Group Vlan Operational Local Peer Up Time Toggle Counter Port-Channel Interface State IP address IP address ---------------------------------------------------------------------------------------------- 1 Po1 1 Up 1.1.1.1 1.1.1.2 0 days 00:00:09 5 Peers state Summary: System-id State Hostname ----------------------------------- F4:52:14:2D:9B:88 Up \u0026lt;Switch 1\u0026gt; F4:52:14:2D:9B:08 Up Switch 2 Examine the MLAG summary table:\nSwitch 1 [my-vip: master] (config) # show interfaces mlag-port-channel summary MLAG Port-Channel Flags: D-Down, U-Up, P-Partial UP, S-suspended by MLAG Port Flags: D: Down P: Up in port-channel (members) S: Suspend in port-channel (members) I: Individual MLAG Port-Channel Summary: ------------------------------------------------------------------------------ Group Type Local Peer Port-Channel Ports Ports (D/U/P/S) (D/P/S/I) (D/P/S/I) ------------------------------------------------------------------------------ 1 Mpo2(U) Static Eth1/2(P) Eth1/2(P) Examine the MLAG statistics. Run:\nSwitch 1 [my-vip: master] (config)# show mlag statistics IPL 1 Rx Heartbeat : 516 Tx Heartbeat : 516 Rx IGMP tunnel : 0 Tx IGMP tunnel : 0 RX XSTP tunnel : 0 TX XSTP tunnel : 0 RX mlag-notification : 0 TX mlag-notification : 0 Rx port-notification : 0 Tx port-notification : 0 Rx FDB sync : 0 Tx FDB sync : 0 RX LACP manager : 1 TX LACP manager : 0 (Optional) In case MLAG-VIP was configured, its functionality can be examined using \u0026ldquo;show mlag-vip\u0026rdquo; command.\nSwitch 1 [my-vip: master] (config)# show mlag-vip MLAG VIP ======== MLAG group name: my-mlag-group MLAG VIP address: 10.234.23.254 /24 Active nodes: 2 Hostname VIP-State IP Address ---------------------------------------------------- Switch 1 master 10.234.23.1 Switch 2 standby 10.234.23.2 No output will appear, if MLAG-VIP is not configured.\nExpected Results\nStep 1: You can configure MCLAG Step 2: You can create an MCLAG interface Step 3: You can add ports to the MCLAG interface Step 4: The output of the show commands is correct Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/mac_auth/",
	"title": "MAC Authentication",
	"tags": [],
	"description": "",
	"content": "MAC Authentication MAC Authentication (MAC Auth) is a method of authenticating devices for access to the network. The default mode of authentication is RADIUS, through which clients are authenticated by an external RADIUS server.\nConfiguration Commands Enter MAC Auth context:\nswitch(config)# aaa authentication port-access mac-auth Enable MAC Auth on all interfaces:\nswitch(config-macauth)# enable Configure MAC Auth MAC address format:\nswitch(config-macauth)# addr-format \u0026lt;no-delimiter|single-dash|multi-dash|multi-colon|no-delimiter Enable MAC Auth password:\nswitch(config-macauth)# password \u0026lt;plaintext|ciphertext\u0026gt; PASSWORD Configure mac-auth RADIUS authentication method:\nswitch(config-macauth)# aaa authentication port-access mac-auth auth-method \u0026lt;chap|pap\u0026gt; Configure mac-auth server group:\nswitch(config-macauth)# radius server-group NAME Configure cached reauthentication period on a port:\nswitch(config-macauth)# cached-reauth-period VALUE Configure the quiet period on a port:\nswitch(config-macauth)# quiet-period VALUE Configure the reauthentication period on a port:\nswitch(config-macauth)# reauth-period VALUE Enable reauthentication on the interface:\nswitch(config-macauth)# reauth Enable authorized on the interface:\nswitch(config-macauth)# authorized Enable cached reauthentication on the interface:\nswitch(config-macauth)# cached-reauth Show commands to validate functionality:\nswitch# show aaa authentication port-access mac-auth interface \u0026lt;IFACE|all\u0026gt; \u0026lt;port-statistics|client-status [mac MAC-ADDR]\u0026gt; Expected Results Administrators can enable MAC auth authentication Administrators are able to authenticate using the specified dot1x authentication method The output of the show commands looks correct Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/retrieve_cluster_health_information_using_kubernetes/",
	"title": "Retrieve Cluster Health Information Using Kubernetes",
	"tags": [],
	"description": "",
	"content": "Retrieve Cluster Health Information Using Kubernetes The kubectl CLI commands can be used to retrieve information about the Kubernetes cluster components.\nNodes Retrieve node status ncn# kubectl get nodes Example output:\nNAME STATUS ROLES AGE VERSION ncn-m001 Ready control-plane,master 27h v1.20.13 ncn-m002 Ready control-plane,master 8d v1.20.13 ncn-m003 Ready control-plane,master 8d v1.20.13 ncn-w001 Ready \u0026lt;none\u0026gt; 8d v1.20.13 ncn-w002 Ready \u0026lt;none\u0026gt; 8d v1.20.13 ncn-w003 Ready \u0026lt;none\u0026gt; 8d v1.20.13 Pods Retrieve information about individual pods ncn# kubectl describe pod POD_NAME -n NAMESPACE_NAME Retrieve a list of all pods ncn# kubectl get pods -A Retrieve a list of healthy pods ncn# kubectl get pods -A | grep -E \u0026#39;Completed|Running\u0026#39; Retrieve a list of unhealthy pods Option 1: List all pods that are not reported as Completed or Running.\nncn# kubectl get pods -A | grep -Ev \u0026#39;Completed|Running\u0026#39; Option 2: List all pods that are reported as Creating, ImagePull, Error, Init, or Crash.\nncn# kubectl get pods -A | grep -E \u0026#39;Creating|ImagePull|Error|Init|Crash\u0026#39; Retrieve status of pods in a specific namespace ncn# kubectl get pods -n NAMESPACE_NAME Example output for vault namespace:\nNAME READY STATUS RESTARTS AGE cray-vault-0 5/5 Running 2 7d cray-vault-1 5/5 Running 2 7d cray-vault-2 5/5 Running 2 7d cray-vault-configurer-7c7dcdb958-p8jfv 2/2 Running 0 7d cray-vault-operator-b48b7874f-flstw 2/2 Running 1 7d spire-intermediate-1-ltzwk 0/2 Completed 0 7d Retrieve pod logs ncn# kubectl logs -n NAMESPACE_NAME POD_NAME Services Retrieve a list of all services ncn# kubectl get services -A Retrieve status of services in a specific namespace ncn# kubectl get services -n NAMESPACE_NAME Example output for operators namespace:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cray-hms-trs-operator-metrics ClusterIP 10.16.222.4 \u0026lt;none\u0026gt; 8383/TCP,8686/TCP 7d cray-kiali-kiali-operator-metrics ClusterIP 10.20.177.208 \u0026lt;none\u0026gt; 8383/TCP,8686/TCP 7d etcd-restore-operator ClusterIP 10.28.72.18 \u0026lt;none\u0026gt; 19999/TCP 7d "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/troubleshoot_cfs_sessions_failing_to_start/",
	"title": "Troubleshoot CFS Sessions Failing to Start",
	"tags": [],
	"description": "",
	"content": "Troubleshoot CFS Sessions Failing to Start Troubleshoot issues where Configuration Framework Service (CFS) sessions are being created, but the pods are never created and never run.\nPrerequisites CFS-batcher is creating automatic sessions, but pods are not starting for those sessions. There are a number of communication reasons this could be happening, so check the cfs-batcher and cfs-operator logs for these signatures.\nCFS-batcher logs should show that it is creating sessions, but giving up waiting for those sessions to start:\n2022-08-24 04:09:12,391 - WARNING - batcher.batch - Session batcher-d7f9bf52-e5f6-4742-849b-4086b1d59ab1 is stuck in pending and will be deleted. 2022-08-24 04:09:12,492 - WARNING - batcher.batch - Session batcher-77c3c4d1-df39-4ebc-a6b4-32b8e102bdc5 is stuck in pending and will be deleted. 2022-08-24 04:09:12,638 - WARNING - batcher.batch - Session batcher-f9a513c1-47ad-4539-98cc-b3f418f6b8bd is stuck in pending and will be deleted. 2022-08-24 04:09:12,761 - WARNING - batcher.batch - Session batcher-7a291f2f-0149-46d7-889b-08331a46af2c is stuck in pending and will be deleted. 2022-08-24 04:09:15,144 - WARNING - batcher.batch - Session batcher-b6c35dcc-b8b9-421b-9090-23bfc2a72ac3 is stuck in pending and will be deleted. CFS-operator logs should show that it is attempting to create sessions, but is unable to find the session records.\n2022-08-24 05:48:06,476 - INFO - cray.cfs.operator.events.session_events - EVENT: CREATE batcher-3b78941e-1c06-474c-89fe-bf241f5002e4 2022-08-24 05:48:06,509 - ERROR - cray.cfs.operator.cfs.sessions - Unexpected response from CFS: 404 Client Error: Not Found for url: http://cray-cfs-api/v2/sessions/batcher-3b78941e-1c06-474c-89fe-bf241f5002e4 If these two things are true, then it is likely that CFS is creating new sessions faster than it can keep up with session creation events.\nProcedure The primary method of handling this problem is the batcherMaxBackoff option. This will slow automatic session creation in these situations and give the cfs-operator a chance to catch up.\nIf this value has been changed from its default value of 3600 (1 hour), then it should be set back to that value:\nncn-mw# cray cfs options update --batcher-max-backoff 3600 The issue should eventually resolve automatically.\nIf there is a reason that users cannot wait for the back-off to resolve this automatically, then the following procedure can be used to purge the event queue. This will disrupt CFS operation and may disrupt existing sessions, so caution should be used.\nSlow down session creation.\nIf any others users or scripts are creating sessions, make sure that they have stopped. CFS-batcher should be slowed by setting its check interval to an arbitrary high number.\nncn-mw# cray cfs options update --batcher-check-interval 99999 Start a new consumer on the Kafka event queue.\nOpen a shell in a Kafka pod.\nncn-mw# kubectl -n services exec -it cray-shared-kafka-kafka-0 -c kafka -- /bin/bash Start a console consumer on the CFS event topic using the cfs-operator consumer group.\npod# bin/kafka-console-consumer.sh --bootstrap-server cray-shared-kafka-kafka-0.cray-shared-kafka-kafka-brokers.services.svc.cluster.local:9092 \\ --topic cfs-session-events --group cfs-operator This command will likely produce not output at first, while Kafka re-balances the consumer group. Leave this command running.\nIn a new window, scale down the cfs-operator.\nThis forces the console consumer to handle the entire event queue.\nncn-mw# kubectl -n services scale --replicas=0 deployment/cray-cfs-operator Wait until the output from the console consumer stops.\nOnce the cfs-operator is scaled down, there should be a final burst of output from the console consumer. Wait until all output has stopped for at least a minute before continuing.\nCancel the console consumer command and exit the pod shell.\nRestore cfs-operator.\nncn-mw# kubectl -n services scale --replicas=1 deployment/cray-cfs-operator Restore cfs-batcher.\nRestore the batcher check interval.\nncn-mw# cray cfs options update --batcher-check-interval 10 Initiate a rolling restart of the batcher.\nncn-mw# kubectl -n services rollout restart deployment/cray-cfs-batcher "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_dynamic_host_configuration_protocol_dhcp/",
	"title": "Troubleshoot Compute Node Boot Issues Related to Dynamic Host Configuration Protocol (DHCP)",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Compute Node Boot Issues Related to Dynamic Host Configuration Protocol (DHCP) DHCP issues can result in node boot failures. This procedure helps investigate and resolve such issues.\nPrerequisites This procedure requires administrative privileges. Procedure Check that the DHCP service is running.\nncn-m001# kubectl get pods -A | grep kea Example output:\nservices cray-dhcp-kea-554698bb69-r9wwt 3/3 Running 0 13h services cray-dhcp-kea-postgres-0 2/2 Running 0 10d services cray-dhcp-kea-postgres-1 2/2 Running 0 3d18h services cray-dhcp-kea-postgres-2 2/2 Running 0 10d services cray-dhcp-kea-wait-for-postgres-3-7gqvg 0/3 Completed 0 10d Start a tcpdump session on the NCN.\nThe following example sends tcpdump data to stdout.\ncray-dhcp# tcpdump Obtain the DHCP pod\u0026rsquo;s ID.\nncn-m001# PODID=$(kubectl get pods --no-headers -o wide | grep cray-dhcp | awk \u0026#39;{print $1}\u0026#39;) Enter the DHCP pod using its ID.\nncn-m001# kubectl exec -it $PODID /bin/sh Start a tcpdump session from within the DHCP pod.\nOpen another terminal to perform the following tasks:\nIssue a DHCP discover request from the NCN using nmap.\nAnalyze the NCN tcpdump data in order to ensure that the DHCP discover request is visible.\nGo back to the original terminal to analyze the DHCP pod\u0026rsquo;s tcpdump data in order to ensure that the DHCP discover request is visible inside the pod.\nTroubleshooting If the DHCP discover request is not visible on the NCN, it may be due to a firewall issue. If the DHCP discover request is not visible inside the pod, double check if the request was issued over the correct interface for the Node Management Network (NMN). If it was, it could indicate a firewall issue.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/list_uais/",
	"title": "List UAIs",
	"tags": [],
	"description": "",
	"content": "List UAIs There are two ways to list UAIs in UAS. One of these is an administrative action and provides access to all currently running UAIs. The other is associated with the Legacy UAI Management mode and provides authorized users access to their own UAIs. Both of these are shown here.\nView the details of every UAI that is running by using a direct UAS administrative command.\nPrerequisites For administrative procedures:\nThe administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) For Legacy Mode user procedures:\nThe user must be logged into a host that has user access to the HPE Cray EX System API Gateway The user must have an installed initialized cray CLI and network access to the API Gateway The user must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The user must be logged in as to the HPE Cray EX System CLI (cray auth login command) Procedure List the existing UAIs as an administrator.\nUse a command of the following form:\nncn-m001-pit# cray uas admin uais list OPTIONS OPTIONS includes includes the following selection options:\n--owner '\u0026lt;user-name\u0026gt;' show only UAIs owned by the named user --class-id '\u0026lt;class-id' show only UAIs of the specified UAI class The following lists Broker UAIs on a system where administrators follow a convention that a Broker UAI is created with an owner called broker:\nncn-m001-pit# cray uas admin uais list --owner broker Example output:\n[[results]] uai_age = \u0026#34;5h3m\u0026#34; uai_connect_string = \u0026#34;ssh broker@34.136.140.107\u0026#34; uai_host = \u0026#34;ncn-w003\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-broker:1.2.4\u0026#34; uai_ip = \u0026#34;34.136.140.107\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-broker-07624d65\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;broker\u0026#34; List the UAIs owned by an authorized user named vers in the Legacy Mode of UAI management.\nvers\u0026gt; cray uas list Example output:\n[[results]] uai_age = \u0026#34;3m\u0026#34; uai_connect_string = \u0026#34;ssh vers@35.188.16.85\u0026#34; uai_host = \u0026#34;ncn-w003\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34; uai_ip = \u0026#34;35.188.16.85\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-4a38a807\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; Top: User Access Service (UAS)\nNext Topic: Creating a UAI\n"
},
{
	"uri": "/docs-csm/en-12/install/restart_network_services_and_interfaces_on_ncns/",
	"title": "Restart Network Services and Interfaces on NCNs",
	"tags": [],
	"description": "",
	"content": "Restart Network Services and Interfaces on NCNs Interfaces within the network stack can be reloaded or reset to fix wedged interfaces. The NCNs have network device names set during first boot. The names vary based on the available hardware. For more information, see NCN Networking. Any process covered on this page will be covered by the installer.\nThe use cases for resetting services:\nInterfaces not showing up IP Addresses not applying Member/children interfaces not being included Topics Restart Network Services and Interfaces))) Command Reference Check interface status (up/down/broken) Show routing and status for all devices Print real devices ( ignore no-device ) Show the currently enabled network service (Wicked or Network Manager) Restart Network Services There are a few daemons that make up the SUSE network stack. The following are sorted by safest to touch relative to keeping an SSH connection up.\nwickedd.service: The daemons handling each interface. Resetting this clears stale configuration. This command restarts the wickedd service without reconfiguring the network interfaces.\nncn# systemctl restart wickedd wicked.service: The overarching service for spawning daemons and manipulating interface configuration. Resetting this reloads daemons and configuration. This command restarts the wicked service which will respawns daemons and reconfigure the network.\nncn# systemctl restart wicked network.service: Responsible for network configuration per interface; This does not reload wicked. This command restarts the network interface configuration, but leaves wicked daemons alone.\nNOTE: Commonly the problem exists within wicked. This is a last resort in the event the configuration is so bad wicked cannot handle it.\n# Restart the network interface configuration, but leaves wicked daemons alone. ncn# systemctl restart network Command Reference Check interface status (up/down/broken):\nncn# wicked ifstatus Show routing and status for all devices:\nncn# wicked ifstatus --verbose all lo up link: #1, state up type: loopback control: persistent config: compat:suse:/etc/sysconfig/network/ifcfg-lo, uuid: 6ad37e59-72d7-5988-9675-93b8df96d9f6 leases: ipv4 static granted leases: ipv6 static granted addr: ipv4 127.0.0.1/8 scope host label lo [static] addr: ipv6 ::1/128 scope host [static] route: ipv6 ::1/128 type unicast table main scope universe protocol kernel priority 256 em1 device-unconfigured link: #2, state down, mtu 1500 type: ethernet, hwaddr a4:bf:01:48:1f:dc config: none em2 device-unconfigured link: #3, state down, mtu 1500 type: ethernet, hwaddr a4:bf:01:48:1f:dd config: none mgmt0 enslaved link: #4, state up, mtu 9000, master bond0 type: ethernet, hwaddr b8:59:9f:f9:1c:8e control: none config: compat:suse:/etc/sysconfig/network/ifcfg-mgmt0, uuid: 7175c041-ee2b-5ce2-a4d7-67fa6cb94a17 mgmt1 device-unconfigured link: #5, state up, mtu 9000, master bond0 type: ethernet, hwaddr b8:59:9f:f9:1c:8e config: none bond0 device-unconfigured link: #6, state up, mtu 9000 type: bond, mode ieee802-3ad, hwaddr b8:59:9f:f9:1c:8e config: none addr: ipv6 fe80::ba59:9fff:fef9:1c8e/64 scope link route: ipv6 fe80::/64 type unicast table main scope universe protocol kernel priority 256 bond0.nmn0 device-unconfigured link: #7, state up, mtu 9000 type: vlan bond0[2], hwaddr b8:59:9f:f9:1c:8e config: none addr: ipv4 10.252.2.2/17 brd 10.252.2.2 scope universe label bond0.nmn0 addr: ipv6 fe80::ba59:9fff:fef9:1c8e/64 scope link route: ipv4 0.0.0.0/0 via 10.252.1.1 dev bond0.nmn0 type unicast table main scope universe protocol boot route: ipv4 10.252.0.0/17 type unicast table main scope link protocol kernel pref-src 10.252.2.2 route: ipv6 fe80::/64 type unicast table main scope universe protocol kernel priority 256 bond0.hmn0 device-unconfigured link: #8, state up, mtu 9000 type: vlan bond0[4], hwaddr b8:59:9f:f9:1c:8e config: none addr: ipv4 10.254.2.2/17 brd 10.254.2.2 scope universe label bond0.hmn0 addr: ipv6 fe80::ba59:9fff:fef9:1c8e/64 scope link route: ipv4 10.254.0.0/17 type unicast table main scope link protocol kernel pref-src 10.254.2.2 route: ipv6 fe80::/64 type unicast table main scope universe protocol kernel priority 256 bond0.can0 device-unconfigured link: #9, state up, mtu 9000 type: vlan bond0[7], hwaddr b8:59:9f:f9:1c:8e config: none addr: ipv4 10.102.9.12/24 brd 10.102.9.12 scope universe label bond0.can0 addr: ipv6 fe80::ba59:9fff:fef9:1c8e/64 scope link route: ipv4 10.102.9.0/24 type unicast table main scope link protocol kernel pref-src 10.102.9.12 route: ipv6 fe80::/64 type unicast table main scope universe protocol kernel priority 256 eth0 no-device Print real devices (ignore no-device):\nncn# wicked show --verbose all Show the currently enabled network service (Wicked or Network Manager):\nncn# systemctl show -p Id network.service Id=wicked.service "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/troubleshoot_a_down_osd/",
	"title": "Troubleshoot a Down OSD",
	"tags": [],
	"description": "",
	"content": "Troubleshoot a Down OSD Identify down OSDs and manually bring them back up.\nTroubleshoot the Ceph health detail reporting down OSDs. Ensuring that OSDs are operational and data is balanced across them will help remove the likelihood of hotspots being created.\nPrerequisites This procedure requires admin privileges.\nProcedure Identify the down OSDs.\nncn-m/s(001/2/3)# ceph osd tree down Example output:\nID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 62.87558 root default -7 20.95853 host ncn-s002 1 ssd 3.49309 osd.1 down 1.00000 1.00000 3 ssd 3.49309 osd.3 down 1.00000 1.00000 7 ssd 3.49309 osd.7 down 1.00000 1.00000 10 ssd 3.49309 osd.10 down 1.00000 1.00000 13 ssd 3.49309 osd.13 down 1.00000 1.00000 16 ssd 3.49309 osd.16 down 1.00000 1.00000 Restart the down OSDs.\nOption 1:\nRestart the OSD utilizing ceph orch\nncn-m/s00(1/2/3)# ceph orch daemon restart osd.\u0026lt;number\u0026gt; Option 2:\nCheck the logs for the OSD that is down.\nUse the OSD number for the down OSD returned in the command above.\nncn-m/s(001/2/3)# ceph osd find OSD_ID Manually restart the OSD.\nThis step must be done on the node with the reported down OSD.\nncn-s# ceph orch daemon restart osd.\u0026lt;number\u0026gt; Troubleshooting: If the service is not restarted with ceph orch, restart it using Manage Ceph Services.\nVerify the OSDs are running again.\n# ceph osd tree down Example output:\nID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 62.87558 root default -7 20.95853 host ncn-s002 1 ssd 3.49309 osd.1 up 1.00000 1.00000 3 ssd 3.49309 osd.3 up 1.00000 1.00000 7 ssd 3.49309 osd.7 up 1.00000 1.00000 10 ssd 3.49309 osd.10 up 1.00000 1.00000 13 ssd 3.49309 osd.13 up 1.00000 1.00000 16 ssd 3.49309 osd.16 up 1.00000 1.00000 If the OSD dies again, check dmesg for drive failures.\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/keycloak_user_localization/",
	"title": "Keycloak User Localization",
	"tags": [],
	"description": "",
	"content": "Keycloak User Localization Verification procedure Verify that the Keycloak users localize job has completed as expected.\nThis section can be skipped if user localization is not required.\nAfter an upgrade, it is possible that all expected Keycloak users were not localized. The procedure below helps determine whether or not this has happened, and provides remediation steps if they are needed.\nCheck to see if the Keycloak users localize job has completed.\nncn-m002# kubectl -n services wait --for=condition=complete --timeout=10s job/`kubectl -n services get jobs | grep users-localize | awk \u0026#39;{print $1}\u0026#39;` The job completed if the output contains the string condition met.\nIf the job completed, check that the count of localized users matches the expected count from the Keycloak server.\nThis can be done by looking at the count of users reported from the command below.\nncn-m002# cray artifacts get wlm etc/passwd /dev/stdout | wc -l If that count looks correct, then no further action is needed and the remainder of this section should be skipped. Otherwise, rerun the localize job by following the remaining steps in the section.\nRecreate the job.\nncn-m002# kubectl get job -n services -l app.kubernetes.io/name=cray-keycloak-users-localize -ojson | jq \u0026#39;.items[0]\u0026#39; \u0026gt; keycloak-users-localize-job.json ncn-m002# kubectl delete job -n services -l app.kubernetes.io/name=cray-keycloak-users-localize ncn-m002# cat keycloak-users-localize-job.json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels)\u0026#39; | kubectl apply -f - Expected output looks similar to:\njob.batch \u0026#34;keycloak-users-localize-1\u0026#34; deleted job.batch/keycloak-users-localize-1 created Repeat the first two steps of this procedure to confirm that the job completed and that the Keycloak user count is correct.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/find_node_type_and_manufacturer/",
	"title": "Find Node Type and Manufacturer",
	"tags": [],
	"description": "",
	"content": "Find Node Type and Manufacturer There are three different vendors providing nodes for air-cooled cabinets, which are Gigabyte, Intel, and HPE. The Hardware State Manager (HSM) contains the information required to determine which type of air-cooled node is installed. The endpoint returned in the HSM command can be used to determine the manufacturer.\nHPE nodes contain the /redfish/v1/Systems/1 endpoint:\nncn-m001# cray hsm inventory componentEndpoints describe XNAME --format json | jq \u0026#39;.RedfishURL\u0026#39; \u0026#34;x3000c0s18b0/redfish/v1/Systems/1\u0026#34; Gigabyte nodes contain the /redfish/v1/Systems/Self endpoint:\nncn-m001# cray hsm inventory componentEndpoints describe XNAME --format json | jq \u0026#39;.RedfishURL\u0026#39; \u0026#34;x3000c0s7b0/redfish/v1/Systems/Self\u0026#34; Intel nodes contain the /redfish/v1/Systems/SERIAL_NUMBER endpoint:\nncn-m001# cray hsm inventory componentEndpoints describe XNAME --format json | jq \u0026#39;.RedfishURL\u0026#39; \u0026#34;x3000c0s15b0/redfish/v1/Systems/BQWT92000021\u0026#34; "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/mlag_architecture/",
	"title": "MLAG (Multi-Chassis LAG)",
	"tags": [],
	"description": "",
	"content": "MLAG (Multi-Chassis LAG) Is a type of Link Aggregation Group where ports from single device such as server terminate on two separate switches providing switch-level redundancy.\nWhat are the benefits of MLAG\nIncreased bandwidth achieved by dual connection to node.\nHigh availability (HA) for servers while allowing full use of the bandwidth of both links\nTo achieve HA on a switch level without the using of STP\nKey limitations of MLAG in mellanox:\nOnly one MLAG domain supported per device\nMaximum number of devices in MLAG domain is two switches.\nAt least one port per switch (in MLAG domain) MUST be reserved for inter-switch link.\nMore details, requirements and limitations on Mellanox devices can be found from:\nhttps://docs.mellanox.com/display/ONYXv381174/MLAG\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/management_interface/",
	"title": "Management Interface",
	"tags": [],
	"description": "",
	"content": "Management Interface The management interface can be used to gain remote management access to the switch. The management interface is accessible using the \u0026ldquo;mgmt\u0026rdquo; VRF and is separate from the data plane interfaces, which are in the \u0026ldquo;default\u0026rdquo; VRF.\nAlternatively, a loopback interface can be configured to be used as management interface.\nAddress Mode Admin State Mac Address IPv4 address/subnet-mask Default gateway IPv4 IPv6 address/prefix IPv6 link local address/prefix: fe10::96f1:28ff:fe1d:a901/64 Default gateway IPv6 Primary Nameserver Secondary Nameserver : : 10.110.135.51 : 10.110.135.52 : dhcp : up : 94:f1:28:1d:a9:01 : 10.93.61.227/21 : 10.93.56.1 Configuration Commands Enable/disable the management interface:\nswitch(config)# interface mgmt switch(config-if-mgmt)# no shutdown switch(config)# interface mgmt switch(config-if-mgmt)# shutdown Assign an IP address to the interface:\nswitch(config-if-mgmt)# ip \u0026lt;dhcp|static IP-ADDR\u0026gt; Show commands to validate functionality:\nswitch# show interface mgmt switch# show interface loopback 0 Create and configure loopback interface:\nswitch(config)# interface loopback 0 8325-Core1(config-loopback-if)# ip address \u0026lt;IP-ADDR\u0026gt; Expected Results Administrators can enable/disable the management interface. Administrators can assign an IP address to the management interface Administrators can configure a loopback interface to be use for Switch management. Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/tds_lower_cpu_requests/",
	"title": "TDS Lower CPU Requests",
	"tags": [],
	"description": "",
	"content": "TDS Lower CPU Requests Systems with only three worker nodes (typically Test and Development Systems (TDS)) will encounter pod scheduling issues when worker nodes are taken out of the Kubernetes cluster to be upgraded.\nFor systems with only three worker nodes, execute the following script to reduce the CPU request for some services with high CPU requests, in order to allow critical upgrade-related services to be successfully scheduled on only two worker nodes:\nncn-mw# /usr/share/doc/csm/upgrade/1.2/scripts/k8s/tds_lower_cpu_requests.sh Note that some services with these lower CPU request may encounter CPU throttling (see Determine if Pods are Hitting Resource Limits). If needed, the top portion of the script (see below) can be edited and re-run to further adjust these CPU requests. Note that commenting out any of the lines below will indicate the script should not adjust the CPU resource for that service. Also see the Kubernetes/Compute Resources/Namespace (Pods) Grafana page for a historical view of a given pod\u0026rsquo;s CPU utilization for this specific system. See Access System Management Health Services.\nspire_postgres_new_limit=1000m elasticsearch_master_new_cpu_request=1500m cluster_kafka_new_cpu_request=1 sma_grafana_new_cpu_request=100m sma_kibana_new_cpu_request=100m cluster_zookeeper_new_cpu_request=100m cray_smd_new_cpu_request=1 cray_smd_postgres_new_cpu_request=500m sma_postgres_cluster_new_cpu_request=500m cray_capmc_new_cpu_request=500m nexus_new_cpu_request=2 cray_metallb_speaker_new_cpu_request=1 The script will output the current value of the request along with the new value. It is recommended to capture and save the output from this script, and store it external to the cluster. This record of the value changes is helpful in the event that rolling back to the original value is desired.\nExample output:\nPatching cray-capmc deployment with new cpu request of 500m (from 2100m) deployment.apps/cray-capmc patched Waiting for deployment spec update to be observed... Waiting for deployment \u0026#34;cray-capmc\u0026#34; rollout to finish: 0 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-capmc\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-capmc\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-capmc\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-capmc\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-capmc\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-capmc\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-capmc\u0026#34; rollout to finish: 1 old replicas are pending termination... Waiting for deployment \u0026#34;cray-capmc\u0026#34; rollout to finish: 1 old replicas are pending termination... deployment \u0026#34;cray-capmc\u0026#34; successfully rolled out "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/update_a_cfs_configuration/",
	"title": "Update a CFS Configuration",
	"tags": [],
	"description": "",
	"content": "Update a CFS Configuration Modify a Configuration Framework Service (CFS) configuration by specifying the JSON of the configuration and its layers. Use the cray cfs configurations update command, similar to creating a configuration.\nPrerequisites A CFS configuration has been created. The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. Procedure Add and/or remove the configuration layers from an existing JSON configuration file.\nDo not include the name of the configuration in the JSON file. This is specified on the command line in the next step.\nncn-mw# cat configurations-example.json Example configuration:\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34; } ] } Update the configuration in CFS.\nncn-mw# cray cfs configurations update configurations-example --file ./configurations-example.json --format json Example output:\n{ \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:30:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; } "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_slow_boot_times/",
	"title": "Troubleshoot Compute Node Boot Issues Related to Slow Boot Times",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Compute Node Boot Issues Related to Slow Boot Times NOTE This section is for Boot Orchestration Service (BOS) v1 only. BOS v2 does not use Cray Advanced Platform Monitoring and Control (CAPMC), nor does it have the Boot Orchestration Agent (BOA). However, the steps outlined below allow for similar debugging of slow steps within Configuration Framework Service (CFS)-initiated sessions.\nInspect BOS, BOA, and CFS job logs in order to obtain information that is critical for boot troubleshooting. Use this procedure to determine why compute nodes are booting slower than expected.\nPrerequisites A boot session has been created with BOS. The Cray CLI is configured. See Configure the Cray CLI. Procedure View the BOA logs.\nFind the BOA job from the boot session.\nThe output of the command below is organized by the creation time of the BOA job with the most recent one listed last.\nncn-mw# kubectl -nservices --sort-by=.metadata.creationTimestamp get pods | grep boa Example output:\nboa-e3c845be-3092-4807-a0c9-272bf0e15896-7pnl4 0/2 Completed 0 3d boa-c740f74d-f5af-41f3-a71b-1a3fc00cbe7a-k5hdw 0/2 Completed 0 2d12h boa-a365b6a2-3614-4b53-9b6b-df0f4485e25d-nbcdb 0/2 Completed 0 2m43s Watch the log from BOA job.\nncn-mw# kubectl logs -n services -f -c boa BOA_JOB_ID Example output:\n2019-11-12 02:14:27,771 - DEBUG - cray.boa - BOA starting 2019-11-12 02:14:28,786 - DEBUG - cray.boa - Boot Agent Image: acad2b43-dff5-483d-a392-8b1b1f91a60c Nodes: x5000c1s1b1n0, x3000c0s35b2n0, x5000c1s3b1n1, x3000c0s35b3n0, x5000c1s0b1n0, x5000c1s3b0n1, x5000c1s2b0n0, x5000c1s3b1n0, x5000c1s1b1n1, x5000c1s2b0n1, x3000c0s35b1n0, x5000c1s3b0n0, x5000c1s0b1n1, x5000c1s1b0n0, x5000c1s2b1n0, x5000c1s1b0n1, x5000c1s2b1n1 created. 2019-11-12 02:14:29,118 - INFO - cray.boa - Boot Session: 88df3fc3-6697-41cc-9f63-7076d78a9110 2019-11-12 02:14:29,505 - DEBUG - cray.boa.logutil - cray.boa.agent.reboot called with args: (Boot Agent Image: acad2b43-dff5-483d-a392-8b1b1f91a60c Nodes: x5000c1s1b1n0, x3000c0s35b2n0, x5000c1s3b1n1, x3000c0s35b3n0, x5000c1s0b1n0, x5000c1s3b0n1, x5000c1s2b0n0, x5000c1s3b1n0, x5000c1s1b1n1, x5000c1s2b0n1, x3000c0s35b1n0, x5000c1s3b0n0, x5000c1s0b1n1, x5000c1s1b0n0, x5000c1s2b1n0, x5000c1s1b0n1, x5000c1s2b1n1,) 2019-11-12 02:14:29,505 - INFO - cray.boa.agent - Rebooting the Session: 88df3fc3-6697-41cc-9f63-7076d78a9110 Set: computes 2019-11-12 02:15:15,898 - DEBUG - cray.boa.logutil - cray.boa.dbclient.db_write_session called with args: (\u0026lt;etcd3.client.Etcd3Client object at 0x7f822db68dd8\u0026gt;, \u0026#39;88df3fc3-6697-41cc-9f63-7076d78a9110\u0026#39;, \u0026#39;computes\u0026#39;, \u0026#39;status\u0026#39;, \u0026#39;boot_capmc_finished\u0026#39;) 2019-11-12 02:15:15,898 - DEBUG - cray.boa.dbclient - Key: /session/88df3fc3-6697-41cc-9f63-7076d78a9110/computes/status/ Value: boot_capmc_finished 2019-11-12 02:15:15,938 - INFO - cray.boa.smd.wait_for_nodes - Standby: 17 entries 2019-11-12 02:15:15,938 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready [...] View the CFS logs related to the boot job.\nFind the most recent CFS jobs.\nThere may be more than one job if multiple components are being configured. If there are multiple different CFS jobs running, check CFS first to find the timestamp value when CFS was updated. Expect a delay of a couple minutes after the CFS session starts depending on the cfs-batcher settings.\nncn-mw# kubectl -n services get cfs Example output:\nNAME JOB STATUS SUCCEEDED REPOSITORY CLONE URL BRANCH COMMIT PLAYBOOK AGE 066bc062-7fc3-11ea-970e-a4bf0138f2ba cfs-1628cf85-e847-49af-891c-1b7655d8056d complete true https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master site.yml 4d10h 3c3758a8-7fd9-11ea-a365-a4bf0138f2ba cfs-05420ebf-fbbc-4d3a-a0af-a840e379fe12 complete true https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master site.yml 4d8h batcher-65f94609-0599-4d86-a8ad-9555d2a9ab9d cfs-b10975a0-fdde-4d00-98f8-0a2895b32d57 complete true https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git 0a38dc0d61f94eb43bf32c8bad801c4d41bf52d9 site.yml 3d17h batcher-6d823363-c7cd-4616-afb8-416156a83522 cfs-78726a81-eb72-4e2f-80e8-a8d08a7c031c complete true https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git 0a38dc0d61f94eb43bf32c8bad801c4d41bf52d9 site.yml 29h batcher-ef877321-922c-4517-bc05-e4f2a5141b2b cfs-a6c6f707-c7ca-48d7-aff7-71b26398c216 complete true https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git a371a11a5cf139ba67cee5823c8ce0e5b61d7a3f site.yml 4d5h ed684272-7fe8-11ea-a0fd-a4bf0138f2ba cfs-6ed992a3-7f63-4187-a2bf-fc4451ead997 complete true https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master site.yml 4d6h ncn-customization-ncn-w001-uai-hosts-load cfs-acb9f57f-e390-49c6-8028-842550f3d73e complete false https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master cray-ncn-customization-load.yml 4d6h ncn-customization-ncn-w002-uai-hosts-load cfs-37aebeb8-1c3f-45a4-a9b1-d25ad4e10a91 complete false https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master cray-ncn-customization-load.yml 4d6h ncn-customization-ncn-w002-uai-hosts-unload cfs-095cce88-1925-4625-a611-ae19d9976a60 complete false https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master cray-ncn-customization-unload.yml 4d6h ncn-customization-ncn-w003-uai-hosts-load cfs-6b3fdebd-ab2b-4751-b29f-436ff2893569 complete false https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master cray-ncn-customization-load.yml 4d6h ncn-customization-ncn-w003-uai-hosts-unload cfs-d94ebbe6-6b61-4f78-9dc4-fd24576d32dd complete false https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master cray-ncn-customization-unload.yml 4d6h If multiple CFS jobs exist, describe the CFS sessions and look at the configuration, as well as which components are included. It is unlikely, but a single session may contain components from multiple separate BOS sessions if they both request the same configuration for different components at around the same time.\nncn-mw# cray cfs sessions describe SESSION_NAME Find the pods for the CFS job.\nncn-mw# kubectl -n services get pods | grep JOB_NAME Example output:\ncfs-1628cf85-e847-49af-891c-1b7655d8056d-29ntt 0/4 Completed 0 4d11h View the log from the CFS pod.\nncn-mw# kubectl -n services logs POD_NAME ansible Example output:\nInventory available % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 HTTP/1.1 200 OK content-type: text/html; charset=UTF-8 cache-control: no-cache, max-age=0 x-content-type-options: nosniff date: Thu, 16 Apr 2020 09:20:15 GMT server: envoy transfer-encoding: chunked [...] PLAY [Compute] ***************************************************************** TASK [rsyslog : Add rsyslog.d config] ****************************************** changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [localtime : Create /etc/localtime symlink] ******************************* changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [ntp : Install stock /etc/chrony.conf] ************************************ changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [cle-hosts-cf : create temporary workarea] ******************************** changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [cle-hosts-cf : copy /etc/hosts from NCN host OS into compute image] ****** changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [cle-hosts-cf : copy /etc/hosts into place] ******************************* changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [cle-hosts-cf : remove temporary workarea] ******************************** changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] [...] Use the data returned in the BOA and CFS logs to determine the underlying issue for slow boot times.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/list_uas_information/",
	"title": "List UAS Version Information",
	"tags": [],
	"description": "",
	"content": "List UAS Version Information Use the cray uas mgr-info list command to determine the version and service name of UAS.\nList UAS Version with cray uas mgr-info list ncn-w001# cray uas mgr-info list Example output:\nservice_name = \u0026#34;cray-uas-mgr\u0026#34; version = \u0026#34;1.16.1\u0026#34; Top: User Access Service (UAS)\nNext-Topic: End-User UAIs\n"
},
{
	"uri": "/docs-csm/en-12/install/safeguards_for_csm_ncn_upgrades/",
	"title": "Safeguards for CSM",
	"tags": [],
	"description": "",
	"content": "Safeguards for CSM This page covers safe-guards for preventing destructive behaviors on management nodes.\nIf reinstalling or upgrading, run through these safe-guards on a by-case basis:\nWhether or not CEPH should be preserved. Whether or not the RAIDs should be protected. Safeguard CEPH OSDs Edit /var/www/ephemeral/configs/data.json and align the following options:\n{ .. // Disables Ceph wipe: \u0026#34;wipe-ceph-osds\u0026#34;: \u0026#34;no\u0026#34; .. } { .. // Restores default behavior: \u0026#34;wipe-ceph-osds\u0026#34;: \u0026#34;yes\u0026#34; .. } pit# vi /var/www/ephemeral/configs/data.json Quickly toggle yes or no to the file:\n# set wipe-ceph-osds=no pit# sed -i \u0026#39;s/wipe-ceph-osds\u0026#34;: \u0026#34;yes\u0026#34;/wipe-ceph-osds\u0026#34;: \u0026#34;no\u0026#34;/g\u0026#39; /var/www/ephemeral/configs/data.json # set wipe-ceph-osds=yes pit# sed -i \u0026#39;s/wipe-ceph-osds\u0026#34;: \u0026#34;no\u0026#34;/wipe-ceph-osds\u0026#34;: \u0026#34;yes\u0026#34;/g\u0026#39; /var/www/ephemeral/configs/data.json Activate the new setting:\npit# systemctl restart basecamp Safeguard RAIDS / BOOTLOADERS / SquashFS / OverlayFS Edit /var/www/boot/script.ipxe and align the following options as follows: rd.live.overlay.reset=0 will prevent any overlayFS files from being cleared.\nmetal.no-wipe=1 will guard against touching RAIDs, disks, and partitions.\npit# vi /var/www/boot/script.ipxe "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/troubleshoot_an_unresponsive_s3_endpoint/",
	"title": "Troubleshoot an Unresponsive Rados-Gateway (radosgw) S3 Endpoint",
	"tags": [],
	"description": "",
	"content": "Troubleshoot an Unresponsive Rados-Gateway (radosgw) S3 Endpoint The following section includes various issues causing an unresponsive radosgw S3 endpoint and how to resolve them.\nIssue 1: Rados-Gateway/s3 endpoint is Not Accessible ncn# response=$(curl --write-out \u0026#39;%{http_code}\u0026#39; --silent --output /dev/null http://rgw-vip)|echo \u0026#34;Curl Response Code: $response\u0026#34; Curl Response Code: 200 Expected Responses: 2xx, 3xx\nProcedure Check the individual endpoints.\nncn# num_storage_nodes=$(craysys metadata get num_storage_nodes);for node_num in $(seq 1 \u0026#34;$num_storage_nodes\u0026#34;); do nodename=$(printf \u0026#34;ncn-s%03d\u0026#34; \u0026#34;$node_num\u0026#34;); response=$(curl --write-out \u0026#39;%{http_code}\u0026#39; --silent --output /dev/null http://$nodename:8080); echo \u0026#34;Curl Response Code for ncn-s00$endpoint: $response\u0026#34;; done Curl Response Code for ncn-s003: 200 Curl Response Code for ncn-s003: 200 Curl Response Code for ncn-s003: 200 Troubleshooting: If an error occurs with the above script, then echo $num_storage_nodes. If it is not an integer that matches the known configuration of the number of Utility Storage nodes, then run cloud-init init to refresh the cloud-init cache. Alternatively, manually set that number if the number of Utility Storage nodes is known.\nCheck the HAProxy endpoint.\nncn# response=$(curl --write-out \u0026#39;%{http_code}\u0026#39; --silent --output /dev/null http://rgw-vip)|echo \u0026#34;Curl Response Code: $response\u0026#34; Curl Response Code: 200 Verify HAProxy and KeepAlived status.\nKeepAlived:\nCheck KeepAlived on each node running ceph-radosgw. By default, this will be all Utility Storage nodes, but may differ based on your configuration. ncn-s# systemctl is-active keepalived.service active should be returned in the output.\nCheck for the KeepAlived instance hosting the VIP (Virtual IP). This command will have to be run on each node until you find the expected output. ncn-s# journalctl -u keepalived.service --no-pager |grep -i gratuitous Example output:\nAug 25 19:33:12 ncn-s001 Keepalived_vrrp[12439]: Registering gratuitous ARP shared channel Aug 25 19:43:08 ncn-s001 Keepalived_vrrp[12439]: Sending gratuitous ARP on bond0.nmn0 for 10.252.1.3 Aug 25 19:43:08 ncn-s001 Keepalived_vrrp[12439]: (VI_0) Sending/queueing gratuitous ARPs on bond0.nmn0 for 10.252.1.3 HAProxy:\nncn-s# systemctl is-active haproxy.service active should be returned in the output.\nIssue 2: Ceph Reports HEALTH_OK but S3 Operations Not Functioning Restart Ceph OSDs to help make the rgw.local:8080 endpoint responsive.\nCeph has an issue where it appears healthy but the rgw.local:8080 endpoint is unresponsive.\nThis issue occurs when ceph -s is run and produces a very high reads per second output:\nio: client: 103 TiB/s rd, 725 KiB/s wr, 2 op/s rd, 44 op/s wr The rgw.local endpoint needs to be responsive in order to interact directly with the Simple Storage Service (S3) RESTful API.\nPrerequisites This procedure requires admin privileges.\nProcedure View the OSD status.\nncn-m001# ceph osd tree Example output:\nID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 20.95312 root default -7 6.98438 host ncn-s001 0 ssd 3.49219 osd.0 up 1.00000 1.00000 3 ssd 3.49219 osd.3 up 1.00000 1.00000 -3 6.98438 host ncn-s002 2 ssd 3.49219 osd.2 up 1.00000 1.00000 5 ssd 3.49219 osd.5 up 1.00000 1.00000 -5 6.98438 host ncn-s003 1 ssd 3.49219 osd.1 up 1.00000 1.00000 4 ssd 3.49219 osd.4 up 1.00000 1.00000 Log in to each node and restart the OSDs.\nThe OSD number in the example below should be replaced with the number of the OSD being restarted.\nncn-m001# ceph orch restart osd.3 Wait for Ceph health to return to OK before moving between nodes.\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/keycloak_user_management_with_kcadm/",
	"title": "Keycloak User Management with kcadm.sh",
	"tags": [],
	"description": "",
	"content": "Keycloak User Management with kcadm.sh Overview The Cray CLI requires a valid Keycloak JWT token. If the token is not present it must be obtained by supplying valid user credentials at the time of CLI initialization. During the installation or upgrade of a Shasta system, the Keycloak user LDAP federation state or credentials may not be known, or the external LDAP system for which the user may have been federated may not be available. These scenarios make it impossible to initialize the Cray CLI with a previously valid Keycloak user or LDAP federated Keycloak user. If for any reason the Keycloak UI is not available for the administrator to verify the state of the user, using Keycloak\u0026rsquo;s kcadm.sh utility from the command line will be required.\nThe kcadm.sh utility is included with the Shasta Keycloak container image and can be used as described below to:\nPerform troubleshooting operations that can verify the existence of a user. Determine if a user is federated. Change a user credential (for a Keycloak local account only). Create a new local Keycloak user that is compatible (has the correct role mappings) with the Cray CLI. All of the operations described below must be run as root from the ncn-m001 node.\nUsage of Bash functions provided below will require making additions to the Linux environment. This can be accomplished by pasting the function into the shell, or pasting the function into a file and then sourcing the file, before running the indicated function.\nTopics Check the Status of a Keycloak User Update a Local Keycloak User Credential Create a Local Keycloak Cray CLI User Delete a Local Keycloak User Check the Status of a Keycloak User This procedure determines whether or not a user exists in Keycloak and whether or not that user is LDAP federated.\nIf a user is LDAP federated and the credential is not known, then that user credential must be updated in LDAP.\nAdd the following Bash function to your environment:\nkc-find-user(){ # Note that filtering does not use exact matching. For example, the below would # match the value of username attribute against \u0026#34;*$1*\u0026#34; pattern. if [ $# -eq 0 ]; then echo \u0026#34;Usage: ${FUNCNAME[0]} userName\u0026#34; return 1 fi local MAA MAA=$(kubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode) if [ -z \u0026#34;$MAA\u0026#34; ]; then echo \u0026#34;Unable to get the master admin authentication credential\u0026#34; return 1 fi kubectl -n services exec cray-keycloak-0 -c keycloak -- sh -c \u0026#34; echo \u0026#39;Looking for the Keycloak user: $1\u0026#39; \u0026amp;\u0026amp; ./opt/jboss/keycloak/bin/kcadm.sh get users -r shasta -q username=\u0026#39;$1\u0026#39; --no-config \\ --server http://localhost:8080/keycloak --realm master --user admin \\ --client admin-cli --password \u0026#39;$MAA\u0026#39;\u0026#34; } Example when the user account is LDAP federated, as indicated by the presence of the federationLink attribute:\nncn-m001# kc-find-user vers Looking for the Keycloak user: vers Logging into http://localhost:8080/keycloak as user admin of realm master [ { \u0026#34;id\u0026#34; : \u0026#34;4c494308-eeb9-4fba-a9c7-155ca42f3b7d\u0026#34;, \u0026#34;createdTimestamp\u0026#34; : 1643219255179, \u0026#34;username\u0026#34; : \u0026#34;vers\u0026#34;, \u0026#34;enabled\u0026#34; : true, \u0026#34;totp\u0026#34; : false, \u0026#34;emailVerified\u0026#34; : false, \u0026#34;firstName\u0026#34; : \u0026#34; ... \u0026#34;, \u0026#34;lastName\u0026#34; : \u0026#34; ... \u0026#34;, \u0026#34;email\u0026#34; : \u0026#34; ... \u0026#34;, \u0026#34;federationLink\u0026#34; : \u0026#34;0fe556b0-06a9-4628-84da-a35ff2f080ec\u0026#34;, \u0026#34;attributes\u0026#34; : { \u0026#34;loginShell\u0026#34; : [ \u0026#34;/bin/bash\u0026#34; ], \u0026#34;homeDirectory\u0026#34; : [ \u0026#34;/home/users/vers\u0026#34; ], \u0026#34;LDAP_ENTRY_DN\u0026#34; : [ \u0026#34;uid=vers,ou=people,dc=dcldap,dc=dit\u0026#34; ], \u0026#34;uidNumber\u0026#34; : [ \u0026#34;1356\u0026#34; ], \u0026#34;gidNumber\u0026#34; : [ \u0026#34;11121\u0026#34; ], \u0026#34;createTimestamp\u0026#34; : [ \u0026#34;20200413200346Z\u0026#34; ], \u0026#34;modifyTimestamp\u0026#34; : [ \u0026#34;20211015174258Z\u0026#34; ], \u0026#34;LDAP_ID\u0026#34; : [ \u0026#34;vers\u0026#34; ] }, \u0026#34;disableableCredentialTypes\u0026#34; : [ ], \u0026#34;requiredActions\u0026#34; : [ ], \u0026#34;notBefore\u0026#34; : 0, \u0026#34;access\u0026#34; : { \u0026#34;manageGroupMembership\u0026#34; : true, \u0026#34;view\u0026#34; : true, \u0026#34;mapRoles\u0026#34; : true, \u0026#34;impersonate\u0026#34; : true, \u0026#34;manage\u0026#34; : true } } ] If the user is a local Keycloak user and the credential is not known, then see Update a Local Keycloak User Credential.\nExample when the user account is a local Keycloak user (no federationLink attribute):\nncn-m001# kc-find-user localcli Looking for the Keycloak user: localcli Logging into http://localhost:8080/keycloak as user admin of realm master [ { \u0026#34;id\u0026#34; : \u0026#34;b49abdcc-a314-4355-89d7-44f4d8d33ab8\u0026#34;, \u0026#34;createdTimestamp\u0026#34; : 1649193410108, \u0026#34;username\u0026#34; : \u0026#34;localcli\u0026#34;, \u0026#34;enabled\u0026#34; : true, \u0026#34;totp\u0026#34; : false, \u0026#34;emailVerified\u0026#34; : false, \u0026#34;disableableCredentialTypes\u0026#34; : [ ], \u0026#34;requiredActions\u0026#34; : [ ], \u0026#34;notBefore\u0026#34; : 0, \u0026#34;access\u0026#34; : { \u0026#34;manageGroupMembership\u0026#34; : true, \u0026#34;view\u0026#34; : true, \u0026#34;mapRoles\u0026#34; : true, \u0026#34;impersonate\u0026#34; : true, \u0026#34;manage\u0026#34; : true } } ] If the user does not exist and it is necessary to create a local Keycloak user for use with the Cray CLI, then see Create a Local Keycloak Cray CLI User.\nExample when the user account is not known to Keycloak:\nncn-m001# kc-find-user nouser Looking for the Keycloak user: nouser Logging into http://localhost:8080/keycloak as user admin of realm master [ ]ncn-m001# Update a Local Keycloak User Credential This procedure resets the credential for a local Keycloak user. This procedure will not work if the user is LDAP federated.\nAdd the following Bash function to your environment:\nkc-set-local-user-password(){ if [ $# -eq 0 ]; then echo \u0026#34;Usage: ${FUNCNAME[0]} userName\u0026#34; return 1 fi local MAA MAA=$(kubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode) if [ -z \u0026#34;$MAA\u0026#34; ]; then echo \u0026#34;Unable to get the master admin authentication credential\u0026#34; return 1 fi # Specify a new default password. # Set KC_USER_NEWPASSWD to a specific password override the default # before calling this function. # i.e.: KC_USER_NEWPASSWD=someNewPassword : ${KC_USER_NEWPASSWD:=changeme1} kubectl -n services exec cray-keycloak-0 -c keycloak -- sh -c \u0026#34; echo \u0026#39;Resetting password for the Keycloak user: $1\u0026#39; \u0026amp;\u0026amp; ./opt/jboss/keycloak/bin/kcadm.sh set-password -r shasta --username \u0026#39;$1\u0026#39; \\ --new-password \u0026#39;$KC_USER_NEWPASSWD\u0026#39; --no-config --server http://localhost:8080/keycloak \\ --realm master --user admin --client admin-cli --password \u0026#39;$MAA\u0026#39;\u0026#34; } If desired, set the KC_USER_NEWPASSWD variable to override the default before calling the function.\nExample of updating the credential with an overridden password:\nncn-m001# KC_USER_NEWPASSWD=wax0nwax0ff kc-set-local-user-password localcli Resetting password for the Keycloak user: localcli Logging into http://localhost:8080/keycloak as user admin of realm master If the user cannot be found by username, then the account credential will not be updated, as in this example:\nncn-m001# kc-set-local-user-password localc Resetting password for the Keycloak user: localc Logging into http://localhost:8080/keycloak as user admin of realm master User not found for username: localc command terminated with exit code 1 If the user is LDAP federated, then the account credential will not be updated, as in this example:\nncn-m001# KC_USER_NEWPASSWD=wax0nwax0ff \u0026amp;\u0026amp; kc-set-local-user-password vers Resetting password for the Keycloak user: vers Logging into http://localhost:8080/keycloak as user admin of realm master null [Can\u0026#39;t reset password as account is read only] command terminated with exit code 1 Create a Local Keycloak Cray CLI User This procedure creates a new local Keycloak user that is compatible with the Cray CLI. This may be needed because it is not possible to use a federated user account to initialize the Cray CLI if LDAP is not available.\nAdd the following Bash function to your environment:\nkc-create-local-cli-user(){ if [ $# -eq 0 ]; then echo \u0026#34;Usage: ${FUNCNAME[0]} userName\u0026#34; return 1 fi local MAA MAA=$(kubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode) if [ -z \u0026#34;$MAA\u0026#34; ]; then echo \u0026#34;Unable to get the master admin authentication credential\u0026#34; return 1 fi # Specify a new default password. # Set KC_USER_NEWPASSWD to a specific password to override the default # before calling this function. # i.e.: KC_USER_NEWPASSWD=someNewPassword : ${KC_USER_NEWPASSWD:=changeme1} kubectl -n services exec cray-keycloak-0 -c keycloak -- sh -c \u0026#34; echo \u0026#39;Creating the Keycloak user: $1\u0026#39; \u0026amp;\u0026amp; ./opt/jboss/keycloak/bin/kcadm.sh create users -r shasta -s enabled=true -s username=\u0026#39;$1\u0026#39; \\ --no-config --server http://localhost:8080/keycloak --realm master --user admin \\ --client admin-cli --password \u0026#39;$MAA\u0026#39; \u0026amp;\u0026amp; echo \u0026#39;Resetting password for the Keycloak user: $1\u0026#39; \u0026amp;\u0026amp; ./opt/jboss/keycloak/bin/kcadm.sh set-password -r shasta --username \u0026#39;$1\u0026#39; \\ --new-password \u0026#39;$KC_USER_NEWPASSWD\u0026#39; --no-config --server http://localhost:8080/keycloak \\ --realm master --user admin --client admin-cli --password \u0026#39;$MAA\u0026#39; \u0026amp;\u0026amp; echo \u0026#39;Adding the Cray client admin role to the user: $1\u0026#39; \u0026amp;\u0026amp; ./opt/jboss/keycloak/bin/kcadm.sh add-roles -r shasta --uusername \u0026#39;$1\u0026#39; --cclientid cray \\ --rolename admin --no-config --server http://localhost:8080/keycloak --realm master \\ --user admin --client admin-cli --password \u0026#39;$MAA\u0026#39; \u0026amp;\u0026amp; echo \u0026#39;Adding the Shasta client admin role to the user: $1\u0026#39; \u0026amp;\u0026amp; ./opt/jboss/keycloak/bin/kcadm.sh add-roles -r shasta --uusername \u0026#39;$1\u0026#39; --cclientid shasta \\ --rolename admin --no-config --server http://localhost:8080/keycloak --realm master \\ --user admin --client admin-cli --password \u0026#39;$MAA\u0026#39;\u0026#34; } If desired, set the KC_USER_NEWPASSWD variable to override the default before calling the function.\nExample of creating a user:\nncn-m001# KC_USER_NEWPASSWD=wax0ffwax0n kc-create-local-cli-user localcli Creating the Keycloak user: localcli Logging into http://localhost:8080/keycloak as user admin of realm master Created new user with id \u0026#39;b49abdcc-a314-4355-89d7-44f4d8d33ab8\u0026#39; Resetting password for the Keycloak user: localcli Logging into http://localhost:8080/keycloak as user admin of realm master Adding the Cray client admin role to the user: localcli Logging into http://localhost:8080/keycloak as user admin of realm master Adding the Shasta client admin role to the user: localcli Logging into http://localhost:8080/keycloak as user admin of realm master If desired, the user creation may be verified by checking the status of the new user. See Check the Status of a Keycloak User.\nInitializing the Cray CLI is possible using this new user and credential. See Configure the Cray CLI.\nIf the user to be created already exists, then the function will just exit, as in this example:\nncn-m001# kc-create-local-cli-user localcli Creating the Keycloak user: localcli Logging into http://localhost:8080/keycloak as user admin of realm master User exists with same username command terminated with exit code 1 Delete a Local Keycloak User This procedure deletes a local Keycloak user. This will not work if the user is LDAP federated.\nAdd the following Bash function to your environment:\nkc-delete-local-user(){ if [ $# -eq 0 ]; then echo \u0026#34;Usage: ${FUNCNAME[0]} userID\u0026#34; echo \u0026#34;where \u0026#39;userID\u0026#39; is the \u0026#39;id\u0026#39; of the user as reported by kc-find-user\u0026#34; return 1 fi local confirm read -p \u0026#34;This will delete the Keycloak userID $1. Continue? (y/N): \u0026#34; \\ confirm \u0026amp;\u0026amp; [[ $confirm == [yY] ]] || return 1 local MAA MAA=$(kubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode) if [ -z \u0026#34;$MAA\u0026#34; ]; then echo \u0026#34;Unable to get the master admin authentication credential\u0026#34; return 1 fi kubectl -n services exec cray-keycloak-0 -c keycloak -- sh -c \u0026#34; echo \u0026#39;Deleting the Keycloak user UUID: $1\u0026#39; \u0026amp;\u0026amp; ./opt/jboss/keycloak/bin/kcadm.sh delete \u0026#39;users/$1\u0026#39; -r shasta --no-config \\ --server http://localhost:8080/keycloak --realm master --user admin \\ --client admin-cli --password \u0026#39;$MAA\u0026#39;\u0026#34; } Example usage:\nncn-m001# kc-find-user localcli Looking for the Keycloak user: localcli Logging into http://localhost:8080/keycloak as user admin of realm master [ { \u0026#34;id\u0026#34; : \u0026#34;b49abdcc-a314-4355-89d7-44f4d8d33ab8\u0026#34;, \u0026#34;createdTimestamp\u0026#34; : 1649193410108, \u0026#34;username\u0026#34; : \u0026#34;localcli\u0026#34;, \u0026#34;enabled\u0026#34; : true, \u0026#34;totp\u0026#34; : false, \u0026#34;emailVerified\u0026#34; : false, \u0026#34;disableableCredentialTypes\u0026#34; : [ ], \u0026#34;requiredActions\u0026#34; : [ ], \u0026#34;notBefore\u0026#34; : 0, \u0026#34;access\u0026#34; : { \u0026#34;manageGroupMembership\u0026#34; : true, \u0026#34;view\u0026#34; : true, \u0026#34;mapRoles\u0026#34; : true, \u0026#34;impersonate\u0026#34; : true, \u0026#34;manage\u0026#34; : true } } ] ncn-m001# kc-delete-local-user b49abdcc-a314-4355-89d7-44f4d8d33ab8 This will delete the Keycloak userID b49abdcc-a314-4355-89d7-44f4d8d33ab8. Continue? (y/N): y Deleting the Keycloak user UUID: b49abdcc-a314-4355-89d7-44f4d8d33ab8 Logging into http://localhost:8080/keycloak as user admin of realm master ncn-m001# kc-find-user localcli Looking for the Keycloak user: localcli Logging into http://localhost:8080/keycloak as user admin of realm master [ ] If the user can not be found by ID, then it will not be removed, as in this example:\nncn-m001# kc-delete-local-user b49abdcc-a314-4355-89d7-44f4d8d33ab8 This will delete the Keycloak userID b49abdcc-a314-4355-89d7-44f4d8d33ab8. Continue? (y/N): y Deleting the Keycloak user UUID: b49abdcc-a314-4355-89d7-44f4d8d33ab8 Logging into http://localhost:8080/keycloak as user admin of realm master Resource not found for url: http://localhost:8080/keycloak/admin/realms/shasta/users/b49abdcc-a314-4355-89d7-44f4d8d33ab8 command terminated with exit code 1 "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/launch_a_virtual_kvm_on_gigabyte_nodes/",
	"title": "Launch a Virtual KVM on Gigabyte Servers",
	"tags": [],
	"description": "",
	"content": "Launch a Virtual KVM on Gigabyte Servers This procedure shows how to launch a virtual KVM to connect to Gigabyte node. The virtual KVM can be launched on any host that is on the same network as the node\u0026rsquo;s BMC. This method of connecting to a node is frequently used during system installation.\nPrerequisites A laptop or workstation with a browser and access to the Internet. The externally visible IP address of the node\u0026rsquo;s integrated BMC. Procedure Connect to the node\u0026rsquo;s BMC by entering the externally visible BMC IP address in the address bar of a web browser.\nThe login page appears.\nLog in to the BMC.\nOn the American Megatrends BMC Firmware Information login page, enter the root user name and password for the BMC.\nThe Dashboard page appears.\nLaunch the remote console.\nClick on the Remote Control tab.\nLaunch the desired KVM viewer.\nH5Viewer is compatible with most browsers. JViewer is an option when HTML5 is not available. Serial Over LAN should be used as a last resort option. The virtual KVM is ready to use. There is now a virtual KVM session connected to the node that enables control via the web similar to standing directly in front of the physical KVM.\nTroubleshooting: If the interface appears to lock up while working in the BMC menus (often encountered when creating virtual drives), it may be necessary to reset the node using Power Control \u0026gt; Power Reset.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/mlag_switch/",
	"title": "MLAG",
	"tags": [],
	"description": "",
	"content": "MLAG A link aggregation group (LAG) is used for extending the bandwidth from a single link to multiple links and provide redundancy in case of link failure. Extending the implementation of the LAG to more than a single device provides yet another level of redundancy that extends from the link level to the node level. This extrapolation of the LAG from single to multiple switches is referred to as multi-chassis link aggregation (MLAG). MLAG is supported on Ethernet blades\u0026rsquo; internal and external ports.\nConfiguring L2 MLAG\nPrerequisites: Enable IP routing:\nswitch (config)# ip routing (Recommended) Enable LACP in the switch:\nswitch (config)# lacp Enable the MLAG protocol commands: switch (config)# protocol mlag Configuring the IPL:\nCreate a VLAN for the inter-peer link (IPL) to run on:\nswitch (config)# vlan 4000 switch (config vlan 4000)# Create a LAG:\nswitch (config)# interface port-channel 1 switch (config interface port-channel 1)# Map a physical port to the LAG in active mode (LACP):\nswitch (config)# interface ethernet 1/1 channel-group 1 mode active Set this LAG as an IPL:\nswitch (config interface port-channel 1)# ipl 1 Create a VLAN interface:\nswitch (config)# interface vlan 4000 switch (config interface vlan 4000)# Configure MTU to 9K:\nswitch (config interface vlan 4000)# mtu 9216 Set an IP address and netmask for the VLAN interface and configure IP address for the IPL link on both switches:\nNOTE: The IPL IP address should not be part of the management network, it could be any IP address and subnet that is not in use in the network. This address is not advertised outside the switch.\nOn Switch 1:\nswitch (config interface vlan 4000)# ip address 1.1.1.1 /30 On Switch 2:\nswitch (config interface vlan 4000)# ip address 1.1.1.2 /30 The peer with the interface VLAN with the highest IP address is the MLAG master.\nIn the example, above, Switch 2 (with IP address 1.1.1.2) is the master.\nThe IP addresses of both peers can be seen in via \u0026ldquo;show mlag\u0026rdquo; command.\nMap the VLAN interface to be used on the IPL and set the peer IP address (the IP address of the IPL port on the second switch) of the IPL peer port. IPL peer ports must be configured on the same netmask.\nOn Switch 1:\nswitch (config interface vlan 4000)# ipl 1 peer-address 1.1.1.2 On Switch 2:\nswitch (config interface vlan 4000)# ipl 1 peer-address 1.1.1.1 (Optional) Configure a virtual IP (VIP) address for the MLAG. MLAG VIP is important for retrieving peer information.\nNOTE: If you have a mgmt0 interface, the IP address should be within the subnet of the management interface. Do not use mgmt1. The management network is used for keepalive messages between the switches. The MLAG domain must be unique name for each MLAG domain. In case you have more than one pair of MLAG switches on the same network, each domain (consist of two switches) should be configured with different name.\nOn Switch 1:\nswitch (config)# mlag-vip my-vip ip 10.234.23.254 /24 On Switch 2:\nswitch (config)# mlag-vip my-vip (Optional) Configure a virtual system MAC for the MLAG:\nswitch (config)# mlag system-mac 00:00:5e:00:01:5d Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/management_network_configuration_example/",
	"title": "Example of How to Configure Scenario A or B",
	"tags": [],
	"description": "",
	"content": "Example of How to Configure Scenario A or B This section provides an example of how to configure the management network.\nProcedure Create the Customer Access Network (CAN) VRF for Aruba.\nsw# config sw# vrf CAN Move the interfaces into CAN VRF.\nIf there is an existing CAN interface configuration, it will be deleted the interface is moved into the new VRF; it will have to be re-applied.\nNOTE: These are example configurations only; most implementations of BICAN will be different.\nExample Aruba primary configuration:\ninterface vlan 7 vsx-sync active-gateways vrf attach CAN description CAN ip mtu 9198 ip address 128.55.176.2/23 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 128.55.176.1 ip ospf 2 area 0.0.0.210 Example Aruba secondary configuration:\ninterface vlan 7 vsx-sync active-gateways vrf attach CAN description CAN ip mtu 9198 ip address 128.55.176.3/23 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 128.55.176.1 ip ospf 2 area 0.0.0.210 Create a new BGP process in CAN VRF.\nA new BGP process will need to be running in the CAN VRF. This will peer with the CAN IP addresses on the NCN workers.\nThese are example configurations only. The neighbors below are the IP addresses of the CAN interface on the NCN workers.\nAruba configuration:\nrouter bgp 65533 vrf CAN maximum-paths 8 neighbor 128.55.176.3 remote-as 65533 neighbor 128.55.176.25 remote-as 65534 neighbor 128.55.176.25 passive neighbor 128.55.176.26 remote-as 65534 neighbor 128.55.176.26 passive neighbor 128.55.176.27 remote-as 65534 neighbor 128.55.176.27 passive Setup the customer Edge router.\nThe customer Edge router must be certified by the Slingshot team The configuration will be unique for most customers The following is an example configuration of a single Arista switch with a static LAG to a single Slingshot switch.\nArista LAG configuration:\ninterface Ethernet24/1 mtu 9214 flowcontrol send on flowcontrol receive on speed forced 100gfull error-correction encoding reed-solomon channel-group 1 mode on interface Ethernet25/1 mtu 9214 flowcontrol send on flowcontrol receive on speed forced 100gfull error-correction encoding reed-solomon channel-group 1 mode on interface Port-Channel1 mtu 9214 switchport access vlan 2 switchport trunk native vlan 2 switchport mode trunk Example VLAN 2 configuration:\nNOTE: VLAN 2 is used for the HSN network.\ninterface Vlan2 ip address 10.101.10.1/24 The following is the Arista BGP configuration for peering over the HSN. The BGP neighbor IP addresses used are HSN IP addresses of worker nodes.\nncn-w# ip a show hsn0 Example HSN IP addresses:\n8: hsn0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9000 qdisc mq state UP group default qlen 1000 link/ether 02:00:00:00:00:0d brd ff:ff:ff:ff:ff:ff inet 10.101.10.10/24 scope global hsn0 valid_lft forever preferred_lft forever inet6 fe80::ff:fe00:d/64 scope link valid_lft forever preferred_lft forever In this example, a prefix list and route-map are created to only accept routes from the HSN.\nExample Arista BGP configuration:\nip prefix-list HSN seq 10 permit 10.101.10.0/24 ge 24 route-map HSN permit 5 match ip address prefix-list HSN router bgp 65534 maximum-paths 32 neighbor 10.101.10.10 remote-as 65533 neighbor 10.101.10.10 transport connection-mode passive neighbor 10.101.10.10 route-map HSN in neighbor 10.101.10.11 remote-as 65533 neighbor 10.101.10.11 transport connection-mode passive neighbor 10.101.10.11 route-map HSN in neighbor 10.101.10.12 remote-as 65533 neighbor 10.101.10.12 transport connection-mode passive neighbor 10.101.10.12 route-map HSN in Configure MetalLB to peer with the new CAN VRF interfaces and the new HSN interface on the customer Edge router.\napiVersion: v1 data: config: | peers: - peer-address: 10.252.0.2 peer-asn: 65533 my-asn: 65533 - peer-address: 10.252.0.3 peer-asn: 65533 my-asn: 65533 - peer-address: 10.101.8.2 peer-asn: 65533 my-asn: 65536 - peer-address: 10.101.8.3 peer-asn: 65533 my-asn: 65536 - peer-address: 10.101.10.1 peer-asn: 65534 my-asn: 65533 address-pools: - name: customer-access protocol: bgp addresses: - 10.101.8.128/25 - name: customer-access-static protocol: bgp addresses: - 10.101.8.112/28 - name: customer-high-speed protocol: bgp addresses: - 10.101.10.128/25 - name: customer-high-speed-static protocol: bgp addresses: - 10.101.10.112/28 - name: hardware-management protocol: bgp addresses: - 10.94.100.0/24 - name: node-management protocol: bgp addresses: - 10.92.100.0/24 Verify BGP and routes.\nOnce MetalLB is configured, then the BGP peers on the customer Edge router and the CAN VRF should be established.\nArista Edge Router:\nsw-edge# show ip bgp summary Example output:\nBGP summary information for VRF default Router identifier 192.168.50.50, local AS number 65534 Neighbor Status Codes: m - Under maintenance Neighbor V AS MsgRcvd MsgSent InQ OutQ Up/Down State PfxRcd PfxAcc 10.101.10.10 4 65533 23 12 0 0 00:03:49 Estab 14 14 10.101.10.11 4 65533 25 11 0 0 00:03:49 Estab 16 16 10.101.10.12 4 65533 23 11 0 0 00:03:49 Estab 14 14 The Arista routing table should now include the external IP addresses exposed by MetalLB.\nThe on-site network team will be responsible for distributing these routes to the rest of their network.\nsw-edge# show ip route Example output:\nB E 10.101.8.113/32 [200/0] via 10.101.10.10, Vlan2 via 10.101.10.11, Vlan2 via 10.101.10.12, Vlan2 B E 10.101.8.128/32 [200/0] via 10.101.10.10, Vlan2 via 10.101.10.11, Vlan2 via 10.101.10.12, Vlan2 B E 10.101.8.129/32 [200/0] via 10.101.10.10, Vlan2 via 10.101.10.11, Vlan2 via 10.101.10.12, Vlan2 B E 10.101.8.130/32 [200/0] via 10.101.10.10, Vlan2 via 10.101.10.11, Vlan2 via 10.101.10.12, Vlan2 O 10.101.8.0/24 [110/20] via 192.168.75.3, Ethernet1/1 via 192.168.75.1, Ethernet2/1 Example of how BGP routes look like in the switch located in the HSN:\nsw-spine# show ip bgp vrf CAN summary Example output:\nVRF name : CAN BGP router identifier : 192.168.75.1 local AS number : 65533 BGP table version : 665 Main routing table version: 665 IPV4 Prefixes : 44 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.101.8.8 4 65536 24725 27717 665 0 0 0:11:52:43 ESTABLISHED/14 10.101.8.9 4 65536 24836 27692 665 0 0 0:08:44:20 ESTABLISHED/16 10.101.8.10 4 65536 24704 27741 665 0 0 0:08:44:18 ESTABLISHED/14 Configure default routes on NCN workers.\nChange the default route on the workers.\nThis is done so that they send their traffic out the HSN interface.\nncn-w# ip route replace default via 10.101.10.1 dev hsn0 Make the change persistent.\nDo this by creating an ifcfg file for hsn0 and removing the old VLAN 7 default route.\nncn-w# mv -v /etc/sysconfig/network/ifroute-bond0.cmn0 /etc/sysconfig/network/ifroute-bond0.cmn0.old ncn-w# echo \u0026#34;default 10.101.10.1 - -\u0026#34; \u0026gt; /etc/sysconfig/network/ifroute-hsn0 Verify the routing table.\nncn-w# ip route Example output:\ndefault via 10.101.10.1 dev hsn0 Verify external connectivity.\nncn-w# ping 8.8.8.8 -c 1 Example output:\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=110 time=13.6 ms There should now be external connectivity from outside the system to the external services offered by MetalLB over the HSN.\nVerify that the connection is going over the HSN with a traceroute.\nexternal# traceroute 10.101.8.113 Example output:\ntraceroute to 10.101.8.113 (10.101.8.113), 64 hops max, 52 byte packets 1 172.30.252.234 (172.30.252.234) 37.652 ms 37.930 ms 36.574 ms 2 10.103.255.228 (10.103.255.228) 37.684 ms 37.180 ms 36.765 ms 3 10.103.255.249 (10.103.255.249) 36.531 ms 38.350 ms 39.593 ms 4 172.30.254.219 (172.30.254.219) 38.543 ms 38.699 ms 40.811 ms 5 172.30.254.212 (172.30.254.212) 37.931 ms 37.347 ms 40.404 ms 6 172.30.254.243 (172.30.254.243) 47.029 ms 39.014 ms 38.292 ms 7 172.30.254.134 (172.30.254.134) 42.197 ms 37.267 ms 38.522 ms 8 172.30.254.130 (172.30.254.130) 39.562 ms 38.094 ms 39.500 ms 9 10.101.15.254 (10.101.15.254) 37.616 ms 37.741 ms 37.529 ms 10 10.101.15.178 (10.101.15.178) 39.465 ms 37.052 ms 36.734 ms 11 10.101.8.113 (10.101.8.113) 39.937 ms 38.565 ms 36.524 ms Listen on all the HSN interfaces for ping/traceroute while running ping on the external-facing IP address.\nIn this example, the IP address is 10.101.8.113.\nStart an ongoing ping command from an external system.\nexternal# ping 10.101.8.113 While the ping command is still running, listen on the HSN interfaces.\nncn-mw# nodes=$(kubectl get nodes| awk \u0026#39;{print $1}\u0026#39; | grep ncn-w | awk -vORS=, \u0026#39;{print $1}\u0026#39;); pdsh -w ${nodes} \u0026#34;tcpdump -envli hsn0 icmp\u0026#34; Example output:\nncn-w002: tcpdump: listening on hsn0, link-type EN10MB (Ethernet), capture size 262144 bytes ncn-w003: tcpdump: listening on hsn0, link-type EN10MB (Ethernet), capture size 262144 bytes ncn-w001: tcpdump: listening on hsn0, link-type EN10MB (Ethernet), capture size 262144 bytes ncn-w003: 04:59:35.826691 98:5d:82:71:ba:2d \u0026gt; 02:00:00:00:00:1e, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 54, id 951, offset 0, flags [none], proto ICMP (1), length 84) ncn-w003: 172.25.64.129 \u0026gt; 10.101.8.113: ICMP echo request, id 37368, seq 0, length 64 ncn-w003: 04:59:36.825591 98:5d:82:71:ba:2d \u0026gt; 02:00:00:00:00:1e, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 54, id 33996, offset 0, flags [none], proto ICMP (1), length 84) Stop the ping command that was started earlier. Back to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/troubleshoot_intermittent_503s/",
	"title": "Troubleshoot Intermittent HTTP 503 Code Failures",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Intermittent HTTP 503 Code Failures There are cases where API calls or cray command invocations will fail (sometimes intermittently) with an HTTP 503 error code. In the event that this occurs, attempt to remediate the issue by taking the following actions, according to specific error codes found in the pod or Envoy container log.\nThe Envoy container is typically named istio-proxy, and it runs as a sidecar for pods that are part of the Istio mesh. For pods with this sidecar, the logs can be viewed by running a command similar to the following:\nncn-mw# kubectl logs \u0026lt;podname\u0026gt; -n \u0026lt;namespace\u0026gt; -c istio-proxy | grep 503 For general Kubernetes troubleshooting information, including more information on viewing pod logs, see Kubernetes troubleshooting topics.\nThis page is broken into different sections, based on the errors found in the log.\nUF,URX with TLS error Symptom Description Remediation UAEX Symptom Description Remediation Other error codes UF,URX with TLS error Symptom (UF,URX with TLS error) [2022-05-10T16:27:29.232Z] \u0026#34;POST /apis/hbtd/hmi/v1/heartbeat HTTP/2\u0026#34; 503 UF,URX \u0026#34;-\u0026#34; \u0026#34;TLS error: Secret is not supplied by SDS\u0026#34; Description (UF,URX with a TLS error) Envoy containers can occasionally get into this state when NCNs are being rebooted or upgraded, as well as when many deployments are being created.\nRemediation (UF,URX with a TLS error) Do a Kubernetes delete or rolling restart:\nIf it is a single replica, then delete the pod.\nIf it is part of a multiple replica exhibiting the issue, then perform a rolling restart of the deployment or StatefulSet.\nHere is an example of how to do that for the istio-ingressgateway deployment in the istio-system namespace.\nInitiate a rolling restart of the deployment.\nncn-mw# kubectl rollout restart -n istio-system deployment istio-ingressgateway Wait for the restart to complete.\nncn-mw# kubectl rollout status -n istio-system deployment istio-ingressgateway Once the roll out is complete, or the new pod is running, then the HTTP 503 message should clear.\nUAEX Symptom (UAEX) [2022-06-24T14:16:27.229Z] \u0026#34;POST /apis/hbtd/hmi/v1/heartbeat HTTP/2\u0026#34; 503 UAEX \u0026#34;-\u0026#34; 131 0 30 - \u0026#34;10.34.0.0\u0026#34; \u0026#34;-\u0026#34; \u0026#34;1797b0d3-56f0-4674-8cf2-a8a61f9adaea\u0026#34; \u0026#34;api-gw-service-nmn.local\u0026#34; \u0026#34;-\u0026#34; - - 10.40.0.29:443 10.34.0.0:15995 api-gw-service-nmn.local - Description (UAEX) This error code typically indicates an issue with the authorization service (for example, Spire).\nRemediation (UAEX) Initiate a rolling restart of Spire.\nncn-mw# kubectl rollout restart -n spire statefulset spire-postgres spire-server ncn-mw# kubectl rollout restart -n spire daemonset spire-agent request-ncn-join-token ncn-mw# kubectl rollout restart -n spire deployment spire-jwks spire-postgres-pooler Wait for all of the restarts to complete.\nncn-mw# kubectl rollout status -n spire statefulset spire-postgres ncn-mw# kubectl rollout status -n spire statefulset spire-server ncn-mw# kubectl rollout status -n spire daemonset spire-agent ncn-mw# kubectl rollout status -n spire daemonset request-ncn-join-token ncn-mw# kubectl rollout status -n spire deployment spire-jwks ncn-mw# kubectl rollout status -n spire deployment spire-postgres-pooler Once the restarts are all complete, the HTTP 503 message should clear.\nOther error codes Although the above codes are most common, various other issues such as networking or application errors can cause different errors in the pod or sidecar logs. Refer to the Envoy access log documentation for a list of possible Envoy response flags. In general, running a rolling restart of the application itself to see if it clears the error is a good practice. If that does not resolve the problem, then an understanding of what the error message or response flag means is required to further troubleshoot the issue.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/update_the_privacy_settings_for_gitea_configuration_content_repositories/",
	"title": "Update the Privacy Settings for Gitea Configuration Content Repositories",
	"tags": [],
	"description": "",
	"content": "Update the Privacy Settings for Gitea Configuration Content Repositories Change the visibility of Gitea configuration content repositories from public to private. All Cray-provided repositories are created as private by default.\nPrerequisites Know the system\u0026rsquo;s external fully qualified domain name, referred to on this page as SYSTEM_DOMAIN_NAME. See System domain name for more information. Procedure Log in to the Version Control Service (VCS) as the crayvcs user.\nUse the following URL to access the VCS web interface: https://vcs.cmn.SYSTEM_DOMAIN_NAME\nNavigate to the cray organization.\nThe following URL should access it directly: https://vcs.cmn.SYSTEM_DOMAIN_NAME/vcs/cray\nSelect the repository title for each repository listed on the page.\nClick the Settings button in the repository header section.\nUpdate the visibility settings for the repository.\nClick the Visibility check box to make the repository private.\nClick the Update Settings button to save the change for the repository.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_trivial_file_transfer_protocol_tftp/",
	"title": "Troubleshoot Compute Node Boot Issues Related to Trivial File Transfer Protocol (TFTP)",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Compute Node Boot Issues Related to Trivial File Transfer Protocol (TFTP) TFTP issues can result in node boot failures. Use this procedure to investigate and resolve such issues.\nPrerequisites This procedure requires administrative privileges.\nLimitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.\nProcedure Check that the TFTP service is running.\nncn-m001# kubectl get pods -n services -o wide | grep cray-tftp Start a tcpdump session on the NCN.\nObtain the TFTP pod\u0026rsquo;s ID.\nncn-m001# PODID=$(kubectl get pods -n services --no-headers -o wide | grep cray-tftp | awk \u0026#39;{print $1}\u0026#39;) ncn-m001# echo $PODID Enter the TFTP pod using the pod ID.\nDouble check that PODID contains only one ID. If there are multiple TFTP pods listed, just choose one as the ID.\nncn-m001# kubectl exec -n services -it $PODID /bin/sh Start a tcpdump session from within the TFTP pod.\nOpen another terminal to perform the following tasks:\nUse a TFTP client to issue a TFTP request from either the NCN or a laptop.\nAnalyze the NCN tcpdump data to ensure that the TFTP discover request is visible.\nGo back to the original terminal to analyze the TFTP pod\u0026rsquo;s tcpdump data in order to ensure that the TFTP request is visible inside the pod.\nTroubleshooting If the TFTP request is not visible on the NCN, it may be caused by a firewall issue. If the TFTP request is not visible inside the pod, then double check that the request was issued over the correct interface for the Node Management Network (NMN). If it was, then the underlying issue could be related to the firewall.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/list_volumes_registered_in_uas/",
	"title": "List Volumes Registered in UAS",
	"tags": [],
	"description": "",
	"content": "List Volumes Registered in UAS List the details of all volumes registered in UAS with the cray uas admin config volumes list command. Use this command to obtain the volume_id value of volume, which is required for other UAS administrative commands.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) Procedure The volume registrations in the UAS configuration can be quite extensive and sometimes difficult to read in the default TOML format used by the cray administrative CLI. The following shows the --format option to the cray CLI being used to switch to various output formats that may be easier to read or more useful for certain tasks. Feel free to use that option with any cray CLI command to select a more comfortable output style.\nList the details of all the volumes registered in UAS.\nRetrieve the list in TOML.\nncn-m001-pit# cray uas admin config volumes list Example output:\n[[results]] mount_path = \u0026#34;/etc/localtime\u0026#34; volume_id = \u0026#34;11a4a22a-9644-4529-9434-d296eef2dc48\u0026#34; volumename = \u0026#34;timezone\u0026#34; [results.volume_description.host_path] path = \u0026#34;/etc/localtime\u0026#34; type = \u0026#34;FileOrCreate\u0026#34; [[results]] mount_path = \u0026#34;/etc/sssd\u0026#34; volume_id = \u0026#34;1ec36af0-d5b6-4ad9-b3e8-755729765d76\u0026#34; volumename = \u0026#34;broker-sssd-config\u0026#34; [results.volume_description.secret] default_mode = 384 secret_name = \u0026#34;broker-sssd-conf\u0026#34; [[results]] mount_path = \u0026#34;/lus\u0026#34; volume_id = \u0026#34;a3b149fd-c477-41f0-8f8d-bfcee87fdd0a\u0026#34; volumename = \u0026#34;lustre\u0026#34; [results.volume_description.host_path] path = \u0026#34;/lus\u0026#34; type = \u0026#34;DirectoryOrCreate\u0026#34; Retrieve the list in YAML format.\nncn-m001-pit# cray uas admin config volumes list --format yaml Example output:\n- mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: 11a4a22a-9644-4529-9434-d296eef2dc48 volumename: timezone - mount_path: /etc/sssd volume_description: secret: default_mode: 384 secret_name: broker-sssd-conf volume_id: 1ec36af0-d5b6-4ad9-b3e8-755729765d76 volumename: broker-sssd-config - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: a3b149fd-c477-41f0-8f8d-bfcee87fdd0a volumename: lustre Retrieve the list in JSON format.\nncn-m001-pit# cray uas admin config volumes list --format json Example output:\n[ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;11a4a22a-9644-4529-9434-d296eef2dc48\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/sssd\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;secret\u0026#34;: { \u0026#34;default_mode\u0026#34;: 384, \u0026#34;secret_name\u0026#34;: \u0026#34;broker-sssd-conf\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;1ec36af0-d5b6-4ad9-b3e8-755729765d76\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;broker-sssd-config\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;a3b149fd-c477-41f0-8f8d-bfcee87fdd0a\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; } ] The JSON formatted output can help guide administrators in constructing new volume descriptions required to add or update a volume description in UAS. JSON is the required input format for volume descriptions in UAS.\nLooking at the above output, each volume has a mount_path, volume_description, volume_name and volume_id entry.\nThe mount_path specifies where in the UAI the volume will be mounted.\nNOTE: While it is acceptable to have multiple volumes configured in UAS with the same mount_path, any given UAI will fail creation if it has more than one volume specified for a given mount path. If multiple volumes with the same mount path exist in the UAS configuration, all UAIs must be created using UAI classes that specify a workable subset of volumes. A UAI created without a UAI Class under such a UAS configuration will try to use all configured volumes and creation will fail.\nThe volume_description is the JSON description of the volume, specified as a dictionary with one entry, whose key identifies the kind of Kubernetes volume is described (i.e. host_path, configmap, secret, etc.) whose value is another dictionary containing the Kubernetes volume description itself. See Kubernetes documentation for details on what goes in various kinds of volume descriptions.\nThe volumename is a string the creator of the volume may chose to describe or name the volume. It must be comprised of only lower case alphanumeric characters and dashes (\u0026rsquo;-\u0026rsquo;) and must begin and end with an alphanumeric character. It is used inside the UAI pod specification to identify the volume that is mounted in a given location in a container. The name is required and administrators are free to use any name that meets the above requirements. Volume names do need to be unique within any given UAI and are far more useful when searching for a volume if they are unique across the entire UAS configuration.\nThe volume_id is a unique identifier used to identify the UAS volume when examining, updating or deleting a volume and when linking a volume to a UAI class. It is assigned automatically by UAS.\nTop: User Access Service (UAS)\nNext Topic: Add a Volume to UAS\n"
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/utility_storage/",
	"title": "Utility Storage",
	"tags": [],
	"description": "",
	"content": "Utility Storage Utility storage is designed to support Kubernetes and the System Management Services (SMS) it orchestrates. Utility storage is a cost-effective solution for storing the large amounts of telemetry and log data collected.\nCeph is the utility storage platform that is used to enable pods to store persistent data. It is deployed to provide block, object, and file storage to the management services running on Kubernetes, as well as for telemetry data coming from the compute nodes.\nIMPORTANT NOTES:\nCommands for Ceph health must be run from either a master NCN,ncn-s001, nnc-s002, or ncn-s003, unless they are otherwise specified to run on the host in question. Those nodes are the only ones with the necessary credentials. Individual procedures will specify when to run a command from a node other than those. Key Concepts Shrink: This only pertains to removing nodes from a cluster. Since Octopus and the move to utilize Ceph orchestrator, the Ceph cluster is probing nodes and adding unused drives. Removing a drive will only work if the actual drive is removed from a server. Add: This will most commonly pertain to adding a node with its full allotment of drives. Replace: This will most commonly pertain to replacing a drive or a node after hardware repairs. "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/make_https_requests_from_sources_outside_the_management_kubernetes_cluster/",
	"title": "Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster",
	"tags": [],
	"description": "",
	"content": "Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster Clients lying outside the system\u0026rsquo;s management cluster need to trust the Certificate Authority (CA) certificate or host certificate in order to make requests to a non-compute node (NCN). Getting the client system to trust the CA certificate depends on the operating system.\nThis procedure shows an example of how to have a client trust the system\u0026rsquo;s CA certificate on a Mac OS X system.\nUpdating the default certificates and password of the Keycloak master administrator is not supported.\nPrerequisites This procedure assumes that it is being carried out on a Mac OS X system.\nProcedure Retrieve the CA public certificate file.\nThe scp command is used in the following example to copy the CA public certificate file to the current directory.\nmac$ scp jones-ncn-m001.us.cray.com:/etc/pki/trust/anchors/platform-ca-certs.crt . Example output:\ncertificate_authority.crt 100% 989 1.4MB/s 00:00 Troubleshooting: Request a copy of the certificate file from a system administrator if it is not possible to log on to the NCN.\nUse Finder to locate the .crt file.\nDouble-click the .crt file to add it to the system keychain.\nThe .crt file might still not be trusted; start up the Keychain Access utility to mark it as trusted for SSL.\nDouble-click the untrusted certificate.\nExpand the Trust menu item to set Change Secure Sockets Layer (SSL) to Always Trust.\nQuit Keychain Access.\nMake secure requests to any NCN as needed.\nMaking a secure request to an NCN requires users to pass an authentication token. See Retrieve an Authentication Token in order to set the $TOKEN variable used in the example below.\nFor example:\nmac$ curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api.cmn.SYSTEM-NAME_DOMAIN-NAME/apis/capmc/capmc/get_node_rules Example output:\n{ \u0026#34;e\u0026#34;:0, \u0026#34;err_msg\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;latency_node_off\u0026#34;:60, \u0026#34;latency_node_on\u0026#34;:120, \u0026#34;latency_node_reinit\u0026#34;:180, \u0026#34;max_off_req_count\u0026#34;:-1, \u0026#34;max_off_time\u0026#34;:-1, \u0026#34;max_on_req_count\u0026#34;:-1, \u0026#34;max_reinit_req_count\u0026#34;:-1, \u0026#34;min_off_time\u0026#34;:-1 } "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/launch_a_virtual_kvm_on_intel_nodes/",
	"title": "Launch a Virtual KVM on Intel Servers",
	"tags": [],
	"description": "",
	"content": "Launch a Virtual KVM on Intel Servers This procedure shows how to launch a virtual KVM to connect to an Intel node. The virtual KVM can be launched on any host that is on the same network as the node\u0026rsquo;s BMC. This method of connecting to a node is frequently used during system installation.\nPrerequisites A laptop or workstation with a browser and access to the Internet. The externally visible IP address of the node\u0026rsquo;s integrated BMC. Procedure Connect to the node\u0026rsquo;s BMC by entering the externally visible BMC IP address in the address bar of a web browser.\nThe login page appears.\nTroubleshooting:\nJava exception: If a Java exception occurs when trying to connect via SOL, see Change Java Security Settings. Unable to access BMC: If unable to access the node\u0026rsquo;s BMC and ConMan is being used, ConMan may be blocking that access. See Troubleshoot ConMan Blocking Access to a Node BMC. Log in to the BMC.\nOn the Intel Integrated BMC Web Console login page, enter the root user name and password for the BMC.\nThe Summary page appears.\nTroubleshooting:\nIf using the Firefox browser and it takes a long time to connect, look in the corner for \u0026ldquo;TLS handshake.\u0026rdquo; That is an indication of a TLS handshake issue, which can occur when the browser has too many self-signed matching certificates. To fix this, locate the cert8.db and cert9.db files on the local host and rename them to cert8.db.bak and cert9.db.bak, respectively. Launch the remote console.\nOn the Remote Control tab, click KVM/Console Redirection.\nClick the Launch Console button near the top of the KVM/Console Redirection page.\nThis launches a Java web application called iKVM Viewer.\nThe virtual KVM (iKVM Viewer) is ready to use. There is now a virtual iKVM session connected to the node that enables control via the web similar to standing directly in front of the physical KVM.\nTroubleshooting: If the interface appears to lock up while working in the BMC menus (often encountered when creating virtual drives), it may be necessary to reset the node using Power Control \u0026gt; Power Reset.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/mstp/",
	"title": "Multiple spanning tree protocol (MSTP)",
	"tags": [],
	"description": "",
	"content": "Multiple spanning tree protocol (MSTP) MSTP (802.1s) ensures that only one active path exists between any two nodes in a spanning-tree instance. A spanning-tree instance comprises a unique set of VLANs. MSTP instances significantly improve network resource utilization while maintaining a loop-free environment.\nConfiguration commands Enable MSTP (default mode for spanning-tree)\nswitch# spanning-tree switch# spanning-tree mode mstp switch# spanning-tree mst revision 1 switch# spanning-tree mst name mellanox Show commands to validate functionality\nswitch# show spanning-tree Expected results Spanning-tree mode is configured Spanning-tree is enabled, if loops are detected ports should go blocked state. Spanning-tree splits traffic domain between two DUTs Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/management_network_function_in_detail/",
	"title": "System Management Network Functions",
	"tags": [],
	"description": "",
	"content": "System Management Network Functions The following is a description of the system management network functions:\nEdge - Any interactions with the Customer network or Internet\nCustomer Jobs - Customer Access Network (CAN) User-facing cloud APIs User Access Instances (UAIs) Customer Administration - Customer Management Network (CMN) Administrative Access to the system by Customer Admins Access from the system to external services: Customer/Internet DNS LDAP authentication System installation and upgrade media (e.g. Nexus) System - Access by the machine to external (Customer and/or Internet) resources. E.g. Internal DNS lookups may resolve to an external DNS Internal - Node-to-node communication inside the system\nAdministrative Hardware - Hardware Management Network (HMN) Direct BMC/iLOM access Hardware Discovery Firmware Updates Cloud Control Plane - Node Management Network (NMN) Job Control Plane - Node Management Network (NMN) Services\nTraditional network services (e.g. TFTP, DHCP, DNS) Cloud API and control Cloud-based System Services Jobs Traditional UAN New UAI Storage\nCeph (IP-based storage) Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/troubleshoot_postgres_database/",
	"title": "Troubleshoot Postgres Database",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Postgres Database This page contains general Postgres troubleshooting topics.\nThe patronictl tool Database unavailable Database disk full Replication lagging Postgres status SyncFailed Cluster member missing Postgres leader missing The patronictl tool The patronictl tool is used to call a REST API that interacts with Postgres databases. It handles a variety of tasks, such as listing cluster members and the replication status, configuring and restarting databases, and more.\nThe tool is installed in the database containers:\nncn-mw# kubectl exec -it -n services keycloak-postgres-0 -c postgres -- su postgres Use the following command for more information on the patronictl command:\npostgres@keycloak-postgres-0:~$ patronictl --help Database unavailable If there are no endpoints for the main service, Patroni will mark the database as unavailable.\nThe following is an example for keycloak-postgres where no endpoints are listed, which means the database is unavailable.\nncn-mw# kubectl get endpoints keycloak-postgres -n services Example output:\nNAME ENDPOINTS AGE keycloak-postgres \u0026lt;none\u0026gt; 3d22h If the database is unavailable, check if Disk Full is the cause of the issue. Otherwise, check the postgres-operator logs for errors.\nncn-mw# kubectl logs -l app.kubernetes.io/name=postgres-operator -n services Database disk full The following is an example for keycloak-postgres. One cluster member is failing to start because of a full pgdata disk. This was likely due to replication issues, which caused the pg_wal files to grow.\nncn-mw# POSTGRESQL=keycloak-postgres ncn-mw# NAMESPACE=services ncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-1\u0026#34; -c postgres -it -n ${NAMESPACE} -- patronictl list Example output:\n+-------------------+---------------------+------------+--------+--------------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+--------------+----+-----------+ | keycloak-postgres | keycloak-postgres-0 | 10.42.0.11 | | start failed | | unknown | | keycloak-postgres | keycloak-postgres-1 | 10.44.0.7 | | running | 4 | 0 | | keycloak-postgres | keycloak-postgres-2 | 10.36.0.40 | Leader | running | 4 | | +-------------------+---------------------+------------+--------+--------------+----+-----------+ ncn-mw# for i in {0..2}; do echo \u0026#34;${POSTGRESQL}-${i}:\u0026#34;; kubectl exec \u0026#34;${POSTGRESQL}-${i}\u0026#34; -n ${NAMESPACE} -c postgres -- df -h pgdata; done Example output:\nkeycloak-postgres-0: Filesystem Size Used Avail Use% Mounted on /dev/sde 976M 960M 0 100% /home/postgres/pgdata keycloak-postgres-1: Filesystem Size Used Avail Use% Mounted on /dev/rbd12 976M 152M 809M 16% /home/postgres/pgdata keycloak-postgres-2: Filesystem Size Used Avail Use% Mounted on /dev/rbd3 976M 136M 825M 15% /home/postgres/pgdata ncn-mw# kubectl logs \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres | grep FATAL Example output:\n2021-07-14 17:52:48 UTC [30495]: [1-1] 60ef2470.771f 0 FATAL: could not write lock file \u0026#34;postmaster.pid\u0026#34;: No space left on device ncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- du -h --max-depth 1 /home/postgres/pgdata/pgroot/data/pg_wal To recover the cluster member that had failed to start because of disk pressure, attempt to reclaim some space on the pgdata disk.\nkubectl exec into that pod, copy the logs off (optional), and then clear the logs in order to recover some disk space. Finally, restart the Postgres cluster and postgres-operator.\nCopy off the logs (optional).\nncn-mw# kubectl cp \u0026#34;${POSTGRESQL}-1\u0026#34;:/home/postgres/pgdata/pgroot/pg_log /tmp -c postgres -n ${NAMESPACE} Clear the logs.\nncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-1\u0026#34; -n ${NAMESPACE} -c postgres -it -- bash root@cray-smd-postgres-1:/home/postgres# for i in {0..7}; do \u0026gt; /home/postgres/pgdata/pgroot/pg_log/postgresql-$i.csv; done Restart the pods by deleting them.\nncn-mw# kubectl delete pod \u0026#34;${POSTGRESQL}-0\u0026#34; \u0026#34;${POSTGRESQL}-1\u0026#34; \u0026#34;${POSTGRESQL}-2\u0026#34; -n ${NAMESPACE} Restart the operator by deleting it.\nncn-mw# kubectl delete pod -l app.kubernetes.io/name=postgres-operator -n services If disk issues persist or exist on multiple nodes and the above does not resolve the issue, then see the Recover from Postgres WAL Event procedure.\nReplication lagging Postgres replication lag can be detected with Prometheus alerts and alert notifications (See Configure Prometheus Email Alert Notifications). If replication lag is not caught early, it can cause the disk mounted on /home/postgres/pgdata to fill up and the database to stop running. If this issue is caught before the database stops, it can be easily remediated using a patronictl command to reinitialize the lagging cluster member.\nCheck if replication is working When services have a Postgres cluster of pods, they need to be able to replicate data between them. When the pods are not able to replicate data, the database will become full. The patronictl list command will show the status of replication.\nReplication is working The following is an example where replication is working:\nncn-mw# kubectl exec keycloak-postgres-0 -c postgres -n services -it -- patronictl list Example output:\n+-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | keycloak-postgres | keycloak-postgres-0 | 10.40.0.23 | Leader | running | 1 | | | keycloak-postgres | keycloak-postgres-1 | 10.42.0.25 | | running | 1 | 0 | | keycloak-postgres | keycloak-postgres-2 | 10.42.0.29 | | running | 1 | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ Replication is not working The following is an example where replication is broken:\n+-------------------+---------------------+--------------+--------+----------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+--------------+--------+----------+----+-----------+ | keycloak-postgres | keycloak-postgres-0 | 10.42.10.22 | | starting | | unknown | | keycloak-postgres | keycloak-postgres-1 | 10.40.11.191 | Leader | running | 47 | | | keycloak-postgres | keycloak-postgres-2 | 10.40.11.190 | | running | 14 | 608 | +-------------------+---------------------+--------------+--------+----------+----+-----------+ Recover replication In the event that a state of broken Postgres replication persists and the space allocated for the WAL files fills up, the affected database will likely shut down and create a state where it can be very difficult to recover. This can impact the reliability of the related service and may require that it be redeployed with data repopulation procedures. If replication lag is caught and remediated before the database shuts down, replication can be recovered using patronictl reinit.\nA reinitialize will get the lagging replica member re-synced and replicating again. This should be done as soon as replication lag is detected. In the preceding example, keycloak-postgres-0 and keycloak-postgres-2 were not replicating properly (unknown or non-zero lag). To remediate, kubectl exec into the leader pod and use patronictl reinit \u0026lt;cluster\u0026gt; \u0026lt;lagging cluster member\u0026gt; to reinitialize the lagging member(s).\nFor example:\nReinitialize the first lagging replica member.\nncn-mw# kubectl exec keycloak-postgres-1 -n services -it -- bash root@keycloak-postgres-1:/home/postgres# patronictl reinit keycloak-postgres keycloak-postgres-0 Example output:\nAre you sure you want to reinitialize members keycloak-postgres-0? [y/N]: y Failed: reinitialize for member keycloak-postgres-0, status code=503, (restarting after failure already in progress) Do you want to cancel it and reinitialize anyway? [y/N]: y Success: reinitialize for member keycloak-postgres-0 Reinitialize the next lagging replica member.\nroot@keycloak-postgres-1:/home/postgres# patronictl reinit keycloak-postgres keycloak-postgres-2 Example output:\nAre you sure you want to reinitialize members keycloak-postgres-2? [y/N]: y Failed: reinitialize for member keycloak-postgres-2, status code=503, (restarting after failure already in progress) Do you want to cancel it and reinitialize anyway? [y/N]: y Success: reinitialize for member keycloak-postgres-2 Verify that replication has recovered.\nncn-mw# kubectl exec keycloak-postgres-0 -c postgres -n services -it -- patronictl list Example output:\n+-------------------+---------------------+--------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+--------------+--------+---------+----+-----------+ | keycloak-postgres | keycloak-postgres-0 | 10.42.10.22 | | running | 47 | 0 | | keycloak-postgres | keycloak-postgres-1 | 10.40.11.191 | Leader | running | 47 | | | keycloak-postgres | keycloak-postgres-2 | 10.40.11.190 | | running | 47 | 0 | +-------------------+---------------------+--------------+--------+---------+----+-----------+ Troubleshooting If patronictl reinit fails with Failed: reinitialize for member \u0026hellip; status code=503, (Cluster has no leader, can not reinitialize):\nFor example:\nncn-mw# kubectl exec cray-console-data-postgres-0 -n services -- bash root@cray-console-data-postgres-0:~# patronictl reinit cray-console-data-postgres cray-console-data-postgres-1 Example output:\n+ Cluster: cray-console-data-postgres (7072784871993835594) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +------------------------------+------------+--------+---------+----+-----------+ | cray-console-data-postgres-0 | 10.39.0.74 | Leader | running | 1 | | | cray-console-data-postgres-1 | 10.36.0.37 | | running | 1 | 16 | | cray-console-data-postgres-2 | 10.32.0.1 | | running | 1 | 16 | +------------------------------+------------+--------+---------+----+-----------+ Are you sure you want to reinitialize members cray-console-data-postgres-1? [y/N]: y Failed: reinitialize for member cray-console-data-postgres-1, status code=503, (Cluster has no leader, can not reinitialize) Delete the Postgres leader pod and wait for the leader to restart.\nDelete the leader pod.\nncn-mw# kubectl delete pod cray-console-data-postgres-0 -n services Wait for the leader to restart.\nRe-run the following command until it succeeds and reports that the leader pod is running.\nncn-mw# kubectl exec keycloak-postgres-0 -c postgres -n services -it -- patronictl list Example output:\n+ Cluster: cray-console-data-postgres (7072784871993835594) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +------------------------------+------------+--------+---------+----+-----------+ | cray-console-data-postgres-0 | 10.39.0.80 | Leader | running | 2 | | | cray-console-data-postgres-1 | 10.36.0.37 | | running | 1 | 49 | | cray-console-data-postgres-2 | 10.32.0.1 | | running | 1 | 49 | +------------------------------+------------+--------+---------+----+-----------+ Re-run Is Replication Lagging? to reinit any lagging members.\nIf the reinit still fails, then delete member pods that are still reporting lag. This should clear up any remaining lag.\nDetermine which pods are reporting lag.\nncn-mw# kubectl exec cray-console-postgres-0 -c postgres -n services -it -- patronictl list Example output:\n+ Cluster: cray-console-data-postgres (7072784871993835594) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +------------------------------+------------+--------+---------+----+-----------+ | cray-console-data-postgres-0 | 10.39.0.80 | Leader | running | 2 | | | cray-console-data-postgres-1 | 10.36.0.37 | | running | 1 | 49 | | cray-console-data-postgres-2 | 10.32.0.1 | | running | 1 | 49 | +------------------------------+------------+--------+---------+----+-----------+ Delete the pods that are still reporting lag.\nncn-mw# kubectl delete pod cray-console-data-postgres-1 cray-console-data-postgres-2 -n services Once the pods restart, verify that the lag has resolved.\nncn-mw# kubectl exec cray-console-postgres-0 -c postgres -n services -it -- patronictl list Example output:\n+ Cluster: cray-console-data-postgres (7072784871993835594) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +------------------------------+------------+--------+---------+----+-----------+ | cray-console-data-postgres-0 | 10.39.0.80 | Leader | running | 2 | | | cray-console-data-postgres-1 | 10.36.0.37 | | running | 2 | 0 | | cray-console-data-postgres-2 | 10.32.0.1 | | running | 2 | 0 | +------------------------------+------------+--------+---------+----+-----------+ If a cluster member is stopped after a successful reinitialization, check for pg_internal.init.* files that may need to be cleaned up. This can occur if the pgdata disk was full prior to the reinitialization, leaving truncated pg_internal.init.* files in the pgdata directory.\nDetermine if any pods are stopped.\nncn-mw# kubectl exec keycloak-postgres-0 -c postgres -n services -it -- patronictl list Example output:\n+-------------------+---------------------+--------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+--------------+--------+---------+----+-----------+ | keycloak-postgres | keycloak-postgres-0 | 10.42.10.22 | | running | 47 | 0 | | keycloak-postgres | keycloak-postgres-1 | 10.40.11.191 | Leader | running | 47 | | | keycloak-postgres | keycloak-postgres-2 | 10.40.11.190 | | stopped | | unknown | +-------------------+---------------------+--------------+--------+---------+----+-----------+ Check the most recent Postgres log in the stopped pod.\nkubectl exec into that pod that is stopped and check the most recent Postgres log for any invalid segment number 0 errors relating to pg_internal.init.* files.\nncn-mw# kubectl exec keycloak-postgres-2 -n services -it -- bash postgres@keycloak-postgres-2:~$ export LOG=`ls -t /home/postgres/pgdata/pgroot/pg_log/*.csv | head -1` postgres@keycloak-postgres-2:~$ grep pg_internal.init $LOG | grep \u0026#34;invalid segment number 0\u0026#34; | tail -1 Example output:\n2022-02-01 16:59:35.529 UTC,\u0026#34;standby\u0026#34;,\u0026#34;\u0026#34;,227600,\u0026#34;127.0.0.1:42264\u0026#34;,61f966f7.37910,3,\u0026#34;sending backup \u0026#34;\u0026#34;pg_basebackup base backup\u0026#34;\u0026#34;\u0026#34;,2022-02-01 16:59:35 UTC,7/0,0,ERROR,XX000,\u0026#34;invalid segment number 0 in file \u0026#34;\u0026#34;pg_internal.init.2239188\u0026#34;\u0026#34;\u0026#34;,,,,,,,,,\u0026#34;pg_basebackup\u0026#34; If the check in the previous step finds such files, then first find any zero length pg_internal.init.* files.\nThis command should be run inside the kubectl exec session from the previous steps.\npostgres@keycloak-postgres-2:~$ find /home/postgres/pgdata -name pg_internal.init.* -size 0 Example output:\n./pgroot/data/base/16622/pg_internal.init.2239004 ... ./pgroot/data/base/16622/pg_internal.init.2239010 Delete the zero length pg_internal.init.* files.\nThis command should be run inside the kubectl exec session from the previous steps. Double check the syntax of the command in this step before executing it.\npostgres@keycloak-postgres-2:~$ find /home/postgres/pgdata -name pg_internal.init.* -size 0 -exec rm {} \\; Find any non-zero length pg_internal.init.* files that were truncated when the file system filled up.\nThis command should be run inside the kubectl exec session from the previous step.\npostgres@keycloak-postgres-2:~$ grep pg_internal.init $LOG | grep \u0026#34;invalid segment number 0\u0026#34; | tail -1 Example output:\n2022-02-01 16:59:35.529 UTC,\u0026#34;standby\u0026#34;,\u0026#34;\u0026#34;,227600,\u0026#34;127.0.0.1:42264\u0026#34;,61f966f7.37910,3,\u0026#34;sending backup \u0026#34;\u0026#34;pg_basebackup base backup\u0026#34;\u0026#34;\u0026#34;,2022-02-01 16:59:35 UTC,7/0,0,ERROR,XX000,\u0026#34;invalid segment number 0 in file \u0026#34;\u0026#34;pg_internal.init.2239188\u0026#34;\u0026#34;\u0026#34;,,,,,,,,,\u0026#34;pg_basebackup\u0026#34; Locate the non-zero length pg_internal.init.* file.\nThis command should be run inside the kubectl exec session from the previous step.\npostgres@keycloak-postgres-2:~$ find ~/pgdata -name pg_internal.init.2239188 Example output:\n/home/postgres/pgdata/pgroot/data/base/16622/pg_internal.init.2239188 Delete (or move to a different location) the non-zero length pg_internal.init.* file.\nThis command should be run inside the kubectl exec session from the previous step.\npostgres@keycloak-postgres-2:~$ rm /home/postgres/pgdata/pgroot/data/base/16622/pg_internal.init.2239188 Repeat the above steps to find, locate, and delete non-zero length pg_internal.init.* files until there are no more new invalid segment number 0 messages.\nVerify that the cluster member has started.\nncn-mw# kubectl exec keycloak-postgres-0 -c postgres -n services -it -- patronictl list Example output:\n+-------------------+---------------------+--------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+--------------+--------+---------+----+-----------+ | keycloak-postgres | keycloak-postgres-0 | 10.42.10.22 | | running | 47 | 0 | | keycloak-postgres | keycloak-postgres-1 | 10.40.11.191 | Leader | running | 47 | | | keycloak-postgres | keycloak-postgres-2 | 10.40.11.190 | | running | 47 | 0 | +-------------------+---------------------+--------------+--------+---------+----+-----------+ Setup alerts for replication lag Alerts exist in Prometheus for the following:\nPostgresqlReplicationLagSMA PostgresqlReplicationLagServices PostgresqlFollowerReplicationLagSMA PostgresqlFollowerReplicationLagServices When alert notifications are configured, replication issues can be detected quickly. If the replication issue persists such that the database becomes unavailable, recovery will likely be much more involved. Catching such issues as soon as possible is desired. See Configure Prometheus Email Alert Notifications.\nPostgres status SyncFailed Check all the postgresql resources Check for any postgresql resource that has a STATUS of SyncFailed. SyncFailed generally means that there is something between the postgres-operator and the Postgres cluster that is out of sync. This does not always mean that the cluster is unhealthy. Check the postgres-operator logs for messages in order to further determine the root cause of the issue.\nOther STATUS values such as Updating are a non-issue. It is expected that this will eventually change to Running or possibly SyncFailed, if the postgres-operator encounters issues syncing updates to the postgresql cluster.\nCheck for any postgresql resource that has a STATUS of SyncFailed.\nncn-mw# kubectl get postgresql -A Example output:\nNAMESPACE NAME TEAM VERSION PODS VOLUME CPU-REQUEST MEMORY-REQUEST AGE STATUS services cray-console-data-postgres cray-console-data 11 3 2Gi 4h10m Running services cray-sls-postgres cray-sls 11 3 1Gi 4h12m SyncFailed services cray-smd-postgres cray-smd 11 3 30Gi 500m 8Gi 4h12m Updating services gitea-vcs-postgres gitea-vcs 11 3 50Gi 4h11m Running services keycloak-postgres keycloak 11 3 1Gi 4h13m Running spire spire-postgres spire 11 3 20Gi 1 4Gi 4h10m Running Find the the postgres-operator pod name.\nncn-mw# kubectl get pods -l app.kubernetes.io/name=postgres-operator -n services Example output:\nNAME READY STATUS RESTARTS AGE cray-postgres-operator-6fffc48b4c-mqz7z 2/2 Running 0 5h26m Check the logs for the postgres-operator.\nncn-mw# kubectl logs cray-postgres-operator-6fffc48b4c-mqz7z -n services -c postgres-operator | grep -i sync | grep -i msg Case 1: some persistent volumes are not compatible with existing resizing providers Case 1: Symptom msg=\u0026#34;could not sync cluster: could not sync persistent volumes: could not sync volumes: could not resize EBS volumes: some persistent volumes are not compatible with existing resizing providers\u0026#34; Case 1: Details This generally means that the postgresql resource was updated to change the volume size from the Postgres operator\u0026rsquo;s perspective, but the additional step to resize the actual PVCs was not done so the operator and the Postgres cluster are not able to sync the resize change. The cluster is still healthy, but to complete the resize of the underlying Postgres PVCs, additional steps are needed.\nThe following example assumes that cray-smd-postgres is in SyncFailed and the volume size was recently increased to 100Gi (possibly by editing the volume size of postgresql cray-smd-postgres resource), but the pgdata-cray-smd-postgres PVC\u0026rsquo;s storage capacity was not updated to align with the change. To confirm this is the case:\nncn-mw# kubectl get postgresql cray-smd-postgres -n services -o jsonpath=\u0026#34;{.spec.volume.size}\u0026#34; 100Gi ncn-mw# kubectl get pvc -n services -l application=spilo,cluster-name=cray-smd-postgres NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pgdata-cray-smd-postgres-0 Bound pvc-020cf339-e372-46ae-bc37-de2b55320e88 30Gi RWO k8s-block-replicated 70m pgdata-cray-smd-postgres-1 Bound pvc-3d42598a-188e-4301-a58e-0f0ce3944c89 30Gi RWO k8s-block-replicated 27m pgdata-cray-smd-postgres-2 Bound pvc-0d659080-7d39-409a-9ee5-1a1806971054 30Gi RWO k8s-block-replicated 27m To resolve this SyncFailed case, resize the pgdata PVCs for the selected Postgres cluster. Create the following function in the shell and execute the function by calling it with the appropriate arguments. For this example the pgdata-cray-smd-postgres PVCs will be resized to 100Gi to match that of the postgresql cray-smd-postgres volume size.\nfunction resize-postgresql-pvc { POSTGRESQL=$1 PGDATA=$2 NAMESPACE=$3 PGRESIZE=$4 # Check for required arguments if [ $# -ne 4 ]; then echo \u0026#34;Illegal number of parameters ($#). Function requires exactly 4 arguments.\u0026#34; exit 2 fi ## Check that PGRESIZE matches current postgresql volume size postgresql_volume_size=$(kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o jsonpath=\u0026#34;{.spec.volume.size}\u0026#34;) if [ \u0026#34;${postgresql_volume_size}\u0026#34; != \u0026#34;${PGRESIZE}\u0026#34; ]; then echo \u0026#34;Invalid resize ${PGRESIZE}, expected ${postgresql_volume_size}\u0026#34; exit 2 fi ## Scale the postgres cluster to 1 member kubectl patch postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/numberOfInstances\u0026#34;, \u0026#34;value\u0026#34; : 1}]\u0026#39; while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; | grep -v NAME | wc -l) != 1 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34; sleep 2 done ## Delete the inactive PVCs, resize the active PVC, and wait for the resize to complete kubectl delete pvc \u0026#34;${PGDATA}-1\u0026#34; \u0026#34;${PGDATA}-2\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; kubectl patch -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;resources\u0026#34;: {\u0026#34;requests\u0026#34;: {\u0026#34;storage\u0026#34;: \u0026#34;\u0026#39;${PGRESIZE}\u0026#39;\u0026#34;}}}}\u0026#39; \u0026#34;pvc/${PGDATA}-0\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; while [ -z \u0026#39;$(kubectl describe pvc \u0026#34;{PGDATA}-0\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; | grep FileSystemResizeSuccessful\u0026#39; ] ; do echo \u0026#34; waiting for PVC to resize\u0026#34; sleep 2 done ## Scale the postgres cluster back to 3 members kubectl patch postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/numberOfInstances\u0026#34;, \u0026#34;value\u0026#34; : 3}]\u0026#39; while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; | grep -v NAME | grep -c \u0026#34;Running\u0026#34;) != 3 ] ; do echo \u0026#34; waiting for pods to restart\u0026#34; sleep 2 done } ncn-mw# resize-postgresql-pvc cray-smd-postgres pgdata-cray-smd-postgres services 100Gi In order to persist any Postgres PVC storage volume size changes, it is necessary that this change also be made to the customer-managed customizations.yaml file. See the Postgres PVC Resize information in the Post Install Customizations document.\nCase 2: could not init db connection Case 2: Symptom msg=\u0026#34;could not sync cluster: could not sync roles: could not init db connection: could not init db connection: still failing after 8 retries\u0026#34; Case 2: Details This generally means that some state in the Postgres operator is out of sync with that of the postgresql cluster, resulting in database connection issues.\nCase 2: Restart the Postgres operator To resolve this SyncFailed case, restarting the Postgres operator by deleting the pod may clear up the issue.\nDelete the pod.\nncn-mw# kubectl delete pod -l app.kubernetes.io/name=postgres-operator -n services Wait for the postgres-operator to restart.\nncn-mw# kubectl get pods -l app.kubernetes.io/name=postgres-operator -n services Example output of restarted pod:\nNAME READY STATUS RESTARTS AGE cray-postgres-operator-6fffc48b4c-mqz7z 2/2 Running 0 6m Case 2: Restart the cluster and operator If the database connection has been down for a long period of time and the SyncFailed persists after the above steps, a restart of the cluster and the postgres-operator may be needed for the service to reconnect to the Postgres cluster. For example, if the cray-gitea service is not able to connect to the Postgres database and the connection has been failing for many hours, restart the cluster and operator.\nSet necessary variables.\nReplace the values of these variables for the appropriate ones for the particular cluster being remediated.\nncn-mw# CLIENT=gitea-vcs ncn-mw# POSTGRESQL=gitea-vcs-postgres ncn-mw# NAMESPACE=services Scale the service to 0.\nncn-mw# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=0 Restart the Postgres cluster and the postgres-operator.\nncn-mw# kubectl delete pod \u0026#34;${POSTGRESQL}-0\u0026#34; \u0026#34;${POSTGRESQL}-1\u0026#34; \u0026#34;${POSTGRESQL}-2\u0026#34; -n ${NAMESPACE} ncn-mw# kubectl delete pods -n services -lapp.kubernetes.io/name=postgres-operator ncn-mw# while [ $(kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ]; do echo \u0026#34;waiting for ${POSTGRESQL} to start running\u0026#34;; sleep 2 done Scale the service back to 1 (for different services this may be to 3).\nncn-mw# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=1 Case 3: password authentication failed for user Case 3: Symptom msg=\u0026#34;error while syncing cluster state: could not sync roles: could not init db connection: could not init db connection: pq: password authentication failed for user \\\u0026lt;username\\\u0026gt;\u0026#34; Case 3: Details This generally means that the password for the given user is not the same as that specified in the Kubernetes secret. This can occur if the postgresql cluster was rebuilt and the data was restored, leaving the Kubernetes secrets out of sync with the Postgres cluster. To resolve this SyncFailed case, gather the username and password for the credential from Kubernetes, and update the database with these values. For example, if the user postgres is failing to authenticate between the cray-smd services and the cray-smd-postgres cluster, then get the password for the postgres user from the Kubernetes secret and update the password in the database.\nSet necessary variables.\nncn-mw# CLIENT=cray-smd ncn-mw# POSTGRESQL=cray-smd-postgres ncn-mw# NAMESPACE=services Scale the service to 0.\nncn-mw# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=0 ncn-mw# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34; sleep 2 done Determine what secrets are associated with the postgresql credentials.\nncn-mw# kubectl get secrets -n ${NAMESPACE} | grep \u0026#34;${POSTGRESQL}.credentials\u0026#34; Example output:\nservices hmsdsuser.cray-smd-postgres.credentials Opaque 2 31m services postgres.cray-smd-postgres.credentials Opaque 2 31m services service-account.cray-smd-postgres.credentials Opaque 2 31m services standby.cray-smd-postgres.credentials Opaque 2 31m Gather the decoded username and password for the user that is failing to authenticate.\nSave the name of the secret with the failing authentication in a variable.\nReplace the secret name in this command with the secret determined in the previous step.\nncn-mw# SECRET=postgres.cray-smd-postgres.credentials Decode the username.\nncn-mw# kubectl get secret ${SECRET} -n ${NAMESPACE} -ojsonpath=\u0026#39;{.data.username}\u0026#39; | base64 -d Example output:\npostgres Decode the password.\nncn-mw# kubectl get secret ${SECRET} -n ${NAMESPACE} -ojsonpath=\u0026#39;{.data.password}\u0026#39;| base64 -d Example output:\nABCXYZ Update the username and password in the database.\nDetermine which pod is the leader.\nncn-mw# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- patronictl list Example output:\n+-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | Leader | running | 1 | | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | running | | 0 | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | running | | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ kubectl exec into the Postgres leader and update the username and password in the database.\nncn-mw# POSTGRES_LEADER=cray-smd-postgres-0 ncn-mw# kubectl exec ${POSTGRES_LEADER} -n ${NAMESPACE} -c postgres -it -- bash root@cray-smd-postgres-0:/home/postgres# /usr/bin/psql postgres postgres postgres=# ALTER USER postgres WITH PASSWORD \u0026#39;ABCXYZ\u0026#39;; On success, output of the final command resembles the following:\nALTER ROLE Restart the postgresql cluster.\nncn-mw# kubectl delete pod \u0026#34;${POSTGRESQL}-0\u0026#34; \u0026#34;${POSTGRESQL}-1\u0026#34; \u0026#34;${POSTGRESQL}-2\u0026#34; -n ${NAMESPACE} ncn-mw# while [ $(kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ]; do echo \u0026#34;waiting for ${POSTGRESQL} to start running\u0026#34; sleep 2 done Scale the service back to 3.\nncn-mw# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=3 ncn-mw# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 3 ] ; do echo \u0026#34; waiting for pods to start running\u0026#34; sleep 2 done Cluster member missing Most services expect to maintain a Postgres cluster consisting of three pods for resiliency (System Monitoring Application (SMA) is one exception where only two pods are expected to exist).\nDetermine if a cluster member is missing For a given Postgres cluster, check how many pods are running.\nncn-mw# POSTGRESQL=keycloak-postgres ncn-mw# NAMESPACE=services ncn-mw# kubectl get pods -A -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; Recover from a missing member If the number of Postgres pods for the given cluster is more or less than expected, increase or decrease as needed. This example will patch the keycloak-postgres cluster resource so that three pods are running.\nSet the POSTGRESQL and NAMESPACE variables.\nncn-mw# POSTGRESQL=keycloak-postgres ncn-mw# NAMESPACE=services Patch the keycloak-postgres cluster resource to ensure three pods are running.\nncn-mw# kubectl patch postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --type=\u0026#39;json\u0026#39; \\ -p=\u0026#39;[{\u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/numberOfInstances\u0026#34;, \u0026#34;value\u0026#34; : 3}]\u0026#39; Confirm the number of cluster members, otherwise known as pods, by checking the postgresql resource.\nncn-mw# kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} Example output:\nNAME TEAM VERSION PODS VOLUME CPU-REQUEST MEMORY-REQUEST AGE STATUS keycloak-postgres keycloak 11 3 10Gi 29m Running If a pod is starting but remains in Pending, CrashLoopBackOff, ImagePullBackOff, or other non-Running states, then describe the pod and get logs from the pod for further analysis.\nFind the pod name.\nncn-mw# kubectl get pods -A -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; Example output:\nNAMESPACE NAME READY STATUS RESTARTS AGE services keycloak-postgres-0 0/3 Pending 0 36m services keycloak-postgres-1 3/3 Running 0 35m services keycloak-postgres-2 3/3 Running 0 34m Describe the pod.\nncn-mw# kubectl describe pod \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} View the pod logs.\nncn-mw# kubectl logs \u0026#34;${POSTGRESQL}-0\u0026#34; -c postgres -n ${NAMESPACE} Postgres leader missing If a Postgres cluster no longer has a leader, the database will need to be recovered.\nDetermine if the Postgres leader is missing Set the POSTGRESQL and NAMESPACE variables.\nncn-mw# POSTGRESQL=cray-smd-postgres ncn-mw# NAMESPACE=services Check if the leader is missing.\nncn-mw# kubectl exec ${POSTGRESQL}-0 -n ${NAMESPACE} -c postgres -- patronictl list Example output:\n+-------------------+---------------------+------------+------+--------------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+------+--------------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | | running | | unknown | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | start failed | | unknown | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | start failed | | unknown | +-------------------+---------------------+------------+------+--------------+----+-----------+ If the output does not list a leader, then proceed to Recover from a missing Postgres leader.\nCheck if there is conflicting leader information.\nIt sometimes happen that the above check reports a leader, but other checks report no leader, or report conflicting leader information. The following steps show the status reported by each member of the cluster.\nMake a list of the Kubernetes pods of the cluster members.\nncn-mw# PODS=$(kubectl get pods -n ${NAMESPACE} --no-headers -o custom-columns=:.metadata.name | grep \u0026#34;^${POSTGRESQL}-[0-9]$\u0026#34;) ; echo ${PODS} Example output:\ncray-smd-postgres-0 cray-smd-postgres-1 cray-smd-postgres-2 Query each pod about the status of the cluster.\nThis script reports the cluster status as perceived by each member of the cluster.\nncn-mw# for POD in ${PODS} ; do echo \u0026#34;Checking ${POD}...\u0026#34; kubectl exec ${POD} -n ${NAMESPACE} -c postgres -- curl -s http://localhost:8008/cluster | jq ; echo done Example output:\nChecking cray-smd-postgres-0... { \u0026#34;members\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;cray-smd-postgres-0\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;api_url\u0026#34;: \u0026#34;http://10.32.0.33:8008/patroni\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.32.0.33\u0026#34;, \u0026#34;port\u0026#34;: 5432, \u0026#34;timeline\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;cray-smd-postgres-1\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;replica\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;api_url\u0026#34;: \u0026#34;http://10.44.0.30:8008/patroni\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.44.0.30\u0026#34;, \u0026#34;port\u0026#34;: 5432, \u0026#34;timeline\u0026#34;: 1, \u0026#34;lag\u0026#34;: 0 }, { \u0026#34;name\u0026#34;: \u0026#34;cray-smd-postgres-2\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;replica\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;api_url\u0026#34;: \u0026#34;http://10.47.0.33:8008/patroni\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.47.0.33\u0026#34;, \u0026#34;port\u0026#34;: 5432, \u0026#34;timeline\u0026#34;: 1, \u0026#34;lag\u0026#34;: 0 } ] } Checking cray-smd-postgres-1... { \u0026#34;members\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;cray-smd-postgres-0\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;api_url\u0026#34;: \u0026#34;http://10.32.0.33:8008/patroni\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.32.0.33\u0026#34;, \u0026#34;port\u0026#34;: 5432, \u0026#34;timeline\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;cray-smd-postgres-1\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;replica\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;api_url\u0026#34;: \u0026#34;http://10.44.0.30:8008/patroni\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.44.0.30\u0026#34;, \u0026#34;port\u0026#34;: 5432, \u0026#34;timeline\u0026#34;: 1, \u0026#34;lag\u0026#34;: 0 }, { \u0026#34;name\u0026#34;: \u0026#34;cray-smd-postgres-2\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;replica\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;api_url\u0026#34;: \u0026#34;http://10.47.0.33:8008/patroni\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.47.0.33\u0026#34;, \u0026#34;port\u0026#34;: 5432, \u0026#34;timeline\u0026#34;: 1, \u0026#34;lag\u0026#34;: 0 } ] } And so on for every member of the cluster. This script does not do any checking \u0026ndash; it only displays the information.\nCheck the output for errors or inconsistencies.\nIn particular, validate the following:\nEvery cluster member reports exactly one leader. Every cluster member reports the same leader. Every cluster member reports the same states for each member. If any of the above are not true, this indicates that the cluster members are no longer properly synchronized. In this case, attempt the Recover replication remediation procedures.\nRecover from a missing Postgres leader See the Recover from Postgres WAL Event procedure.\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/use_a_custom_ansible-cfg_file/",
	"title": "Use a Custom ansible.cfg File",
	"tags": [],
	"description": "",
	"content": "Use a Custom ansible.cfg File The Configuration Framework Service (CFS) allows for flexibility with the Ansible Execution Environment (AEE) by allowing for changes to included ansible.cfg file. When installed, CFS imports a custom ansible.cfg file into the cfs-default-ansible-cfg Kubernetes ConfigMap in the services namespace.\nAdministrators who want to make changes to the ansible.cfg file on a per-session or system-wide basis can upload a new file to a new ConfigMap in the services namespace, and then direct CFS to use their file. See Set the ansible.cfg for a Session for more information.\nCreate a new ansible.cfg file.\nCreate a new Kubernetes ConfigMap in the services namespace from this ansible.cfg file.\nncn-mw# kubectl create configmap custom-ansible-cfg -n services --from-file=ansible.cfg To use this Ansible configuration file for a specific session, set --ansible-config custom-ansible-cfg when creating a session. Alternatively, make the new file the default for new CFS sessions by specifying --default-ansible-config custom-ansible-cfg when setting global CFS options with the cray cfs options update command.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_unified_extensible_firmware_interface_uefi/",
	"title": "Troubleshoot Compute Node Boot Issues Related to Unified Extensible Firmware Interface (UEFI)",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Compute Node Boot Issues Related to Unified Extensible Firmware Interface (UEFI) If a node is stuck in the UEFI shell, ConMan will be able to connect to it, but nothing else will appear in its logs. The node\u0026rsquo;s logs will look similar to the following, indicating that ConMan is updating its log hourly:\n\u0026lt;ConMan\u0026gt; Console [86] log at 2018-09-07 20:00:00 CDT. \u0026lt;ConMan\u0026gt; Console [86] log at 2018-09-07 21:00:00 CDT. \u0026lt;ConMan\u0026gt; Console [86] log at 2018-09-07 22:00:00 CDT. \u0026lt;ConMan\u0026gt; Console [86] log at 2018-09-07 23:00:00 CDT. \u0026lt;ConMan\u0026gt; Console [86] log at 2018-09-08 00:00:00 CDT. This procedure helps resolve this issue.\nPrerequisites This procedure requires administrative privileges.\nLimitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.\nProcedure Log onto the node using ipmitool or ConMan.\nType Enter.\nThe system will present one of the following prompts, both of which indicate that the user has entered the UEFI shell.\nUse one of the following options to resolve the issue:\nContact the system administrator or someone who is knowledgeable about UEFI if the node is stuck in the UEFI shell. Reseat the node if it is suspected that this may be a hardware-related issue. To reseat a node, pull the power cable off the back end of the box. Wait a few seconds, then reconnect the power cable and push the power button on the front of the box. Reseating a node must be done on site. Contact customer support for more information. "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/log_in_to_a_broker_uai/",
	"title": "Log in to a Broker UAI",
	"tags": [],
	"description": "",
	"content": "Log in to a Broker UAI SSH to log into a Broker UAI and reach the End-User UAIs on demand.\nPrerequisites The user must be logged into a host that can reach the external IP address of the Broker UAI The user must know the external IP address or DNS host name of the Broker UAI Procedure Log in to the Broker UAI.\nThe following example is the first login for the vers user:\nvers\u0026gt; ssh vers@35.226.246.154 The authenticity of host \u0026#39;35.226.246.154 (35.226.246.154)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:k4ef6vTtJ1Dtb6H17cAFh5ljZYTl4IXtezR3fPVUKZI. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;35.226.246.154\u0026#39; (ECDSA) to the list of known hosts. Password: Creating a new UAI... The authenticity of host \u0026#39;10.21.138.52 (10.21.138.52)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:TX5DMAMQ8yQuL4YHo9qFEJWpKaaiqfeSs4ndYXOTjkU. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added \u0026#39;10.21.138.52\u0026#39; (ECDSA) to the list of known hosts. There are several things to notice here:\nThe first time the user logs in the Broker UAI\u0026rsquo;s SSH host key is unknown, as is normal for SSH. The user is asked for a password in this example. If the user\u0026rsquo;s home directory, as defined in LDAP had been mounted in the Broker UAI and a .ssh/authorized_keys entry had been present, there would not have been a password prompt. Home directory trees can be mounted as volumes just as any other directory can. The broker mechanism in the Broker UAI creates a new UAI because vers has never logged into this Broker UAI before. There is a second prompt to acknowledge an unknown host which is, in this case, the End-User UAI itself. The Broker UAI constructs a public/private key pair for the hidden SSH connection between the broker and the End-User UAI shown in the image in Broker Mode UAI Management. Log out of the Broker UAI.\nLog in to the Broker UAI again.\nThe next time vers logs in, it will look similar to the following:\nvers\u0026gt; ssh vers@35.226.246.154 Password: vers@uai-vers-ee6f427e-6c7468cdb8-2rqtv\u0026gt; Only the password prompt appears now, because the hosts are all known and the End-User UAI exists but there is no .ssh/authorized_keys known yet by the Broker UAI for vers.\nTop: User Access Service (UAS)\nNext Topic: UAI Image Customization\n"
},
{
	"uri": "/docs-csm/en-12/operations/validate_csm_health/",
	"title": "Validate CSM Health",
	"tags": [],
	"description": "",
	"content": "Validate CSM Health Anytime after the installation of the CSM services, the health of the management nodes and all CSM services can be validated.\nThe following are examples of when to run health checks:\nAfter completing the Install CSM Services step of the CSM install (not before) Before and after NCN reboots After the system is brought back up Any time there is unexpected behavior observed In order to provide relevant information to create support tickets The areas should be tested in the order they are listed on this page. Errors in an earlier check may cause errors in later checks because of dependencies.\nTopics 0. Cray command line interface 1. Platform health checks 1.1 NCN health checks 1.1.1 Known issues with NCN health checks 1.2 NCN resource checks (optional) 1.2.1 Known issues with NCN resource checks 1.3 Check of system management monitoring tools 2. Hardware Management Services health checks 2.1 HMS CT test execution 2.2 Hardware State Manager discovery validation 2.2.1 Interpreting HSM discovery results 2.2.2 Known issues with HSM discovery validation 3. Software Management Services health checks 3.1 SMS test execution 3.2 Interpreting cmsdev results 3.3 Known issues with SMS tests 4. Gateway health and SSH access checks 4.1 Gateway health tests 4.1.1 Gateway health tests overview 4.1.2 Gateway health tests on an NCN 4.1.3 Gateway health tests from outside the system 4.2 Internal SSH access test execution 4.2.1 Known issues with internal SSH access test execution 4.3 External SSH access test execution 5. Booting CSM barebones image 5.1 Run the test script 6. UAS/UAI tests 6.1 Validate the basic UAS installation 6.2 Validate UAI creation 6.3 Test UAI gateway health 6.4 UAS/UAI troubleshooting 6.4.1 Authorization issues 6.4.2 UAS cannot access Keycloak 6.4.3 UAI images not in registry 6.4.4 Missing volumes and other container startup issues 0. Cray command line interface The first time these checks are performed during a CSM install, the Cray Command Line Interface (CLI) has not yet been configured. Some of the health check tests cannot be run without the Cray CLI being configured. Tests with this dependency are noted in their descriptions below. These tests may be skipped but this is not recommended.\nThe Cray CLI must be configured on all NCNs and the PIT node. The following procedures explain how to do this:\nConfigure Keycloak Account Configure the Cray Command Line Interface (CLI) 1. Platform health checks All platform health checks are expected to pass. Each check has been implemented as a Goss test which reports a PASS or FAIL.\nAvailable platform health checks:\nNCN health checks Known issues with NCN health checks OPTIONAL Check of ncnHealthChecks resources Known issues with NCN resource checks Check of system management monitoring tools 1.1 NCN health checks These checks require that the Cray CLI is configured on all worker NCNs.\nIf ncn-m001 is the PIT node, then run these checks on ncn-m001; otherwise run them from any master NCN.\nSpecify the admin user password for the management switches in the system.\nThis is required for the ncn-healthcheck tests.\nread -s is used to prevent the password from being written to the screen or the shell history.\nncn-m/pit# read -s SW_ADMIN_PASSWORD ncn-m/pit# export SW_ADMIN_PASSWORD Run the NCN health checks.\nncn-m/pit# /opt/cray/tests/install/ncn/automated/ncn-healthcheck | tee ncn-healthcheck.log The following command will extract the test totals for the various nodes:\nncn-m/pit# grep \u0026#34;Total Test\u0026#34; ncn-healthcheck.log Run the Kubernetes checks.\nncn-m/pit# /opt/cray/tests/install/ncn/automated/ncn-kubernetes-checks | tee ncn-kubernetes-checks.log The following command will extract the test totals for the various nodes:\nncn-m/pit# grep \u0026#34;Total Test\u0026#34; ncn-kubernetes-checks.log Review results.\nReview the output for Result: FAIL and follow the instructions provided to resolve any such test failures. With the exception of the Known Test Issues, all health checks are expected to pass.\n1.1.1 Known issues with NCN health checks It is possible that the first pass of running these tests may fail due to cloud-init not being completed on the storage nodes. In this case, please wait five minutes and re-run the tests.\nFor any failures related to SSL certificates, see the Platform CA Issues troubleshooting guide.\nKubernetes Query BSS Cloud-init for ca-certs\nThis test may fail immediately after platform install. It should pass after the TrustedCerts operator has updated BSS (Global cloud-init meta) with CA certificates. Kubernetes Velero No Failed Backups\nBecause of a known issue with Velero, a backup may be attempted immediately upon the deployment of a backup schedule (for example, Vault). It may be necessary to delete backups from a Kubernetes node to clear this situation. See the output of the test for more details on how to cleanup backups that have failed due to a known interruption. For example: Find the failed backup.\nncn-mw/pit# kubectl get backups -A -o json | jq -e \u0026#39;.items[] | select(.status.phase == \u0026#34;PartiallyFailed\u0026#34;) | .metadata.name\u0026#39; Delete the backup.\nIn the following command, replace \u0026lt;backup\u0026gt; with a backup returned in the previous step.\nThis command will not work on the PIT node.\nncn-mw# velero backup delete \u0026lt;backup\u0026gt; --confirm Verify spire-agent is enabled and running\nThe spire-agent service may fail to start on Kubernetes NCNs (all worker and master nodes). In this case, it may log errors (using journalctl) similar to join token does not exist or has already been used, or the last log entries may contain multiple instances of systemd[1]: spire-agent.service: Start request repeated too quickly.. Deleting the request-ncn-join-token daemonset pod running on the node may clear the issue. Even though the spire-agent systemctl service on the Kubernetes node should eventually restart cleanly, the user may have to log in to the impacted nodes and restart the service. The following recovery procedure can be run from any Kubernetes node in the cluster.\nDefine the following function\nncn-mw/pit# function renewncnjoin() { for pod in $(kubectl get pods -n spire |grep request-ncn-join-token | awk \u0026#39;{print $1}\u0026#39;); do if kubectl describe -n spire pods $pod | grep -q \u0026#34;Node:.*$1\u0026#34;; then echo \u0026#34;Restarting $pod running on $1\u0026#34; kubectl delete -n spire pod \u0026#34;$pod\u0026#34; fi done } Run the function as follows (substituting the name of the impacted NCN):\nncn-mw/pit# renewncnjoin ncn-xxxx The spire-agent service may also fail if an NCN was powered off for too long and its tokens expired. If this happens, then delete /root/spire/agent_svid.der, /root/spire/bundle.der, and /root/spire/data/svid.key off the NCN before deleting the request-ncn-join-token daemonset pod.\ncfs-state-reporter service ran successfully\nIf this test is failing, it could be due to SSL certificate issues on that NCN.\nRun the following command on the node where the test is failing.\nncn# systemctl status cfs-state-reporter | grep HTTPSConnectionPool If the previous command gives any output, this indicates possible SSL certificate problems on that NCN.\nSee the Platform CA Issues troubleshooting guide. If this test is failing on a storage node, it could be an issue with the node\u0026rsquo;s Spire token. The following procedure may resolve the problem:\nRun the following script on ncn-m002:\nncn-m002# /opt/cray/platform-utils/spire/fix-spire-on-storage.sh Then re-run the check to see if the problem has been resolved.\nClock skew test failures\nIt can take up to 15 minutes, and sometimes longer, for NCN clocks to synchronize after an upgrade or when a system is brought back up. If a clock skew test fails, wait 15 minutes and try again. To check status, run the following command, preferably on ncn-m001:\nncn-m001# chronyc sources -v 210 Number of sources = 9 .-- Source mode \u0026#39;^\u0026#39; = server, \u0026#39;=\u0026#39; = peer, \u0026#39;#\u0026#39; = local clock. / .- Source state \u0026#39;*\u0026#39; = current synced, \u0026#39;+\u0026#39; = combined , \u0026#39;-\u0026#39; = not combined, | / \u0026#39;?\u0026#39; = unreachable, \u0026#39;x\u0026#39; = time may be in error, \u0026#39;~\u0026#39; = time too variable. || .- xxxx [ yyyy ] +/- zzzz || Reachability register (octal) -. | xxxx = adjusted offset, || Log2(Polling interval) --. | | yyyy = measured offset, || \\ | | zzzz = estimated error. || | | \\ MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* ntp.hpecorp.net 2 10 377 650 -421us[ -571us] +/- 30ms =? ncn-m002.nmn 10 4 377 213 +82us[ +82us] +/- 367us =- ncn-m003.nmn 3 1 377 1 -2033us[-2033us] +/- 28ms =- ncn-s001.nmn 6 5 377 20 +53us[ +53us] +/- 193us =- ncn-s002.nmn 5 5 377 25 +29us[ +29us] +/- 275us =- ncn-s003.nmn 6 6 377 27 +47us[ +47us] +/- 237us =- ncn-w001.nmn 5 9 377 234m +8305us[ +10ms] +/- 38ms =- ncn-w002.nmn 3 5 377 8 -1910us[-1910us] +/- 27ms =- ncn-w003.nmn 3 8 377 74m -1122us[-1002us] +/- 31ms 1.2 NCN resource checks (optional) These optional checks display the NCN uptimes, the node resource consumptions, and/or the list of pods not in a running state. If ncn-m001 is the PIT node, then run these checks on ncn-m001; otherwise run them from any master NCN.\nncn-m/pit# /opt/cray/platform-utils/ncnHealthChecks.sh -s ncn_uptimes ncn-m/pit# /opt/cray/platform-utils/ncnHealthChecks.sh -s node_resource_consumption ncn-m/pit# /opt/cray/platform-utils/ncnHealthChecks.sh -s pods_not_running 1.2.1 Known issues with NCN resource checks pods_not_running If the output of pods_not_running indicates that there are pods in the Evicted state, it may be due to the root file system being filled up on the Kubernetes node in question. Kubernetes will begin evicting pods once the root file system space is at 85% full until it is back under 80%. This commonly happens on ncn-m001, because it is a location where install and documentation files may have been downloaded. It may be necessary to clean up space in the / directory if this is the root cause of pod evictions. Listing the top 10 files that are 1024M or larger is one way to start the analysis.\nncn-mw# df -h / Filesystem Size Used Avail Use% Mounted on LiveOS_rootfs 280G 245G 35G 88% / ncn-mw# du -h -s /root/ 225G /root/ ncn-mw# du -ah -B 1024M /root | sort -n -r | head -n 10 The cray-crus- pod is expected to be in the Init state until Slurm and MUNGE are installed. In particular, this will be the case if executing this as part of the validation after completing the Install CSM Services. If in doubt, validate the CRUS service using the CMS Validation Tool. If the CRUS check passes using that tool, do not worry about the cray-crus- pod state.\nThe hmn-discovery and cray-dns-unbound-manager cronjob pods may be in various transitional states such as Pending, Init, PodInitializing, NotReady, or Terminating. This is expected because these pods are periodically started and often can be caught in intermediate states.\nIf some *postgresql-db-backup cronjob pods are in Error state, they can be ignored if the most recent pod Completed. The Error pods are cleaned up over time but are left to troubleshoot issues in the case that all retries for the postgresql-db-backup job fail.\n1.3 Check of system management monitoring tools If all designated prerequisites are met, the availability of system management health services may optionally be validated by accessing the URLs listed in Access System Management Health Services. It is very important to check the Prerequisites section of this document.\nIf one or more of the the URLs listed in the procedure are inaccessible, it does not necessarily mean that system is not healthy. It may simply mean that not all of the prerequisites have been met to allow access to the system management health tools via URL.\nInformation to assist with troubleshooting some of the components mentioned in the prerequisites can be accessed here:\nTroubleshoot CMN Issues Troubleshoot DNS Configuration Issues Check BGP Status and Reset Sessions Troubleshoot BGP not Accepting Routes from MetalLB Troubleshoot Services without an Allocated IP Address Troubleshoot Prometheus Alerts 2. Hardware Management Services health checks The checks in this section require that the Cray CLI is configured on nodes where the checks are executed.\nExecute the HMS tests to confirm that the Hardware Management Services are running and operational.\nNote: Do not run HMS tests concurrently on multiple nodes. They may interfere with one another and cause false failures.\nHMS CT test execution Hardware State Manager discovery validation Interpreting HSM discovery results Known issues with HSM discovery validation 2.1 HMS CT test execution These tests may be executed on any one worker or master NCN (but not ncn-m001 if it is still the PIT node).\nRun the HMS CT tests. This is done by running the run_hms_ct_tests.sh script:\nncn# /opt/cray/csm/scripts/hms_verification/run_hms_ct_tests.sh The return value of the script is 0 if all CT tests ran successfully, non-zero if not. On CT test failures the script will instruct the admin to look at the CT test log files. If one or more failures occur, investigate the cause of each failure. See the Interpreting HMS Health Check Results documentation for more information.\n2.2 Hardware State Manager discovery validation By this point in the installation process, the Hardware State Manager (HSM) should have done its discovery of the system.\nThe foundational information for this discovery is from the System Layout Service (SLS). Thus, a comparison needs to be done to see that what is specified in SLS (focusing on BMC components and Redfish endpoints) are present in HSM.\nTo perform this comparison execute the verify_hsm_discovery.py script on a Kubernetes master or worker NCN. The result is pass/fail (returns 0 or non-zero):\nncn# /opt/cray/csm/scripts/hms_verification/verify_hsm_discovery.py The output will ideally appear as follows, if there are mismatches these will be displayed in the appropriate section of the output. Refer to 2.2.1 Interpreting results and 2.2.2 Known Issues below to troubleshoot any mismatched BMCs.\nHSM Cabinet Summary =================== x1000 (Mountain) Discovered Nodes: 50 Discovered Node BMCs: 25 Discovered Router BMCs: 32 Discovered Chassis BMCs: 8 x3000 (River) Discovered Nodes: 23 (12 Mgmt, 7 Application, 4 Compute) Discovered Node BMCs: 24 Discovered Router BMCs: 2 Discovered Cab PDU Ctlrs: 0 River Cabinet Checks ==================== x3000 Nodes: PASS NodeBMCs: PASS RouterBMCs: PASS ChassisBMCs: PASS CabinetPDUControllers: PASS Mountain/Hill Cabinet Checks ============================ x1000 (Mountain) ChassisBMCs: PASS Nodes: PASS NodeBMCs: PASS RouterBMCs: PASS The script will have an exit code of 0 if there are no failures. If there is any FAIL information displayed, the script will exit with a non-zero exit code. Failure information interpretation is described in the next section.\n2.2.1 Interpreting HSM discovery results The Cabinet Checks output is divided into three sections:\nSummary information for each cabinet Detail information for River cabinets Detail information for Mountain/Hill cabinets. In the River section, any hardware found in SLS and not discovered by HSM is considered a failure, with the exception of PDU controllers, which is a warning. Also, the BMC of one of the management NCNs (typically ncn-m001) will not be connected to the HSM HW network and thus will show up as being not discovered and/or not having any mgmt network connection. This is treated as a warning.\nIn the Mountain section, the only thing considered a failure are Chassis BMCs that are not discovered in HSM. All other items (nodes, node BMCs and router BMCs) which are not discovered are considered warnings.\nAny failures need to be investigated by the admin for rectification. Any warnings should also be examined by the administrator to ensure they are accurate and expected.\nFor each of the BMCs that show up as not being present in HSM components or Redfish Endpoints use the following notes to determine whether the issue with the BMC can be safely ignored or needs to be addressed before proceeding.\nThe node BMC of ncn-m001 will not typically be present in HSM component data, as it is typically connected to the site network instead of the HMN network.\nThe node BMCs for HPE Apollo XL645D nodes may report as a mismatch depending on the state of the system when the verify_hsm_discovery.py script is run. If the system is currently going through the process of installation, then this is an expected mismatch as the Prepare Compute Nodes procedure required to configure the BMC of the HPE Apollo 6500 XL645D node may not have been completed yet.\nFor more information refer to Configure HPE Apollo 6500 XL645D Gen10 Plus Compute Nodes for additional required configuration for this type of BMC.\nExample mismatch for the BMC of an HPE Apollo XL654D:\nNodes: FAIL - x3000c0s30b1n0 (Compute, NID 5) - Not found in HSM Components. NodeBMCs: FAIL - x3000c0s19b1 - Not found in HSM Components; Not found in HSM Redfish Endpoints. Chassis Management Controllers (CMC) may show up as not being present in HSM. CMCs for Intel node blades can be ignored. Gigabyte node blade CMCs not found in HSM is not normal and should be investigated. If a Gigabyte CMC is expected to not be connected to the HMN network, then it can be ignored. Otherwise, verify that the root service account is configured for the CMC and add it if needed by following the steps outlined in Add Root Service Account for Gigabyte Controllers.\nCMCs have component names (xnames) in the form of xXc0sSb999, where X is the cabinet and S is the rack U of the compute node chassis.\nExample mismatch for a CMC an Intel node blade:\nChassisBMCs/CMCs: FAIL - x3000c0s10b999 - Not found in HSM Components; Not found in HSM Redfish Endpoints; No mgmt port connection. Cabinet PDU Controllers have component names (xnames) in the form of xXmM, where X is the cabinet and M is the ordinal of the Cabinet PDU Controller.\nExample mismatch for a PDU:\nCabinetPDUControllers: WARNING - x3000m0 - Not found in HSM Components ; Not found in HSM Redfish Endpoints If the PDU is accessible over the network, the following can be used to determine the vendor of the PDU.\nncn-m001# PDU=x3000m0 ncn-m001# curl -k -s --compressed https://$PDU -i | grep Server: Example ServerTech output:\nServer: ServerTech-AWS/v8.0v Example HPE output:\nServer: HPE/1.4.0 ServerTech PDUs may need passwords changed from their defaults to become functional. See Change Credentials on ServerTech PDUs.\nHPE PDUs are supported and should show up as being found in HSM. If they are not, they should be investigated since that may indicate that configuration steps have not yet been executed which are required for the PDUs to be discovered. Refer to HPE PDU Admin Procedures for additional configuration for this type of PDU. The steps to run will depend on if the PDU has been set up yet, and whether or not an upgrade or fresh install of CSM is being performed.\nBMCs having no association with a management switch port will be annotated as such, and should be investigated. Exceptions to this are in Mountain or Hill configurations where Mountain BMCs will show this condition on SLS/HSM mismatches, which is normal.\nIn Hill configurations SLS assumes BMCs in chassis 1 and 3 are fully populated (32 Node BMCs), and in Mountain configurations SLS assumes all BMCs are fully populated (128 Node BMCs). Any non-populated BMCs will have no HSM data and will show up in the mismatch list.\nIf it was determined that the mismatch can not be ignored, then proceed onto the the 2.2.2 Known Issues below to troubleshoot any mismatched BMCs.\n2.2.2 Known issues with HSM discovery validation Known issues that may prevent hardware from getting discovered by Hardware State Manager:\nHMS Discovery job not creating Redfish Endpoints in Hardware State Manager 3 Software Management Services health checks SMS test execution Interpreting cmsdev Results Known issues with SMS tests 3.1 SMS test execution The test in this section requires that the Cray CLI is configured on nodes where the test is executed.\nThe following test can be run on any Kubernetes node (any master or worker node, but not the PIT node).\nncn# /usr/local/bin/cmsdev test -q all The cmsdev tool logs to /opt/cray/tests/cmsdev.log The -q (quiet) and -v (verbose) flags can be used to decrease or increase the amount of information sent to the screen. The same amount of data is written to the log file in either case. 3.2 Interpreting cmsdev results If all checks passed, then the following will be true: The return code will be zero. The final line of output will begin with SUCCESS. For example: SUCCESS: All 7 service tests passed: bos, cfs, conman, crus, ims, tftp, vcs If one or more checks failed, then the following will be true: The return code will be non-zero. The final line of output will begin with FAILURE and will list which checks failed. For example: FAILURE: 2 service tests FAILED (conman, ims), 5 passed (bos, cfs, crus, tftp, vcs) After remediating a test failure for a particular service, just that single service test can be re-run by replacing all in the cmsdev command line with the name of the service. For example: /usr/local/bin/cmsdev test -q cfs Additional test execution details can be found in /opt/cray/tests/cmsdev.log.\n3.3 Known issues with SMS tests persistentvolumeclaims not found If an Etcd restore has been performed on one of the SMS services (such as BOS or CRUS), then the first Etcd pod that comes up after the restore will not have a PVC (Persistent Volume Claim) attached to it (until the pod is restarted). The Etcd cluster is in a healthy state at this point, but the SMS health checks will detect the above condition and may report test failures similar to the following:\nERROR (run tag 1khv7-bos): persistentvolumeclaims \u0026#34;cray-bos-etcd-ncchqgnczg\u0026#34; not found ERROR (run tag 1khv7-crus): persistentvolumeclaims \u0026#34;cray-crus-etcd-ffmszl7bvh\u0026#34; not found In this case, these errors can be ignored, or the pod with the same name as the PVC mentioned in the output can be restarted (as long as the other two Etcd pods are healthy).\nBOS subtest hangs On systems where too many BOS sessions exist, the cmsdev test will hang when trying to list them. See Hang Listing BOS Sessions for more information.\n4. Gateway health and SSH access checks 4.1 Gateway health tests 4.1.1 Gateway health tests overview The gateway tests check the health of the API Gateway on all of the relevant networks. The gateway tests check that the gateway is accessible on all networks where it should be accessible, and NOT accessible on all networks where it should NOT be accessible. They also check several service endpoints to verify that they return the proper response on each accessible network.\nThe test will complete with an overall test status based on the result of the individual health checks on all of the networks.\nOverall Gateway Test Status: PASS For more detailed information on the tests results and examples, see Gateway Testing.\nThe gateway tests can be run from various locations. For this part of the CSM validation, check gateway access from the NCNs and from outside the system. Externally, the API gateway is accessible on the CMN and either the CAN or CHN, depending on the configuration of the system. On NCNs, the API gateway is accessible on the same networks (CMN and CAN/CHN) and it is also accessible on the NMNLB network.\n4.1.2 Gateway health tests on an NCN The gateway tests may be run on any NCN with the docs-csm RPM installed. For details on installing the docs-csm RPM, see Check for Latest Documentation.\nTo execute the tests, see Running Gateway Tests on an NCN Management Node.\n4.1.3 Gateway health tests from outside the system To execute the tests, see Running Gateway Tests on a Device Outside the System.\n4.2 Internal SSH access test execution The internal SSH access tests may be run on any NCN with the docs-csm RPM installed. For details on installing the docs-csm RPM, see Check for Latest Documentation.\nExecute the tests by running the following command:\nncn# /usr/share/doc/csm/scripts/operations/pyscripts/start.py test_bican_internal By default, SSH access will be tested on all relevant networks between master nodes and spine switches. It is possible to customize which nodes and networks will be tested. For example, it is possible to include UANs, to exclude master nodes, or to exclude the HMN. See the test usage statement for details. The test usage statement is displayed by calling the test with the --help argument:\nncn# /usr/share/doc/csm/scripts/operations/pyscripts/start.py test_bican_internal --help The test will complete with an overall pass/failure status such as the following:\nOverall status: PASSED (Passed: 40, Failed: 0) 4.2.1 Known issues with internal SSH access test execution It is possible this test will fail if the procedure to deploy the final NCN has not been performed.\nBefore running this procedure, the static IP address reservation data has not yet been loaded into the Hardware State Manager (HSM), so DNS records may be missing.\nAfter deploying the final NCN, this test may fail with an UnresolvedHostname error or a CannotLoginException.\nTo work around this issue, perform the following procedure:\nInspect the cray-powerdns-manager pod log for Failed to patch RRsets errors.\nncn-mw# kubectl -n services logs -l app.kubernetes.io/name=cray-powerdns-manager -c cray-powerdns-manager Example output:\n{\u0026#34;level\u0026#34;:\u0026#34;error\u0026#34;,\u0026#34;ts\u0026#34;:1644510069.0068583,\u0026#34;msg\u0026#34;:\u0026#34;Failed to patch RRsets!\u0026#34;, \u0026#34;zone\u0026#34;:\u0026#34;nmn.hela.dev.cray.com.\u0026#34;, \u0026#34;error\u0026#34;:\u0026#34;RRset x3000c0s6b0n0.nmn.hela.dev.cray.com. IN CNAME: Conflicts with pre-existing RRset\u0026#34;, \u0026#34;zone\u0026#34;:\u0026#34;nmn.hela.dev.cray.com.\u0026#34;} Identify the cray-dns-powerdns pod.\nncn-mw# kubectl -n services get pod -l app.kubernetes.io/name=cray-dns-powerdns Example output:\nNAME READY STATUS RESTARTS AGE cray-dns-powerdns-86c9685d78-bxz2z 2/2 Running 0 13d Delete the zone reported in the cray-powerdns-manager log output.\nIn the following example command, be sure to replace nmn.hela.dev.cray.com with the actual zone identified in the earlier step.\nncn-mw# kubectl -n services exec -it cray-dns-powerdns-86c9685d78-bxz2z \\ -c cray-dns-powerdns -- pdnsutil delete-zone nmn.hela.dev.cray.com The cray-powerdns-manager reconciliation loop runs every 30 seconds, and the next run will recreate the zone with the correct records.\n4.3 External SSH access test execution The external SSH access tests may be run on any system external to the cluster. The tests should not be run from another system running the Cray System Management software if that system was configured with the same internal network ranges as the system being tested as this will cause some tests to fail.\nPython version 3 must be installed (if it is not already).\nObtain the test code.\nThere are two options for doing this:\nInstall the docs-csm RPM.\nSee Check for Latest Documentation.\nCopy over the following folder from a system where the docs-csm RPM is installed:\n/usr/share/doc/csm/scripts/operations/pyscripts Install the Python dependencies\nRun the following command from the pyscripts directory in order to install the required Python dependencies:\nexternal:/usr/share/doc/csm/scripts/operations/pyscripts# pip install . Obtain the admin client secret.\nBecause kubectl will not work outside of the cluster, obtain the admin client secret by running the following command on an NCN.\nncn# kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d Example output:\n26947343-d4ab-403b-14e937dbd700 On the external system, execute the tests.\nexternal:/usr/share/doc/csm/scripts/operations/pyscripts# ./start.py test_bican_external By default, SSH access will be tested on all relevant networks between master nodes and spine switches. It is possible to customize which nodes and networks will be tested. For example, it is possible to include compute nodes, to exclude spine switches, or to exclude the NMN. See the test usage statement for details. The test usage statement is displayed by calling the test with the --help argument:\nexternal:/usr/share/doc/csm/scripts/operations/pyscripts# ./start.py test_bican_external --help When prompted by the test, enter the system domain and the admin client secret.\nThe test will complete with an overall pass/failure status such as the following:\nOverall status: PASSED (Passed: 20, Failed: 0) 5. Booting CSM barebones image Included with the Cray System Management (CSM) release is a pre-built node image that can be used to validate that core CSM services are available and responding as expected. The CSM Barebones image contains only the minimal set of RPMs and configuration required to boot an image and is not suitable for production usage. To run production work loads, it is suggested that an image from the Cray OS (COS) product, or similar, be used.\nThis test is very important to run, particularly during the CSM install prior to rebooting the PIT node, because it validates all of the services required for nodes to PXE boot from the cluster. The CSM Barebones image included with the release will not successfully complete beyond the dracut stage of the boot process. However, if the dracut stage is reached, the boot can be considered successful and shows that the necessary CSM services needed to boot a node are up and available. This inability to boot the Barebones image fully will be resolved in future releases of the CSM product. In addition to the CSM Barebones image, the release also includes an IMS Recipe that can be used to build the CSM Barebones image. However, the CSM Barebones recipe currently requires RPMs that are not installed with the CSM product. The CSM Barebones recipe can be built after the Cray OS (COS) product stream is also installed on to the system. In future releases of the CSM product, work will be undertaken to resolve these dependency issues. This test can be run on any NCN, but not the PIT node. This script uses the Kubernetes API Gateway to access CSM services. This gateway must be properly configured to allow an access token to be generated by the script. This script is installed as part of the cray-cmstools-crayctldeploy RPM. For additional information on the script and for troubleshooting help look at the document Barebones Image Boot. 5.1 Run the test script The script is executable and can be run without any arguments. It returns zero on success and non-zero on failure.\nncn# /opt/cray/tests/integration/csm/barebonesImageTest A successful run would generate output like the following:\ncray.barebones-boot-test: INFO Barebones image boot test starting cray.barebones-boot-test: INFO For complete logs look in the file /tmp/cray.barebones-boot-test.log cray.barebones-boot-test: INFO Creating bos session with template:csm-barebones-image-test, on node:x3000c0s10b1n0 cray.barebones-boot-test: INFO Starting boot on compute node: x3000c0s10b1n0 cray.barebones-boot-test: INFO Found dracut message in console output - success!!! cray.barebones-boot-test: INFO Successfully completed barebones image boot test. The script will choose an enabled compute node that is listed in the Hardware State Manager (HSM) for the test, unless the user passes in a specific node using the --xname argument. If a compute node is specified but unavailable, an available node will be used instead and a warning will be logged.\nncn# /opt/cray/tests/integration/csm/barebonesImageTest --xname x3000c0s10b4n0 6. UAS/UAI tests The commands in this section require that the Cray CLI is configured on nodes where the commands are being executed.\nThe procedures below use the CLI as an authorized user and run on two separate node types. The first part runs on the LiveCD node, while the second part runs on a non-LiveCD Kubernetes master or worker node. In either case, the CLI configuration needs to be initialized on the node and the user running the procedure needs to be authorized.\nThe following procedures run on separate nodes of the system.\nValidate the basic UAS installation Validate UAI creation Test UAI gateway health UAS/UAI troubleshooting Authorization issues UAS cannot access Keycloak UAI images not in registry Missing volumes and other container startup issues 6.1 Validate the basic UAS installation This section can be run on any NCN or the PIT node.\nShow information about cray-uas-mgr.\nncn# cray uas mgr-info list --format toml Expected output looks similar to the following:\nservice_name = \u0026#34;cray-uas-mgr\u0026#34; version = \u0026#34;1.11.5\u0026#34; In this example output, it shows that UAS is installed and running the 1.11.5 version.\nList UAIs on the system.\nncn# cray uas list --format toml Expected output looks similar to the following:\nresults = [] This example output shows that there are no currently running UAIs. It is possible, if someone else has been using the UAS, that there could be UAIs in the list. That is acceptable too from a validation standpoint.\nVerify that the pre-made UAI images are registered with UAS\nncn# cray uas images list --format toml Expected output looks similar to the following:\ndefault_image = \u0026#34;artifactory.algol60.net/csm-docker/stable/cray-uai-sles15sp3:1.6.0\u0026#34; image_list = [ \u0026#34;artifactory.algol60.net/csm-docker/stable/cray-uai-sles15sp3:1.6.0\u0026#34;, \u0026#34;artifactory.algol60.net/csm-docker/stable/cray-uai-gateway-test:1.6.0\u0026#34;, \u0026#34;artifactory.algol60.net/csm-docker/stable/cray-uai-broker:1.6.0\u0026#34;,] This example output shows that the pre-made end-user UAI images (artifactory.algol60.net/csm-docker/stable/cray-uai-sles15sp3:1.6.0, artifactory.algol60.net/csm-docker/stable/cray-uai-gateway-test:1.6.0, and artifactory.algol60.net/csm-docker/stable/cray-uai-broker:1.6.0) are registered with UAS. This does not necessarily mean these images are installed in the container image registry, but they are configured for use. If other UAI images have been created and registered, they may also show up here, which is acceptable.\n6.2 Validate UAI creation IMPORTANT: If the site does not use UAIs, skip UAS and UAI validation. If UAIs are used, there are products that configure UAS like Cray Analytics and Cray Programming Environment that must be working correctly with UAIs, and should be validated (the procedures for this are beyond the scope of this document) prior to validating UAS and UAI. Failures in UAI creation that result from incorrect or incomplete installation of these products will generally take the form of UAIs stuck in waiting state trying to set up volume mounts. See the UAI Troubleshooting section for more information.\nThis procedure must run on a master or worker node (not the PIT node).\nVerify that a UAI can be created:\nncn# cray uas create --publickey ~/.ssh/id_rsa.pub --format toml Expected output looks similar to the following:\nuai_connect_string = \u0026#34;ssh vers@10.16.234.10\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp3:1.0.11\u0026#34; uai_ip = \u0026#34;10.16.234.10\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-a00fb46b\u0026#34; uai_status = \u0026#34;Pending\u0026#34; username = \u0026#34;vers\u0026#34; [uai_portmap] This has created the UAI and the UAI is currently in the process of initializing and running. The uai_status in the output from this command may instead be Waiting, which is also acceptable.\nSet UAINAME to the value of the uai_name field in the previous command output (uai-vers-a00fb46b in our example):\nncn# UAINAME=uai-vers-a00fb46b Check the current status of the UAI:\nncn# cray uas list --format toml Expected output looks similar to the following:\n[[results]] uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.16.234.10\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp3:1.0.11\u0026#34; uai_ip = \u0026#34;10.16.234.10\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-a00fb46b\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; If the uai_status field is Running: Ready, proceed to the next step. Otherwise, wait and repeat this command until that is the case. It normally should not take more than a minute or two.\nThe UAI is ready for use. Log into it with the command in the uai_connect_string field in the previous command output:\nncn# ssh vers@10.16.234.10 vers@uai-vers-a00fb46b-6889b666db-4dfvn:~\u0026gt; Run a command on the UAI:\nvers@uai-vers-a00fb46b-6889b666db-4dfvn:~\u0026gt; ps -afe Expected output looks similar to the following:\nUID PID PPID C STIME TTY TIME CMD root 1 0 0 18:51 ? 00:00:00 /bin/bash /usr/bin/uai-ssh.sh munge 36 1 0 18:51 ? 00:00:00 /usr/sbin/munged root 54 1 0 18:51 ? 00:00:00 su vers -c /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 55 54 0 18:51 ? 00:00:00 /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 62 55 0 18:51 ? 00:00:00 sshd: vers [priv] vers 67 62 0 18:51 ? 00:00:00 sshd: vers@pts/0 vers 68 67 0 18:51 pts/0 00:00:00 -bash vers 120 68 0 18:52 pts/0 00:00:00 ps -afe Log out from the UAI\nvers@uai-vers-a00fb46b-6889b666db-4dfvn:~\u0026gt; exit ncn# Clean up the UAI.\nncn# cray uas delete --uai-list $UAINAME --format toml Expected output looks similar to the following:\nresults = [ \u0026#34;Successfully deleted uai-vers-a00fb46b\u0026#34;,] If the commands ran with similar results, then the basic functionality of the UAS and UAI is working.\n6.3 Test UAI gateway health Like the NCN gateway health check, the gateway tests check the health of the API Gateway on all of the relevant networks. On UAIs, the API gateway should only be accessible on the user network (either CAN or CHN depending on the configuration of the system). The gateway tests check that the gateway is accessible on all networks where it should be accessible, and NOT accessible on all networks where it should NOT be accessible. They also check several service endpoints to verify that they return the proper response on each accessible network.\n6.3.1 Gateway test execution The UAI gateway tests may be run on any NCN with the docs-csm RPM installed. For details on installing the docs-csm RPM, see Check for Latest Documentation.\nThe UAI gateway tests are executed by running the following command.\nncn# /usr/share/doc/csm/scripts/operations/gateway-test/uai-gateway-test.sh The test will launch a UAI with the gateway-test image, execute the gateway tests, and then delete the UAI that was launched. The test will complete with an overall test status based on the result of the individual health checks on all of the networks.\nOverall Gateway Test Status: PASS For more detailed information on the tests results and examples, see Gateway Testing.\n6.4 UAS/UAI troubleshooting The following subsections include common failure modes seen with UAS / UAI operations and how to resolve them.\n6.4.1 Authorization issues An error will be returned when running CLI commands if the user is not logged in as a valid Keycloak user or is accidentally using the CRAY_CREDENTIALS environment variable. This variable is set regardless of the user credentials being used.\nFor example:\nncn# cray uas list The symptom of this problem is output similar to the following:\nUsage: cray uas list [OPTIONS] Try \u0026#39;cray uas list --help\u0026#39; for help. Error: Bad Request: Token not valid for UAS. Attributes missing: [\u0026#39;gidNumber\u0026#39;, \u0026#39;loginShell\u0026#39;, \u0026#39;homeDirectory\u0026#39;, \u0026#39;uidNumber\u0026#39;, \u0026#39;name\u0026#39;] Fix this by logging in as a real user (someone with actual Linux credentials) and making sure that CRAY_CREDENTIALS is unset.\n6.4.2 UAS cannot access Keycloak When running CLI commands, a Keycloak error may be returned.\nFor example:\nncn# cray uas list The symptom of this problem is output similar to the following:\nUsage: cray uas list [OPTIONS] Try \u0026#39;cray uas list --help\u0026#39; for help. Error: Internal Server Error: An error was encountered while accessing Keycloak If the wrong hostname was used to reach the API gateway, re-run the CLI initialization steps above and try again to check that. There may also be a problem with the Istio service mesh inside of the system. Troubleshooting this is beyond the scope of this section, but there may be useful information in the UAS pod logs in Kubernetes. There are generally two UAS pods, so the user may need to look at logs from both to find the specific failure. The logs tend to have a very large number of GET events listed as part of the liveness checking.\nThe following shows an example of looking at UAS logs effectively (this example shows only one UAS manager, normally there would be two):\nDetermine the pod name of the uas-mgr pod\nncn-mw# kubectl get po -n services | grep \u0026#34;^cray-uas-mgr\u0026#34; | grep -v etcd Expected output looks similar to:\ncray-uas-mgr-6bbd584ccb-zg8vx 2/2 Running 0 12d Set PODNAME to the name of the manager pod whose logs are going to be viewed.\nncn-mw# export PODNAME=cray-uas-mgr-6bbd584ccb-zg8vx View the last 25 log entries of the cray-uas-mgr container in that pod, excluding GET events:\nncn-mw# kubectl logs -n services $PODNAME cray-uas-mgr | grep -v \u0026#39;GET \u0026#39; | tail -25 Example output:\n2021-02-08 15:32:41,211 - uas_mgr - INFO - getting deployment uai-vers-87a0ff6e in namespace user 2021-02-08 15:32:41,225 - uas_mgr - INFO - creating deployment uai-vers-87a0ff6e in namespace user 2021-02-08 15:32:41,241 - uas_mgr - INFO - creating the UAI service uai-vers-87a0ff6e-ssh 2021-02-08 15:32:41,241 - uas_mgr - INFO - getting service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:32:41,252 - uas_mgr - INFO - creating service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:32:41,267 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:32:41,360 - uas_mgr - INFO - No start time provided from pod 2021-02-08 15:32:41,361 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 127.0.0.1 - - [08/Feb/2021 15:32:41] \u0026#34;POST /v1/uas?imagename=registry.local%2Fcray%2Fno-image-registered%3A1.0.11 HTTP/1.1\u0026#34; 200 - 2021-02-08 15:32:54,455 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:32:54,455 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:32:54,455 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:32:54,484 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:32:54,596 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:25,053 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:40:25,054 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:40:25,054 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:40:25,085 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:40:25,212 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:51,210 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:40:51,210 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:40:51,210 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:40:51,261 - uas_mgr - INFO - deleting service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:51,291 - uas_mgr - INFO - delete deployment uai-vers-87a0ff6e in namespace user 127.0.0.1 - - [08/Feb/2021 15:40:51] \u0026#34;DELETE /v1/uas?uai_list=uai-vers-87a0ff6e HTTP/1.1\u0026#34; 200 - 6.4.3 UAI images not in registry When listing or describing a UAI, an error in the uai_msg field may be returned. For example:\nncn# cray uas list --format toml There may be something similar to the following output:\n[[results]] uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.103.13.172\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp3:1.0.11\u0026#34; uai_ip = \u0026#34;10.103.13.172\u0026#34; uai_msg = \u0026#34;ErrImagePull\u0026#34; uai_name = \u0026#34;uai-vers-87a0ff6e\u0026#34; uai_status = \u0026#34;Waiting\u0026#34; username = \u0026#34;vers\u0026#34; This means the pre-made end-user UAI image is not in the local registry (or whatever registry it is being pulled from; see the uai_img value for details). To correct this, locate and push/import the image to the registry.\n6.4.4 Missing volumes and other container startup issues Various packages install volumes in the UAS configuration. All of those volumes must also have the underlying resources available, sometimes on the host node where the UAI is running sometimes from with Kubernetes. If a UAI gets stuck with a ContainerCreating uai_msg field for an extended time, this is a likely cause. UAIs run in the user Kubernetes namespace, and are pods that can be examined using kubectl describe.\nLocate the pod.\nncn-mw# kubectl get po -n user | grep \u0026lt;uai-name\u0026gt; Investigate the problem using the pod name from the previous step.\nncn-mw# kubectl describe pod -n user \u0026lt;pod-name\u0026gt; If volumes are missing they will show up in the Events: section of the output. Other problems may show up there as well. The names of the missing volumes or other issues should indicate what needs to be fixed to make the UAI run.\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/manage_sealed_secrets/",
	"title": "Manage Sealed Secrets",
	"tags": [],
	"description": "",
	"content": "Manage Sealed Secrets Sealed secrets are essential for managing sensitive information on the system. The following procedures for managing sealed secrets are included in this section:\nGenerate Sealed Secrets Post-Install Prevent Regeneration of Tracked Sealed Secrets View Tracked Sealed Secrets Decrypt Sealed Secrets for Review Fix an Incorrect Value in a Sealed Secret In the following sections, the term \u0026ldquo;tracked sealed secrets\u0026rdquo; is used to describe any existing secrets stored in spec.kubernetes.tracked_sealed_secrets that are available to be regenerated.\nMany of the examples in this section assume that the system was installed using a USB PIT node, and that the USB stick used to install the system is still available. If site-init is no longer available on the USB stick and a backup has not been made, a new site-init will need to be created following step 1 in the Generate Sealed Secrets Post-Install section.\nThe customizations.yaml file used in this procedure will be in one of the following locations depending on the state of the system:\nFresh install location: /mnt/pitdata/${CSM_DISTDIR}/shasta-cfg/customizations.yaml Post-install location: /root/site-init/${CSM_DISTDIR}/shasta-cfg/customizations.yaml Generate Sealed Secrets Post-Install Sealed secrets are stored in customizations.yaml as SealedSecret resources (encrypted secrets), which are deployed by specific charts and decrypted by the sealed secrets operator. First, those secrets must be seeded, generated, and encrypted.\nThe steps in this section assume that the system was not installed using a USB PIT node, or that the USB stick is no longer available.\nIf LDAP user federation is required, then refer to Add LDAP User Federation.\nPrepare to customize the customizations.yaml file.\nIf the customizations.yaml file is managed in an external Git repository (as recommended), then clone a local working tree. Replace the \u0026lt;URL\u0026gt; value in the following command before running it.\nncn# git clone \u0026lt;URL\u0026gt; /root/site-init ncn# cd /root/site-init If there is not a backup of site-init, perform the following steps to create a new one using the values stored in the Kubernetes cluster.\nCreate a new site-init directory using the CSM tarball.\nDetermine the location of the initial unpacked install tarball and set ${CSM_DISTDIR} accordingly.\nNOTE: If the unpacked set of CSM directories was copied, no action is required to expand the tarball. If the tarball tgz file was copied, the command to unpack it is tar -zxvf CSM_RELEASE.tar.gz. Replace the CSM_RELEASE value before running the command to unpack the tarball.\nncn# cp -r ${CSM_DISTDIR}/shasta-cfg/* /root/site-init ncn# cd /root/site-init Extract customizations.yaml from the site-init secret.\nncn# kubectl -n loftsman get secret site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d - \u0026gt; customizations.yaml Extract the certificate and key used to create the sealed secrets.\nncn# mkdir -p certs ncn# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39; | base64 -d - \u0026gt; certs/sealed_secrets.crt ncn# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.key}\u0026#39; | base64 -d - \u0026gt; certs/sealed_secrets.key (Optional) Prevent tracked sealed secrets from being regenerated.\nRemove the sealed secrets not being regenerated from the spec.kubernetes.tracked_sealed_secrets list in /root/site-init/${CSM_DISTDIR}/shasta-cfg/customizations.yaml prior to executing the remaining steps in this section.\nRetain the REDS/MEDS/RTS credentials.\nncn# yq delete -i customizations.yaml spec.kubernetes.tracked_sealed_secrets.cray_reds_credentials ncn# yq delete -i customizations.yaml spec.kubernetes.tracked_sealed_secrets.cray_meds_credentials ncn# yq delete -i customizations.yaml spec.kubernetes.tracked_sealed_secrets.cray_hms_rts_credentials Prepare to generate sealed secrets.\nncn# ./utils/secrets-reencrypt.sh customizations.yaml ./certs/sealed_secrets.key ./certs/sealed_secrets.crt Encrypt the static values in the customizations.yaml file after making changes.\nThe following command must be run within the site-init directory.\nncn# ./utils/secrets-seed-customizations.sh customizations.yaml Expected output looks similar to:\nCreating Sealed Secret keycloak-certs Generating type static_b64... Creating Sealed Secret keycloak-master-admin-auth Generating type static... Generating type static... Generating type randstr... Generating type static... Creating Sealed Secret cray_reds_credentials Generating type static... Generating type static... Creating Sealed Secret cray_meds_credentials Generating type static... Creating Sealed Secret cray_hms_rts_credentials Generating type static... Generating type static... Creating Sealed Secret vcs-user-credentials Generating type randstr... Generating type static... Creating Sealed Secret generated-platform-ca-1 Generating type platform_ca... Creating Sealed Secret pals-config Generating type zmq_curve... Generating type zmq_curve... Creating Sealed Secret munge-secret Generating type randstr... Creating Sealed Secret slurmdb-secret Generating type static... Generating type static... Generating type randstr... Generating type randstr... Creating Sealed Secret keycloak-users-localize Generating type static... Prevent Regeneration of Tracked Sealed Secrets Before performing the task to generate or regenerate sealed secrets, administrators are able to prevent existing tracked sealed secrets from being regenerated.\nTo prevent regeneration, sealed secrets MUST BE REMOVED from the spec.kubernetes.tracked_sealed_secrets list in the customizations.yaml file prior to executing the \u0026ldquo;Generate Sealed Secrets\u0026rdquo; section of the Prepare Site Init procedure.\nTo retain the REDS/MEDS/RTS credentials:\nncn# yq delete -i /mnt/pitdata/${CSM_DISTDIR}/shasta-cfg/customizations.yaml spec.kubernetes.tracked_sealed_secrets.cray_reds_credentials ncn# yq delete -i /mnt/pitdata/${CSM_DISTDIR}/shasta-cfg/customizations.yaml spec.kubernetes.tracked_sealed_secrets.cray_meds_credentials ncn# yq delete -i /mnt/pitdata/${CSM_DISTDIR}/shasta-cfg/customizations.yaml spec.kubernetes.tracked_sealed_secrets.cray_hms_rts_credentials View Tracked Sealed Secrets Tracked sealed secrets are regenerated every time secrets are seeded (see the use of utils/secrets-seed-customizations.sh above).\nTo view currently tracked sealed secrets:\nncn# yq read /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/customizations.yaml spec.kubernetes.tracked_sealed_secrets Expected output looks similar to the following:\n- cray_reds_credentials - cray_meds_credentials - cray_hms_rts_credentials Decrypt Sealed Secrets for Review Use the secrets-decrypt.sh utility in the SHASTA-CFG to decrypt and review previously encrypted sealed secrets.\nSyntax: secret-decrypt.sh SEALED-SECRET-NAME SEALED-SECRET-PRIVATE-KEY CUSTOMIZATIONS-YAML\nFor example:\nncn# cd /mnt/pitdata/prep/site-init \u0026amp;\u0026amp; ./utils/secrets-decrypt.sh cray_meds_credentials ./certs/sealed_secrets.key ./customizations.yaml | \\ jq .data.vault_redfish_defaults | sed -e \u0026#39;s/\u0026#34;//g\u0026#39; | base64 -d; echo Expected output looks similar to the following:\n{\u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;...\u0026#34;} Fix an Incorrect Value in a Sealed Secret This procedure describes how to correct an invalid password in the customizations.yaml file during an install. In the following example, a typo was made in the SNMPAuthPassword field of the vault_switch_defaults credentials in the cray_reds_credentials secret, resulting in hardware not being discovered.\nThe general process outlined in the following steps can be followed if a different value is incorrect.\nncn# ./utils/secrets-decrypt.sh cray_reds_credentials | jq -r \u0026#39;.data.vault_switch_defaults\u0026#39; | base64 --decode Output looks similar to the following:\n{\u0026#34;SNMPUsername\u0026#34;: \u0026#34;\u0026lt;USERID\u0026gt;\u0026#34;, \u0026#34;SNMPAuthPassword\u0026#34;: \u0026#34;\u0026lt;A-PASS\u0026gt;\u0026#34;, \u0026#34;SNMPPrivPassword\u0026#34;: \u0026#34;\u0026lt;P-PASS\u0026gt;\u0026#34;} Decrypt the cray_reds_credentials secret.\nncn# ./utils/secrets-decrypt.sh cray_reds_credentials \u0026gt; cray_reds_credentials.json The output file should look similar to the following. Note that the data values are base64 encoded.\n{ \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cray-reds-credentials\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null, \u0026#34;annotations\u0026#34;: { \u0026#34;sealedsecrets.bitnami.com/cluster-wide\u0026#34;: \u0026#34;true\u0026#34; }, \u0026#34;ownerReferences\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;bitnami.com/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;SealedSecret\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cray-reds-credentials\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;controller\u0026#34;: true } ] }, \u0026#34;data\u0026#34;: { \u0026#34;vault_redfish_defaults\u0026#34;: \u0026#34;eyJDcmF5IjogeyJVc2VybmFtZSI6ICJyb290IiwgIlBhc3N3b3JkIjogImluaXRpYWwwIn19\u0026#34;, \u0026#34;vault_switch_defaults\u0026#34;: \u0026#34;eyJTTk1QVXNlcm5hbWUiOiAidGVzdHVzZXIiLCAiU05NUEF1dGhQYXNzd29yZCI6ICJ0ZXN0cGFzMSIsICJTTk1QUHJpdlBhc3N3b3JkIjogInRlc3RwYXNzMiJ9\u0026#34; } } Decode the vault_switch_defaults credentials to a working file.\nncn# jq -r \u0026#39;.data.vault_switch_defaults\u0026#39; cray_reds_credentials.json | base64 --decode \u0026gt; vault_switch_defaults.json Correct the password in the vault_switch_defaults.json file.\n{\u0026#34;SNMPUsername\u0026#34;: \u0026#34;\u0026lt;USERID\u0026gt;\u0026#34;, \u0026#34;SNMPAuthPassword\u0026#34;: \u0026#34;\u0026lt;A-PASS\u0026gt;\u0026#34;, \u0026#34;SNMPPrivPassword\u0026#34;: \u0026#34;\u0026lt;P-PASS\u0026gt;\u0026#34;} Update cray_reds_credentials.json with an encoded version of the new password.\nncn# cat \u0026lt;\u0026lt;\u0026lt; $(jq \u0026#34;.data.vault_switch_defaults=\\\u0026#34;$(base64 --wrap=0 vault_switch_defaults.json)\\\u0026#34;\u0026#34; cray_reds_credentials.json) \u0026gt; cray_reds_credentials.json Verify that cray_reds_credentials.json has been updated with the new password.\nncn# jq -r \u0026#39;.data.vault_switch_defaults\u0026#39; cray_reds_credentials.json | base64 --decode Example output:\n{\u0026#34;SNMPUsername\u0026#34;: \u0026#34;\u0026lt;USERID\u0026gt;\u0026#34;, \u0026#34;SNMPAuthPassword\u0026#34;: \u0026#34;\u0026lt;A-PASS\u0026gt;\u0026#34;, \u0026#34;SNMPPrivPassword\u0026#34;: \u0026#34;\u0026lt;P-PASS\u0026gt;\u0026#34;} Replace the cray_reds_credentials secret in customizations.yaml with one containing the new credentials.\nncn# cat cray_reds_credentials.json | ./utils/secrets-encrypt.sh | yq w -f - -i customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_reds_credentials\u0026#39; Verify that customizations.yaml contains the updated password.\nncn# ./utils/secrets-decrypt.sh cray_reds_credentials | jq -r \u0026#39;.data.vault_switch_defaults\u0026#39; | base64 --decode Example output:\n{\u0026#34;SNMPUsername\u0026#34;: \u0026#34;\u0026lt;USERID\u0026gt;\u0026#34;, \u0026#34;SNMPAuthPassword\u0026#34;: \u0026#34;\u0026lt;A-PASS\u0026gt;\u0026#34;, \u0026#34;SNMPPrivPassword\u0026#34;: \u0026#34;\u0026lt;P-PASS\u0026gt;\u0026#34;} "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/move_a_standard_rack_node/",
	"title": "Move a Standard Rack Node",
	"tags": [],
	"description": "",
	"content": "Move a Standard Rack Node Update the location-based component name (xname) for a standard rack node within the system.\nPrerequisites An authentication token has been retrieved.\nncn# function get_token () { curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39; } The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI.\nProcedure Set variables for the new and old component name (xname) locations.\nThe NEWPORT variable is the component name (xname) of the port that the node BMC will be connected to after it is moved. The xname is typically something similar to x3003c0w41j42. The OLDENDPOINT variable is the xname of the BMC at its old location (for example, x3006c0r41b0).\nncn# NEWPORT=x3003c0w41j42 ncn# OLDENDPOINT=x3006c0r41b0 Generate and upload new management switch port information to the System Layout Service (SLS) and save it to a file.\nThis step may be skipped if this is a direct swap of nodes, where both the source and destination are already populated.\nQuery SLS to generate content for the new file.\nQuery the old port in SLS and replace the old component name (xname) (x3000c0w31j31 in this example) with the name of the current location of the hardware in the system.\nncn# cray sls hardware describe x3000c0w31j31 --format json Example output:\n{ \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w31\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w31j31\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s24b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;ethernet1/1/31\u0026#34; } } Create the new file with the updated location of the node.\nThe following is an example file. The Parent, Xname, NodeNics, and VendorName properties must be adjusted to match the new location of the node. The VendorName property may be obtained by logging into the switch that the node will be connected to.\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3003c0w41\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3003c0w41j42\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3004c0r42b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;ethernet1/1/42\u0026#34; } } Upload the updated node settings captured in the new JSON file.\nReplace the CUSTOM_FILE value in the following command with the name of the file created in the previous step.\nncn# curl -i -X PUT -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; https://api-gw-service-nmn.local/apis/sls/v1/hardware/$NEWPORT -d @CUSTOM_FILE Delete the existing redfishEndpoint and ethernetInterfaces from the Hardware State Manager (HSM).\nDelete the redfishEndpoint.\nncn# cray hsm inventory redfishEndpoints delete $OLDENDPOINT -- format toml Expected output resembles the following:\nmessage = \u0026#34;deleted 1 entry\u0026#34; code = 0 Delete the ethernetInterfaces.\nncn# for ID in $(cray hsm inventory ethernetInterfaces list --format json | jq -r \u0026#34;.[] | select(.ComponentID==\\\u0026#34;$OLDENDPOINT\\\u0026#34;).ID\u0026#34;); do cray hsm inventory ethernetInterfaces delete $ID --format toml; done Expected output resembles the following (once for each associated interface that is deleted).\nmessage = \u0026#34;deleted 1 entry\u0026#34; code = 0 "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/native_vlan/",
	"title": "Native VLAN",
	"tags": [],
	"description": "",
	"content": "Native VLAN Untagged ingress packets are destined to the native VLAN. An interface can be configured in one of 2 native modes - Native-Untagged or Native-Tagged.\nA native-untagged port accepts any untagged or tagged (with native VLAN ID) traffic on ingress.\nPackets that egress on a native-untagged port in the native VLAN will not have an 802.1Q header. A native-tagged port accepts only tagged traffic (with native VLAN ID) on ingress.\nAny untagged packet ingress on a native-tagged port is always dropped. Packets that egress on a native-tagged port in the native VLAN will always have an 802.1Q header.\nRelevant Configuration\nConfigure a VLAN as native\n[standalone: master] \u0026gt; enable [standalone: master] # configure terminal [standalone: master] (config) # vlan 3 [standalone: master] (config vlan 3) exit [standalone: master] (config) # interface ethernet 1/1 [standalone: master] (config interface ethernet 1/1) # switchport mode hybrid [standalone: master] (config interface ethernet 1/1) # switchport access vlan 3 Show Commands to Validate Functionality\nswitch# show interface switchport Expected Results\nStep 1: You can create a VLAN Step 2: You can assign a native VLAN to the physical interface Step 3: You can configure an IP address on the VLAN interface Step 4: You can successfully ping the other switch\u0026rsquo;s VLAN interface Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/mclag_isl_ha/",
	"title": "VSX ISL HA",
	"tags": [],
	"description": "",
	"content": "VSX: ISL HA The intent here is to showcase an inter-switch-link (ISL) link failover scenario where one of the two links between spine switches goes down, but ISL is still connected with single link.\nThe following image is a visualization of disconnected ISL link:\nThe following things are expected to be seen in this scenario:\nAfter disconnecting one ISL, the VSX functionality should not be affected A small percentage of packets will be dropped when disconnecting the cable where traffic is flowing; A sub second value is expected during this event When connecting back the cable, the hashing needs to be recalculated and some packets may be dropped during this event as well; A sub second value is expected during this event Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/view_postgres_information_for_system_databases/",
	"title": "View Postgres Information for System Databases",
	"tags": [],
	"description": "",
	"content": "View Postgres Information for System Databases Postgres uses SQL language to store and manage databases on the system. This procedure describes how to view and obtain helpful information about system databases, as well as the types of data being stored.\nPrerequisites This procedure requires administrative privileges.\nProcedure Log in to the Postgres container.\nncn-mw# kubectl -n services exec -it cray-smd-postgres-0 -c postgres -- bash Example output:\nDefaulting container name to postgres. Use \u0026#39;kubectl describe pod/cray-smd-postgres-0 -n services\u0026#39; to see all of the containers in this pod. ____ _ _ / ___| _ __ (_) | ___ \\___ \\| \u0026#39;_ \\| | |/ _ \\ ___) | |_) | | | (_) | |____/| .__/|_|_|\\___/ |_| This container is managed by runit, when stopping/starting services use sv Examples: sv stop cron sv restart patroni Current status: (sv status /etc/service/*) run: /etc/service/cron: (pid 26) 487273s run: /etc/service/patroni: (pid 24) 487273s run: /etc/service/pgqd: (pid 25) 487273s Log in as the postgres user.\npostgres-container# psql -U postgres Example output:\npsql (12.2 (Ubuntu 12.2-1.pgdg18.04+1), server 11.7 (Ubuntu 11.7-1.pgdg18.04+1)) Type \u0026#34;help\u0026#34; for help. postgres=# List the existing databases.\npostgres# \\l Example output:\nList of databases Name | Owner | Encoding | Collate | Ctype | Access privileges ------------+-----------------+----------+-------------+-------------+----------------------- hmsds | hmsdsuser | UTF8 | en_US.UTF-8 | en_US.UTF-8 | postgres | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | service_db | service_account | UTF8 | en_US.UTF-8 | en_US.UTF-8 | template0 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | =c/postgres + | | | | | postgres=CTc/postgres (5 rows) Establish a connection to the desired database.\nIn the example below, the hmsds database is used.\npostgres# \\c hmsds Example output:\npsql (12.2 (Ubuntu 12.2-1.pgdg18.04+1), server 11.7 (Ubuntu 11.7-1.pgdg18.04+1)) You are now connected to database \u0026#34;hmsds\u0026#34; as user \u0026#34;postgres\u0026#34;. hmsds=# List the data types that are in the database being viewed.\npostgres# \\dt Example output:\nList of relations Schema | Name | Type | Owner --------+-------------------------+-------+----------- public | comp_endpoints | table | hmsdsuser public | comp_eth_interfaces | table | hmsdsuser public | component_group_members | table | hmsdsuser public | component_groups | table | hmsdsuser public | component_lock_members | table | hmsdsuser public | component_locks | table | hmsdsuser public | components | table | hmsdsuser public | discovery_status | table | hmsdsuser public | hwinv_by_fru | table | hmsdsuser public | hwinv_by_loc | table | hmsdsuser public | hwinv_hist | table | hmsdsuser public | job_state_rf_poll | table | hmsdsuser public | job_sync | table | hmsdsuser public | node_nid_mapping | table | hmsdsuser public | power_mapping | table | hmsdsuser public | rf_endpoints | table | hmsdsuser public | schema_migrations | table | hmsdsuser public | scn_subscriptions | table | hmsdsuser public | service_endpoints | table | hmsdsuser public | system | table | hmsdsuser (20 rows) "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/use_a_specific_inventory_in_a_configuration_session/",
	"title": "Use a Specific Inventory in a Configuration Session",
	"tags": [],
	"description": "",
	"content": "Use a Specific Inventory in a Configuration Session A special repository can be added to a Configuration Framework Service (CFS) configuration to help with certain scenarios, specifically when developing Ansible plays for use on the system. A static inventory often changes along with the Ansible content, and CFS users may need to test different configuration values simultaneously and not be forced to use the global additionalInventoryUrl.\nTherefore, an additional_inventory mapping can be added to the CFS configuration. Similar to a standard configuration layer, the additional inventory only requires a commit and repository clone URL, and it overrides the global additionalInventoryUrl if it is specified in the global CFS options.\nFor example:\nncn-m001# cat configurations-example-additional-inventory.json Example configuration:\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34; } ], \u0026#34;additional_inventory\u0026#34;: { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/inventory.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;a7d08b6e1be590ac01711e39c684b6893c1da0a9\u0026#34; } } "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_the_boot_script_service_bss/",
	"title": "Troubleshoot Compute Node Boot Issues Related to the Boot Script Service (BSS)",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Compute Node Boot Issues Related to the Boot Script Service (BSS) Boot Script Service (BSS) delivers a boot script to a node based on its MAC address. This boot script tells the node where to obtain its boot artifacts, which include:\nkernel initrd In addition, the boot script also contains the kernel boot parameters. This procedure helps resolve issues related to missing boot artifacts.\nPrerequisites This procedure requires administrative privileges.\nLimitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.\nProcedure Check that BSS is running.\nncn-mw# kubectl get pods -n services -o wide | grep cray-bss | grep -v -etcd- Example output:\ncray-bss-fd888bd54-gvpxq 2/2 Running 0 2d3h 10.32.0.16 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Check that the boot script of the node that is failing to boot contains the correct boot artifacts.\nIf nodes are identified by their host names, then execute the following:\nncn-mw# cray bss bootparameters list --hosts HOST_NAME If nodes are identified by their node IDs, then execute the following:\nncn-mw# cray bss bootparameters list --nids NODE_ID View the entire BSS contents.\nncn-mw# cray bss dumpstate list View the actual boot script.\nUsing host name:\nncn-mw# cray bss bootscript list --host HOST_NAME Using the MAC address:\nncn-mw# cray bss bootscript list --mac MAC_ADDRESS Using node ID:\nncn-mw# cray bss bootscript list --nid NODE_ID "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/login_to_a_users_uai_to_troubleshoot_issues/",
	"title": "This Page Has Moved",
	"tags": [],
	"description": "",
	"content": "This Page Has Moved The information on this page has moved to Troubleshoot UAIs with Administrative Access. Refer to that page for these procedures.\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/manage_system_passwords/",
	"title": "Manage System Passwords",
	"tags": [],
	"description": "",
	"content": "Manage System Passwords Many system services require login credentials to gain access to them. The information below is a comprehensive list of system passwords and how to change them.\nContact HPE Cray service in order to obtain the default usernames and passwords for any of these components or services.\nKeycloak Default Keycloak admin user login credentials:\nUsername: admin\nThe password can be obtained with the following command:\nncn-w001# kubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode To update the default password for the admin account, refer to Change the Keycloak Admin Password.\nTo create new accounts, refer to Create Internal User Accounts in the Keycloak Shasta Realm.\nGitea The default Gitea user credentials is crayvcs. The password is randomly generated at install time and can be found in the vcs-user-credentials secret.\nncn-w001# kubectl get secret -n services vcs-user-credentials \\ --template={{.data.vcs_password}} | base64 --decode For more information on Gitea, including how to change the password, see Version Control Service VCS.\nSystem Management Health Service The default username is admin.\nNOTE: Contact HPE Cray service in order to obtain the default password for Grafana and Kiali.\nManagement Network Switches Each rack type includes a different set of passwords. During different stages of installation, these passwords are subject to change.\nNOTE: Contact HPE Cray service in order to obtain the default passwords.\nThe tables below include the default login credentials for each rack type. These passwords can be changed by going into the console on a given switch and changing it. However, if the user gets locked out attempting to change the password or the configuration gets corrupted for an individual switch, it can wipe out the entire network configuration for the system.\nNOTE: IP addresses can be found from the generated SLS file.\nLiquid-Cooled Cabinet:\nName Role Switch Login sw-leaf-bmc Leaf-BMC/Mgmt Dell S3048-ON admin sw-spine Spine Mellanox SN2100 admin sw-leaf-bmc Leaf-BMC/Mgmt Aruba 6300 admin sw-spine Spine Aruba 8325 admin sw-leaf Leaf Aruba 8325 admin Air-Cooled Cabinet:\nName Role Switch Login sw-leaf-bmc Leaf/Mgmt Dell S3048-ON admin sw-leaf-bmc Leaf-BMC/Mgmt Aruba 6300 admin Coolant Distribution Unit (CDU):\nName Role Switch Login sw-cdu CDU/Leaf Dell S4048T-ON admin sw-cdu CDU/Leaf Aruba 8360 admin ClusterStor:\nName Role Switch IP Address Login Arista DCS-7060CX-32S 172.16.249.10 admin Sonexion Entry point to Arista CS-L300 172.30.49.178 admin E1000 CS-E1000 admin Redfish Credentials Redfish accounts are only valid with the Redfish API. They do not allow system logins via ssh or serial console.\nThree accounts are created by default:\nRoot - Administrative account\nUsername: root Operator - Power components on/off, read values, and configure accounts\nUsername: operator ReadOnly - Log in, configure self, and read values\nUsername: guest NOTE: Contact HPE Cray service in order to obtain the default passwords.\nThe System Configuration Service (SCSD) is used to set the credentials for Redfish BMCs. Refer to Set BMC Credentials for more information.\nThe account database is automatically saved to the non-volatile settings partition (/nvram/redfish/redfish-accounts) any time an account or account policy is modified. The file is stored as a redis command dump and is replayed (if it exists) anytime the core Redfish schema is loaded via the init script. If default accounts must be restored, delete the redis command dump and reboot the controller.\nList accounts:\nUse the following API path to list all accounts:\nGET /redfish/v1/AccountService/Accounts { \u0026#34;@odata.context\u0026#34;: \u0026#34;/redfish/v1/$metadata#ManagerAccountCollection.ManagerAccountCollection\u0026#34;, \u0026#34;@odata.etag\u0026#34;: \u0026#34;W/\\\u0026#34;1559675674\\\u0026#34;\u0026#34;, \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts\u0026#34;, \u0026#34;@odata.type\u0026#34;: \u0026#34;#ManagerAccountCollection.ManagerAccountCollection\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Collection for Manager Accounts\u0026#34;, \u0026#34;Members\u0026#34;: [ { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/1\u0026#34; }, { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/2\u0026#34; } ], \u0026#34;Members@odata.count\u0026#34;: 2, \u0026#34;Name\u0026#34;: \u0026#34;Accounts Collection\u0026#34; } Use the following API path to list a single account:\nGET /redfish/v1/AccountService/Accounts/1 { \u0026#34;@odata.context\u0026#34;: \u0026#34;/redfish/v1/$metadata#ManagerAccount.ManagerAccount(*)\u0026#34;, \u0026#34;@odata.etag\u0026#34;: \u0026#34;W/\u0026#34;1559675272\u0026#34;\u0026#34;, \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/1\u0026#34;, \u0026#34;@odata.type\u0026#34;: \u0026#34;#ManagerAccount.v1_1_1.ManagerAccount\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Default Account\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;Id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Links\u0026#34;: { \u0026#34;Role\u0026#34;: { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Roles/Administrator\u0026#34; } }, \u0026#34;Locked\u0026#34;: false, \u0026#34;Name\u0026#34;: \u0026#34;Default Account\u0026#34;, \u0026#34;RoleId\u0026#34;: \u0026#34;Administrator\u0026#34;, \u0026#34;UserName\u0026#34;: \u0026#34;root\u0026#34; } Add accounts:\nIf an account is successfully created, then the account information data structure will be returned. The most important bit returned is the Id because it is part of the URL used for any further manipulation of the account.\nUse the following API path to add accounts:\nPOST /redfish/v1/AccountService/Accounts Content-Type: application/json { \u0026#34;Name\u0026#34;: \u0026#34;Test Account\u0026#34;, \u0026#34;RoleId\u0026#34;: \u0026#34;Administrator\u0026#34;, \u0026#34;UserName\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;test123\u0026#34;, \u0026#34;Locked\u0026#34;: false, \u0026#34;Enabled\u0026#34;: true } Response: { \u0026#34;@odata.context\u0026#34;: \u0026#34;/redfish/v1/$metadataAccountService/Members/Accounts\u0026#34;, \u0026#34;@odata.etag\u0026#34;: \u0026#34;W/\u0026#34;1559679136\u0026#34;\u0026#34;, \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts\u0026#34;, \u0026#34;@odata.type\u0026#34;: \u0026#34;#ManagerAccount.v1_1_1.ManagerAccount\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Collection of Account Details\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;5\u0026#34;, **\u0026lt;\u0026lt;-- Note this value** \u0026#34;Links\u0026#34;: { \u0026#34;Role\u0026#34;: { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Roles/Administrator\u0026#34; } }, \u0026#34;Enabled\u0026#34;: true, \u0026#34;Locked\u0026#34;: false, \u0026#34;Name\u0026#34;: \u0026#34;Test\u0026#34;, \u0026#34;RoleId\u0026#34;: \u0026#34;Administrator\u0026#34;, \u0026#34;UserName\u0026#34;: \u0026#34;test\u0026#34; } Delete accounts:\nDelete an account with the curl command:\n# curl -u root:xxx -X DELETE https://x0c0s0b0/redfish/v1/AccountService/Accounts/ACCOUNT_ID Update passwords:\nUpdate the password for an account with the curl command:\nWARNING: Changing Redfish credentials outside of Cray System Management (CSM) services may cause the Redfish device to be no longer manageable under CSM. If the credentials for other devices need to be changed, refer to the following device-specific credential changing procedures:\nTo change liquid-cooled BMC credentials, refer to Change Cray EX Liquid-Cooled Cabinet Global Default Password. To change air-cooled node BMC credentials, refer to Change Air-Cooled Node BMC Credentials. To change Slingshot switch BMC credentials, refer to \u0026ldquo;Change Rosetta Login and Redfish API Credentials\u0026rdquo; in the Slingshot Operations Guide (\u0026gt; 1.6.0). # curl -u root:xxx -X PATCH \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;Name\u0026#34;: \u0026#34;Test\u0026#34;}\u0026#39; \\ https://x0c0s0b0/redfish/v1/AccountService/Accounts/ACCOUNT_ID System Controllers For SSH access, the system controllers have the following default credentials:\nNode controller (nC)\nUsername: root Chassis controller (cC)\nUsername: root Switch controller (sC)\nUsername: root sC minimal recovery firmware image (rec)\nUsername: root NOTE: Contact HPE Cray service in order to obtain the default passwords.\nPasswords for nC, cC, and sC controllers are all managed with the following process. The cfgsh tool is a configuration shell that can be used interactively or scripted. Interactively, it may be used as follows after logging in as root via ssh:\nx0c1# config x0c1(conf)# CURRENT_PASSWORD root NEW_PASSWORD x0c1(conf)# exit x0c1# copy running-config startup-config x0c1# exit It may be used non-interactively as well. This is useful for separating out several of the commands used for the initial setup. The shell utility returns non-zero on error.\n# cfgsh --config CURRENT_PASSWORD root NEW_PASSWORD # cfgsh copy running-config startup-config In both cases, a running-config must be saved out to non-volatile storage in a startup configuration file. If it is not, the password will revert to default on the next boot. This is the exact same behavior as standard managed Ethernet switches.\nSNMP Credentials To adjust the SNMP credentials, perform the following tasks:\nUpdate the default credentials specified in the customizations.yaml file.\nSee Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials Update the credentials actively being used for existing leaf switches.\nSee Change SNMP Credentials on Leaf BMC Switches HPE Cray EX Liquid-Cooled Cabinet Hardware Change the global default credential on HPE Cray EX liquid-cooled cabinet embedded controllers (BMCs). The chassis management module (CMM) controller (cC), node controller (nC), and Slingshot switch controller (sC) are generically referred to as \u0026ldquo;BMCs\u0026rdquo; in these procedures.\nSee Change EX Liquid-Cooled Cabinet Global Default Password Provision a Glibc compatible SHA-512 administrative password hash to a cabinet environmental controller (CEC). This password becomes the Redfish default global credential to access the CMM controllers and node controllers (BMCs).\nSee Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials Change the credential for HPE Cray EX liquid-cooled cabinet chassis controllers and node controller (BMCs) used by CSM services after the CECs have been set to a new global default credential.\nSee Updating the Liquid-Cooled EX Cabinet CEC with Default Credentials after a CEC Password Change Gigabyte The default username is admin.\nNOTE: Contact HPE Cray service in order to obtain the default password for Gigabyte.\nPasswords Managed in Other Product Streams Refer to the following product stream documentation for detailed procedures about updating passwords for compute nodes and User Access Nodes (UANs).\nCray Operating System (COS): To update the root password for compute nodes, refer to \u0026ldquo;Set Root Password for Compute Nodes\u0026rdquo; in the COS product stream documentation for more information.\nUser Access Node (UAN): Refer to \u0026ldquo;Create UAN Boot Images\u0026rdquo; in the UAN product stream documentation for the steps required to change the password on UANs. The \u0026ldquo;uan_shadow\u0026rdquo; header in the \u0026ldquo;UAN Ansible Roles\u0026rdquo; section includes more context on setting the root password on UANS.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/move_a_standard_rack_node_samerack_samehsnports/",
	"title": "Move a Standard Rack Node (Same Rack/Same HSN Ports)",
	"tags": [],
	"description": "",
	"content": "Move a Standard Rack Node (Same Rack/Same HSN Ports) This procedure move standard rack UAN or compute node to a different location and uses the same Slingshot switch ports and management network ports.\nUpdate the location-based component name (xname) for a standard rack node within the system.\nIf a node has an incorrect component name (xname) based on its physical location, then this procedure can be used to correct the component name (xname) of the node without the need to physically move the node.\nPrerequisites An authentication token has been retrieved.\nncn-mw# function get_token () { curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39; } The Cray command line interface (CLI) tool is initialized and configured on the system. See configure the Cray CLI.\nThis procedure applies only to standard rack nodes. Liquid-cooled compute blades do not require the use of the MgmtSwitchConnector object in the System Layout Service (SLS) to perform discovery.\nThis procedure moves an application node or compute node to a different location in an HPE Cray standard rack.\nThe node must use the same Slingshot switch switch ports.\nThe node must use the same management network switch ports.\nLimitations This procedure assumes there are no changes to the node high-speed network switch port or management network ports.\nProcedure This procedure works with both application and compute nodes. This example moves a compute node in rack 3000 at U17 to U27 in the same rack.\nShut down software and power off the node.\nDisconnect the power cables, management network cables, and high-speed network (HSN) cables.\nIf this procedure is being followed to correct a node\u0026rsquo;s component name (xname), then this step can be skipped.\nMove the node to the new location in the rack (U27), connect the management cables and HSN cables, but do not connect the power cables.\nIf this procedure is being followed to correct a node\u0026rsquo;s component name (xname), then this step can be skipped.\nUpdate Node in the System Layout Service (SLS) Set up environment variables for the original node and node BMC component names (xnames).\nncn-mw# OLD_NODE_XNAME=x3000c0s17b1n0 ncn-mw# echo $OLD_NODE_XNAME Example output:\nx3000c0s17b1n0 ncn-mw# OLD_BMC_XNAME=$(echo $OLD_NODE_XNAME | egrep -o \u0026#39;x[0-9]+c[0-9]+s[0-9]+b[0-9]+\u0026#39;) ncn-mw# echo $OLD_BMC_XNAME Example output:\nx3000c0s17b1 Set up environment variables for the new node and node BMC component names (xnames).\nncn-mw# NEW_NODE_XNAME=x3000c0s27b1n0 ncn-mw# echo $NEW_NODE_XNAME Example output:\nx3000c0s27b1n0 ncn-mw# NEW_BMC_XNAME=$(echo $NEW_NODE_XNAME | egrep -o \u0026#39;x[0-9]+c[0-9]+s[0-9]+b[0-9]+\u0026#39;) ncn-mw# echo $NEW_BMC_XNAME Example output:\nx3000c0s27b1 Update SLS with the node\u0026rsquo;s new component name (xname).\nGet node from SLS:\nncn-mw# cray sls hardware describe \u0026#34;$OLD_NODE_XNAME\u0026#34; --format json \u0026gt; sls_node.original.json Example contents of sls_node.original.json\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s17b1\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s17b1n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;LastUpdated\u0026#34;: 1631829089, \u0026#34;LastUpdatedTime\u0026#34;: \u0026#34;2021-09-16 21:51:29.997834 +0000 +0000\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000001\u0026#34; ], \u0026#34;NID\u0026#34;: 1, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34; } } Update the SLS node object with the new component names (xnames):\nncn-mw# jq --arg NODE_XNAME \u0026#34;$NEW_NODE_XNAME\u0026#34; --arg BMC_XNAME \u0026#34;$NEW_BMC_XNAME\u0026#34; \\ \u0026#39;.Parent = $BMC_XNAME | .Xname = $NODE_XNAME\u0026#39; sls_node.original.json \\ \u0026gt; sls_node.json Expected contents of sls_node.original.json:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s19b1\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;LastUpdated\u0026#34;: 1631829089, \u0026#34;LastUpdatedTime\u0026#34;: \u0026#34;2021-09-16 21:51:29.997834 +0000 +0000\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000001\u0026#34; ], \u0026#34;NID\u0026#34;: 1, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34; } } Only the fields Parent and Xname should have been updated.\nCreate new node object in SLS:\nncn-mw# curl -i -X POST -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; \\ https://api-gw-service-nmn.local/apis/sls/v1/hardware -d @sls_node.json NOTE If a 503 is returned, verify that get_token function has been defined.\nExpected output:\nHTTP/2 200 content-type: application/json date: Mon, 18 Oct 2021 20:30:02 GMT content-length: 42 x-envoy-upstream-service-time: 71 server: istio-envoy {\u0026#34;code\u0026#34;:0,\u0026#34;message\u0026#34;:\u0026#34;inserted new entry\u0026#34;} Delete old node object from SLS:\nncn-mw# cray sls hardware delete $OLD_NODE_XNAME --format toml Expected output:\ncode = 0 message = \u0026#34;deleted entry and its descendants\u0026#34; Update MgmtSwitchConnector in SLS with the node BMC\u0026rsquo;s new component name (xname):\nGet MgmtSwitchConnector object from SLS:\nncn-mw# cray sls search hardware list --node-nics \u0026#34;$OLD_BMC_XNAME\u0026#34; --format json \u0026gt; sls_MgmtSwitchConnector.original.json Example contents of sls_MgmtSwitchConnector.original.json:\n[ { \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w22\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w22j33\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;LastUpdated\u0026#34;: 1631829089, \u0026#34;LastUpdatedTime\u0026#34;: \u0026#34;2021-09-16 21:51:29.997834 +0000 +0000\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s17b1\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;ethernet1/1/33\u0026#34; } } ] Update MgmtSwitchConnector object with the new node BMC component name (xname):\nncn-mw# jq --arg BMC_XNAME \u0026#34;$NEW_BMC_XNAME\u0026#34; \\ \u0026#39;.[0] | .ExtraProperties.NodeNics = [ $BMC_XNAME ]\u0026#39; sls_MgmtSwitchConnector.original.json \\ \u0026gt; sls_MgmtSwitchConnector.json Expected contents of sls_MgmtSwitchConnector.json:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w22\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w22j33\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;LastUpdated\u0026#34;: 1631829089, \u0026#34;LastUpdatedTime\u0026#34;: \u0026#34;2021-09-16 21:51:29.997834 +0000 +0000\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s27b1\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;ethernet1/1/33\u0026#34; } } Only the NodeNics field should have been updated.\nDetermine the component name (xname) of the MgmtSwitchConnector:\nncn-mw# MGMT_SWITCH_CONNECTOR_XNAME=$(jq -r .Xname sls_MgmtSwitchConnector.json) ncn-mw# echo $MGMT_SWITCH_CONNECTOR_XNAME Example output:\nx3000c0w22j36 Update the MgmtSwitchConnector in SLS:\nncn-mw# curl -i -X PUT -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; \\ https://api-gw-service-nmn.local/apis/sls/v1/hardware/$MGMT_SWITCH_CONNECTOR_XNAME -d @sls_MgmtSwitchConnector.json Expected output:\nHTTP/2 200 content-type: application/json date: Mon, 18 Oct 2021 20:33:36 GMT content-length: 301 x-envoy-upstream-service-time: 8 server: istio-envoy {\u0026#34;Parent\u0026#34;:\u0026#34;x3000c0w22\u0026#34;,\u0026#34;Xname\u0026#34;:\u0026#34;x3000c0w22j36\u0026#34;,\u0026#34;Type\u0026#34;:\u0026#34;comptype_mgmt_switch_connector\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;River\u0026#34;,\u0026#34;TypeString\u0026#34;:\u0026#34;MgmtSwitchConnector\u0026#34;,\u0026#34;LastUpdated\u0026#34;:1631829089,\u0026#34;LastUpdatedTime\u0026#34;:\u0026#34;2021-09-16 21:51:29.997834 +0000 +0000\u0026#34;,\u0026#34;ExtraProperties\u0026#34;:{\u0026#34;NodeNics\u0026#34;:[\u0026#34;x3000c0s21b4\u0026#34;],\u0026#34;VendorName\u0026#34;:\u0026#34;ethernet1/1/36\u0026#34;}} Remove previously discovered node data from HSM Remove previously discovered components from HSM:\nRemove Node component from HSM:\nncn-mw# cray hsm state components delete $OLD_NODE_XNAME Remove NodeBMC component from HSM:\nncn-mw# cray hsm state components delete $OLD_BMC_XNAME Remove NodeEnclosure component from HSM. The component name (xname) for a NodeEnclosure is similar to the node BMC component name (xname), but the b is replaced with a e.\nncn-mw# OLD_NODE_ENCLOSURE_XNAME=x3000c0s17e0 ncn-mw# cray hsm state components delete $OLD_NODE_ENCLOSURE_XNAME Delete the NodeBMC, Node NIC MAC addresses, and the Redfish endpoint for the U17 node from the HSM.\nDelete the Node MAC addresses from the HSM.\nncn-mw# for ID in $(cray hsm inventory ethernetInterfaces list --component-id $OLD_NODE_XNAME --format json | jq -r .[].ID); do echo \u0026#34;Deleting MAC address: $ID\u0026#34; cray hsm inventory ethernetInterfaces delete $ID; done Delete each NodeBMC MAC address from the Hardware State Manager (HSM) Ethernet interfaces table.\nncn-mw# for ID in $(cray hsm inventory ethernetInterfaces list --component-id $OLD_BMC_XNAME --format json | jq -r .[].ID); do echo \u0026#34;Deleting MAC address: $ID\u0026#34; cray hsm inventory ethernetInterfaces delete $ID; done Delete the Redfish endpoint for the removed node.\nncn-mw# cray hsm inventory redfishEndpoints delete $OLD_BMC_XNAME Connect the power cables to the node to power on the BMC.\nIf this procedure is being followed to correct a node\u0026rsquo;s component name (xname), then this step can be skipped.\nWait for 5 minutes for the BMC to power on and the node BMCs to be discovered.\nVerify the node BMC has been discovered by the HSM.\nncn-mw# cray hsm inventory redfishEndpoints describe $NEW_BMC_XNAME --format json Example output:\n{ \u0026#34;ID\u0026#34;: \u0026#34;x3000c0s27b1\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;NodeBMC\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;x3000c0s27b1\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x3000c0s27b1\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;UUID\u0026#34;: \u0026#34;e005dd6e-debf-0010-e803-b42e99be1a2d\u0026#34;, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MACAddr\u0026#34;: \u0026#34;b42e99be1a2d\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2021-01-29T16:15:37.643327Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;1.7.0\u0026#34; } } When LastDiscoveryStatus displays as DiscoverOK, then the node BMC has been successfully discovered. If the last discovery state is DiscoveryStarted then the BMC is currently being inventoried by HSM. If the last discovery state is HTTPsGetFailed or ChildVerificationFailed then an error occurred during the discovery process. For HTTPsGetFailed, verify that the BMC is pingable by its component name (xname). If the component name (xname) of the BMC is not resolvable, then more time may be needed for DNS to update.\nIf hostname does resolve, then issue a discovery request to HSM:\nncn-mw# cray hsm inventory discover create --xnames $NEW_BMC_XNAME Verify that the nodes are enabled in the HSM.\nncn-mw# cray hsm state components describe $NEW_NODE_XNAME --format toml Beginning of example output:\nType = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;Off\u0026#34; If necessary, enable the nodes in the HSM database (in this example, the nodes are x3000c0s27b[1-4]n0).\nncn-mw# cray hsm state components bulkEnabled update --enabled true --component-ids x3000c0s27b1n0,x3000c0s27b2n0,x3000c0s27b3n0,x3000c0s27b4n0 Use boot orchestration to power on and boot the nodes.\nSpecify the appropriate BOS template for the node type.\nncn-mw# cray bos session create --template-uuid cle-VERSION --operation reboot --limit x3000c0s27b1n0,x3000c0s27b2n0,x3000c0s27b3n0,x3000c0s27b4n0 "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/ncn_tcpdump/",
	"title": "TCPDUMP",
	"tags": [],
	"description": "",
	"content": "TCPDUMP If your host is not getting an IP address you can run a packet capture to see if DHCP traffic is being transmitted.\nOn ncn-w001 or a worker/manager with kubectl, run:\ntcpdump -w dhcp.pcap -envli bond0.nmn0 port 67 or port 68 This will make a .pcap file named dhcp in your current directory. It will collect all DHCP traffic on the port you specify, in this example we are looking for DHCP traffic on interface bond0.nmn0 (10.252.0.0/17)\nTo view the DHCP traffic, run:\ntcpdump -r dhcp.pcap -v -n The output may be very long so you might have to use filters.\nIf you want to do a tcpdump for a certain MAC address you can run:\ntcpdump -i eth0 -vvv -s 1500 \u0026#39;((port 67 or port 68) and (udp[38:4] = 0x993b7030))\u0026#39; Note: This example is using the MAC of b4:2e:99:3b:70:30 and will show the output on your terminal and not save to a file.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/mclag_link_ha/",
	"title": "VSX MCLAG Link HA",
	"tags": [],
	"description": "",
	"content": "VSX: MCLAG Link HA The intent here is to showcase a typical MCLAG link failover scenario, where one of the links goes to the edge device, whether that is the connected switch or server.\nIn the following image, a typical traffic pattern coming off from MCLAG connected device is shown. The traffic is going north to south and ISL is not carrying any traffic. The only time ISL will carry traffic is if one of the links to downstream devices is down.\nThe network is now fully configured up, and you decide to test HA functionality by pulling MCLAG link off from Spine-02 and the bottom switch, what would you be expected to see?\nA small percentage of packets will be dropped when disconnecting the cable where traffic is flowing. A sub second value is expected during this event. When connecting back the cable, the hashing needs to be recalculated and some packets may be dropped during this event as well. A sub second value is expected during this event. Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/vcs_branching_strategy/",
	"title": "VCS Branching Strategy",
	"tags": [],
	"description": "",
	"content": "VCS Branching Strategy Individual products import configuration content (Ansible plays, roles, and more) into a repository in the Version Control Service (VCS) through their installation process. Typically, this repository exists in the cray organization in VCS and its name has the format [product name]-config-management.\nThe import branch of the product is considered \u0026ldquo;pristine content\u0026rdquo; and is added to VCS in a read-only branch. This step is taken to ensure the future updates of the product\u0026rsquo;s configuration content can be based on a clean branch, and that upgrades can proceed without merging issues.\nHPE Cray products import their content into a branch with a name that has the format cray/[product]/[product version].\nThe version is in SemVer format. Product installation and operational workflows will direct users to create branches from these pristine product branches to begin customizing their configuration from the default provided by the HPE Cray product.\nThe workflow of branching from pristine imported content and handling upgrades to that content is shown in the diagram below (see \u0026ldquo;Branch Workflow\u0026rdquo;). In this scenario, the configuration customizations are made in a customer/main branch, which can be any branch in the repository, such as master, main, integration, or others. As configuration changes are made and the system is configured as required, users can point to the commits (white circles) in their CFS configuration layers to test and run HPE Cray products on the system. Users may also point to the commits on pristine branches (blue circle) in CFS configuration layers if the default configuration is sufficient.\nWhen a product upgrade occurs (cray/product/1.4.0 in the above diagram), the product installer will create a new branch and commit based off the previous pristine branch. If the previous customizations on the customer/main branch also need to be included with the new 1.4.0 content, the user should initiate a Git merge of the new pristine content into their branch (or possibly into a test branch based on customer/main). This process will be the same for subsequent updates of the product as well.\nIMPORTANT: If the HPE Cray product has specific instructions for installing and upgrading configuration content, those should take precedence over this generic workflow.\nThe \u0026ldquo;Branch Workflow\u0026rdquo; diagram is an example workflow that can be used to manage new product content being introduced during upgrades. However, CFS and VCS do not require any specific branching strategy. Users are free to manage the branches as they see fit with the exception of the pristine branches imported by individual HPE Cray products. CFS configuration layers (see Configuration Layers) only require a Git commit ID, a Git repository clone URL, and the path to an Ansible playbook to run the configuration content in the repository.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_using_kubernetes/",
	"title": "Troubleshoot Compute Node Boot Issues Using Kubernetes",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Compute Node Boot Issues Using Kubernetes A number of Kubernetes commands can be used to debug issues related to the node boot process. All of the traffic bound for the DHCP server, TFTP server, and Boot Script Service (BSS) is sent on the Node Management Network (NMN).\nIn the current arrangement, all three services are located on a management non-compute node (NCN). Thus, traffic must first travel through the NCN to reach these services inside their pods. When attempting to track down missing requests for either DHCP or TFTP, it is helpful to set up tcpdump on the NCN where the pod is resident to ensure that the request got that far. The NODE column in the output of kubectl get pods -o wide shows which node the pod is running on.\nTroubleshooting tips Check if a Kubernetes pod is running.\nncn-mw# kubectl get pods -A -o wide | grep pod_name Example command:\nncn-mw# kubectl get pods -n services -o wide|grep -E \u0026#34;NAME|kea\u0026#34; Example output:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cray-dhcp-kea-6b78789fc4-lzmff 3/3 Running 0 5d12h 10.42.0.30 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Retrieve logs for a specific Kubernetes pod.\nncn-mw# kubectl logs -n NAMESPACE pod_name Example command:\nncn-mw# kubectl get pods -n services -o wide|grep -E \u0026#34;NAME|kea\u0026#34; Example output:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cray-dhcp-kea-6b78789fc4-lzmff 3/3 Running 0 5d12h 10.42.0.30 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Example command:\nncn-mw# kubectl logs -n services DHCP_KEA_POD_ID -c CONTAINER Beginning of example output:\nDHCPDISCOVER from a4:bf:01:23:1a:f4 via vlan100 ICMP Echo reply while lease 10.100.160.199 valid. Abandoning IP address 10.100.160.199: pinged before offer Reclaiming abandoned lease 10.100.160.195. Gain access to a Kubernetes pod.\nUse the following command to enter a shell inside a container within the pod. Once inside the shell, execute commands as needed.\nncn-mw# kubectl exec -A -it pod_name /bin/sh Example command for a pod named cray-dhcp-kea-6b78789fc4-lzmff:\nncn-mw# kubectl exec -A -it cray-dhcp-kea-6b78789fc4-lzmff /bin/sh "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/modify_a_uai_class/",
	"title": "Modify a UAI Class",
	"tags": [],
	"description": "",
	"content": "Modify a UAI Class Update a UAI class with a modified configuration.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Class ID of the UAI Class to be modified: List UAI Classes Limitations The ID of the UAI class cannot be modified.\nProcedure To update an existing UAI class, use a command of the following form:\ncray uas admin config classes update OPTIONS UAI_CLASS_ID OPTIONS are the same options supported for UAI class creation. They can be seen by using the command. UAI_CLASS_ID is the Class ID of the UAI class to be modified.\ncray uas admin config classes update --help Modify a UAI class.\nThe following example changes the comment on the UAI class with an ID of bb28a35a-6cbc-4c30-84b0-6050314af76b.\nncn-m001-pit#cray uas admin config classes update \\ --replicas 3 \\ bdb4988b-c061-48fa-a005-34f8571b88b4 Any change made using this command affects only UAIs that are both created using the modified class and are created after the modification. Existing UAIs using the class will not change.\nOptional: Update currently running UAIs by deleting and recreating them, or deleting them and allowing them to be re-created through a Broker UAI. See Delete a UAI for more details.\nTop: User Access Service (UAS)\nNext Topic: Delete a UAI Class\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/pki_certificate_authority_ca/",
	"title": "PKI Certificate Authority (CA)",
	"tags": [],
	"description": "",
	"content": "PKI Certificate Authority (CA) An instance of HashiCorp Vault, deployed via the Bitnami Bank-vaults operator, stores private and public Certificate Authority (CA) material, and serves APIs through a Public Key Infrastructure (PKI) engine instance.\nCA material is injected as a start-up secret into Vault through a SealedSecret that translates into a Kubernetes Secret.\nCA Certificate Distribution Trusted CA certificates are distributed via two channels:\nCloud-init metadata Kubernetes ConfigMaps Kubernetes-native workloads generally leverage ConfigMap-based distribution. The Cloud-init based method is used for non-compute node (NCN) OS distribution.\nOn NCNs, trusted certificates are installed by Cloud-init in the /etc/pki/trust/anchors/platform-ca-certs.crt file. Refer to Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster for more information for clients that are outside the system\u0026rsquo;s management cluster.\nOn compute nodes (CNs), trusted certificates are installed at image build time by the Image Management Service (IMS), and are located in the /etc/cray/ca/certificate_authority.crt file.\nFor NCNs and CNs, the trusted certificates are added to the base OS trust store. The TrustedCerts Kubernetes Operator manages updates to trusted CA material across the noted channels.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/move_a_liquid-cooled_blade_within_a_system/",
	"title": "Move a liquid-cooled blade within a System",
	"tags": [],
	"description": "",
	"content": "Move a liquid-cooled blade within a System This top level procedure outlines common scenarios for moving blades around within an HPE Cray EX system.\nBlade movement scenarios:\nScenario 1: Swap locations of two blades Scenario 2: Move blade into a populated slot Scenario 3: Move blade into an unpopulated slot Prerequisites Knowledge of whether DVS is operating over the Node Management Network (NMN) or the High Speed Network (HSN).\nThe Slingshot fabric must be configured with the desired topology for desired state of the blades in the system.\nThe System Layout Service (SLS) must have the desired HSN configuration.\nCheck the status of the high-speed network (HSN) and record link status before the procedure.\nThe blades must have the coolant drained and filled during the swap to minimize cross-contamination of cooling systems.\nReview procedures in HPE Cray EX Coolant Service Procedures H-6199 Review the HPE Cray EX Hand Pump User Guide H-6200 Scenario 1: Swap locations of two blades This scenario will swap the locations of blade A in location A with blade B in location B.\nFollow the Removing a Liquid-cooled blade from a System procedure to remove blade A from location A.\nFollow the Removing a Liquid-cooled blade from a System procedure to remove blade B from location B.\nFollow the Adding a Liquid-cooled blade to a System procedure to add blade A in location B.\nFollow the Adding a Liquid-cooled blade to a System procedure to add blade B in location A.\nScenario 2: Move blade into a populated slot This scenario will move blade A in location A into the location B of blade B, but not repopulate location A with a blade.\nFollow the Removing a Liquid-cooled blade from a System procedure to remove blade A from location A.\nFollow the Removing a Liquid-cooled blade from a System procedure to remove blade B from location B.\nFollow the Adding a Liquid-cooled blade to a System procedure to add blade A in location B.\nScenario 3: Move blade into an unpopulated slot This scenario will move blade A in location A to the unpopulated slot of location B.\nFollow the Removing a Liquid-cooled blade from a System procedure to remove blade A from location A.\nFollow the Adding a Liquid-cooled blade to a System procedure to add blade A in location B.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/ncns_on_install/",
	"title": "NCNs on Install",
	"tags": [],
	"description": "",
	"content": "NCNs on Install Verify the DNSMASQ configuration file matches what is configured on the switches.\nExample DNSMASQ Configuration File Here is a DNSMASQ configuration file for the Metal network (VLAN1). As you can see the router is 10.1.0.1, this has to match what the IP address is on the switches doing the routing for the MTL network.\nThis is most commonly on the spines.\nThis configuration is commonly missed on the CSI input file.\nMTL DNSMASQ file:\n# MTL: server=/mtl/ address=/mtl/ domain=mtl,10.1.1.0,10.1.1.233,local dhcp-option=interface:bond0,option:domain-search,mtl interface=bond0 interface-name=pit.mtl,bond0 This needs to point to the LiveCD IP address for provisioning in bare-metal environments:\ndhcp-option=interface:bond0,option:dns-server,10.1.1.2 dhcp-option=interface:bond0,option:ntp-server,10.1.1.2 It must also point at the router for the network; the L3/IP for the VLAN:\ndhcp-option=interface:bond0,option:router,10.1.0.1 dhcp-range=interface:bond0,10.1.1.33,10.1.1.233,10m The following is an example of what the Spine configuration should be.\nAruba configuration:\nsw-spine-001# show run int vlan 1 interface vlan1 vsx-sync active-gateways ip address 10.1.0.2/16 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.1.0.1 ip mtu 9198 ip bootp-gateway 10.1.0.2 ip helper-address 10.92.100.222 exit sw-spine-002# show run int vlan 1 interface vlan1 vsx-sync active-gateways ip address 10.1.0.3/16 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.1.0.1 ip mtu 9198 ip helper-address 10.92.100.222 exit Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/mclag_power_failure/",
	"title": "VSX Member Power Failure",
	"tags": [],
	"description": "",
	"content": "VSX: Member Power Failure The intent here is to showcase a complete member failure scenario where the spine-01 switch is completely down.\nThe following is a visualization of the powered down spine-01:\nThe following is expected in this scenario:\nAfter disconnecting the power supply from one member the other member should be able to detect the member is down and continue a normal operation without any problems. If traffic was originally flowing through the member that was shut down, a small percentage of packets may be dropped. A sub second value is expected during this event. When restoring the power, the hashing needs to be recalculated and some packets may be dropped during this event as well. A sub second value is expected during this event. Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/version_control_service_vcs/",
	"title": "Version Control Service (VCS)",
	"tags": [],
	"description": "",
	"content": "Version Control Service (VCS) VCS overview Cloning a VCS repository VCS administrative user Change VCS administrative user password Access the cray Gitea organization Backup and restore data Backup Postgres data Backup PVC data Restore Postgres data Restore PVC data Alternative backup/restore strategy Alternative export method Alternative import method VCS overview The Version Control Service (VCS) includes a web interface for repository management, pull requests, and a visual view of all repositories and organizations. The following URL is for the VCS web interface:\nhttps://vcs.ext.system.domain.com\nTo look up the external system domain name, run the following command:\nncn-mw# kubectl get secret site-init -n loftsman -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d | grep \u0026#34;external:\u0026#34; Example output:\nexternal: ext.system.domain.com Cloning a VCS repository On cluster nodes, the VCS service can be accessed through the gateway. VCS credentials for the crayvcs user are required before cloning a repository (see VCS administrative user below).\nTo clone a repository in the cray organization, use the following command:\nncn# git clone https://api-gw-service-nmn.local/vcs/cray/REPO_NAME.git VCS administrative user The Cray System Management (CSM) product installation creates the administrative user crayvcs that is used by CSM and other product installers to import their configuration content into VCS.\nThe initial VCS credentials for the crayvcs user are obtained with the following command:\nncn-mw# kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode Change VCS administrative user password The initial VCS login credentials for the crayvcs user are stored in three places:\nvcs-user-credentials Kubernetes secret: This is used to initialize the other two locations, as well as providing a place where other users can query for the password. VCS (Gitea): These credentials are used when pushing to Git using the default username and password. The password should be changed through the Gitea UI. Keycloak: These credentials are used to access the VCS UI. They must be changed through Keycloak. For more information on accessing Keycloak, see Access the Keycloak User Management UI. WARNING: These three sources of credentials are not synced by any mechanism. Changing the default password requires that is it changed in all three places. Changing only one may result in difficulty determining the password at a later date, or may result in losing access to VCS altogether.\nTo change the password in the vcs-user-credentials Kubernetes secret, use the following procedure:\nLog in to Keycloak with the default admin credentials.\nPoint a browser at https://auth.SYSTEM_DOMAIN_NAME/keycloak/admin, replacing SYSTEM_DOMAIN_NAME with the actual NCN\u0026rsquo;s DNS name.\nThe following is an example URL for a system: https://auth.cmn.system1.us.cray.com/keycloak/admin\nUse the following admin login credentials:\nUsername: admin\nThe password can be obtained with the following command:\nncn-mw# kubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode Ensure the selected Realm is Shasta from the top-left dropdown in the left sidebar.\nFrom the left sidebar, under the Manage section, select Users.\nIn the Search... textbox, type in crayvcs and click the search icon.\nIn the filtered table below, click on the ID for the row that shows crayvcs in the Username column.\nGo to the Credentials tab and change the password.\nEnter the new password in the Reset Password form. Ensure Temporary is switched off. Click on Reset Password button.\nLog in to Gitea with the default admin credentials.\nPoint the browser at https://vcs.SHASTA_CLUSTER_DNS_NAME/vcs/user/settings/account.\nIf presented with Keycloak login, use crayvcs as the username and the new VCS password. Wait to be redirected to the Gitea login page before continuing to the next step.\nUse the following Gitea login credentials:\nUsername: crayvcs\nThe old VCS password, which can be obtained with the following command:\nncn-mw# kubectl get secret -n services vcs-user-credentials \\ --template={{.data.vcs_password}} | base64 --decode Enter the existing password (from previous step), new password, and confirmation, and then click Update Password.\nSSH into ncn-w001 or ncn-m001.\nFollow the Redeploying a Chart procedure with the following specifications:\nName of chart to be redeployed: gitea\nBase name of manifest: sysmgmt\nWhen reaching the step to update customizations, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nClone the CSM repository.\nncn-mw# git clone https://github.com/Cray-HPE/csm.git Change the password in the customizations.yaml file.\nThe Gitea crayvcs password is stored in the vcs-user-credentials Kubernetes Secret in the services namespace. This must be updated so that clients which need to make requests can authenticate with the new password.\nIn the customizations.yaml file, set the values for the gitea keys in the spec.kubernetes.sealed_secrets field. The value in the data element where the name is password needs to be changed to the new Gitea password. The section below will replace the existing sealed secret data in the customizations.yaml file.\nFor example:\ngitea: generate: name: vcs-user-credentials data: - type: static args: name: vcs_password value: my_secret_password - type: static args: name: vcs_username value: crayvcs Encrypt the values after changing the customizations.yaml file.\nThis command makes use of the $CUSTOMIZATIONS variable that is set earlier in the Redeploying a Chart procedure.\n./utils/secrets-seed-customizations.sh \u0026#34;${CUSTOMIZATIONS}\u0026#34; If the above command complains that it cannot find certs/sealed_secrets.crt, then run the following commands to create it:\nmkdir -pv ./certs \u0026amp;\u0026amp; ./utils/bin/linux/kubeseal --controller-name sealed-secrets --fetch-cert \u0026gt; ./certs/sealed_secrets.crt When reaching the step to validate that the redeploy was successful, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nVerify that the Secret has been updated.\nGive the SealedSecret controller a few seconds to update the Secret, then run the following command to see the current value of the Secret:\nncn-mw# kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode Run a VCS health check.\n/usr/local/bin/cmsdev test -q vcs For more details on this test, including known issues and other command line options, see Software Management Services health checks.\nMake sure to perform the entire linked procedure, including the step to save the updated customizations.\nAccess the cray Gitea organization The VCS UI uses Keycloak to authenticate users on the system. However, users from external authentication sources are not automatically associated with permissions in the cray Gitea organization. As a result, users configured via Keycloak can log in and create organizations and repositories of their own, but they cannot modify the cray organization that is created during system installation, unless they are given permissions to do so.\nThe crayvcs Gitea admin user that is created during CSM installation can log in to the UI via Keycloak. To allow users other than crayvcs to have access to repositories in the cray organization, use the following procedure:\nLog in to VCS as the crayvcs user on the system.\nURL: https://vcs.SHASTA_CLUSTER_DNS_NAME\nNavigate to the cray organization owners page.\nURL: https://vcs.SHASTA_CLUSTER_DNS_NAME/vcs/cray/teams/owners\nEnter the username of the user who should have access to the organization in the Search user... text field, and click the Add Team Member button.\nIMPORTANT The Owner role has full access to all repositories in the organization, as well as administrative access to the organization, including the ability to create and delete repositories.\nFor granting non-administrative access to the organization and its repositories, create a new team at the following URL:\nhttps://vcs.SHASTA_CLUSTER_DNS_NAME/vcs/org/cray/teams/new\nSelect the permissions appropriately, and then navigate to the following URL to add members to the newly created team:\nhttps://vcs.SHASTA_CLUSTER_DNS_NAME/vcs/org/cray/teams/NEWTEAM\nBackup and restore data Data for Gitea is stored in two places: Git content is stored directly in a PVC, while structural data, such as Gitea users and the list and attributes of repositories, is stored in a Postgres database. Because of this, both sources must be backed up and restored together.\nBackup Postgres data Determine which Postgres member is the leader.\nncn-mw# kubectl exec gitea-vcs-postgres-0 -n services -c postgres -it -- patronictl list Example output:\n+ Cluster: gitea-vcs-postgres (6995618180238446669) -----+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +----------------------+--------------+--------+---------+----+-----------+ | gitea-vcs-postgres-0 | 10.45.0.21 | Leader | running | 1 | | | gitea-vcs-postgres-1 | 10.46.128.19 | | running | 1 | 0 | | gitea-vcs-postgres-2 | 10.47.0.21 | | running | 1 | 0 | +----------------------+--------------+--------+---------+----+-----------+ Log into the leader pod and dump the data to a local file.\nncn-mw# POSTGRES_LEADER=gitea-vcs-postgres-0 ncn-mw# kubectl exec -it ${POSTGRES_LEADER} -n services -c postgres -- pg_dumpall -c -U postgres \u0026gt; gitea-vcs-postgres.sql Determine what secrets are associated with the PostgreSQL credentials:\nncn-mw# kubectl get secrets -n services | grep gitea-vcs-postgres.credentials Example output:\npostgres.gitea-vcs-postgres.credentials Opaque 2 13d service-account.gitea-vcs-postgres.credentials Opaque 2 13d standby.gitea-vcs-postgres.credentials Opaque 2 13d Export each secret to a manifest file.\nThe creationTimestamp, resourceVersion, selfLink, and uid metadata fields are removed from each secret as they are exported.\nncn-mw# SECRETS=\u0026#34;postgres service-account standby\u0026#34; ncn-mw# tmpfile=$(mktemp) ncn-mw# echo \u0026#34;---\u0026#34; \u0026gt; gitea-vcs-postgres.manifest \u0026amp;\u0026amp; for secret in $SECRETS; do kubectl get secret \u0026#34;${secret}.gitea-vcs-postgres.credentials\u0026#34; -n services -o yaml \u0026gt; \u0026#34;${tmpfile}\u0026#34; for field in creationTimestamp resourceVersion selfLink uid; do yq d -i \u0026#34;${tmpfile}\u0026#34; \u0026#34;metadata.${field}\u0026#34; done cat \u0026#34;${tmpfile}\u0026#34; \u0026gt;\u0026gt; gitea-vcs-postgres.manifest echo \u0026#34;---\u0026#34; \u0026gt;\u0026gt; gitea-vcs-postgres.manifest done ; rm \u0026#34;${tmpfile}\u0026#34; Copy all files to a safe location.\nBackup PVC data The VCS Postgres backups should be accompanied by backups of the VCS PVC. The export process can be run at any time while the service is running using the following commands:\nncn-mw# POD=$(kubectl -n services get pod -l app.kubernetes.io/instance=gitea -o json | jq -r \u0026#39;.items[] | .metadata.name\u0026#39;) ncn-mw# kubectl -n services exec ${POD} -- tar -cvf /tmp/vcs.tar /var/lib/gitea/ \u0026amp;\u0026amp; kubectl -n services cp ${POD}:/tmp/vcs.tar ./vcs.tar Be sure to save the resulting tar file to a safe location.\nRestore Postgres data See Restore Postgres for VCS.\nRestore PVC data When restoring the VCS Postgres database, the PVC should also be restored to the same point in time. The restore process can be run at any time while the service is running using the following commands:\nncn-mw# POD=$(kubectl -n services get pod -l app.kubernetes.io/instance=gitea -o json | jq -r \u0026#39;.items[] | .metadata.name\u0026#39;) ncn-mw# kubectl -n services cp ./vcs.tar ${POD}:/tmp/vcs.tar \u0026amp;\u0026amp; kubectl -n services exec ${POD} -- tar -C / -xvf /tmp/vcs.tar \u0026amp;\u0026amp; kubectl -n services rollout restart deployment gitea-vcs Alternative backup/restore strategy An alternative to the separate backups of the Postgres and PVC data is to backup the Git data. This has the advantage that only one backup is needed and that the Git backups can be imported into any Git server, not just Gitea. This has the disadvantage that some information about the Gitea deployment is lost (such as user and organization information) and may need to be recreated manually if the VCS deployment is lost.\nThe following scripts create and use a vcs-content directory that contains all Git data. This should be copied to a safe location after export, and moved back to the system before import.\nAlternative export method WARNING: The following example uses the VCS admin username and password in plaintext on the command line, meaning it will be stored in the shell history as well as be visible to all users on the system in the process table. These dangers can be avoided by modifying or replacing the curl command (such as supplying the credentials to curl using the --netrc-file argument instead of the --user argument, or replacing it with a simple Python script).\nUse the following commands to do the export:\nncn-mw# RESULTS=vcs-content ncn-mw# mkdir $RESULTS ncn-mw# VCS_USER=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_username}} | base64 --decode) ncn-mw# VCS_PASSWORD=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode) ncn-mw# git config --global credential.helper store ncn-mw# echo \u0026#34;https://${VCS_USER}:${VCS_PASSWORD}@api-gw-service-nmn.local\u0026#34; \u0026gt; ~/.git-credentials ncn-mw# for repo in $(curl -s https://api-gw-service-nmn.local/vcs/api/v1/orgs/cray/repos --user ${VCS_USER}:${VCS_PASSWORD}| jq -r \u0026#39;.[] | .name\u0026#39;) ; do git clone --mirror https://api-gw-service-nmn.local/vcs/cray/${repo}.git cd ${repo}.git git bundle create ${repo}.bundle --all cp ${repo}.bundle ../$RESULTS cd .. rm -r $repo.git done Alternative import method Use the following commands to do the import:\nncn-mw# SOURCE=vcs-content ncn-mw# VCS_USER=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_username}} | base64 --decode) ncn-mw# VCS_PASSWORD=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode) ncn-mw# git config --global credential.helper store ncn-mw# echo \u0026#34;https://${VCS_USER}:${VCS_PASSWORD}@api-gw-service-nmn.local\u0026#34; \u0026gt; ~/.git-credentials ncn-mw# for file in $(ls $SOURCE); do repo=$(echo $file | sed \u0026#39;s/.bundle$//\u0026#39;) git clone --mirror ${SOURCE}/${repo}.bundle cd ${repo}.git git remote set-url origin https://api-gw-service-nmn.local/vcs/cray/${repo}.git git push cd .. rm -r ${repo}.git done Prior to import, the repository structure may need to be recreated if it has not already been by an install.\nAdjust the repository list as necessary, if any additional repositories are present. Repository settings such as public or private will also need to be manually set, if applicable.\nFor example:\nWARNING: The following example uses the VCS admin username and password in plaintext on the command line, meaning it will be stored in the shell history as well as be visible to all users on the system in the process table. These dangers can be avoided by modifying or replacing the curl command (such as supplying the credentials to curl using the --netrc-file argument instead of the --user argument, or replacing it with a simple Python script).\nncn-mw# VCS_USER=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_username}} | base64 --decode) ncn-mw# VCS_PASSWORD=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode) ncn-mw# REPOS=\u0026#34;analytics-config-management cos-config-management cpe-config-management slurm-config-management sma-config-management uan-config-management csm-config-management\u0026#34; ncn-mw# for repo in $REPOS ; do curl -X POST https://api-gw-service-nmn.local/vcs/api/v1/orgs/cray/repos -u ${VCS_USER}:${VCS_PASSWORD} -d name=${repo} done "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/troubleshoot_uan_boot_issues/",
	"title": "Troubleshoot UAN Boot Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAN Boot Issues Use this topic to guide troubleshooting of User Access Node (UAN) boot issues.\n[UAN boot process](#uan-boot process) PXE issues initrd (dracut) issues Image boot issues UAN boot process The Boot Orchestration Service (BOS) boots UANs. BOS uses session templates to define various parameters such as:\nWhich nodes to boot Which image to boot Kernel parameters Whether to perform post-boot configuration of the nodes by the Configuration Framework Service (CFS); this is also referred to as Node Personalization. Which CFS configuration to use if Node Personalization is enabled. UAN boots are performed in three phases:\nPXE booting an iPXE binary that will load the initrd of the chosen UAN image to boot. Booting the initrd (dracut) image which configures the UAN for booting the UAN image. This process consists of two phases: Configuring the UAN node to use the Content Projection Service (CPS) and Data Virtualization Service (DVS). These services manage the UAN image rootfs mounting and make that image available to the UANs. Mounting the rootfs. Booting the UAN image rootfs. PXE issues Most failures to PXE are the result of misconfigured network switches and/or BIOS settings. The UAN must PXE boot over the Node Management Network (NMN) and the switches must be configured to allow connectivity to the NMN. The cable for the NMN must be connected to the first port of the OCP card,on HPE DL325 and DL385 nodes, or to the first port of the built-in LAN-On-Motherboard (LOM), on Gigabyte nodes.\ninitrd (dracut) issues Failures in dracut are often caused by the wrong interface being named nmn0, or by having multiple entries for the UAN component name (xname) in DNS. The latter is a result of multiple interfaces making DHCP requests. Either condition can cause IP address mismatches in the dvs_node_map. DNS configures entries based on DHCP leases.\nWhen dracut starts, it renames the network device named by the ifmap=netX:nmn0 kernel parameter to nmn0. This interface is the only one dracut will enable DHCP on. The ip=nmn0:dhcp kernel parameter limits dracut to DHCP only nmn0. The ifmap value must be set correctly in the kernel_parameters field of the BOS session template.\nFor UAN nodes that have more than one PCI card installed, ifmap=net2:nmn0 is the correct setting. If only one PCI card is installed, ifmap=net0:nmn0 is normally the correct setting.\nUANs require CPS and DVS to boot from images. These services are configured in dracut to retrieve the rootfs and mount it. If the image fails to download, check that DVS and CPS are both healthy, and that DVS is running on all worker nodes.\nRun the following commands to check DVS and CPS:\nncn-mw# kubectl get nodes -l cps-pm-node=True -o custom-columns=\u0026#34;:metadata.name\u0026#34; --no-headers Example output:\nncn-w001 ncn-w002 ncn-mw# for node in `kubectl get nodes -l cps-pm-node=True -o custom-columns=\u0026#34;:metadata.name\u0026#34; --no-headers`; do ssh $node \u0026#34;lsmod | grep \u0026#39;^dvs \u0026#39;\u0026#34; done Example output:\nncn-w001 ncn-w002 If DVS and CPS are both healthy, then both of these commands will return all the worker Non-Compute Nodes (NCNs) in the HPE Cray EX system.\nImage boot issues Once dracut exits, the UAN will boot the rootfs image. Failures seen in this phase tend to be failures of spire-agent, cfs-state-reporter, or both, to start. The cfs-state-reporter tells CFS that the node is ready and allows CFS to start node personalization. If cfs-state-reporter does not start, check if the spire-agent has started. The cfs-state-reporter depends on the spire-agent. Running systemctl status spire-agent will show that that service is enabled and running if there are no issues with that service. Similarly, running systemctl status cfs-state-reporter will show a status of SUCCESS.\nVerify that the spire-agent service is enabled and running.\nuan# systemctl status spire-agent Example output:\n● spire-agent.service - SPIRE Agent Loaded: loaded (/usr/lib/systemd/system/spire-agent.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2021-02-24 14:27:33 CST; 19h ago Main PID: 3581 (spire-agent) Tasks: 57 CGroup: /system.slice/spire-agent.service └─3581 /usr/bin/spire-agent run -expandEnv -config /root/spire/conf/spire-agent.conf Verify that cfs-state-reporter is healthy and returns SUCCESS.\nuan# systemctl status cfs-state-reporter Example output:\n● cfs-state-reporter.service - cfs-state-reporter reports configuration level of the system Loaded: loaded (/usr/lib/systemd/system/cfs-state-reporter.service; enabled; vendor preset: enabled) Active: inactive (dead) since Wed 2021-02-24 14:29:51 CST; 19h ago Main PID: 3827 (code=exited, status=0/SUCCESS) "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/obtain_configuration_of_a_uas_volume/",
	"title": "Obtain the Configuration of a UAS Volume",
	"tags": [],
	"description": "",
	"content": "Obtain the Configuration of a UAS Volume View the configuration information of a specific UAS volume. This procedure requires the volume_ID of that volume.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Volume ID of the volume to be retrieved: List Volumes Registered in UAS Procedure View the configuration of a specific UAS volume.\nThis command returns output in TOML format by default. JSON or YAML formatted output can be obtained by using the --format json or --format yaml options respectively.\nncn-m001-pit# cray uas admin config volumes describe 11a4a22a-9644-4529-9434-d296eef2dc48 --format json Example output:\n{ \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;11a4a22a-9644-4529-9434-d296eef2dc48\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; } Top: User Access Service (UAS)\nNext Topic: Update a UAS Volume\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/pki_services/",
	"title": "PKI Services",
	"tags": [],
	"description": "",
	"content": "PKI Services The services in this section are integral parts of the Public Key Infrastructure (PKI) implementation.\nHashiCorp Vault Jetstack Cert-manager TrustedCerts Operator HashiCorp Vault A deployment of HashiCorp Vault, managed via the Bitnami Bank-vaults operator, stores private and public Certificate Authority (CA) material, and serves APIs through a PKI engine instance. This instance also serves as a general secrets engine for the system.\nKubernetes service account authorization is utilized to authenticate access to Vault. The configuration of Vault, as deployed on the system, can be viewed with the following command:\nncn-mw# kubectl get vault -n vault cray-vault -o yaml A Kubernetes operator manages the deployment of Vault, based on this definition. The resulting instance is deployed to the vault namespace.\nIMPORTANT: Changing the cray-vault custom resource definition is not supported unless directed by customer support.\nFor more information, refer to the following resources:\nHashiCorp Vault Bank-vault external documentation Vault external documentation Jetstack Cert-manager A deployment of Jetstack Cert-manager provides a Kubernetes-native API to request x.509 certificates and perform key management operations.\nCert-manager is integrated with HashiCorp Vault for use as a CA. Cert-manager generates key material and a certificate signing request (CSR), and then submits the CSR to Vault for signature. Once Vault has signed the certificate, it is made available, along with other key materials, via a Kubernetes Secret. Kubernetes pods or other platform-aware components can then source the resulting secret.\nCert-manager will also automatically manage renewal of certificates prior to their expiration time. Cert-manager is deployed on the system using namespace-specific certificate issuers.\nTo view issuers:\nncn-mw# kubectl get issuer -A -o wide To view certificates:\nncn-mw# kubectl get certificate -A -o wide Once a certificate is ready, the resulting secret will contain the following data fields:\nField Description ca.crt Contains trusted CA certificates tls.crt Contains the generated certificate, along with trusted CA certificates in the trust chain tls.key Contains the private key To view certificate signing requests:\nncn-mw# kubectl get certificaterequest -A -o wide The Cert-manager workload is deployed to the cert-manager namespace.\nFor more information, see the Cert-manager external documentation.\nTrustedCerts Operator The TrustedCerts Operator is an HPE Kubernetes Operator. It acts on the TrustedCertificates custom resource definitions. Its function is to source CA certificates via use of a Vault API, and then distribute them.\nTo see the deployed TrustedCertificates resources:\nncn-mw# kubectl get trustedcertificates -A These resources can be used to further examine the ConfigMap and Boot Script Service (BSS) destination references. The TrustedCerts workload is deployed to the pki-operator namespace.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/ncn_identify_drives_using_ledctl/",
	"title": "NCN Drive Identification",
	"tags": [],
	"description": "",
	"content": "NCN Drive Identification Basic usage for the ledmon/ledctl software for drive identification using the drive LEDs.\nUsage Turn on led locator beacon\nledctl locate=/dev/\u0026lt;drive\u0026gt; Turn off led locator beacon\nledctl locate_off=/dev/\u0026lt;drive\u0026gt; "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/network_naming_function/",
	"title": "Network types – Naming and segment Function",
	"tags": [],
	"description": "",
	"content": "Network types – Naming and segment Function Description\nIn below you can find the overview of the different networks services defined inside of our spine and leaf architecture.\n*********** Administration: Hardware Administration: Cloud/Job Customer: Jobs Customer: Administration Storage Full name Hardware Management Network Node Management Network Customer Access Network Customer Management Network Storage User Network Short name / acronym HMN NMN CAN CMN SUN Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/mclag_split/",
	"title": "VSX Split",
	"tags": [],
	"description": "",
	"content": "VSX: Split The intent here is to showcase a complete inter-switch-link (ISL) link failure scenario where both of the ISL links between spine switches goes down.\nThe following is a visualization of a disconnected ISL link and how the traffic pattern would look:\nThe following is expected in this scenario:\nAfter disconnecting both ISL Links and Keepalive is up and properly configured the VSX Secondary Switch should put all its MCLAGs into lacp-blocked state and traffic should only flow through VSX Primary. VSX Primary switch should continue to operate without any problems. If traffic was originally flowing through secondary VSX member a small percentage of packets may be dropped when disconnecting the ISL. A sub second value is expected during this event. When connecting back ISL link, the hashing needs to be recalculated and some packets may be dropped during this event as well. A sub second value is expected during this event. Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/view_configuration_session_logs/",
	"title": "View Configuration Session Logs",
	"tags": [],
	"description": "",
	"content": "View Configuration Session Logs Logs for the individual steps of a session are available via the kubectl log command for each container of a Configuration Framework Service (CFS) session. Refer to Configuration Sessions for more info about these containers.\nTo find the name of the Kubernetes pod that is running the CFS session:\nncn-mw# kubectl get pods --no-headers -o custom-columns=\u0026#34;:metadata.name\u0026#34; -n services -l cfsession=example Store the returned pod name as the CFS_POD_NAME variable for future use:\nncn-mw# CFS_POD_NAME=cfs-f9d18751-e6d1-4326-bf76-434293a7b1c5-q8tsc Alternatively, if the session is one of many recent sessions and the session name is not known, it is possible to list all CFS pods by start time and pick the desired pod based on status or start time:\nncn-mw# kubectl -n services --sort-by=.metadata.creationTimestamp get pods | grep cfs Example output:\ncfs-47bed8b5-e1b1-4dd7-b71c-40e9750d3183-7msmr 0/7 Completed 0 36m cfs-0675d19f-5bec-424a-b0e1-9d466299aff5-dtwhl 0/7 Error 0 5m25s cfs-f49af8e9-b8ab-4cbb-a4f6-febe519ef65f-nw76v 0/7 Error 0 4m14s cfs-31635b42-6d03-4972-9eba-b011baf9c5c2-jmdjx 6/7 NotReady 0 3m33s cfs-b9f50fbe-04de-4d9a-b5eb-c75d2d561221-dhgg6 6/7 NotReady 0 2m10s To view the logs of the various containers:\nncn-mw# kubectl logs -n services ${CFS_POD_NAME} -c ${CONTAINER_NAME} The ${CONTAINER_NAME} value is one of the containers mentioned in Configuration Sessions. Depending on the number of configuration layers in the session, some sessions will have more containers available. Use the -f option in the previous command to follow the logs if the session is still running.\nTo view the Ansible logs, determine which configuration layer\u0026rsquo;s logs to view from the order of the configuration set in the session. For example, if it is the first layer, then the ${CONTAINER_NAME} will be ansible-0.\nncn-mw# kubectl logs -n services ${CFS_POD_NAME} -c ansible-0 The git-clone-# and ansible-# containers may not start at 0 and may not be numbered sequentially if the session was created with the --configuration-limit option.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/upload_node_boot_information_to_boot_script_service_bss/",
	"title": "Upload Node Boot Information to Boot Script Service (BSS)",
	"tags": [],
	"description": "",
	"content": "Upload Node Boot Information to Boot Script Service (BSS) The following information must be uploaded to BSS as a prerequisite to booting a node via iPXE:\nThe location of an initrd image in the artifact repository The location of a kernel image in the artifact repository Kernel boot parameters The nodes associated with that information, using either host name or node ID (NID) BSS manages the iPXE boot scripts that coordinate the boot process for nodes, and it enables basic association of boot scripts with nodes. The boot scripts supply a booting node with a pointer to the necessary images (kernel and initrd) and a set of boot-time parameters.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. The Boot Script Service (BSS) is running. An initrd image and kernel image for one or more nodes have been uploaded to the artifact repository. See Manage Artifacts with the Cray CLI. Procedure Because the parameters that must be specified in the PUT command are lengthy, this procedure shows a simple Bash script (not to be confused with iPXE boot scripts) to enter the boot information into BSS. The first step creates a script that can use either NID or host name to identify the nodes with which to associate the boot information.\nCreate a Bash script to enter the following boot information into BSS in preparation for booting one or more nodes.\nNCN = the host name of a non-compute node (NCN) that is a Kubernetes master node. This procedure uses api-gw-service-nmn.local, the API service name on the Node Management Network (NMN). For more information, see Access to System Management Services.\nKERNEL = the download URL of the kernel image artifact that was uploaded to S3, which is in the s3://s3_BUCKET/S3_OBJECT_KEY/kernel format.\nINITRD = the download URL of the initrd image artifact that was uploaded to S3, which is in the s3://s3_BUCKET/S3_OBJECT_KEY/initrd format.\nPARAMS = the boot kernel parameters.\nIMPORTANT: The PARAMS line must always include the substring crashkernel=360M. This enables node dumps, which are needed to troubleshoot node crashes.\nNIDS = a list of node IDs of the nodes to be booted.\nHOSTS = a list of strings identifying by host name the nodes to be booted.\nThe following script is generic. A script with specific values is below this one.\n#!/bin/bash NCN=api-gw-service-nmn.local KERNEL=s3://S3_BUCKET/S3_OBJECT_KEY/initrd INITRD=s3://S3_BUCKET/S3_OBJECT_KEY/kernel PARAMS=\u0026#34;STRING_WITH_BOOT_PARAMETERS crashkernel=360M\u0026#34; # # By NID NIDS=NID1,NID2,NID3 cray bss bootparameters create --nids $NIDS --kernel $KERNEL --initrd $INITRD --params $PARAMS # # By host name #HOSTS=\u0026#34;STRING_IDENTIFYING_HOST1\u0026#34;,\u0026#34;STRING_IDENTIFYING_HOST2\u0026#34; #cray bss bootparameters create --hosts $HOSTS --kernel $KERNEL --initrd $INITRD --params $PARAMS BSS supports a mechanism that allows for a default boot setup, rather than needing to specify boot details for each specific node. The HOSTS value should be set to \u0026ldquo;Default\u0026rdquo; in order to utilize the default boot setup. This feature is particular useful with larger systems.\nThe following script has specific values for the kernel/initrd image names, the kernel parameters, and the list of NIDs and hosts.\n#!/bin/bash NCN=api-gw-service-nmn.local KERNEL=s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/kernel INITRD=s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/initrd PARAMS=\u0026#34;console=ttyS0,115200n8 console=tty0 initrd=97b548b9-2ea9-45c9-95ba-dfc77e5522eb root=nfs:$NCN:/var/lib/nfsroot/cmp000001_image rw nofb selinux=0 rd.net.timeout.carrier=20 crashkernel=360M\u0026#34; PARAMS=\u0026#34;console=ttyS0,115200n8 console=tty0 initrd=${INITRD##*/} \\ root=nfs:10.2.0.1:$NFS_IMAGE_ROOT_DIR rw nofb selinux=0 rd.shell crashkernel=360M \\ ip=dhcp rd.neednet=1 htburl=https://10.2.100.50/apis/hbtd/hmi/v1/heartbeat\u0026#34; # # By NID NIDS=1 cray bss bootparameters create --nids $NIDS --kernel $KERNEL --initrd $INITRD --params $PARAMS # # By host name #HOSTS=\u0026#34;nid000001-nmn\u0026#34; #cray bss bootparameters create --hosts $HOSTS --kernel $KERNEL --initrd $INITRD --params $PARAMS Run the script to upload the boot information to BSS for the identified nodes.\nncn-mw# chmod +x script.sh \u0026amp;\u0026amp; ./script.sh View the boot script in BSS.\nThis will show the specific boot script that will be passed to a given node when requesting a boot script. This is useful for debugging boot problems and to verify that BSS is configured correctly.\nncn-mw# cray bss bootscript list --nid NODE_ID Confirm that the information has been uploaded to BSS.\nIf nodes identified by host name:\nncn-mw# cray bss bootparameters list --hosts HOST_NAME For example:\nncn-mw# cray bss bootparameters list --hosts Default If nodes identified by NID:\nncn-mw# cray bss bootparameters list --nids NODE_ID For example:\nncn-mw# cray bss bootparameters list --nids 1 View entire contents of BSS, if desired.\nncn-mw# cray bss dumpstate list To view the information retrieved from the HSM:\nncn-mw# cray bss hosts list To view the view the configured boot parameter information:\nncn-mw# cray bss bootparameters list Boot information has been added to BSS in preparation for iPXE booting all nodes in the list of host names or NIDs.\nAs part of power up the nodes in the host name or NID list, the next step is to reboot the nodes.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/register_a_uai_image/",
	"title": "Register a UAI Image",
	"tags": [],
	"description": "",
	"content": "Register a UAI Image Register a UAI image with UAS. Registration tells UAS where to locate the image and whether to use the image as the default for UAIs.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The UAI image must be created and uploaded to the container registry: Customize End-User UAI Images Procedure Register a UAI image with UAS.\nThe following is the minimum required CLI command form:\nncn-m001-pit# cray uas admin config images create --imagename IMAGE_NAME In this example, IMAGE_NAME is the full name of the image, including registry host and version tag, to be registered.\nThe following example registers a UAI image stored in the registry.local registry as registry.local/cray/custom-end-user-uai:latest. This example also explicitly sets the default attribute to true because the --default yes option is used in the command.\nncn-m001-pit# cray uas admin config images create --imagename registry.local/cray/custom-end-user-uai:latest --default yes To register the image explicitly as non-default:\nncn-m001-pit# cray uas admin config images create --imagename registry.local/cray/custom-end-user-uai:latest --default no Registering an image with the --default no option is usually unnecessary. Omitting the --default option causes UAS to set the default attribute as false. So, the following command would be equivalent to the previous command:\nncn-m001-pit# cray uas admin config images create --imagename registry.local/cray/custom-end-user-uai:latest Top: User Access Service (UAS)\nNext Topic: Retrieve UAI Image Registration Information\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/preserve_username_capitalization_for_users_exported_from_keycloak/",
	"title": "Preserve Username Capitalization for Users Exported from Keycloak",
	"tags": [],
	"description": "",
	"content": "Preserve Username Capitalization for Users Exported from Keycloak Keycloak converts all characters in a username to lowercase when users are exported. Use this procedure to update the keycloak-users-localize tool with a configuration option that enables administrators to preserve the username letter case when users are exported from Keycloak.\nThe LDAP server that provides password resolution and user account federation supports mixed case usernames. If the usernames are changed to lowercase when exported from Keycloak, it can cause issues.\nPrerequisites The kubectl command is installed. Each user\u0026rsquo;s homeDirectory attribute has the exact username as the last element of the path. Procedure Update the user export setting in the customizations.yaml file.\nSet the userExportNameSource field to homeDirectory in the spec.kubernetes.services.cray-keycloak-users-localize field in the customizations.yaml file.\nncn-mw# vi customizations.yaml Re-apply the cray-keycloak-users-localize Helm chart with the updated customizations.yaml file.\nUpload the modified customizations.yaml file to Kubernetes.\nncn-mw# kubectl delete secret -n loftsman site-init ncn-mw# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/node_management/",
	"title": "Node Management",
	"tags": [],
	"description": "",
	"content": "Node Management The HPE Cray EX systems include two node types:\nCompute Nodes that run high-performance computing applications and are named nidXXXXXX. Every system must contain four or more compute nodes, starting at nid000001. Non-Compute Nodes (NCNs) that carry out system management functions as part of the management Kubernetes cluster. NCNs outside of the Kubernetes cluster function as application nodes (AN). Nine or more management NCNs host system services:\nncn-m001, ncn-m002, and ncn-m003 are Kubernetes master nodes. ncn-w001, ncn-w002, and ncn-w003 are Kubernetes worker nodes. Every system contains three or more worker nodes. ncn-s001, ncn-s002, and ncn-s003 are utility storage nodes. Every system contains three or more utility storage nodes. Application nodes (AN) are any NCN that is not providing system management functions. One special type of AN is the UAN (User Access Node), but different systems may have need for other types of ANs, such as:\nNodes which provide a Lustre routing function (LNet router) Gateways between the HSN and other networks Data movers between two different network file systems Visualization servers Other special-purpose nodes "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/network_traffic_pattern/",
	"title": "Network traffic pattern inside of the system",
	"tags": [],
	"description": "",
	"content": "Network traffic pattern inside of the system Internal Networks:\nNode Management Network (NMN) - Provides the internal control plane for systems management and jobs control. Hardware Management Network (HMN) - Provides internal access to system baseboard management controllers (BMC/iLO) and other lower-level hardware access. External and Edge Networks:\nCustomer Management Network (CMN) - Provides customer access from the Site to the System for administrators. Customer Access Network (CAN) or Customer High Speed Network (CHN) provide: Customer access from the site to the System for job control and jobs data movement. Access from the System to the Site for network services like DNS, LDAP, etc. Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/mlag/",
	"title": "Multi-Chassis Link Aggregation Group (MCLAG)",
	"tags": [],
	"description": "",
	"content": "Multi-Chassis Link Aggregation Group (MCLAG) Multi-Chassis Link Aggregation Group (MCLAG) is a link aggregation technique where two or more links across two switches are aggregated together to form a trunk.\nConfiguration Commands Create the MCLAG interface:\nswitch(config)# interface lag LAG multi-chassis switch(config-lag-if)# no shutdown Associate member links with the MCLAG interface:\nswitch(config)# interface IFACE switch(config-if)# no shutdown switch(config-if)# lag LAG Show commands to validate functionality:\nswitch# show mclag \u0026lt;brief|configuration|status\u0026gt; Example Output switch(config)# interface lag 23 multi-chassis switch(config-lag-if)# no shutdown switch(config-lag-if)# exit switch(config)# interface 1/1/10 switch(config-if)# no shutdown switch(config-if)# lag 23 switch(config-if)# end Expected Results Administrators can configure MCLAG Administrators can create an MCLAG interface Administrators can add ports to the MCLAG interface The output of the show commands is correct Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/write_ansible_code_for_cfs/",
	"title": "Write Ansible Code for CFS",
	"tags": [],
	"description": "",
	"content": "Write Ansible Code for CFS HPE Cray provides Ansible plays and roles for software products deemed necessary for the system to function. Customers are free to write their own Ansible plays and roles to augment what HPE Cray provides or implement new features. Basic knowledge of Ansible is needed to write plays and roles. The information below includes recommendations and best practices for writing and running Ansible code on the system successfully with the Configuration Framework Service (CFS).\nHelp with Ansible can be found in the external Ansible documentation:\nAnsible playbook best practices Ansible examples Ansible code structure The Version Control Service (VCS) is setup during the Cray System Management (CSM) product installation, which is the appropriate place to store configuration content. Individual product installations include the Ansible code to properly configure each product.\nThe structure of the individual repository directories matches the recommended directory layout from the external Ansible documentation. Playbooks are found at the top level, and the Ansible roles and variables are in their appropriate directories. Inventory directories like group_vars and host_vars may exist, but they are empty and left for variable overrides and customizations by the user.\nTargeting specific node types with playbooks Within an Ansible playbook, users can designate which node groups the various tasks and roles will run against. For example, a playbook may contain a list of roles that are run on only the Compute nodes, or a list of roles that will run on only Application nodes. This is designated using the hosts parameter.\nFor example, hosts: Compute would be used to target the compute nodes. Users can create additional sections that target other node types, or adjust the hosts that the included roles will run against as necessary. It is also possible to target multiple groups within a section of a playbook, or to specify complex targets, such as nodes that are in one group and not in another group. The syntax for this is available in the external Ansible documentation. Hosts can also be in more than one group at a time. In this case, Ansible will run all sections that match the node type against the node.\nSee the Ansible Inventory section for more information about groups that are made automatically available through CFS dynamic inventory.\nPerformance and scaling tips CFS will handle scaling up Ansible to run on many hosts, but there are still places where performance can be improved by correctly writing Ansible plays.\nUsing image customization Use image customization when possible; doing so will improve boot times by limiting how many times a task is run. Configuration that is the same for all nodes of the same type will benefit from image customization. Use different playbooks for image customization and node personalization. Moving image customizations tasks to their own playbook can remove the need to evaluate conditionals in a shared playbook, as well as ensuring that tasks are not accidentally running in both modes needlessly. See Target Ansible Tasks for Image Customization for more information on writing for image customization. Disable fact gathering Turn off facts that are not needed in a playbook by setting gather_facts: false. If only a few facts are required, it is also possible to limit fact gathering by setting gather_subset. For more information on gather_subset, see the external Ansible module setup documentation. Avoid importing playbooks in other playbooks. This will trigger fact gathering for each imported playbook, potentially collecting the same information multiple times. Reduce wasted time Use include_* (dynamic re-use) to skip multiple tasks at once when using conditionals. Ansible evaluates conditionals for every node in every task. This includes when the conditional is applied to a block, or a role imported with roles: or the import_role task. This is because these are static imports that are compiled at the beginning of the playbook, and the conditional is inherited by every task in the role or block. Evaluating these conditionals for each task may only take a second or two, but across the hundreds of tasks that might be part of a playbook, this can add up to significant wasted time. Instead use dynamic imports with the include_* tasks. Because these are evaluated at runtime, a conditional can skip the import of the role or tasks entirely, and is only evaluated once. See the Ansible documentation on Conditionals with re-use and Re-using files and roles for more information. Avoid using the same CFS configuration/playbook for all nodes and relying on the hosts keyword to determine what tasks will run against each node. Ansible will skip sections of the playbook that have a hosts target that does not match any nodes in the current inventory/limit, but when multiple types of nodes are configured at the same time with the same configuration, they may end up in the same batch and Ansible run. This would mean that Ansible has to run through the sections for both types of nodes, taking more time than if the nodes were in separate batches and could skip past the unneeded code. Other tips Use loops rather than individual tasks where modules are called multiple times. Some Ansible modules will optimize the command, such as grouping package installations into a single transaction (Refer to the external Ansible playbook loops documentation). Use Ansible retries for small, recoverable failures. CFS supports retries on a large scale, but it takes far more time for CFS to detect a failed component and spin up a new session than it does for Ansible to retry a task. Do not use Ansible retries for failures than take a long time to recover from. Retrying for a significant amount of time on one node can hold up all the other successful nodes in a batch. If you cannot recover from a failure quickly, then let the node fail and CFS will separate it out from the successful nodes when new sessions are started. Avoid any_errors_fatal. In addition to not working with all Ansible strategies, this can cause an Ansible run to exit early, and the nodes that did not have the error will have to start from the beginning of the playbook in the next session. Design playbooks to be run with the free Ansible strategy. This means avoiding situations where all nodes in a batch need to complete a task before moving onto the next, and can save time by allowing nodes to proceed through a playbook at their own pace. Ansible limitations with CFS Because CFS splits components into multiple batches, and components may also configure at different times when they are rebooted, some keywords meant for coordinating the runs of multiple nodes may not work as expected.\nKeyword Notes any_errors_fatal This keyword is intended to stop execution as soon as any node reports a failure. However, this will only stop execution for the current batch. run_once This keyword is intended to limit a task to running on a single node. However this will only cause the task to be run once per batch. serial This keyword is intended to limit runs to a small number of nodes at a time, such as during a rolling upgrade. However, this will only function within the batch, so more nodes may be running the task than intended when multiple batches are running. Selecting an Ansible strategy CFS supports two Ansible strategies: cfs_linear and cfs_free. cfs_linear runs all task in a playbook serially, with all nodes completing a task before Ansible moves on to the next task. cfs_free decouples the nodes allowing each node to proceed through the playbook at its own pace. Switching to cfs_free from the default strategy of cfs_linear may result in better configuration time, however currently not all included playbooks support the cfs_free strategy, so this should only be done when playbooks that are confirmed to work correctly with the cfs_free strategy. In addition, the cfs_free strategy is limited by the fact that configuration in CFS is applied over multiple layers and multiple playbooks. This means that even when using the cfs_free strategy, all nodes must complete a playbook together before moving onto the next playbook.\nThe CFS Ansible strategies extend the similarly named Ansible strategy, adding reporting callbacks that are used to track components\u0026rsquo; state. cfs_linear and cfs_free should always be used in place of linear and free to ensure that CFS functions correctly.\n"
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/view_the_status_of_a_bos_session/",
	"title": "View the Status of a BOS Session",
	"tags": [],
	"description": "",
	"content": "View the Status of a BOS Session The Boot Orchestration Service (BOS) supports a status endpoint that reports the status for individual BOS sessions. The status can be retrieved for each boot set within the session, as well as the individual items within a boot set.\nMetadata View the status of a v1 session View the status of a boot set View the status for an individual phase View the status for an individual category In BOS v1, the status can be retrieved for each boot set within the session, as well as the individual items within a boot set.\nBOS sessions contain one or more boot sets. Each boot set contains one or more phases, depending upon the operation for that session. For example, a reboot operation would have a shutdown, boot, and possibly configuration phase, but a shutdown operation would only have a shutdown phase. Each phase contains the following categories: not_started, in_progress, succeeded, failed, and excluded.\nMetadata Each session, boot set, and phase contains similar metadata. The following is a table of useful attributes to look for in the metadata:\nAttribute Meaning start_time The time when a session, boot set, or phase started work. in_progress If true, it means that the session, boot set, or phase has started and still has work going on. complete If true, it means the session, boot set, or phase has finished. error_count The number of errors encountered in the boot sets or phases. stop_time The time when a session, boot set, or phase ended work. The following table summarizes how to interpret the various combinations of values for the in_progress and complete flags:\nin_progress complete Meaning false false Item has not started. true false Item is in progress. false true Item has completed. true true Invalid state (should not occur). The in_progress, complete, and error_count fields are cumulative, meaning that they summarize the state of the sub-items.\nItem in_progress meaning complete meaning Phase If true, it means there is at least one node in the in_progress category. If true, it means that there are no nodes in the in_progress or not_started categories. Boot set If true, it means there is at least one phase that is in_progress. If true, it means that all phases in the boot set are complete. Session If true, it means that at least one boot set is in_progress. If true, it means that all boot sets are complete. View the status of a v1 session The BOS session ID is required to view the status of a session. To list the available sessions, use the following command:\nNote: If this command fails, there may be too many BOS sessions. For more information, see Hang Listing BOS Sessions.\nncn-mw# cray bos session list --format json Example output:\n[ \u0026#34;99a192c2-050e-41bc-a576-548610851742\u0026#34;, \u0026#34;4374f3e6-e8ed-4e66-bf63-3ebe0e618db2\u0026#34;, \u0026#34;fb14932a-a9b7-41b2-ad21-b4bc632cf1ef\u0026#34;, \u0026#34;9321ab7a-bf7f-42fd-8103-94a296552856\u0026#34;, \u0026#34;50aaaa85-6807-45c7-b6de-f984a930e2eb\u0026#34;, \u0026#34;972cfd09-3403-4282-ab93-b41992f7c0d8\u0026#34;, \u0026#34;2c86c1b9-5281-4610-b044-479f1536727a\u0026#34;, \u0026#34;7719385a-e462-4bb6-8fd8-55caa0836528\u0026#34;, \u0026#34;0aac0252-4637-4198-919f-6bafda7fafef\u0026#34;, \u0026#34;13207c87-0b9f-410c-88c1-6e26ff63cb34\u0026#34;, \u0026#34;bd18e7e3-978f-4699-b8f2-8a4ce2d46f75\u0026#34;, \u0026#34;b741e4de-2064-4de4-9f23-20b6c1d0dc1a\u0026#34;, \u0026#34;f4eebe51-a217-46d0-8733-b9499a092042\u0026#34; ] It is recommended to describe the session using the session ID above to verify the desired selection was selected:\nncn-mw# cray bos session describe SESSION_ID --format toml Example output:\nstatus_link = \u0026#34;/v1/session/f4eebe51-a217-46d0-8733-b9499a092042/status\u0026#34; complete = false start_time = \u0026#34;2020-07-22 13:39:07.706774\u0026#34; templateName = \u0026#34;cle-1.3.0\u0026#34; error_count = 4 boa_job_name = \u0026#34;boa-f4eebe51-a217-46d0-8733-b9499a092042\u0026#34; in_progress = false operation = \u0026#34;reboot\u0026#34; The status for the session will show the session ID, the boot sets in the session, the metadata, and some links. In the following example, there is only one boot set (named computes), and the session ID being used is f4eebe51-a217-46d0-8733-b9499a092042.\nTo display the status for the session:\nncn-mw# cray bos session status list SESSION_ID -–format json Example output:\n{ \u0026#34;boot_sets\u0026#34;: [ \u0026#34;computes\u0026#34; ], \u0026#34;id\u0026#34;: \u0026#34;f4eebe51-a217-46d0-8733-b9499a092042\u0026#34;, \u0026#34;links\u0026#34;: [ { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/f4eebe51-a217-46d0-8733-b9499a092042/status\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;self\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/f4eebe51-a217-46d0-8733-b9499a092042/status/computes\u0026#34; , \u0026#34;rel\u0026#34;: \u0026#34;Boot Set\u0026#34; } ], \u0026#34;metadata\u0026#34;: { \u0026#34;in_progress\u0026#34;: false, \u0026#34;start_time\u0026#34;: \u0026#34;2020-07-22 13:39:07.706774\u0026#34;, \u0026#34;complete\u0026#34;: false, \u0026#34;error_count\u0026#34;: 4 } } View the status of a boot set Run the following command to view the status for a specific boot set in a session. For more information about retrieving the session ID and boot set name, see the View the status of a v1 session section above. Descriptions of the different status sections are described below.\nBoot set The id parameter identifies which session this status belongs to. The name parameter is the name of the boot set. The links section displays links that enable administrators to drill down into each phase of the boot set. There is metadata section for the boot set as a whole. Phases The name parameter is the name of the phase. There is a metadata section for each phase. Each phase contains the following categories: not_started, in_progress, succeeded, failed, and excluded. The nodes are listed in the category they are currently occupying. ncn-mw# cray bos session status describe BOOT_SET_NAME SESSION_ID --format json Example output:\n{ \u0026#34;phases\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;shutdown\u0026#34;, \u0026#34;categories\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;not_started\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x3000c0s19b4n0\u0026#34;, \u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;x3000c0s19b3n0\u0026#34;, \u0026#34;x3000c0s19b2n0\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;excluded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;in_progress\u0026#34;, \u0026#34;node_list\u0026#34;: [] } ], \u0026#34;metadata\u0026#34;: { \u0026#34;stop_time\u0026#34;: \u0026#34;2020-07-22 13:53:19.842705\u0026#34;, \u0026#34;in_progress\u0026#34;: false, \u0026#34;start_time\u0026#34;: \u0026#34;2020-07-22 13:39:08.276530\u0026#34;, \u0026#34;complete\u0026#34;: true, \u0026#34;error_count\u0026#34;: 4 } }, { \u0026#34;name\u0026#34;: \u0026#34;boot\u0026#34;, \u0026#34;categories\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;not_started\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x3000c0s19b4n0\u0026#34;, \u0026#34;x3000c0s19b3n0\u0026#34;, \u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;x3000c0s19b2n0\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;excluded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;in_progress\u0026#34;, \u0026#34;node_list\u0026#34;: [] } ], \u0026#34;metadata\u0026#34;: { \u0026#34;in_progress\u0026#34;: false, \u0026#34;start_time\u0026#34;: \u0026#34;2020-07-22 13:39:08.276542\u0026#34;, \u0026#34;complete\u0026#34;: false, \u0026#34;error_count\u0026#34;: 0 } }, { \u0026#34;name\u0026#34;: \u0026#34;configure\u0026#34;, \u0026#34;categories\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;not_started\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x3000c0s19b4n0\u0026#34;, \u0026#34;x3000c0s19b3n0\u0026#34;, \u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;x3000c0s19b2n0\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;excluded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;in_progress\u0026#34;, \u0026#34;node_list\u0026#34;: [] } ], \u0026#34;metadata\u0026#34;: { \u0026#34;in_progress\u0026#34;: false, \u0026#34;start_time\u0026#34;: \u0026#34;2020-07-22 13:39:08.276552\u0026#34;, \u0026#34;complete\u0026#34;: false, \u0026#34;error_count\u0026#34;: 0 } } ], \u0026#34;session\u0026#34;: \u0026#34;f4eebe51-a217-46d0-8733-b9499a092042\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;computes\u0026#34;, \u0026#34;links\u0026#34;: [ { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/f4eebe51-a217-46d0-8733-b9499a092042/status/computes\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;self\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/f4eebe51-a217-46d0-8733-b9499a092042/status/computes/shutdown\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;Phase\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/f4eebe51-a217-46d0-8733-b9499a092042/status/computes/boot\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;Phase\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/f4eebe51-a217-46d0-8733-b9499a092042/status/computes/configure\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;Phase\u0026#34; } ], \u0026#34;metadata\u0026#34;: { \u0026#34;in_progress\u0026#34;: false, \u0026#34;start_time\u0026#34;: \u0026#34;2020-07-22 13:39:08.276519\u0026#34;, \u0026#34;complete\u0026#34;: false, \u0026#34;error_count\u0026#34;: 4 } } View the status for an individual phase Direct calls to the API are needed to retrieve the status for an individual phase. Support for the Cray CLI is not currently available.\nThe following command is used to view the status of a phase:\nncn-mw# curl -H \u0026#34;Authorization: Bearer BEARER_TOKEN\u0026#34; -X GET https://api-gw-service-nmn.local/apis/bos/v1/session/SESSION_ID/status/BOOT_SET_NAME/PHASE In the following example, the session ID is f89eb554-c733-4197-b2f2-4e1e5ba0c0ec, the boot set name is computes, and the individual phase is shutdown.\nncn-mw# curl -H \u0026#34;Authorization: Bearer BEARER_TOKEN\u0026#34; -X GET https://api-gw-service-nmn.local/apis/bos/v1/session/f89eb554-c733-4197-b2f2-4e1e5ba0c0ec/status/computes/shutdown Example output:\n{ \u0026#34;categories\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;not_started\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;excluded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;in_progress\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x5000c1s2b0n1\u0026#34;, \u0026#34;x5000c1s0b0n0\u0026#34;, \u0026#34;x3000c0s19b4n0\u0026#34;, \u0026#34;x5000c1s0b1n0\u0026#34;, \u0026#34;x5000c1s0b1n1\u0026#34;, \u0026#34;x5000c1s1b1n1\u0026#34;, \u0026#34;x5000c1s2b0n0\u0026#34;, \u0026#34;x3000c0s19b3n0\u0026#34;, \u0026#34;x5000c1s0b0n1\u0026#34;, \u0026#34;x5000c1s2b1n1\u0026#34;, \u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;x5000c1s1b1n0\u0026#34;, \u0026#34;x5000c1s2b1n0\u0026#34;, \u0026#34;x3000c0s19b2n0\u0026#34;, \u0026#34;x5000c1s1b0n1\u0026#34;, \u0026#34;x5000c1s1b0n0\u0026#34; ] } ], \u0026#34;metadata\u0026#34;: { \u0026#34;complete\u0026#34;: false, \u0026#34;error_count\u0026#34;: 0, \u0026#34;in_progress\u0026#34;: true, \u0026#34;start_time\u0026#34;: \u0026#34;2020-06-30 21:42:39.355423\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;shutdown\u0026#34; } View the status for an individual category Direct calls to the API are needed to retrieve the status for an individual category. Support for the Cray CLI is not currently available.\nThe following command is used to view the status of a phase:\nncn-mw# curl -H \u0026#34;Authorization: Bearer BEARER_TOKEN\u0026#34; -X GET https://api-gw-service-nmn.local/apis/bos/v1/session/SESSION_ID/status/BOOT_SET_NAME/PHASE/CATEGORY In the following example, the session ID is f89eb554-c733-4197-b2f2-4e1e5ba0c0ec, the boot set name is computes, the phase is shutdown, and the category is in_progress.\nncn-mw# curl -H \u0026#34;Authorization: Bearer BEARER_TOKEN\u0026#34; -X GET https://api-gw-service-nmn.local/apis/bos/v1/session/f89eb554-c733-4197-b2f2-4e1e5ba0c0ec/status/computes/shutdown/in_progress Example output:\n{ \u0026#34;name\u0026#34;: \u0026#34;in_progress\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x5000c1s2b0n1\u0026#34;, \u0026#34;x5000c1s0b0n0\u0026#34;, \u0026#34;x3000c0s19b4n0\u0026#34;, \u0026#34;x5000c1s0b1n0\u0026#34;, \u0026#34;x5000c1s0b1n1\u0026#34;, \u0026#34;x5000c1s1b1n1\u0026#34;, \u0026#34;x5000c1s2b0n0\u0026#34;, \u0026#34;x3000c0s19b3n0\u0026#34;, \u0026#34;x5000c1s0b0n1\u0026#34;, \u0026#34;x5000c1s2b1n1\u0026#34;, \u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;x5000c1s1b1n0\u0026#34;, \u0026#34;x5000c1s2b1n0\u0026#34;, \u0026#34;x3000c0s19b2n0\u0026#34;, \u0026#34;x5000c1s1b0n1\u0026#34;, \u0026#34;x5000c1s1b0n0\u0026#34; ] } "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/reset_the_uas_configuration_to_original_installed_settings/",
	"title": "Clear UAS Configuration",
	"tags": [],
	"description": "",
	"content": "Clear UAS Configuration WARNING: The procedure described here will remove all UAS configuration including some configuration that is installed upon installation / upgrade of the HPE Cray EX system. If this procedure is used, the update-uas Helm chart must be removed and re-deployed to restore the full HPE provided configuration. This procedure should only be used in an extreme situation where the UAS configuration has become corrupted to the point where it can no longer be managed. All UAS configuration can normally be managed through the cray uas admin config ... commands.\nWARNING: Configuration lost using this procedure is not recoverable.\nHow to remove a customized UAS configuration and restore the base installed configuration.\nThe configuration set up using the Cray CLI to interact with UAS persists as long as UAS remains installed and survives upgrades. This is called the running configuration and it is both persistent and malleable. During installation and localization, however, the installer creates a base installed configuration. It may be necessary to return to this base configuration. To do this, delete the running configuration, which will cause the UAS to reset to the base installed configuration.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) Procedure Delete the running configuration.\nncn-w001 # cray uas admin config delete This will delete all locally applied configuration, Are you sure? [y/N]: Confirm the command. This will delete the running configuration and cannot be undone.\nncn-w001 # cray uas admin config delete This will delete all locally applied configuration, Are you sure? [y/N]: y Alternatively, note that the interactive prompt can be bypassed by supplying the -y option.\nncn-w001 # cray uas admin config delete -y Top: User Access Service (UAS)\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/provisioning_a_liquid-cooled_ex_cabinet_cec_with_default_credentials/",
	"title": "Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials",
	"tags": [],
	"description": "",
	"content": "Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials This procedure provisions a Glibc compatible SHA-512 administrative password hash to a cabinet environmental controller (CEC). This password becomes the Redfish default global credential to access the CMM controllers and node controllers (BMCs).\nThis procedure does not provision Slingshot switch BMCs. Slingshot switch BMC default credentials must be changed using the procedures in the Slingshot product documentation. Refer to \u0026ldquo;Change Rosetta Login and Redfish API Credentials\u0026rdquo; in the Slingshot Operations Guide (\u0026gt; 1.6.0).\nPrerequisites All blades in the cabinet have been powered off.\nPhysical access to the CEC LCD panel to enable privileged command mode. The CEC does not enable users to set, display, or clear the password hash in restricted command mode.\nA laptop with a terminal program such as Netcat (nc), telnet, or PuTTY that supports 10/100 IPv6 Ethernet connectivity to the CEC Ethernet port is required.\nA generated SHA-512 hash for the CEC credentials:\nThe passhash tool that is installed on the CMMs can be used to generate a SHA-512 password hash. This HPE tool is provided for convenience, but any tool that generates an SHA-512 hash that is compatible with glibc can be used. The salt portion must be between 8 and 16 chars inclusive. The CEC does not support the optional \u0026ldquo;rounds=\u0026rdquo; parameter in the hash. See the man 3 crypt page for a description: https://man7.org/linux/man-pages/man3/crypt.3.html remote# passhash PASSWORD $6$v5YlqfghB$scBci.GbT8... Note: The example password hash is truncated to prevent using this example value. The password hash is a SHA-512 hash.\nProcedure Disconnect the CEC Ethernet cable from the Ethernet port.\nConnect an Ethernet cable from an Apple Mac or Linux laptop to the CEC Ethernet port. The CEC Ethernet PHY will auto negotiate to either 10/100Mb speed and it supports auto crossover functionality. Any standard Ethernet patch cord should work for this.\nUse the Right Arrow on the display controls to select the CEC Network Settings Menu. The IPv6 link local address is displayed on this menu.\nStart the terminal program and use Netcat (nc), telnet, or PuTTY to connect to CEC command shell and provide the CEC IPv6 link local address.\n# nc -t -6 \u0026#39;fe80::a1:3e8:0%en14\u0026#39; 23 # telnet fe80::a1:3e8:0%eth0 en14 and eth0 in these examples are the Ethernet interfaces for the laptop.\nEnter return a few times to start the connection.\nNote: If the network connection to the CEC is lost, or if a CEC command does not return to the prompt, it may be necessary to reboot the CEC. Use the Right Arrow on the CEC control panel to display the Action menu, select Reset CEC, and press the green checkmark button to reboot the CEC. Then re-establish the nc or telnet connection.\nFrom the CEC\u0026gt; prompt, enter help to view the list of commands.\nCEC\u0026gt; help Caution: Run only the CEC commands in this procedure. Do not change other CEC settings.\nFrom the CEC\u0026gt; prompt, generate an unlock token for the CEC. Use the enable command (alias for unlock command) without arguments to display a random unlock token on the CEC front panel.\nCEC\u0026gt; enable ab12903c Record the unlock token displayed on the CEC front panel.\nThe unlock code is valid as long as the remote shell connection is open to the CEC. If you enter the unlock token incorrectly, a new unlock token is displayed on the front panel.\nEnter the enable command again but supply the token as an argument to unlock the CEC and enter privileged command mode.\nCEC\u0026gt; enable AB12903C EXE\u0026gt; If the token code is typed in incorrectly a new one is generated on screen. When unlocked, the LCD screen displays UNLOCKED and the shell prompt changes to EXE\u0026gt;.\nDo not use the get_hash command to display the password hash. If there is no password hash set, this command will not return to the prompt and the connection will be lost.\nEnter set_hash and provide the password hash value as the argument.\nThe CEC validates the input syntax of the hash. Adding an extra character or omitting a character is flagged as an error. If a character is changed, the password entered in the serial console login shell or the Redfish root account will not work. If that happens, then rerun this procedure from the beginning.\nEXE\u0026gt; set_hash $6$v5YlqxKB$scBci.GbT8... Note: Example truncated to prevent accidental setting of production password hash to example values. The password hash is a SHA-512 hash.\nExit privileged command mode.\nEXE\u0026gt; lock CEC\u0026gt; The CEC remains in privileged mode until it is reset with the lock command or if the X button on the CEC front panel is pressed. Typing exit or terminating the connection exits privileged mode. There is no connection timeout.\nUse the front panel Right Arrow to select the CEC Action menu.\nReset the CMMs 3, 2, 1, and 0.\nThe Reset CMM commands reboot either the even numbered, or odd numbered CMMs in the cabinet, depending on which CEC is issuing the commands.\nPower cycle the compute blade slots in each chassis.\nSkip this step if the compute blade slots in each chassis have already been powered off.\nTo perform blade power control operations, SSH to a CMM and and use the redfish command to perform the power cycle. This must be done for each populated compute blade in each odd- or even-numbered chassis in the cabinet depending on which CEC issued the reset above.\n\u0026gt; ssh root@x9000c1 x9000c1:\u0026gt; redfish -h \u0026#34;redfish\u0026#34; -- redfish API debugging tool \u0026lt;snip\u0026gt; redfish chassis status redfish chassis power [on|off|forceoff] redfish [blade|perif] [0-7] [on|off|forceoff] redfish node status redfish node [0-1] [on|off|forceoff] \u0026lt;snip\u0026gt; x9000c1:\u0026gt; x9000c1:\u0026gt; redfish blade 0 off x9000c1:\u0026gt; redfish blade 1 off x9000c1:\u0026gt; redfish blade 2 off x9000c1:\u0026gt; redfish blade 3 off x9000c1:\u0026gt; redfish blade 4 off x9000c1:\u0026gt; redfish blade 5 off x9000c1:\u0026gt; redfish blade 6 off x9000c1:\u0026gt; redfish blade 7 off x9000c1:\u0026gt; redfish blade 0 on x9000c1:\u0026gt; redfish blade 1 on x9000c1:\u0026gt; redfish blade 2 on x9000c1:\u0026gt; redfish blade 3 on x9000c1:\u0026gt; redfish blade 4 on x9000c1:\u0026gt; redfish blade 5 on x9000c1:\u0026gt; redfish blade 6 on x9000c1:\u0026gt; redfish blade 7 on x9000c1:\u0026gt; To test the password, connect to the CMM serial console though the CEC. The IPv6 address is the same, but the port numbers are different as described below.\n#!/bin/bash trap \u0026#34;stty sane \u0026amp;\u0026amp; echo \u0026#39;\u0026#39;\u0026#34; EXIT stty -icanon -echo nc -6 \u0026#39;fe80::a1:2328:0%en14\u0026#39; 50000 The even numbered CEC manages the CMM serial console for chassis 0, 2, 4, 6 on TCP port numbers 50000-50003 respectively. The odd numbered CEC manages the CMM serial console for chassis 1, 3, 5, 7 on TCP port numbers 50000-50003 respectively. If using the script shown in the example to connect to the CMM console, type exit to return to the CMM login prompt and enter ctrl-c to close the console connection. Perform this procedure for each CEC in all system cabinets.\nHPE Cray EX3000 and EX4000 cabinets have two CECs per cabinet.\nHPE Cray EX2000 cabinets have a single CEC per cabinet.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/node_management_workflows/",
	"title": "Node Management Workflows",
	"tags": [],
	"description": "",
	"content": "Node Management Workflows The following workflows are intended to be high-level overviews of node management tasks. These workflows depict how services interact with each other during node management and help to provide a quicker and deeper understanding of how the system functions.\nThe workflows and procedures in this section include:\nAdd Nodes Remove Nodes Replace Nodes Move Nodes Add Nodes Add a Standard Rack Node Use Cases: Administrator permanently adds select compute nodes to expand the system.\nComponents: This workflow is based on the interaction of the System Layout Service (SLS) with other hardware management services (HMS).\nMentioned in this workflow:\nSystem Layout Service (SLS) serves as a \u0026ldquo;single source of truth\u0026rdquo; for the system design. It details the physical locations of network hardware, compute nodes and cabinets. Further, it stores information about the network, such as which port on which switch should be connected to each compute node. Hardware State Manager (HSM) monitors and interrogates hardware components in an HPE Cray EX system, tracking hardware state and inventory information, and making it available via REST queries and message bus events when changes occur. HMS Notification Fanout Daemon (hmnfd) receives component state change notifications from the HSM. It fans notifications out to subscribers (typically compute nodes). Endpoint Discovery Service (REDS/MEDS) manages initial discovery, configuration, and geolocation of Redfish-enabled BMCs. It periodically makes Redfish requests to determine if hardware is present or missing. Heartbeat Tracker Service (hbtd) listens for heartbeats from components (mainly compute nodes). It tracks changes in heartbeats and conveys changes to HSM. Workflow Overview: The following sequence of steps occur during this workflow.\nAdministrator updates SLS\nAdministrator creates a new hardware entry for the select component names (xnames) in SLS. Enter the node component names (xnames) in the SLS input file.\nAdministrator adds compute nodes\nThe Administrator physically adds select compute nodes and powers them on. Because the nodes are unknown, the DHCP and TFTP servers give it the special initialization ramdisk. The compute nodes performs local configuration.\nThe following steps (3-11) occur automatically as different APIs interact with each other.\nSet BMC credentials\nThe compute node requests per-node BMC credentials. This message must include the MAC address of the BMC. A new set of credentials is generated by the discovery service.\nOnce the compute node is powered on, initialized, and discovered, REDS gets details about the new node like IP address, MAC address, sets the username and password for a BMC, state etc.\nREDS/MEDS to SLS\nREDS/MEDS query SLS database for information about the new node.\nFor example: \u0026ldquo;What component name (xname) is connected to port XX on switch Y?\u0026rdquo;\nSLS to REDS/MEDS\nSLS updates the discovery service with the new compute node and its component name (xname).\nFor example: \u0026ldquo;xname x0c0\u0026hellip; is connected to port XX\u0026rdquo;.\nREDS/MEDS to HSM\nDiscovery services update HSM about the new Redfish endpoint for the node. Details like component name (xname) and IP address of the new node are updated in HSM.\nFor example: \u0026ldquo;x0c0\u0026hellip; at IP address AAA.BBB.CCC.DDD\u0026rdquo;\nHSM to SLS\nHSM queries SLS for NID and role assignments for the new node.\nSLS to HSM\nHSM updates the Nodemap based on information received from SLS.\nNode to Heartbeat Tracker Service\nThe Heartbeat Tracker Service receives heartbeats from the new compute node after the node is powered on.\nHeartbeat Tracker Service to HSM\nThe Heartbeat Tracker Service report the heartbeat status to HSM.\nHSM to HMNFD\nHSM sends the new compute node state information with State as ON to HMNFD. HMNFD fans out these notifications to the subscribing compute nodes.\nRemove Nodes Use Cases: Administrator permanently removes select compute nodes to contract the system.\nComponents: This workflow is based on the interaction of the System Layout Service (SLS) with other hardware management services (HMS).\nMentioned in this workflow:\nSystem Layout Service (SLS) serves as a \u0026ldquo;single source of truth\u0026rdquo; for the system design. It details the physical locations of network hardware, compute nodes and cabinets. Further, it stores information about the network, such as which port on which switch should be connected to each compute node. Hardware State Manager (HSM) monitors and interrogates hardware components in an HPE Cray EX system, tracking hardware state and inventory information, and making it available via REST queries and message bus events when changes occur. HMS Notification Fanout Daemon (hmnfd) receives component state change notifications from the HSM. It fans notifications out to subscribers (typically compute nodes). Endpoint Discovery Service (REDS/MEDS) manages initial discovery, configuration, and geolocation of Redfish-enabled BMCs. It periodically makes Redfish requests to determine if hardware is present or missing. Heartbeat Tracker Service (hbtd) listens for heartbeats from components (mainly compute nodes). It tracks changes in heartbeats and conveys changes to HSM. Workflow Overview: The following sequence of steps occur during this workflow.\nAdministrator updates SLS\nAdministrator deletes the node entries with the specific component name (xname) from SLS. Note that if deleting a parent object, then the children are also deleted from SLS. If the child object happens to be a parent, then the deletion can cascade down levels. If deleting a child object, it does not affect the parent.\nAdministrator physically removes the compute nodes\nThe Administrator powers off and physically removes the compute nodes.\nThe following steps (3-9) occur automatically as different APIs interact with each other.\nNo heartbeats\nThe Heartbeat Tracker Service stops receiving heartbeats and marks the nodes status as standby and then off as per Redfish event.\nStandby status implies that the node is no longer ready and presumed dead. It typically means that the heartbeat is lost. Off status implies that the location is not populated with a component.\nHeartbeat Tracker Service to HSM\nThe Heartbeat Tracker Service reports the heartbeat status to HSM.\nREDS/MEDS detect no BMC\nThe discovery service detects that the BMC is not there.\nREDS/MEDS to SLS\nREDS/MEDS query SLS database for information about the missing BMCs.\nSLS to REDS/MEDS\nSLS updates the discovery service that the BMC was removed.\nREDS/MEDS to HSM\nDiscovery services update HSM that the BMC Redfish endpoints for the nodes were removed. HSM marks the state of BMCs and the nodes as empty.\nEmpty state implies that the location is not populated with a component.\nHSM to HMNFD\nHSM sends the compute node state information with State as empty to HMNFD. HMNFD fans out this notification to the subscribing compute nodes.\nReplace Nodes Replace a Compute Blade Swap a Compute Blade with a Different System Move Nodes Move a Standard Rack Node Move a Standard Rack Node (Same HSN Ports) "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/ntp/",
	"title": "Network Time Protocol (NTP) Client",
	"tags": [],
	"description": "",
	"content": "Network Time Protocol (NTP) Client Summary of NTP from RFC-1305 Network Time Protocol (Version 3):\nNTP is used to synchronize timekeeping among a set of distributed time servers and clients \u0026hellip; It provides the protocol mechanisms to synchronize time in principle to precisions in the order of nanoseconds while preserving a non-ambiguous date well into the next century.\nThe Network Time Protocol (NTP) client is essential for syncing time on various clients in the system. This document shows how to view NTP status and configure NTP on a Mellanox switch.\nEnable NTP Test the NTP server Specify a remote NTP server Configure the system timezone Validate functionality Expected results Enable NTP switch (config) # ntp enable Test the NTP server Test the NTP server by querying the current time:\nswitch (config) # ntpdate 10.4.0.134 Specify a remote NTP server Specify a remote NTP server to use for time synchronization:\nswitch(config)# ntp server \u0026lt;FQDN|IP-ADDR\u0026gt; Configure the system timezone switch (config) # clock timezone UTC-offset UTC-7 Validate functionality switch (config)# show ntp Expected results The NTP client can be configured. The functionality can be validated using the show command. The system time of the switch matches that of the NTP server. Back to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/motd/",
	"title": "Message-Of-The-Day (MOTD)",
	"tags": [],
	"description": "",
	"content": "Message-Of-The-Day (MOTD) Banners are custom messages displayed to users attempting to connect to the management interfaces. MOTD banners are displayed pre-login while exec banners are displayed post-login. Multiple lines of text can be stored using a custom delimiter to mark the end of message.\nConfiguration Commands Create a banner:\nswitch(config)# banner \u0026lt;motd|exec\u0026gt; DELIM Show commands to validate functionality:\nswitch# show banner \u0026lt;motd|exec\u0026gt; Example Output switch(config)# banner motd $ Enter a new banner, when you are done enter a new line containing only your chosen delimiter. (banner-motd)# This is an example of a custom pre-login banner (banner-motd)# that spans multiple lines. (banner-motd)# $ switch(config)# do show banner motd This is an example of a custom pre-login banner that spans multiple lines.\nExpected Results Administrators can create the MOTD banner The output of the MOTD banner looks correct Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/configure_cray_cli/",
	"title": "Configure the Cray Command Line Interface (cray CLI)",
	"tags": [],
	"description": "",
	"content": "Configure the Cray Command Line Interface (cray CLI) The cray command line interface (CLI) is a framework created to integrate all of the system management REST APIs into easily usable commands.\nProcedures in the CSM installation workflow use the cray CLI to interact with multiple services. The cray CLI configuration needs to be initialized for the Linux account, and the Keycloak user running the procedure needs to be authorized. This section describes how to initialize the cray CLI for use by a user and how to authorize that user.\nThe cray CLI only needs to be initialized once per user on a node.\nProcedure Unset the CRAY_CREDENTIALS environment variable, if previously set.\nSome CSM installation procedures use the CLI with a Kubernetes managed service account that is normally used for internal operations. There is a procedure for extracting the OAUTH token for this service account and assigning it to the CRAY_CREDENTIALS environment variable to permit simple CLI operations. It must be unset in order to validate that the CLI is working with user authentication.\nncn# unset CRAY_CREDENTIALS Initialize the cray CLI for the root account.\nThe cray CLI needs to know what host to use to obtain authorization and what user is requesting authorization, so it can obtain an OAUTH token to talk to the API gateway. This is accomplished by initializing the CLI configuration.\nIn this example, the vers username is used. It should be replaced with an appropriate user account:\nIf LDAP configuration was enabled, then use a valid account in LDAP. If LDAP configuration was not enabled, or is not working, then a Keycloak local account may be created. See Configure Keycloak Account to create this local account in Keycloak. ncn# cray init --hostname api-gw-service-nmn.local Expected output (including the typed input) should look similar to the following:\nUsername: vers Password: Success! Initialization complete. Verify that the cray CLI is operational.\nncn# cray artifacts buckets list -vvv Expected output looks similar to the following:\nLoaded token: /root/.config/cray/tokens/api_gw_service_nmn_local.vers REQUEST: PUT to https://api-gw-service-nmn.local/apis/sts/token OPTIONS: {\u0026#39;verify\u0026#39;: False} S3 credentials retrieved successfully results = [ \u0026#34;alc\u0026#34;, \u0026#34;badger\u0026#34;, \u0026#34;benji-backups\u0026#34;, \u0026#34;boot-images\u0026#34;, \u0026#34;etcd-backup\u0026#34;, \u0026#34;fw-update\u0026#34;, \u0026#34;ims\u0026#34;, \u0026#34;install-artifacts\u0026#34;, \u0026#34;nmd\u0026#34;, \u0026#34;postgres-backup\u0026#34;, \u0026#34;prs\u0026#34;, \u0026#34;sat\u0026#34;, \u0026#34;sds\u0026#34;, \u0026#34;sls\u0026#34;, \u0026#34;sma\u0026#34;, \u0026#34;ssd\u0026#34;, \u0026#34;ssm\u0026#34;, \u0026#34;vbis\u0026#34;, \u0026#34;velero\u0026#34;, \u0026#34;wlm\u0026#34;,] If an error occurs, then continue to the troubleshooting section below.\nTroubleshooting More information about what is failing can be found by adding -vvvvv to the cray init ... commands.\nInitialization fails If CLI initialization fails, there are several common causes:\nDNS failure looking up api-gw-service-nmn.local may be preventing the CLI from reaching the API Gateway and Keycloak for authorization Network connectivity issues with the NMN may be preventing the CLI from reaching the API Gateway and Keycloak for authorization Certificate mismatch or trust issues may be preventing a secure connection to the API Gateway Istio failures may be preventing traffic from reaching Keycloak Keycloak may not yet be set up to authorize the user Internal error If an error similar to the following is seen, then restart radosgw on the storage nodes.\nThe server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application. Restart radosgw using the following steps. These steps must be run on one of the storage nodes running the Ceph radosgw process. By default these nodes are ncn-s001, ncn-s002, and ncn-s003.\nRestart the Ceph radosgw process.\nThe expected output will be similar to the following, but it will vary based on the nodes running radosgw.\nncn-s# ceph orch restart rgw.site1.zone1 Example output:\nrestart rgw.site1.zone1.ncn-s001.cshvbb from host \u0026#39;ncn-s001\u0026#39; restart rgw.site1.zone1.ncn-s002.tlegbb from host \u0026#39;ncn-s002\u0026#39; restart rgw.site1.zone1.ncn-s003.vwjwew from host \u0026#39;ncn-s003\u0026#39; Check to see that the processes restarted.\nncn-s# ceph orch ps --daemon_type rgw Example output:\nNAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID rgw.site1.zone1.ncn-s001.cshvbb ncn-s001 running (29s) 23s ago 9h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 2a712824adc1 rgw.site1.zone1.ncn-s002.tlegbb ncn-s002 running (29s) 28s ago 9h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c e423f22d06a5 rgw.site1.zone1.ncn-s003.vwjwew ncn-s003 running (29s) 23s ago 9h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 1e6ad6bc2c62 A process which has restarted should have an AGE in seconds. Restarting all of them could require a couple of minutes depending on how many.\nIn the event that more than five minutes have passed and the radosgw processes have not restarted, then fail the ceph-mgr process.\nDetermine the active ceph-mgr.\nncn-s#ceph mgr dump | jq -r .active_name Example output:\nncn-s002.zozbqp Fail the active ceph-mgr.\nncn-s# ceph mgr fail $(ceph mgr dump | jq -r .active_name) Confirm that ceph-mgr has moved to a different ceph-mgr container.\nncn-s# ceph mgr dump | jq -r .active_name Example output:\nncn-s001.qucrpr Verify that the radosgw processes restarted using the command from the previous step.\nAt this point the processes should restart. If they do not, then attempt this remediation procedure a second time.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/resource_specifications/",
	"title": "Resource Specifications",
	"tags": [],
	"description": "",
	"content": "Resource Specifications Kubernetes uses resource limits and resource requests, to manage the system resources available to pods. Because UAIs run as pods under Kubernetes, UAS takes advantage of Kubernetes to manage the system resources available to UAIs.\nIn the UAS configuration, resource specifications contain that configuration. A UAI that is assigned a resource specification will use that instead of the default resource limits or requests on the Kubernetes namespace containing the UAI. This can be used to fine-tune resources assigned to UAIs.\nTop: User Access Service (UAS)\nNext Topic: List UAI Resource Specifications\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/public_key_infrastructure_pki/",
	"title": "Public Key Infrastructure (PKI)",
	"tags": [],
	"description": "",
	"content": "Public Key Infrastructure (PKI) Public Key Infrastructure (PKI) represents the algorithms, infrastructure, policies, and processes required to leverage applied public key cryptography methods for operational security use cases. The Rivest-Shamir-Adleman (RSA) and Elliptic-curve (ECC) are some example algorithm systems.\nThe use of PKI for the system is in the Transport Layer Security (TLS) protocol, which is the successor of the now deprecated Secure Sockets Layer (SSL). This is where trusted chains of Certificate Authorities (CAs) are used to authenticate the identity of servers and sometimes clients (for example, mutual TLS) for relying parties. This chain of trust is anchored by a root CA and is used to make assertions that a particular public and private key pair belong to a given party by assigning a certificate for the party. This party is still required to prove they actually own the key material through enciphering, deciphering, and digital signature operations that require private keys that are not shared among parties. However, public keys are shared through certificates and are policy bound in that respect.\nThe post-installation PKI implementation for the system is made up of Kubernetes services (illustrated in the figure below). During installation, either the platform can be directed to generate certificate authorities (CAs), or a customer-supplied intermediate CA can be supplied. After installation, the CA material resides in a Kubernetes Secret, and ultimately in the HashiCorp Vault.\nRefer to PKI Services for more information on the services in the figure above.\nLimitations The following limitations exist within the PKI implementation:\nCert-manager renewal Supported cryptography suites CA rotation Implications of transitive trust Abuse of PKI APIs to sign malicious products Security of CA material Revocation lists and Online Certificate Status Protocol (OCSP) Key escrow Cert-manager renewal An outstanding bug in the Keycloak Gatekeeper service prevents it from updating its TLS certificate and key material upon Cert-manager renewal. It may be necessary to monitor the situation and proactively renew or force reload Keycloak Gatekeeper.\nSupported cryptography suites RSA-based CAs and certificates are currently supported. CAs must have either a 3072- or 4096-bit modulus and use SHA256 as the signature algorithm. Installation paths are designed to force convention.\nPassword-encrypted private keys are not currently supported.\nCA rotation Changing the platform CA post-installation is not currently supported. Changing it requires a re-install.\nImplications of transitive trust If the platform is configured to generate a dynamic CA, then customer services or users that interact with the platform must trust the platform CA to validate TLS sessions. Thus, provided the platform has a disjoint DNS domain name (for example, shasta.acme.org), and the PKI trust realm is established at this FQDN or a subdomain of this FQDN, then a compromise of platform CA material should be limited to the platform itself (subject to many nuances).\nIf a customer supplies a CA to the platform, and the CA is part of an expanded PKI trust realm, then a compromise of platform CA material could be leveraged to compromise the broader environment through PKI APIs available on the system. Customers should consider this risk, and, if providing a CA is desired, consider strictly limiting the PKI trust realm established by the provided CA.\nAbuse of PKI APIs to sign malicious products Compromise of a platform could lead to the generation of certificates for potentially malicious workloads. Current HashiCorp Vault policies that control legitimate signing activities are fairly broad in allowed certificate CSR properties. This is due largely to common name and SAN requirements for certificate workloads across the platform.\nSecurity of CA material During installation, CA material is exposed in the following cases:\nWhen they are staged by the installer By installation processes (e.g., shasta-cfg) After installation, CA material is exposed in the following cases:\nIn a SealedSecret In a Kubernetes Secret In Kubernetes etcd backups and other backups taken of the platform To Vault Through the creation of additional subordinate CAs for Spire Revocation lists and Online Certificate Status Protocol (OCSP) The platform does not provide revocation lists or access to a revocation service (OCSP).\nKey escrow The platform does not provide any key escrow services.\n"
},
{
	"uri": "/docs-csm/en-12/operations/node_management/reboot_ncns/",
	"title": "Reboot NCNs",
	"tags": [],
	"description": "",
	"content": "Reboot NCNs The following is a high-level overview of the non-compute node (NCN) reboot workflow:\nRun the NCN pre-reboot checks and procedures.\nEnsure that ncn-m001 is not booted to the LiveCD / PIT node. Check the metal.no-wipe settings for all NCNs. Run all platform health checks, including checks on the Border Gateway Protocol (BGP) peering sessions. Validate the current boot order (or specify the boot order). Run the rolling NCN reboot procedure.\nLoop through reboots on storage nodes, worker nodes, and master nodes, where each reboot consists of the following workflow:\nEstablish console session with node to reboot. Execute a Linux graceful shutdown or power off/on sequence to the node, allowing it to boot up to completion. Execute NCN/platform health checks and do not go on to reboot the next NCN until health has been ensured on the most recently rebooted NCN. Disconnect console session with the node that was rebooted. Re-run all platform health checks.\nThe time duration for this procedure (if health checks are being executed in between each boot, as recommended) could take between two to four hours for a system with nine management nodes.\nThis same procedure can be used to reboot a single management node as outlined above. Be sure to carry out the NCN pre-reboot checks and procedures before and after rebooting the node. Execute the rolling NCN reboot procedure steps for the particular node type being rebooted.\nPrerequisites The kubectl command is installed. The Cray command line interface is configured on at least one NCN. See Configure the Cray CLI. The latest CSM documentation is installed, if rebooting ncn-m001 or any worker nodes. If rebooting ncn-m001, then the latest CSM documentation must be installed on ncn-m001. If rebooting a worker node, then the latest CSM documentation must be installed on some master or worker node. See Check for latest documentation. NCN pre-reboot health checks Ensure that ncn-m001 is not booted to the LiveCD / PIT node.\nThis mode should only be in effect during the initial product install. If the word pit is NOT in the hostname of ncn-m001, then it is not in the LiveCD mode.\nIf pit is in the hostname of ncn-m001, then the system is not in normal operational mode and rebooting ncn-m001 may have unexpected results. This procedure assumes that the node is not running in the LiveCD mode that occurs during product install.\nRun the platform health checks and analyze the results.\nRefer to the \u0026ldquo;Platform Health Checks\u0026rdquo; section in Validate CSM Health for an overview of the health checks.\nRun the platform health script.\nRun this on any master or worker node. The output of the following script will need to be referenced in some of the remaining sub-steps.\nncn-mw# /opt/cray/platform-utils/ncnHealthChecks.sh NOTE: If the ncnHealthChecks script output indicates any kube-multus-ds- pods are in a Terminating state, that can indicate that a previous restart of these pods did not complete. In this case, it is safe to force delete these pods in order to let them properly restart; this is done by running kubectl delete po -n kube-system kube-multus-ds.. --force. After executing this command, re-running the ncnHealthChecks script should indicate that a new pod is in the Running state.\nValidate Postgres health.\nRun this on any master or worker node. It will run a set of checks on every Postgres cluster.\nncn-mw# /opt/cray/tests/install/ncn/automated/ncn-postgres-tests Example output:\nNCN Postgres Tests --------------- .......... Total Duration: 27.170s Count: 10, Failed: 0, Skipped: 0 .......... Total Duration: 27.265s Count: 10, Failed: 0, Skipped: 0 .......... \u0026hellip;\nTotal Duration: 27.264s Count: 10, Failed: 0, Skipped: 0 Remediate any reported failures before proceeding.\nCheck the status of the slurmctld and slurmdbd pods to determine if they are starting.\nncn-mw# kubectl describe pod -n user -lapp=slurmctld ncn-mw# kubectl describe pod -n user -lapp=slurmdbd Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \u0026#34;314ca4285d0706ec3d76a9e953e412d4b0712da4d0cb8138162b53d807d07491\u0026#34;: Multus: Err in tearing down failed plugins: Multus: error in invoke Delegate add - \u0026#34;macvlan\u0026#34;: failed to allocate for range 0: no IP addresses available in range set: 10.252.2.4-10.252.2.4 Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox ... If the preceding error is displayed, then remove all files in the following directories on all worker nodes:\n/var/lib/cni/networks/macvlan-slurmctld-nmn-conf /var/lib/cni/networks/macvlan-slurmdbd-nmn-conf Check that the BGP peering sessions are established.\nThis check will need to be run after all worker node have been rebooted. Ensure that the checks have been run to check BGP peering sessions on the spine switches.\nSet SW_ADMIN_PASSWORD to the admin user password for the management switches in the system.\nread -s is used to prevent the password from being written to the screen or the shell history.\nncn# read -r -s -p \u0026#34;Management switch admin user password: \u0026#34; SW_ADMIN_PASSWORD Run the validation.\nncn# export SW_ADMIN_PASSWORD ncn# GOSS_BASE=/opt/cray/tests/install/ncn goss \\ -g /opt/cray/tests/install/ncn/tests/goss-switch-bgp-neighbor-aruba-or-mellanox.yaml \\ --vars=/opt/cray/tests/install/ncn/vars/variables-ncn.yaml validate Ensure that no nodes are in a failed state in CFS.\nNodes that are in a failed state prior to the reboot will not be automatically configured once they have been rebooted.\nThe following script will find all CFS components in the failed state and for each such component it will reset its CFS error count to 0 and disable it in CFS. It is disabled in order to prevent CFS from immediately triggering a configuration. The components will be automatically re-enabled when they boot.\nThis can be run on any NCN where the Cray CLI is configured. See Configure the Cray CLI.\nncn# cray cfs components list --status failed --format json | jq .[].id -r | while read -r xname ; do echo \u0026#34;${xname}\u0026#34; cray cfs components update \u0026#34;${xname}\u0026#34; --enabled False --error-count 0 done Alternatively, this can be done manually. To get a list of nodes in the failed state:\nncn# cray cfs components list --status failed --format json | jq .[].id To reset the error count and disable a node:\nNOTE: Be sure to replace the \u0026lt;xname\u0026gt; in the following command with the component name (xname) of the NCN component to be reset and disabled.\nncn# cray cfs components update \u0026lt;xname\u0026gt; --enabled False --error-count 0 NCN rolling reboot Before rebooting NCNs:\nEnsure that pre-reboot checks have been completed, including checking the metal.no-wipe setting for each NCN. Do not proceed if any of the NCN metal.no-wipe settings are zero. Utility storage nodes (Ceph) Reboot each of the storage nodes (one at a time), going from the highest to lowest number.\nEstablish a console session to each storage node.\nUse the Establish a Serial Connection to NCNs procedure referenced in step 4.\nIf booting from disk is desired then set the boot order.\nReboot the selected node.\nncn-s# shutdown -r now IMPORTANT: If the node does not shut down after 5 minutes, then proceed with the power reset below.\nPower off the node.\nread -s is used to prevent the password from being written to the screen or the shell history.\nIn the example commands below, be sure to replace \u0026lt;node\u0026gt; with the name of the node being rebooted. For example, ncn-s002.\nncn# USERNAME=root ncn# read -r -s -p \u0026#34;NCN BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power off ncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power status Ensure that the power is reporting as off. It may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nPower on the node.\nIn the example commands below, be sure to replace \u0026lt;node\u0026gt; with the name of the node being rebooted. For example, ncn-s002.\nncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power on ncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power status Ensure that the power is reporting as on. It may take 5-10 seconds for this to update.\nWatch on the console until the node has successfully booted and the login prompt is reached.\nIf desired, verify that the method of boot is as expected.\nIf the /proc/cmdline file begins with BOOT_IMAGE, then this NCN booted from disk.\nncn-s# egrep -o \u0026#39;^(BOOT_IMAGE|kernel)\u0026#39; /proc/cmdline Example output for a disk boot is:\nBOOT_IMAGE=(mduuid/a3899572a56f5fd88a0dec0e89fc12b4)/boot/grub2/../kernel Retrieve the component name (xname) for the node that was rebooted.\nThis xname is available on the node that was rebooted in the /etc/cray/xname file.\nncn# ssh NODE cat /etc/cray/xname Check the Configuration Framework Service (CFS) configurationStatus for the rebooted node\u0026rsquo;s desiredConfig.\nThe following command will indicate if a CFS job is currently in progress for this node. Replace the XNAME value in the following command with the component name (xname) of the node that was rebooted.\nThis can be run on any NCN where the Cray CLI is configured. See Configure the Cray CLI.\nncn# cray cfs components describe XNAME --format json Example output:\n{ \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, If the configurationStatus is pending, then wait for the job to finish before continuing. If the configurationStatus is failed, then this means the failed CFS job configurationStatus should be addressed now for this node. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, then this can be ignored. If the configurationStatus is failed, then see Troubleshoot Ansible Play Failures in CFS Sessions for how to analyze the pod logs from cray-cfs in order to determine why the configuration may not have completed. Run the platform health checks from the Validate CSM Health procedure.\nTroubleshooting: If the slurmctld and slurmdbd pods do not start after powering back up the node, then check for the following error:\nncn-mw# kubectl describe pod -n user -lapp=slurmctld Example output:\nWarning FailedCreatePodSandBox 27m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \u0026#34;82c575cc978db00643b1bf84a4773c064c08dcb93dbd9741ba2e581bc7c5d545\u0026#34;: Multus: Err in tearing down failed plugins: Multus: error in invoke Delegate add - \u0026#34;macvlan\u0026#34;: failed to allocate for range 0: no IP addresses available in range set: 10.252.2.4-10.252.2.4 ncn-mw# kubectl describe pod -n user -lapp=slurmdbd Example output:\nWarning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \u0026#34;314ca4285d0706ec3d76a9e953e412d4b0712da4d0cb8138162b53d807d07491\u0026#34;: Multus: Err in tearing down failed plugins: Multus: error in invoke Delegate add - \u0026#34;macvlan\u0026#34;: failed to allocate for range 0: no IP addresses available in range set: 10.252.2.4-10.252.2.4 Remove the following files on every worker node to resolve the failure:\n/var/lib/cni/networks/macvlan-slurmctld-nmn-conf /var/lib/cni/networks/macvlan-slurmdbd-nmn-conf Disconnect from the console.\nRepeat all of the sub-steps above for the remaining storage nodes, going from the highest to lowest number, until all storage nodes have successfully rebooted.\nImportant: Ensure that ceph -s shows that Ceph is healthy (HEALTH_OK) BEFORE MOVING ON to reboot the next storage node. Once Ceph has recovered the downed mon, it may take a several minutes for Ceph to resolve clock skew.\nNCN worker nodes Reboot each of the worker nodes (one at a time), going from the highest to lowest number.\nNOTE: A single worker is being rebooted at a time, so be sure to follow the steps on the correct worker node.\nEstablish a console session to the worker node being rebooted.\nIMPORTANT: If the ConMan console pod is on the node being rebooted, then the session must be re-established after the cordon/drain step.\nSee Establish a Serial Connection to NCNs for more information.\nFailover any Postgres leader that is running on the worker node being rebooted.\nThis script must be run from a master or worker node with the latest CSM documentation installed. See Check for latest documentation.\nncn-mw# /usr/share/doc/csm/upgrade/1.2/scripts/k8s/failover-leader.sh \u0026lt;node to be rebooted\u0026gt; Cordon and drain the node.\nncn-mw# kubectl drain --ignore-daemonsets=true --delete-local-data=true \u0026lt;node to be rebooted\u0026gt; There may be pods that cannot be gracefully evicted because of Pod Disruption Budgets (PDB). This will result in messages like the following:\nerror when evicting pod \u0026#34;\u0026lt;pod\u0026gt;\u0026#34; (will retry after 5s): Cannot evict pod as it would violate the pod\u0026#39;s disruption budget. In this case, there are some options. First, if the service is scalable, then increase the scale to start up another pod on another node, and then the drain will be able to delete it. However, it will probably be necessary to force the deletion of the pod:\nncn-mw# kubectl delete pod [-n \u0026lt;namespace\u0026gt;] --force --grace-period=0 \u0026lt;pod\u0026gt; This will delete the offending pod, and Kubernetes should schedule a replacement on another node. Then rerun the kubectl drain command, and it should report that the node is drained.\nncn-mw# kubectl drain --ignore-daemonsets=true --delete-local-data=true \u0026lt;node to be rebooted\u0026gt; If booting from disk is desired, then set the boot order.\nReboot the selected node.\nncn-w# shutdown -r now IMPORTANT: If the node does not shut down after 5 minutes, then proceed with the power reset below.\nPower off the node.\nread -s is used to prevent the password from being written to the screen or the shell history.\nIn the example commands below, be sure to replace \u0026lt;node\u0026gt; with the name of the node being rebooted. For example, ncn-w002.\nncn# USERNAME=root ncn# read -r -s -p \u0026#34;NCN BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power off ncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power status Ensure that the power is reporting as off. It may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nPower on the node.\nIn the example commands below, be sure to replace \u0026lt;node\u0026gt; with the name of the node being rebooted. For example, ncn-w002.\nncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power on ncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power status Ensure that the power is reporting as on. It may take 5-10 seconds for this to update.\nWatch on the console until the node has successfully booted and the login prompt is reached.\nIf desired, verify that the method of boot is as expected.\nIf the /proc/cmdline file begins with BOOT_IMAGE, then this NCN booted from disk.\nncn-w# egrep -o \u0026#39;^(BOOT_IMAGE|kernel)\u0026#39; /proc/cmdline Example output for a disk boot is:\nBOOT_IMAGE=(mduuid/a3899572a56f5fd88a0dec0e89fc12b4)/boot/grub2/../kernel Retrieve the component name (xname) for the node that was rebooted.\nThis xname is available on the node that was rebooted in the /etc/cray/xname file.\nncn# ssh NODE cat /etc/cray/xname Check the Configuration Framework Service (CFS) configurationStatus for the rebooted node\u0026rsquo;s desiredConfig.\nThe following command will indicate if a CFS job is currently in progress for this node. Replace the XNAME value in the following command with the component name (xname) of the node that was rebooted.\nThis can be run on any NCN where the Cray CLI is configured. See Configure the Cray CLI.\nncn# cray cfs components describe XNAME --format json Example output:\n{ \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, \u0026#34;lines omitted\u0026#34;: \u0026#34;...\u0026#34; } If the configurationStatus is pending, then wait for the job to finish before continuing. If the configurationStatus is failed, then this means the failed CFS job configurationStatus should be addressed now for this node. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, then this can be ignored. If the configurationStatus is failed, then see Troubleshoot Ansible Play Failures in CFS Sessions for how to analyze the pod logs from cray-cfs in order to determine why the configuration may not have completed. Remove the node cordon.\nncn-mw# kubectl uncordon \u0026lt;node that just rebooted\u0026gt; Verify that pods are running on the rebooted node.\nWithin a minute or two, the following command should begin to show pods in a Running state (replace NCN in the command below with the name of the rebooted worker node):\nncn-mw# kubectl get pods -o wide -A | grep \u0026lt;node that was rebooted\u0026gt; Run the platform health checks from the Validate CSM Health procedure.\nIn particular, verify that no etcd errors are reported.\nIf terminating pods are reported when checking the status of the Kubernetes pods, then wait for all pods to recover before proceeding.\nDisconnect from the console.\nRepeat all of the sub-steps above for the remaining worker nodes, going from the highest to lowest number, until all worker nodes have successfully rebooted.\nEnsure that BGP sessions are reset so that all BGP peering sessions with the spine switches are in an ESTABLISHED state.\nSee Check BGP Status and Reset Sessions.\nNCN master nodes Reboot each of the master nodes (one at a time), going from the highest to lowest number, excluding ncn-m001. There are special instructions for ncn-m001 later, because its console connection is not managed by ConMan.\nEstablish a console session to the master node being rebooted.\nSee step Establish a Serial Connection to NCNs for more information.\nIf booting from disk is desired, then set the boot order.\nReboot the selected node.\nncn-m# shutdown -r now IMPORTANT: If the node does not shut down after 5 minutes, then proceed with the power reset below.\nPower off the node.\nread -s is used to prevent the password from being written to the screen or the shell history.\nIn the example commands below, be sure to replace \u0026lt;node\u0026gt; with the name of the node being rebooted. For example, ncn-m002.\nncn# USERNAME=root ncn# read -r -s -p \u0026#34;NCN BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD ncn# export IPMI_PASSWORD ncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power off ncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power status Ensure that the power is reporting as off. It may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nPower on the node.\nIn the example commands below, be sure to replace \u0026lt;node\u0026gt; with the name of the node being rebooted. For example, ncn-m002.\nncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power on ncn# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;node\u0026gt;-mgmt -I lanplus power status Ensure that the power is reporting as on. It may take 5-10 seconds for this to update.\nWatch on the console until the node has successfully booted and the login prompt is reached.\nIf desired, verify that the method of boot is as expected.\nIf the /proc/cmdline file begins with BOOT_IMAGE, then this NCN booted from disk.\nncn-m# egrep -o \u0026#39;^(BOOT_IMAGE|kernel)\u0026#39; /proc/cmdline Example output for a disk boot is:\nBOOT_IMAGE=(mduuid/a3899572a56f5fd88a0dec0e89fc12b4)/boot/grub2/../kernel Retrieve the component name (xname) for the node that was rebooted.\nThis xname is available on the node that was rebooted in the /etc/cray/xname file.\nncn# ssh NODE cat /etc/cray/xname Check the Configuration Framework Service (CFS) configurationStatus for the rebooted node\u0026rsquo;s desiredConfig.\nThe following command will indicate if a CFS job is currently in progress for this node. Replace the XNAME value in the following command with the component name (xname) of the node that was rebooted.\nThis can be run on any NCN where the Cray CLI is configured. See Configure the Cray CLI.\nncn# cray cfs components describe XNAME --format json Example output:\n{ \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, \u0026#34;lines omitted\u0026#34;: \u0026#34;...\u0026#34; } If the configurationStatus is pending, then wait for the job to finish before continuing. If the configurationStatus is failed, then this means the failed CFS job configurationStatus should be addressed now for this node. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, then this can be ignored. If the configurationStatus is failed, then see Troubleshoot Ansible Play Failures in CFS Sessions for how to analyze the pod logs from cray-cfs in order to determine why the configuration may not have completed. Run the platform health checks in Validate CSM Health.\nDisconnect from the console.\nRepeat all of the sub-steps above for the remaining master nodes (excluding ncn-m001), going from the highest to lowest number, until all master nodes have successfully rebooted.\nReboot ncn-m001.\nDetermine the CAN IP address for one of the other NCNs in the system, in order to establish an SSH session with that NCN.\nEstablish a console session to ncn-m001 from a system external to the cluster.\nIf booting from disk is desired, then set the boot order.\nPower cycle the node.\nEnsure that the expected results are returned from the power status check before rebooting.\nread -s is used to prevent the password from being written to the screen or the shell history.\nIn the example commands below, be sure to replace \u0026lt;ncn-m001-bmc\u0026gt; with the external IP or hostname of the BMC of ncn-m001.\nexternal# USERNAME=root external# read -r -s -p \u0026#34;ncn-m001 BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD external# export IPMI_PASSWORD external# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;ncn-m001-bmc\u0026gt; -I lanplus power status Power off ncn-m001.\nIn the example commands below, be sure to replace \u0026lt;ncn-m001-bmc\u0026gt; with the external IP or hostname of the BMC of ncn-m001.\nexternal# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;ncn-m001-bmc\u0026gt; -I lanplus power off external# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;ncn-m001-bmc\u0026gt; -I lanplus power status Ensure that power is reporting as off. It may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nPower on the node.\nIn the example commands below, be sure to replace \u0026lt;ncn-m001-bmc\u0026gt; with the external IP or hostname of the BMC of ncn-m001.\nexternal# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;ncn-m001-bmc\u0026gt; -I lanplus power on external# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -H \u0026lt;ncn-m001-bmc\u0026gt; -I lanplus power status Ensure that the power is reporting as on. It may take 5-10 seconds for this to update.\nWatch on the console until the node has successfully booted and the login prompt is reached.\nIf desired, verify that the method of boot is as expected.\nIf the /proc/cmdline file begins with BOOT_IMAGE, then this NCN booted from disk.\nncn-m001# egrep -o \u0026#39;^(BOOT_IMAGE|kernel)\u0026#39; /proc/cmdline Example output for a disk boot is:\nBOOT_IMAGE=(mduuid/a3899572a56f5fd88a0dec0e89fc12b4)/boot/grub2/../kernel Retrieve the component name (xname) for the node that was rebooted.\nThis xname is available on the node that was rebooted in the /etc/cray/xname file.\nncn# ssh ncn-m001 cat /etc/cray/xname Check the Configuration Framework Service (CFS) configurationStatus for the desiredConfig after rebooting the node.\nThe following command will indicate if a CFS job is currently in progress for this node. Replace the XNAME value in the following command with the component name (xname) of the node that was rebooted.\nThis can be run on any NCN where the Cray CLI is configured. See Configure the Cray CLI.\nncn# cray cfs components describe XNAME --format json Example output:\n{ \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, \u0026#34;lines omitted\u0026#34;: \u0026#34;...\u0026#34; } If the configurationStatus is pending, then wait for the job to finish before continuing. If the configurationStatus is failed, then this means the failed CFS job configurationStatus should be addressed now for this node. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, then this can be ignored. If the configurationStatus is failed, then see Troubleshoot Ansible Play Failures in CFS Sessions for how to analyze the pod logs from cray-cfs in order to determine why the configuration may not have completed. Run the platform health checks in Validate CSM Health.\nDisconnect from the console.\nRemove any dynamically assigned interface IP addresses that did not get released automatically.\nThis script must be run from ncn-m001, which must have the latest CSM documentation installed. See Check for latest documentation.\nncn-m001# /usr/share/doc/csm/scripts/CASMINST-2015.sh Validate CSM health.\nAt a minimum, run the platform health checks.\nSee Validate CSM Health for the platform health checks.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/ospfv2/",
	"title": "Open shortest path first (OSPF) v2",
	"tags": [],
	"description": "",
	"content": "Open shortest path first (OSPF) v2 \u0026ldquo;OSPF is a link-state based routing protocol. It is designed to be run internal to a single Autonomous System. Each OSPF router maintains an identical database describing the Autonomous System\u0026rsquo;s topology. From this database, a routing table is calculated by constructing a shortest-path tree. OSPF recalculates routes quickly in the face of topological changes, utilizing a minimum of routing protocol traffic. OSPF provides support for equal-cost multipath. An area routing capability is provided, enabling an additional level of routing protection and a reduction in routing protocol traffic.\u0026rdquo; –rfc1247\nRelevant Configuration\nEnable Ip routing\nswitch(config)# ip routing Configure ospf protocol\nswitch(config)# protocol ospf switch(config)#. router ospf Associate area to vlan interface\nswitch(config)# interface vlan 10 switch(config interface vlan 10)# no shutdown switch(config interface vlan 10)# ip address 10.10.10.1/24 switch(config interface vlan 10)# ip ospf area 0 Show Commands to Validate Functionality\nswitch# show ip ospf Expected Results\nStep 1: You can enable OSPF globally on the switch Step 2: You can enable OSPF on the loopback, SVI or routed interfaces. Step 3: The output of the show commands looks correct. Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/msdp/",
	"title": "Multicast Source Discovery Protocol (MSDP)",
	"tags": [],
	"description": "",
	"content": "Multicast Source Discovery Protocol (MSDP) The Multicast Source Discovery Protocol (MSDP) describes a mechanism to connect multiple IP Version 4 Protocol Independent Multicast Sparse-Mode (PIM-SM) domains together. Each PIM-SM domain uses its own independent Rendezvous Point (RP) and does not have to depend on RPs in other domains. When an RP in a PIM-SM domain first learns of a new sender, e.g., via PIM register messages, it constructs a \u0026ldquo;Source-Active\u0026rdquo; (SA) message and sends it to its MSDP peers. The SA message contains the following fields:\nThe source address of the data source. The group address the data source sends to. The IP address of the RP. –rfc3618\nConfiguration Commands MSDP is typically run on an IP address bound to a loopback interface. In order for two devices to establish an MSDP neighbor relationship, L3 connectivity must already be established.\nswitch(config)# router msdp switch(config-msdp)# enable switch(config-msdp)# ip msdp peer \u0026lt;IP\u0026gt; switch(config-msdp-peer)# enable switch(config-msdp-peer)# connect-source \u0026lt;IFNAME\u0026gt; Show commands to validate functionality:\nswitch# show ip msdp peer switch# show ip msdp count switch# show ip msdp sa-cache Test Steps Configure a loopback interface on both 8325 that are acting as core devices. Enable PIM on loopback interface Configure MSDP and create a peer relationship between 8325\u0026rsquo;s using a loopback as the source. Expected Results Verify MSDP session is up and it is using loopback interface as source.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/retrieve_resource_specification_details/",
	"title": "Retrieve Resource Specification Details",
	"tags": [],
	"description": "",
	"content": "Retrieve Resource Specification Details Display a specific resource specification using the resource_id of that specification.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Resource ID of the resource specification to be retrieved: List Resource Specifications Procedure Retrieve a resource specification.\nTo examine a particular resource specification, use a command of the following form:\nncn-m001-pit# cray uas admin config resources describe RESOURCE_ID For example:\nncn-m001-pit# cray uas admin config resources describe f26ee12c-6215-4ad1-a15e-efe4232f45e6 comment = \u0026#34;Resource Specification to use with Brokered End-User UAIs\u0026#34; limit = \u0026#34;{\\\u0026#34;cpu\\\u0026#34;: \\\u0026#34;300m\\\u0026#34;, \\\u0026#34;memory\\\u0026#34;: \\\u0026#34;1Gi\\\u0026#34;}\u0026#34; request = \u0026#34;{\\\u0026#34;cpu\\\u0026#34;: \\\u0026#34;300m\\\u0026#34;, \\\u0026#34;memory\\\u0026#34;: \\\u0026#34;1Gi\\\u0026#34;}\u0026#34; resource_id = \u0026#34;f26ee12c-6215-4ad1-a15e-efe4232f45e6\u0026#34; Top: User Access Service (UAS)\nNext Topic: Update a Resource Specification\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/recovering_from_mismatched_bmc_credentials/",
	"title": "Recovering from Mismatched BMC Credentials",
	"tags": [],
	"description": "",
	"content": "Recovering from Mismatched BMC Credentials Use this procedure to recover from the situation when new or replacement hardware has root credentials that do not match the system\u0026rsquo;s current default root user credentials.\nThis type of problem can occur in the following scenarios:\nThe site has customized the default root credentials using either the Updating the Liquid-Cooled EX Cabinet CEC with Default Credentials after a CEC Password Change or Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials procedures. Hardware has the factory default root password or a different known root password configured. For example, hardware that has moved from a different system with a customized default root password. Procedure Specify the BMC hostname with the mismatched credentials:\nncn-mw# BMC=x1000c0r1b0 Specify the current root user password for the BMC:\nDepending on the origin of the piece of hardware, this could be the factory default password or a different system\u0026rsquo;s default password.\nncn-mw# read -s CURRENT_ROOT_PASSWORD ncn-mw# echo $CURRENT_ROOT_PASSWORD Verify the credentials work with Redfish using curl:\nncn-mw# curl -k -u \u0026#34;root:$CURRENT_ROOT_PASSWORD\u0026#34; https://$BMC/redfish/v1/Managers -i The following example output shows the CURRENT_ROOT_PASSWORD environment variable contains a valid root password for the BMC.\nHTTP/1.1 200 OK ...output truncated... Conversely, the following output shows the CURRENT_ROOT_PASSWORD environment variable contains an invalid root user password for the BMC. Update the CURRENT_ROOT_PASSWORD environment variable to contain a valid root user password for the BMC.\nHTTP/1.1 401 Unauthorized ...output truncated... Update the credentials for the Redfish endpoint stored in Vault using Hardware State Manager (HSM):\nncn-mw# cray hsm inventory redfishEndpoints update $BMC --user root --password $CURRENT_ROOT_PASSWORD Wait a few minutes for HSM to attempt to inventory the BMC:\nncn-mw# sleep 120 Verify the BMC\u0026rsquo;s discovery status is DiscoverOK:\nncn-mw# cray hsm inventory redfishEndpoints describe $BMC If DiscoveryStarted, then wait and recheck the discovery status again. If HTTPsGetFailed, then examine the HSM logs to troubleshoot the issue.\nDetermine the system\u0026rsquo;s default BMC root user password:\nncn-mw# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-mw# alias vault=\u0026#39;kubectl -n vault exec -i cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 VAULT_FORMAT=json vault\u0026#39; Retrieve the default root password.\nFor liquid-cooled hardware:\nncn-mw# SYSTEM_ROOT_PASSWORD=$(vault kv get secret/meds-cred/global/ipmi | jq .data.Password -r) For air-cooled hardware:\nncn-mw# SYSTEM_ROOT_PASSWORD=$(vault kv get secret/reds-creds/defaults | jq .data.Cray.password -r) Verify the systems\u0026rsquo;s default root user password:\nncn-mw# echo $SYSTEM_ROOT_PASSWORD Create a payload for the System Configuration Service (SCSD):\nncn-mw# jq --arg BMC \u0026#34;$BMC\u0026#34; --arg PASSWORD \u0026#34;$SYSTEM_ROOT_PASSWORD\u0026#34; -n \\ \u0026#39;{Targets:[{Xname: $BMC, Creds: {Username: \u0026#34;root\u0026#34;, Password: $PASSWORD}}]}\u0026#39; \u0026gt; scsd_payload.json Inspect the payload:\nncn-mw# jq . scsd_payload.json Example payload contents:\n{ \u0026#34;Targets\u0026#34;: [ { \u0026#34;Xname\u0026#34;: \u0026#34;x1000c0r1b0\u0026#34;, \u0026#34;Creds\u0026#34;: { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foobar\u0026#34; } } ] } Apply the the systems\u0026rsquo;s default BMC root user credentials to the BMC:\nncn-mw# cray scsd bmc discreetcreds create scsd_payload.json Example of a successful credential change:\n[[Targets]] Xname = \u0026#34;x1000c0r1b0\u0026#34; StatusCode = 204 StatusMsg = \u0026#34;No Content\u0026#34; If the operation is not successful inspect the the SCSD logs.\nRemove SCSD payload file containing credentials from the file system:\nncn-mw# rm scsd_payload.json "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/physical_interfaces/",
	"title": "Physical interfaces",
	"tags": [],
	"description": "",
	"content": "Physical interfaces Interfaces in Mellanox are enabled by default.\nRelevant Configuration\nEnter interface context\nswitch (config) # interface ethernet 1/1 Show Commands to Validate Functionality\nswitch# show interfaces ethernet 1/1 Expected Results\nStep 1: You can enter the interface context for the port Step 2: You can establish a link with a partner Step 3: You can pass traffic as expected Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/mstp/",
	"title": "Multiple Spanning Tree Protocol (MSTP)",
	"tags": [],
	"description": "",
	"content": "Multiple Spanning Tree Protocol (MSTP) MSTP (802.1s) ensures that only one active path exists between any two nodes in a spanning-tree instance. A spanning-tree instance comprises a unique set of VLANs. MSTP instances significantly improve network resource utilization while maintaining a loop-free environment.\nConfiguration commands Enable MSTP (default mode for spanning-tree):\nswitch# spanning-tree switch# spanning-tree config-name \u0026lt;NAME\u0026gt; switch# spanning-tree config-revision \u0026lt;VALUE\u0026gt; Configure an MSTP instance and priority switch# spanning-tree instance VALUE vlan VLANS switch# spanning-tree instance VALUE priority VALUE Show commands to validate functionality:\nswitch# show spanning-tree mst detail Example output switch# show span Spanning tree status Extended System-id Ignore PVID Inconsistency : Disabled Path cost method : Long VLAN1 Root ID Priority : 32769 MAC-Address: 70:72:cf:1d:32:04 This bridge is the root Hello time(in seconds):2 Max Age(in seconds):20 Forward Delay(in seconds):15 : Enabled Protocol: MSTP : Enabled Bridge ID Priority : 32768 MAC-Address: 70:72:cf:1d:32:04 Hello time(in seconds):2 Max Age(in seconds):20 Forward Delay(in seconds):15 Port Role State Cost Priority Type ------------ -------------- ------------ ------- ---------- ---------- Expected results Spanning-tree mode is configured Spanning-tree is enabled, if loops are detected ports should go blocked state Spanning-tree splits traffic domain between two DUTs Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/retrieve_uai_image_registration_information/",
	"title": "Retrieve UAI Image Registration Information",
	"tags": [],
	"description": "",
	"content": "Retrieve UAI Image Registration Information Use this procedure to obtain the default and imagename values for a registered UAI image. This procedure can also be used to confirm that a specific image ID is still registered with UAS.\nThis procedure returns the same information as List Registered UAI Images, but only for one image.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Image ID of the UAI Image Registration to be retrieved: List UAI Images Procedure Obtain the image ID for a UAI that has been registered with UAS.\nQuery UAS for the registration details for a specific registered UAI.\nncn-m001-pit# cray uas admin config images describe 8fdf5d4a-c190-24c1-2b96-74ab98c7ec07 Example output:\n[[results]] default = false image_id = \u0026#34;8fdf5d4a-c190-24c1-2b96-74ab98c7ec07\u0026#34; imagename = \u0026#34;registry.local/cray/custom-end-user-uai:latest\u0026#34; Top: User Access Service (UAS)\nNext Topic: Update a UAI Image Registration\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/remove_internal_groups_from_the_keycloak_shasta_realm/",
	"title": "Remove Internal Groups from the Keycloak Shasta Realm",
	"tags": [],
	"description": "",
	"content": "Remove Internal Groups from the Keycloak Shasta Realm Remove a group in the Keycloak Shasta realm. Unused Keycloak groups can be removed.\nPrerequisites This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process.\nThe password can be obtained using the following command:\nncn-mw# kubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode Procedure Open the Keycloak user management interface.\nSee Access the Keycloak User Management UI for more information.\nClick the Groups text in the Manage section in the navigation area on the left side of the screen.\nSearch for the group and select the group in the groups table.\nClick the Delete button at the top of the table.\nOnce the groups are removed from Keycloak, follow the instructions in Re-Sync Keycloak Users to Compute Nodes to update the groups on the compute nodes.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/pim/",
	"title": "PIM-SM bootstrap router (BSR) and rendezvous-point (RP)",
	"tags": [],
	"description": "",
	"content": "PIM-SM bootstrap router (BSR) and rendezvous-point (RP) \u0026ldquo;Every PIM multicast group needs to be associated with the IP address of a Rendezvous Point (RP) [\u0026hellip;] For all senders to reach all receivers, it is crucial that all routers in the domain use the same mappings of group addresses to RP addresses. [\u0026hellip;] The BSR mechanism provides a way in which viable group-to-RP mappings can be created and rapidly distributed to all the PIM routers in a domain.\u0026rdquo; –rfc5059\nRelevant Configuration\nEnable PIM protocol\nswitch(config)# router pim Configuring static address of rendezvous point for multicast group:\nswitch (config) # ip pim rp-address 10.10.10.10 switch (config) # ip pim vrf default rp-address 100.100.100.100 group-list 233.3.3.3/32 bidir Configure PIM BSR candidate\nswitch (config) # ip pim bsr-candidate vlan 10 priority 100 Configure PIM RP candidate\nswitch (config) # ip pim vrf default rp-candidate ethernet 1/12 group-list 225.1.0.0/16 switch (config) # ip pim vrf default rp-candidate ethernet 1/12 bidir Show Commands to Validate Functionality\nswitch# show ip pim protocol Expected Results.\nYou can configure OSPF routing for loopback1. You successfully enabled PIM-SM on loopback1. You configured loopback1 to act as a PIM-SM RP. You configured the specific group-prefix that will be used in the next test. You successfully enabled the BSR on both 8325s using loopback0 as the BSR source IP. Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/native_vlan/",
	"title": "Native VLAN",
	"tags": [],
	"description": "",
	"content": "Native VLAN Untagged ingress packets are destined to the native VLAN. An interface can be configured in one of two native modes - Native-Untagged or Native-Tagged. A native-untagged port accepts any untagged or tagged (with native VLAN ID) traffic on ingress. Packets that egress on a native-untagged port in the native VLAN will not have an 802.1Q header. A native-tagged port accepts only tagged traffic (with native VLAN ID) on ingress. Any untagged packet ingress on a native-tagged port is always dropped. Packets that egress on a native-tagged port in the native VLAN will always have an 802.1Q header.\nConfiguration Commands Configure a VLAN as native:\nswitch(config-if)# vlan trunk native VLAN Show commands to validate functionality:\nswitch# show vlan [VLAN] Example Output switch(config)# vlan 100 switch(config-vlan-100)# no shutdown switch(config-vlan-100)# end switch(config)# interface 1/1/1 switch(config-if)# no shutdown switch(config-if)# no routing switch(config-if)# vlan trunk native 100 switch(config-if)# exit switch# show vlan -------------------------------------------------------------------------------------- VLAN Name Status Reason Type Interfaces -------------------------------------------------------------------------------------- 1 DEFAULT_VLAN_1 down no_member_port default 100 VLAN100 up ok static 1/1/1 Expected Results Administrators can create a VLAN Administrators can assign a native VLAN to the physical interface Administrators can configure an IP address on the VLAN interface Administrators can successfully ping the other switch\u0026rsquo;s VLAN interface Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/setting_uai_timeouts/",
	"title": "Setting UAI Timeouts",
	"tags": [],
	"description": "",
	"content": "Setting UAI Timeouts The procedures and specific values used for setting UAI timeouts are explained in the UAI Classes section. Please refer to that section.\nOn systems where UAIs are used as part of normal user activities, the number of UAIs can grow large. Stale UAIs (i.e. UAIs that sit idle for long periods of time) can prevent creation of fresh UAIs for users who are actually active on the system. To address this without constant administrative monitoring, UAS permits the administrator to place both hard and soft timeouts on UAI Classes, which then constrain the amount of time a UAI can exist before it will terminate and need to be recreated. UAI timeouts are particularly useful in the Brokered UAI Management mode because automatic creation of UAIs makes the coming and going of UAIs seamless, and also tends to cause the creation of many UAIs. UAI timeouts are also useful in the Legacy UAI Management mode, however, since, in that mode, it is common for users to create a UAI for a task and forget to remove it when the task is done. This can be compounded by the user forgetting that a UAI already exists and making another.\nThe choice of what kind of timeouts and the duration of the timeouts is likely to be very site specific, however some general guidelines apply, especially when it comes to choosing what kinds of timeouts to use.\nIf a Class of UAI has users who tend to remain logged in and actively using their UAIs for long periods of time, it can make sense to set a soft timeout without setting a hard timeout on that UAI Class. In this case, a reasonably aggressive (30 minutes, for example) soft timeout can keep idle UAIs to a minimum while not impeding users who need to remain logged into their UAIs for days or weeks.\nNOTE: Consider moving users with workflows like the above onto UANs instead of UAIs if the site provides UANs due to the inherent impermanence of End-User UAIs.\nIf a UAI Class is intended to provide UAIs for occasional launching or checking the status of workload management jobs and not for extended login sessions, it may make sense to set a fairly aggressive hard timeout (10 minutes, for example) and a very aggressive (30 seconds, for example) soft timeout. This will make sure that users do not overstay their welcome in this class of UAI, and will generally cause the UAI to terminate as soon as the user logs out.\nIf a UAI Class creates more general purpose UAIs that are neither especially disruptive when they time out nor intended for especially short term work, a somewhat generous (24 hours, for example) hard timeout combined with a fairly aggressive (30 minutes, for example) soft timeout will keep idle UAIs of that class under control while permitting longer term users to stay logged in without giving them unlimited login durations.\nIf hard timeouts are used, a warning should usually be added to the timeout specification in the UAI Class. This will give users a chance to finish up any in-progress work and log out prior to termination of the UAI as a result of the hard timeout.\nTop: User Access Service (UAS)\nNext Topic: Broker UAI Resiliency and Load Balancing\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/remove_the_email_mapper_from_the_ldap_user_federation/",
	"title": "Remove the Email Mapper from the LDAP User Federation",
	"tags": [],
	"description": "",
	"content": "Remove the Email Mapper from the LDAP User Federation The email mapper is automatically added to the LDAP user federation in Keycloak, but it can be removed. The system does not use the user\u0026rsquo;s email for anything, so this function can be removed.\nIf there are duplicate email addresses for LDAP users, it can cause Keycloak to have issues syncing with LDAP. Removing the email mapper will fix this problem.\nProcedure Create a function to get a token as a Keycloak master administrator.\nMASTER_USERNAME=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.user}\u0026#39; | base64 -d) MASTER_PASSWORD=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d) SITE_DOMAIN=\u0026#34;$(craysys metadata get site-domain)\u0026#34; SYSTEM_NAME=\u0026#34;$(craysys metadata get system-name)\u0026#34; AUTH_FQDN=\u0026#34;auth.cmn.${SYSTEM_NAME}.${SITE_DOMAIN}\u0026#34; function get_master_token { curl -ks -d client_id=admin-cli -d username=\u0026#34;${MASTER_USERNAME}\u0026#34; -d password=\u0026#34;${MASTER_PASSWORD}\u0026#34; \\ -d grant_type=password \u0026#34;https://${AUTH_FQDN}/keycloak/realms/master/protocol/openid-connect/token\u0026#34; | \\ jq -r .access_token } Verify that the new get_master_token function is working.\nIf the function is working, it will return a long string that is base-64-encoded.\nncn-w001# get_master_token eyJhbGciO...YceX4Ig Get the ID of the LDAP user federation.\nReplace SHASTA-USER-FEDERATION-LDAP in the command below with the name of the user federation being used.\nncn-mw# FEDERATION_ID=$(curl -s -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \\ \u0026#34;https://${AUTH_FQDN}/keycloak/admin/realms/shasta/components?name=shasta-user-federation-ldap\u0026#34; \\ | jq .[0].id -r) ncn-mw# echo $FEDERATION_ID e080d20a-51b0-40ad-8f21-98f7b752e39c Get the ID of the email mapper.\nncn-mw# EMAIL_COMPONENT_ID=$(curl -s -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \\ \u0026#34;https://${AUTH_FQDN}/keycloak/admin/realms/shasta/components?name=email\u0026amp;parent=$FEDERATION_ID\u0026#34; \\ | jq .[0].id -r) ncn-mw# # echo $EMAIL_COMPONENT_ID ba3cfe20-c2ed-4c92-aac0-3b6fc865989c Delete the email mapper.\nncn-mw# curl -i -s -XDELETE -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \\ \u0026#34;https://${AUTH_FQDN}/keycloak/admin/realms/shasta/components/$EMAIL_COMPONENT_ID\u0026#34; Verify in the Keycloak UI that there is no longer an email mapper for the LDAP user federation.\nFor more information on accessing the Keycloak UI, see Access the Keycloak User Management UI.\nThe email row shown in the image below should no longer be present.\nClick the Synchronize all users button in the Settings tab for the LDAP user federation.\nThe Synchronize all users button will be at the bottom of the page.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/reboot_pxe_fail/",
	"title": "Rebooting NCN and PXE fails",
	"tags": [],
	"description": "",
	"content": "Rebooting NCN and PXE fails Common Error messages.\n2021-04-19 23:27:09 PXE-E18: Server response timeout. 2021-02-02 17:06:13 PXE-E99: Unexpected network error. Verify the ip helper-address on VLAN 1 on the switches.\nThis is the same configuration as above \u0026ldquo;Aruba Configuration\u0026rdquo;.\nVerify DHCP packets can be forwarded from the workers to the MTL network (VLAN1)\nIf the Worker nodes cannot reach the metal network DHCP will fail. ALL WORKERS need to be able to reach the MTL network! This can normally be achieved by having a default route Simple connectivity tests below:\nncn-w001# ping 10.1.0.1 PING 10.1.0.1 (10.1.0.1) 56(84) bytes of data. 64 bytes from 10.1.0.1: icmp_seq=1 ttl=64 time=0.361 ms 64 bytes from 10.1.0.1: icmp_seq=2 ttl=64 time=0.145 ms If this fails you may have a misconfigured CAN or need to add a route to the MTL network.\nncn-w001# ip route add 10.1.0.0/16 via 10.252.0.1 dev bond0.nmn0 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/ncn_tcpdump/",
	"title": "NCN tcpdump",
	"tags": [],
	"description": "",
	"content": "NCN tcpdump Running a packet capture View traffic captured to a file Filter packet capture output Running a packet capture If a host is not getting an IP address, then run a packet capture to see if DHCP traffic is being transmitted.\nThis example will look for DHCP traffic on interface bond0.nmn0. It will collect all DHCP traffic on ports 67 and 68, and write the output to a file named dhcp.pcap in the current directory.\nncn-mw# tcpdump -w dhcp.pcap -envli bond0.nmn0 port 67 or port 68 View traffic captured to a file To view previously captured traffic from a generated file:\nThis example uses the file generated from the Running a packet capture example.\nncn-mw# tcpdump -r dhcp.pcap -v -n Filter packet capture output Use filters to sort the output if it is very long.\nTo do a tcpdump for a specific MAC address:\nThis example uses the MAC address of b4:2e:99:3b:70:30, and shows the output on the terminal rather than saving it to a file.\nncn-mw# tcpdump -i eth0 -vvv -s 1500 \u0026#39;((port 67 or port 68) and (udp[38:4] = 0x993b7030))\u0026#39; Back to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/setting_up_multi-replica_brokers/",
	"title": "Broker UAI Resiliency and Load Balancing",
	"tags": [],
	"description": "",
	"content": "Broker UAI Resiliency and Load Balancing Broker UAI resiliency and load balancing is achieved through the use of Multi-Replica Broker UAIs. The procedures and data involved in configuring a UAI Class to create Multi-Replica Broker UAIs can be found in UAI Classes. This page describes some of the reasons to use Multi-Replica Broker UAIs and some of the implications of doing so.\nWhen a Broker UAI runs with multiple replicas, access to the broker remains channeled through a single external IP address, but the connections are load balanced and dispatched to multiple Kubernetes pods where the Broker UAI functionality is running. This has two beneficial effects:\nSSH Connections to Broker UAIs are load balanced so that no single broker carries all of the weight of users logged into or copying data to UAIs of a given class Individual Broker UAI pods can be evicted or restarted by Kubernetes without interrupting access to End-User UAIs NOTE: When a Broker UAI pod terminates for any reason, all SSH sessions going through that pod are dropped. This is because the Broker UAI pods forward SSH sessions to the End-User UAIs, so they are always an active part of the connection.\nThe number of replicas a Multi-Replica UAI Broker should have is dictated primarily by the number of host nodes on which Broker UAIs can be deployed. From a load-balancing perspective, it makes sense to make the number of replicas equal to the number of available host nodes. From a resiliency perspective, that number could be considerably smaller (3 for example) on the assumption that multi-node failures or evictions are unlikely, and brokers that are evicted or restarted will start up relatively quickly elsewhere. It does not make sense from either perspective, however, to over-subscribe the available number of host nodes (except, perhaps during a temporary outage) since that will result in no additional resiliency and the potential for network traffic and resource consumption hot spots.\nTop: User Access Service (UAS)\nNext Topic: Broker Mode UAI Management\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/remove_the_ldap_user_federation_from_keycloak/",
	"title": "Remove the LDAP User Federation from Keycloak",
	"tags": [],
	"description": "",
	"content": "Remove the LDAP User Federation from Keycloak Use the Keycloak UI or Keycloak REST API to remove the LDAP user federation from Keycloak.\nRemoving user federation is useful if the LDAP server was decommissioned or if the administrator would like to make changes to the Keycloak configuration using the Keycloak user localization tool.\nPrerequisites LDAP user federation is currently configured in Keycloak.\nProcedure Follow the steps in only one of the sections below:\nUse the Keycloak administration console UI Use the Keycloak REST API Use the Keycloak administration console UI Log in to the administration console.\nSee Access the Keycloak User Management UI for more information.\nClick on User Federation under the Configure header of the navigation panel on the left side of the page.\nClick on the Delete button on the line for the LDAP provider in the User Federation table.\nUse the Keycloak REST API Create a function to get a token as a Keycloak master administrator.\nncn-mw# MASTER_USERNAME=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.user}\u0026#39; | base64 -d) ncn-mw# MASTER_PASSWORD=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d) ncn-mw# SITE_DOMAIN=\u0026#34;$(craysys metadata get site-domain)\u0026#34; ncn-mw# SYSTEM_NAME=\u0026#34;$(craysys metadata get system-name)\u0026#34; ncn-mw# AUTH_FQDN=\u0026#34;auth.cmn.${SYSTEM_NAME}.${SITE_DOMAIN}\u0026#34; ncn-mw# function get_master_token { curl -ks -d client_id=admin-cli -d username=\u0026#34;${MASTER_USERNAME}\u0026#34; -d password=\u0026#34;${MASTER_PASSWORD}\u0026#34; \\ -d grant_type=password \u0026#34;https://${AUTH_FQDN}/keycloak/realms/master/protocol/openid-connect/token\u0026#34; | \\ jq -r .access_token } Get the component ID for the LDAP user federation.\nncn-mw# COMPONENT_ID=$(curl -s -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \\ \u0026#34;https://${AUTH_FQDN}/keycloak/admin/realms/shasta/components\u0026#34; \\ | jq -r \u0026#39;.[] | select(.providerId==\u0026#34;ldap\u0026#34;).id\u0026#39;) ncn-mw# echo \u0026#34;${COMPONENT_ID}\u0026#34; Example output:\n57817383-e4a0-4717-905a-ea343c2b5722 Delete the LDAP user federation by performing a DELETE operation on the LDAP resource.\nncn-mw# curl -i -XDELETE -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \u0026#34;https://${AUTH_FQDN}/keycloak/admin/realms/shasta/components/${COMPONENT_ID}\u0026#34; If the operation is successful, then the expected HTTP status code is 204. In this case, the command output should begin with the following line:\nHTTP/2 204 "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/remote_logging/",
	"title": "Remote logging",
	"tags": [],
	"description": "",
	"content": "Remote logging \u0026ldquo;In its most simplistic terms, the syslog protocol provides a transport to allow a machine to send event notification messages across IP networks to event message collectors - also known as syslog servers.\u0026rdquo; –rfc3164\nNote: the default facility is 3(DAEMON)\nRelevant Configuration\nConfigure logging\nswitch(config)# logging \u0026lt;syslog-ip-address\u0026gt; [trap {\u0026lt;log-level\u0026gt; | override class \u0026lt;class\u0026gt; priority \u0026lt;log-level\u0026gt;}] Expected Results\nStep 1: You can configure remote logging Step 2: You can see the log files from the switch on the remote server Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/ncns_on_install/",
	"title": "NCNs on Install",
	"tags": [],
	"description": "",
	"content": "NCNs on Install Use this procedure to verify the DNSMASQ config file on the NCNs is accurate.\nProcedure Verify the DNSMASQ config file matches what is configured on the switches.\nThe following is a DNSMASQ config file for the Metal network (VLAN1). The router is 10.1.0.1, which has to match what the IP address is on the switches doing the routing for the Metal (MTL) network.\nExample MTL DNSMASQ file:\n# MTL: server=/mtl/ address=/mtl/ domain=mtl,10.1.1.0,10.1.1.233,local dhcp-option=interface:bond0,option:domain-search,mtl interface=bond0 interface-name=pit.mtl,bond0 This is most commonly on the spines. This configuration is commonly missed on the CSI input file.\nVerify it points to the LiveCD IP address for provisioning in bare-metal environments:\ndhcp-option=interface:bond0,option:dns-server,10.1.1.2 dhcp-option=interface:bond0,option:ntp-server,10.1.1.2 Verify it points at the router for the network; the L3/IP for the VLAN:\ndhcp-option=interface:bond0,option:router,10.1.0.1 dhcp-range=interface:bond0,10.1.1.33,10.1.1.233,10m Configuration Example The following is an example Aruba configuration for the spine:\nsw-spine-001# show run int vlan 1 interface vlan1 vsx-sync active-gateways ip address 10.1.0.2/16 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.1.0.1 ip mtu 9198 ip bootp-gateway 10.1.0.2 ip helper-address 10.92.100.222 exit sw-spine-002# show run int vlan 1 interface vlan1 vsx-sync active-gateways ip address 10.1.0.3/16 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.1.0.1 ip mtu 9198 ip helper-address 10.92.100.222 exit Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/special_purpose_uais/",
	"title": "Special Purpose UAIs",
	"tags": [],
	"description": "",
	"content": "Special Purpose UAIs Even though most UAIs are End-User UAIs, UAI classes make it possible to construct UAIs to serve special purposes that are not strictly end-user oriented.\nOne kind of special purpose UAI is the Broker UAI, which provides on demand End-User UAI launch and management. While no other specialty UAI types currently exist, other applications are expected to arise and sites are encouraged to innovate as needed.\nTop: User Access Service (UAS)\nNext Topic: Elements of a UAI\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/restrict_access_to_ncn_images_s3_bucket/",
	"title": "Restrict Network Access to the ncn-images S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Restrict Network Access to the ncn-images S3 Bucket The configuration documented in this procedure is intended to prevent user-facing dedicated nodes (UANs, Compute Nodes) from retrieving NCN image content from Ceph S3 services, as running on storage nodes.\nSpecifically, the controls enacted via this procedure should do the following:\nBlock HAProxy access to the ncn-images bucket if the client is not an NCN (NMN) or PXE booting from the MTL network. This via a HAProxy ACL on the storage servers. Enable access logging for HAProxy. Block Rados GW network access (port 8080) to if the client is not an NCN (NMN) or originating from the HMN network. This via iptables rules on the storage servers. Limitations This is not designed to prevent UAIs (if in use) from retrieving NCN image content.\nIf a storage node is rebuilt, this procedure (for the rebuilt node) will need to be applied after the rebuild. The same is true if NCNs are added or removed from the system.\nPrerequisites and scope Procedure should be executed after install or upgrade is otherwise complete, but prior to opening the system for user access.\nUnless otherwise noted, the procedure should be run from ncn-m001 (not PIT).\nThe configuration applied in this procedure was tested against barebones images boot, FAS firmware upgrade and downgrade, and NCN rebuilds. The version tested was CSM 1.2.0.\nProcedure Test connectivity before applying the ACL.\nSave the following script to a file (for example, con_test.sh).\n#!/bin/bash SNCNS=\u0026#34;$(grep \u0026#39;ncn-s.*\\.nmn\u0026#39; /etc/hosts | awk \u0026#39;{print $NF;}\u0026#39; | xargs)\u0026#34; SCSNS_NMN=\u0026#34;$(echo $SNCNS | xargs -n 1 | sed -e \u0026#39;s/$/.nmn/g\u0026#39;)\u0026#34; SCSNS_HMN=\u0026#34;$(echo $SNCNS | xargs -n 1 | sed -e \u0026#39;s/$/.hmn/g\u0026#39;)\u0026#34; SCSNS_CMN=\u0026#34;$(echo $SNCNS | xargs -n 1 | sed -e \u0026#39;s/$/.cmn/g\u0026#39;)\u0026#34; RADOS_HTTP_PORT=\u0026#34;8080\u0026#34; HAPROXY_HTTP_PORT=\u0026#34;80\u0026#34; HAPROXY_HTTPSPORT=\u0026#34;443\u0026#34; PASS=\u0026#34;PASS\u0026#34; FAIL=\u0026#34;FAIL\u0026#34; function rados_test { NODES=\u0026#34;$1\u0026#34; MSG=\u0026#34;$2\u0026#34; TTYPE=\u0026#34;$3\u0026#34; echo \u0026#34;[i] $MSG\u0026#34; for n in $NODES do echo -n \u0026#34; RADOS $n: \u0026#34; if [ \u0026#34;$TTYPE\u0026#34; == \u0026#34;CONN_FAIL\u0026#34; ] then curl -sI --connect-timeout 2 http://${n}:${RADOS_HTTP_PORT}/ \u0026amp;\u0026gt; /dev/null rc=$? rc_pass=28 else curl -I --connect-timeout 2 http://${n}:${RADOS_HTTP_PORT}/ 2\u0026gt;/dev/null | grep -q \u0026#34;200 OK\u0026#34; rc=$? rc_pass=0 fi if [ $rc -eq $rc_pass ] then echo $PASS else echo $FAIL fi done } function haproxy_test { NODES=\u0026#34;$1\u0026#34; MSG=\u0026#34;$2\u0026#34; echo \u0026#34;[i] $MSG\u0026#34; for n in $NODES do echo -n \u0026#34; HAPROXY (CEPH) HTTP $n: \u0026#34; curl -I --connect-timeout 2 http://${n}:${HAPROXY_HTTP_PORT}/ncn-images/ 2\u0026gt;/dev/null | grep -q \u0026#34;x-amz-request-id\u0026#34; if [ $? -eq 0 ] then echo $PASS else echo $FAIL fi echo -n \u0026#34; HAPROXY (CEPH) HTTPS $n: \u0026#34; curl -kI --connect-timeout 2 https://${n}:${HAPROXY_HTTPS_PORT}/ncn-images/ 2\u0026gt;/dev/null | grep -q \u0026#34;x-amz-request-id\u0026#34; if [ $? -eq 0 ] then echo $PASS else echo $FAIL fi done } rados_test \u0026#34;$SCSNS_NMN\u0026#34; \u0026#34;MGMT RADOS over NMN\u0026#34; rados_test \u0026#34;$SCSNS_HMN\u0026#34; \u0026#34;MGMT RADOS over HMN\u0026#34; rados_test \u0026#34;$SCSNS_CMN\u0026#34; \u0026#34;MGMT RADOS over CMN\u0026#34; \u0026#34;CONN_FAIL\u0026#34; haproxy_test \u0026#34;$SCSNS_NMN\u0026#34; \u0026#34;MGMT HAProxy over NMN\u0026#34; Execute the script, if the ACLs have not been applied, results similar to the following will be returned:\nncn-m001# bash ./con_test.sh [i] MGMT RADOS over NMN RADOS ncn-s001.nmn: PASS RADOS ncn-s002.nmn: PASS RADOS ncn-s003.nmn: PASS [i] MGMT RADOS over HMN RADOS ncn-s001.hmn: PASS RADOS ncn-s002.hmn: PASS RADOS ncn-s003.hmn: PASS [i] MGMT RADOS over CMN RADOS ncn-s001.cmn: FAIL RADOS ncn-s002.cmn: FAIL RADOS ncn-s003.cmn: FAIL [i] MGMT HAProxy over NMN HAPROXY (CEPH) HTTP ncn-s001.nmn: PASS HAPROXY (CEPH) HTTPS ncn-s001.nmn: PASS HAPROXY (CEPH) HTTP ncn-s002.nmn: PASS HAPROXY (CEPH) HTTPS ncn-s002.nmn: PASS HAPROXY (CEPH) HTTP ncn-s003.nmn: PASS HAPROXY (CEPH) HTTPS ncn-s003.nmn: PASS Configure HAProxy ACLs and logging.\nRequired to limit RGW VIP access for the ncn-images bucket to only management NCNs.\nBuild an IP address list of NCNs on the NMN.\nCross-check to verify the count seems appropriate for the system in use.\nncn-m001# grep \u0026#39;ncn-[mws].*.nmn\u0026#39; /etc/hosts | awk \u0026#39;{print $1;}\u0026#39; | sed -e \u0026#39;s/\\./ /g\u0026#39; | sort -nk 4 | sed -e \u0026#39;s/ /\\./g\u0026#39; | tee allowed_ncns.lst 10.252.1.4 10.252.1.5 10.252.1.6 10.252.1.7 10.252.1.8 10.252.1.9 10.252.1.10 10.252.1.11 10.252.1.12 10.252.1.13 10.252.1.14 Add the MTL subnet (needed for network boots of NCNs).\nncn-m001# echo \u0026#39;10.1.0.0/16\u0026#39; \u0026gt;\u0026gt; allowed_ncns.lst Verify the allowed_ncns.lst contains contain NMN addresses for all management NCNs nodes and the MTL subnet (10.1.0.0/16).\nncn-m001# cat allowed_ncns.lst 10.252.1.4 10.252.1.5 10.252.1.6 10.252.1.7 10.252.1.8 10.252.1.9 10.252.1.10 10.252.1.11 10.252.1.12 10.252.1.13 10.252.1.14 10.1.0.0/16 Confirm HAProxy configurations are identical across storage nodes.\nAdjust the -w predicate to represent the full set of storage nodes for the system. Applies to this step and subsequent steps.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;cat /etc/haproxy/haproxy.cfg\u0026#34; | dshbak -c ---------------- ncn-s[001-004] ---------------- # Please do not change this file directly since it is managed by Ansible and will be overwritten global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 8000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats tune.ssl.default-dh-param 4096 ssl-default-bind-ciphers EECDH+AESGCM:EDH+AESGCM ssl-default-bind-options no-sslv3 no-tlsv10 no-tlsv11 no-tls-tickets defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 8000 frontend http-rgw-frontend bind *:80 default_backend rgw-backend frontend https-rgw-frontend bind *:443 ssl crt /etc/ceph/rgw.pem default_backend rgw-backend backend rgw-backend option forwardfor balance static-rr option httpchk GET / server server-ncn-s001-rgw0 10.252.1.7:8080 check weight 100 server server-ncn-s002-rgw0 10.252.1.6:8080 check weight 100 server server-ncn-s003-rgw0 10.252.1.5:8080 check weight 100 server server-ncn-s004-rgw0 10.252.1.4:8080 check weight 100 Create a backup of haproxy.cfg files on storage nodes.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg-dist\u0026#34; Grab a copy of haproxy.cfg to modify from a storage node, preserving permissions.\nncn-m001# scp -p ncn-s001:/etc/haproxy/haproxy.cfg . haproxy.cfg Edit the haproxy.cfg, adding in the following ACLs and log directives to each front-end.\nncn-m001# diff -Naur haproxy.cfg-dist haproxy.cfg --- haproxy.cfg-dist 2022-06-30 18:20:55.000000000 +0000 +++ haproxy.cfg 2022-07-07 16:56:40.000000000 +0000 @@ -1,6 +1,6 @@ # Please do not change this file directly since it is managed by Ansible and will be overwritten global - log 127.0.0.1 local2 + log 127.0.0.1:514 local0 info chroot /var/lib/haproxy pidfile /var/run/haproxy.pid @@ -31,12 +31,22 @@ maxconn 8000 frontend http-rgw-frontend + log global + option httplog bind *:80 default_backend rgw-backend + acl allow_ncns src -n -f /etc/haproxy/allowed_ncns.lst + acl restrict_ncn_images path_beg /ncn-images + http-request deny if restrict_ncn_images !allow_ncns frontend https-rgw-frontend + log global + option httplog bind *:443 ssl crt /etc/ceph/rgw.pem default_backend rgw-backend + acl allow_ncns src -n -f /etc/haproxy/allowed_ncns.lst + acl restrict_ncn_images path_beg /ncn-images + http-request deny if restrict_ncn_images !allow_ncns backend rgw-backend option forwardfor Create a new rsyslog configuration for HAProxy to have it listen to UDP 514 on the local host.\nWith the log directive additions to HAProxy, and allowing a local host UDP 514 socket, access logging should work properly. Set permissions to 640 on the file.\nncn-m001# cat haproxy.conf # Collect log with UDP $ModLoad imudp $UDPServerAddress 127.0.0.1 $UDPServerRun 514 ncn-m001# chmod 0640 haproxy.conf Make sure HAProxy is running on storage nodes.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;systemctl status haproxy\u0026#34; | grep \u0026#34;Active\u0026#34; ncn-s001: Active: active (running) since Thu 2022-07-07 17:38:49 UTC; 54min ago ncn-s003: Active: active (running) since Thu 2022-07-07 17:38:49 UTC; 54min ago ncn-s002: Active: active (running) since Thu 2022-07-07 17:38:49 UTC; 54min ago ncn-s004: Active: active (running) since Thu 2022-07-07 17:38:49 UTC; 54min ago Determine where the HAProxy VIP currently resides (for awareness in the event debug is necessary).\nncn-m001# host rgw-vip rgw-vip.nmn has address 10.252.1.3 ncn-m001# host rgw-vip.nmn rgw-vip.nmn has address 10.252.1.3 ncn-m001# host 10.252.1.3 3.1.252.10.in-addr.arpa domain name pointer rgw-vip. 3.1.252.10.in-addr.arpa domain name pointer rgw-vip.local. 3.1.252.10.in-addr.arpa domain name pointer rgw-vip.local.local. 3.1.252.10.in-addr.arpa domain name pointer rgw-vip.nmn. 3.1.252.10.in-addr.arpa domain name pointer rgw-vip.nmn.local. ncn-m001# ssh rgw-vip \u0026#39;hostname\u0026#39; ncn-s001 Propagate the rsyslog configuration out to all storage nodes.\nncn-m001# pdcp -w ncn-s00[1-4] haproxy.conf /etc/rsyslog.d/ Propagate the HAProxy configuration out to all storage nodes.\nncn-m001# pdcp -w ncn-s00[1-4] haproxy.cfg allowed_ncns.lst /etc/haproxy/ Verify the configurations are identical across storage nodes.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;cat /etc/haproxy/haproxy.cfg\u0026#34; | dshbak -c ---------------- ncn-s[001-004] ---------------- # Please do not change this file directly since it is managed by Ansible and will be overwritten global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 8000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats tune.ssl.default-dh-param 4096 ssl-default-bind-ciphers EECDH+AESGCM:EDH+AESGCM ssl-default-bind-options no-sslv3 no-tlsv10 no-tlsv11 no-tls-tickets defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 8000 frontend http-rgw-frontend bind *:80 default_backend rgw-backend acl allow_ncns src -n -f /etc/haproxy/allowed_ncns.lst acl restrict_ncn_images path_beg /ncn-images http-request deny if restrict_ncn_images !allow_ncns frontend https-rgw-frontend bind *:443 ssl crt /etc/ceph/rgw.pem default_backend rgw-backend acl allow_ncns src -n -f /etc/haproxy/allowed_ncns.lst acl restrict_ncn_images path_beg /ncn-images http-request deny if restrict_ncn_images !allow_ncns backend rgw-backend option forwardfor balance static-rr option httpchk GET / server server-ncn-s001-rgw0 10.252.1.7:8080 check weight 100 server server-ncn-s002-rgw0 10.252.1.6:8080 check weight 100 server server-ncn-s003-rgw0 10.252.1.5:8080 check weight 100 server server-ncn-s004-rgw0 10.252.1.4:8080 check weight 100 Restart rsyslog across all storage nodes.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;systemctl restart rsyslog\u0026#34; ncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;systemctl status rsyslog\u0026#34; | grep Active ncn-s001: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago ncn-s002: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago ncn-s003: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago ncn-s004: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago Restart HAProxy across all storage nodes.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;systemctl restart haproxy\u0026#34; ncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;systemctl status haproxy\u0026#34; | grep Active ncn-s001: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago ncn-s002: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago ncn-s003: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago ncn-s004: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago Apply server-side iptables rules to storage nodes.\nThis is needed to prevent direct access to the Ceph Rados GW Service (not through HAProxy).\nThe process is written to support change on individual nodes, but could be scripted after analysis of the running firewall rule set (notably with respect to local modifications, if they exist).\nThis process must be completed on each storage node (steps 19 - 22) before continuing to subsequent steps.\nDocument where Rados GW is running (port wise).\nncn-s001# ss -tnpl | grep rados LISTEN 0 128 0.0.0.0:8080 0.0.0.0:* users:((\u0026#34;radosgw\u0026#34;,pid=25018,fd=77)) LISTEN 0 128 [::]:8080 [::]:* users:((\u0026#34;radosgw\u0026#34;,pid=25018,fd=78)) List existing iptables rules.\nncn-s001# iptables -L -nx -v Chain INPUT (policy ACCEPT 399480930 packets, 1051007801113 bytes) pkts bytes target prot opt in out source destination 0 0 DROP tcp -- * * 0.0.0.0/0 10.102.4.135 tcp dpt:22 Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 400599807 packets, 1035420933926 bytes) pkts bytes target prot opt in out source destination Run the following to add iptables rules for control.\nThe range should include all NMN NCN IP addresses generated for the HAProxy ACL step.\niptables -A INPUT -i bond0.hmn0 -p tcp --dport 8080 -j ACCEPT iptables -A INPUT -i lo -p tcp --dport 8080 -j ACCEPT iptables -A INPUT -p tcp --dport 8080 -m iprange --src-range 10.252.1.4-10.252.1.14 -j ACCEPT iptables -A INPUT -p tcp --dport 8080 -j LOG --log-prefix \u0026#34;RADOSGW-DROP\u0026#34; iptables -A INPUT -p tcp --dport 8080 -j DROP List iptables rules again, verify rules are in place.\nncn-s001# iptables -L -nx -v Chain INPUT (policy ACCEPT 22144 packets, 28721015 bytes) pkts bytes target prot opt in out source destination 0 0 DROP tcp -- * * 0.0.0.0/0 10.102.4.135 tcp dpt:22 0 0 ACCEPT tcp -- bond0.hmn0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 85 4862 ACCEPT tcp -- lo * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 276 15438 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 source IP range 10.252.1.4-10.252.1.14 0 0 LOG tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 LOG flags 0 level 4 prefix \u0026#34;RADOSGW-DROP\u0026#34; 0 0 DROP tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 22099 packets, 30141111 bytes) pkts bytes target prot opt in out source destination Test connectivity after applying the ACL.\nRe-run the connectivity test. While the results will be similar, they should all now be passing:\nncn-m001# bash ./con_test.sh [i] MGMT RADOS over NMN RADOS ncn-s001.nmn: PASS RADOS ncn-s002.nmn: PASS RADOS ncn-s003.nmn: PASS [i] MGMT RADOS over HMN RADOS ncn-s001.hmn: PASS RADOS ncn-s002.hmn: PASS RADOS ncn-s003.hmn: PASS [i] MGMT RADOS over CMN RADOS ncn-s001.cmn: PASS RADOS ncn-s002.cmn: PASS RADOS ncn-s003.cmn: PASS [i] MGMT HAProxy over NMN HAPROXY (CEPH) HTTP ncn-s001.nmn: PASS HAPROXY (CEPH) HTTPS ncn-s001.nmn: PASS HAPROXY (CEPH) HTTP ncn-s002.nmn: PASS HAPROXY (CEPH) HTTPS ncn-s002.nmn: PASS HAPROXY (CEPH) HTTP ncn-s003.nmn: PASS HAPROXY (CEPH) HTTPS ncn-s003.nmn: PASS Validate no connection can be made to HAProxy for ncn-images or Ceph RADOS GW (at all) from compute nodes and UANs.\nUse rgw-vip as it will resolve to one of the storage nodes.\nnid000002# host rgw-vip rgw-vip has address 10.252.1.3 nid000002# curl http://rgw-vip/ncn-images/ \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;403 Forbidden\u0026lt;/h1\u0026gt; Request forbidden by administrative rules. \u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; nid000002# curl --connect-timeout 2 rgw-vip:8080 curl: (28) Connection timed out after 2001 milliseconds Look for a 403 response in the HAProxy logs:\nncn-m001# pdsh -N -w ncn-s00[1-4] \u0026#34;cd /var/log \u0026amp;\u0026amp; zgrep -h -i -E \u0026#39;haproxy.*frontend\u0026#39; messages || exit 0\u0026#34; | grep \u0026#34;ncn-images\u0026#34; | grep \u0026#34;10.252.1.13\u0026#34; 2022-07-13T13:57:08+00:00 xxx-ncn-s001.local haproxy[43591]: 10.252.1.13:50238 [13/Jul/2022:13:57:08.363] http-rgw-frontend http-rgw-frontend/\u0026lt;NOSRV\u0026gt; 0/-1/-1/-1/0 403 212 - - PR-- 1/1/0/0/0 0/0 \u0026#34;GET /ncn-images/ HTTP/1.1\u0026#34; 2022-07-13T14:01:11+00:00 xxx-ncn-s001.local haproxy[43591]: 10.252.1.13:50240 [13/Jul/2022:14:01:11.038] http-rgw-frontend http-rgw-frontend/\u0026lt;NOSRV\u0026gt; 0/-1/-1/-1/0 403 212 - - PR-- 1/1/0/0/0 0/0 \u0026#34;GET /ncn-images/ HTTP/1.1\u0026#34; In the firewall logs, the Ceph RADOS GW traffic will be dropped on the storage node. For example:\nncn-m001# pdsh -N -w ncn-s00[1-4] \u0026#34;grep RADOSGW /var/log/firewall\u0026#34; | grep \u0026#34;10.252.1.13\u0026#34; | head -1 2022-07-13T14:02:03.418750+00:00 xxx-ncn-s001 kernel: [4397628.546654] RADOSGW-DROPIN=bond0.nmn0 OUT= MAC=b8:59:9f:f9:1d:22:a4:bf:01:3f:6f:91:08:00 SRC=10.252.1.13 DST=10.252.1.3 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=9727 DF PROTO=TCP SPT=59278 DPT=8080 WINDOW=42340 RES=0x00 SYN URGP=0 For further validation, the following script can be saved to a UAN or compute node with a storage node count as an input.\nThis will test cross-network select access that should not be possible based on a correctly configured switch ACL posture, as well.\nnid000002# cat user_con_test.sh CURL_O=\u0026#34;--connect-timeout 2 -f\u0026#34; NODE_COUNT=\u0026#34;$1\u0026#34; function curl_rept { echo -n \u0026#34;[i] $1 -\u0026gt; \u0026#34; $1 \u0026amp;\u0026gt; /dev/null if [ $? -ne 0 ] then echo \u0026#34;PASS\u0026#34; else echo \u0026#34;FAIL\u0026#34; fi return } for n in `seq 1 $NODE_COUNT` do for t in nmn cmn hmn do curl_rept \u0026#34;curl $CURL_O ncn-s00${n}.${t}:8080\u0026#34; # ceph rados curl_rept \u0026#34;curl $CURL_O http://ncn-s00${n}.${t}/ncn-images/\u0026#34; # ncn images, http curl_rept \u0026#34;curl $CURL_O -k https://ncn-s00${n}.${t}/ncn-images/\u0026#34; # ncn images, https done done nid000002# bash ./user_con_test.sh 4 [i] curl --connect-timeout 2 -f ncn-s001.nmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s001.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s001.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s001.cmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s001.cmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s001.cmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s001.hmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s001.hmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s001.hmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s002.nmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s002.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s002.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s002.cmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s002.cmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s002.cmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s002.hmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s002.hmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s002.hmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s003.nmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s003.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s003.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s003.cmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s003.cmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s003.cmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s003.hmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s003.hmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s003.hmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s004.nmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s004.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s004.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s004.cmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s004.cmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s004.cmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s004.hmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s004.hmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s004.hmn/ncn-images/ -\u0026gt; PASS Save the iptables rule set on all storage nodes.\niptables is currently reloaded via a one shot systemd service.\nncn-s001:/usr/lib/systemd # cat ./system/metal-iptables.service [Unit] Description=Loads Metal iptables config After=local-fs.target network.service [Service] Type=oneshot ExecStart=iptables-restore /etc/iptables/metal.conf Restart=no RemainAfterExit=no [Install] WantedBy=multi-user.target Make a backup of the firewall rules.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;cp -a /etc/iptables/metal.conf /etc/iptables/metal.conf-dist\u0026#34; Use iptables-save to commit running rules to the persistent configuration.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;iptables-save -f /etc/iptables/metal.conf\u0026#34; Verify the rule set is consistent across nodes.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;cat /etc/iptables/metal.conf\u0026#34; | grep \u0026#34;8080\u0026#34; | dshbak -c ---------------- ncn-s[001-004] ---------------- -A INPUT -i bond0.hmn0 -p tcp -m tcp --dport 8080 -j ACCEPT -A INPUT -i lo -p tcp -m tcp --dport 8080 -j ACCEPT -A INPUT -p tcp -m tcp --dport 8080 -m iprange --src-range 10.252.1.4-10.252.1.14 -j ACCEPT -A INPUT -p tcp -m tcp --dport 8080 -j LOG --log-prefix RADOSGW-DROP -A INPUT -p tcp -m tcp --dport 8080 -j DROP Troubleshooting NOTE: If SMA log forwarders are not yet running, then it might be necessary to temporarily disable the /etc/rsyslog.d/01-cray-rsyslog.conf rule (for logs to flow to the local nodes without delay). Restart rsyslog if this action is required.\nLook for RADOSGW drops in /var/log/firewall on storage nodes, not that the connectivity test will attempt access on the CMN.\nncn-m001# pdsh -N -w ncn-s00[1-3] \u0026#34;grep RADOSGW /var/log/firewall\u0026#34; 2022-07-11T19:26:22.655077+00:00 xxx-ncn-s003 kernel: [265870.330981] RADOSGW-DROPIN=bond0.cmn0 OUT= MAC=b8:59:9f:f9:1b:fa:b8:59:9f:f9:1b:fe:08:00 SRC=10.101.8.35 DST=10.101.8.43 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=1170 DF PROTO=TCP SPT=52462 DPT=8080 WINDOW=35840 RES=0x00 SYN URGP=0 2022-07-11T19:26:23.690023+00:00 xxx-ncn-s003 kernel: [265871.365959] RADOSGW-DROPIN=bond0.cmn0 OUT= MAC=b8:59:9f:f9:1b:fa:b8:59:9f:f9:1b:fe:08:00 SRC=10.101.8.35 DST=10.101.8.43 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=1171 DF PROTO=TCP SPT=52462 DPT=8080 WINDOW=35840 RES=0x00 SYN URGP=0 ... Look for HAProxy access logs in /var/log/messages on storage nodes that have HTTP 403 responses (or other responses depending upon context).\nncn-m001# pdsh -N -w ncn-s00[1-3] \u0026#34;cd /var/log \u0026amp;\u0026amp; zgrep -h \u0026#39;haproxy.*frontend\u0026#39; messages || exit 0\u0026#34; | grep \u0026#34; 403 \u0026#34; | sort -k 1 "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/requirements_and_optional_configuration/",
	"title": "How to connect management network to your campus network",
	"tags": [],
	"description": "",
	"content": "How to connect management network to your campus network In the event that you want to connect the Supercomputer directly to your campus network. In this guide we will go over the two most typical ways of accomplishing this. Further explained in Scenario A and B that will cover the examples of adding connections through management network or highspeed network.\nRequirements and optional configuration\nSystem needs to be completely installed and running. The edge router should be cabled either to the management network or Highspeed network switch. An IP address range on the management or highspeed network switch that is routable to the campus network. Other configuration items that may be required to facilitate remote connectivity however not covered in this example Configuration may require a new LAG Configuration may require a new VLAN Configuration may require a new router OSPF context Other things to consider ACL Stubby OSPF area Route restrictions i.e. only provide default route IMPORTANT: As there are multiple ways of achieving the connectivity these are just simple examples of how remote access could be achieved. And more complex configurations such as security etc. are up to the site network administrators.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/network_naming_function/",
	"title": "Network Types – Naming and Segment Function",
	"tags": [],
	"description": "",
	"content": "Network Types – Naming and Segment Function The following table provides an overview of the different network services defined inside of the spine and leaf architectures.\n*********** Administration: Hardware Administration: Cloud/Job Customer: Jobs Customer: Administration Storage Full name Hardware Management Network Node Management Network Customer Access Network Customer Management Network Storage User Network Short name / acronym HMN NMN CAN CMN SUN Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/start_a_broker_uai/",
	"title": "Start a Broker UAI",
	"tags": [],
	"description": "",
	"content": "Start a Broker UAI Create a Broker UAI after a Broker UAI class has been created.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) There must be an appropriate Broker UAI Class defined: Configure a Broker UAI Class The administrator must know the Class ID of the desired Broker UAI Class: List UAI Classes Optional: the administrator may choose a site defined name for the Broker UAI to be used in conjunction with the HPE Cray EX System External DNS mechanism.\nProcedure Use the following command to create a Broker UAI:\nncn-m001-pit# cray uas admin uais create --class-id \u0026lt;class-id\u0026gt; [--owner \u0026lt;name\u0026gt;] To make the broker obvious in the list of UAIs, giving it an owner name of broker is handy. The owner name on a broker is used for naming and filtering (for listing or deleting), but nothing else, so this is a convenient convention. Alternatively, giving it a descriptive owner to make it easy to tell the differences between brokers of different kinds can be useful. Keep in mind that the owner here can only be lower-case alphanumeric or - (dash) characters.\nThe following is an example using the class created above:\nncn-m001-pit# cray uas admin uais create --class-id d764c880-41b8-41e8-bacc-f94f7c5b053d --owner broker Example output:\nuai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh broker@35.226.246.154\u0026#34; uai_host = \u0026#34;ncn-w003\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-broker:1.2.4\u0026#34; uai_ip = \u0026#34;35.226.246.154\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-broker-70512bbb\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;broker\u0026#34; [uai_portmap] When a UAI is created on an external IP address (as is always the case with Broker UAIs) the UAI name (uai_name field above) is given to the HPE Cray EX System External DNS mechanism to be advertised to the site DNS. Unless a site defined name is used, a unique name like the one shown above is calculated and used. If a site defined DNS name for the Broker UAI is desired, the UAI name may be added to the command that creates the Broker UAI as follows:\nncn-m001-pit# cray uas admin uais create --class-id d764c880-41b8-41e8-bacc-f94f7c5b053d --owner broker --uai-name my-broker-uai Example output:\nuai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh broker@35.226.246.154\u0026#34; uai_host = \u0026#34;ncn-w003\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-broker:1.2.4\u0026#34; uai_ip = \u0026#34;35.226.246.154\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;my-broker-uai\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;broker\u0026#34; [uai_portmap] If a UAI name is specified for creation that matches that of an already created UAI, no new UAI will be created, but the creation operation will appear to succeed and will return the status of the already existing UAI.\nTop: User Access Service (UAS)\nNext Topic: Log in to a Broker UAI\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/resync_keycloak_users_to_compute_nodes/",
	"title": "Re-Sync Keycloak Users to Compute Nodes",
	"tags": [],
	"description": "",
	"content": "Re-Sync Keycloak Users to Compute Nodes Resubmit the keycloak-users-localize job and run the keycloak-users-compute.yml Ansible play to synchronize the users and groups from Keycloak to the compute nodes. This procedure alters the /etc/passwd and /etc/group files used on compute nodes.\nUse this procedure to quickly synchronize changes made in Keycloak to the compute nodes.\nPrerequisites The COS product must be installed.\nProcedure Resubmit the keycloak-users-localize job.\nThe output might appear slightly different than in the example below.\nncn-mw# kubectl get job -n services -l app.kubernetes.io/name=cray-keycloak-users-localize -ojson | jq \u0026#39;.items[0]\u0026#39; \u0026gt; keycloak-users-localize-job.json ncn-mw# cat keycloak-users-localize-job.json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels)\u0026#39; | kubectl replace --force -f - Expected output looks similar to the following:\njob.batch \u0026#34;keycloak-users-localize-1\u0026#34; deleted job.batch/keycloak-users-localize-1 replaced Watch the pod to check the status of the job.\nThe pod will go through the normal Kubernetes states. It will stay in a Running state for a while, and then it will go to Completed.\nncn-mw# kubectl get pods -n services | grep keycloak-users-localize Expected output looks similar to the following:\nkeycloak-users-localize-1-sk2hn 0/2 Completed 0 2m35s Check the pod\u0026rsquo;s logs.\nReplace the KEYCLOAK_POD_NAME value with the pod name from the previous step.\nncn-mw# kubectl logs -n services KEYCLOAK_POD_NAME keycloak-localize Expected output should contain the following line:\n2020-07-20 18:26:15,774 - INFO - keycloak_localize - keycloak-localize complete Synchronize the users and groups from Keycloak to the compute nodes.\nGet the crayvcs password for pushing the changes.\nncn-mw# kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode Checkout content from the cos-config-management VCS repository.\nncn-mw# git clone https://api-gw-service-nmn.local/vcs/cray/cos-config-management.git ncn-mw# cd cos-config-management ncn-mw# git checkout integration Create the group_vars/Compute/keycloak.yaml file.\nThe file should contain the following contents:\n--- keycloak_config_computes: True Push the changes to VCS with the crayvcs username.\nncn-mw# git add group_vars/Compute/keycloak.yaml ncn-mw# git commit -m \u0026#34;Configure keycloak on computes\u0026#34; ncn-mw# git push origin integration Eeboot the compute nodes with the Boot Orchestration Service (BOS).\nncn-mw# cray bos session create --template-uuid BOS_TEMPLATE --operation reboot "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/routed_interface/",
	"title": "Routed interfaces",
	"tags": [],
	"description": "",
	"content": "Routed interfaces By default Mellanox interfaces are set as \u0026ldquo;switchports\u0026rdquo; which is to allow L2 communication. To change to routed only port, you have to disable L2 functionality.\nRelevant Configuration\nDisable L2 functionality\nSwitch (config) # interface ethernet ¼ Switch (config-int) no switchport force Give an interface an IP address\nswitch (config) # interface ethernet 1/14 ip address 192.168.75.1/31 primary Show Commands to Validate Functionality\nswitch# show ethernet interface IFACE Expected Results\nStep 1: You are able to configure an IP address on the interface Step 2: You can configure an IP address on the connected network client Step 3: The interface is up, and you can validate the IP address and subnet are correct Step 4: You can ping from the switch to the client and from the client to the switch Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/network_topologies/",
	"title": "Network Topologies",
	"tags": [],
	"description": "",
	"content": "Network Topologies The following images are example network topologies for systems of various sizes.\nVery Large Large Medium Small "
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_broker_sssd_cant_use_sssd_conf/",
	"title": "Troubleshoot Broker UAI SSSD Cannot Use /etc/sssd/sssd.conf",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Broker UAI SSSD Cannot Use /etc/sssd/sssd.conf Symptom A Broker UAI has been created using an SSSD configuration in a secret and volume as described in Configure a Broker UAI Class, but logging into the Broker UAI does not work.\nDiagnose the problem as follows:\nFind the UAI name of the Broker UAI in a list of existing UAIs:\nncn-m001# cray uas admin uais list --format yaml Find the Broker UAI pod name by looking for a pod with the UAI name as the first part of its name in the list of Broker UAI pods:\nncn-m001# kubectl get po -n uas Obtain logs from the Broker UAI:\nncn-m001# kubectl logs -n uas \u0026lt;pod-name\u0026gt; -c \u0026lt;uai-name\u0026gt; See if the the following errors appear in the log output:\n(2022-01-28 17:46:44:642510): [sssd] [confdb_ldif_from_ini_file] (0x0020): Permission check on config file failed. (2022-01-28 17:46:44:642549): [sssd] [confdb_init_db] (0x0020): Cannot convert INI to LDIF [1]: [Operation not permitted] (2022-01-28 17:46:44:642568): [sssd] [confdb_setup] (0x0010): ConfDB initialization has failed [1]: Operation not permitted (2022-01-28 17:46:44:642644): [sssd] [load_configuration] (0x0010): Unable to setup ConfDB [1]: Operation not permitted (2022-01-28 17:46:44:642764): [sssd] [main] (0x0020): Cannot read config file /etc/sssd/sssd.conf. Please check that the file is accessible only by the owner and owned by root.root. Problem Explanation In the current release of UAS, the default Service Account on the uas namespace in Kubernetes is bound to a Cluster Role that uses a Pod Security Policy that defines a specific fsGroup range and a MustRunAs rule instead of simply using the RunAsAny rule. Because of the way Kubernetes handles volumes, when a volume containing a Secret or a ConfigMap is mounted on a Kubernetes pod with an fsGroup rule that is not RunAsAny in the Pod Security Policy, the requested mode of the volume is adjusted to something Kubernetes deems more appropriate. In this case, the requested mode (decimal 384 or octal 600) becomes octal 640 instead in the mounted volume. Unfortunately, SSSD requires that this mode be octal 600 or it will refuse to use the configuration file.\nWorkaround While this problem will be resolved in an upcoming release of UAS, if this behavior occurs, it is necessary to create a new Pod Security Policy and a Cluster Role using that Pod Security Policy, then change the existing Cluster Role Binding to bind the new Cluster Role instead of the one it currently uses. The following procedure does that.\nVerify that the system is set up the same way as the system on which this workaround was prepared. To do that, list the Cluster Role Binding named uas-default-psp and determine what Cluster Role it is bound to:\nncn-m001# kubectl get clusterrolebindings uas-default-psp -o yaml Example output:\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: [...] roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: restricted-transition-net-raw-psp subjects: - kind: ServiceAccount name: default namespace: uas Notice the roleRef setting binds the restricted-transition-net-raw-psp Cluster Role. That Cluster Role uses the restricted-transition-net-raw-psp Pod Security Policy, which is used by several CSM services, but does not work for Broker UAIs.\nAssuming the system is configured as shown above, the following steps will\nRemove the existing incorrect ClusterRoleBinding so that it can be replaced later Create a new Pod Security Policy called uas-default-psp. Create a Cluster Role called uas-default-psp that uses the new Pod Security Policy Replace the Cluster Role Binding called uas-default-psp with a new one that binds the new Cluster Role to the default Service Account in the uas namespace If the system is configured differently, it may be necessary to investigate further, which is largely beyond the scope of this section. The important thing here is that the default Service Account in the uas namespace must not be bound to a Pod Security Policy with an fsGroup or supplementalGroups configured with anything but the RunAsAny rule.\nRemove the existing Cluster Role Binding:\nncn-m001# kubectl delete clusterrolebindings uas-default-psp Prepare a YAML file containing the new Kubernetes objects:\nncn-m001# cat \u0026lt;\u0026lt; EOF \u0026gt; /tmp/uas-default-psp.yaml apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: uas-default-psp spec: allowPrivilegeEscalation: true allowedCapabilities: - NET_ADMIN - NET_RAW allowedHostPaths: - pathPrefix: /lustre - pathPrefix: /root/registry - pathPrefix: /lib/modules - pathPrefix: / - pathPrefix: /var/lib/nfsroot/nmd - pathPrefix: /lus - pathPrefix: /var/tmp/cps-local fsGroup: rule: RunAsAny hostNetwork: true privileged: true runAsUser: rule: RunAsAny seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny volumes: - configMap - emptyDir - projected - secret - downwardAPI - persistentVolumeClaim - hostPath - flexVolume --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: uas-default-psp rules: - apiGroups: - policy resourceNames: - uas-default-psp resources: - podsecuritypolicies verbs: - use --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: uas-default-psp roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: uas-default-psp subjects: - kind: ServiceAccount name: default namespace: uas EOF Apply this new configuration to Kubernetes:\nncn-m001# kubectl apply -f /tmp/uas-default-psp.yaml Delete and re-create the offending Broker UAI(s) and they should come up and SSSD should run properly.\nncn-m001# cray uas admin uais delete OPTIONS ncn-m001# cray uas admin uais create OPTIONS Top: User Access Service (UAS)\nNext Topic: Clear UAS Configuration\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/retrieve_an_authentication_token/",
	"title": "Retrieve an Authentication Token",
	"tags": [],
	"description": "",
	"content": "Retrieve an Authentication Token Retrieve a token for authenticating to one of the API gateways, for the shasta realm.\nThe following are important properties of authentication tokens:\nKeycloak access tokens remain valid for 365 days. Secrets do not expire; they are persistent in Keycloak. Tokens and secrets can be revoked at any time by an administrator. The API gateways use OAuth2 for authentication. A token is required to authenticate with one of the gateways.\nThere are multiple API gateways that are used to access services from the different networks.\nservices-gateway accessible at api.nmnlb.SYSTEM_DOMAIN_NAME or api-gw-service-nmn.local customer-admin-gateway accessible at api.cmn.SYSTEM_DOMAIN_NAME customer-user-gateway accessible at api.can.SYSTEM_DOMAIN_NAME or api.chn.SYSTEM_DOMAIN_NAME The appropriate token must be retrieved from the gateway in order to access services on that gateway.\nProcedure Some of the example commands pipe their output to python -mjson.tool. This is not required; it is simply used to format the output for readability.\nRetrieve a token.\nRetrieving a token depends on whether the request is based on a regular user (as defined directly in Keycloak or backed by LDAP) or a service account.\nResource owner password grant (user account)\nIn this case, the user account flow requires the username, password, and the client ID.\nIn the example below, replace myuser, mypass, and shasta in the command with site-specific values. The shasta client is created during the CSM install process.\nncn-mw# curl -s -d grant_type=password \\ -d client_id=shasta \\ -d username=myuser \\ -d password=mypass \\ https://auth.cmn.SYSTEM_DOMAIN_NAME/keycloak/realms/shasta/protocol/openid-connect/token | python -mjson.tool Example output:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;ey...IA\u0026#34;, \u0026#34;expires_in\u0026#34;: 300, \u0026#34;not-before-policy\u0026#34;: 0, \u0026#34;refresh_expires_in\u0026#34;: 1800, \u0026#34;refresh_token\u0026#34;: \u0026#34;ey...qg\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;profile email\u0026#34;, \u0026#34;session_state\u0026#34;: \u0026#34;10c7d2f7-8921-4652-ad1e-10138ec6fbc3\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34; } Use the value of access_token to make requests.\nClient credentials (service account)\nThe client credentials flow requires a client ID and client secret.\nThere are a couple of ways to use a service account:\nBy creating a new service account. By using the Keycloak client that was generated by the CSM installation process. The client ID is admin-client.\nThe client secret is generated during the install and put into a Kubernetes secret named admin-client-auth.\nRetrieve the client secret from this secret as follows:\nncn-mw# echo \u0026#34;$(kubectl get secrets admin-client-auth -ojsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d)\u0026#34; Example output:\n2b0d6df0-183b-40e6-93be-51c7854388a1 Given the client ID and secret, the user can retrieve a token by requesting one from Keycloak.\nIn the example below, replace the string being assigned to client_secret with the actual client secret from the previous step.\nncn-mw# curl -s -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=2b0d6df0-183b-40e6-93be-51c7854388a1 \\ https://auth.cmn.SYSTEM_DOMAIN_NAME/keycloak/realms/shasta/protocol/openid-connect/token | python -mjson.tool Expected output:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;ey...DA\u0026#34; \u0026#34;expires_in\u0026#34;: 300, \u0026#34;not-before-policy\u0026#34;: 0, \u0026#34;refresh_expires_in\u0026#34;: 1800, \u0026#34;refresh_token\u0026#34;: \u0026#34;ey...kg\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;profile email\u0026#34;, \u0026#34;session_state\u0026#34;: \u0026#34;ca8ab15c-2378-40c1-8063-7a522274fce0\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34; } Use the value of access_token to make requests.\nPresent the token.\nTo present the access token on the request, put it in the Authorization header on the request as a Bearer token.\nFor example:\nncn# TOKEN=access_token ncn# curl -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; https://api.cmn.SYSTEM_DOMAIN_NAME/apis/capmc/capmc/get_node_rules Example output:\n{ \u0026#34;e\u0026#34;:0, \u0026#34;err_msg\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;latency_node_off\u0026#34;:60, \u0026#34;latency_node_on\u0026#34;:120, \u0026#34;latency_node_reinit\u0026#34;:180, \u0026#34;max_off_req_count\u0026#34;:-1, \u0026#34;max_off_time\u0026#34;:-1, \u0026#34;max_on_req_count\u0026#34;:-1, \u0026#34;max_reinit_req_count\u0026#34;:-1, \u0026#34;min_off_time\u0026#34;:-1 } "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/scenario-a/",
	"title": "Scenario A network connection via management network",
	"tags": [],
	"description": "",
	"content": "Scenario A: network connection via management network Description The example here covers outside connections achieved via management network.\nSummary Create a new VRF Move interfaces to the new VRF Create a new BGP process for the new VRF Setup the edge router Configure MetalLB Verification step for BGP routes Configure default route for workers Verification of external communication Topology Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/network_traffic_pattern/",
	"title": "Network Traffic Pattern",
	"tags": [],
	"description": "",
	"content": "Network Traffic Pattern Internal networks:\nNode Management Network (NMN) - Provides the internal control plane for systems management and jobs control Hardware Management Network (HMN) - Provides internal access to system baseboard management controllers (BMC/iLO) and other lower-level hardware access External and Edge networks:\nCustomer Management Network (CMN) - Provides customer access from the site to the system for administrators Customer Access Network (CAN) or Customer High Speed Network (CHN) provide: Customer access from the site to the system for job control and jobs data movement Access from the system to the site for network services like DNS, LDAP, and more Back to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_common_mistakes_when_creating_a_custom_end-user_uai_image/",
	"title": "Troubleshoot Common Mistakes when Creating a Custom End-User UAI Image",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Common Mistakes when Creating a Custom End-User UAI Image There a several problems that may occur while making or working with a custom end-user UAI images. The following are some basic troubleshooting questions to ask:\nDoes SESSION_NAME match an actual entry in cray bos sessiontemplate list? Is the SESSION_ID set to an appropriate UUID format? Did the awk command not parse the UUID correctly? Did the file /etc/security/limits.d/99-slingshot-network.conf get removed from the tarball correctly? Does the ENTRYPOINT /usr/bin/uai-ssh.sh exist? Did the container image get pushed and registered with UAS? Did the creation process run from a real worker or master node as opposed to a LiveCD node? Top: User Access Service (UAS)\nNext Topic: Troubleshoot UAS / CLI Authentication Issues\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/retrieve_the_client_secret_for_service_accounts/",
	"title": "Retrieve the Client Secret for Service Accounts",
	"tags": [],
	"description": "",
	"content": "Retrieve the Client Secret for Service Accounts Get the client secret that is generated by Keycloak when the client or service account was created. The secret can be regenerated any time with an administrative action.\nA client secret is needed to make requests using a new client or service account.\nPrerequisites Procedure Use the Keycloak administration console UI Use the Keycloak REST API Prerequisites A client or service account has been created. See Create a Service Account in Keycloak.\nProcedure Follow the steps in only one of the sections below:\nUse the Keycloak administration console UI Use the Keycloak REST API Use the Keycloak administration console UI Log in to the administration console.\nSee Access the Keycloak User Management UI for more information.\nClick on Clients under the Configure header of the navigation panel on the left side of the page.\nClick on the ID for the target client in the Clients table.\nSwitch to the Credentials tab.\nSave the client secret stored in the Secret field.\nCreate a variable for the client secret.\nLeave the Keycloak UI and create a variable for the client secret on the system.\nReplace 8a91fdf2-f9c5-4c7f-8da8-49cfbb97680a with the client secret returned for the service account being used.\nlinux# export CLIENT_SECRET=8a91fdf2-f9c5-4c7f-8da8-49cfbb97680a Use the Keycloak REST API Create the get_master_token function to get a token as a Keycloak master administrator.\nncn-mw# MASTER_USERNAME=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.user}\u0026#39; | base64 -d) ncn-mw# MASTER_PASSWORD=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d) ncn-mw# function get_master_token { curl -ks -d client_id=admin-cli -d username=$MASTER_USERNAME -d password=$MASTER_PASSWORD -d grant_type=password \\ https://api-gw-service-nmn.local/keycloak/realms/master/protocol/openid-connect/token | \\ python -c \u0026#34;import sys.json; print json.load(sys.stdin)[\u0026#39;access_token\u0026#39;]\u0026#34; } Get a unique ID for a client from Keycloak.\nIn the example below, the client ID is my-test-client, which should be replaced with the client ID for the target client. The returned 82d009de-1e36-41b6-8c21-4c390a25c188 in the output is the unique ID of the client.\nncn-mw# CLIENT_ID=$(curl -s -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \\ https://api-gw-service-nmn.local/keycloak/admin/realms/shasta/clients | \\ jq -r \u0026#39;.[] | select(.clientId==\u0026#34;my-test-client\u0026#34;).id\u0026#39;) ncn-mw# echo \u0026#34;${CLIENT_ID}\u0026#34; Example output:\n82d009de-1e36-41b6-8c21-4c390a25c188 Retrieve the client secret.\nIn the example below, the returned client secret is 8a91fdf2-f9c5-4c7f-8da8-49cfbb97680a.\nncn-mw# curl -s -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; https://api-gw-service-nmn.local/keycloak/admin/realms/shasta/clients/$CLIENT_ID/client-secret | jq -r .value Example output:\n8a91fdf2-f9c5-4c7f-8da8-49cfbb97680a Create a variable for the client secret.\nReplace 8a91fdf2-f9c5-4c7f-8da8-49cfbb97680a with the client secret returned for the service account being used.\nncn-mw# export CLIENT_SECRET=8a91fdf2-f9c5-4c7f-8da8-49cfbb97680a "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/scenario-b/",
	"title": "Scenario B network connection via high speed network",
	"tags": [],
	"description": "",
	"content": "Scenario B: network connection via high speed network Description The example here covers outside connections achieved via highspeed network.\nSummary Create a new VRF Move interfaces to the new VRF Create a new BGP process for the new VRF Setup the edge router Configure MetalLB Verification step for BGP routes Configure default route for workers Verification of external communication Topology Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/notice/",
	"title": "Notices",
	"tags": [],
	"description": "",
	"content": "© 2021 Hewlett Packard Enterprise Development LP\nNotices The information contained herein is subject to change without notice. The only warranties for Hewlett Packard Enterprise products and services are set forth in the express warranty statements accompanying such products and services. Nothing herein should be construed as constituting an additional warranty. Hewlett Packard Enterprise shall not be liable for technical or editorial errors or omissions contained herein.\nConfidential computer software: Valid license from Hewlett Packard Enterprise required for possession, use, or copying. Consistent with FAR 12.211 and 12.212, Commercial Computer Software, Computer Software Documentation, and Technical Data for Commercial Items are licensed to the U.S. Government under vendor\u0026rsquo;s standard commercial license.\nLinks to third-party websites take you outside the Hewlett Packard Enterprise website. Hewlett Packard Enterprise has no control over and is not responsible for information outside the Hewlett Packard Enterprise website.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_duplicate_mount_paths_in_a_uai/",
	"title": "Troubleshoot Duplicate Mount Paths in a UAI",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Duplicate Mount Paths in a UAI If a user attempts to create a UAI in the legacy mode and cannot create the UAI at all, a good place to look is at volumes. Duplicate mount_path specifications in the list of volumes in a UAI will cause a failure that looks like this:\nncn-m001-pit# cray uas create --publickey ~/.ssh/id_rsa.pub Example output:\nUsage: cray uas create [OPTIONS] Try \u0026#39;cray uas create --help\u0026#39; for help. Error: Unprocessable Entity: Failed to create deployment uai-erl-543cdbbc: Unprocessable Entity Currently, there is not a lot of UAS log information available from this error (this is a known problem), but a likely cause is duplicate mount_path specifications in volumes. Looking through the configured volumes for duplicates can be helpful.\nncn-m001-pit# cray uas admin config volumes list | grep -e mount_path -e volumename -e volume_id Example output:\nmount_path = \u0026#34;/app/broker\u0026#34; volume_id = \u0026#34;1f3bde56-b2e7-4596-ab3a-6aa4327d29c7\u0026#34; volumename = \u0026#34;broker-entrypoint\u0026#34; mount_path = \u0026#34;/etc/sssd\u0026#34; volume_id = \u0026#34;4dc6691e-e7d9-4af3-acde-fc6d308dd7b4\u0026#34; volumename = \u0026#34;broker-sssd-config\u0026#34; mount_path = \u0026#34;/etc/localtime\u0026#34; volume_id = \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34; volumename = \u0026#34;timezone\u0026#34; mount_path = \u0026#34;/root/slurm_config/munge\u0026#34; volume_id = \u0026#34;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026#34; volumename = \u0026#34;munge-key\u0026#34; mount_path = \u0026#34;/opt/forge\u0026#34; volume_id = \u0026#34;7b924270-c9e9-4b0e-85f5-5bc62c02457e\u0026#34; volumename = \u0026#34;delete-me\u0026#34; mount_path = \u0026#34;/lus\u0026#34; volume_id = \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34; volumename = \u0026#34;lustre\u0026#34; mount_path = \u0026#34;/etc/switchboard\u0026#34; volume_id = \u0026#34;d5058121-c1b6-4360-824d-3c712371f042\u0026#34; volumename = \u0026#34;broker-sshd-config\u0026#34; mount_path = \u0026#34;/etc/slurm\u0026#34; volume_id = \u0026#34;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026#34; volumename = \u0026#34;slurm-config\u0026#34; mount_path = \u0026#34;/opt/forge_license\u0026#34; volume_id = \u0026#34;ecfae4b2-d530-4c06-b757-49b30061c90a\u0026#34; volumename = \u0026#34;optforgelicense\u0026#34; mount_path = \u0026#34;/opt/forge\u0026#34; volume_id = \u0026#34;fc95d0da-6296-4d0b-8f26-2d4338604991\u0026#34; volumename = \u0026#34;optforge\u0026#34; Looking through this list, the mount path for the volume named delete-me and the mount path for the volume named optforge are the same. The obvious candidate for deletion in this case is delete-me, so it can be deleted.\nncn-m001-pit# cray uas admin config volumes delete 7b924270-c9e9-4b0e-85f5-5bc62c02457e Example output:\nmount_path = \u0026#34;/opt/forge\u0026#34; volume_id = \u0026#34;7b924270-c9e9-4b0e-85f5-5bc62c02457e\u0026#34; volumename = \u0026#34;delete-me\u0026#34; [volume_description.host_path] path = \u0026#34;/tmp/foo\u0026#34; type = \u0026#34;DirectoryOrCreate\u0026#34; Top: User Access Service (UAS)\nNext Topic: Troubleshoot Missing or Incorrect UAI Images\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/ssh_keys/",
	"title": "Update NCN User SSH Keys",
	"tags": [],
	"description": "",
	"content": "Update NCN User SSH Keys Change the SSH keys for users on non-compute nodes (NCNs) on the system using the rotate-ssh-keys-mgmt-nodes.yml Ansible playbook provided by CSM or through NCN node personalization (site.yml).\nThe NCNs deploy with SSH keys for the root user that are changed during the system install. See Change NCN Image Root Password and SSH Keys for more information on changing the default keys during install. It is a recommended best practice for system security to change the SSH keys after the install is complete on a schedule. This procedure defines how to change the keys once the system is operational.\nThe NCN root user keys are stored in the HashiCorp Vault instance, and applied with the csm.ssh_keys Ansible role via a CFS session. If no keys are added to Vault as in the procedure below, this Ansible role will skip any updates.\nProcedure: Configure root SSH keys in Vault Procedure: Apply root SSH keys to NCNs (standalone) Procedure for other users Procedure: Configure root SSH keys in Vault Generate a new SSH key pair for the root user.\nUse ssh-keygen to generate a new pair or stage an existing pair, as per site security policies and procedures.\nGet the HashiCorp Vault root token.\nncn-mw# kubectl get secrets -n vault cray-vault-unseal-keys -o jsonpath=\u0026#39;{.data.vault-root}\u0026#39; | base64 -d; echo Write the private and public halves of the key pair to the HashiCorp Vault.\nWARNING: The CSM instance of Vault does not support the patch operation. Ensure that if the ssh_private_key and ssh_public_key fields in the secret/csm/users/root secret are being updated, then any other desired fields are also included in the write command. For example the user\u0026rsquo;s password hash. See Update NCN Passwords. Any fields omitted from the write command will be cleared from Vault.\nThe path to the secret and the SSH key fields are configurable locations in the CSM csm.ssh_keys Ansible role located in the CSM configuration management Git repository that is in use. If not using the defaults as shown in the command examples, ensure that the paths are consistent between Vault and the values in the Ansible role. See roles/csm.ssh_keys in the repository for more information.\nOpen an interactive shell in the Vault Kubernetes pod.\nncn-mw# kubectl exec -itn vault cray-vault-0 -c vault -- sh Write the SSH keys to Vault.\nThe vault login command will request the token value from the output of the previous step. Use the SSH keys from the earlier step. The ssh_private_key and ssh_public_key fields should contain the exact content from the id_rsa and id_rsa.pub files (if using RSA key types). NOTE: It is important to enclose the key content in single quotes to preserve any special characters. The vault read command allows the administrator to verify that the contents of the secret were stored correctly. cray-vault# export VAULT_ADDR=http://cray-vault:8200 cray-vault# vault login cray-vault# vault write secret/csm/users/root ssh_private_key=\u0026#39;...\u0026#39; \\ ssh_public_key=\u0026#39;...\u0026#39; \\ [... other fields (see warning below) ...] cray-vault# vault read secret/csm/users/root cray-vault# exit Procedure: Apply root SSH keys to NCNs (standalone) Use the following procedure with the rotate-ssh-keys-mgmt-nodes.yml playbook to only change the root SSH keys on NCNs. This is a quick alternative to running a full management NCN personalization, as documented in the Configure Non-Compute Nodes with CFS procedure.\nCreate a CFS configuration layer to run the SSH key change Ansible playbook.\nNOTE This step only needs to be done once, as long as the commit in the CSM configuration management Git repository has not changed. If the commit has not changed since the last time this step was run, this step may be skipped, because the previously created CFS configuration will still work.\nCreate a file containing only this CFS configuration layer.\nThe file contents should be as follows, except replace the \u0026lt;INSERT GIT COMMIT ID\u0026gt; text with the commit in the CSM configuration management Git repository that is in use.\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ncn-root-keys-update\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;rotate-ssh-keys-mgmt-nodes.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;INSERT GIT COMMIT ID\u0026gt;\u0026#34; } ] } Create the ncn-root-keys-update configuration in CFS.\nReplace the \u0026lt;INSERT FILE PATH HERE\u0026gt; text with the path to the file created in the previous step. If a CFS configuration already exists with this name, the following command will overwrite it.\nncn-mw# cray cfs configurations update ncn-root-keys-update --file \u0026lt;INSERT FILE PATH HERE\u0026gt; Create a CFS configuration session to apply the SSH keys update.\nncn-mw# cray cfs sessions create --name ncn-root-keys-update-`date +%Y%m%d%H%M%S` --configuration-name ncn-root-keys-update Monitor the CFS session.\nSee Track the Status of a Session.\nProcedure for other users The csm.ssh_key Ansible role supports setting SSH keys for non-root users.\nMake a copy of the rotate-ssh-keys-mgmt-nodes.yml Ansible playbook and modify the role variables to specify a different ssh_keys_username.\nUsing that username, add the SSH keys to Vault.\nFollow Procedure: Configure root SSH keys in Vault.\nCreate a configuration layer using the new Ansible playbook and create a CFS session using that layer.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/small/",
	"title": "Small",
	"tags": [],
	"description": "",
	"content": "Small Back to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/ntp/",
	"title": "Network Time Protocol (NTP) Client",
	"tags": [],
	"description": "",
	"content": "Network Time Protocol (NTP) Client Summary of NTP from RFC-1305 Network Time Protocol (Version 3):\nNTP is used to synchronize timekeeping among a set of distributed time servers and clients \u0026hellip; It provides the protocol mechanisms to synchronize time in principle to precisions in the order of nanoseconds while preserving a non-ambiguous date well into the next century.\nThe Network Time Protocol (NTP) client is essential for syncing time on various clients in the system. This document shows how to view NTP status and configure NTP on an Aruba switch.\nSpecify a remote NTP server Force NTP to use a specific VRF for requests Configure the system timezone Validate functionality Expected results Specify a remote NTP server Specify a remote NTP server to use for time synchronization:\nswitch(config)# ntp server \u0026lt;FQDN|IP-ADDR\u0026gt; Force NTP to use a specific VRF for requests switch(config)# ntp vrf VRF Configure the system timezone switch(config)# clock timezone TIMEZONE Validate functionality switch# show ntp status Example output:\nNTP is enabled. NTP authentication is enabled. NTP is using the default VRF for NTP server connections. Wed Nov 23 23:29:10 PDT 2016 Uptime: 187 days, 1 hours, 37 minutes, 48 seconds Synchronized to NTP Server 10.0.0.1 at stratum 2. Poll interval = 1024 seconds. Time accuracy is within 0.994 seconds Reference time: Thu Jan 28 2016 0:57:06.647 (UTC) Expected results The NTP client can be configured. The functionality can be validated using the show command. The system time of the switch matches that of the NTP server. Back to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_missing_or_incorrect_uai_images/",
	"title": "Troubleshoot Missing or Incorrect UAI Images",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Missing or Incorrect UAI Images If a UAI shows a uai_status of Waiting and a uai_msg of ImagePullBackOff, that indicates that the UAI or the UAI class is configured to use an image that is not in the image registry.\nEither obtaining and pushing the image to the image registry, or correcting the name or version of the image in the UAS configuration will usually resolve this.\nTop: User Access Service (UAS)\nNext Topic: Troubleshoot UAIs with Administrative Access\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/system_security_and_authentication/",
	"title": "System Security and Authentication",
	"tags": [],
	"description": "",
	"content": "System Security and Authentication The system uses a number of mechanisms to ensure the security and authentication of internal and external requests.\nAPI gateway service Keycloak JSON Web Tokens (JWTs) Security infrastructure Istio ingress gateway API gateway service The Cray API gateway service provides a common access gateway for all of the systems management REST APIs. Authentication is provided by an Identity and Access Management (IAM) service that integrates with Istio.\nKeycloak Keycloak is an open source Identity and Access Management (IAM) solution. It provides authentication and authorization services that are used to secure access to services on the system.\nTo learn more about Keycloak, see:\nSecurity and authentication Keycloak home page JSON Web Tokens (JWTs) The approach for system management authentication and authorization is to leverage the OpenID Connect standard, as much as practical. OpenID Connect consists of a specific application of the OAuth v2.0 standard, which leverages the use of JWTs.\nSecurity infrastructure Istio ingress gateway All connections through the Istio ingress gateway require authentication with a valid JWT from Keycloak, except for the following endpoints accessed via the shasta external hostname:\n/keycloak /apis/tokens /vcs /spire-jwks- /spire-bundle /meta-data /user-data /phone-home /repository /v2 /service/rest /capsules/ "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/snmp_community/",
	"title": "SNMPv2c community",
	"tags": [],
	"description": "",
	"content": "SNMPv2c community The switch supports SNMPv2c community-based security for Read-Only access.\nRelevant Configuration\nConfigure an SNMPv2c community name\nEnable SNMP\nswitch(config)# snmp-server community private rw Configure a SNMPv2c trap receiver host\nswitch(config)# snmp-server host IP-ADDR \u0026lt;trap|inform\u0026gt; version v2c [community NAME] Show Commands to Validate Functionality\nswitch# show snmp Expected Results\nStep 1: You can configure the community name Step 2: You can bind the SNMP server to the default VRF Step 3: You can connect from the workstation using the community name Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/ospfv2/",
	"title": "Open Shortest Path First (OSPF) v2",
	"tags": [],
	"description": "",
	"content": "Open Shortest Path First (OSPF) v2 \u0026ldquo;OSPF is a link-state based routing protocol. It is designed to be run internal to a single Autonomous System. Each OSPF router maintains an identical database describing the Autonomous System\u0026rsquo;s topology. From this database, a routing table is calculated by constructing a shortest-path tree. OSPF recalculates routes quickly in the face of topological changes, utilizing a minimum of routing protocol traffic. OSPF provides support for equal-cost multipath. An area routing capability is provided, enabling an additional level of routing protection and a reduction in routing protocol traffic.\u0026rdquo; –rfc1247\nConfiguration Commands Enable an OSPF instance:\nswitch(config)# router ospf INSTANCE [vrf NAME] switch(config-ospf)# router-id ROUTER Configure an OSPF area:\nswitch(config-ospf)# area AREA [stub|nssa|default-metric COST] Configure external Route redistribution and control:\nswitch(config-ospf)# redistribute \u0026lt;bgp|connected|static\u0026gt; switch(config-ospf)# default-metric VALUE switch(config-ospf)# maximum-paths VALUE Influence route choice by changing the administrative distance:\nswitch(config-ospf)# distance VALUE Enable OSPF on an interface:\nswitch(config-if)# ip ospf PROCESS-ID area AREA Configure optional OSPF interface settings:\nswitch(config-if)# ip ospf cost COST switch(config-if)# ip ospf hello-interval SECONDS switch(config-if)# ip ospf dead-interval SECONDS switch(config-if)# ip ospf retransmit-interval SECONDS switch(config-if)# ip ospf transit-delay SECONDS switch(config-if)# ip ospf network \u0026lt;broadcast|point-to-point\u0026gt; switch(config-if)# ip ospf priority VALUE switch(config-if)# ip ospf \u0026lt;active|passive\u0026gt; switch(config-if)# ip ospf bfd Configure OSPF interface authentication:\nswitch(config-if)# ip ospf authentication \u0026lt;message-digest|simple-text|null\u0026gt; switch(config-if)# ip ospf authentication-key PSWD switch(config-if)# ip ospf message-digest-key md5 \u0026lt;cipher|plain\u0026gt;text KEY Show commands to validate functionality:\nswitch# show ip ospf [interface|neighbors] switch# show ip route ospf Expected Results Administrators can enable OSPF globally on the switch Administrators can enable OSPF on the loopback, SVI or routed interfaces The output of the show commands looks correct Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_stale_brokered_uais/",
	"title": "Troubleshoot Stale Brokered UAIs",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Stale Brokered UAIs When a Broker UAI terminates and restarts, the SSH key used to forward SSH sessions to End-User UAIs changes (this is a known problem) and subsequent Broker UAIs are unable to forward sessions to End-User UAIs. The symptom of this is that a user logging into a Broker UAI will receive a password prompt from the End-User UAI and be unable to log in even if providing the correct password. To fix this, remove the stale End-User UAIs and allow the Broker UAI to recreate them. The easy way to do this is to use the command specifying the uai-creation-class identifier from the Broker\u0026rsquo;s UAI class.\ncray uas admin uais delete --class-id \u0026lt;creation-class-id\u0026gt; For example:\nncn-m001-pit# cray uas admin config classes list | grep -e class_id -e comment class_id = \u0026#34;74970cdc-9f94-4d51-8f20-96326212b468\u0026#34; comment = \u0026#34;UAI broker class\u0026#34; class_id = \u0026#34;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026#34; comment = \u0026#34;UAI User Class\u0026#34; class_id = \u0026#34;bb28a35a-6cbc-4c30-84b0-6050314af76b\u0026#34; comment = \u0026#34;Non-Brokered UAI User Class\u0026#34; ncn-m001-pit# cray uas admin config classes describe 74970cdc-9f94-4d51-8f20-96326212b468 | grep uai_creation_class uai_creation_class = \u0026#34;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026#34; ncn-m001-pit# cray uas admin uais delete --class-id a623a04a-8ff0-425e-94cc-4409bdd49d9c results = [ \u0026#34;Successfully deleted uai-vers-6da50e7a\u0026#34;,] After that, users should be able to log into the Broker UAI and be directed to an End-User UAI as before.\nTop: User Access Service (UAS)\nNext Topic: Troubleshoot UAI Stuck in ContainerCreating\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/transport_layer_security_for_ingress_services/",
	"title": "Transport Layer Security (TLS) for Ingress Services",
	"tags": [],
	"description": "",
	"content": "Transport Layer Security (TLS) for Ingress Services The Istio Secure Gateway and Keycloak Gatekeeper services utilize Cert-manager for their Transport Layer Security (TLS) certificate and private key. Certificate custom resource definitions are deployed as part of Helm Charts for these services.\nTo view properties of the Istio Secure Gateway certificate:\n# kubectl describe certificate -n istio-system ingress-gateway-cert To view the properties of the Keycloak Gatekeeper certificate:\n# kubectl describe certificate -n services keycloak-gatekeeper An outstanding bug in the Keycloak Gatekeeper service prevents it from updating its TLS certificate and key material upon Cert-manager renewal. Thus, it may be necessary to monitor the situation and proactively renew/force reload Keycloak Gatekeeper.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/snmpv3_users/",
	"title": "SNMPv3 users",
	"tags": [],
	"description": "",
	"content": "SNMPv3 users SNMPv3 supports cryptographic security by a combination of authenticating and encrypting the SNMP protocol packets over the network. Read-Only access is currently supported. The admin user can add or remove SNMPv3 users.\nRelevant Configuration\nConfigure a new SNMPv3 user (Minimum 8 characters for passwords)\nswitch(config)# snmp-server user admin v3 enable Show Commands to Validate Functionality\nswitch# show snmp users Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/physical_interfaces/",
	"title": "Physical Interfaces",
	"tags": [],
	"description": "",
	"content": "Physical Interfaces Configure the physical interfaces for a switch.\nConfiguration Commands Enable the interface:\nswitch(config)# interface IFACE switch(config-if)# no shutdown Show commands to validate functionality:\nswitch# show interface IFACE [transceiver|brief|dom|extended] Expected Results The switch recognizes the transceiver without errors Administrators can enter the interface context for the port and enable it Administrators can establish a link with a partner Administrators can pass traffic as expected Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uai_authentication_issues/",
	"title": "Troubleshoot UAS / CLI Authentication Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAS / CLI Authentication Issues Several troubleshooting steps related to authentication in a UAI.\nInternal Server Error An error was encountered while accessing Keycloak because of an invalid token.\n# cray uas create --publickey ~/.ssh/id_rsa.pub Usage: cray uas create [OPTIONS] Try \u0026#34;cray uas create --help\u0026#34; for help. Error: Internal Server Error: An error was encountered while accessing Keycloak The uas-mgr logs show:\n2020-03-06 18:52:07,642 - uas_auth - ERROR - \u0026lt;class \u0026#39;requests.exceptions.HTTPError\u0026#39;\u0026gt; HTTPError(\u0026#39;401 Client Error: Unauthorized for url: https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/userinfo\u0026#39;) 2020-03-06 18:52:07,643 - uas_auth - ERROR - UasAuth HTTPError:401 Client Error: Unauthorized for url: https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/userinfo The Keycloak pod logs shows:\n18:53:19,617 WARN [org.keycloak.events] (default task-1) type=USER_INFO_REQUEST_ERROR, realmId=028be52c-ceca-4dbd-b765-0386b42b1866, clientId=cray, userId=null, ipAddress=10.40.0.0, error=user_session_not_found, auth_method=validate_access_token This is caused by the authentication token being invalid. This can happen for many reasons, such as the token expiring after its lifetime has ended or the Keycloak server restarting because of a failure or being moved to a different node.\nTo resolve this issue, run cray auth login to refresh the access token.\nAuthorization is Local to a Host: whenever you are using the CLI (cray command) on a host (e.g. a workstation or NCN) where it has not been used before, it is necessary to authenticate on that host using cray auth login. There is no mechanism to distribute CLI authorization amongst hosts.\nInvalid Token # cray uas create --publickey ~/.ssh/id_rsa.pub Usage: cray uas create [OPTIONS] Try \u0026#34;cray uas create --help\u0026#34; for help. Error: Bad Request: Token not valid for UAS. Attributes missing: [\u0026#39;name\u0026#39;, \u0026#39;uidNumber\u0026#39;, \u0026#39;preferred_username\u0026#39;, \u0026#39;gidNumber\u0026#39;, \u0026#39;loginShell\u0026#39;, \u0026#39;homeDirectory\u0026#39;] To resolve this issue, make sure the cray command is configured to use one of the following URLs for an API gateway (excluding the /keycloak/realms/shastaendpoint).\n# kubectl exec -c api-gateway api-gateway-544d5c676f-682m2 -- curl -s http://localhost:8001/consumers/remote-admin/jwt | python -mjson.tool | grep \u0026#34;\u0026#34;key\u0026#34;\u0026#34; \u0026#34;key\u0026#34;: \u0026#34;https://api-gateway.default.svc.cluster.local/keycloak/realms/shasta\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/keycloak/realms/shasta\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;https://mgmt-plane-cmn.local/keycloak/realms/shasta\u0026#34;, # cray config describe | grep hostname \u0026#34;hostname\u0026#34;: \u0026#34;https://172.30.51.127:30443\u0026#34; \u0026lt;---- 172.30.51.127:30443 will not work # Change to \u0026#34;https://api-gw-service-nmn.local\u0026#34; cray init --hostname \u0026#34;https://api-gw-service-nmn.local\u0026#34; Overwrite configuration file at: /root/.config/cray/configurations/default ? [y/N]: y Username: user Password: Success! Initialization complete. Invalid Credentials # cray auth login --username \u0026lt;user\u0026gt; --password \u0026lt;wrongpassword\u0026gt; Usage: cray auth login [OPTIONS] Try \u0026#34;cray auth login --help\u0026#34; for help. Error: Invalid Credentials To resolve this issue:\nLog in to Keycloak and verify the user exists. Make sure the username and password are correct. cray uas describe \u0026lt;user\u0026gt; Does Not Work The cray uas describe \u0026lt;user\u0026gt; is no longer a valid command.\n# cray uas describe \u0026lt;user\u0026gt; Usage: cray uas [OPTIONS] COMMAND [ARGS]... Try \u0026#34;cray uas --help\u0026#34; for help. Error: No such command \u0026#34;describe\u0026#34;. Use cray uas list instead.\n# cray uas list Example output:\n[[results]] username = \u0026#34;\u0026#34; uai_host = \u0026#34;\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; uai_connect_string = \u0026#34;\u0026#34; uai_img = \u0026#34;\u0026#34; uai_age = \u0026#34;11m\u0026#34; uai_name = \u0026#34;\u0026#34; Top: User Access Service (UAS)\nNext Topic: Troubleshoot Broker UAI SSSD Cannot Use /etc/sssd/sssd.conf\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/troubleshoot_common_vault_cluster_issues/",
	"title": "Troubleshoot Common Vault Cluster Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Common Vault Cluster Issues Search for underlying issues causing unhealthy Vault clusters. Check the Vault statefulset and various pod logs to determine what is impacting the health of the Vault.\nProcedure View the Vault statefulset.\nncn-mw# kubectl -n vault get statefulset --show-labels Example output:\nNAME READY AGE LABELS cray-vault 3/3 8d app.kubernetes.io/name=vault,vault_cr=cray-vault Check the pod logs for the bank-vaults container for Vault statefulset pods.\nncn-mw# kubectl logs -n vault cray-vault-0 --tail=-1 --prefix -c bank-vaults ncn-mw# kubectl logs -n vault cray-vault-1 --tail=-1 --prefix -c bank-vaults ncn-mw# kubectl logs -n vault cray-vault-0 --tail=-1 --prefix -c bank-vaults Check the Vault container logs within the pod.\nncn-mw# kubectl logs -n vault cray-vault-0 --tail=-1 --prefix -c vault ncn-mw# kubectl logs -n vault cray-vault-1 --tail=-1 --prefix -c vault ncn-mw# kubectl logs -n vault cray-vault-2 --tail=-1 --prefix -c vault Check the Vault operator pod logs using ephemerally named pods.\nncn-mw# kubectl logs -n vault cray-vault-operator-7dbbdbb68b-zvg2g --tail=-1 --prefix "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/spine_leaf_architecture/",
	"title": "Spine-leaf architecture",
	"tags": [],
	"description": "",
	"content": "Spine-leaf architecture Description\nThe network design used in majority of our supercomputer installations is spine leaf architecture. In more sizeable systems we also utilize super-spine to accommodate number of spines that connect the network to provide additional HA capabilities.\nWhat is Spine-Leaf Architecture? A spine-leaf architecture is data center network topology that consists of two switching layers—a spine and leaf. The leaf layer consists of access switches that aggregate traffic from servers and connect directly into the spine or network core. Spine switches interconnect all leaf switches in a full-mesh topology.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/pim/",
	"title": "PIM-SM Bootstrap Router (BSR) and Rendezvous Point (RP)",
	"tags": [],
	"description": "",
	"content": "PIM-SM Bootstrap Router (BSR) and Rendezvous Point (RP) \u0026ldquo;Every PIM multicast group needs to be associated with the IP address of a Rendezvous Point (RP) [\u0026hellip;] For all senders to reach all receivers, it is crucial that all routers in the domain use the same mappings of group addresses to RP addresses. [\u0026hellip;] The BSR mechanism provides a way in which viable group-to-RP mappings can be created and rapidly distributed to all the PIM routers in a domain.\u0026rdquo; –rfc5059\nConfiguration Commands Configure the BSR and RP\nswitch(config)# router pim switch(config-pim)# bsr-candidate source-ip-interface IFACE switch(config-pim)# rp-candidate source-ip-interface IFACE Show commands to validate functionality:\nswitch# show ip pim bsr switch# show ip pim rp-candidate switch# show ip pim rp-set Test Steps Use the previous IGMP, MSDP configuration and topology. On both Core Switches create loopback1 interface using the same IP address for both devices. Enable OSPF on loopback interface and make sure route redistribution is configured. Enable PIM-SM on loopback1 Configure loopback1 to act as RP for both 8325s using: rp-candidate source-ip-interface loopback1 Configure both core devices to advertise the same specific multicast subnet (which we will use later) by typing \u0026ldquo;rp-candidate group-prefix 239.1.1.0/24\u0026rdquo;. Enable BSR on both routers using: bsr-candidate source-ip-interface loopback0 in the router pim context. Expected Results Administrators can configure loopback1 on both 8325s using the same IP address Administrators can configure OSPF routing for loopback1 Administrators successfully enabled PIM-SM on loopback1 Administrators configured loopback1 to act as a PIM-SM RP Administrators configured the specific group-prefix that will be used in the next test Administrators successfully enabled the BSR on both 8325s using loopback0 as the BSR source IP Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uai_stuck_in_containercreating/",
	"title": "Troubleshoot UAI Stuck in ContainerCreating",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAI Stuck in ContainerCreating Resolve an issue causing UAIs to show a uai_status field of Waiting, and a uai_msg field of ContainerCreating. It is possible that this is just a matter of starting the UAI taking longer than normal, perhaps as it pulls in a new UAI image from a registry. If the issue persists for a long time, it is worth investigating.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must be on an NCN or host that has Kubernetes (kubectl command) access to the HPE Cray EX System Symptoms The UAI has been in the ContainerCreating status for several minutes.\nProcedure Find the UAI.\nncn-m001-pit# cray uas admin uais list --owner ctuser Example output:\n[[results]] uai_age = \u0026#34;1m\u0026#34; uai_connect_string = \u0026#34;ssh ctuser@10.103.13.159\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.103.13.159\u0026#34; uai_msg = \u0026#34;ContainerCreating\u0026#34; uai_name = \u0026#34;uai-ctuser-bcd1ff74\u0026#34; uai_status = \u0026#34;Waiting\u0026#34; username = \u0026#34;ctuser\u0026#34; Look up the UAI\u0026rsquo;s pod in Kubernetes.\nncn-m001-pit# kubectl get po -n user | grep uai-ctuser-bcd1ff74 Example output:\nuai-ctuser-bcd1ff74-7d94967bdc-4vm66 0/1 ContainerCreating 0 2m58s Describe the pod in Kubernetes.\nncn-m001-pit# kubectl describe pod -n user uai-ctuser-bcd1ff74-7d94967bdc-4vm66 Example output:\nName: uai-ctuser-bcd1ff74-7d94967bdc-4vm66 Namespace: user Priority: -100 Priority Class Name: uai-priority Node: ncn-w001/10.252.1.12 Start Time: Wed, 03 Feb 2021 18:33:00 -0600 [...] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled \u0026lt;unknown\u0026gt; default-scheduler Successfully assigned user/uai-ctuser-bcd1ff74-7d94967bdc-4vm66 to ncn-w001 Warning FailedMount 2m53s (x8 over 3m57s) kubelet, ncn-w001 MountVolume.SetUp failed for volume \u0026#34;broker-sssd-config\u0026#34; : secret \u0026#34;broker-sssd-conf\u0026#34; not found Warning FailedMount 2m53s (x8 over 3m57s) kubelet, ncn-w001 MountVolume.SetUp failed for volume \u0026#34;broker-sshd-config\u0026#34; : configmap \u0026#34;broker-sshd-conf\u0026#34; not found Warning FailedMount 2m53s (x8 over 3m57s) kubelet, ncn-w001 MountVolume.SetUp failed for volume \u0026#34;broker-entrypoint\u0026#34; : configmap \u0026#34;broker-entrypoint\u0026#34; not found Warning FailedMount 114s kubelet, ncn-w001 Unable to attach or mount volumes: unmounted volumes=[broker-sssd-config broker-entrypoint broker-sshd-config], unattached volumes=[optcraype optlmod etcprofiled optr optforgelicense broker-sssd-config lustre timezone optintel optmodulefiles usrsharelmod default-token-58t5p optarmlicenceserver optcraycrayucx slurm-config opttoolworks optnvidiahpcsdk munge-key optamd opttotalview optgcc opttotalviewlicense broker-entrypoint broker-sshd-config etccrayped opttotalviewsupport optcraymodulefilescrayucx optforge usrlocalmodules varoptcraypepeimages]: timed out waiting for the condition This produces a lot of output, all of which can be useful for diagnosis. A good place to start is in the Events section at the bottom. Notice the warnings here about volumes whose secrets and ConfigMaps are not found. In this case, that means the UAI cannot start because it was started in legacy mode without a default UAI class, and some of the volumes configured in the UAS are in the uas namespace to support localization of Broker UAIs and cannot be found in the user namespace. To solve this particular problem, configure a default UAI class with the correct volume list in it, delete the UAI, and allow the user to try creating it again using the default class.\nOther problems can usually be quickly identified using this and other information found in the output from the kubectl describe pod command.\nTop: User Access Service (UAS)\nNext Topic: Troubleshoot Duplicate Mount Paths in a UAI\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/update_default_air-cooled_bmc_and_leaf_bmc_switch_snmp_credentials/",
	"title": "Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials",
	"tags": [],
	"description": "",
	"content": "Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials This procedure updates the default credentials used when new air-cooled hardware is discovered for the first time. This includes the default Redfish credentials used for new air-cooled NodeBMCs and Slingshot switch BMCs (RouterBMCs), and SNMP credentials for new management leaf-BMC switches.\nIMPORTANT: After this procedure is completed, all future air-cooled hardware added to the system will be assumed to be configured with the new global default credential.\nNOTE: This procedure will not update the Redfish or SNMP credentials for existing air-cooled devices. To change the credentials on existing air-cooled hardware follow the Change Air-Cooled Node BMC Credentials and Change SNMP Credentials on Leaf-BMC Switches procedures.\nLimitation Procedure Limitation The default global credentials used for liquid-cooled BMCs in the Change Cray EX Liquid-Cooled Cabinet Global Default Password procedure needs to be the same as the one used in this procedure for air-cooled BMCs (River hardware).\nProcedure The River Endpoint Discovery Service (REDS) sealed secret contains the default global credential used by REDS.\nFollow the Redeploying a Chart procedure with the following specifications:\nChart name: cray-hms-reds\nBase manifest name: core-services\nWhen reaching the step to update the customizations, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nClone the CSM repository.\nncn-mw# git clone https://github.com/Cray-HPE/csm.git Acquire sealed secret keys.\nncn-mw# mkdir -pv certs \u0026amp;\u0026amp; kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39; | base64 -d \u0026gt; certs/sealed_secrets.crt \u0026amp;\u0026amp; kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.key}\u0026#39; | base64 -d \u0026gt; certs/sealed_secrets.key Modify REDS sealed secret to use new global default credentials.\nInspect the original default Redfish credentials used by REDS and HMS discovery.\nncn-mw# ./utils/secrets-decrypt.sh cray_reds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_redfish_defaults -r | base64 -d | jq Expected output looks similar to the following:\n{ \u0026#34;Cray\u0026#34;: { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foo\u0026#34; } } Inspect the original default switch SNMP credentials used by REDS and HMS discovery.\nncn-mw# ./utils/secrets-decrypt.sh cray_reds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_switch_defaults -r | base64 -d | jq Expected output looks similar to the following:\n{ \u0026#34;SNMPUsername\u0026#34;: \u0026#34;testuser\u0026#34;, \u0026#34;SNMPAuthPassword\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;SNMPPrivPassword\u0026#34;: \u0026#34;bar\u0026#34; } Update the default credentials in customizations.yaml for REDS and HMS discovery to use.\nSpecify the desired default Redfish credentials.\nncn-mw# echo \u0026#39;{\u0026#34;Cray\u0026#34;:{\u0026#34;Username\u0026#34;:\u0026#34;root\u0026#34;,\u0026#34;Password\u0026#34;:\u0026#34;foobar\u0026#34;}}\u0026#39; | base64 \u0026gt; reds.redfish.creds.json.b64 Specify the desired default SNMP credentials.\nncn-mw# echo \u0026#39;{\u0026#34;SNMPUsername\u0026#34;:\u0026#34;testuser\u0026#34;,\u0026#34;SNMPAuthPassword\u0026#34;:\u0026#34;foo1\u0026#34;,\u0026#34;SNMPPrivPassword\u0026#34;:\u0026#34;bar2\u0026#34;}\u0026#39; | base64 \u0026gt; reds.switch.creds.json.b64 Update and regenerate the cray_reds_credentials sealed secret.\nncn-mw# cat \u0026lt;\u0026lt; EOF | yq w - \u0026#39;data.vault_redfish_defaults\u0026#39; \u0026#34;$(\u0026lt;reds.redfish.creds.json.b64)\u0026#34; | yq w - \u0026#39;data.vault_switch_defaults\u0026#39; \u0026#34;$(\u0026lt;reds.switch.creds.json.b64)\u0026#34; | yq r -j - | ./utils/secrets-encrypt.sh | yq w -f - -i ./customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_reds_credentials\u0026#39; { \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cray-reds-credentials\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: {} } EOF Decrypt generated secret for review.\nReview the default Redfish credentials.\nncn-mw# ./utils/secrets-decrypt.sh cray_reds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_redfish_defaults -r | base64 -d | jq Expected output looks similar to the following:\n{ \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foobar\u0026#34; } Review the default switch SNMP credentials.\nncn-mw# ./utils/secrets-decrypt.sh cray_reds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_switch_defaults -r | base64 -d | jq Expected output looks similar to the following:\n{ \u0026#34;SNMPUsername\u0026#34;: \u0026#34;testuser\u0026#34;, \u0026#34;SNMPAuthPassword\u0026#34;: \u0026#34;foo1\u0026#34;, \u0026#34;SNMPPrivPassword\u0026#34;: \u0026#34;bar2\u0026#34; } When reaching the step to validate the redeployed chart, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nWait for the REDS Vault loader job to run to completion.\nncn-mw# kubectl -n services wait job cray-reds-vault-loader --for=condition=complete --timeout=5m Verify that the default Redfish credentials have updated in Vault.\nncn-mw# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-mw# kubectl -n vault exec -it cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 vault kv get secret/reds-creds/defaults Expected output:\n==== Data ==== Key Value --- ----- Cray map[password:foobar username:root] Verify that the default SNMP credentials have updated in Vault.\nncn-mw# kubectl -n vault exec -it cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 vault kv get secret/reds-creds/switch_defaults Expected output:\n========== Data ========== Key Value --- ----- SNMPAuthPassword foo1 SNMPPrivPassword bar2 SNMPUsername testuser Make sure to perform the entire linked procedure, including the step to save the updated customizations.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/spine_leaf_architecture2/",
	"title": "Spine-leaf architecture",
	"tags": [],
	"description": "",
	"content": "Spine-leaf architecture How does a spine-leaf architecture differ from traditional network designs? Traditionally, data center networks were based on a three-tier model:\nAccess switches connect to servers Aggregation or distribution switches provide redundant connections to access switches Core switches provide fast transport between aggregation switches, typically connected in a redundant pair for high availability At the most basic level, a spine-leaf architecture collapses one of these tiers, as depicted in these diagrams.\nOther common differences in spine-leaf topologies include:\nThe removal of Spanning Tree Protocol (STP) where feasible A scale-out vs. scale-up of infrastructure Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/port_mirroring/",
	"title": "Port Mirroring",
	"tags": [],
	"description": "",
	"content": "Port Mirroring Port mirroring, also known as Switched Port Analyzer (SPAN), enables traffic on one or more switch interfaces to be replicated on another interface for purposes such as monitoring.\nConfiguration Commands Create and enable a mirror session:\nswitch(config)# mirror session \u0026lt;1-4\u0026gt; switch(config-mirror)# enable Configure a source interface:\nswitch(config-mirror)# source interface IFACE \u0026lt;both|tx|rx\u0026gt; Configure an interface as the mirror destination:\nswitch(config-mirror)# destination interface IFACE Configure a tunnel as the mirror destination (ERSPAN):\nswitch(config-mirror)# destination tunnel IP-ADDR source IP-ADDR [id VALUE\u0026gt; [vrf VRF] Configure CPU as the mirror destination:\nswitch(config-mirror)# destination cpu Generate and copy the internal packet capture:\nswitch# diagnostics switch# diag utilities tshark [file] switch# copy tshark-pcap REMOTE-URL vrf VRF Show commands to validate functionality:\nswitch# show mirror \u0026lt;1-4\u0026gt; NOTES:\nAdministrators can set the Switch CPU as the destination for mirrored traffic. Keep in mind that all the traffic from an interface will be sent to the CPU and could create high CPU utilization. It is not recommended to use this method on taking captures in live network as the amount of traffic could negatively hit the CPU; so in those cases, the recommendation would be to use external capture station. To do a port capture directly on device:\n8325(config)# mirror session 1 8325(config-mirror-1)# destination cpu 8325(config-mirror-1)# source interface 1/1/1 both A source of transmit \u0026amp; receive traffic rx A source of receive-only traffic tx A source of transmit-only traffic 8325(config-mirror-1)# source interface 1/1/1 both 8325(config-mirror-1)# enable To start TCPDUMP from shell:\n8325# start-shell 8325:~$ sudo su 8325:/home/admin# ip netns VRF_1 wireless_mgmt ntb (id: 0) mirror_ns nonet swns 8325:/home/admin# ip netns exec mirror_ns bash 8325:/home/admin# ifconfig MirrorRxNetLink encap:Ethernet HWaddr 02:10:18:96:FD:EE UP BROADCAST RUNNING MULTICAST MTU:9326 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 TX bytes:0 lo Link encap:Local Loopback inet addr:127.0.0.1 Bcast:0.0.0.0 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 TX bytes:0 8325:/home/admin# tcpdump -i MirrorRxNet -xx tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on MirrorRxNet, link-type EN10MB (Ethernet), capture size 262144 bytes ^C 0 packets captured 0 packets received by filter 0 packets dropped by kernel NOTE: host/dst arguments to the tcpdump command can help to restrict the filter to only capture the desired packets.\nExpected Results Administrators can configure port mirroring The output of the show commands is correct Administrators can see the traffic for the source interface on the sniffer Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uais_by_viewing_log_output/",
	"title": "Troubleshoot UAIs by Viewing Log Output",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAIs by Viewing Log Output Sometimes a UAI will come up and run but will not work correctly. It is possible to see errors reported by elements of the UAI entrypoint script using the kubectl logs command.\nProcedure Find the UAI of interest.\nThis starts by identifying the UAI name using the CLI:\nncn-m001-pit# cray uas admin uais list Example output:\n[[results]] uai_age = \u0026#34;4h30m\u0026#34; uai_connect_string = \u0026#34;ssh broker@10.103.13.162\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026#34; uai_ip = \u0026#34;10.103.13.162\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-broker-2e6ce6b7\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;broker\u0026#34; [[results]] uai_age = \u0026#34;1h12m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.20.49.135\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.20.49.135\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-6da50e7a\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; Find the UAI in question, remembering that End-User UAIs run in the user Kubernetes namespace and Broker UAIs run in the uas Kubernetes namespace.\nEnd-User:\nncn-m001-pit# kubectl get po -n user | grep uai-vers-6da50e7a Example output:\nuai-vers-6da50e7a-54dbc99fdd-csxmk 1/1 Running 0 76m Broker:\nncn-m001-pit# kubectl get po -n uas | grep uai-broker-2e6ce6b7 Example output:\nuai-broker-2e6ce6b7-68d78c6c95-s28dh 2/2 Running 0 4h34m Use the pod name and the namespace to get the logs.\nEnd-User UAI pod name and the user namespace:\nncn-m001-pit# kubectl logs -n user uai-vers-6da50e7a-54dbc99fdd-csxmk uai-vers-6da50e7a Example output:\nSetting up passwd and group entries for vers Setting profile for vers Adding vers to groups Disabling password based login passwd: password expiry information changed. Checking to see if /home/users/vers exists If this hangs, please ensure that /home/users/vers is properly mounted/working on the host of this pod No home directory exists, creating one Checking for munge.key Setting up munge.key Check for pbs.conf Generating ssh keys and sshd_config ssh-keygen: generating new host keys: RSA DSA ECDSA ED25519 [...] Broker UAI pod name and the uas namespace:\nncn-m001-pit# kubectl logs -n uas uai-broker-2e6ce6b7-68d78c6c95-s28dh uai-broker-2e6ce6b7 Example output:\n/bin/bash: warning: setlocale: LC_ALL: cannot change locale (C.UTF-8) Configure PAM to use sssd... Generating broker host keys... ssh-keygen: generating new host keys: RSA DSA ECDSA ED25519 Checking for UAI_CREATION_CLASS... Starting sshd... Starting sssd... (Wed Feb 3 18:34:41:792821 2021) [sssd] [sss_ini_get_config] (0x0020): Config merge error: Directory /etc/sssd/conf.d does not exist. The above is from a successful Broker starting and running.\nTop: User Access Service (UAS)\nNext Topic: Troubleshoot Stale Brokered UAIs\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/update_default_servertech_pdu_credentials_used_by_the_redfish_translation_service/",
	"title": "Update Default ServerTech PDU Credentials used by the Redfish Translation Service (RTS)",
	"tags": [],
	"description": "",
	"content": "Update Default ServerTech PDU Credentials used by the Redfish Translation Service (RTS) This procedure updates the default credentials used by the Redfish Translation Service (RTS) for when new ServerTech PDUs are discovered in a system.\nThe Redfish Translation Service provides a Redfish interface that the Hardware State Manager (HSM) and Cray Advanced Platform Monitoring and Control (CAPMC) services can use interact with ServerTech PDUs which do not natively support Redfish.\nThere are two sets of default credentials that are required for RTS to function:\nThe default credentials to use when new ServerTech PDUs are discovered in the system. The global default credential that RTS uses for its Redfish interface with other CSM services. Important:: After this procedure is completed going forward all future ServerTech PDUs added to the system will be assumed to be already configured with the new global default credential when getting added to the system.\nNOTE: This procedure will not change the credentials on existing ServerTech PDUs in a system. To change the credential on existing air-cooled hardware, follow the Change Credentials on ServerTech PDUs procedure. However, this procedure will update the global default credential that RTS uses for its Redfish interface to other CSM services.\nProcedure Follow the Redeploying a Chart procedure with the following specifications:\nChart name: cray-hms-rts\nBase manifest name: sysmgmt\nWhen reaching the step to update the customizations, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nClone the CSM repository.\nncn-mw# git clone https://github.com/Cray-HPE/csm.git Acquire sealed secret keys.\nncn-mw# mkdir -pv certs ncn-mw# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39; | base64 -d \u0026gt; certs/sealed_secrets.crt ncn-mw# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.key}\u0026#39; | base64 -d \u0026gt; certs/sealed_secrets.key Modify RTS sealed secret to use new global default credentials.\nInspect the original default ServerTech PDU credentials.\nncn-mw# ./utils/secrets-decrypt.sh cray_hms_rts_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_pdu_defaults -r | base64 -d | jq Expected output looks similar to the following:\n{ \u0026#34;Username\u0026#34;: \u0026#34;admn\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foo\u0026#34; } Inspect the original default RTS Redfish interface credentials.\nncn-mw# ./utils/secrets-decrypt.sh cray_hms_rts_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_rts_defaults -r | base64 -d | jq Expected output looks similar to the following:\n{ \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;secret\u0026#34; } Update the default credentials in customizations.yaml for RTS.\nSpecify the desired default ServerTech PDU credentials.\nncn-mw# echo \u0026#39;{\u0026#34;Username\u0026#34;:\u0026#34;admn\u0026#34;, \u0026#34;Password\u0026#34;:\u0026#34;foobar\u0026#34;}\u0026#39; | base64 \u0026gt; rts.pdu.creds.json.b64 Specify the desired default RTS Redfish interface credentials.\nncn-mw# echo \u0026#39;{\u0026#34;Username\u0026#34;:\u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;:\u0026#34;supersecret\u0026#34;}\u0026#39; | base64 \u0026gt; rts.redfish.creds.json.b64 Update and regenerate the cray_hms_rts_credentials sealed secret.\nncn-mw# cat \u0026lt;\u0026lt; EOF | yq w - \u0026#39;data.vault_pdu_defaults\u0026#39; \u0026#34;$(\u0026lt;rts.pdu.creds.json.b64)\u0026#34; | yq w - \u0026#39;data.vault_rts_defaults\u0026#39; \u0026#34;$(\u0026lt;rts.redfish.creds.json.b64)\u0026#34; | yq r -j - | ./utils/secrets-encrypt.sh | yq w -f - -i ./customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_hms_rts_credentials\u0026#39; { \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cray-hms-rts-credentials\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: {} } EOF Decrypt generated secret for review.\nReview the default ServerTech PDU credentials.\nncn-mw# ./utils/secrets-decrypt.sh cray_hms_rts_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_pdu_defaults -r | base64 -d | jq Expected output looks similar to the following:\n{ \u0026#34;Username\u0026#34;: \u0026#34;admn\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foobar\u0026#34; } Review the Default RTS Redfish interface credentials.\nncn-mw# ./utils/secrets-decrypt.sh cray_hms_rts_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_rts_defaults -r | base64 -d | jq Expected output looks similar to the following:\n{ \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;supersecret\u0026#34; } When reaching the step to validate the redeployed chart, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nWait for the RTS job to run to completion:\nncn-mw# kubectl -n services wait job cray-hms-rts-init --for=condition=complete --timeout=5m Verify that the default ServerTech PDU credentials have updated in Vault.\nncn-mw# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-mw# kubectl -n vault exec -it cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 vault kv get secret/pdu-creds/global/pdu Expected output:\n====== Data ====== Key Value --- ----- Password foobar Username admn Verify that the default RTS Redfish interface credential has updated in Vault.\nncn-mw# kubectl -n vault exec -it cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 vault kv get secret/pdu-creds/global/rts Expected output:\n====== Data ====== Key Value --- ----- Password supersecret Username root Make sure to perform the entire linked procedure, including the step to save the updated customizations.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/spine_leaf_architecture3/",
	"title": "Why are spine-leaf architectures becoming more popular?",
	"tags": [],
	"description": "",
	"content": "Why are spine-leaf architectures becoming more popular? Given the prevalence of cloud and containerized infrastructure in modern data centers, east-west traffic continues to increase. East-west traffic moves laterally, from server to server. This shift is primarily explained by modern applications having components that are distributed across more servers or VMs.\nWith east-west traffic, having low-latency, optimized traffic flows is imperative for performance, especially for time-sensitive or data-intensive applications. A spine-leaf architecture aids this by ensuring traffic is always the same number of hops from its next destination, so latency is lower and predictable.\nCapacity also improves because STP is no longer required or at least the impact zones of STP can be limited to the edge. While STP enables redundant paths between two switches, only one can be active at any time. As a result, paths often become oversubscribed. Conversely, spine-leaf architectures rely on protocols such as Equal-Cost Multipath (ECMPM) routing to load balance traffic across all available paths while still preventing network loops.\nIn addition to higher performance, spine-leaf topologies provide better scalability. Additional spine switches can be added and connected to every leaf, increasing capacity. Likewise, new leaf switches can be seamlessly inserted when port density becomes a problem. In either case, this \u0026ldquo;scale-out\u0026rdquo; of infrastructure does not require any re-architecting of the network, and there is no downtime.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/port_security/",
	"title": "Port Security",
	"tags": [],
	"description": "",
	"content": "Port Security Port security allows user to do the following:\nConfigure each switch port with a list of unique MAC addresses Limit network access to authorized MAC addresses Detect, prevent, and log unauthorized access of devices on individual ports Limit the number of MACs learned Intrusion detection enables a device to notify the user or shutdown the port in the case of a violation, and a timer can be configured to allow auto-recovery of ports shutdown in a violation state to come back up after the timer expires.\nThe violation state of a port is reset with the port is administratively shutdown, port security is disabled on the port, or the port comes back up due to auto-recovery.\nIMPORTANT:\nPort security is only supported on physical ports and is mutually exclusive with dot1x and MAC auth Port security is feature of \u0026ldquo;edge\u0026rdquo; switches such as 63/6400 and not available on 83xx Configuration Commands Enable port security globally:\nswitch(config)# port-access port-security enable Enable port security on an interface:\nswitch(config-if)# port-access port-security Configure port access security violation action:\nswitch(config-if)# port-access security violation action \u0026lt;notify|shutdown\u0026gt; Configure port access security violation recovery timer:\nswitch(config-if)# port-access security violation action shutdown recovery-timer \u0026lt;10-600\u0026gt; Configure port access security violation auto recovery:\nswitch(config-if)# port-access security violation action shutdown auto-recovery enable Configure port security:\nswitch(config-if-port-security)# mac-address \u0026lt;MAC-ADDR\u0026gt; switch(config-if-port-security)# client-limit \u0026lt;1-64\u0026gt; Show commands to validate functionality:\nswitch# show port-access port-security interface \u0026lt;all|IFACE\u0026gt; \u0026lt;client-status|portstatistics\u0026gt; Example Output switch(config)# port-access port-security enable switch(config)# interface 1/1/1 switch(config-if)# port-access port-security switch(config-if-port-security)# client-limit 64 switch(config-if-port-security)# mac-address aa:bb:cc:dd:ee:ff switch(config-if-port-security)# end switch# show port-access port-security interface all client-status Port Security Client Status Details Authorized-Clients Port ---------------------------- AB:CD:DE:FF:AA:BB 1/1/1 DD:CD:AB:CD:EE:O1 1/1/2 show port-access port-security interface 1/1/1 client-status mac ab:cd:de:ff:aa:bb Port Security Client Status Details Authorized-Clients Port ---------------------------- AB:CD:DE:FF:AA:BB 1/1/1 switch#show port-access port-security interface all port-statistics Port 1/1/1 ========== Client Details -------------- Number of authorized clients : 2 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uais_with_administrative_access/",
	"title": "Troubleshoot UAIs with Administrative Access",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAIs with Administrative Access Sometimes there is no better way to figure out a problem with a UAI than to get inside it and look around as an administrator. This is done using kubectl exec to start a shell inside the running container as root (in the container). With this an administrator can diagnose problems, make changes to the running UAI, and find solutions. It is important to remember that any change made inside a UAI is transitory. These changes only last as long as the UAI is running. To make a permanent change, either the UAI image has to be changed or external customizations must be applied.\nSummary Example Next topic Summary The high-level steps of the procedure are the following:\nFind the name of the UAI in question. Use that name with kubectl to find the pod containing that UAI. Use that pod name, the UAI name (as the container name), and the user namespace to open an interactive shell in the container with kubectl exec. From this shell, look around the UAI as needed. Example Here is an example session showing a ps command inside the container of a UAI by an administrator.\nList the UAIs.\nncn-mw# cray uas admin uais list --format toml Example output:\n[[results]] uai_age = \u0026#34;1d4h\u0026#34; uai_connect_string = \u0026#34;ssh broker@10.103.13.162\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026#34; uai_ip = \u0026#34;10.103.13.162\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-broker-2e6ce6b7\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;broker\u0026#34; [[results]] uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.29.162.104\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.29.162.104\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-4ebe1966\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; Find the pod name.\nncn-mw# kubectl get po -n user | grep uai-vers-4ebe1966 Example output:\nuai-vers-4ebe1966-77b7c9c84f-xgqm4 1/1 Running 0 77s Open an interactive shell in the pod.\nncn-mw# kubectl exec -it -n user uai-vers-4ebe1966-77b7c9c84f-xgqm4 -c uai-vers-4ebe1966 -- /bin/sh Run the ps command inside the container of a UAI.\nuai# ps -afe Example output:\nUID PID PPID C STIME TTY TIME CMD root 1 0 0 22:56 ? 00:00:00 /bin/bash /usr/bin/uai-ssh.sh munge 36 1 0 22:56 ? 00:00:00 /usr/sbin/munged root 54 1 0 22:56 ? 00:00:00 su vers -c /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 55 54 0 22:56 ? 00:00:00 /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D root 90 0 0 22:58 pts/0 00:00:00 /bin/sh root 97 90 0 22:58 pts/0 00:00:00 ps -afe sh-4.4# Next topic Troubleshoot Common Mistakes when Creating a Custom End-User UAI Image\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/update_ncn_passwords/",
	"title": "Set NCN User Passwords",
	"tags": [],
	"description": "",
	"content": "Set NCN User Passwords The management node images do not contain a default root password or default SSH keys.\nUse one of these methods to change or set the root password in the image.\nIf the PIT node is booted, see Change NCN Image Root Password and SSH Keys on PIT Node for more information.\nIf the PIT node is not booted, see Change NCN Image Root Password and SSH Keys for more information.\nThe rest of this procedure describes how to change the root password stored in the HashiCorp Vault instance and then apply it immediately to management nodes with the csm.password Ansible role via a CFS session. The same root password from Vault will be applied anytime that the NCN personalization including the CSM layer is run. If no password is added to Vault as in the procedure below, this Ansible role will skip any password updates.\nNew in CSM release 1.2.0 Procedure: Configure root password in Vault Procedure: Apply root password to NCNs (standalone) Procedure for other users New in CSM release 1.2.0 The location of the password secret in Vault changed in CSM version 1.2. Prior to CSM 1.2, the location was secret/csm/management_nodes root_password=.... Starting in CSM version 1.2, the location is secret/csm/users/root password=....\nProcedure: Configure root password in Vault Generate a new password hash for the root user.\nThis script uses read -s to prevent the password from being echoed to the screen or saved in the shell history. It unsets the plaintext password variables at the end, so that they cannot be viewed later.\nlinux# read -r -s -p \u0026#34;New root password for NCN images: \u0026#34; PW1 ; echo ; if [[ -z ${PW1} ]]; then echo \u0026#34;ERROR: Password cannot be blank\u0026#34; else read -r -s -p \u0026#34;Enter again: \u0026#34; PW2 echo if [[ ${PW1} != ${PW2} ]]; then echo \u0026#34;ERROR: Passwords do not match\u0026#34; else echo -n \u0026#34;${PW1}\u0026#34; | openssl passwd -6 -salt $(\u0026lt; /dev/urandom tr -dc ./A-Za-z0-9 | head -c4) --stdin fi fi ; unset PW1 PW2 Get the HashiCorp Vault root token.\nncn-mw# kubectl get secrets -n vault cray-vault-unseal-keys -o jsonpath=\u0026#39;{.data.vault-root}\u0026#39; | base64 -d; echo Write the password hash to the HashiCorp Vault.\nWARNING: The CSM instance of Vault does not support the patch operation. Ensure that if the password field in the secret/csm/users/root secret is being updated, then any other desired fields are also included in the write command. For example the user\u0026rsquo;s SSH keys. See SSH keys. Any fields omitted from the write command will be cleared from Vault.\nThe path to the secret and the password field are configurable locations in the CSM csm.password Ansible role located in the CSM configuration management Git repository that is in use. If not using the defaults as shown in the command examples, ensure that the paths are consistent between Vault and the values in the Ansible role. See roles/csm.password in the repository for more information.\nOpen an interactive shell in the Vault Kubernetes pod.\nncn-mw# kubectl exec -itn vault cray-vault-0 -c vault -- sh Write the password hash to Vault.\nThe vault login command will request the token value from the output of the previous step. Use the password hash generated in the earlier step. NOTE: It is important to enclose the hash in single quotes to preserve any special characters. The vault read command allows the administrator to verify that the contents of the secret were stored correctly. cray-vault# export VAULT_ADDR=http://cray-vault:8200 cray-vault# vault login cray-vault# vault write secret/csm/users/root password=\u0026#39;\u0026lt;INSERT HASH HERE\u0026gt;\u0026#39; [... other fields (see warning above) ...] cray-vault# vault read secret/csm/users/root cray-vault# exit Procedure: Apply root password to NCNs (standalone) Use the following procedure with the rotate-pw-mgmt-nodes.yml playbook to only change the root password on NCNs. This is a quick alternative to running a full management NCN personalization, as documented in the Configure Non-Compute Nodes with CFS procedure.\nCreate a CFS configuration layer to run the password change Ansible playbook.\nNOTE This step only needs to be done once, as long as the commit in the CSM configuration management Git repository has not changed. If the commit has not changed since the last time this step was run, this step may be skipped, because the previously created CFS configuration will still work.\nCreate a file containing only this CFS configuration layer.\nThe file contents should be as follows, except replace the \u0026lt;INSERT GIT COMMIT ID\u0026gt; text with the commit in the CSM configuration management Git repository that is in use.\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ncn-password-update\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;rotate-pw-mgmt-nodes.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;INSERT GIT COMMIT ID\u0026gt;\u0026#34; } ] } Create the ncn-password-update configuration in CFS.\nReplace the \u0026lt;INSERT FILE PATH HERE\u0026gt; text with the path to the file created in the previous step. If a CFS configuration already exists with this name, the following command will overwrite it.\nncn-mw# cray cfs configurations update ncn-password-update --file \u0026lt;INSERT FILE PATH HERE\u0026gt; Create a CFS configuration session to apply the password update.\nncn-mw# cray cfs sessions create --name ncn-password-update-`date +%Y%m%d%H%M%S` --configuration-name ncn-password-update Monitor the CFS session.\nSee Track the Status of a Session.\nProcedure for other users The csm.password Ansible role supports setting passwords for non-root users.\nMake a copy of the rotate-pw-mgmt-nodes.yml Ansible playbook and modify the role variables to specify a different password_username.\nUsing that username, add the hashed password to Vault.\nFollow Procedure: Configure root password in Vault.\nCreate a configuration layer using the new Ansible playbook and create a CFS session using that layer.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/ssh/",
	"title": "Secure shell (SSH)",
	"tags": [],
	"description": "",
	"content": "Secure shell (SSH) SSH server enables an SSH client to make a secure and encrypted connection to a switch. Currently, switch supports SSH version 2.0 only. The user authentication mechanisms supported for SSH are public key authentication and password authentication (RADIUS, TACACS+ or locally stored password). Secure File Transfer Protocol (SFTP) provides file transfer. SSH Server and sftp-client via the copy command are supported for managing the router.\nRelevant Configuration\nConfigure SSH authentication\nswitch(config)# ssh server enable Generate SSH server key\nswitch(config)# ssh server host-key dsa2 private-key NOTE:\nkey-type\nrsa1 – RSAv1 rsa2 – RSAv2 dsa2 – DSAv2 private-key\tSets new private-key for the host keys of the specified type\npublic-key\tSets new public-key for the host keys of the specified type\ngenerate\tGenerates new RSA and DSA host keys for SSH\nEnable SSH to listen for incoming connections\nswitch(config)# ssh server listen enable Show Commands to Validate Functionality\nswitch# show ssh server Expected Results\nStep 1: You can create the user account Step 2: You can generate working SSH keys Step 3: The output of the show commands is correct Step 4: You can successfully connect to the switch via an SSH client using SSH 2.0. Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/queuing_and_scheduling/",
	"title": "Queuing and Scheduling",
	"tags": [],
	"description": "",
	"content": "Queuing and Scheduling When defining end-to-end behavior via CoS or DSCP, different priorities of traffic must be placed in different queues so the network device can service them appropriately. Separate queues allow delay- or jitter-sensitive traffic to be serviced before bulk or less time-critical traffic.\nQueue policies configure which queues the different priorities of traffic will use. Queues are numbered in priority order, with zero being the lowest priority. The larger the queue number, the higher the priority of that queue.\nSchedule policies configure the order of the queues from which packets are removed (de-queued) for transmitting. The schedule discipline is the algorithm the scheduler employs each round it must select the next packet for transmission. The supported scheduling disciplines are:\nStrict priority (SP): This is the simplest of scheduling disciplines. For each round, the scheduler will choose the highest-priority queue when removing packets for transmission. While this does provide prioritization of traffic, when spikes of high-priority traffic occur, it will prevent lower-priority traffic from being transmitted (queue starvation). Weighted fair queuing (WFQ): Weighted fair queuing can limit queue starvation by providing a fairer distribution of available bandwidth across the priorities. Lower-priority queues will have some service even when packets are present in higher-priority queues, depending on the weights assigned to each queue; the larger the weight, the greater the potential amount of service. WFQ accommodates variable traffic and services each packet as fairly as possible. Configuration Commands Create a profile:\nswitch(config)# qos queue-profile NAME switch(config)# qos schedule-profile NAME Apply a profile:\nswitch(config)# apply qos queue-profile NAME schedule-profile NAME Configure a profile:\nswitch(config-queue)# map queue \u0026lt;0-7\u0026gt; local-priority \u0026lt;0-7\u0026gt; switch(config-schedule)# strict queue \u0026lt;0-7\u0026gt; switch(config-schedule)# wfq queue \u0026lt;0-7\u0026gt; weight \u0026lt;0-253\u0026gt; Show commands to validate functionality:\nswitch# show interface IFACE queues switch# show qos queue-profile [QUEUE-NAME] switch# show qos schedule-profile [SCHED-NAME] Expected Results Administrators can configure a new queue profile and a schedule profile Administrators can apply queue profile and schedule profile The output of the show commands is correct The traffic pattern matches the scheduler configuration Example Output switch(config)# qos queue-profile VOICE-Q-PROFILE switch(config-queue)# map queue 0 local-priority 0 switch(config-queue)# map queue 1 local-priority 1 switch(config-queue)# map queue 2 local-priority 2 switch(config-queue)# map queue 3 local-priority 3 switch(config-queue)# map queue 4 local-priority 4 switch(config-queue)# map queue 5 local-priority 6 switch(config-queue)# map queue 6 local-priority 7 switch(config-queue)# map queue 7 local-priority 5 switch(config-queue)# exit switch(config)# qos schedule-profile VOICE-SCHED-PROFILE switch(config-schedule)# wfq queue 0 weight 3 switch(config-schedule)# wfq queue 1 weight 6 switch(config-schedule)# wfq queue 2 weight 12 switch(config-schedule)# wfq queue 3 weight 25 switch(config-schedule)# wfq queue 4 weight 50 switch(config-schedule)# wfq queue 5 weight 100 switch(config-schedule)# wfq queue 6 weight 200 switch(config-schedule)# strict queue 7 switch(config-schedule)# exit Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uas_issues/",
	"title": "Troubleshoot UAS Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAS Issues This section provides examples of some commands that can be used to troubleshoot UAS-related issues.\nTroubleshoot Connection Issues packet_write_wait: Connection to 203.0.113.0 port 30841: Broken pipe If an error message related to broken pipes returns, enable keep-alives on the client side. The admin should update the /etc/ssh/sshd_config and /etc/ssh/ssh_config files to add the following:\nTCPKeepAlive yes ServerAliveInterval 120 ServerAliveCountMax 720 Invalid Credentials ncn-w001 # cray auth login --username USER --password WRONGPASSWORD Example output:\nUsage: cray auth login [OPTIONS] Try \u0026#34;cray auth login --help\u0026#34; for help. Error: Invalid Credentials To resolve this issue:\nLog in to Keycloak and verify the user exists. Make sure the username and password are correct. Retrieve UAS Logs The system administrator can execute the following commands to retrieve UAS and the remote execution service logs:\nncn-w001# kubectl logs -n services -c cray-uas-mgr -l \u0026#34;app=cray-uas-mgr\u0026#34; Ensure that Slurm is Running and Configured Correctly Check if Slurm is running:\n[user@uai-user-be3a6770-6876c88676-2p2lk ~] $ sinfo The system returns a message similar to the following if Slurm is not running:\nslurm_load_partitions: Unable to contact slurm controller (connect failure) If this error is returned, it is likely that Slurm is not running. The system administrator can use the following commands to debug the issue:\nncn-w001# kubectl logs -n user -l app=slurmdb -c slurmdb --tail=-1 ncn-w001# kubectl logs -n user -l app=slurmdbd -c slurmdbd --tail=-1 ncn-w001# kubectl logs -n user -l app=slurmctld -c slurmctld --tail=-1 Troubleshoot Default Images Issues when Using the CLI If the image name provided while creating a new UAI is not registered for use by the system, the system returns an error message similar to the following:\nncn-w001# cray uas create --publickey ~/.ssh/id_rsa.pub --imagename fred Usage: cray uas create [OPTIONS] Try \u0026#34;cray uas create --help\u0026#34; for help. Error: Bad Request: Invalid image (fred). Valid images: [\u0026#39;dtr.dev.cray.com:443/cray/cray-uas-sles15sp1:latest\u0026#39;]. Default: dtr.dev.cray.com:443/cray/cray-uas-sles15sp1:latest Retry creating the UAI using the list of images and the name of the default image provided in the error message.\nVerify that the User Access Instances (UAIs) are Running The system administrator can use the kubectl command to check the status of the UAI.\nncn-w001# kubectl get pod -n user -l uas=managed -o wide Example output:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES uai-user-603d55f1-85d5ddb4b7-zk6nl 0/1 ContainerCreating 0 109s \u0026lt;none\u0026gt; sms-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; uai-user-d7f8d2e7-6dbdc64d98-7h5t5 0/1 ContainerCreating 0 116s \u0026lt;none\u0026gt; sms-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; uai-user-f6b72c9f-5dccd879bd-grbjw 0/1 ContainerCreating 0 113s \u0026lt;none\u0026gt; sms-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; If UAS pods are stuck in the Pending state, the admin needs to ensure the Kubernetes cluster has nodes available for running UAIs. Check that nodes are labeled with uas=True and are in the Ready state.\nncn-w001# kubectl get nodes -l uas Example output:\nNAME STATUS ROLES AGE VERSION ncn-w001 Ready \u0026lt;none\u0026gt; 11d v1.20.13 If none of the nodes are found or if the nodes listed are marked as NotReady, the UAI pods will not be scheduled and will not start.\nTroubleshoot kubectl Certificate Issues While kubectl is supported in a UAI, kubeconfig file to access a Kubernetes cluster is not provided. To use kubectl to interface with a Kubernetes cluster, the user must supply their own kubeconfig.\n[user@uai-user-be3a6770-6876c88676-2p2lk ~]# kubectl get nodes The connection to the server localhost:8080 was refused - did you specify the right host or port? Specify the location of the Kubernetes certificate with KUBECONFIG.\n[user@uai-user-be3a6770-6876c88676-2p2lk ~]# KUBECONFIG=/tmp/CONFIG kubectl get nodes Example output:\nNAME STATUS ROLES AGE VERSION ncn-m001 Ready control-plane,master 16d v1.20.13 ncn-m002 Ready control-plane,master 16d v1.20.13 Users must specify KUBECONFIG with every kubectl command or specify the kubeconfig file location for the life of the UAI. To do this, either set the KUBECONFIG environment variable or set the --kubeconfig flag.\nTroubleshoot X11 Issues The system may return the following error if the user attempts to use an application that requires an X window (such as xeyes):\n# ssh user@203.0.113.0 -i ~/.ssh/id_rsa Example output:\n______ ____ ___ __ __ __ __ ___ ____ / ____// __ \\ / |\\ \\/ / / / / // | / _/ / / / /_/ // /| | \\ / / / / // /| | / / / /___ / _, _// ___ | / / / /_/ // ___ | _/ / \\____//_/ |_|/_/ |_|/_/ \\____//_/ |_|/___/ [user@uai-user-be3a6770-6876c88676-2p2lk ~]$ xeyes Error: Can\u0026#39;t open display: To resolve this issue, pass the -X option with the ssh command as shown below:\n# ssh UAI_USERNAME@UAI_IP_ADDRESS -i ~/.ssh/id_rsa -X Example output:\n______ ____ ___ __ __ __ __ ___ ____ / ____// __ \\ / |\\ \\/ / / / / // | / _/ / / / /_/ // /| | \\ / / / / // /| | / / / /___ / _, _// ___ | / / / /_/ // ___ | _/ / \\____//_/ |_|/_/ |_|/_/ \\____//_/ |_|/___/ /usr/bin/xauth: file /home/users/user/.Xauthority does not exist [user@uai-user-be3a6770-6876c88676-2p2lk ~]$ echo $DISPLAY 203.0.113.0 The warning stating \u0026quot;Xauthority does not exist\u0026quot; will disappear with subsequent logins.\nTroubleshoot SSH Host Key Issues If strict host key checking enabled is enabled on the user\u0026rsquo;s client, the below error may appear when connecting to a UAI over SSH.\nWARNING: REMOTE HOST IDENTIFICATION HAS CHANGED This can occur in a few circumstances, but is most likely to occur after the UAI container is restarted. If this occurs, remove the offending ssh host key from the local known_hosts file and try to connect again. The error message from ssh will contain the correct path to the known_hosts file and the line number of the problematic key.\nDelete UAS Objects with kubectl If Kubernetes resources used to create a UAI are not cleaned up during the normal deletion process, resources can be deleted with the following commands.\nDelete anything created by the User Access Service (uas-mgr):\nWARNING: This command will delete all UAS resources for the entire system, it is not for targeted cleanup of a single UAI.\nncn-w001# kubectl delete all -n user -l uas=managed Delete all objects associated with a particular UAI:\nncn-w001# kubectl delete all -n user -l app=UAI-NAME Delete all objects for a single user:\nncn-w001# kubectl delete all -n user -l user=USERNAME Hard limits on UAI Creation Each Kubernetes worker node has limits on how many pods it can run. Nodes are installed by default with a hard limit of 110 pods per node, but the number of pods may be further limited by memory and CPU utilization constraints. For a standard node the maximum number of UAIs per node is 110; if other pods are co-scheduled on the node, the number will be reduced.\nDetermine the hard limit on Kubernetes pods with kubectl describe node and look for the Capacity section.\n# kubectl describe node NODE_NAME -o yaml Example output:\n[...] capacity: cpu: \u0026#34;16\u0026#34; ephemeral-storage: 1921298528Ki hugepages-1Gi: \u0026#34;0\u0026#34; hugepages-2Mi: \u0026#34;0\u0026#34; memory: 181009640Ki pods: \u0026#34;110\u0026#34; [...] When UAIs are created, some UAIs might left in the Pending state. The Kubernetes scheduler is unable to schedule them to a node, because of CPU, memory, or pod limit constraints. Use kubectl describe pod to check why the pod is Pending. For example, this pod is Pending because the node has reached the hard limit of 110 pods.\n# kubectl describe pod UAI-POD Example output:\nWarning Failed Scheduling 21s (x20 over 4m31s) default-scheduler 0/4 nodes are available: 1 Insufficient pods, 3 node(s) didn\u0026#39;t match node selector. Top: User Access Service (UAS)\nNext Topic: Troubleshoot UAS by Viewing Log Output\n"
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/updating_the_liquid-cooled_ex_cabinet_default_credentials_after_a_cec_password_change/",
	"title": "Updating the Liquid-Cooled EX Cabinet CEC with Default Credentials after a CEC Password Change",
	"tags": [],
	"description": "",
	"content": "Updating the Liquid-Cooled EX Cabinet CEC with Default Credentials after a CEC Password Change This procedure changes the credential for liquid-cooled EX cabinet chassis controllers and node controller (BMCs) used by CSM services after the CECs have been set to a new global default credential.\nNOTE: This procedure does not provision Slingshot switch BMCs (RouterBMCs). Slingshot switch BMC default credentials must be changed using the procedures in the Slingshot product documentation. To update Slingshot switch BMCs, refer to \u0026ldquo;Change Rosetta Login and Redfish API Credentials\u0026rdquo; in the Slingshot Operations Guide (\u0026gt; 1.6.0).\nThis procedure provisions only the default Redfish root account passwords. It does not modify Redfish accounts that have been added after an initial system installation.\nPrerequisites\nProcedure\nUpdate the default credentials used by MEDS for new hardware Update credentials for existing EX hardware in the system Reapply BMC settings if a StatefulReset was performed on any BMC Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. The hms-discovery Kubernetes CronJob has been disabled. All blades in the cabinets have been powered off. The procedures in Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials have been performed on all CECs in the system. All of the CECs must be configured with the same global credential. The previous default global credential for liquid-cooled BMCs must be known. Procedure The Mountain Endpoint Discovery Service (MEDS) sealed secret contains the default global credential used by MEDS when it discovers new liquid-cooled EX cabinet hardware.\n1. Update the default credentials used by MEDS for new hardware Follow the Redeploying a Chart procedure with the following specifications:\nChart name: cray-hms-bss\nBase manifest name: core-services\nWhen reaching the step to update the customizations, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nClone the CSM repository.\nncn-mw# git clone https://github.com/Cray-HPE/csm.git Acquire sealed secret keys.\nncn-mw# mkdir -pv certs \u0026amp;\u0026amp; kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39; | base64 -d \u0026gt; certs/sealed_secrets.crt \u0026amp;\u0026amp; kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.key}\u0026#39; | base64 -d \u0026gt; certs/sealed_secrets.key Modify MEDS sealed secret to use new global default credential.\nInspect the original default credential for MEDS.\nncn-mw# ./utils/secrets-decrypt.sh cray_meds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_redfish_defaults -r | base64 -d | jq Example output:\n{ \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;bar\u0026#34; } Specify the desired default credentials for MEDS to use with new hardware.\nReplace foobar with the root user password configured on the CECs.\nncn-mw# echo \u0026#39;{ \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foobar\u0026#34; }\u0026#39; | base64 \u0026gt; creds.json.b64 Update and regenerate the cray_meds_credentials sealed secret.\nncn-mw# cat \u0026lt;\u0026lt; EOF | yq w - \u0026#39;data.vault_redfish_defaults\u0026#39; \u0026#34;$(\u0026lt;creds.json.b64)\u0026#34; | yq r -j - | ./utils/secrets-encrypt.sh | yq w -f - -i ./customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_meds_credentials\u0026#39; { \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cray-meds-credentials\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: {} } EOF Decrypt updated sealed secret for review.\nThe sealed secret should match the credentials set on the CEC.\nncn-mw# ./utils/secrets-decrypt.sh cray_meds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_redfish_defaults -r | base64 -d | jq Example output:\n{ \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foobar\u0026#34; } When reaching the step to validate the redeployed chart, perform the following steps:\nOnly follow these steps as part of the previously linked chart redeploy procedure.\nWait for the MEDS Vault loader job to run to completion.\nncn-mw# kubectl wait -n services job cray-meds-vault-loader --for=condition=complete --timeout=5m Verify that the default credentials have changed in Vault.\nncn-mw# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-mw# kubectl -n vault exec -it cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 vault kv get secret/meds-cred/global/ipmi Example output:\n====== Data ====== Key Value --- ----- Password foobar Username root Make sure to perform the entire linked procedure, including the step to save the updated customizations.\n2. Update credentials for existing EX hardware in the system Set CRED_PASSWORD to the new updated password:\nncn-mw# read -s CRED_PASSWORD ncn-mw# echo $CRED_PASSWORD Expected output:\nfoobar Update the credentials used by CSM services for all previously discovered EX cabinet BMCs to the new global default.\nncn-mw# \\ REDFISH_ENDPOINTS=$(cray hsm inventory redfishEndpoints list --type \u0026#39;!RouterBMC\u0026#39; --format json | jq .RedfishEndpoints[].ID -r | sort -V ) cray hsm state components list --format json \u0026gt; /tmp/components.json for RF in $REDFISH_ENDPOINTS; do echo \u0026#34;$RF: Checking...\u0026#34; CLASS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.Components[] | select(.ID == $XNAME).Class\u0026#39; /tmp/components.json) if [[ \u0026#34;$CLASS\u0026#34; != \u0026#34;Mountain\u0026#34; ]]; then echo \u0026#34;$RF is not Mountain, skipping...\u0026#34; continue fi echo \u0026#34;$RF: Updating credentials\u0026#34; cray hsm inventory redfishEndpoints update ${RF} --user root --password ${CRED_PASSWORD} done It will take some time for the above bash script to run. It will take approximately 5 minutes to update all of the credentials for a single fully populated cabinet.\nAlternatively, use the following command on each BMC. Replace BMC_XNAME with the BMC component name (xname) to update the credentials:\nncn-mw# cray hsm inventory redfishEndpoints update BMC_XNAME --user root --password ${CRED_PASSWORD} Restart the hms-discovery Kubernetes CronJob.\nncn-mw# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : false }}\u0026#39; After 2-3 minutes, the hms-discovery CronJob will start to power on all of the currently powered off compute slots.\nWait for compute slots to be powered on and for HSM to re-discover the updated Redfish endpoints.\nncn-mw# sleep 300 Wait for all updated Redfish endpoints to become DiscoverOK.\nThe following Bash script will find all Redfish endpoints for the liquid-cooled BMCs that are not in DiscoverOK, and display their lastDiscoveryStatus.\nncn-mw# \\ cray hsm inventory redfishEndpoints list --laststatus \u0026#39;!DiscoverOK\u0026#39; --type \u0026#39;!RouterBMC\u0026#39; --format json \u0026gt; /tmp/redfishEndpoints.json cray hsm state components list --format json \u0026gt; /tmp/components.json REDFISH_ENDPOINTS=$(jq .RedfishEndpoints[].ID -r /tmp/redfishEndpoints.json | sort -V) for RF in $REDFISH_ENDPOINTS; do CLASS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.Components[] | select(.ID == $XNAME).Class\u0026#39; /tmp/components.json) if [[ \u0026#34;$CLASS\u0026#34; != \u0026#34;Mountain\u0026#34; ]]; then continue fi DISCOVERY_STATUS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.RedfishEndpoints[] | select(.ID == $XNAME).DiscoveryInfo.LastDiscoveryStatus\u0026#39; /tmp/redfishEndpoints.json) echo \u0026#34;$RF: $DISCOVERY_STATUS\u0026#34; done Example output:\nx1001c0r5b0: HTTPsGetFailed x1001c1s0b0: HTTPsGetFailed x1001c1s0b1: HTTPsGetFailed x1001c2s0b1: DiscoveryStarted For each Redfish endpoint that is reported use the following to troubleshoot why it is not DiscoverOK or DiscoveryStarted:\nIf the Redfish endpoint is DiscoveryStarted, then that BMC is currently in the process of being inventoried by HSM. Wait a few minutes and re-try the Bash script above to re-check the current discovery status of the RedfishEndpoints.\nThe hms-discovery cronjob (if enabled) will trigger a discover on BMCs that are not currently in DiscoverOK or DiscoveryStarted every three minutes.\nIf the Redfish endpoint is HTTPsGetFailed, then HSM had issues contacting BMC.\nVerify that the BMC component name (xname) is resolvable and pingable.\nncn-mw# ping x1001c1s0b0 If a NodeBMC is not pingable, then verify that the slot powering the BMC is powered on.\nIf this is a ChassisBMC, then skip this step.\nFor example, the NodeBMC x1001c1s0b0 is in slot x1001c1s0:\nncn-mw# cray capmc get_xname_status create --xnames x1001c1s0 Example output:\ne = 0 err_msg = \u0026#34;\u0026#34; on = [ \u0026#34;x1001c1s0b0\u0026#34;,] If the slot is off, power it on:\nncn-mw# cray capmc xname_on create --xnames x1001c1s0 If the BMC is reachable and in HTTPsGetFailed, then verify that the BMC is accessible with the new default global credential.\nReplace BMC_XNAME with the hostname of the Redfish endpoint.\nncn-mw# curl -k -u root:$CRED_PASSWORD https://BMC_XNAME/redfish/v1/Managers | jq If the error message below is returned, then the BMC must have a StatefulReset action performed on it. The StatefulReset action clears previously user-defined credentials that are taking precedence over the CEC-supplied credential. It also clears NTP, syslog, and SSH key configurations on the BMC.\n{ \u0026#34;error\u0026#34;: { \u0026#34;@Message.ExtendedInfo\u0026#34;: [ { \u0026#34;@odata.type\u0026#34;: \u0026#34;#Message.v1_0_5.Message\u0026#34;, \u0026#34;Message\u0026#34;: \u0026#34;While attempting to establish a connection to /redfish/v1/Managers, the service was denied access.\u0026#34;, \u0026#34;MessageArgs\u0026#34;: [ \u0026#34;/redfish/v1/Managers\u0026#34; ], \u0026#34;MessageId\u0026#34;: \u0026#34;Security.1.0.AccessDenied\u0026#34;, \u0026#34;Resolution\u0026#34;: \u0026#34;Attempt to ensure that the URI is correct and that the service has the appropriate credentials.\u0026#34;, \u0026#34;Severity\u0026#34;: \u0026#34;Critical\u0026#34; } ], \u0026#34;code\u0026#34;: \u0026#34;Security.1.0.AccessDenied\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;While attempting to establish a connection to /redfish/v1/Managers, the service was denied access.\u0026#34; } } Perform a StatefulReset on the liquid-cooled BMC. Replace BMC_XNAME with the hostname of the BMC. The OLD_DEFAULT_PASSWORD must match the credential that was previously set on the BMC. This is mostly likely the previous global default credential for liquid-cooled BMCs.\nncn-mw# curl -k -u root:OLD_DEFAULT_PASSWORD -X POST -H \u0026#39;Content-Type: application/json\u0026#39; -d \\ \u0026#39;{\u0026#34;ResetType\u0026#34;: \u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://BMC_XNAME/redfish/v1/Managers/BMC/Actions/Manager.Reset After the StatefulReset action has been issued, the BMC will be unreachable for a few minutes as it performs the StatefulReset.\n3. Reapply BMC settings if a StatefulReset was performed on any BMC This section only needs to be performed if any liquid-cooled node or chassis BMCs that had to be StatefulReset.\nFor each liquid-cooled BMC to which the StatefulReset action was applied, delete the BMC from HSM.\nReplace BMC_XNAME with the BMC component name (xname) to delete.\nncn-mw# cray hsm inventory redfishEndpoints delete BMC_XNAME Restart MEDS to re-setup the NTP and syslog configuration for the Redfish endpoints.\nView running MEDS pods.\nncn-mw# kubectl -n services get pods -l app.kubernetes.io/instance=cray-hms-meds Example output:\nNAME READY STATUS RESTARTS AGE cray-meds-6d8b5875bc-4jngc 2/2 Running 0 17d Restart MEDS.\nncn-mw# kubectl -n services rollout restart deployment cray-meds ncn-mw# kubectl -n services rollout status deployment cray-meds Wait five minutes for MEDS to re-discover the deleted Redfish endpoints.\nncn-mw# sleep 300 Verify that all expected hardware has been discovered.\nThe following Bash script will find all Redfish endpoints for the liquid-cooled BMCs that are not in DiscoverOK, and display their last discovery status.\nncn-mw# \\ cray hsm inventory redfishEndpoints list --laststatus \u0026#39;!DiscoverOK\u0026#39; --type \u0026#39;!RouterBMC\u0026#39; --format json \u0026gt; /tmp/redfishEndpoints.json cray hsm state components list --format json \u0026gt; /tmp/components.json REDFISH_ENDPOINTS=$(jq .RedfishEndpoints[].ID -r /tmp/redfishEndpoints.json | sort -V) for RF in $REDFISH_ENDPOINTS; do CLASS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.Components[] | select(.ID == $XNAME).Class\u0026#39; /tmp/components.json) if [[ \u0026#34;$CLASS\u0026#34; != \u0026#34;Mountain\u0026#34; ]]; then continue fi DISCOVERY_STATUS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.RedfishEndpoints[] | select(.ID == $XNAME).DiscoveryInfo.LastDiscoveryStatus\u0026#39; /tmp/redfishEndpoints.json) echo \u0026#34;$RF: $DISCOVERY_STATUS\u0026#34; done Restore SSH keys configured by Cray console services on liquid-cooled Node BMCs.\nGet the SSH console private key from Vault:\nncn-mw# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys \\ -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-mw# kubectl -n vault exec -t cray-vault-0 -c vault \\ -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 \\ VAULT_FORMAT=json vault read transit/export/signing-key/mountain-bmc-console \\ | jq -r .data.keys[] \u0026gt; ssh-console.key Generate the SSH public key.\nncn-mw# chmod 0600 ssh-console.key ncn-mw# export SCSD_SSH_CONSOLE_KEY=$(ssh-keygen -yf ssh-console.key) ncn-mw# echo $SCSD_SSH_CONSOLE_KEY Delete the SSH console private key from disk.\nncn-mw# rm ssh-console.key Generate a payload for the SCSD service.\nThe administrator must be authenticated to the Cray CLI before proceeding. See Configure the Cray Command Line Interface.\nncn-mw# cat \u0026gt; scsd_cfg.json \u0026lt;\u0026lt;DATA { \u0026#34;Force\u0026#34;:false, \u0026#34;Targets\u0026#34;: $(cray hsm state components list --class Mountain --type NodeBMC --format json | jq -r \u0026#39;[.Components[] | .ID]\u0026#39;), \u0026#34;Params\u0026#34;:{ \u0026#34;SSHConsoleKey\u0026#34;:\u0026#34;$(echo $SCSD_SSH_CONSOLE_KEY)\u0026#34; } } DATA Alternatively create a scsd_cfg.json file with only the SSH console key:\nncn-mw# cat \u0026gt; scsd_cfg.json \u0026lt;\u0026lt;DATA { \u0026#34;Force\u0026#34;:false, \u0026#34;Targets\u0026#34;:[ \u0026#34;x1000c0s0b0\u0026#34;, \u0026#34;x1000c0s0b0\u0026#34; ], \u0026#34;Params\u0026#34;:{ \u0026#34;SSHConsoleKey\u0026#34;:\u0026#34;$(echo $SCSD_SSH_CONSOLE_KEY)\u0026#34; } } DATA Edit the Targets array to contain the NodeBMCs that have have had the StatefulReset action.\nInspect the generated scsd_cfg.json file.\nEnsure that the following are true before running the cray scsd command in the following step:\nThe component name (xname) looks valid/appropriate. Limit the scsd_cfg.json file to NodeBMCs that have had the StatefulReset action applied to them. The SSHConsoleKey settings match the desired public key. Apply SSH console key to the NodeBMCs:\nncn-mw# cray scsd bmc loadcfg create scsd_cfg.json Check the output to verify all hardware has been set with the correct keys.\nPasswordless SSH to the consoles should now function as expected.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/static_mac/",
	"title": "Mac address Table",
	"tags": [],
	"description": "",
	"content": "Mac address Table You can configure static MAC addresses for unicast traffic. This feature improves security and reduces unknown unicast flooding.\nTo configure Unicast Static MAC address:\nSwitch (config) # mac-address-table static unicast \u0026lt;destination mac address\u0026gt; vlan \u0026lt;vlan identifier(1-4094)\u0026gt; interface ethernet \u0026lt;slot\u0026gt;/\u0026lt;port\u0026gt; For example:\nswitch (config) # mac-address-table static 00:11:22:33:44:55 vlan 1 interface ethernet 1/1 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/radius/",
	"title": "RADIUS",
	"tags": [],
	"description": "",
	"content": "RADIUS RADIUS servers provide a method for remote users to access the switch. The following commands show how to configure a RADIUS server, and how remote users can authenticate and access the switch.\nConfiguration Commands Configure RADIUS server:\nswitch(config)# radius-server host IP-ADDR [key \u0026lt;plain|cipher\u0026gt;text KEY] [timeout VALUE] [port PORT] [auth-type TYPE] [acct-port PORT] [retries VALUE] [vrf VRF] [tracking \u0026lt;enable|disable\u0026gt;] Configure AAA:\nswitch(config)# aaa authentication login default group radius local switch(config)# aaa accounting all default start-stop group radius Show commands to validate functionality:\nswitch# show radius-server [detail] switch# show aaa \u0026lt;server-groups|authentication\u0026gt; Expected Results SSH is enabled Administrators can configure the RADIUS server (reachable from the switch) The output of the show commands is correct Administrators can successfully access the switch using credentials validated by the RADIUS server Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uas_by_viewing_log_output/",
	"title": "Troubleshoot UAS by Viewing Log Output",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAS by Viewing Log Output At times there will be problems with UAS. Usually this takes the form of errors showing up on CLI commands that are not immediately interpretable as some sort of input error. It is sometimes useful to examine the UAS service logs to find out what is wrong.\nProcedure Find the names of the Kubernetes pods running UAS:\nncn-m001-pit# kubectl get po -n services | grep uas | grep -v etcd Example output:\ncray-uas-mgr-6bbd584ccb-zg8vx 2/2 Running 0 7d7h cray-uas-mgr-6bbd584ccb-acg7y 2/2 Running 0 7d7h View the logs for the pods running UAS.\nThe logs are collected in the pods, and can be seen using the kubectl logs command on each of the pods. The pods produce a lot of debug logging in the form:\n127.0.0.1 - - [02/Feb/2021 22:57:18] \u0026#34;GET /v1/mgr-info HTTP/1.1\u0026#34; 200 - Because of that, it is a good idea to filter this out unless the problem lies in specifically in the area of GET operations or aliveness checks. The following is an example where the last 25 lines of useful log output are retrieved from the pod cray-uas-mgr-6bbd584ccb-zg8vx:\nncn-m001-pit# kubectl logs -n services cray-uas-mgr-6bbd584ccb-zg8vx cray-uas-mgr | grep -v \u0026#39;\u0026#34;GET \u0026#39; | tail -25 Example output:\n2021-02-03 22:02:01,576 - uas_mgr - INFO - UAS request for: vers 2021-02-03 22:02:01,628 - uas_mgr - INFO - opt_ports: [] 2021-02-03 22:02:01,702 - uas_mgr - INFO - cfg_ports: [30123] 2021-02-03 22:02:01,702 - uas_mgr - INFO - UAI Name: uai-vers-32079250; Container ports: [{\u0026#39;container_port\u0026#39;: 30123, \u0026#39;host_ip\u0026#39;: None, \u0026#39;host_port\u0026#39;: None, \u0026#39;name\u0026#39;: \u0026#39;port30123\u0026#39;, \u0026#39;protocol\u0026#39;: \u0026#39;TCP\u0026#39;}]; Optional ports: [] 2021-02-03 22:02:02,211 - uas_mgr - INFO - opt_ports: [] 2021-02-03 22:02:02,566 - uas_mgr - INFO - cfg_ports: [30123] 2021-02-03 22:02:02,703 - uas_mgr - INFO - getting deployment uai-vers-32079250 in namespace user 2021-02-03 22:02:02,718 - uas_mgr - INFO - creating deployment uai-vers-32079250 in namespace user 2021-02-03 22:02:02,734 - uas_mgr - INFO - creating the UAI service uai-vers-32079250-ssh 2021-02-03 22:02:02,734 - uas_mgr - INFO - getting service uai-vers-32079250-ssh in namespace user 2021-02-03 22:02:02,746 - uas_mgr - INFO - creating service uai-vers-32079250-ssh in namespace user 2021-02-03 22:02:02,757 - uas_mgr - INFO - getting pod info uai-vers-32079250 2021-02-03 22:02:02,841 - uas_mgr - INFO - No start time provided from pod 2021-02-03 22:02:02,841 - uas_mgr - INFO - getting service info for uai-vers-32079250-ssh in namespace user 127.0.0.1 - - [03/Feb/2021 22:02:02] \u0026#34;POST /v1/uas HTTP/1.1\u0026#34; 200 - 2021-02-03 22:15:32,697 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-03 22:15:32,698 - uas_mgr - INFO - UAS request for: vers 2021-02-03 22:15:32,698 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-03 22:15:32,770 - uas_mgr - INFO - deleting service uai-vers-32079250-ssh in namespace user 2021-02-03 22:15:32,802 - uas_mgr - INFO - delete deployment uai-vers-32079250 in namespace user 127.0.0.1 - - [03/Feb/2021 22:15:32] \u0026#34;DELETE /v1/uas?uai_list=uai-vers-32079250 HTTP/1.1\u0026#34; 200 - If an error had occurred in UAS that error would likely show up here. Because there are two replicas of cray-uas-mgr running, the logging of interest may be in the other pod, so apply the same command to the other pod if the information is not here.\nTop: User Access Service (UAS)\nNext Topic: Troubleshoot UAIs by Viewing Log Output\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/static_routing/",
	"title": "Static routing",
	"tags": [],
	"description": "",
	"content": "Static routing \u0026ldquo;Static routing is manually performed by the network administrator. The administrator is responsible for discovering and propagating routes through the network. These definitions are manually programmed in every routing device in the environment. After a device has been configured, it simply forwards packets out the predetermined ports. There is no communication between routers regarding the current topology of the network.\u0026rdquo; –IBM Redbook, TCP/IP\nRelevant Configuration\nswitch(config)# ip route vrf default 0.0.0.0/0 null0 Show Commands to Validate Functionality\nswitch# show ip route Expected Results\nStep 1: You can configure a static route on the DUT Step 2: You can validate using the show command(s) above Step 3: You can ping the connected device Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/reboot_pxe_fail/",
	"title": "Rebooting NCNs and PXE Fails",
	"tags": [],
	"description": "",
	"content": "Rebooting NCNs and PXE Fails The following are common error messages when PXE fails:\n2021-04-19 23:27:09 PXE-E18: Server response timeout. 2021-02-02 17:06:13 PXE-E99: Unexpected network error. Procedure Verify the IP helper-address on VLAN 1 on the switches.\nThis is the same configuration as above \u0026ldquo;Aruba Configuration\u0026rdquo;.\nVerify DHCP packets can be forwarded from the workers to the MTL network (VLAN1).\nIf the worker nodes cannot reach the Metal (MTL) network DHCP will fail ALL WORKERS need to be able to reach the MTL network This can normally be achieved by having a default route Run connectivity tests.\nncn-w001# ping 10.1.0.1 Example output:\nPING 10.1.0.1 (10.1.0.1) 56(84) bytes of data. 64 bytes from 10.1.0.1: icmp_seq=1 ttl=64 time=0.361 ms 64 bytes from 10.1.0.1: icmp_seq=2 ttl=64 time=0.145 ms If this fails, CAN may be misconfigured, or a route might need to be added to the MTL network.\nncn-w001# ip route add 10.1.0.0/16 via 10.252.0.1 dev bond0.nmn0 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_classes/",
	"title": "UAI Classes",
	"tags": [],
	"description": "",
	"content": "UAI Classes UAI Classes provide templates for the creation of UAIs. They permit precise configuration of the behavior, volumes, resources, and other elements of the UAI. When a UAI is created using a UAI Class, it is configured to use exactly what that UAI Class has in it at the time the UAI was created. UIA Classes permit Broker UAIs to create different kinds of UAIs based on the UAI Creation Class setting of the Broker UAI. UAI Classes also provide the foundation on which Broker UAIs are built, defining specific configuration options without which it would not be possible to construct a Broker UAI.\nIn the Legacy UAI Creation mode, default UAI classes allow the precise configuration of user-created UAIs. This is particularly useful with regard to volumes, since, without a default UAI Class, all user-created UAIs would simply try to attach all configured volumes. Finally, default UAI Classes enable the Legacy UAI Creation mode to access Resource Specifications and other configuration not normally available to user-created UAIs.\nThis topic describes the content and purpose of the the fields in a UAI Class and gives guidance on setting those when creating UAI classes or various kinds.\nExample Listing and Overview The following is JSON-formatted example output from the cray uas admin config classes list command (see List Available UAI Classes).\nThis output contains examples of three UAI classes:\nA brokered End-User UAI class A UAI broker class A non-brokered End-User UAI class ncn-m001-pit# cray uas admin config classes list --format json Example output:\n[ { \u0026#34;class_id\u0026#34;: \u0026#34;bdb4988b-c061-48fa-a005-34f8571b88b4\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;UAI Class to Create Brokered End-User UAIs\u0026#34;, \u0026#34;default\u0026#34;: false, \u0026#34;image_id\u0026#34;: \u0026#34;1996c7f7-ca45-4588-bc41-0422fe2a1c3d\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: false, \u0026#34;replicas\u0026#34;: 1, \u0026#34;resource_config\u0026#34;: { \u0026#34;comment\u0026#34;: \u0026#34;Resource Specification to use with Brokered End-User UAIs\u0026#34;, \u0026#34;limit\u0026#34;: \u0026#34;{\\\u0026#34;cpu\\\u0026#34;: \\\u0026#34;1\\\u0026#34;, \\\u0026#34;memory\\\u0026#34;: \\\u0026#34;1Gi\\\u0026#34;}\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;{\\\u0026#34;cpu\\\u0026#34;: \\\u0026#34;1\\\u0026#34;, \\\u0026#34;memory\\\u0026#34;: \\\u0026#34;1Gi\\\u0026#34;}\u0026#34;, \u0026#34;resource_id\u0026#34;: \u0026#34;f26ee12c-6215-4ad1-a15e-efe4232f45e6\u0026#34; }, \u0026#34;resource_id\u0026#34;: \u0026#34;f26ee12c-6215-4ad1-a15e-efe4232f45e6\u0026#34;, \u0026#34;service_account\u0026#34;: null, \u0026#34;timeout\u0026#34;: { \u0026#34;hard\u0026#34;: \u0026#34;86400\u0026#34;, \u0026#34;soft\u0026#34;: \u0026#34;1800\u0026#34;, \u0026#34;warning\u0026#34;: \u0026#34;60\u0026#34; }, \u0026#34;tolerations\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: true, \u0026#34;uai_creation_class\u0026#34;: null, \u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: true, \u0026#34;image_id\u0026#34;: \u0026#34;1996c7f7-ca45-4588-bc41-0422fe2a1c3d\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34; }, \u0026#34;volume_list\u0026#34;: [ \u0026#34;11a4a22a-9644-4529-9434-d296eef2dc48\u0026#34;, \u0026#34;a3b149fd-c477-41f0-8f8d-bfcee87fdd0a\u0026#34; ], \u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;11a4a22a-9644-4529-9434-d296eef2dc48\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;a3b149fd-c477-41f0-8f8d-bfcee87fdd0a\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; } ] }, { \u0026#34;class_id\u0026#34;: \u0026#34;d764c880-41b8-41e8-bacc-f94f7c5b053d\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;UAI broker class\u0026#34;, \u0026#34;default\u0026#34;: false, \u0026#34;image_id\u0026#34;: \u0026#34;8f180ddc-37e5-4ead-b261-2b401914a79f\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;uas\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: true, \u0026#34;replicas\u0026#34;: 3, \u0026#34;resource_config\u0026#34;: null, \u0026#34;resource_id\u0026#34;: null, \u0026#34;service_account\u0026#34;: null, \u0026#34;timeout\u0026#34;: null, \u0026#34;tolerations\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: false, \u0026#34;uai_creation_class\u0026#34;: \u0026#34;bdb4988b-c061-48fa-a005-34f8571b88b4\u0026#34;, \u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: false, \u0026#34;image_id\u0026#34;: \u0026#34;8f180ddc-37e5-4ead-b261-2b401914a79f\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-broker:1.2.4\u0026#34; }, \u0026#34;volume_list\u0026#34;: [ \u0026#34;11a4a22a-9644-4529-9434-d296eef2dc48\u0026#34;, \u0026#34;1ec36af0-d5b6-4ad9-b3e8-755729765d76\u0026#34;, \u0026#34;a3b149fd-c477-41f0-8f8d-bfcee87fdd0a\u0026#34; ], \u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;11a4a22a-9644-4529-9434-d296eef2dc48\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/sssd\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;secret\u0026#34;: { \u0026#34;default_mode\u0026#34;: 384, \u0026#34;secret_name\u0026#34;: \u0026#34;broker-sssd-conf\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;1ec36af0-d5b6-4ad9-b3e8-755729765d76\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;broker-sssd-config\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;a3b149fd-c477-41f0-8f8d-bfcee87fdd0a\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; } ] }, { \u0026#34;class_id\u0026#34;: \u0026#34;5eb523ba-a3b7-4a39-ba19-4cfe7d19d296\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;UAI Class to Create Non-Brokered End-User UAIs\u0026#34;, \u0026#34;default\u0026#34;: true, \u0026#34;image_id\u0026#34;: \u0026#34;1996c7f7-ca45-4588-bc41-0422fe2a1c3d\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: true, \u0026#34;replicas\u0026#34;: 1, \u0026#34;resource_config\u0026#34;: null, \u0026#34;resource_id\u0026#34;: null, \u0026#34;service_account\u0026#34;: null, \u0026#34;timeout\u0026#34;: { \u0026#34;hard\u0026#34;: \u0026#34;86400\u0026#34;, \u0026#34;soft\u0026#34;: \u0026#34;1800\u0026#34;, \u0026#34;warning\u0026#34;: \u0026#34;60\u0026#34; }, \u0026#34;tolerations\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: true, \u0026#34;uai_creation_class\u0026#34;: null, \u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: true, \u0026#34;image_id\u0026#34;: \u0026#34;1996c7f7-ca45-4588-bc41-0422fe2a1c3d\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34; }, \u0026#34;volume_list\u0026#34;: [ \u0026#34;11a4a22a-9644-4529-9434-d296eef2dc48\u0026#34;, \u0026#34;a3b149fd-c477-41f0-8f8d-bfcee87fdd0a\u0026#34; ], \u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;11a4a22a-9644-4529-9434-d296eef2dc48\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;a3b149fd-c477-41f0-8f8d-bfcee87fdd0a\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; } ] } ] UAI Class Parameters The following selection is the core of a UAI Class configuration:\n\u0026#34;class_id\u0026#34;: \u0026#34;bdb4988b-c061-48fa-a005-34f8571b88b4\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;UAI Class to Create Brokered End-User UAIs\u0026#34;, \u0026#34;default\u0026#34;: false, \u0026#34;image_id\u0026#34;: \u0026#34;1996c7f7-ca45-4588-bc41-0422fe2a1c3d\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: false, \u0026#34;replicas\u0026#34;: 1, \u0026#34;resource_id\u0026#34;: \u0026#34;f26ee12c-6215-4ad1-a15e-efe4232f45e6\u0026#34;, \u0026#34;service_account\u0026#34;: null, \u0026#34;timeout\u0026#34;: { \u0026#34;hard\u0026#34;: \u0026#34;86400\u0026#34;, \u0026#34;soft\u0026#34;: \u0026#34;1800\u0026#34;, \u0026#34;warning\u0026#34;: \u0026#34;60\u0026#34; }, \u0026#34;tolerations\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: true, \u0026#34;uai_creation_class\u0026#34;: null, The following table explains each of these fields.\nField Description Notes class_id The identifier used for this class when examining, updating, and deleting the class. This identifier is also used to create UAIs using this class with the cray uas admin uais create command, and by Broker UAI Classes to specify what kind of End-User UAIs to create using the uai_creation_class field of the Broker UAI Class comment A free-form string describing the UAI class default A boolean value (flag) indicating whether this class is the default class. When this field is set to true, this class overrides both the default UAI image and any specified image name when the cray uas create command is used to create an End-User UAI for a user. Setting a class to \u0026quot;default\u0026quot;: true, gives the administrator fine-grained control over the behavior of End-User UAIs that are created by authorized users in legacy mode. namespace The Kubernetes namespace in which this UAI will run. The default setting is user. Broker UAIs should be configured to run in the uas namespace opt_ports An optional list of TCP port numbers that will be opened on the external IP address of the UAI when it runs. This field controls whether services other than SSH can be run and reached publicly on the UAI. If this list is empty (as in this example), only SSH will be externally accessible. In order for any service other than SSH to be publicly reachable the public_ip field must be set to true priority_class_name The Kubernetes priority class of the UAI. uai_priority is the default. Using other values affects both Kubernetes default resource limit and request assignments and the Kubernetes scheduling priority for the UAI. public_ip A boolean value that indicates whether the UAI will be given an external IP address from the LoadBalancer service. Such an address enables clients outside the Kubernetes cluster to reach the UAI. This field controls whether the UAI is reachable by SSH from external clients, but it also controls whether the ports in opt_ports are reachable. If this field is set to false, the UAI will have only an internal IP address, reachable from within the Kubernetes cluster. replicas The number of replica UAI pods to be created when a UAI of this class is created. This defaults to 1 and should not be set or should be set to 1 on End-User UAI Classes, since replica UAI pods for End-User UAIs only consume resources and potentially confuse the Broker UAI mechanism. For Broker UAI Classes, however, setting replicas to a larger value establishes both a degree of Broker UAI resiliency and a degree of load balancing, both for the purpose increasing network throughput on End-User UAI connections and for the purpose of avoiding overload of a single Broker UAI\u0026rsquo;s resources. resource\\id The ID of the Resource Specification used by this UAI Class By configuring a Resource Specification in a UAI Class the default resource requests and limits can be overridden when creating a UAI from that UAI Class service_account An optional Kubernetes Service Account name to be granted to UAIs using this class This is normally not set on End-User UAIs or Broker UAIs. It can be used to confer specific Kubernetes Role Based Access Control (RBAC) permissions on UAIs created using a UAI Class timeout An optional specification of hard and soft timeouts used to control the life-cycle of UAIs created using this UAI Class If either timeout setting is omitted that timeout will never expire. When a soft timeout, expires, the UAI terminates and is removed if it is or becomes idle, defined as having no logged in user sessions. When a hard timeout expires the UAI is terminated and removed immediately regardless of logged in user sessions. A warning may also be configured, specifying the number of seconds before a hard timeout that a warning will be sent to logged in users telling them of impending termination. The example here sets a hard timeout of 24 hours, a soft timeout of 30 minutes and a warning 60 seconds prior to arriving at the hard timeout. tolerations An optional list of Kubernetes tolerations that can be used in combination with \u0026ldquo;taints\u0026rdquo; on Kubernetes worker nodes to permit only UAIs of this class to run on those nodes. Tolerations and Taints can be used to designate certain Kubernetes Worker NCNs as hosts for UAIs and not for general management plane activities. They can also be used to specify that UAIs of a given class run only on nodes with specific resources. By default, all UAIs receive a toleration of uai_only op=Exists meaning that all UAIs can run on nodes that are tainted with a uai_only setting. uai_compute_network A flag that indicates whether this UAI uses the macvlan mechanism to gain access to the HPE Cray EX compute node network. This field must be true to support workload management from UAIs created by this class. It should be set to false on Broker UAIs. uai_creation_class A field used in Broker UAI Classes to tell the Broker UAI what kind of UAI to create when automatically creating a UAI. This field is not set in the preceding example. UAI Image Descriptions, Resource Descriptions, and Volume Descriptions in UAI Classes The following image description is provided as a convenience to allow the user to see the image information used when creating UAIs of this class:\n\u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: true, \u0026#34;image_id\u0026#34;: \u0026#34;1996c7f7-ca45-4588-bc41-0422fe2a1c3d\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-sles15sp2:1.2.4\u0026#34; }, The following Resource Specification description is provided as a convenience to allow the user to see the resource configuration used when creating UAIs of this UAI Class:\n\u0026#34;resource_config\u0026#34;: { \u0026#34;comment\u0026#34;: \u0026#34;Resource Specification to use with Brokered End-User UAIs\u0026#34;, \u0026#34;limit\u0026#34;: \u0026#34;{\\\u0026#34;cpu\\\u0026#34;: \\\u0026#34;1\\\u0026#34;, \\\u0026#34;memory\\\u0026#34;: \\\u0026#34;1Gi\\\u0026#34;}\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;{\\\u0026#34;cpu\\\u0026#34;: \\\u0026#34;1\\\u0026#34;, \\\u0026#34;memory\\\u0026#34;: \\\u0026#34;1Gi\\\u0026#34;}\u0026#34;, \u0026#34;resource_id\u0026#34;: \u0026#34;f26ee12c-6215-4ad1-a15e-efe4232f45e6\u0026#34; }, The following list of volume descriptions is provided as a convenience to allow the user to see the specific volume configuration used when creating UAIs of this class:\n\u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;11a4a22a-9644-4529-9434-d296eef2dc48\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;a3b149fd-c477-41f0-8f8d-bfcee87fdd0a\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; } ] Refer to Elements of a UAI for a full explanation of UAI images, Resource Specifications and volumes.\nIn the preceding section of output, the End-User UAI inherits the timezone from the host node by importing /etc/localtime. This UAI also gains access to the Lustre file system mounted on the host node. On the host node, the file system is mounted at /lus and the UAI mounts the file system at the same mount point as the host node.\nSpecifics of a Broker UAI Class Notice the following settings in the Broker UAI class example above:\n\u0026#34;default\u0026#34;: false, ... \u0026#34;image_id\u0026#34;: \u0026#34;8f180ddc-37e5-4ead-b261-2b401914a79f\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;uas\u0026#34;, ... \u0026#34;public_ip\u0026#34;: true, \u0026#34;replicas\u0026#34;: 3, ... \u0026#34;timeout\u0026#34;: null, ... \u0026#34;uai_compute_network\u0026#34;: false, \u0026#34;uai_creation_class\u0026#34;: \u0026#34;bdb4988b-c061-48fa-a005-34f8571b88b4\u0026#34;, \u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: false, \u0026#34;image_id\u0026#34;: \u0026#34;8f180ddc-37e5-4ead-b261-2b401914a79f\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-broker:1.2.4\u0026#34; }, Default is False Usually a site will not want or need to set a Broker UAI\u0026rsquo;s default flag to true because Broker UAIs will be administratively launched, not launched through the legacy mode UAI management procedure.\nImage ID specifies the HPE Supplied Broker UAI Image A Broker UAI runs in a special image that knows how to authenticate multiple users, find or create End-User UAIs on behalf of those users, and forward SSH connections to those End-User UAIs. HPE provides a Broker UAI image with this logic built into it.\nNamespace is uas Broker UAIs run in the uas namespace which is configured to set up pods with access to the API gateway. This is needed by Broker UAIs so that they can call UAS APIs to create, find and manage End-User UAIs.\nPublic IP is True Broker UAIs accept incoming SSH connections from external hosts, so they need to have a presence on an external network. Setting public_ip to true makes this work.\nReplicas is greater than 1 While it is not required to make the number of replicas for a Broker UAI greater than 1, setting a larger number makes the Broker UAI more resilient to node outages, resource starvation, and other possible issues. A larger replica count also reduces the networking and computational load on individual Broker UAI pods by permitting connections to be load balanced across the replicas. The replica count should not exceed the number of Kubernetes Worker Nodes permitted to host Broker UAIs.\nNo Timeout is Specified Broker UAIs cannot time out (there is no timeout mechanism in them) so setting a timeout on Broker UAIs is meaningless. Furthermore, since Broker UAIs are resources that should remain in place on a running system, putting a timeout on a Broker UAI would be counterproductive. Broker UAIs should have either no timeout specified or an empty timeout.\nUAI Compute Network is False Broker UAIs do not need access to workload management services, so they should not run with UAI Compute Network access. Setting this to true would consume IP addresses on the UAI Compute Network unnecessarily and reduce the number of End-User UAIs available on the system.\nSpecifics of a Brokered End-User UAI Class Notice the following settings in the Brokered End-User UAI Class:\n\u0026#34;default\u0026#34;: false, \u0026#34;image_id\u0026#34;: \u0026#34;8f180ddc-37e5-4ead-b261-2b401914a79f\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, ... \u0026#34;public_ip\u0026#34;: false, \u0026#34;replicas\u0026#34;: 1, ... \u0026#34;timeout\u0026#34;: { \u0026#34;hard\u0026#34;: \u0026#34;86400\u0026#34;, \u0026#34;soft\u0026#34;: \u0026#34;1800\u0026#34;, \u0026#34;warning\u0026#34;: \u0026#34;60\u0026#34; }, \u0026#34;uai_compute_network\u0026#34;: true, \u0026#34;uai_creation_class\u0026#34;: null, \u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: false, \u0026#34;image_id\u0026#34;: \u0026#34;8f180ddc-37e5-4ead-b261-2b401914a79f\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-broker:1.2.4\u0026#34; }, default is False The UAI Class used for Brokered End-User UAIs has characteristics that do not make it suitable for use as a Non-Brokered UAI, so a Brokered UAI Class should never be the default UAI Class.\nUAI Image is an End-User UAI Image In this example, the UAI image used is the HPE provided basic End-User UAI image. This could also be a custom End-User UAI image. The important thing for any End-User UAI Class is that the image is an End-User UAI image of some kind.\nNamespace is user In this example the namespace setting is user. This is the default setting and causes UAIs created by this UAI Class to run in the user namespace. The user namespace is isolated from Kubernetes resources in other namespaces and does not set up a connection to the API Gateway for pods running inside it. This, or a similarly isolated namespace should always be used for End-User UAIs since it keeps End-User UAIs isolated from management plane activities even though they are running inside the Kubernetes cluster.\nPublic IP Is False Brokered UAIs are always reached through Broker UAIs, so they do not need to and should not expose public IP access.\nReplicas is 1 or Not Specified Using replica pods in an End-User UAI simply wastes UAI Compute Network IP addresses, thereby limiting the number of End-User UAIs that can be created. The default value of 1 should be used for replicas in all End-User UAI Classes.\nTimeout is Provided While setting a timeout on End-User UAIs is not required, it is a good idea. Stale and idle UAIs consume resources that could be used by active fresh UAIs. By setting, at least, a soft timeout on End-User UAI Classes, the administrator can ensure that resources are released to the system when a user\u0026rsquo;s UAI becomes idle for an extended time. The above timeout specification will terminate the UAI, even if it is not idle, after 24 hours, with a 60 second warning. It will terminate an idle UAI after 30 minutes.\nUAI Compute Network is True End-User UAIs generally require access to workload management, so they require access the compute node network. Setting uai_compute_network to true makes this work.\nUAI Creation Class is not specified UAI Creation Class is only meaningful to UAIs that create other UAIs (specifically Broker UAIs).\nSpecifics of a Non-Brokered End-User UAI Class Non-Brokered End-User UAIs are very similar to Brokered End-User UAIs, but Non-Brokered End-User UAIs have some special traits. Notice the specific settings in the Non-Brokered End-User UAI Class that are different from those in the Brokered End-User UAI Class:\n\u0026#34;default\u0026#34;: true, \u0026#34;public_ip\u0026#34;: true, Default is True The UAI Class used for Non-Brokered End-User UAIs must be the default UAI Class. There is no way to create a UAI from a class in the Legacy Mode UAI Creation procedures.\nPublic IP Is True Manually created UAIs must be reached by direct SSH from external hosts, so they need to have a presence on an external network. Setting public_ip to true makes this work.\nTop: User Access Service (UAS)\nNext Topic: List Available UAI Classes\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/status_of_cray-dhcp-kea_pods/",
	"title": "Confirm the status of the cray-dhcp-kea pods/services",
	"tags": [],
	"description": "",
	"content": "Confirm the status of the cray-dhcp-kea pods/services Check if the kea DHCP services are running. On ncn-w001 or a worker/manager with kubectl, run:\nkubectl get -n services pods | grep kea You should see the following services as output:\nncn# kubectl get services -n services | grep kea cray-dhcp-kea-api Cluster IP 10.31.247.201 \u0026lt;none\u0026gt; 8000/TCP 3h36m cray-dhcp-kea-tcp-hmn LoadBalancer 10.25.109.178 10.94.100.222 67:30833/TCP 3h36m cray-dhcp-kea-tcp-nmn LoadBalancer 10.21.240.208 10.92.100.222 67:31915/TCP 3h36m cray-dhcp-kea-udp-hmn LoadBalancer 10.20.37.60 10.94.100.222 67:30357/UDP 3h36m cray-dhcp-kea-udp-nmn LoadBalancer 10.24.246.19 10.92.100.222 67:32188/UDP 3h36m On ncn-w001 or a worker/manager with kubectl, run:\nkubectl get pods -n services -o wide | grep kea You should get a list of the following pods as output:\nncn# kubectl get pods -n services -o wide | grep kea cray-dhcp-kea-788b4c899b-x6ltd 3/3 Running 0 36h 10.40.3.183 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-dhcp-kea-postgres-0 2/2 Running 0 5d23h 10.40.3.121 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-dhcp-kea-postgres-1 2/2 Running 0 5d23h 10.42.2.181 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-dhcp-kea-postgres-2 2/2 Running 0 5d23h 10.39.0.208 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; This output will also show which worker node the kea-dhcp pod is currently on.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/redundant_power_supplies/",
	"title": "Redundant Power Supplies",
	"tags": [],
	"description": "",
	"content": "Redundant Power Supplies There are no configuration commands for switch power supply functionality.\nNOTE: HA will be covered in HA section.\nShow commands to validate functionality:\nswitch# show environment power-supply Expected Results Validate the switch recognizes the additional power supplies Validate system remains powered after removing power from all but one power supply Validate all power supplies are operational Example Output switch# show environment power-supply Product Serial PSU Wattage Mbr/PSU Number Number Status --------------------------------------------------------- 1/1 JL372A M031SS004TAPC OK 2701 1/2 JL372A M031SS004UAPC OK 2430 1/3 N/A N/A Absent 0 1/4 N/A N/A Absent 0 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_host_node_selection/",
	"title": "UAI Host Node Selection",
	"tags": [],
	"description": "",
	"content": "UAI Host Node Selection When selecting UAI host nodes, it is a good idea to take into account the amount of combined load users and system services will bring to those nodes. UAIs run by default at a lower priority than system services on worker nodes which means that, if the combined load exceeds the capacity of the nodes, Kubernetes will eject UAIs and/or refuse to schedule them to protect system services. This can be disruptive or frustrating for users. This section explains how to identify the currently configured UAI host nodes and how to adjust that selection to meet the needs of users.\nIdentify UAI Host Nodes UAI host node identification is an exclusive activity, not an inclusive one, so it starts by identifying the nodes that could potentially be UAI host nodes by their Kubernetes role:\nIdentify nodes that could potentially be UAI host nodes by their Kubernetes role.\nncn-m001-pit# kubectl get nodes | grep -v master Example output:\nNAME STATUS ROLES AGE VERSION ncn-w001 Ready \u0026lt;none\u0026gt; 10d v1.20.13 ncn-w002 Ready \u0026lt;none\u0026gt; 25d v1.20.13 ncn-w003 Ready \u0026lt;none\u0026gt; 23d v1.20.13 In this example, there are three nodes known by Kubernetes that are not running as Kubernetes master nodes. These are all potential UAI host nodes.\nIdentify the nodes that are excluded from eligibility as UAI host nodes.\nncn-m001-pit# kubectl get no -l uas=False Example output:\nNAME STATUS ROLES AGE VERSION ncn-w001 Ready \u0026lt;none\u0026gt; 10d v1.20.13 NOTE: Given the fact that labels are textual not boolean, it is a good idea to try various common spellings of false. The ones that will prevent UAIs from running are \u0026lsquo;False\u0026rsquo;, \u0026lsquo;false\u0026rsquo; and \u0026lsquo;FALSE\u0026rsquo;. Repeat the above with all three options to be sure.\nOf the non-master nodes, there is one node in this example that is configured to reject UAIs, ncn-w001. So, ncn-w002 and ncn-w003 are UAI host nodes.\nSpecify UAI Host Nodes UAI host nodes are determined by labeling the nodes to reject UAIs. For example:\nncn-m001-pit# kubectl label node ncn-w001 uas=False --overwrite Please note here that setting uas=True or any variant of that, while potentially useful for local book keeping purposes, does NOT transform the node into a UAS host node. With that setting the node will be a UAS node because the value of the uas flag is not in the list False, false or FALSE, but unless the node previously had one of the false values, it was a UAI node all along. Perhaps more to the point, removing the uas label from a node labeled uas=True does not take the node out of the list of UAI host nodes. The only way to make a non-master Kubernetes node not be a UAS host node is to explicitly set the label to False, false or FALSE.\nMaintain an HSM Group for UAI Host Nodes When it comes to customizing non-compute node (NCN) contents for UAIs, it is useful to have a Hardware State Manager (HSM) node group containing the NCNs that are UAI hosts nodes. The hpe-csm-scripts package provides a script called make_node_groups that is useful for this purpose. This script is normally installed as /opt/cray/csm/scripts/node_management/make_node_groups. It can create and update node groups for management master nodes, storage nodes, management worker nodes, and UAI host nodes.\nThe following summarizes its use:\nncn-m001# /opt/cray/csm/scripts/node_management/make_node_groups --help Example output:\ngetopt: unrecognized option \u0026#39;--help\u0026#39; usage: make_node_groups [-m][-s][-u][w][-A][-R][-N] Where: -m - creates a node group for management master nodes -s - creates a node group for management storage nodes -u - creates a node group for UAI worker nodes -w - creates a node group for management worker nodes -A - creates all of the above node groups -N - executes a dry run, showing commands not running them -R - deletes existing node group(s) before creating them Here is an example of a dry-run that will create or update a node group for UAI host nodes:\nncn-m001# /opt/cray/csm/scripts/node_management/make_node_groups -N -R -u Example output:\n(dry run)cray hsm groups delete uai (dry run)cray hsm groups create --label uai (dry run)cray hsm groups members create uai --id x3000c0s4b0n0 (dry run)cray hsm groups members create uai --id x3000c0s5b0n0 (dry run)cray hsm groups members create uai --id x3000c0s6b0n0 (dry run)cray hsm groups members create uai --id x3000c0s7b0n0 (dry run)cray hsm groups members create uai --id x3000c0s8b0n0 (dry run)cray hsm groups members create uai --id x3000c0s9b0n0 Notice that when run in dry-run (-N option) mode, the script only prints out the CLI commands it will execute without actually executing them. When run with the -R option, the script removes any existing node groups before recreating them, effectively updating the contents of the node group. The -u option tells the script to create or update only the node group for UAI host nodes. That node group is named uai in the HSM.\nSo, to create a new node group or replace an existing one, called uai, containing the list of UAI host nodes, use the following command:\n# /opt/cray/csm/scripts/node_management/make_node_groups -R -u Use Kubernetes Taints and Tolerations to Make NCNs Exclusive UAI Hosts If a site has the NCN resources to host UAIs exclusively on a set of NCNs without impacting the operation of the HPE Cray EX System Management Plane, it is recommended to isolate UAIs on NCNs that do not also host Management Plane services. This can be done using Kubernetes Taints and Tolerations. By default, UAIs tolerate running on nodes where the uai_only taint has been set, while management services beyond the minimal set required to make the node a functioning Kubernetes Worker node on HPE Cray EX do not tolerate that taint. By adding that taint to nodes in the Kubernetes cluster, sites can keep management services away from those nodes while allowing UAIs to run there. By further adding the uas=False label to all worker nodes in the Kubernetes cluster where UAIs are not allowed, sites can ensure that UAIs only run on exclusive UAI hosts.\nFurther selection of UAI hosts can be achieved by any site by adding further taints to Kubernetes nodes, and configuring tolerations for those taints into specific UAI Classes.\nTop: User Access Service (UAS)\nNext Topic: UAI macvlans Network Attachments\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/system_images/",
	"title": "System images",
	"tags": [],
	"description": "",
	"content": "System images Mellanox switches can hold two firmware images. These images, once uploaded, are called the Running and Image available for install.\nRelevant Configuration\nCopy an image from a local server using sftp\nswitch (config)#image delete XXX // --\u0026gt; delete old images, if exist switch (config)#image fetch scp://root:password@server/path-to-image/image-X86_64-3.4.2002.img switch (config)#image install image-X86_64-3.4.2002.img Boot the switch into the new firmware\nswitch (config)#image boot next switch (config)#configuration write switch (config)#reload Show Commands to Validate Functionality\nswitch# show version Expected Results\nStep 1: You can upload an image to the switch Step 2: You can see the versions of code for the primary and secondary images Step 3: You can boot into the uploaded image Step 4: You can see you are running the uploaded image Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/remote_logging/",
	"title": "Remote Logging",
	"tags": [],
	"description": "",
	"content": "Remote Logging Configure remote logging to view log files from the switch on a remote server. This functionality is enabled by syslog.\nNOTE: The default facility is three (DAEMON).\nConfiguration Commands Configure logging:\nswitch(config)# logging IP-ADDR Expected Results Administrators can configure remote logging Administrators can see the log files from the switch on the remote server Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_host_nodes/",
	"title": "UAI Host Nodes",
	"tags": [],
	"description": "",
	"content": "UAI Host Nodes UAIs run as Kubernetes pods on Kubernetes worker nodes. UAS provides a a mechanism using Kubernetes labels to prevent UAIs from running on a specific worker nodes, but any Kubernetes node that is not labeled to prevent UAIs from running on it is considered eligible to host UAIs. The administrator of a given site may control the set of UAI host nodes by labeling Kubernetes worker nodes appropriately.\nCertain product installation procedures call for the installation of product components on the UAI Host Nodes so that UAIs can use those resources directly from the host node (as opposed to, for example, external shared storage). It is important to make sure that any such resources are maintained on the UAI host nodes. If a UAI is configured to use resources from the host node that cannot be found, then the UAI will fail to start, usually remaining in a Waiting state. This documentation contains procedures for diagnosing and fixing issues related to missing host node resources.\nNodes can also be \u0026ldquo;tainted\u0026rdquo; in Kubernetes to permit UAIs but not permit general HPE Cray EX System management plane services to run on those nodes. Through the use of Kubernetes Taints and Tolerations, with tolerations configured in a UAI Class, it is possible to achieve fine-grained control of where UAIs of different classes are deployed on an HPE Cray EX System.\nTop: User Access Service (UAS)\nNext Topic: UAI Host Node Selection\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/test_tftp_traffic/",
	"title": "Test TFTP traffic (Aruba Only)",
	"tags": [],
	"description": "",
	"content": "Test TFTP traffic (Aruba Only) You can test the TFTP traffic by trying to download the ipxe.efi binary. Log into the leaf switch and try to download the iPXE binary. This requires that the leaf switch can talk to the TFTP server \u0026ldquo;10.92.100.60\u0026rdquo; sw-leaf-001# start-shell sw-leaf-001:~$ sudo su sw-leaf-001:/home/admin# tftp 10.92.100.60 tftp\u0026gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds tftp\u0026gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds tftp\u0026gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds You can see here that the ipxe.efi binary is downloaded three times in a row.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/requirements_and_optional_configuration/",
	"title": "Connect the Management Network to a Campus Network",
	"tags": [],
	"description": "",
	"content": "Connect the Management Network to a Campus Network There are several ways to connect an HPE Cray EX system directly to a campus network. In this guide, the two most typical ways of accomplishing this will be covered. The Scenario A and Scenario B examples will cover adding connections through the management network or high-speed network.\nRequirements and optional configuration:\nSystem needs to be completely installed and running The edge router should be cabled either to the management network or Highspeed network switch An IP address range on the management or high-speed network switch that is routable to the campus network Other configuration items that may be required to facilitate remote connectivity: Configuration may require a new LAG Configuration may require a new VLAN Configuration may require a new router OSPF context Other things to consider ACL Stubby OSPF area Route restrictions i.e. only provide default route IMPORTANT: As there are multiple ways of achieving the connectivity these are just simple examples of how remote access could be achieved. And more complex configurations such as security etc. are up to the site network administrators.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_image_customization/",
	"title": "UAI Image Customization",
	"tags": [],
	"description": "",
	"content": "UAI Image Customization This section covers common customizations of both End-User UAIs and Broker UAIs.\nRefer to the following topics for more information:\nCustomize the Broker UAI Image Customize End-User UAI Images Top: User Access Service (UAS)\nNext Topic: Customize the Broker UAI Image\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/typical_mlag_port_configuration/",
	"title": "Typical configuration of MLAG link connecting to NCN",
	"tags": [],
	"description": "",
	"content": "Typical configuration of MLAG link connecting to NCN The intent here is to show case very basic MLAG link configuration and your configuration may differ. This is what defines the LAG to be able to peer both to Spine-01 and Spine-02.\ninterface mlag-port-channel 1 interface mlag-port-channel 1 mtu 9216 force interface ethernet 1/1 mlag-channel-group 1 mode active interface mlag-port-channel 1 switchport mode hybrid interface mlag-port-channel 1 interface mlag-port-channel 1-11 lacp-individual enable force interface mlag-port-channel 1 switchport hybrid allowed-vlan add 2 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 4 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 7 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 10 interface mlag-port-channel 1 interface mlag-port-channel 1 mtu 9216 force interface ethernet 1/1 mlag-channel-group 1 mode active interface mlag-port-channel 1 switchport mode hybrid interface mlag-port-channel 1 interface mlag-port-channel 1-11 lacp-individual enable force interface mlag-port-channel 1 switchport hybrid allowed-vlan add 2 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 4 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 7 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 10 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/routed_interface/",
	"title": "Routed interfaces",
	"tags": [],
	"description": "",
	"content": "Routed interfaces For platforms 8400 and 83xx: By default, all interfaces are configured as routed interfaces with support for both IPv4 and IPv6.\nFor platforms 6400 and 6300: By default, all interfaces are configured as access ports on VLAN 1\nConfiguration Commands Give an interface an IP address:\nswitch(config-if)# \u0026lt;ip|ipv6\u0026gt; address IP-ADDR/\u0026lt;SUBNET|PREFIX\u0026gt; Show commands to validate functionality:\nswitch# show \u0026lt;ip|ipv6\u0026gt; interface IFACE Expected Results Administrators are able to configure an IP address on the interface Administrators can configure an IP address on the connected network client The interface is up, and you can validate the IP address and subnet are correct Administrators can ping from the switch to the client and from the client to the switch Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_images/",
	"title": "UAI Images",
	"tags": [],
	"description": "",
	"content": "UAI Images There are three kinds of UAI images used by UAS:\nA pre-packaged Broker UAI image provided with the UAS A pre-packaged basic End-User UAI Image provided with the UAS Custom End-User UAI images created on site, usually based on compute node contents UAS provides two stock UAI images when installed. The first is a standard End-User UAI Image that has the necessary software installed in it to support a basic Linux distribution login experience. This image is provided for the purpose of sanity testing the UAS installation and as a simple starting point for administrative experimentation with UAS and UAIs.\nThe second image is a Broker UAI image. Broker UAIs are a special type of UAIs used in the \u0026ldquo;broker based\u0026rdquo; operation model. Broker UAIs present a single SSH endpoint that responds to each SSH connection by locating or creating a suitable End-User UAI and redirecting the SSH session to that End-User UAI.\nA site may provide any number of Custom End-User UAI Images as needed to support various use cases and workflows.\nTop: User Access Service (UAS)\nNext Topic: Listing Registered UAI Images\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/typical_mlag_switch_configuration/",
	"title": "Typical configuration of MLAG between switches",
	"tags": [],
	"description": "",
	"content": "Typical configuration of MLAG between switches The intent here is to show case very basic mlag configuration between two spine switches.\nmlag-vip cray-mlag-domain ip 192.168.255.242 /29 force no mlag shutdown mlag system-mac 00:00:5E:00:01:01 interface port-channel 100 ipl 1 interface vlan 4000 ipl 1 peer-address 192.168.255.253 mlag-vip cray-mlag-domain ip 192.168.255.242 /29 force no mlag shutdown mlag system-mac 00:00:5E:00:01:5D interface port-channel 100 ipl 1 interface vlan 4000 ipl 1 peer-address 192.168.255.254 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/scenario-a/",
	"title": "Scenario A Network Connection via Management Network",
	"tags": [],
	"description": "",
	"content": "Scenario A: Network Connection via Management Network The example here covers outside connections achieved with the management network.\nSummary Create a new VRF Move interfaces to the new VRF Create a new BGP process for the new VRF Setup the edge router Configure MetalLB Verification step for BGP routes Configure default route for workers Verification of external communication Topology The following is an example topology:\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_management/",
	"title": "UAI Management",
	"tags": [],
	"description": "",
	"content": "UAI Management UAS supports two manual methods and one automated method of UAI management:\nDirect administrative UAI management Legacy mode user driven UAI management UAI broker mode UAI management Direct administrative UAI management is available mostly to allow administrators to set up Broker UAIs for the Broker Mode UAI Management and to control UAIs that are created under one of the other two methods. It is unlikely that a site will choose to create End-User UAIs this way, but it is possible to do. The administrative UAI management API provides an administrative way to list, create, examine, and delete UAIs.\nThe legacy mode of UAI management gives users the authority to create, list, and delete UAIs that belong to them. While this is a conceptually simple mode, it can lead to an unnecessary proliferation of UAIs belonging to a single user if the user is not careful to create UAIs only when needed. The legacy mode also cannot take advantage of the breadth of UAI classes to create more than one kind of UAI for different users\u0026rsquo; needs. In the legacy mode, UAIs can be created using a default UAI Class as a template and will always be created this way if a default UAI Class is configured, or, in the absence of a default UAI Class, using any of the registered UAI images and all of the volumes configured in the UAS. Use of a default UAI Class for legacy mode UAI creation is strongly advised.\nBroker UAIs mode creates / re-use UAIs on demand when a user logs into a Broker UAI using SSH. A site may run multiple Broker UAIs, each configured to create UAIs of a different UAI class and each running with its own externally visible IP address. By choosing the correct IP address and logging into the broker, a user ultimately arrives in an End-User UAI tailored for a given use case. Because the broker is responsible for managing the underlying End-User UAIs, users need not be given authority to create UAIs directly and, therefore, cannot cause a proliferation of unneeded UAIs. Because the Broker UAIs each run separately on different IP addresses with, potentially, different user authorizations configured, a site can control which users are given access to which classes of End-User UAIs.\nTop: User Access Service (UAS)\nNext Topic: List UAIs\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/upgrade/",
	"title": "Performing Upgrade On Mellanox Switches",
	"tags": [],
	"description": "",
	"content": "Performing Upgrade On Mellanox Switches Supported Software Upgrades\nTarget Version Verified Starting Versions 3.9.3xxx 3.9.2xxx, 3.9.1xxx 3.9.2xxx 3.9.1xxx, 3.9.0xxx 3.9.1xxx 3.9.0xxx, 3.8.2xxx 3.9.0xxx 3.8.2xxx, 3.8.1xxx Repeated the following procedure for each \u0026ldquo;upgrade hop\u0026rdquo;.\nUpgrading the Switch Using the CLI The Switch OS software packages include the switch firmware and the CPU software for the specific switch board CPU (x86).\nInstalling the CPU software also automatically installs the embedded firmware. Similarly, once the OS is upgraded, the firmware is upgraded as well.\nThe switch\u0026rsquo;s OS image and its documentation collateral (release notes, user manual) can be found in MyMellanox.\nRun the following commands in order to upgrade (in this example, upgrading to version 3.9.0606):\nswitch (config)#image delete XXX // --\u0026gt; delete old images one at a time, if exist switch (config)#image fetch scp://root:password@server/path-to-image/image-X86_64-3.9.0606.img switch (config)#image install image-X86_64-3.9.0606.img switch (config)#image boot next switch (config)#configuration write switch (config)#reload Wait a few minutes to allow the OS\u0026rsquo;s upgrade process to complete, and then reconnect to the system.\nIn order to verify that the installation was completed successfully, run:\n# show version Example output looks similar to the following:\nProduct name: MLNX-OS Product release: 3.9.0606 Build ID: #1-dev Build date: 2020-05-01 08:20:15 Target arch: x86_64 Target hw: x86_64 Built by: sw-r2d2-bot@b13770d14a06 Version summary: X86_64 3.9.0606 2020-05-01 08:20:15 x86_64 Product model: x86 Host ID: 7CFE900BC470 System UUID: 03000200-0400-0500-0006-000700080009 Uptime: 16m 54.930s CPU load averages: 0.02 / 0.08 / 0.11 Number of CPUs: 2 System memory: 485 MB used / 3278 MB free / 3763 MB total Swap: 0 MB used / 0 MB free / 0 MB total In order to verify the system\u0026rsquo;s firmware version, run the following command:\n# show asic-version Output looks similar to the following:\n--------------------------------------------------- Module Device Version --------------------------------------------------- MGMT SIB2 15.2007.0900 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/scenario-b/",
	"title": "Scenario B Network Connection via High-Speed Network",
	"tags": [],
	"description": "",
	"content": "Scenario B: Network Connection via High-Speed Network This example covers outside connections achieved via highspeed network.\nSummary Create a new VRF Move interfaces to the new VRF Create a new BGP process for the new VRF Setup the edge router Configure MetalLB Verification step for BGP routes Configure default route for workers Verification of external communication Topology The following is an example topology:\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_network_attachments/",
	"title": "UAI Network Attachment Customization",
	"tags": [],
	"description": "",
	"content": "UAI Network Attachment Customization The UAI network attachment configuration flows from the Cray Site Initializer (CSI) localization data through customizations.yaml into the UAS Helm chart and, ultimately, into Kubernetes in the form of a \u0026ldquo;network-attachment-definition\u0026rdquo;.\nThis section describes the data at each of those stages to show how the final network attachment gets created. Customization of the network attachments may be needed by some sites to, for example, increase the size of the reserved sub-net used for UAI macvlan attachments.\nCSI Localization Data The details of CSI localization are beyond the scope of this guide, but here are the important settings, and the values used in the following examples:\nThe interface name on which the Kubernetes worker nodes reach their Node Management Network (NMN) subnet: bond0.nmn0 The network and CIDR configured on that interface: 10.252.0.0/17 The IP address of the gateway to other NMN subnets found on that network: 10.252.0.1 The subnets where compute nodes reside on this system: 10.92.100.0/24 10.106.0.0/17 10.104.0.0/17 Contents of customizations.yaml When CSI runs, it produces the following data structure in the spec section of customizations.yaml:\nspec: [...] wlm: [...] macvlansetup: nmn_subnet: 10.252.2.0/23 nmn_supernet: 10.252.0.0/17 nmn_supernet_gateway: 10.252.0.1 nmn_vlan: bond0.nmn0 # NOTE: the term DHCP here is misleading, this is merely # a range of reserved IP addresses for UAIs that should not # be handed out to others because the network # attachment will hand them out to UAIs. nmn_dhcp_start: 10.252.2.10 nmn_dhcp_end: 10.252.3.254 routes: - dst: 10.92.100.0/24 gw: 10.252.0.1 - dst: 10.106.0.0/17 gw: 10.252.0.1 - dst: 10.104.0.0/17 gw: 10.252.0.1 The nmn_subnet value shown here is not relevant for this section.\nThese values, in turn, feed into the following translation to UAS Helm chart settings:\ncray-uas-mgr: uasConfig: uai_macvlan_interface: \u0026#39;{{ wlm.macvlansetup.nmn_vlan }}\u0026#39; uai_macvlan_network: \u0026#39;{{ wlm.macvlansetup.nmn_supernet }}\u0026#39; uai_macvlan_range_start: \u0026#39;{{ wlm.macvlansetup.nmn_dhcp_start }}\u0026#39; uai_macvlan_range_end: \u0026#39;{{ wlm.macvlansetup.nmn_dhcp_end }}\u0026#39; uai_macvlan_routes: \u0026#39;{{ wlm.macvlansetup.routes }}\u0026#39; UAS Helm Chart The inputs in the previous section tell the UAS Helm chart how to install the network attachment for UAIs. While the actual template used for this is more complex, the following is a simplified view of the template used to generate the network attachment.\napiVersion: \u0026#34;k8s.cni.cncf.io/v1\u0026#34; kind: NetworkAttachmentDefinition [...] spec: config: \u0026#39;{ \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;macvlan\u0026#34;, \u0026#34;master\u0026#34;: \u0026#34;{{ .Values.uasConfig.uai_macvlan_interface }}\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;{{ .Values.uasConfig.uai_macvlan_network }}\u0026#34;, \u0026#34;rangeStart\u0026#34;: \u0026#34;{{ .Values.uasConfig.uai_macvlan_range_start }}\u0026#34;, \u0026#34;rangeEnd\u0026#34;: \u0026#34;{{ .Values.uasConfig.uai_macvlan_range_end }}\u0026#34;, \u0026#34;routes\u0026#34;: [ {{- range $index, $route := .Values.uasConfig.uai_macvlan_routes }} {{- range $key, $value := $route }} { \u0026#34;{{ $key }}\u0026#34;: \u0026#34;{{ $value }}\u0026#34;, }, {{- end }} {{- end }} ] } }\u0026#39; The range templating in the routes section expands the routes from customizations.yaml into the network attachment routes.\nUAI Network Attachment in Kubernetes All of this produces a network attachment definition in Kubernetes called macvlan-uas-nmn-conf which is used by UAS.\nThe following contents would result from the above data:\napiVersion: v1 items: - apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition ... spec: config: \u0026#39;{ \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;macvlan\u0026#34;, \u0026#34;master\u0026#34;: \u0026#34;bond0.nmn0\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;10.252.0.0/17\u0026#34;, \u0026#34;rangeStart\u0026#34;: \u0026#34;10.252.124.10\u0026#34;, \u0026#34;rangeEnd\u0026#34;: \u0026#34;10.252.125.244\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;10.92.100.0/24\u0026#34;, \u0026#34;gw\u0026#34;: \u0026#34;10.252.0.1\u0026#34; }, { \u0026#34;dst\u0026#34;: \u0026#34;10.106.0.0/17\u0026#34;, \u0026#34;gw\u0026#34;: \u0026#34;10.252.0.1\u0026#34; }, { \u0026#34;dst\u0026#34;: \u0026#34;10.104.0.0/17\u0026#34;, \u0026#34;gw\u0026#34;: \u0026#34;10.252.0.1\u0026#34; } ] } }\u0026#39; [...] In this example, Kubernetes will assign UAI IP addresses in the range 10.252.2.10 through 10.252.3.244 on the network attachment, and will permit those UAIs to reach compute nodes on any of four possible NMN subnets:\nDirectly through the NMN subnet hosting the UAI host node itself (10.252.0.0/17 here) Through the gateway in the local NMN subnet (10.252.0.1 here) to: 10.92.100.0/24 10.106.0.0/17 10.104.0.0/17 Top: User Access Service (UAS)\nNext Topic: Configure UAIs in UAS\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/verify-switches_are_forwarding_dhcp_traffic/",
	"title": "Verify the switches are forwarding DHCP traffic",
	"tags": [],
	"description": "",
	"content": "Verify the switches are forwarding DHCP traffic If you made it this far and still cannot pxe boot, you may have run into the IP-Helper breaking on the switch.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/snmp-agent/",
	"title": "Simple Network Management Protocol (SNMP) Agent",
	"tags": [],
	"description": "",
	"content": "Simple Network Management Protocol (SNMP) Agent Simple Network Management Protocol (SNMP) minimizes the number and complexity of management functions. For monitoring and control, it is extensible to accommodate additional, possibly unanticipated aspects of network operation and management. SNMP is universal and independent of the architecture and mechanisms of particular hosts or particular gateways. SNMP server is supported either on the default or mgmt VRF.\nConfiguration Commands Enable SNMP agent:\nswitch(config)# snmp-server vrf VRF Configure the port to which the SNMP agent is bound:\nswitch(config)# snmp-server agent-port PORT Configure an SNMPv2c community name:\nswitch(config)# snmp-server community NAME Configure a SNMPv2c trap receiver host:\nswitch(config)# snmp-server host IP-ADDR \u0026lt;trap|inform\u0026gt; version v2c [community NAME] Show commands to validate functionality:\nswitch# show snmp [agent-port|community|trap|vrf] [vsx-peer] Example Output switch(config)# snmp-server vrf default switch(config)# snmp-server agent-port 10601 switch(config)# snmp-server community public switch(config)# snmp-server host 1.2.3.4 trap version v2c community public switch(config)# snmp-server host 1.2.3.4 inform version v2c community public switch(config)# end switch# show snmp community --------------------- SNMP communities --------------------- Public switch# show snmp vrf SNMP enabled VRF ---------------------------- Default switch# show snmp agent-port SNMP agent port : 10601 switch# show snmp trap ------------------------------------------------------------------------------------------ Host Port Type Version SecName vrf ------------------------------------------------------------------------------------------ 1.2.3.4 162 trap v2c public default 1.2.3.4 162 inform v2c public default Expected Results Administrators can configure the port number The output of all show commands is correct Administrators can connect to the switch from the workstation Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_macvlans_network_attachments/",
	"title": "UAI macvlans Network Attachments",
	"tags": [],
	"description": "",
	"content": "UAI macvlans Network Attachments UAIs need to be able to reach compute nodes across the HPE Cray EX internal networks. When the compute node networks are structured as multiple subnets, this requires routing from the UAIs to those subnets. The default route in a UAI goes to the public network through the Customer Access Network (CAN) so that will not work for reaching compute nodes. To solve this problem, UAS installs Kubernetes Network Attachments within the Kubernetes user namespace. One of these network attachments is used by UAIs.\nThe type of network attachment used on HPE Cray EX hardware for this purpose is a macvlan network attachment, so this is often referred to on HPE Cray EX systems as macvlans. This network attachment integrates the UAI into the HPE Cray EX internal networks on the UAI host node where the UAI is running and assigns the UAI an IP address on the network defined by the network attachment. The network attachment also installs a set of routes in the UAI used to reach the compute nodes in the HPE Cray EX platform.\nWARNING This release sets a route over the NMN by default. In CPE release 22.04, instructions for Workload Managers specify that macvlan be changed to use the high speed network. This was found to have a negative impact on the slingshot fabric, as unknown MAC addresses would result in broadcast traffic. If macvlan is being changed to use the HSN, make sure the CPE instructions specify how to use ipvlan instead of macvlan. In a future CSM release, the network attachment definition will be using ipvlan instead of macvlan to avoid this issue.\nCheck how the network attachment is configured with:\nkubectl describe net-attach-def -n user macvlan-uas-nmn-conf Top: User Access Service (UAS)\nNext Topic: UAI Network Attachment Customization\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/verify_bgp/",
	"title": "Verify BGP",
	"tags": [],
	"description": "",
	"content": "Verify BGP Verify the BGP neighbors are in the established state on BOTH the switches.\nHow to check Aruba BGP status:\nsw-spine-002# show bgp ipv4 u s VRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.3 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.2 65533 45052 45044 02m:02w:02d Established Up 10.252.1.7 65533 78389 90090 02m:02w:02d Established Up 10.252.1.8 65533 78384 90059 02m:02w:02d Established Up 10.252.1.9 65533 78389 90108 02m:02w:02d Established Up Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/snmp-community/",
	"title": "SNMPv2c Community",
	"tags": [],
	"description": "",
	"content": "SNMPv2c Community The switch supports SNMPv2c community-based security for read-only access.\nConfiguration Commands Configure an SNMPv2c community name:\nswitch(config)# snmp-server community NAME Bind the SNMP server to a VRF:\nswitch(config)# snmp-server vrf \u0026lt;default|VRF\u0026gt; Show commands to validate functionality:\nswitch# show snmp community Example Output switch(config)# snmp-server community public switch(config)# snmp-server vrf default switch(config)# end switch# show snmp community --------------------- SNMP communities --------------------- mysnmp switch# show snmp vrf SNMP enabled VRF ---------------------------- default Expected Results Administrators can configure the community name Administrators can bind the SNMP server to the default VRF Administrators can connect from the workstation using the community name Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/uas_limitations/",
	"title": "UAS Limitations",
	"tags": [],
	"description": "",
	"content": "UAS Limitations Functionality that is currently not supported while using UAS.\nFunctionality Not Currently Supported by the User Access Service Lustre (lfs) commands within the UAS service pod Executing Singularity containers within the UAS service Building Docker containers within the UAS environment Building containerd containers within the UAS environment dmesg cannot run inside a UAI because of container security limitations Users cannot SSH from ncn-w001 to a UAI because UAIs use LoadBalancer IP addresses on the Customer Access Network (CAN) instead of NodePorts and the LoadBalancer IP addresses are not accessible from ncn-w001 Other Limitations There is a known issue where X11 traffic may not forward DISPLAY correctly if the user logs into an NCN node before logging into a UAI The cray uas uais commands are not restricted to operating on UAIs owned by the user authenticated with cray auth login Limitations Related To Restarts Changes made to a running UAI will be lost if the UAI is restarted or deleted. The only changes in a UAI that will persist are those written to an externally mounted file system (such as Lustre or NFS). To make changes to the base image for a UAI, see Customize End-User UAI Images.\nA UAI may restart because of an issue on the physical node, scheduled node maintenance, or intentional restarts by a site administrator. In this case, any running processes (such as compiles), Slurm interactive jobs, or changes made to the UAI (such as package installations) are lost. A UAI may also terminate and have to be restarted if its hard timeout is reached while a user is logged in or if its soft timeout is reached while it is idle \u0026ndash; defined as having no logged in user sessions \u0026ndash; or before it becomes idle.\nIf a UAI restarts on a node that was recently rebooted, some of the configured volumes may not be ready and it could appear that content in the UAI is missing. In this case, restart the UAI.\nTop: User Access Service (UAS)\nNext Topic: List UAS Version Information\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/verify_dhcp_traffic_on_workers/",
	"title": "Verify the DHCP traffic on the workers",
	"tags": [],
	"description": "",
	"content": "Verify the DHCP traffic on the workers Example issue: Source address of the DHCP Offer is the MetalLB address of KEA \u0026ldquo;10.92.100.222\u0026rdquo;.\nThe source address of the DHCP Reply/Offer NEEDS to be the address of the vlan interface on the Worker.\nHere is how to look at DHCP traffic on the workers:\nncn-w001# tcpdump -envli bond0 port 67 or 68 You are looking for the source IP address of the DHCP Reply/Offer, this is an example of working offer:\n10.252.1.9.67 \u0026gt; 255.255.255.255.68: BOOTP/DHCP, Reply, length 309, hops 1, xid 0x98b0982e, Flags [Broadcast] Your-IP 10.252.1.17 Server-IP 10.92.100.60 Gateway-IP 10.252.0.1 Client-Ethernet-Address 14:02:ec:d9:79:88 file \u0026#34;ipxe.efi\u0026#34;[|bootp] If the Source IP address of the DHCP Reply/Offer is the MetalLB IP address, the DHCP packet will never make it out of the NCN. An example of this is below. 10.92.100.222.116 \u0026gt; 255.255.255.255.68: BOOTP/DHCP, Reply, length 309, hops 1, xid 0x260ea655, Flags [Broadcast] Your-IP 10.252.1.14 Server-IP 10.92.100.60 Gateway-IP 10.252.0.4 Client-Ethernet-Address 14:02:ec:d9:79:88 file \u0026#34;ipxe.efi\u0026#34;[|bootp] If you run into this, the only solution that we have found so far is restarting KEA and making sure that it gets moved to a different worker.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/snmp_trap/",
	"title": "SNMP traps",
	"tags": [],
	"description": "",
	"content": "SNMP traps The SNMP agent can send trap notifications to a receiver. The receiver\u0026rsquo;s host IP address and port number can be defined along with the notification type, version, and community string.\nConfiguration Commands Configure a SNMPv2c trap receiver host:\nswitch(config)# snmp-server host IP-ADDR trap version v2c community xxx Show commands to validate functionality:\nswitch# show snmp trap Example Output switch# show snmp trap ------------------------------------------------------------------------------------------ Host Port Type Version SecName vrf ------------------------------------------------------------------------------------------ 1.2.3.4 162 trap v1 public 1.2.3.4 162 trap v2c public 1.2.3.4 162 inform v2c public default default default Expected Results Administrators can configure a trap host for your SNMP Manager Administrators can log trap events Administrators can successfully trigger a trap event Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/uas_and_uai_health_checks/",
	"title": "UAS and UAI Legacy Mode Health Checks",
	"tags": [],
	"description": "",
	"content": "UAS and UAI Legacy Mode Health Checks Check the health of UAS and UAI to validate installation / upgrade of an HPE Cray EX system. This is a legacy mode procedure that can be run at installation / upgrade time to make sure that the following are true:\nUAS is installed and running correctly UAI images are installed and registered correctly UAIs can be created in legacy mode Initialize and Authorize the CLI The procedures below use the CLI as an authorized user and run on two separate node types. The first part runs on the LiveCD node while the second part runs on a non-LiveCD Kubernetes master or worker node. When using the CLI on either node, the CLI configuration must be initialized and the user running the procedure must be authorized. This section describes how to initialize the CLI for use by a user and authorize the CLI as a user to run the procedures on any given node. The procedures will need to be repeated in both stages of the validation procedure.\nDiscontinue Use of the CRAY_CREDENTIALS Service Account Token Installation procedures leading up to production mode on Shasta use the CLI with a Kubernetes managed service account normally used for internal operations. There is a procedure for extracting the OAUTH token for this service account and assigning it to the CRAY_CREDENTIALS environment variable to permit simple CLI operations. The UAS / UAI validation procedure runs as a post-installation procedure and requires an actual user with Linux credentials, not this service account. Prior to running any of the steps below you must unset the CRAY_CREDENTIALS environment variable.\nncn-m002# unset CRAY_CREDENTIALS Initialize the CLI Configuration The CLI needs to know what host to use to obtain authorization and what user is requesting authorization so it can obtain an OAUTH token to talk to the API Gateway. This is accomplished by initializing the CLI configuration. This example uses the vers username. In practice, vers and the response to the password: prompt should be replaced with the username and password of the administrator running the validation procedure.\nTo check whether the CLI needs initialization, run the following command.\nncn-m002# cray config describe If the output appears as follows, the CLI requires initialization.\nUsage: cray config describe [OPTIONS] Error: No configuration exists. Run `cray init` If the output appears more like the following, then the CLI is initialized and logged in as vers. If that is the incorrect username, authorize the correct username and password in the next section. If vers is the correct user, proceed to the validation procedure on that node.\nIf the CLI must be initialized again, use the following command and include the correct username, password, and the password response.\nncn-m002# cray init Cray Hostname: api-gw-service-nmn.local Username: vers Password: Success! Initialization complete. Authorize the Correct CLI User If the CLI is initialized but authorized for a user different, run the following command and substitute the correct username and password.\nncn-m002# cray auth login Username: vers Password: Success! Authorization Is Local to a Host: whenever you are using the CLI (cray command) on a host (e.g. a workstation or NCN) where it has not been used before, it is necessary to authenticate on that host using cray auth login. There is no mechanism to distribute CLI authorization amongst hosts.\nTroubleshoot CLI Initialization or Authorization Issues If initialization or authorization fails in any of the preceding steps, there are several common causes.\nDNS failure looking up api-gw-service-nmn.local may be preventing the CLI from reaching the API Gateway and Keycloak for authorization Network connectivity issues with the NMN may be preventing the CLI from reaching the API Gateway and Keycloak for authorization Certificate mismatch or trust issues may be preventing a secure connection to the API Gateway Istio failures may be preventing traffic from reaching Keycloak Keycloak may not yet be set up to authorize the current user While resolving these issues is beyond the scope of this section, adding -vvvvv to the cray auth or cray init commands may offer clues as to why the initialization or authorization is failing.\nValidate the Basic UAS Installation This procedure and the following procedures run on separate nodes on the system and validate the basic UAS installation. Ensure this runs on the LiveCD node and that the CLI is authorized for the user.\nncn-m002# cray uas mgr-info list Example output:\nservice_name = \u0026#34;cray-uas-mgr\u0026#34; version = \u0026#34;1.11.5\u0026#34; ncn-m001-pit# cray uas list Example output:\nresults = [] This shows that UAS is installed and running version 1.11.5 and that no UAIs are running. If another user has been using the UAS, it is possible to see UAIs in the list. That is acceptable from a validation standpoint.\nTo verify that the pre-made UAI images are registered with UAS, run the following command.\nncn-m002# cray uas images list Example output:\ndefault_image = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; image_list = [ \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34;,] The output shows that the pre-made End-User UAI image, cray/cray-uai-sles15sp1:latest, is registered with UAS. This does not necessarily mean this image is installed in the container image registry, but it is configured for use. If other UAI images have been created and registered, they may also appear in the output.\nValidate UAI Creation The following are needed for this procedure:\nMust run on a master or worker node (and not ncn-w001) Must run on the HPE Cray EX system (or from an external host, but the procedure for that is not covered here) Requires that the CLI be initialized and authorized as for the current user Verify that the user account can create a UAI.\nncn-w003# cray uas create --publickey ~/.ssh/id_rsa.pub Example output:\nuai_connect_string = \u0026#34;ssh vers@10.16.234.10\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.16.234.10\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-a00fb46b\u0026#34; uai_status = \u0026#34;Pending\u0026#34; username = \u0026#34;vers\u0026#34; [uai_portmap] The UAI is now created and in the process of initializing and running.\nView the state of the UAI.\nThe following can be repeated as many times as desired. If the results appear like the following, the UAI is ready for use.\nncn-w003# cray uas list Example output:\n[[results]] uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.16.234.10\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.16.234.10\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-a00fb46b\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; Log into the UAI (without a password) as follows:\nSSH to the UAI.\nncn-w003# ssh vers@10.16.234.10 Example output:\nThe authenticity of host \u0026#39;10.16.234.10 (10.16.234.10)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:BifA2Axg5O0Q9wqESkLqK4z/b9e1usiDUZ/puGIFiyk. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;10.16.234.10\u0026#39; (ECDSA) to the list of known hosts. List the processes.\nvers@uai-vers-a00fb46b-6889b666db-4dfvn:~\u0026gt; ps -afe Example output:\nUID PID PPID C STIME TTY TIME CMD root 1 0 0 18:51 ? 00:00:00 /bin/bash /usr/bin/uai-ssh.sh munge 36 1 0 18:51 ? 00:00:00 /usr/sbin/munged root 54 1 0 18:51 ? 00:00:00 su vers -c /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 55 54 0 18:51 ? 00:00:00 /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 62 55 0 18:51 ? 00:00:00 sshd: vers [priv] vers 67 62 0 18:51 ? 00:00:00 sshd: vers@pts/0 vers 68 67 0 18:51 pts/0 00:00:00 -bash vers 120 68 0 18:52 pts/0 00:00:00 ps -afe Exit the connection.\nvers@uai-vers-a00fb46b-6889b666db-4dfvn:~\u0026gt; exit Clean up the UAI and note that the UAI name used is the same as the name in the output from cray uas create above.\nncn-w003# cray uas delete --uai-list uai-vers-a00fb46b In this example, results = [ \u0026quot;Successfully deleted uai-vers-a00fb46b\u0026quot;,] will be returned if successful.\nTroubleshoot UAS and UAI Operations Issues Authorization Issues:\nIf the user is not logged in as a valid Keycloak user or is inadvertently using the CRAY_CREDENTIALS environment variable (i.e. the variable is set if the user is logged in with the their username or another username), the output of running the cray uas list command will produce output like the following.\nncn-w003# cray uas list Usage: cray uas list [OPTIONS] Try \u0026#39;cray uas list --help\u0026#39; for help. Error: Bad Request: Token not valid for UAS. Attributes missing: [\u0026#39;gidNumber\u0026#39;, \u0026#39;loginShell\u0026#39;, \u0026#39;homeDirectory\u0026#39;, \u0026#39;uidNumber\u0026#39;, \u0026#39;name\u0026#39;] Fix this by logging in as a \u0026ldquo;real user\u0026rdquo; (a user with Linux credentials) and ensure that CRAY_CREDENTIALS is unset.\nUAS Cannot Access Keycloak If the output of the cray uas list command appears similar to the following, the wrong hostname to reach the API gateway may be in use. In that case, run the CLI initialization steps again.\nncn-w003# cray uas list Usage: cray uas list [OPTIONS] Try \u0026#39;cray uas list --help\u0026#39; for help. Error: Internal Server Error: An error was encountered while accessing Keycloak There also may be a problem with the Istio service mesh inside of the Shasta system. Troubleshooting this is beyond the scope of this section, but viewing the UAS pod logs in Kubernetes may provide useful information.\nThere are typically two UAS pods. View logs from both pods to identify the specific failure. The logs have a very large number of GET events listed as part of the aliveness checking. The following shows an example of viewing UAS logs (the example shows only one UAS manage, normally there would be two).\nncn-w003# kubectl get po -n services | grep uas-mgr | grep -v etcd Example output:\ncray-uas-mgr-6bbd584ccb-zg8vx 2/2 Running 0 12d ncn-w003# kubectl logs -n services cray-uas-mgr-6bbd584ccb-zg8vx cray-uas-mgr | grep -v \u0026#39;GET \u0026#39; | tail -25 2021-02-08 15:32:41,211 - uas_mgr - INFO - getting deployment uai-vers-87a0ff6e in namespace user 2021-02-08 15:32:41,225 - uas_mgr - INFO - creating deployment uai-vers-87a0ff6e in namespace user 2021-02-08 15:32:41,241 - uas_mgr - INFO - creating the UAI service uai-vers-87a0ff6e-ssh 2021-02-08 15:32:41,241 - uas_mgr - INFO - getting service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:32:41,252 - uas_mgr - INFO - creating service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:32:41,267 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:32:41,360 - uas_mgr - INFO - No start time provided from pod 2021-02-08 15:32:41,361 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 127.0.0.1 - - [08/Feb/2021 15:32:41] \u0026#34;POST /v1/uas?imagename=registry.local%2Fcray%2Fno-image-registered%3Alatest HTTP/1.1\u0026#34; 200 - 2021-02-08 15:32:54,455 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:32:54,455 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:32:54,455 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:32:54,484 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:32:54,596 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:25,053 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:40:25,054 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:40:25,054 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:40:25,085 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:40:25,212 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:51,210 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:40:51,210 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:40:51,210 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:40:51,261 - uas_mgr - INFO - deleting service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:51,291 - uas_mgr - INFO - delete deployment uai-vers-87a0ff6e in namespace user 127.0.0.1 - - [08/Feb/2021 15:40:51] \u0026#34;DELETE /v1/uas?uai_list=uai-vers-87a0ff6e HTTP/1.1\u0026#34; 200 - UAI Images not in Registry If output is similar to the following, the pre-made End-User UAI image is not in the user\u0026rsquo;s local registry (or whatever registry it is being pulled from, see the uai_img value for details). Locate and the image and push / import it to the registry.\nncn-w003# cray uas list Example output:\n[[results]] uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.103.13.172\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.103.13.172\u0026#34; uai_msg = \u0026#34;ErrImagePull\u0026#34; uai_name = \u0026#34;uai-vers-87a0ff6e\u0026#34; uai_status = \u0026#34;Waiting\u0026#34; username = \u0026#34;vers\u0026#34; Missing Volumes and Other Container Startup Issues Various packages install volumes in the UAS configuration. All of those volumes must also have the underlying resources available, sometimes on the host node where the UAI is running and sometimes from within Kubernetes. If the UAI gets stuck with a ContainerCreating uai_msg field for an extended time, this is a likely cause. UAIs run in the user Kubernetes namespace and are pods that can be examined using kubectl describe.\nRun the following command to locate the pod.\nncn-w003# kubectl get po -n user | grep \u0026lt;uai-name\u0026gt; Run the following command to investigate the problem.\nncn-w003# kubectl describe -n user \u0026lt;pod-name\u0026gt; If volumes are missing, they will be in the Events:section of the output. Other problems may show up there as well. The names of the missing volumes or other issues should indicate what needs to be fixed to enable the UAI.\nTop: User Access Service (UAS)\nNext Topic: Troubleshoot UAS Issues\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/verify_route_to_tftp/",
	"title": "Verify route to TFTP",
	"tags": [],
	"description": "",
	"content": "Verify route to TFTP On BOTH Aruba switches we need a single route to the TFTP server 10.92.100.60 (your configuration may differ).\nThis is needed because there are issues with Aruba ECMP hashing and TFTP traffic.\nsw-spine-002# show ip route 10.92.100.60 Displaying ipv4 routes selected for forwarding \u0026#39;[x/y]\u0026#39; denotes [distance/metric] 10.92.100.60/32, vrf default, tag 0 via 10.252.1.9, [70/0], bgp This route can be a static route or a BGP route that is pinned to a single worker. (1.4.2 patch introduces the BGP pinned route) Verify that you can ping the next hop of this route. For example above we would ping 10.252.1.9. If this is not reachable this is your problem. Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/snmpv3_users/",
	"title": "Aruba SNMPv3 Users",
	"tags": [],
	"description": "",
	"content": "Aruba SNMPv3 Users SNMPv3 supports cryptographic security through a combination of authenticating and encrypting the SNMP protocol packets over the network. Read-only access is currently supported. The admin user can add or remove SNMPv3 users.\nConfiguration Commands Configure a new SNMPv3 user (minimum eight characters for passwords):\nswitch(config)# snmpv3 user \u0026lt;USER\u0026gt; auth md5 auth-pass \u0026lt;A-PSWD\u0026gt; priv aes priv-pass \u0026lt;P-PSWD\u0026gt; Remove an SNMPv3 user:\nswitch(config)# no snmpv3 user \u0026lt;USER\u0026gt; Show commands to validate functionality:\nswitch# show snmpv3 users Example Output switch(config)# snmp-server community public switch(config)# snmpv3 context public vrf default community public switch(config)# show snmpv3 context -------------------------------------------------------------------------- Name vrf Community -------------------------------------------------------------------------- public mgmt. public switch(config)# show snmp vrf SNMP enabled VRF ---------------------------- default switch(config)# show snmpv3 users -------------------------------------------------------------------------- User AuthMode PrivMode Context Enabled -------------------------------------------------------------------------- Snmpv3user md5 aes none True Expected Results Administrators can configure the new user Administrators can connect to the server from the workstation Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/update_a_resource_specification/",
	"title": "Update a Resource Specification",
	"tags": [],
	"description": "",
	"content": "Update a Resource Specification Modify a specific UAI resource specification using the resource_id of that specification.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Resource ID of the resource specification to be updated: List UAI Resource Specifications Procedure To modify a particular resource specification, use a command of the following form:\nncn-m001-pit# cray uas admin config resources update [OPTIONS] RESOURCE_ID The [OPTIONS] used by this command are the same options used to create resource specifications. See Create a UAI Resource Specification and Elements of a UAI for a full description of those options.\nUpdate a UAI resource specification.\nThe following example changes the CPU and memory limits on a UAI resource specification to 1 CPU and 1GiB, respectively.\nncn-m001-pit# cray uas admin config resources update \\ --limit \u0026#39;{\u0026#34;cpu\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;1Gi\u0026#34;}\u0026#39; 85645ff3-1ce0-4f49-9c23-05b8a2d31849 The following example does the same for the CPU and memory requests:\nncn-m001-pit# cray uas admin config resources update \\ --request \u0026#39;{\u0026#34;cpu\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;1Gi\u0026#34;}\u0026#39; 85645ff3-1ce0-4f49-9c23-05b8a2d31849 Top: User Access Service (UAS)\nNext Topic: Delete a UAI Resource Specification\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/very_large/",
	"title": "Very Large (Exascale)",
	"tags": [],
	"description": "",
	"content": "Very Large (Exascale) Back to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/spine_leaf_architecture/",
	"title": "Spine-Leaf Architecture",
	"tags": [],
	"description": "",
	"content": "Spine-Leaf Architecture The network design used in majority of HPE Cray EX installations is spine leaf architecture. In more sizeable systems, we also utilize super-spine to accommodate the number of spines that connect the network to provide additional HA capabilities.\nWhat is Spine-Leaf Architecture? A spine-leaf architecture is data center network topology that consists of two switching layers—a spine and leaf. The leaf layer consists of access switches that aggregate traffic from servers and connect directly into the spine or network core. Spine switches interconnect all leaf switches in a full-mesh topology.\nHow Does a Spine-Leaf Architecture Differ from Traditional Network Designs? Traditionally, data center networks were based on a three-tier model:\nAccess switches connect to servers Aggregation or distribution switches provide redundant connections to access switches Core switches provide fast transport between aggregation switches, typically connected in a redundant pair for high availability At the most basic level, a spine-leaf architecture collapses one of these tiers, as depicted in these diagrams.\nOther common differences in spine-leaf topologies include:\nThe removal of Spanning Tree Protocol (STP) where feasible A scale-out vs. scale-up of infrastructure Why are Spine-Leaf Architectures Becoming More Popular? Given the prevalence of cloud and containerized infrastructure in modern data centers, east-west traffic continues to increase. East-west traffic moves laterally, from server to server. This shift is primarily explained by modern applications having components that are distributed across more servers or VMs.\nWith east-west traffic, having low-latency, optimized traffic flows is imperative for performance, especially for time-sensitive or data-intensive applications. A spine-leaf architecture aids this by ensuring traffic is always the same number of hops from its next destination, so latency is lower and predictable.\nCapacity also improves because STP is no longer required or at least the impact zones of STP can be limited to the edge. While STP enables redundant paths between two switches, only one can be active at any time. As a result, paths often become oversubscribed. Conversely, spine-leaf architectures rely on protocols such as Equal-Cost Multipath (ECMPM) routing to load balance traffic across all available paths while still preventing network loops.\nIn addition to higher performance, spine-leaf topologies provide better scalability. Additional spine switches can be added and connected to every leaf, increasing capacity. Likewise, new leaf switches can be seamlessly inserted when port density becomes a problem. In either case, this \u0026ldquo;scale-out\u0026rdquo; of infrastructure does not require any re-architecting of the network, and there is no downtime.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/update_a_uai_image_registration/",
	"title": "Update a UAI Image Registration",
	"tags": [],
	"description": "",
	"content": "Update a UAI Image Registration Modify the UAS registration information of a UAI image.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Image ID of the UAI Image Registration to be updated: List UAI Registered Images Procedure Once a UAI image has been registered, it may be necessary to change its attributes. For example, the default image may need to change.\nModify the registration information of a UAI image by using a command of the form:\nncn-m001-pit# cray uas admin config images update OPTIONS IMAGE_ID Use the --default or --imagename options as specified when registering an image to update those specific elements of an existing image registration. For example, to make the registry.local/cray/custom-end-user-uai:latest image shown in other procedures the default image, use the following command:\nncn-m001-pit# cray uas admin config images update --default yes 8fdf5d4a-c190-24c1-2b96-74ab98c7ec07 Top: User Access Service (UAS)\nNext Topic: Delete a UAI Image Registration\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/vlan/",
	"title": "Virtual local access networks (VLANs)",
	"tags": [],
	"description": "",
	"content": "Virtual local access networks (VLANs) VLANs allow for the logical grouping of switch interfaces, enabling communication as if all connected devices were on the same isolated network.\nRelevant Configuration\nCreate VLAN\nswitch(config)# vlan \u0026lt;VLAN\u0026gt; Configure an interface to associate it with a VLAN\nswitch (config) # interface ethernet 1/22 switch (config interface ethernet 1/22) # From within the interface context, configure the interface mode to Access.\nswitch (config interface ethernet 1/22) # switchport mode access From within the interface context, configure the Access VLAN membership.\nswitch (config interface ethernet 1/22) # switchport access vlan 6 Configure an interface as a trunk port .\nswitch (config) # interface ethernet 1/35 switch (config interface ethernet 1/35) # From within the interface context, configure the interface mode to Trunk.\nswitch (config interface ethernet 1/35) # switchport mode trunk Show Commands to Validate Functionality\nswitch# show vlan [VLAN] Expected Results\nStep 1: You can create a VLAN Step 2: You can assign a VLAN to the physical interface Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/spine_leaf_architecture2/",
	"title": "Spine-leaf Architecture",
	"tags": [],
	"description": "",
	"content": "Spine-leaf Architecture How does a spine-leaf architecture differ from traditional network designs? Traditionally, data center networks were based on a three-tier model:\nAccess switches connect to servers Leaf or distribution switches provide redundant connections to access switches Core switches provide fast transport between leaf switches, typically connected in a redundant pair for high availability At the most basic level, a spine-leaf architecture collapses one of these tiers, as depicted in these diagrams.\nOther common differences in spine-leaf topologies include:\nThe removal of Spanning Tree Protocol (STP) where feasible A scale-out vs. scale-up of infrastructure Back to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/update_a_uas_volume/",
	"title": "Update a UAS Volume",
	"tags": [],
	"description": "",
	"content": "Update a UAS Volume Modify the configuration of an already-registered UAS volume. Almost any part of the configuration of a UAS volume can be modified.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know UAS volume ID of a volume; perform List Volumes Registered in UAS if needed The administrator should be familiar with Add a Volume to UAS; the options and caveats for updating volumes are the same as for creating volumes Procedure Modify the configuration of a UAS volume.\nOnce a UAS volume has been configured, any part of it except for the volume_id can be updated with a command of the following form:\ncray uas admin config volumes update [options] \u0026lt;volume-id\u0026gt; For example:\nncn-m001-pit# cray uas admin config volumes update --volumename \u0026#39;my-example-volume\u0026#39; a0066f48-9867-4155-9268-d001a4430f5c The --volumename, --volume-description, and --mount-path options may be used in any combination to update the configuration of a given volume.\nTop: User Access Service (UAS)\nNext Topic: Delete a Volume Configuration\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/vlan_interface/",
	"title": "VLAN interface",
	"tags": [],
	"description": "",
	"content": "VLAN interface The switch also supports classic L3 VLAN interfaces.\nRelevant Configuration\nConfigure the VLAN\nswitch (config) # vlan 6 switch (config vlan 6) # Create and enable the VLAN interface, and assign it an IP address\nswitch(config vlan 6)# ip address 10.1.0.2/16 Show Commands to Validate Functionality\nswitch# show vlan Expected Results\nStep 1: You can configure the VLAN Step 2: You can enable the interface and associate it with the VLAN Step 3: You can create an IP-enabled VLAN interface, and it is up Step 4: You validate the configuration is correct Step 5: You can ping from the switch to the client and from the client to the switch Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/ssh/",
	"title": "Secure Shell (SSH)",
	"tags": [],
	"description": "",
	"content": "Secure Shell (SSH) SSH server enables an SSH client to make a secure and encrypted connection to a switch. Currently, switch supports SSH version 2.0 only. The user authentication mechanisms supported for SSH are public key authentication and password authentication (RADIUS, TACACS+ or locally stored password). Secure File Transfer Protocol (SFTP) provides file transfer. SSH Server and sftp-client via the copy command are supported for managing the router.\nConfiguration Commands Configure SSH authentication:\nswitch(config)# ssh password-authentication Generate SSH server key:\nswitch(config)# ssh host-key \u0026lt;rsa [bits 2048]|ecdsa CURVE|ed25519\u0026gt; Enable SSH on the VRF:\nswitch(config)# ssh server vrf \u0026lt;default|mgmt|VRF\u0026gt; Configure SSH options:\nswitch(config)# ssh certified-algorithms-only switch(config)# ssh maximum-auth-attempts VALUE switch(config)# ssh known-host remove \u0026lt;all|IP-ADDR\u0026gt; Show commands to validate functionality:\nswitch# show ssh server [vrf VRF|all-vrfs] Example Output switch# show ssh server all-vrfs SSH server configuration on VRF vrf_default : IP Version TCP Port Host-keys : IPv4 and IPv6 SSH Version : 22 Grace Timeout (sec) : 120 : ECDSA, ED25519, RSA Ciphers : chacha20-poly1305@openssh.com, aes128-ctr,aes192-ctr,aes256-ctr, aes128-gcm@openssh.com,aes256-gcm@openssh.com MACs : umac-64-etm@openssh.com,umac-128-etm@openssh.com, hmac-sha2-256-etm@openssh.com, hmac-sha2-512-etm@openssh.com, hmac-sha1-etm@openssh.com, umac-64@openssh.com, umac-128@openssh.com, hmac-sha2-256,hmac-sha2-512,hmac-sha1 Expected Results Administrators can create the user account Administrators can generate working SSH keys The output of the show commands is correct Administrators can successfully connect to the switch via an SSH client using SSH 2.0 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/view_a_uai_class/",
	"title": "View a UAI Class",
	"tags": [],
	"description": "",
	"content": "View a UAI Class Display all the information for a specific UAI class by referencing its class ID.\nPrerequisites The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Class ID of a UAI class: List Available UAI Classes Procedure View all the information about a specific UAI class.\nTo examine an existing UAI class, use a command of the following form:\nncn-m001-pit# cray uas admin config classes describe \u0026lt;class-id\u0026gt; The following example uses the --format yaml option to display the UAI class configuration in YAML format. Replace yaml with json to return JSON-formatted output. Omitting the --format option displays the UAI class in the default TOML format.\nReplace bb28a35a-6cbc-4c30-84b0-6050314af76b in the example command with the ID of the UAI class to be examined.\nncn-m001-pit# cray uas admin config classes describe bdb4988b-c061-48fa-a005-34f8571b88b4 --format yaml Example output:\nclass_id: bdb4988b-c061-48fa-a005-34f8571b88b4 comment: UAI Class to Create Brokered End-User UAIs default: false image_id: 1996c7f7-ca45-4588-bc41-0422fe2a1c3d namespace: user opt_ports: [] priority_class_name: uai-priority public_ip: false replicas: 3 resource_config: comment: Resource Specification to use with Brokered End-User UAIs limit: \u0026#39;{\u0026#34;cpu\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;1Gi\u0026#34;}\u0026#39; request: \u0026#39;{\u0026#34;cpu\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;1Gi\u0026#34;}\u0026#39; resource_id: f26ee12c-6215-4ad1-a15e-efe4232f45e6 resource_id: f26ee12c-6215-4ad1-a15e-efe4232f45e6 service_account: timeout: hard: \u0026#39;86400\u0026#39; soft: \u0026#39;1800\u0026#39; warning: \u0026#39;60\u0026#39; tolerations: uai_compute_network: true uai_creation_class: uai_image: default: true image_id: 1996c7f7-ca45-4588-bc41-0422fe2a1c3d imagename: registry.local/cray/cray-uai-sles15sp2:1.2.4 volume_list: - 11a4a22a-9644-4529-9434-d296eef2dc48 - a3b149fd-c477-41f0-8f8d-bfcee87fdd0a volume_mounts: - mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: 11a4a22a-9644-4529-9434-d296eef2dc48 volumename: timezone - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: a3b149fd-c477-41f0-8f8d-bfcee87fdd0a volumename: lustre Refer to UAI Classes and Elements of a UAI for an explanation of the output of this command.\nTop: User Access Service (UAS)\nNext Topic Modify a UAI Class\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/vlan_trunking_8021q/",
	"title": "VLAN trunking 802.1Q",
	"tags": [],
	"description": "",
	"content": "VLAN trunking 802.1Q A trunk port carries packets on one or more VLANs specified. Packet that ingress on a trunk port are in the VLAN specified in its 802.1Q header, or native VLAN if the packet has no 802.1Q header. A packet that egresses through a trunk port will have an 802.1Q header if it has a nonzero VLAN ID. Any packet that ingresses on a trunk port tagged with a VLAN that the port does not trunk is dropped.\nRelevant Configuration\nCreate a VLAN:\nswitch (config) # vlan 100 switch (config vlan 100) # Exit config mode:\nswitch (config vlan 100) # exit switch (config) # Enter the interface configuration mode:\nswitch (config) # interface ethernet 1/35 switch (config interface ethernet 1/35) # From within the interface context, configure the interface mode to Hybrid:\nswitch (config interface ethernet 1/35) # switchport mode hybrid From within the interface context, configure the allowed VLAN membership:\nswitch (config interface ethernet 1/35) # switchport hybrid allowed-vlan add 100 Expected Results\nStep 1: You can create and enable multiple VLAN interfaces Step 2: You can assign the trunk VLAN interfaces Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/static_routing/",
	"title": "Static Routing",
	"tags": [],
	"description": "",
	"content": "Static Routing \u0026ldquo;Static routing is manually performed by the network administrator. The administrator is responsible for discovering and propagating routes through the network. These definitions are manually programmed in every routing device in the environment. After a device has been configured, it simply forwards packets out the predetermined ports. There is no communication between routers regarding the current topology of the network.\u0026rdquo; –IBM Redbook, TCP/IP Tutorial and Technical Overview\nConfiguration Commands switch(config)# \u0026lt;ip|ipv6\u0026gt; route IP-ADDR/\u0026lt;SUBNET|PREFIX\u0026gt; IP-ADDR Show commands to validate functionality:\nswitch# show \u0026lt;ip|ipv6\u0026gt; route [static] Example Output switch# show ip route Displaying ipv4 routes selected for forwarding \u0026#39;[x/y]\u0026#39; denotes [distance/metric]30.0.0.0/30, 1 (null) next-hops via 1/1/3, [0/0], connected, vrf vrf_default 40.0.0.0/24, 1 (null) next-hops via 30.0.0.2, [1/0], static, vrf vrf_default switch# show ip route static Displaying ipv4 routes selected for forwarding \u0026#39;[x/y]\u0026#39; denotes [distance/metric] 40.0.0.0/24, 1 (null) next-hops via 30.0.0.2, [1/0], static, vrf vrf_default switch# show ipv6 route Displaying ipv6 routes selected for forwarding \u0026#39;[x/y]\u0026#39; denotes [distance/metric] 2001:10::/64, 1 (null) next-hops via 1/1/1, [0/0], connected, vrf default 2001:30::/64, 1 (null) next-hops via 2001:10::2, [1/0], static, vrf default switch# show ipv6 route static Displaying ipv6 routes selected for forwarding \u0026#39;[x/y]\u0026#39; denotes [distance/metric] 2001:30::/64, 1 (null) next-hops via 2001:10::2, [1/0], static, vrf default Expected Results Administrators can configure a static route on the DUT Administrators can validate using the show command(s) Administrators can ping the connected device Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/volumes/",
	"title": "Volumes",
	"tags": [],
	"description": "",
	"content": "Volumes Volumes provide a way to connect UAIs to external data, whether they be Kubernetes managed objects, external file systems or files, host node files and directories, or remote networked data to be used within the UAI.\nThe following are examples of how volumes are commonly used by UAIs:\nTo connect UAIs to configuration files like /etc/localtime maintained by the host node To connect End-User UAIs to Slurm or PBS Professional Workload Manager configuration shared through Kubernetes To connect End-User UAIs to Programming Environment libraries and tools hosted on the UAI host nodes To connect End-User UAIs to Lustre or other external storage for user data To connect Broker UAIs to a directory service (see Configure a Broker UAI Class) or SSH configuration (see Customize the Broker UAI Image) needed to authenticate and redirect user sessions Any kind of volume recognized by the Kubernetes installation can be installed as a volume within UAS. In the case of Legacy Mode UAI creation without a default UAI Class, all configured volumes are used when creating UAIs. This can be controlled more precisely by defining and using UAI Classes. There is more information on Kubernetes volumes in the Kubernetes Documentation.\nNOTE: As with UAI images, registering a volume with UAS creates the configuration that will be used to create a UAI. If the underlying object referred to by the volume does not exist at the time the UAI is created, the UAI will, in most cases, wait until the object becomes available before starting up. This will be visible in the UAI state which will eventually move to waiting.\nTop: User Access Service (UAS)\nNext Topic: List Volumes Registered in UAS\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/mellanox/web-ui/",
	"title": "Web user interface (WebUI)",
	"tags": [],
	"description": "",
	"content": "Web user interface (WebUI) A web-based management user interface provides a visual representation of a subset of the current switch configuration and states. The Web-UI allows for easy access from modern browsers to modify some aspects of the configuration.\nRelevant Configuration\nEnable the WebUI\nswitch(config)# web enable Configure REST API\nswitch(config)# web enable http|https Show Commands to Validate Functionality\nswitch# show web Expected Results\nStep 1: You can connect the management interface to a private network Step 2: You can enable web-management Step 3: You can connect to the IP address from a browser login to the management menu Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/status_of_cray-dhcp-kea_pods/",
	"title": "Confirm the Status of the cray-dhcp-kea Pods",
	"tags": [],
	"description": "",
	"content": "Confirm the Status of the cray-dhcp-kea Pods Use this procedure to verify the status of the cray-dhcp-kea pods and services. The commands in this procedure must be run on ncn-w001 or a worker/manager NCN with kubectl installed.\nProcedure Check if the Kea DHCP services are running.\nncn# kubectl get -n services pods | grep kea The following services should be returned as output:\ncray-dhcp-kea-api Cluster IP 10.31.247.201 \u0026lt;none\u0026gt; 8000/TCP 3h36m cray-dhcp-kea-tcp-hmn LoadBalancer 10.25.109.178 10.94.100.222 67:30833/TCP 3h36m cray-dhcp-kea-tcp-nmn LoadBalancer 10.21.240.208 10.92.100.222 67:31915/TCP 3h36m cray-dhcp-kea-udp-hmn LoadBalancer 10.20.37.60 10.94.100.222 67:30357/UDP 3h36m cray-dhcp-kea-udp-nmn LoadBalancer 10.24.246.19 10.92.100.222 67:32188/UDP 3h36m View the Kea pods.\nncn# kubectl get pods -n services -o wide | grep kea A list of the following pods will be returned as output:\ncray-dhcp-kea-788b4c899b-x6ltd 3/3 Running 0 36h 10.40.3.183 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-dhcp-kea-postgres-0 2/2 Running 0 5d23h 10.40.3.121 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-dhcp-kea-postgres-1 2/2 Running 0 5d23h 10.42.2.181 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-dhcp-kea-postgres-2 2/2 Running 0 5d23h 10.39.0.208 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; This output will also show which worker node the kea-dhcp pod is currently on.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/uas_user_and_admin_topics/",
	"title": "User Access Service (UAS)",
	"tags": [],
	"description": "",
	"content": "User Access Service (UAS) The User Access Service (UAS) is a service that manages User Access Instances (UAIs) which are containerized services under Kubernetes that provide application developers and users with a lightweight login environment in which to create and run user applications. UAIs run on non-compute nodes (NCN), specifically Kubernetes Worker nodes.\nAt a high level, there are two ways to configure UAS with respect to allowing users access to UAIs. The standard configuration involves the use of Broker UAIs through which users establish SSH login sessions. When a login session is established to a Broker UAI the Broker UAI either locates or creates a new UAI on behalf of the user and forwards the user\u0026rsquo;s SSH connection to that UAI. A legacy configuration requires users to create their own UAIs through the cray CLI. Once a UAI is created in this way, the users can use SSH to log into the UAI directly. The legacy configuration will soon be deprecated. Sites using it should migrate to the Broker UAI based configuration.\nOnce logged into a UAI, users can use most of the facilities found on a User Access Node (UAN) with certain limitations. Users can also use UAIs to transfer data between the Cray system and external systems.\nBy default, the timezone inside the UAI container is configured to match the timezone on the host NCN on which it is running, For example, if the timezone on the host NCN is set to CDT, the UAIs on that host will also be set to CDT.\nComponent Function/Description User Access Instance (UAI) An instance of UAS container. cray-uas-mgr Manages UAI life cycles. Container Element Components Operating system SLES15 SP2 kubectl command Utility to interact with Kubernetes. cray command Command that allows users to create, describe, and delete UAIs. Administrative users use cray uas admin uais list to list the following parameters for all existing UAIs:\nNOTE: The example values below are used throughout the UAS procedures. They are used as examples only. Users should substitute with site-specific values.\nParameter Description Example value uai_connect_string The UAI connection string ssh user@203.0.113.0 -i ~/.ssh/id\\_rsa uai_img The UAI image ID registry.local/cray/cray-uas-sles15sp1-slurm:latest uai_name The UAI name uai-user-be3a6770 uai_status The state of the UAI. Running: Ready username The user who created the UAI. user uai_age The age of the UAI. 11m uai_host The node hosting the UAI. ncn-w001 Authorized users in Legacy UAI Management use cray uas list to see the same information on all existing UAIs owned by the user (if any).\nGetting Started UAS is highly configurable and it is recommended that administrators familiarize themselves with, at least, the major concepts covered in the Table of Contents below before allowing users to use UAIs. In particular, the concepts of End-User UAIs and Broker UAIs, and the procedures for setting up and customizing Broker UAIs are critical to setting up UAS properly.\nAnother important topic, once administrators are familiar with setting up UAS to provide basic UAIs, is customizing the UAI image to support user workflows. At the simplest level, administrators will want to create and use a UAI image that matches the booted compute nodes. This can be done by following the Customize End-User UAI Images procedure.\nTable of Contents UAS Limitations List UAS Version Information End-User UAIs Special Purpose UAIs Elements of a UAI UAI Host Nodes UAI Host Node Selection UAI macvlans Network Attachments UAI Network Attachment Customization Configure UAIs in UAS UAI Images Listing Registered UAI Images Register a UAI Image Retrieve UAI Image Registration Information Update a UAI Image Registration Delete a UAI Image Registration Volumes List Volumes Registered in UAS Add a Volume to UAS Obtain Configuration of a UAS Volume Update a UAS Volume Delete a Volume Configuration Resource Specifications List UAI Resource Specifications Create a UAI Resource Specification Retrieve Resource Specification Details Update a Resource Specification Delete a UAI Resource Specification UAI Classes List Available UAI Classes Create a UAI Class View a UAI Class Modify a UAI Class Delete a UAI Class UAI Management List UAIs Creating a UAI Examining a UAI Using a Direct Administrative Command Deleting a UAI Common UAI Configurations Choosing UAI Resource Settings Setting End-User UAI Timeouts Broker UAI Resiliency and Load Balancing Broker Mode UAI Management Configure End-User UAI Classes for Broker Mode Configure a Broker UAI class Start a Broker UAI Log in to a Broker UAI UAI Image Customization Customize the Broker UAI Image Customize End-User UAI Images Legacy Mode User-Driven UAI Management Configure A Default UAI Class for Legacy Mode Create and Use Default UAIs in Legacy Mode List Available UAI Images in Legacy Mode Create UAIs From Specific UAI Images in Legacy Mode UAS and UAI Legacy Mode Health Checks Troubleshoot UAS Issues Troubleshoot UAS by Viewing Log Output Troubleshoot UAIs by Viewing Log Output Troubleshoot Stale Brokered UAIs Troubleshoot UAI Stuck in ContainerCreating Troubleshoot Duplicate Mount Paths in a UAI Troubleshoot Missing or Incorrect UAI Images Troubleshoot UAIs with Administrative Access Troubleshoot Common Mistakes when Creating a Custom End-User UAI Image Troubleshoot UAS / CLI Authentication Issues Troubleshoot Broker UAI SSSD Cannot Use /etc/sssd/sssd.conf Clear UAS Configuration Next Topic: UAS Limitations\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/network_tests/",
	"title": "Network Tests",
	"tags": [],
	"description": "",
	"content": "Network Tests The CSM Automatic Network Utility (CANU) has the ability to run tests against the management network.\nIf doing a CSM install or upgrade, a CANU RPM is located in the release tarball. For more information, refer to the Update CANU From CSM Tarball procedure.\nThe switch inventory is dynamically created from either a System Layout Service (SLS) file --sls-file, or it will automatically query the SLS API if an SLS file is not specified.\nSee also:\nCANU Aruba test suite Test The Network with CANU Prerequisites SSH access to the switches SLS file or SLS API access CANU has to be on version 1.1.4 or later; run canu --version to verify Examples Pulling switch inventory from SLS and logging to screen, this requires the API gateway to be up.\nncn# canu test --log Pulling switch inventory from SLS file and connecting to the switches via their CMN IP addresses, this can be done outside the Shasta cluster.\nncn# canu test --sls-file ../Hela/sls_input_file.json --network CMN Pulling switch inventory from SLS and having the output be in JSON format.\nncn# canu test --json Running the tests can take some time if there are a lot of management switches.\nThe output will look similar to the following:\n+----+-----------------+----------------------------------------------+----------+----------------------+ | 46 | sw-spine-002 | Software version test | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 47 | sw-spine-002 | lacp interfaces test | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 48 | sw-spine-002 | Interface error check | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 49 | sw-spine-002 | running-config different from startup-config | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 50 | sw-spine-002 | STP check for blocked ports | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 51 | sw-spine-002 | CPU Utilization over 70% | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 52 | sw-spine-002 | Memory Utilization over 70% | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 53 | sw-spine-002 | vlan 1 ip-helper test | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 54 | sw-spine-002 | vlan 2 ip-helper test | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 55 | sw-spine-002 | vlan 4 ip-helper test | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 56 | sw-spine-002 | vlan 7 ip-helper test | FAIL | IP-Helper is missing | +----+-----------------+----------------------------------------------+----------+----------------------+ | 57 | sw-spine-002 | tftp route | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 58 | sw-spine-002 | BGP Test | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 59 | sw-spine-002 | STP check for root bridge spine | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 60 | sw-leaf-bmc-001 | Software version test | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 61 | sw-leaf-bmc-001 | lacp interfaces test | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 62 | sw-leaf-bmc-001 | Interface error check | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 63 | sw-leaf-bmc-001 | running-config different from startup-config | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 64 | sw-leaf-bmc-001 | STP check for blocked ports | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 65 | sw-leaf-bmc-001 | CPU Utilization over 70% | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 66 | sw-leaf-bmc-001 | Memory Utilization over 70% | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ | 67 | sw-leaf-bmc-001 | STP check for root bridge leaf | PASS | | +----+-----------------+----------------------------------------------+----------+----------------------+ "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/tacacs/",
	"title": "TACACS",
	"tags": [],
	"description": "",
	"content": "TACACS \u0026ldquo;TACACS+ provides access control for routers, network access servers and other networked computing devices via one or more centralized servers. TACACS+ provides separate authentication, authorization and accounting services.\u0026rdquo; –ietf draft-grant-tacacs-02\nConfiguration Commands Configure TACACS:\nswitch(config)# tacacs-server host IP-ADDR [key \u0026lt;plain|cipher\u0026gt;text KEY] Depending on the TACACS server, change the auth-type from PAP to CHAP:\nswitch(config)# tacacs-server auth-type [pap|chap] Configure AAA:\nswitch(config)# aaa authentication login default group tacacs local switch(config)# aaa authorization commands default group tacacs switch(config)# aaa accounting all default start-stop group tacacs Show commands to validate functionality:\nswitch# show tacacs-server [detail] Expected Results SSH is enabled Administrators can configure TACACS between the server and the DUT correctly The key on the DUT matches the key on the server Administrators have a valid and working user account in the TACACS configuration file on the server Administrators can validate the configuration using the show command listed above Administrators can log into the switch via SSH from the client, and the CLI available to you is unrestricted Administrators can see the start-stop logs in the logfile of the TACACS server Administrators can log into the switch via SSH from the client, but the CLI available to you is restricted Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/reinstall/",
	"title": "Reinstall",
	"tags": [],
	"description": "",
	"content": "Reinstall Reinstall the same CSM version.\nBefore continuing with install, make sure that CANU is running the most current version:\nInstall/Upgrade CANU\nCAUTION: All of these steps should be done using an out-of-band connection. This process is disruptive and will require downtime.\nProcedure If the switches being reinstalled are already in the right CSM version, no configuration changes should be required.\nCheck the differences between generated configurations and the configurations on the system.\nRefer to Validate switch configurations.\nRun a suite of tests against the management network switches.\nRefer to Network tests.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/test_tftp_traffic/",
	"title": "Test TFTP Traffic (Aruba Only)",
	"tags": [],
	"description": "",
	"content": "Test TFTP Traffic (Aruba Only) TFTP traffic can be tested by attempting to download the ipxe.efi binary.\nLog into the leaf switch and try to download the iPXE binary.\nThis requires that the leaf switch can talk to the TFTP server \u0026ldquo;10.92.100.60\u0026rdquo;.\nsw-leaf-001# start-shell sw-leaf-001:~$ sudo su sw-leaf-001:/home/admin# tftp 10.92.100.60 tftp\u0026gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds tftp\u0026gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds tftp\u0026gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds The ipxe.efi binary is downloaded three times in a row in this example.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/replace_switch/",
	"title": "Replace Switch",
	"tags": [],
	"description": "",
	"content": "Replace Switch CAUTION: Do not plug in a switch that is not configured. This can cause unpredictable behavior and network outages.\nPrerequisites Out-of-band access to the switches (console). GA generated switch configuration or backed-up switch configuration exists. Generate Switch Configurations Configuration Management Procedure The following steps are required to replace a switch.\nUpdate firmware on new switch.\nSee Update Management Network Firmware.\nApply the configuration.\nSee Apply Switch Configurations.\nUnplug all the network and power cables and remove the failed switch.\nPlug in the network cables.\nPlug in the power cables.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/typical_vsx_configuration/",
	"title": "Typical Configuration of VSX",
	"tags": [],
	"description": "",
	"content": "Typical Configuration of VSX The following is a very basic VSX configuration between two spine switches. Do note that the inter-switch-link (ISL) between the two spine switches is configured as regular lag, not a multi-chassis lag like a connected server would.\nSpine-01 vrf keepalive interface lag 254 no shutdown description ISL link no routing vlan trunk native 1 tag vlan trunk allowed all lacp mode active\ninterface 1/1/51 no shutdown mtu 9198 lag 254 interface 1/1/52 no shutdown mtu 9198 lag 254\nvsx system-mac 02:01:00:00:01:00 inter-switch-link lag 254 role primary keepalive peer 192.168.255.1 source 192.168.255.0 vrf keepalive linkup-delay-timer 600 vsx-sync vsx-global\ninterface 1/1/47 no shutdown mtu 9198 vrf attach keepalive description VSX keepalive ip address 192.168.255.0/31\nSpine-02 vrf keepalive interface lag 254 no shutdown description ISL link no routing vlan trunk native 1 tag vlan trunk allowed all lacp mode active\ninterface 1/1/51 no shutdown mtu 9198 lag 254 interface 1/1/52 no shutdown mtu 9198 lag 254\nvsx system-mac 02:01:00:00:01:00 inter-switch-link lag 254 role secondary keepalive peer 192.168.255.0 source 192.168.255.1 vrf keepalive linkup-delay-timer 600 vsx-sync vsx-global\ninterface 1/1/47 no shutdown mtu 9198 vrf attach keepalive description VSX keepalive ip address 192.168.255.1/31\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/saving_config/",
	"title": "Save a Configuration",
	"tags": [],
	"description": "",
	"content": "Save a Configuration Write Memory To keep track of what configuration version is running on the switch, create a new configuration file using the CSN version and the CANU version from the MOTD banner from the running config.\nMellanox Get the CSM and CANU versions from the MOTD banner.\nsw-spine-001 [mlag-domain: master] (config) # show banner Example output:\nBanners: Message of the Day (MOTD): ############################################################################### # CSM version: 1.0 # CANU version: 1.1.11 ############################################################################### Save a configuration file with the CSM and CANU versions.\nsw-spine-001 [mlag-domain: master] (config) # configuration write to csm1.0-canu1.1.11 Dell Get the CSM and CANU version from the MOTD banner.\nsw-leaf-bmc-001# show running-configuration | grep motd Example output:\nbanner motd ^C ############################################################################### # CSM version: 1.0 # CANU version: 1.1.11 ############################################################################### Create a configuration file with the CSM/CANU versions.\nsw-leaf-bmc-001(config)# copy config://startup.xml config://csm1.0-canu1.1.11 Copy completed will be returned if successful.\nAruba Get the CSM and CANU versions from the EXEC banner.\nsw-leaf-bmc-001(config)# show banner exec Example output:\n############################################################################### # CSM version: 1.2 # CANU version: 1.1.11 ############################################################################### Create a checkpoint with the CSM/CANU versions.\nsw-leaf-bmc-001(config)# copy running-config checkpoint CSM1_2_CANU_1_1_11 "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/typical_edge_port_configuration/",
	"title": "Typical Edge Port Configuration",
	"tags": [],
	"description": "",
	"content": "Typical Edge Port Configuration The following is a very basic configuration for devices that are single homed to the network. For instance, network ILO cards, BMCs, PDUs, and so on.\nLeaf-01 interface 1/1/47 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge Leaf-02 interface 1/1/47 no shutdown mtu 9198 description BMC no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/snmp_exporter_configs/",
	"title": "Prometheus SNMP Exporter",
	"tags": [],
	"description": "",
	"content": "Prometheus SNMP Exporter The Prometheus SNMP Exporter is deployed by the cray-sysmgmt-health chart to the sysmgmt-health namespace as part of the Cray System Management (CSM) release.\nConfiguration In order to provide data to the Grafana SNMP dashboards, the SNMP Exporter must be configured with a list of management network switches to scrape metrics from.\nThis procedure assumes that this is being done as part of a CSM install as part of the Prepare site-init procedure. Specifically, it assumes that the SYSTEM_NAME and PITDATA variables are set, and that the PITDATA mount is in place.\nObtain the list of switches to use as targets using CSM Automatic Network Utility (CANU).\nlinux# canu init --sls-file ${PITDATA}/prep/${SYSTEM_NAME}/sls_input_file.json --out - Expected output looks similar to the following:\n10.252.0.2 10.252.0.3 10.252.0.4 10.252.0.5 4 IP addresses saved to \u0026lt;stdout\u0026gt; Update customizations.yaml with the list of switches.\nlinux# yq write -s - -i ${PITDATA}/prep/site-init/customizations.yaml \u0026lt;\u0026lt;EOF - command: update path: spec.kubernetes.services.cray-sysmgmt-health.prometheus-snmp-exporter value: serviceMonitor: enabled: true params: enabled: true conf: module: - if_mib target: - 127.0.0.1 - 10.252.0.2 - 10.252.0.3 - 10.252.0.4 - 10.252.0.5 EOF Review the SNMP Exporter configuration.\nlinux# yq r ${PITDATA}/prep/site-init/customizations.yaml spec.kubernetes.services.cray-sysmgmt-health.prometheus-snmp-exporter The expected output looks similar to:\nserviceMonitor: enabled: true params: enabled: true conf: module: - if_mib target: - 127.0.0.1 - 10.252.0.2 - 10.252.0.3 - 10.252.0.4 - 10.252.0.5 The most common configuration parameters are specified in the following table. They must be set in the customizations.yaml file under the spec.kubernetes.services.cray-sysmgmt-health.prometheus-snmp-exporter service definition.\nCustomization Default Description serviceMonitor.enabled true Enables serviceMonitor for SNMP Exporter (default chart value is true) params.enabled false Sets the SNMP Exporter params change to true (default chart value is false) params.conf.module if_mib SNMP Exporter to select which module (default chart value is if_mib) params.conf.target 127.0.0.1 Add list of switch targets to SNMP Exporter to monitor (default chart value is 127.0.0.1) For a complete set of available parameters, consult the values.yaml file for the cray-sysmgmt-health chart.\nConfiguration after CSM install This procedure is to correct the SNMP exporter settings once the PIT node no longer exists by editing manifest and deploying cray-sysmgmt-health chart.\nGet the current cached customizations.\nncn-mw# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; customizations.yaml Get the current cached platform manifest.\nncn-mw# kubectl get cm -n loftsman loftsman-platform -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; \u0026gt; platform.yaml Edit the customizations as desired by adding or updating spec.kubernetes.services.cray-sysmgmt-health.prometheus-snmp-exporter.\nncn-mw# yq write -s - -i /root/customizations.yaml \u0026lt;\u0026lt;EOF - command: update path: spec.kubernetes.services.cray-sysmgmt-health.prometheus-snmp-exporter value: serviceMonitor: enabled: true params: enabled: true conf: module: - if_mib target: - 127.0.0.1s - 10.252.0.2 - 10.252.0.3 - 10.252.0.4 - 10.252.0.5 EOF Check that the customization file has been updated.\nncn-mw# yq read customizations.yaml \u0026#34;spec.kubernetes.services.cray-sysmgmt-health.prometheus-snmp-exporter\u0026#34; Example output:\nserviceMonitor: enabled: true params: enabled: true conf: module: - if_mib target: - 127.0.0.1 - 10.252.0.2 - 10.252.0.3 - 10.252.0.4 - 10.252.0.5 Edit the platform.yaml to only include the cray-sysmgmt-health chart and all its current data.\nThe resources specified above will be updated in the next step. The version may differ, because this is an example.\napiVersion: manifests/v1beta1 metadata: name: platform spec: charts: - name: cray-sysmgmt-health namespace: sysmgmt-health values: . . . version: 0.12.0 Generate the manifest that will be used to redeploy the chart with the modified resources.\nncn-mw# manifestgen -c customizations.yaml -i platform.yaml -o manifest.yaml Check that the manifest file contains the desired resource settings.\nncn-mw# yq read manifest.yaml \u0026#39;spec.charts.(name==cray-sysmgmt-health).values.prometheus-snmp-exporter\u0026#39; Example output:\nserviceMonitor: enabled: true params: enabled: true conf: module: - if_mib target: - 127.0.0.1 - 10.252.0.2 - 10.252.0.3 - 10.252.0.4 - 10.252.0.5 Redeploy the same chart version but with the desired SNMP configuration settings.\nncn-mw# loftsman ship charts-path /helm --manifest-path /root/manifest.yaml Verify that the pod restarts and that the desired resources have been applied.\nWatch the cray-sysmgmt-health-prometheus-snmp-exporter-* pod restart.\nncn-mw# watch \u0026#34;kubectl get pods -n sysmgmt-health -l app.kubernetes.io/name=prometheus-snmp-exporter\u0026#34; It may take about 10 minutes for the cray-sysmgmt-health-prometheus-snmp-exporter-* pod to terminate. It can be forced deleted if it remains in the terminating state:\nncn-mw# kubectl delete pod cray-sysmgmt-health-prometheus-snmp-exporter-* --force --grace-period=0 -n sysmgmt-health Store the modified customizations.yaml file in the site-init repository in the customer-managed location.\nThis step is critical. If this is not done, then these changes will not persist in future installs or upgrades.\nncn-mw# kubectl delete secret -n loftsman site-init ncn-mw# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml Verify that the resource changes are in place.\nncn-mw# kubectl get servicemonitor cray-sysmgmt-health-prometheus-snmp-exporter -n sysmgmt-health -o json | jq -r \u0026#39;.spec.endpoints[].params\u0026#39; Example output:\n{ \u0026#34;module\u0026#34;: [ \u0026#34;if_mib\u0026#34; ], \u0026#34;target\u0026#34;: [ \u0026#34;10.254.0.2\u0026#34; ] } "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/typical_mclag_port_configuration/",
	"title": "Typical Configuration of MCLAG Link",
	"tags": [],
	"description": "",
	"content": "Typical Configuration of MCLAG Link The following is a very basic MCLAG link configuration connecting to NCNs. An administrators configuration may differ.\nNOTE: The multi-chassis definition after the interface lag xx command. This is what defines the LAG to be able to peer both to Spine-01 and Spine-02.\nSpine-01\ninterface lag 1 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/1 no shutdown mtu 9198 lag 1 Spine-02\ninterface lag 1 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/1 no shutdown mtu 9198 lag 1 Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/upgrade/",
	"title": "Upgrade Switches From 1.0 to 1.2 Preconfig",
	"tags": [],
	"description": "",
	"content": "Upgrade Switches From 1.0 to 1.2 Preconfig Use the following procedure to upgrade switches from 1.0 to 1.2 preconfig.\nTo check if the management network is using generated switch configurations, log onto a management switch and check for a banner with a CANU version. This indicates the switch configuration has been generated.\n############################################################################### # CSM version: 1.0 # CANU version: 1.1.10 ############################################################################### To upgrade from CSM 1.2 preconfig (switch configurations generated) to CSM 1.2, use the Management Network 1.0 (1.2 Preconfig) to 1.2 procedure.\nIf the configurations have not been generated, follow the procedure below.\nProcedure CAUTION: All of these steps should be done using an out-of-band connection. This process is disruptive and will require downtime.\nCAUTION: This procedure must be done in coordination with the CSM network team.\nCollect system data.\nRefer to Collect data.\nUpgrade switch firmware to specified firmware version.\nRefer to Update Management Network Firmware.\nIf the system had a previous version of CSM on it, you need to backup all custom configuration and credential configuration.\nRefer to Backup a Custom Configuration.\nBackup switch configurations.\nRefer to Configuration Management.\nValidate the SHCD.\nThe SHCD defines the topology of a Shasta system, this is needed when generating switch configurations. Refer to Validate the SHCD.\nValidate cabling between SHCD generated data and actual switch configuration.\nRefer to Validate Cabling.\nGenerate the switch configuration file(s).\nRefer to Generate Switch Configurations.\nIf the switches have any configuration, it is recommenced to erase it before any configuration.\nRefer to Wipe Management Switch Configuration.\nApply the configuration to switch.\nRefer to Apply Switch Configurations.\nApply the custom configuration to switch, which includes site connection and credential information.\nRefer to one of the following procedures:\nApply Custom Switch Configurations 1.0 Apply Custom Switch Configurations 1.2 Check the differences between the generated configurations and the configurations on the system.\nRefer to Validate Switch Configurations.\nRun a suite of tests against the management network switches.\nRefer to Network Tests.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/udld/",
	"title": "Unidirectional Link Detection (UDLD)",
	"tags": [],
	"description": "",
	"content": "Unidirectional Link Detection (UDLD) \u0026ldquo;The purpose of the UDLD protocol is to detect the presence of anomalous conditions in the Layer 2 communication channel, while relying on the mechanisms defined by the IEEE in the 802.3 standard to properly handle conditions inherent to the physical layer.\u0026rdquo; –rfc5171\nCompatible with existing HPE products: Forward-then-verify: Packets are forwarded until the link is considered unidirectional Verify-then-forward: Packets are not forwarded until the link has been determined to be bidirectional Compatible with RFC5171-compliant devices: Normal: Determines link unidirectionality but will not block the port Aggressive: Once a port has been determined to be bidirectional and then becomes unidirectional, it will be blocked NOTE: The default UDLD mode is forward-then-verify.\nConfiguration Commands Enable UDLD:\nswitch(config-if)# udld Show commands to validate functionality:\nswitch# show udld [interface IFACE] Example Output switch(config)# interface 1/1/1 switch(config-if)# udld switch(config-if)# exit switch# show udld interface 1/1/1 Interface 1/1/1 Config: enabled State: inactive Substate: uninitialized Link: unblock Version: aruba os Mode: forward then verify Interval: 7000 milliseconds Retries: 4 Tx: 0 packets Rx: 0 packets, 0 discarded packets, 0 dropped packets Port transitions: 0 Expected Results Administrators can enable UDLD on an interface UDLD state should be \u0026ldquo;Unblocked | UDLD determined the link is bidirectional\u0026rdquo; NOTE: With SFP+ XCVERS, Aruba Switches automatically detect a broken bidirectional link, rendering the port into a down state.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/validate_cabling/",
	"title": "Validate Cabling",
	"tags": [],
	"description": "",
	"content": "Validate Cabling Warning: If this step is completed when NCNs are offline or shutdown, the information compared here will not match the actual connections. Therefore, this step should be re-run again once the whole system is up.\nTo validate the cabling you can run command similar to below:\ncanu validate shcd-cabling --shcd ./SHCD.xlsx --tabs 40G_10G --corners J12,T36 --ips 10.252.0.2,10.252.0.3 NOTE: Modify the command to use the correct SHCD file and correct --tabs, --corners, and IP addresses.\nThe output should look as follows:\nsw-spine-001 Rack: x3000 Elevation: u24 -------------------------------------------------------------- Port | SHCD | Cabling -------------------------------------------------------------- 1 ncn-w001:pcie-slot1:1 ncn-w001:pcie-slot1:2 2 ncn-w002:pcie-slot1:1 ncn-w002:pcie-slot1:1 3 ncn-w003:pcie-slot1:1 ncn-w003:pcie-slot1:1 sw-spine-002 Rack: x3000 Elevation: u24 -------------------------------------------------------------- Port | SHCD | Cabling -------------------------------------------------------------- 1 ncn-w001:pcie-slot1:2 ncn-w001:pcie-slot1:2 2 ncn-w002:pcie-slot1:2 ncn-w002:pcie-slot1:2 3 ncn-w003:pcie-slot1:2 ncn-w003:pcie-slot1:2 In the returned output, look for differences between the SHCD and actual network configuration.\nIn the above example, incorrect cabling was detected on the sw-spine-001 switch on port 1:\n1 ncn-w001:pcie-slot1:1 ncn-w001:pcie-slot1:2*\nThe SHCD has the correct information on port 1 but the actual switch configuration is mismatched.\nTo fix this issue, re-cable ncn-w001 so that it is correctly connected to sw-spine-001.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/upgrade/",
	"title": "Perform a VSX Upgrade on Aruba Switches",
	"tags": [],
	"description": "",
	"content": "Perform a VSX Upgrade on Aruba Switches The vsx update-software command is used to update the switch software with minimal to no downtime. This command gives administrators the option to save the running configuration on the primary and secondary VSX switches. After the command saves the running configuration, it downloads new software from the TFTP server and verifies the download. After a successful verification, the command installs the software to the alternative image of both the VSX primary and secondary switches.\nThe command displays the status of the VSX primary and secondary switches during the upgrade. The command also refreshes the progress bar as the image update progresses. Do not interrupt the VSX primary CLI session until the software updates completes; however, software update process can be stopped.\nIf you stop the upgrade when the secondary switch has already installed the image in its flash memory or the secondary switch has started the reboot the process, it comes up with the new software.\nThe primary switch continues to have with older software. Administrators can stop the software update process by pressing ctrl+c.\nPrerequisites Choose the method to upload the new software to the switches: Via USB Via WEB UI Via TFTP or SFTP NOTE: If you do not want to proceed with pre-staging you can also upload the new software directly using vsx update-software command. However, you will be limited to only using TFTP if you choose not to pre-stage the firmware.\nVSX Upgrade Command The vsx update-software command is used to update the switch software. The following describes the syntax and parameters of the command.\nSyntax vsx update-software \u0026lt;REMOTE-URL\u0026gt; [vrf \u0026lt;VRF-NAME\u0026gt;] Parameters \u0026lt;REMOTE-URL\u0026gt; Specifies the TFTP URL for downloading the software.\nvrf \u0026lt;VRF-NAME\u0026gt; (Optional) Specifies the VRF name for downloading the software.\nExample of Updating Software via TFTP NOTE: If the new software is already pre-staged, call the image bank where the new image is located instead of using the TFTP.\nswitch# vsx update-software tftp://192.168.1.1/XL.10.0x.xxxx vrf mgmt Do you want to save the current configuration (y/n)? y The running configuration was saved to the startup configuration. This command will download new software to the %s image of both the VSX primary and secondary systems, then reboot them in sequence. The VSX secondary will reboot first, followed by the primary. Continue (y/n)? y VSX Primary Software Update Status : \u0026lt;VSX primary software update status\u0026gt; VSX Secondary Software Update Status : \u0026lt;VSX secondary software update status\u0026gt; VSX ISL Status : \u0026lt;VSX ISL status\u0026gt; Progress [..........................................................................................] Secondary VSX system updated completely. Rebooting primary. Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/validate_shcd/",
	"title": "Validate the SHCD",
	"tags": [],
	"description": "",
	"content": "Validate the SHCD Use the CSM Automatic Network Utility (CANU) to validate the Shasta Cabling Diagram (SHCD). SHCD validation is required to ensure that Plan of Record (PoR) network configurations are generated. This is an iterative process to create a model of the entire network topology connection by connection.\nTopics Prerequisites before validating the SHCD Begin validation in the following order Checks and Validations Logging and Updates Output SHD to JSON Prerequisites Up to date SHCD. CANU installed with version 1.1.11 or greater. Run canu --version to see version. If doing a CSM install or upgrade, a CANU RPM is located in the release tarball. For more information, see this procedure: Update CANU From CSM Tarball Prepare to Validate Open existing SHCD in Excel.\nSave a new copy with an incremented revision number and make sure the updated version is being edited.\nSeveral worksheets (tabs) are used to store the topology of the management network.\nBegin Validation The SHCD must be validated in the following order:\n10G_25G_40G_100G tab (or some variation thereof) contains switch-to-switch connections, as well as NCN server connections to the switch Node Management Network (NMN) contains network management nodes Hardware Management Network (HMN) contains device BMCs and other 1G management ports MTN_TDS, Mountain-TDS-Management, or some variation thereof for Mountain cabinets PDU Validation Steps Validate the 10G_25G_40G_100G tab and select the upper left corner and lower right corner of the spreadsheet with the Source Rack Location Slot Port Destination Rack Location Port information.\nThis is a block of data on the right hand of the worksheet and is not the calculated values used for cable labels on the left-hand side.\nIn this example above, the 10G_25G_40G_100G worksheet has the upper left and lower right corners of I37 and T107 respectively. Note, the above screenshot is trimmed and only the first full 68 rows are shown.\nUse CANU to validate this worksheet.\nncn# canu validate shcd -a full --shcd ./HPE\\ System\\ Hela\\ CCD.revA27.xlsx --tabs 10G_25G_40G_100G --corners I37,T107 The -a or –architecture parameter can be set to tds, full, or v1 (case insensitive):\ntds – Aruba-based Test and Development System. These are small systems characterized by Kubernetes NCNs cabled directly to the spine. full – Aruba-based Leaf-Spine systems. These are usually customer production systems. v1 – Dell and Mellanox based systems of either a TDS or Full layout. CANU will ensure that each cell has valid data and that the connections between devices are allowed. Errors will stop processing and must be fixed in the spreadsheet before moving on. A \u0026ldquo;clean run\u0026rdquo; through a worksheet will include the model, a port-map of each node and may include warnings. See a list of typical errors at the end of this document to help in fixing the worksheet data.\nCheck for errors after validating the worksheet.\nncn# canu validate shcd -a full --shcd ./HPE\\ System\\ Hela\\ CCD.revA27.xlsx --tabs 10G_25G_40G_100G,NMN --corners I37,T107,J15,T16 --log DEBUG Checks and Validation A worksheet that runs \u0026ldquo;cleanly\u0026rdquo; will have checked that:\nNodes are \u0026ldquo;architecturally allowed\u0026rdquo; to connect to each other.\nNo overlapping ports specified.\nA worksheet that runs cleanly will have checked that:\nNodes are architecturally allowed to connect to each other.\nNo overlapping ports specified.\nNode connections can be made at the appropriate speeds.\nIn addition, a clean run will have the following sections:\nSHCD Node Connections – A high level list of all node connections on the system.\nSHCD Port Usage – A Port-by-port detailed listing of all node connections on the system.\nWarnings:\nA list of nodes found that are not categorized on the system.\nNote: This list is important as it could include misspellings of nodes that should be included!\nA list of cell-by-cell warnings of misspellings and other nit-picking items that CANU has autocorrected on the system.\nCheck Warnings Critical: The Warnings output will contain a section headed Node type could not be determined for the following. This needs to be carefully reviewed because it may contain site uplinks that are not tracked by CANU, and may also contain misspelled or miscategorized nodes. As an example:\nFor example:\nNode type could not be determined for the following. These nodes are not currently included in the model. (This may be a missing architectural definition/lookup or a spelling error) -------------------------------------------------------------------------------- Sheet: 10G_25G_40G_100G Cell: I96 Name: CAN switch Cell: I97 Name: CAN switch Cell: O87 Name: CAN switch Cell: O90 Name: CAN switch Cell: O93 Name: CAN switch Cell: O100 Name: CAN switch Cell: O103 Name: CAN switch Cell: I38 Name: sw-spinx-002 Sheet: HMN Cell: R36 Name: SITE Sheet: NMN Cell: P16 Name: SITE From the above example, two important observations can be made:\nCAN and SITE uplinks are not in the \u0026ldquo;clean run\u0026rdquo; model. This means that these ports will not be configured.\nCritically, cell I38 has a name of sw-spinx-002. This should be noted as a misspelling of sw-spine-002 and corrected.\nCheck SHCD Port Usage Today CANU validates many things, but a future feature is full cable specification checking of nodes (e.g. which NCN ports go to which switches to properly form bonds). There are several CANU roadmap items, but today a manual review of the SHCD Port Usage connections list is vital. Specifically, check:\nBoth Management NCNs (manager, worker, storage) and Application Nodes (UANs, viz, and others) follow Plan of Record (PoR) cabling. See Cable Management Network Servers.\nSwitch pair cabling is appropriate for VSX, MAGP, etc.\nSwitch-to-switch cabling is appropriate for LAG formation.\nOther nodes on the network seem sane.\nLogging and Updates Once the SHCD has run cleanly through CANU and CANU output has been manually validated, changes to the SHCD should be committed so that work is not lost, and other users can take advantage of the CANU changes.\nAdd an entry to the changelog on the first worksheet (Summary).\nThe changelog should include:\nThe CANU command line used to validate the spreadsheet The CANU version being used to validate the spreadsheet An overview of changes made to the spreadsheet Upload the SHCD to an official storage location after it has been validated.\nEither of the following options can be used:\ncustomer communication (CAST ticket for customers) SharePoint (internal systems and sometimes customer systems) Output SHCD to JSON Once the SHCD is fully validated, the user will be able to output all the connection details to a json file. This output json file is used to generate switch configurations. ncn# canu validate shcd -a v1 --shcd ./test.xlsx --tabs 40G_10G,NMN,HMN --corners I12,S37,I9,S20,I20,S31 --json --out cabling.json "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/verify-switches_are_forwarding_dhcp_traffic/",
	"title": "Verify the Switches are Forwarding DHCP Traffic",
	"tags": [],
	"description": "",
	"content": "Verify the Switches are Forwarding DHCP Traffic If this point is reached and PXE booting is still not possible, it is likely the IP-Helper is broken on the switch.\nBack to index.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/validate_switch_configs/",
	"title": "Validate Switch Configurations",
	"tags": [],
	"description": "",
	"content": "Validate Switch Configurations Prerequisites SSH access to the switches or the running configuration file. Generated switch configurations. Generate Switch Configurations CANU installed with version 1.1.11 or greater. Run canu --version to see version. If doing a CSM install or upgrade, a CANU RPM is located in the release tarball. For more information, see this procedure: Update CANU From CSM Tarball Compare CSM 1.2 switch configurations with running configurations Compare the current running configuration with the generated configuration.\nFor the comparison, because we have pulled the configuration to our working directory we can compare the files locally. CANU can also pull the configuration from the switch by using the --ip, --username, and --password arguments.\nExample of CANU pulling configuration.\nncn# canu validate switch config --ip 192.168.1.1 --username USERNAME --password PASSWORD --generated ./generated/sw-spine-001.cfg Doing file comparisons on your local machine:\nComparing configuration file for single switch: ncn# canu validate switch config --running ./running/sw-spine-001.cfg --generated sw-spine-001.cfg Please enter the vendor (Aruba, Dell, Mellanox): Aruba\nComparing configuration files for full system: ncn# canu validate network config --csm 1.2 --running ./running/ --generated ./generated/ CANU-generated switch configurations will not include any ports or devices not defined in the model. These were previously discussed in the \u0026ldquo;Validate the SHCD section\u0026rdquo; but include edge uplinks (CAN/CMN) and custom configurations applied by the customer. When looking at the generated configurations being applied against existing running configurations CANU will recommend removal of some critical configurations. It is vital that these devices and configurations be identified and protected. This can be accomplished in three ways:\nProvide CANU validation of generated configurations against running configurations with an override or \u0026ldquo;blackout\u0026rdquo; configuration – a YAML file which tells CANU to ignore customer-specific configurations. The process of creating this file was previously described in the This file will be custom to every site and must be distributed with the analysis and configuration file bundle to be used in the future.\nBased on experienced networking knowledge, manually reorder the proposed upgrade configurations. This may require manual exclusion of required configurations which the CANU analysis says to remove.\nSome devices may be used by multiple sites and may not currently be in the CANU architecture and configuration. If a device type is more universally used on several sites, then it should be added to the architectural and configuration definitions via the CANU code and Pull Request (PR) process.\nNote: A roadmap item for CANU is the ability to \u0026ldquo;inject\u0026rdquo; customer configurations into CANU and provide solid, repeatable configuration customization.\nAnalyze CSM 1.2 configuration upgrade Configuration updates depending on the current version of network configuration may be as easy as adding few lines or be a complete \u0026ldquo;rip and replace\u0026rdquo; operation which may lead you to choosing to wipe the existing configuration or just simply adding few lines in the configuration.\nAlways before making configuration changes, analyze the changes shown in the above configuration diff section.\n:exclamation: All of these steps should be done using an out of band connection. This process is disruptive and will require downtime :exclamation:\nCaveats and known issues Mellanox and Dell support is limited. Some configuration may need to be applied in a certain order. Example: Customer VRF needs to be applied before adding interfaces/routes to the VRF. When applying certain configuration it may wipe out pre-existing configuration. An example of this would be adding a VRF to a port. For example:\nConfig differences between running config and generated config Safe Commands These commands should be safe to run while the system is running. ------------------------------------------------------------------------- interface 1/1/mgmt0 no shutdown interface 1/1/30 mtu 9198 description vsx isl interface vlan 7 ip ospf 1 area 0.0.0.0 router ospf 1 vrf Customer router-id 10.2.0.2 default-information originate area 0.0.0.0 Manual Commands These commands may cause disruption to the system and should be done only during a maintenance period. It is recommended to have an out-of-band connection while running these commands. ------------------------------------------------------------------------- interface 1/1/mgmt0 vrf attach keepalive ip address 192.168.255.0/31 interface 1/1/30 no vrf attach keepalive lag 256 ------------------------------------------------------------------------- Commands NOT classified as Safe or Manual These commands include authentication as well as unique commands for the system. These should be looked over carefully before keeping/applying. ------------------------------------------------------------------------- no user admin group administrators password ciphertext AQBapa3xRMDxuA1PmoQJEc3kv1FjET4ix0HtN5hHGJDLa3PKYgAAAO7tAGcAlW6jst5Byl50ax+JA+ViqsHr8Sl1KCzSFzgBtaIYz3iTPD3zk5wmbJ1IKbMQ9+TcgFUO7baupypo7ftDMIbZhn+A7UaLALJzFj+W+NIqmWbOGfKw9ie0jTM5JUfl no profile Leaf no debug ospfv2 all no snmp-server vrf default no snmpv3 user testuser auth md5 auth-pass ciphertext AQBapflTKYh28GLx4x7Bp5XyAT0j2jnm9fDMNei1tR+BTyrqCQAAAITcQ4YsQX2noQ== priv des priv-pass ciphertext AQBapaNP67WbY49eqp0jL27tInN1FeAD9TjgkcbW31S85/SBCQAAAP6e+534mdJiaA== no route-map CMN permit seq 10 no router ospf 2 vrf Customer router bgp 65533 vrf Customer no exit-address-family ------------------------------------------------------------------------- Switch: sw-spine-001 Differences ------------------------------------------------------------------------- In Generated Not In Running (+) | In Running Not In Generated (-) ------------------------------------------------------------------------- Total Additions: 6 | Total Deletions: 33 Interface: 1 | Interface: 3 Router: 1 | Router: 2 "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/verify_bgp/",
	"title": "Verify BGP",
	"tags": [],
	"description": "",
	"content": "Verify BGP Verify the BGP neighbors are in the established state on BOTH the switches.\nProcedure Check Aruba BGP status.\nsw-spine-002# show bgp ipv4 u s Example output:\nVRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.3 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.2 65533 45052 45044 02m:02w:02d Established Up 10.252.1.7 65533 78389 90090 02m:02w:02d Established Up 10.252.1.8 65533 78384 90059 02m:02w:02d Established Up 10.252.1.9 65533 78389 90108 02m:02w:02d Established Up Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/wipe_mgmt_switches/",
	"title": "Wipe Management Switch Configuration",
	"tags": [],
	"description": "",
	"content": "Wipe Management Switch Configuration This procedure describes how to wipe Aruba, Dell, and Mellanox switch configurations.\nPrerequisites Out-of-band access to the switches (console)\nAruba Create a checkpoint before erasing the switch configuration.\nMore information related to backing up configuration can be found on the Configuration Management procedure.\nsw-spine-001# copy running-config checkpoint CSM1_0 Verify the checkpoint was created.\nsw-spine-001# show checkpoint Example output:\nNAME TYPE WRITER DATE(YYYY/MM/DD) IMAGE VERSION CSM1_0 latest User 2022-01-27T18:52:31Z GL.10.08.1021 Erase the startup configuration.\nsw-spine-002# erase startup-config Erase checkpoint startup-config ? (y/n): y Reboot after erasing the startup configuration.\nsw-spine-001# boot system Checking if the configuration needs to be saved... Do you want to save the current configuration (y/n)? n The switch will reboot without any configuration.\nThe default user is admin without any password.\nFollow the Apply Switch Configurations procedure.\nDell Save startup configuration to a new XML configuration file.\nsw-leaf-bmc-001# copy config://startup.xml config://csm1.2.xml Erase the startup configuration.\nsw-leaf-bmc-001# delete startup-configuration Proceed to delete startup-configuration [confirm yes/no(default)]:yes Reboot after erasing the startup configuration.\nsw-leaf-bmc-001# reload System configuration has been modified. Save? [yes/no]:no Continuing without saving system configuration Proceed to reboot the system? [confirm yes/no]:yes The default username and password are admin. This will boot the switch to factory defaults.\nFollow the Apply Switch Configurations procedure.\nMellanox Create a new configuration file.\nWhen a new configuration file is created, no data is written to it. We will boot to this new, blank configuration file.\n(config) # configuration new csm1.2 If that configuration exists already, delete it with configuration delete csm1.2, or reset to factory defaults with reset factory.\nCheck that the configuration files contain the new csm1.2 blank configuration that was just created.\n(config) # show configuration files Example output:\nfiles csm1.0 (active) csm1.2 initial initial.bak Active configuration: csm1.0 Unsaved changes : no Switch to the new configuration, which requires a reboot.\n(config) # configuration switch-to csm1.2 This requires a reboot. Type \u0026#39;yes\u0026#39; to confirm: yes The default username and password are admin\nFollow the prompts as shown below.\nNVIDIA Switch Configuration wizard Do you want to use the wizard for initial configuration? Please answer \u0026#39;yes\u0026#39; or \u0026#39;no\u0026#39;. Do you want to use the wizard for initial configuration? no Enable password hardening: [yes] no New password for \u0026#39;admin\u0026#39; account must be typed, please enter new password: Confirm: New password for \u0026#39;monitor\u0026#39; account must be typed, please enter new password: Confirm: "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/verify_dhcp_traffic_on_workers/",
	"title": "Verify the DHCP Traffic on the Worker Nodes",
	"tags": [],
	"description": "",
	"content": "Verify the DHCP Traffic on the Worker Nodes This section is an example issue of where the source address of the DHCP Offer is the Metallb address of KEA \u0026ldquo;10.92.100.222\u0026rdquo;.\nThe source address of the DHCP Reply/Offer MUST be the address of the VLAN interface on the worker node.\nUse the following command to look at DHCP traffic on the workers:\nncn-w001# tcpdump -envli bond0 port 67 or 68 Look for the source IP address of the DHCP Reply/Offer. The following is an example of working offer:\n10.252.1.9.67 \u0026gt; 255.255.255.255.68: BOOTP/DHCP, Reply, length 309, hops 1, xid 0x98b0982e, Flags [Broadcast] Administratorsr-IP 10.252.1.17 Server-IP 10.92.100.60 Gateway-IP 10.252.0.1 Client-Ethernet-Address 14:02:ec:d9:79:88 file \u0026#34;ipxe.efi\u0026#34;[|bootp] If the Source IP address of the DHCP Reply/Offer is the MetalLB IP, the DHCP packet will never make it out of the NCN An example of this is below. 10.92.100.222.116 \u0026gt; 255.255.255.255.68: BOOTP/DHCP, Reply, length 309, hops 1, xid 0x260ea655, Flags [Broadcast] Administratorsr-IP 10.252.1.14 Server-IP 10.92.100.60 Gateway-IP 10.252.0.4 Client-Ethernet-Address 14:02:ec:d9:79:88 file \u0026#34;ipxe.efi\u0026#34;[|bootp] Resolution If this issue occurs, the only solution is to restart KEA and making sure that it gets moved to a different worker node.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/verify_route_to_tftp/",
	"title": "Verify Route to TFTP",
	"tags": [],
	"description": "",
	"content": "Verify Route to TFTP On BOTH Aruba switches, a single route to the TFTP server 10.92.100.60 is needed. The configuration may differ on the system in use.\nThis is needed because there are issues with Aruba ECMP hashing and TFTP traffic.\nsw-spine-002# show ip route 10.92.100.60 Example output:\nDisplaying ipv4 routes selected for forwarding \u0026#39;[x/y]\u0026#39; denotes [distance/metric] 10.92.100.60/32, vrf default, tag 0 via 10.252.1.9, [70/0], bgp This route can be a static route or a BGP route that is pinned to a single worker. The 1.4.2 patch introduced the BGP pinned route.\nVerify that you can ping the next hop of this route. For example, in the example above we would ping 10.252.1.9. If this is not reachable, this is the problem.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/vlan/",
	"title": "Virtual Local Access Networks (VLANs)",
	"tags": [],
	"description": "",
	"content": "Virtual Local Access Networks (VLANs) VLANs allow for the logical grouping of switch interfaces, enabling communication as if all connected devices were on the same isolated network.\nConfiguration commands Create VLAN:\nswitch(config)# vlan \u0026lt;VLAN\u0026gt; Configure an interface to associate it with a VLAN:\nswitch(config)# interface \u0026lt;IFACE\u0026gt; switch(config-if)# no shutdown switch(config-if)# no routing Configure an interface as an access port:\nswitch(config-if)# vlan access VLAN Configure an interface as a trunk port:\nswitch(config-if)# vlan trunk native \u0026lt;VLAN\u0026gt; switch(config-if)# vlan trunk allowed \u0026lt;VLAN\u0026gt; Configure VLAN as Voice:\nNOTE: To give a specific VLAN a voice designation and adding the proper hooks, you need to add the voice command in the VLAN context. This configuration is the same for all CX-series switches.\nswitch(config)# vlan \u0026lt;VLAN\u0026gt; switch(config-vlan-100)# voice Show commands to validate functionality:\nswitch# show vlan [VLAN] Example output switch# show vlan -------------------------------------------------------------------------------------- VLAN Name Status Reason Type Interfaces -------------------------------------------------------------------------------------- 1 DEFAULT_VLAN_1 up no_member_port static 1/1/2 10 VLAN10 up ok static 1/1/1-1/1/2 Expected results Administrators can create a VLAN Administrators can assign a VLAN to the physical interface Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/vlan_interface/",
	"title": "VLAN Interface",
	"tags": [],
	"description": "",
	"content": "VLAN Interface The switch also supports classic L3 VLAN interfaces.\nConfiguration Commands Configure the VLAN:\nswitch(config)# vlan VLAN Create and enable the VLAN interface, and assign it an IP address:\nswitch(config)# interface vlan VLAN switch(config-if-vlan)# ip address IP-ADDR/SUBNET switch(config-if-vlan)# no shutdown Show commands to validate functionality:\nswitch# show vlan [VLAN|interface IFACE|summary] Example Output switch(config)# vlan 10 switch(config-vlan)# exit switch(config)# int 1/1/1 switch(config-if)# vlan access 10 switch(config-if)# int vlan 10 switch(config-if-vlan)# ip address 10.0.0.1/24 switch(config-if-vlan)# no shutdown switch(config-if-vlan)# end 108 bytes from 10.0.0.101: icmp_seq=4 ttl=64 time=2.07 ms 108 bytes from 10.0.0.101: icmp_seq=5 ttl=64 time=1.79 ms Expected Results Administrators can configure the VLAN Administrators can enable the interface and associate it with the VLAN Administrators can create an IP-enabled VLAN interface, and it is up Administrators validate the configuration is correct Administrators can ping from the switch to the client and from the client to the switch Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/vlan_trunking_8021q/",
	"title": "VLAN Trunking 802.1Q",
	"tags": [],
	"description": "",
	"content": "VLAN Trunking 802.1Q A trunk port carries packets on one or more VLANs specified. Packet that ingress on a trunk port are in the VLAN specified in its 802.1Q header, or native VLAN if the packet has no 802.1Q header. A packet that egresses through a trunk port will have an 802.1Q header if it has a nonzero VLAN ID. Any packet that ingresses on a trunk port tagged with a VLAN that the port does not trunk is dropped.\nConfiguration Commands Configure an interface as a trunk port:\nswitch(config-if)# vlan trunk allowed VLANS Show commands to validate functionality:\nswitch# show vlan [VLAN-ID] Example Output switch(config)# vlan 10 switch(config-vlan-10)# no shutdown switch(config-vlan-10)# exit switch(config)# vlan 20 switch(config-vlan-20)# no shutdown switch(config-vlan-20)# exit switch(config)# interface 1/1/1 switch(config-if)# no shutdown switch(config-if)# no routing switch(config-if)# vlan trunk native 10 switch(config-if)# vlan trunk allowed 10,20 switch(config-if)# end Expected Results Administrators can create and enable multiple VLAN interfaces Administrators can assign the trunk VLAN interfaces Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/vsf/",
	"title": "Virtual Switching Framework (VSF) - 6300 Only",
	"tags": [],
	"description": "",
	"content": "Virtual Switching Framework (VSF) - 6300 Only Virtual Switching Framework (VSF) defines a virtual switch comprised of multiple individual physical switches, inter-connected through standard Ethernet links. These physical switches will operate with one control plane, thereby visible to the peers as a virtual switch stack.\nWithin the stack, one switch is the \u0026ldquo;Master\u0026rdquo; switch, which runs all the control plane software and manages the ASICs of all the stack members. A second switch can be configured as the \u0026ldquo;Standby\u0026rdquo; switch, which will take over as master if the master fails.\nEach stack member must have a unique member number. While deploying a stack, the user must ensure that each member has a distinct member number by renumbering the switches to desired member numbers. Stack formation will fail if there is a member number conflict.\nA primary member will become master under normal circumstances (no member / link failures). The secondary member will become standby member under normal circumstances.\nThe primary member is member number 1. This is not configurable. Also note that 1 is the default member number. A factory-default switch boots up as a VSF-enabled switch, with member number \u0026ldquo;1\u0026rdquo;. It behaves as a 1-member stack, of which it is master.\nThe secondary member number is user configurable, and there is no default secondary member. It is strongly recommended that the customer configure a secondary member in the stack, because a stack with a standby offers resiliency and high-availability.\nOther than the primary and secondary members, no members can ever become master / standby of the stack.\nConfiguration Commands Create a VSF member:\nswitch(config)# vsf member \u0026lt;ID\u0026gt; switch(vsf-member)# link \u0026lt;ID\u0026gt; \u0026lt;IFACE-RANGE\u0026gt; Show commands to validate functionality:\nswitch# show vsf \u0026lt;brief|configuration|status\u0026gt; Example Output switch# show vsf topology Stby Master CPU Utilization Memory Utilization : 15% VSF link 1 : Down VSF link 2 : Down +---+ +---+ +---+ | 3 |1==2| 2 |1==1| 1 | +---+ +---+ +---+ Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/vsx/",
	"title": "Virtual Switching Extension (VSX)",
	"tags": [],
	"description": "",
	"content": "Virtual Switching Extension (VSX) Aruba\u0026rsquo;s Virtual Switching Extension (VSX) is a solution that integrates two independent ArubaOS-CX switches into an active/active virtualized high availability (HA) solution. The two switch peers utilize a connected link for control and data. This solution allows the switches to present as one virtualized switch in critical areas.\nConfiguration synchronization is one aspect of this VSX solution where the primary switch configuration is synced to the secondary switch. This allows for pseudo-single pane of glass configuration and helps keep key configuration pieces in sync as operational changes are made. Since the solution is primarily for HA, it is expected that the vast majority of configuration policy is the same across both peers.\nConfiguration Commands Enable VSX:\nswitch(config)# vsx Give the device a role of primary or secondary:\nswitch(config-vsx)# role \u0026lt;primary|secondary\u0026gt; Configure the VSX keepalive between the two VSX peer switches:\nswitch(config-vsx)# keepalive peer PEER-IP source SRC-IP Select a physical or LAG interface to become the inter-switch-link:\nswitch(config-vsx)# inter-switch-link IFACE Create Multi-Chassis LAG interfaces:\nswitch(config)# interface lag LAG multi-chassis Associate the physical interfaces with the LAG:\nswitch(config-if)# lag LAG Synchronize global configurations:\nswitch(config-vsx)# vsx-sync [aaa] [sflow] [snmp] [static-routes] [time] [copp-policy] [dns] [mclag-interfaces] [qos-global] [ssh] Synchronize interface memberships switch(config-if)# vsx-sync [access-lists] [policies] [qos] [vlans] [rate-limits] Synchronize VLAN interface memberships switch(config-if-vlan)# vsx-sync [active-gateways] [policies] Synchronize feature configurations switch(config-vlan)# vsx-sync switch(config-acl-ip)# vsx-sync switch(config-class-ip)# vsx-sync switch(config-policy)# vsx-sync switch(config-pbr-action-list)# vsx-sync switch(config-schedule)# vsx-sync switch(config-queue)# vsx-sync switch(config-portgroup)# vsx-sync switch(config-addrgroup)# vsx-sync Show commands to validate functionality:\nswitch# show vsx \u0026lt;brief|configuration|status\u0026gt; [config-sync] Example Output switch(config)# int 1/1/49 switch(config-if)# description VSX KEEPALIVE LINK switch(config-if)# no shut switch(config-if)# ip address 1.1.1.1/30 switch(config-if)# ex switch(config)# conf switch(config)# int lag 128 switch(config-lag-if)# description VSX INTER-SWITCH-LINK switch(config-lag-if)# lacp mode active switch(config-lag-if)# no shut switch(config-lag-if)# no routing switch(config-lag-if)# vlan trunk allowed all switch(config-lag-if)# exit switch(config)# int 1/1/50 switch(config-if)# no shut switch(config-if)# lag 128 switch(config-if)# ex switch(config)# vsx switch(config-vsx)# role primary switch(config-vsx)# keepalive peer 1.1.1.2 source 1.1.1.1 switch(config-vsx)# inter-switch-link lag 128 switch(config-vsx)# exit switch(config)# show run int 1/1/49 interface 1/1/49 no shutdown ip address 1.1.1.1/30 exit switch(config)# show run int lag128 interface lag 128 description VSX INTER-SWITCH-LINK no shutdown no routing vlan trunk native 1 tag vlan trunk allowed all lacp mode active exit switch(config)# show vsx status VSX Operational State --------------------- Platform Software Version Device Role X86-64 X86-64 Virtual.10.01.0001G Virtual.10.01.0001G primary secondary switch(config-if)# int lag 1 multi-chassis switch(config-lag-if)# no routing switch(config-lag-if)# vlan trunk allowed all switch(config-lag-if)# no shut switch(config-lag-if)# exit switch(config)# int 1/1/11 switch(config-if)# no shut switch(config-if)# lag 1 switch(config-if)# exit switch(config)# show lacp interfaces State abbreviations : A - Active P - Passive F - Aggregable I - Individual S - Short-timeout L - Long-timeout N - InSync O - OutofSync C - Collecting D - Distributing X - State m/c expired E - Default neighbor state Actor details of all interfaces: ------------------------------------------------------------------------------ Intf Aggr Port Port State System-id System Aggr Forwarding Name Id Pri Pri Key State ------------------------------------------------------------------------------ 1/1/11 lag1(mc) 11 1 ALFNCD 98:f2:b3:68:a2:7e 65534 1 up 1/1/50 lag128 51 1 ALFNCD 98:f2:b3:68:a2:7e 65534 128 up Partner details of all interfaces: ------------------------------------------------------------------------------ Intf Aggr Port Port State System-id System Aggr Name Id Pri Pri Key ------------------------------------------------------------------------------ 1/1/11 lag1(mc) 56 0 ALFNCD 50:65:f3:12:6d:00 27904 986 1/1/50 lag128 51 1 ALFNCD 98:f2:b3:68:c4:9a 65534 128 switch(config)# vlan 10 switch(config-vlan-10)# vsx-sync switch(config)# access-list ip secure_mcast_sources switch(config-acl-ip)# vsx-sync switch(config-acl-ip)# 10 permit igmp any any switch(config-acl-ip)# 15 comment block downstream from sourcing mcast switch(config-acl-ip)# 20 deny any any 224.0.0.0/4 switch(config-acl-ip)# 30 permit any any switch(config-acl-ip)# int 1/1/1 switch(config-if)# no shutdown switch(config-if)# no routing switch(config-if)# vlan access 10 switch(config-if)# apply access-list ip secure_mcast_sources switch(config-if)# vsx-sync switch(config-if)# end switch# show run vsx-sync Current vsx-sync configuration: vlan 10 vsx-sync vlan 20 vsx-sync vlan 30 vsx-sync access-list ip secure_mcast_sources vsx-sync ! 10 comment allow igmp traffic from downstream 10 permit igmp any any 15 comment block downstream from sourcing mcast 20 deny any any 224.0.0.0/240.0.0.0 30 permit any any any Expected Results Administrators can configure VSX Administrators can create a multi-chassis interface Administrators can add ports to the multi-chassis interface Administrators can configure the VLANs and ACLs for synchronization Everything is synchronized from the primary to the secondary Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/vsx_architecture/",
	"title": "What is VSX?",
	"tags": [],
	"description": "",
	"content": "What is VSX? Enterprise networks face the challenge of delivering 24x7 always-on reliable access and cloud-based services. As a business and network grows, this non-stop availability becomes more critical due to simple economics. Downtime leads to a loss of productivity, user satisfaction and revenue.\nSwitches in the campus and data center sit at the heart of the network and are responsible for the delivery of a high availability (HA) solution that is capable of ensuring always-on access with robust performance.\nTo address this requirement, Aruba\u0026rsquo;s Virtual Switching Extension (VSX) has been designed from the ground up to provide industry-leading performance and high availability with much-needed simplicity. This is accomplished through the resiliency of AOS-CX, a modern network operating system that performs continuous state synchronization.\nWhat are the Benefits of VSX? No downtime, even during upgrades.\nHigh-availability (HA) refers to a system or component that is continuously operational for extended periods of time. For organizations today, this translates to 24x7 uptime, without disruption to network access or performance.\nPlanning for high availability of campus and data center switches involves complex tasks for addressing backup and failover processing. This includes copying and accessing the real-time networking state, including routing and forwarding tables.\nAruba VSX takes a new and innovative approach to solving high availability challenges by combining the best aspects of existing HA technologies such as multi-chassis link aggregation (MC-LAG) and equal-cost multi-path (ECMP) routing. This combination provides a distributed and redundant architecture that is highly available, with minimal to zero traffic loss, even during software upgrades.\nAruba VSX benefits:\nHA design with live upgrades and no downtime Built for redundancy across campus and data center networks Continuous config synchronization via AOS-CX Flexible active-active network designs at L2 and L3 Operational simplicity for configuration and troubleshooting Thanks to its design principles, VSX also supports live upgrades, delivering on the promise of in-service software upgrades where no maintenance windows are required. Within an hour, core and aggregation switches in the campus and top of rack (ToR) switches in the data center can be successfully upgraded while continuously delivering high-performing network services, without compromise.\nVSX Summary HA is a must-have requirement at the core and aggregation layers of campus networks, as well as within leaf-spine architectures in data centers. Aruba\u0026rsquo;s Virtual Switching Extension (VSX) is designed from the ground up to deliver the availability, virtualization, and simplicity required at these mission-critical layers of the network. Our goal is to offer operators a better way to ensure business success with a network that is always available, even during upgrades.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/vsx_switch_replacement/",
	"title": "Switch Replacement in the VSX Cluster",
	"tags": [],
	"description": "",
	"content": "Switch Replacement in the VSX Cluster Replace the VSX primary or the VSX secondary with the following steps.\nMake sure all cables are labelled with clear identification. Unplug power and all fibers and copper cables. Un-rack the failing unit and rack the replacement unit. Power-up the unit. Restore switch firmware (see note below) and configuration. SSH/Console to the replacement switch and shutdown all ports. Example: 8320, 8325 Switch# config Switch(config)# interface 1/1/1-1/1/52 shutdown NOTE: Restoring firmware is required only if replacing the primary VSX member. Otherwise, restore the configuration because VSX sync would force automatic software upgrade on secondary member.\nRe-cable SSH/Console to the replacement switch and re-enable all ports: The switch should now enable the VSX ports. Once VSX sync is completed, all ports should get enabled after hold-down timer has expired.\nNOTE: If replacing the secondary member and a software upgrade was not done prior to enabling the ports, there will be a reboot on the secondary unit after it has detected the primary VSX member and received new software from it before normal ports would come up.\nBack to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/vsx_sync/",
	"title": "VSX Sync",
	"tags": [],
	"description": "",
	"content": "VSX Sync Configuration synchronization is one aspect of this VSX solution where the primary switch configuration is synced to the secondary switch. This allows for pseudo single pane of glass configuration and helps keep key configuration pieces in sync as operational changes are made. Since the solution is primarily for HA, it is expected that the vast majority of configuration policy is the same across both peers.\nConfiguration Commands Synchronize VLANs:\nswitch(config-vlan)# vsx-sync Synchronize ACLs:\nswitch(config-acl-ip)# vsx-sync Synchronize classifier and policy:\nswitch(config-class-ip)# vsx-sync Synchronize PBR:\nswitch(config-pbr-action-list)# vsx-sync Synchronize VLAN memberships and ACLs on physical or LAG interfaces:\nswitch(config-if)# vsx-sync access-lists vlans Show commands to validate functionality:\nswitch# show run vsx-sync Example Output On the first switch:\nswitch(config)# vlan 10 switch(config-vlan-10)# vsx-sync switch(config)# access-list ip secure_mcast_sources switch(config-acl-ip)# vsx-sync switch(config-acl-ip)# 10 permit igmp any any switch(config-acl-ip)# 15 comment block downstream from sourcing mcast switch(config-acl-ip)# 20 deny any any 224.0.0.0/4 switch(config-acl-ip)# 30 permit any any On the secondary switch:\nswitch2# show run vsx-sync Current vsx-sync configuration: vlan 10 vsx-sync vlan 20 vsx-sync vlan 30 vsx-sync access-list ip secure_mcast_sources vsx-sync ! 10 comment allow igmp traffic from downstream 10 permit igmp any any 15 comment block downstream from sourcing mcast 20 deny any any 224.0.0.0/240.0.0.0 30 permit any any any Expected Results Administrators can configure the VLAN Administrators can create the ACL Everything synchronized on the primary is now on the secondary Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/web-ui/",
	"title": "Web User Interface (WebUI)",
	"tags": [],
	"description": "",
	"content": "Web User Interface (WebUI) A web-based management user interface provides a visual representation of a subset of the current switch configuration and states. The Web-UI allows for easy access from modern browsers to modify some aspects of the configuration. The Web-UI also provides extensive access to the Network Analytics Engine. Many aspects of the hardware can be monitored in a dashboard view and customized.\nConfiguration Commands Enable the WebUI on a VRF:\nswitch(config)# https-server vrf \u0026lt;mgmt|default|VRF\u0026gt; Configure REST API:\nswitch(config)# https-server rest access-mode read-\u0026lt;only|write\u0026gt; Show commands to validate functionality:\nswitch# show https-server Example Output switch# config switch(config)# https-server rest REST API configuration vrf Configure HTTPS Server for VRF \u0026lt;cr\u0026gt; switch(config)# https-server vrf default switch(config)# https-server vrf mgmt Expected Results Administrators can connect the management interface to a private network Administrators can enable web-management Administrators can connect to the IP address from a browser login to the management menu Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/aruba/zeroize/",
	"title": "Erase All zeroize",
	"tags": [],
	"description": "",
	"content": "Erase All zeroize Erases customer data on the management modules in a secure manner. The command prompts for confirmation of zeroization.\nSyntax erase all zeroize Example Erasing Customer Data on the Management Modules in a Secure Manner switch# erase all zeroize This will securely erase all customer data and reset the switch to factory defaults. This will initiate a reboot and render the. switch unavailable until the zeroization is complete.This should take several minutes to one hour to complete. Continue (y/n)? ServiceOS Information: Version: GT.01.01.0007 Build Date: 2017-12-07 11:48:44 PST Build ID: ServiceOS:GT.01.01.0007:42c7d15cf7e5:201712071148 SHA: 42c7d15cf7e5af5bf1c7d8764ff673471084c2a4 ######### Preparing for zeroization ################ ######### Storage zeroization ###################### ######### WARNING: DO NOT POWER OFF UNTIL ########## ######### ZEROIZATION IS COMPLETE ################## ######### This should take several minutes ######### ######### to one hour to complete ################## ######### Restoring files ########################## Boot Profiles:0. Service OS Console1. Primary Software Image [XL.10.00.0006]2. Secondary Software Image [XL.10.00.0006] Select profile(primary): Back to Index\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/backup_custom_configurations/",
	"title": "Backup a Custom Configuration",
	"tags": [],
	"description": "",
	"content": "Backup a Custom Configuration Prerequisites Access to the switches. If you are doing a fresh install of CSM but previously had a different version of CSM installed, then you will need to backup/restore certain switch configurations after the switch has been wiped.\nThe backup needs to be done before wiping the switch.\nThis includes:\nUsers and passwords SNMP credentials Site connections Interface speed commands Default routes Any other customized configuration for this system This configuration will likely vary from site to site. This guide will cover the most common site setup.\nBackup site connection configuration Find the site connections in the SHCD file.\nCAN switch cfcanb6s1 - 31 sw-25g01 x3000 u39 - j36 CAN switch cfcanb6s1 - 46 sw-25g02 x3000 u40 - j36 This information indicates that the configuration on port 36 on both spine switches must be backed up.\nLog into the switches. Get the configurations of the ports and the default route. Save this output; this will be used after we apply the generated configurations.\nAruba site connection sw-spine-001# show run int 1/1/36 interface 1/1/36 no shutdown description to:CANswitch_cfcanb6s1-31:from:sw-25g01_x3000u39-j36 ip address 10.101.15.142/30 exit sw-spine-001(config)# show run | include interface-group system interface-group 3 speed 10g sw-spine-002# show run int 1/1/36 interface 1/1/36 no shutdown description to:CANswitch_cfcanb6s1-46:from:sw-25g02_x3000u40-j36 ip address 10.101.15.190/30 exit sw-spine-002(config)# show run | include interface-group system interface-group 3 speed 10g sw-spine-001# show run | include \u0026#34;ip route\u0026#34; ip route 0.0.0.0/0 10.101.15.141 sw-spine-002# show run | include \u0026#34;ip route\u0026#34; ip route 0.0.0.0/0 10.101.15.189 Mellanox site connection sw-spine-001 [mlag-domain: master] # show run int ethernet 1/16 interface ethernet 1/16 speed 10G force interface ethernet 1/16 mtu 1500 force interface ethernet 1/16 no switchport force interface ethernet 1/16 ip address 10.102.255.10/30 primary sw-spine-002 [mlag-domain: master] # show run int ethernet 1/16 interface ethernet 1/16 speed 10G force interface ethernet 1/16 mtu 1500 force interface ethernet 1/16 no switchport force interface ethernet 1/16 ip address 10.102.255.86/30 primary sw-spine-001 [mlag-domain: master] # show run | include \u0026#34;ip route\u0026#34; ip route 0.0.0.0/0 10.102.3.3 5 ip route 0.0.0.0/0 10.102.255.9 sw-spine-002 [mlag-domain: master] # show run | include \u0026#34;ip route\u0026#34; ip route 0.0.0.0/0 10.102.3.2 5 ip route 0.0.0.0/0 10.102.255.85 Backup users and passwords Aruba users and passwords sw-leaf-bmc-001# show run | include user user admin group administrators password ciphertext xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Dell users and passwords sw-leaf-001# show running-configuration | grep user system-user linuxadmin password xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx username admin password xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx role sysadmin priv-lvl 15 Mellanox users and passwords sw-spine-001 [standalone: master] # show run | include username username admin password 7 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx username monitor password 7 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Backup SNMP credentials For more information on SNMP credentials, see Change SNMP Credentials on Leaf-BMC Switches and Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials.\nOnce these credentials are retrieved from Vault, the xxxxxx fields below can be filled in.\nAruba SNMP sw-leaf-001# show run | include snmp snmp-server vrf default snmpv3 user testuser auth md5 auth-pass plaintext xxxxxx priv des priv-pass plaintext xxxxxx Dell SNMP sw-leaf-001# show running-configuration | grep snmp snmp-server group cray-reds-group 3 noauth read cray-reds-view snmp-server user xxxxxx cray-reds-group 3 auth md5 xxxxxx priv des xxxxxx snmp-server view cray-reds-view 1.3.6.1.2 included "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/bican_disable_uan_vlan7/",
	"title": "Remove UAN Access to the CMN",
	"tags": [],
	"description": "",
	"content": "Remove UAN Access to the CMN To minimize downtime for UANs during an upgrade from CSM 1.0 to CSM 1.2, switch configurations allow UANs to run on both the CSM 1.0 CAN (which becomes the CSM 1.2 CMN) as well as the new CSM 1.2 CAN. At the end of upgrade, switch configurations for UAN ports must be updated to remove the transitionally allowed access of UANs to the CMN. Manual removal of this port access is described in this document. CSM 1.2.6 will automate this procedure.\nPrerequisites As described in Minimize UAN Downtime, administrators can schedule a minimal-outage transition with users after the Management Network has been upgraded, but before new UANs are booted. During this scheduled outage, the UAN IPv4 addresses are transitioned from the CMN to the CSM 1.2 CAN, via either reboot or running a CFS play.\nAll users of UANs running during the CSM 1.2 upgrade must have been transitioned to the CSM 1.2 CAN or CHN. The administrator performing this procedure must have both of the following: The CSM management network switch passwords. The system network topology CCJ/Paddle file (preferred) or the system SHCD used to updated the management network switches. Procedure Retrieve a token.\nncn-m001# export TOKEN=$(curl -s -k -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) ncn-m001# echo $TOKEN If the TOKEN is blank, it is likely that something is in error on the system or the core system is still being upgraded.\nQuery SLS for the BICAN toggle (SystemDefaultRoute).\nncn-m001# BICAN=$(curl -s -k -S -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api-gw-service-nmn.local/apis/sls/v1/networks/BICAN | jq -r \u0026#39;.ExtraProperties.SystemDefaultRoute\u0026#39;) ncn-m001# echo $BICAN An error in this step likely means that SLS has not been upgraded to include the BICAN network structure. NOTE If the value of BICAN is CHN, then skip the rest of this procedure; UAN switch port configurations are already correct.\nQuery SLS for the CAN VLAN.\nncn-m001# CAN_VLAN=$(curl -s -k -S -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api-gw-service-nmn.local/apis/sls/v1/networks/CAN | jq \u0026#39;.ExtraProperties.Subnets | .[] | select(.Name==\u0026#34;bootstrap_dhcp\u0026#34;) | .VlanID\u0026#39;) ncn-m001# echo $CAN_VLAN An error in this step likely means that SLS has not been upgraded.\nBack up the switch configurations.\nncn-m001# cd /root \u0026amp;\u0026amp; canu backup network --folder switch_backups Use CANU to print the network topology.\nThe Paddle/CCJ JSON file or SHCD used to update the Management Network should be reused here. The CCJ file is preferred as a means of input.\nGenerating the CANU command line for the SHCD as an exercise here and not using the same command line used to update the switches in Update the Management Network will be time consuming and likely error prone. This is why the CCJ/Paddle option is recommended.\nCCJ/Paddle (recommended)\nncn-m001# canu validate paddle --ccj SYSTEM_CCJ_FILENAME SHCD (ensure this command line is exactly the same as was used in updating the management network)\nncn-m001# canu validate shcd -a NETWORK_ARCHITECTURE_TYPE --shcd SHCD_FILENAME --tabs LIST,OF,WORKSHEETS --corners TAB1_UPPER_LEFT, TAB1_LOWER_RIGHT... Identify switches and ports associated with UAN on CAN.\nThe CANU output will contain a list of switches and the port use for each switch. UANs will only be associated with leaf or spine switches (named sw-leaf- and sw-spine- respectively). UANs will be identified in the output via their common names (for example, uan001). Only UAN CAN ports need to be identified. Only the second port of OCP and PCIe slots are used for UAN CAN, according to cabling standards. Custom or non-Plan-of-Record UAN configurations will need to be handled appropriately.\nExample: Identify switches and UAN CAN ports.\nOnly port two for OCP and PCIe cards. Note that CANU output only lists the switch port, not the chassis or slot. On Aruba switches themselves, interfaces are identified by CHASSIS/SLOT/PORT. On all CSM Management switches, values for CHASSIS and SLOT are 1. As an example, CANU slot 27 would be interface 1/1/27 on the switch. Mellanox switches follow the format 1/PORT.\n1: sw-spine-001 has the following port usage: ...snip... 67==\u0026gt;uan002:ocp:2 ...snip... 71==\u0026gt;uan001:ocp:2 ...snip... 2: sw-spine-002 has the following port usage: ...snip... 68==\u0026gt;uan002:pcie-slot1:2 ...snip... 72==\u0026gt;uan001:pscie-slot1:2 ...snip... Example: (Recommended) Create a document of UAN ports.\nThis step is simply to consolidate the output and make the manual switch modification steps less error prone. Using the identified CANU output, create a document listing switches and associated UAN ports. This should look similar in layout (not content) to the following:\nsw-spine-001: 1/1/67 1/1/71 sw-spine-002: 1/1/68 1/1/72 Log in to each switch in the previously generated list document and run the following commands for each identified UAN port.\nOnly one port reconfiguration is shown in this example. Ensure that the procedure is run for all identified ports. Be aware of the particular switch type: Aruba, Dell, or Mellanox.\nLog in to the switch, entering the administrative password when prompted.\nncn-m001# ssh admin@SWITCH Enter configuration mode on the switch.\nAruba\nsw# configure terminal Mellanox\nsw# enable sw# configure terminal For each UAN port identified with UAN CAN (UAN_PORT) find the associated LAG (bond) (UAN_LAG).\nAruba\nsw# show running-config interface 1/1/UAN_PORT Example output:\ninterface 1/1/UAN_PORT no shutdown mtu 9198 description uan001:ocp:2\u0026lt;==sw-spine-001 lag UAN_LAG Mellanox\nsw# show running-config interface ethernet 1/UAN_PORT Example output:\ninterface ethernet 1/UAN_PORT speed 40G force interface ethernet 1/UAN_PORT mlag-channel-group UAN_LAG mode active interface ethernet 1/UAN_PORT description \u0026#34;uan001:ocp:2\u0026#34; For each UAN LAG (bond) identified (UAN_LAG) on the switch, allow the port to access only the VLAN identified in a previous step with the CAN_VLAN.\nAruba\nsw# interface lag UAN_LAG multi-chassis sw# vlan trunk allowed CAN_VLAN Mellanox\nsw# interface mlag-port-channel UAN_LAG switchport hybrid allowed-vlan CAN_VLAN Save the switch configuration.\nAruba\nsw# write memory Mellanox\nsw# write memory "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/bican_enable/",
	"title": "Enabling Customer High Speed Network Routing",
	"tags": [],
	"description": "",
	"content": "Enabling Customer High Speed Network Routing Prerequisites Configuration tasks Configure SLS Configure UAN Minimize UAN downtime Configure UAI Configure compute nodes Retrieve SLS data as JSON Add compute IP addresses to CHN SLS data Upload migrated SLS file to SLS service Enable CFS layer Configure NCNs Configure the API gateways Validation tasks Validating SLS Validating UAN Validating UAI Validate compute nodes Validate NCNs Validate the API gateways Disabling VLAN7 for UANs Prerequisites Many of the procedures use the Cray Command Line Interface. These can be identified by their use of the cray command. For these procedures, the Cray CLI must be configured on the node where the procedure is being followed. See Configure the Cray CLI. Some of the procedures require the CSM documentation RPM to be installed on the node where the procedure is being done. This is called out in the procedures where it is a requirement. For information on installing the RPM, see Check for Latest Documentation. Configuration tasks Configure SLS Configuration for the default route of a BICAN enabled system is contained in the System Layout Service (SLS) BICAN data structure in the SystemDefaultRoute attribute value. This structure was created and its value set during Prerequisites Stage 0.2, and can be either CAN or CHN.\nThe commands in this section can be run on any master or worker NCN with the CSM documentation RPM installed. See Check for Latest Documentation.\nFor CSM 1.2, the recommended value for the BICAN SystemDefaultRoute is CAN. This allows continued use of UAN, UAI, and API resources over the CAN and allows a fully supported transition to CHN in a later CSM release. To update SLS with CAN as the SystemDefaultRoute:\nncn-mw# /usr/share/doc/csm/scripts/operations/bifurcated_can/bican_route.py --route CAN Example output:\nSetting SystemDefaultRoute to CAN Preview: High Speed Network access by users to UAN, UAI, and API resources is the CHN option. This is available during the CSM 1.2 release for those who wish to forge ahead of release-supported features. To set and use the CHN in SLS, update the SystemDefaultRoute with:\nncn-mw# /usr/share/doc/csm/scripts/operations/bifurcated_can/bican_route.py --route CHN Example output:\nSetting SystemDefaultRoute to CHN Configure UAN During the next CFS run, the UANs will be configured to the network set in the SLS BICAN SystemDefaultRoute attribute, if the following Ansible variable is set.\nuan_can_setup: yes\nFor more information, see the \u0026ldquo;HPE Cray User Access Node (UAN) Software Administration Guide (S-8033)\u0026rdquo; document on the HPE Support Center.\nMinimize UAN downtime UANs running before and during an upgrade to CSM 1.2 will continue running with no connectivity or local data impacts until an administrator-scheduled transition takes place. While access to currently running UANs continues during the upgrade, UAN rebuilds and reboots not supported.\nThe time frame over which the transition can be scheduled is quite large and the transition requires only that UAN users log out of the UAN (over the old IPv4 address) and log back in (over a new IPv4 address). The following diagram illustrates the UAN timeline before, during, and after the CSM 1.2 upgrade.\nConcretely, users on a running UAN may be transitioned from the CMN to the new CAN between the two following upgrade points:\nAfter SLS Upgrade has been completed. Before UANs are booted with new images. Configure UAI Newly created User Access Instances (UAI) will use the network configured as the SystemDefaultRoute in the SLS BICAN network.\nExisting UAIs will continue to use the network that was set when it was created.\nConfigure compute nodes Prerequisites for this task:\nCray Operating System, Slingshot Host Software, and Slingshot have been installed and configured. Egress connection from compute nodes to site resources (e.g. license server) is required. A NAT device is not in place to enable use of HSN IP addresses. The CHN subnet is large enough to contain all compute nodes. Retrieve SLS data as JSON Obtain a token.\nncn-mw# export TOKEN=$(curl -s -k -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Create a working directory.\nncn-mw# mkdir /root/sls_chn_ips \u0026amp;\u0026amp; cd /root/sls_chn_ips Extract SLS data to a file.\nncn-mw# curl -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api-gw-service-nmn.local/apis/sls/v1/dumpstate | jq -S . \u0026gt; sls_input_file.json Add compute IP addresses to CHN SLS data The CSM documentation RPM must be installed on the node where this command is run. See Check for Latest Documentation.\nProcess the SLS file:\nncn-mw# DOCDIR=/usr/share/doc/csm/upgrade/1.2/scripts/sls ncn-mw# ${DOCDIR}/add_computes_to_chn.py --sls-input-file sls_input_file.json The default output file name will be chn_with_computes_added_sls_file.json, but can be changed by using the flag --sls-output-file with the script.\nUpload migrated SLS file to SLS service If the following command does not complete successfully, check if the TOKEN environment variable is set correctly.\nncn-mw# curl --fail -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -k -L -X POST \u0026#39;https://api-gw-service-nmn.local/apis/sls/v1/loadstate\u0026#39; -F \u0026#39;sls_dump=@chn_with_computes_added_sls_file.json\u0026#39; Enable CFS layer CHN network configuration of compute nodes is performed by the UAN CFS configuration layer. This procedure describes how to identify the UAN layer and add it to the compute node configuration.\nDetermine the CFS configuration in use on the compute nodes.\nIdentify the compute nodes.\nncn# cray hsm state components list --role Compute --format json | jq -r \u0026#39;.Components[] | .ID\u0026#39; Example output:\nx1000c5s1b0n1 x1000c5s1b0n0 x1000c5s0b0n0 x1000c5s0b0n1 Identify CFS configuration in use on the compute nodes.\nncn# cray cfs components describe --format toml x1000c5s1b0n1 Example output:\nconfigurationStatus = \u0026#34;configured\u0026#34; desiredConfig = \u0026#34;cos-config-full-2.3-integration\u0026#34; enabled = true errorCount = 0 id = \u0026#34;x1000c5s1b0n1\u0026#34; Extract the CFS configuration.\nncn# cray cfs configurations describe cos-config-full-2.3-integration --format json | jq \u0026#39;del(.lastUpdated) | del(.name)\u0026#39; \u0026gt; cos-config-full-2.3-integration.json Identify the UAN CFS configuration.\nIdentify the UAN nodes.\nncn# cray hsm state components list --role Application --subrole UAN --format json | jq -r \u0026#39;.Components[] | .ID\u0026#39; Example output:\nx3000c0s25b0n0 x3000c0s16b0n0 x3000c0s15b0n0 Identify the UAN CFS configuration in use.\nncn# cray cfs components describe --format toml x3000c0s25b0n0 Example output:\nconfigurationStatus = \u0026#34;configured\u0026#34; desiredConfig = \u0026#34;chn-uan-cn\u0026#34; enabled = true errorCount = 0 id = \u0026#34;x3000c0s25b0n0\u0026#34; Identify the UAN CFS configuration layer.\nncn# cray cfs configurations describe chn-uan-cn --format json The resulting output should look similar to this. Installed products, versions, and commit hashes will vary.\n{ \u0026#34;lastUpdated\u0026#34;: \u0026#34;2022-05-27T20:15:10Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;359611be2f6893ddd0020841b73a3d4924120bb1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;chn-uan-cn\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;chn-uan-cn\u0026#34; } Edit the extracted compute node configuration and add the UAN layer to it.\nUpdate the compute node CFS configuration.\nncn# cray cfs configurations update cos-config-full-2.3-integration --file cos-config-full-2.3-integration.json --format toml Example output:\nlastUpdated = \u0026#34;2022-05-27T20:47:18Z\u0026#34; name = \u0026#34;cos-config-full-2.3-integration\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/ slingshot-host-software-config-management.git\u0026#34; commit = \u0026#34;dd428854a04a652f825a3abbbf5ae2ff9842dd55\u0026#34; name = \u0026#34;shs-integration\u0026#34; playbook = \u0026#34;shs_mellanox_install.yml\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34; commit = \u0026#34;92ce2c9988fa092ad05b40057c3ec81af7b0af97\u0026#34; name = \u0026#34;csm-1.9.21\u0026#34; playbook = \u0026#34;site.yml\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/cos-config-management.git\u0026#34; commit = \u0026#34;dd2bcbb97e3adbfd604f9aa297fb34baa0dd90f7\u0026#34; name = \u0026#34;cos-compute-integration-2.3.75\u0026#34; playbook = \u0026#34;cos-compute.yml\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/sma-config-management.git\u0026#34; commit = \u0026#34;2219ca094c0a2721f3bf52f5bd542d8c4794bfed\u0026#34; name = \u0026#34;sma-base-config\u0026#34; playbook = \u0026#34;sma-base-config.yml\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/sma-config-management.git\u0026#34; commit = \u0026#34;2219ca094c0a2721f3bf52f5bd542d8c4794bfed\u0026#34; name = \u0026#34;sma-ldms-ncn\u0026#34; playbook = \u0026#34;sma-ldms-ncn.yml\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/slurm-config-management.git\u0026#34; commit = \u0026#34;0982661002a857d743ee5b772520e47c97f63acc\u0026#34; name = \u0026#34;slurm master\u0026#34; playbook = \u0026#34;site.yml\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/pbs-config-management.git\u0026#34; commit = \u0026#34;874050c9820cc0752c6424ef35295289487acccc\u0026#34; name = \u0026#34;pbs master\u0026#34; playbook = \u0026#34;site.yml\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/analytics-config-management.git\u0026#34; commit = \u0026#34;d4b26b74d08e668e61a1e5ee199e1a235e9efa3b\u0026#34; name = \u0026#34;analytics integration\u0026#34; playbook = \u0026#34;site.yml\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/cos-config-management.git\u0026#34; commit = \u0026#34;dd2bcbb97e3adbfd604f9aa297fb34baa0dd90f7\u0026#34; name = \u0026#34;cos-compute-last-integration-2.3.75\u0026#34; playbook = \u0026#34;cos-compute-last.yml\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git\u0026#34; commit = \u0026#34;359611be2f6893ddd0020841b73a3d4924120bb1\u0026#34; name = \u0026#34;chn-uan-cn\u0026#34; playbook = \u0026#34;site.yml\u0026#34; Check that CFS configuration of the compute node completes successfully.\nUpdating the CFS configuration will cause CFS to schedule the nodes for configuration. Run the following command to verify this has occurred.\nncn# cray cfs components describe --format toml x1000c5s1b0n1 Example output:\nconfigurationStatus = \u0026#34;pending\u0026#34; desiredConfig = \u0026#34;cos-config-full-2.3-integration\u0026#34; enabled = true errorCount = 0 id = \u0026#34;x1000c5s1b0n1\u0026#34; state = [] [tags] configurationStatus should change from pending to configured once CFS configuration of the node is complete.\nFor more information on managing node with CFS, see the Configuration Management documentation.\nConfigure NCNs Prerequisites for this task:\nCSM NCN personalization has been configured. Cray Operating System, Slingshot Host Software, and Slingshot have been installed and configured. Determine the CFS configuration in use on the worker nodes.\nIdentify the worker nodes.\nncn# cray hsm state components list --role Management --subrole Worker --format json | jq -r \u0026#39;.Components[] | .ID\u0026#39; Example output:\nx3000c0s4b0n0 x3000c0s6b0n0 x3000c0s5b0n0 x3000c0s7b0n0 Identify CFS configuration in use on the worker nodes.\nncn# cray cfs components describe --format toml x3000c0s4b0n0 Example output:\nconfigurationStatus = \u0026#34;configured\u0026#34; desiredConfig = \u0026#34;ncn-personalization\u0026#34; enabled = true errorCount = 0 id = \u0026#34;x3000c0s4b0n0\u0026#34; Extract the CFS configuration.\nncn# cray cfs configurations describe ncn-personalization --format json | jq \u0026#39;del(.lastUpdated) | del(.name)\u0026#39; \u0026gt; ncn-personalization.json The resulting output file should look similar to this. Installed products, versions, and commit hashes will vary.\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/slingshot-host-software-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;f4e2bb7e912c39fc63e87a9284d026a5bebb6314\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;shs-1.7.3-45-1.0.26\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;shs_mellanox_install.yml\u0026#34; }, { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;92ce2c9988fa092ad05b40057c3ec81af7b0af97\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;csm-1.9.21\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; }, { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/sat-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;4e14a37b32b0a3b779b7e5f2e70998dde47edde1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;sat-2.3.4\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;sat-ncn.yml\u0026#34; }, { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/cos-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;dd2bcbb97e3adbfd604f9aa297fb34baa0dd90f7\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cos-integration-2.3.75\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;ncn.yml\u0026#34; }, { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/cos-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;dd2bcbb97e3adbfd604f9aa297fb34baa0dd90f7\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cos-integration-2.3.75\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;ncn-final.yml\u0026#34; } ] } Edit the extracted file and take the existing CSM layer and create an new layer to run the enable_chn.yml playbook.\n{ \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;92ce2c9988fa092ad05b40057c3ec81af7b0af97\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;csm-1.9.21\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;enable_chn.yml\u0026#34; } Important: This new layer must run after the SHS and COS ncn-final.yml layers, otherwise the HSN interfaces will not be configured correctly and this playbook will fail.\nUpdate the NCN personalization configuration.\nncn# cray cfs configurations update ncn-personalization --file ncn-personalization.json --format toml Example output:\nlastUpdated = \u0026#34;2022-05-25T09:22:44Z\u0026#34; name = \u0026#34;ncn-personalization\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/ slingshot-host-software-config-management.git\u0026#34; commit = \u0026#34;f4e2bb7e912c39fc63e87a9284d026a5bebb6314\u0026#34; name = \u0026#34;shs-1.7.3-45-1.0.26\u0026#34; playbook = \u0026#34;shs_mellanox_install.yml\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34; commit = \u0026#34;92ce2c9988fa092ad05b40057c3ec81af7b0af97\u0026#34; name = \u0026#34;csm-1.9.21\u0026#34; playbook = \u0026#34;site.yml\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/sat-config-management.git\u0026#34; commit = \u0026#34;4e14a37b32b0a3b779b7e5f2e70998dde47edde1\u0026#34; name = \u0026#34;sat-2.3.4\u0026#34; playbook = \u0026#34;sat-ncn.yml\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/cos-config-management.git\u0026#34; commit = \u0026#34;dd2bcbb97e3adbfd604f9aa297fb34baa0dd90f7\u0026#34; name = \u0026#34;cos-integration-2.3.75\u0026#34; playbook = \u0026#34;ncn.yml\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/cos-config-management.git\u0026#34; commit = \u0026#34;dd2bcbb97e3adbfd604f9aa297fb34baa0dd90f7\u0026#34; name = \u0026#34;cos-integration-2.3.75\u0026#34; playbook = \u0026#34;ncn-final.yml\u0026#34; [[layers]] cloneUrl = \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34; commit = \u0026#34;92ce2c9988fa092ad05b40057c3ec81af7b0af97\u0026#34; name = \u0026#34;csm-1.9.21\u0026#34; playbook = \u0026#34;enable_chn.yml\u0026#34; Check that NCN personalization runs and completes successfully on the worker nodes.\nUpdating the CFS configuration will cause CFS to schedule the nodes for configuration. Run the following command to verify this has occurred.\nncn# cray cfs components describe --format toml x3000c0s4b0n0 Example output:\nconfigurationStatus = \u0026#34;pending\u0026#34; desiredConfig = \u0026#34;ncn-personalization\u0026#34; enabled = true errorCount = 0 id = \u0026#34;x3000c0s4b0n0\u0026#34; state = [] [tags] configurationStatus should change from pending to configured once NCN personalization completes successfully.\nFor more information on managing NCN personalization, see Perform NCN Personalization.\nConfigure the API gateways No additional steps are required to configure the API gateways for CHN.\nIf CHN is selected during CSM installation or upgrade, then the customer-high-speed MetalLB pool is defined and the load balancers configured with IP addresses from this pool.\nValidation tasks Validating SLS To display current setting of the SystemDefaultRoute SLS BICAN network, run the following command.\nThe CSM documentation RPM must be installed on the node where this command is run. See Check for Latest Documentation.\nncn-mw# /usr/share/doc/csm/scripts/operations/bifurcated_can/bican_route.py --check Example output:\nConfigured SystemDefaultRoute: CHN Validating UAN Retrieve the CHN network information from SLS.\nncn# cray sls search networks list --name CHN --format json | jq \u0026#39;.[]. ExtraProperties.Subnets[] | select(.Name==\u0026#34;bootstrap_dhcp\u0026#34;) | del(.IPReservations)\u0026#39; Example output:\n{ \u0026#34;CIDR\u0026#34;: \u0026#34;10.103.9.0/25\u0026#34;, \u0026#34;DHCPEnd\u0026#34;: \u0026#34;10.103.9.62\u0026#34;, \u0026#34;DHCPStart\u0026#34;: \u0026#34;10.103.9.16\u0026#34;, \u0026#34;FullName\u0026#34;: \u0026#34;CHN Bootstrap DHCP Subnet\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;10.103.9.1\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;bootstrap_dhcp\u0026#34;, \u0026#34;VlanID\u0026#34;: 5 } Verify that the default route is set correctly on the UAN.\nuan# ip r Example output:\ndefault via 10.103.9.1 dev hsn0 10.92.100.0/24 via 10.252.0.1 dev nmn0 10.100.0.0/17 via 10.252.0.1 dev nmn0 10.103.9.0/25 dev hsn0 proto kernel scope link src 10.103.9.15 10.252.0.0/17 dev nmn0 proto kernel scope link src 10.252.1.16 10.253.0.0/16 dev hsn0 proto kernel scope link src 10.253.0.25 Validating UAI Retrieve the configured CHN subnet from SLS.\nncn# cray sls search networks list --name CHN --format json | jq \u0026#39;.[]. ExtraProperties.Subnets[] | select(.Name==\u0026#34;chn_metallb_address_pool\u0026#34;)\u0026#39; Example output:\n{ \u0026#34;CIDR\u0026#34;: \u0026#34;10.103.9.64/27\u0026#34;, \u0026#34;FullName\u0026#34;: \u0026#34;CHN Dynamic MetalLB\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;10.103.9.65\u0026#34;, \u0026#34;MetalLBPoolName\u0026#34;: \u0026#34;customer-high-speed\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;chn_metallb_address_pool\u0026#34;, \u0026#34;VlanID\u0026#34;: 5 } Verify that UAIs are being created with IP addresses in the correct range.\nncn# cray uas admin uais list --format json | jq -c \u0026#39;.[] | {uai_name, uai_ip}\u0026#39; Example output:\n{\u0026#34;uai_name\u0026#34;:\u0026#34;uai-vers-93f0289d\u0026#34;,\u0026#34;uai_ip\u0026#34;:\u0026#34;10.103.9.69\u0026#34;} {\u0026#34;uai_name\u0026#34;:\u0026#34;uai-vers-9f67ac89\u0026#34;,\u0026#34;uai_ip\u0026#34;:\u0026#34;10.103.9.70\u0026#34;} {\u0026#34;uai_name\u0026#34;:\u0026#34;uai-vers-b773a5d9\u0026#34;,\u0026#34;uai_ip\u0026#34;:\u0026#34;10.103.9.71\u0026#34;} Run the UAI gateway tests.\nThe CSM documentation RPM must be installed on the node where this command is run. See Check for Latest Documentation.\nncn-mw# /usr/share/doc/csm/scripts/operations/gateway-test/uai-gateway-test.sh The test will launch a UAI with the gateway-test image, execute the gateway tests, and then delete the UAI that was launched. The test will complete with an overall test status based on the result of the individual health checks on all of the networks. Example output:\nOverall Gateway Test Status: PASS See Gateway Testing for more information.\nValidate compute nodes Retrieve the CHN network information from SLS.\nncn# cray sls search networks list --name CHN --format json | jq \u0026#39;.[]. ExtraProperties.Subnets[] | select(.Name==\u0026#34;bootstrap_dhcp\u0026#34;) | del(.IPReservations)\u0026#39; Example output:\n{ \u0026#34;CIDR\u0026#34;: \u0026#34;10.103.9.0/25\u0026#34;, \u0026#34;DHCPEnd\u0026#34;: \u0026#34;10.103.9.62\u0026#34;, \u0026#34;DHCPStart\u0026#34;: \u0026#34;10.103.9.16\u0026#34;, \u0026#34;FullName\u0026#34;: \u0026#34;CHN Bootstrap DHCP Subnet\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;10.103.9.1\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;bootstrap_dhcp\u0026#34;, \u0026#34;VlanID\u0026#34;: 5 } Verify that the compute nodes have CHN IP addresses set on the hsn0 interface.\ncn# ip ad show hsn0 Example output:\n3: hsn0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9000 qdisc mq state UP group default qlen 1000 link/ether 02:00:00:00:08:73 brd ff:ff:ff:ff:ff:ff altname enp194s0np0 inet 10.253.0.54/16 scope global hsn0 valid_lft forever preferred_lft forever inet 10.103.9.48/25 scope global hsn0 valid_lft forever preferred_lft forever inet6 fe80::ff:fe00:873/64 scope link valid_lft forever preferred_lft forever Verify that the default route is set correctly on the compute nodes.\ncn# ip route show Example output:\ndefault via 10.103.9.1 dev hsn0 10.92.100.0/24 via 10.100.0.1 dev nmn0 10.100.0.0/22 dev nmn0 proto kernel scope link src 10.100.0.13 10.100.0.0/17 via 10.100.0.1 dev nmn0 10.103.9.0/25 dev hsn0 proto kernel scope link src 10.103.9.48 10.252.0.0/17 via 10.100.0.1 dev nmn0 10.253.0.0/16 dev hsn3 proto kernel scope link src 10.253.0.53 10.253.0.0/16 dev hsn2 proto kernel scope link src 10.253.0.37 10.253.0.0/16 dev hsn1 proto kernel scope link src 10.253.0.38 10.253.0.0/16 dev hsn0 proto kernel scope link src 10.253.0.54 Validate NCNs Retrieve the CHN network information from SLS.\nncn# cray sls search networks list --name CHN --format json | jq \u0026#39;.[]. ExtraProperties.Subnets[] | select(.Name==\u0026#34;bootstrap_dhcp\u0026#34;) | del(.IPReservations)\u0026#39; Example output:\n{ \u0026#34;CIDR\u0026#34;: \u0026#34;10.103.9.0/25\u0026#34;, \u0026#34;DHCPEnd\u0026#34;: \u0026#34;10.103.9.62\u0026#34;, \u0026#34;DHCPStart\u0026#34;: \u0026#34;10.103.9.16\u0026#34;, \u0026#34;FullName\u0026#34;: \u0026#34;CHN Bootstrap DHCP Subnet\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;10.103.9.1\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;bootstrap_dhcp\u0026#34;, \u0026#34;VlanID\u0026#34;: 5 } Verify that the worker nodes have CHN IP addresses set on the hsn0 interface.\nncn# pdsh -w $(grep -oP \u0026#39;ncn-w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) \\ \u0026#39;ip ad show hsn0 | grep inet\\ \u0026#39; | dshbak -c Example output:\n---------------- ncn-w001 ---------------- inet 10.253.0.21/16 brd 10.253.255.255 scope global hsn0 inet 10.103.9.10/25 scope global hsn0 ---------------- ncn-w002 ---------------- inet 10.253.0.3/16 brd 10.253.255.255 scope global hsn0 inet 10.103.9.9/25 scope global hsn0 ---------------- ncn-w003 ---------------- inet 10.253.0.19/16 brd 10.253.255.255 scope global hsn0 inet 10.103.9.8/25 scope global hsn0 ---------------- ncn-w004 ---------------- inet 10.253.0.1/16 brd 10.253.255.255 scope global hsn0 inet 10.103.9.7/25 scope global hsn0 Validate the API gateways Check that the istio-ingressgateway-chn API gateway has an IP address.\nncn-mw# kubectl -n istio-system get svc istio-ingressgateway-chn Example output:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway-chn LoadBalancer 10.23.158.228 10.103.9.65 80:30126/TCP,443:31972/TCP 74d Run the NCN gateway health checks.\nThe CSM documentation RPM must be installed on the node where this command is run. See Check for Latest Documentation.\nncn-mw# /usr/share/doc/csm/scripts/operations/gateway-test/ncn-gateway-test.sh The test will complete with an overall test status based on the result of the individual health checks on all of the networks. Example output:\nOverall Gateway Test Status: PASS See Gateway Testing for more information.\nDisabling VLAN7 for UANs After updating to CSM 1.2, the UAN nodes need to have their access to the CMN (VLAN7) removed. The procedure to Remove UAN Access to the CMN restricts access to UAN by removing VLAN 7 from the switch ports.\n"
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/bican_support_matrix/",
	"title": "BICAN Support Matrix - Shasta Customer Access Networks",
	"tags": [],
	"description": "",
	"content": "BICAN Support Matrix - Shasta Customer Access Networks Data sheet - Shasta networking - Customer Access Networks Overview Customer Access Networks provide the interface between Shasta system networking and the customer site network. Customer Access Networks (CANs) are routed networks with broadcast domain separation. Customer Access Networks provide higher availability and more flexibility in accessing cloud services compared to traditional \u0026ldquo;bastion hosts\u0026rdquo;, and are more in line with the cloud-native architecture of Shasta as whole.\nCustomer Access Networks (CANs) provide flexible networking at the edge between the site and Shasta system to do the following:\nPerform administrative tasks on the system. Run jobs and move job data to and from the system. Access site resources like DNS and LDAP from the system. Feature access matrix For CSM 1.2, the notion of the CAN has been expanded to meet customer requests for increased flexibility and policy control.\nSystem Resource Traffic to and from System Management Network or CAN High Speed Network CHN Management Network CMN System Cloud Resources (APIs) Ingress Jobs-related APIs Jobs-related APIs Administrative APIs Application Node Servers (UAI, UAN, re-purposed CN) Ingress Allowed Allowed Not Allowed Non-Compute Node (NCN) Servers Ingress Not Allowed Not Allowed Allowed System Access to External/Site (LDAP, DNS) Egress Allowed Allowed Not Allowed Selection of user access for job control and data movement over the Shasta Management Network (CAN) or the High Speed Network (CHN) is made during system installation or upgrade.\nCreation of the Customer Management Network (CMN) during installation or upgrade is mandatory.\nNetwork overview Internal networks Node Management Network (NMN) - Provides the internal control plane for systems management and jobs control. Hardware Management Network (HMN) - Provides internal access to system baseboard management controllers (BMC/iLO) and other lower-level hardware access. External and edge networks Customer Management Network (CMN) - Provides customer access from the site to the system for administrators. Customer Access Network (CAN) or Customer High Speed Network (CHN) provide: Customer access from the site to the system for job control and jobs data movement. Access from the system to the site for network services like DNS, LDAP, etc. Supported configurations Option A: CMN + CAN (Management Network only - Layer 2 separation) Option B: CMN + CHN (Administration over Management Network, User Access over High Speed Network) Note: During installation, the High Speed Network is not configured until relatively late in the process. Installation generally requires site access for deployment artifacts, site DNS, etc. In order to achieve this, the CMN is used during the installation process for system traffic egress until the High Speed Network is available.\nNetwork capabilities Layer 2 CMN, CAN, and CHN have broadcast boundaries at the edge between the system and the site. Layer 3 Addressing IPv4 supported (default) IPv6 roadmap Routing Static routes (default) exist on the edge router/switches. Dynamic routing (OSPF) is possible at the edge. Network sizing and requirements CMN sizing and requirements CMN IPv4 sizing and requirements Site routable Contiguous (CIDR block) Non-overlapping with internal networks (configurable during installation) Size estimate is the sum of: Number of Non-Compute Nodes (NCNs) of type master, worker, or storage used by the Kubernetes cluster Number of switches on the Management Network Number of administrative API endpoints Several administrative addresses for switch interfaces and routing. A /26 block is typically sufficient for systems less than approximately 4000 nodes. CAN or CHN sizing and requirements CAN or CHN IPv4 sizing and requirements Site routable Contiguous (CIDR block) Non-overlapping with internal networks (configurable during installation) Size estimate is the sum of: Number of application nodes requiring access from the site (User Access Node (UAN), login nodes, etc.) Number of User Access Instances (UAI) in Kubernetes (if used). Number of API endpoints Several administrative addresses for switch interfaces and routing NOTE: CAN or CHN sizing is largely dependent on customer-specific use cases and application node hardware. "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/bican_technical_details/",
	"title": "Bifurcating the CAN - CSM 1.2 Feature Details",
	"tags": [],
	"description": "",
	"content": "Bifurcating the CAN - CSM 1.2 Feature Details CAN new features overview Customer High Speed Network (CHN) CHN system ingress endpoints accessible in CSM 1.2 CHN system egress endpoints accessible in CSM 1.2 Endpoint naming Touchpoints: effects and changes When naming occurs Ability to change post-install Endpoint addressing Touchpoints: effects and changes When addressing occurs Ability to change post-install Traffic separation and routing Touchpoints: effects and changes When configuration occurs Ability to change post-install Customer Management Management Network (CMN) Traffic separation and routing Endpoint naming Endpoint addressing Changes Touchpoints: effects and changes When configuration occurs Ability to change post-install Customer Access - external/site access Traffic separation and routing Changes Touchpoints: effects and changes When configuration occurs Ability to change post-install 1 CAN new features overview Bifurcation or splitting of the Customer Access Network (CAN) enables customization of customer traffic to and from the system. Customization will be performed during installation. For CSM 1.2 there are two new customer access networks being introduced as part of the process to split the existing monolithic CAN:\nHigh Speed Customer Access - CHN (Customer High Speed Network) : This feature adds the ability to connect to User Application Nodes (UAN), UAI, Compute Nodes and Kubernetes API endpoints from the customer site via the High Speed Network (HSN). Management Customer Access - CMN (Customer Management Network) : Using a new VLAN on the Management Network, this feature allows system administrative access from the customer site. Administrative access was previously available on the original CAN; this feature provides a traffic path and access split. Enabling BICAN will remove the original CAN.\nDuring installation, the opportunity to enable the new features will be presented.\nAt this time, the customers must accept:\nExternal administrator access to administrative endpoints moves to the Customer Management Network (CMN). External user access to user endpoints moves to the Customer High-speed Network (CHN). The CHN is now the default ingress network. Customer access over the legacy CAN is disabled by default. Reverting or changing any decisions will be manual.\nFor the feature matrix of the CAN, see BICAN Support Matrix. Details of the High Speed CAN (CHN) and the Management CAN (CMN) are described below.\n2 Customer High Speed Network (CHN) Access to system resources from the customer site over the High Speed Network is provided by the High Speed Customer Access Network (CHN). As can be seen in the diagram above, traffic ingress from the site for the CHN is over the edge routers. Typically these are a pair of Arista switches which provide other HSN access \u0026ndash; for ClusterStor, for example.\n2.1 CHN system ingress endpoints accessible in CSM 1.2 Designated Application Nodes, particularly UAN, over SSH. Designated Compute Nodes (CN), including those used for Compute as UAN, over SSH. Kubernetes API endpoints over https. 2.2 CHN system egress endpoints accessible in CSM 1.2 System access to site external resources , including LDAP and DNS servers. 2.3 Endpoint Naming A .chn DNS suffix will be used for all endpoints accessed over the CHN. Endpoints naming will be resolved and maintained in the system authoritative DNS (another CSM 1.2 feature). As part of the introduction of authoritative DNS endpoints, endpoints will also have a top-level-domain appended, creating a fully qualified domain system.\nExamples:\nuan01.chn.tld as resolved externally. uan01.chn can be resolved internal to the system (maintained via local resolv.conf). nid000001.chn.tld api-gateway-service.chn.tld Where tld is configurable at installation and can be a subdomain of the site domain system. Exchange of system DNS with the site may be via delegation (preferred) or zone transfer (AXFR).\nOnce added to CSI, names and IP addresses will use the standard CSM data flow, end up in SLS, and be available for use via both DNS and DHCP services.\n2.3.1 Touchpoints: effects and changes Will require installation and administration document changes. The CHN requires a small change in CSI to add this network. Will automatically use the DNS infrastructure from previous CSM install. Name aliases can be added, changed, and removed via the API to SLS and become available in DNS automatically. DNS tooling for this was released in Shasta v1.4 with SLS 2.3.2 When naming occurs Installation as part of Cray Site Initialization (CSI) data. During site customizations. 2.3.3 Ability to change post-install Yes - Add, remove, and change aliases. No - Change FQDN and domain suffixes. 2.4 Endpoint addressing For the CSM 1.2 release, CHN endpoints will have IPv4 addressing only, with IPv6 introduction in a future release. The current limitation to system introduction of IPv6 is Kubernetes Weave, as well as a vast amount of system configuration and testing required to certify IPv6 system-wide.\nThe CHN will by default have a private IPv4 address block. This is intended to be changed during installation to a customer-supplied IPv4 address block.\n2.4.1 Touchpoints: effects and changes Installer and documentation changes to support new network and path as part of configuration. CSI for network generation and initial configuration. NCN images to support additional subnets and routing. CFS configurations for CN and UAN addressing and routing. UAI to support changes to addressing and routing. MetalLB to create new API endpoints and peer with edge router. Arista switch pair to create a new BGP routing instance, and add or modify existing virtual routing instance for path and access control. HSN required for transport of application traffic, so new procedures need to be developed for troubleshooting and support. 2.4.2 When addressing occurs Installation, as part of CSI data. 2.4.3 Ability to change post-install No 2.5 Traffic separation and routing In CSM 1.2, there is Layer 3 separation internal to the system but co-mingled Layer 2 between the CHN IPv4 addressing and the internal HSN private IPv4 addresses. Isolation will be within the Slingshot network as well as separated at the edge router.\n2.5.1 Touchpoints: effects and changes Edge router provides all routing and access controls in CSM 1.2 (via a virtual routing instance, if Arista switch pair is used). Internal to the system, CHN traffic will exist in the same Layer 2 domain with internal HSN traffic until the Slingshot network supports VLAN separation. Compute Node (CN) and Application Node (UAN, in this case) configuration or IPv4 addressing and routing is via CFS. When multiple HSN interfaces exist, the CHN will be configured on the HSN0 NIC. UAI addressing and routing over the HSN interfaces for the worker NCNs is required. API endpoints in MetalLB for the CHN will be accessible over worker NCN HSN interfaces (via ECMP Layer 3 routing). MetalLB will peer with the edge routers to supply load balanced API access. 2.5.2 When configuration occurs Installation, as part of a virtual routing instance on the edge routers. 2.5.3 Ability to change post-install Not Recommended - Edge router controls external access. No - Node images could be changed, but routing and IP address changes to CFS configurations would need extensive testing to certify. 3 Customer Management Management Network (CMN) The original CAN released in Shasta 1.1 contained the ability to access NCN workers, masters, and storage directly via SSH for administrative purposes. This administrative traffic was co-mingled with general user traffic for jobs. Based on customer requests, a new mechanism for administrative access to workers, masters, and storage nodes has been added in CSM 1.2. The new Customer Management Network (CMN) is created as a separate and distinct VLAN and subnet on the Management Network, and uplinks at the edge to the customer network. This new CMN network allows SSH into the NCNs and CAN access is disallowed. NOTE: This is generally for ingress access for administrative purposes.\n3.1 Traffic separation and routing Enabling the CMN at installation time will have the following effects:\nAdding uplinks to the customer site similar to the original CAN. Creation of a new VLAN on the Management Network. Adding a new subnet and routing to the Management Network and NCNs. 3.2 Endpoint naming If enabled during installation, a .cmn suffix will be generated by CSI and used in SLS and authoritative DNS. The \u0026ldquo;plumbing\u0026rdquo; of this will occur as previously described in the CHN section.\nExamples:\nSSH ncn-w001.cmn.tld 3.3 Endpoint addressing For the CSM 1.2 release, the CMN is only available via customer-supplied IPv4 addressing.\n3.4 Changes 3.4.1 Touchpoints: effects and changes CMN required beginning with CSM 1.2. Customer will supply a subnet similar to the way the CAN is deployed. Sizing is the number of NCNs plus a couple more addresses. Edge access to the CMN will need to be configured with the customer site. ACL and route updates on the edge switches. Arista guide. MetalLB to Arista peering guide. The Management network will require the following changes: Addition of the new CMN VLAN. This should be similar to the existing CAN configuration. Termination of the new CMN VLAN on ports supporting NCNs. Addition of customer-supplied CMN IP addresses to the management switches to support routing. NCN workers, masters, and storage will require the following changes: Image support for CMN VLAN, addressing, and routing. 3.4.2 When configuration occurs During installation, as part of CSI data generation. During installation, as part of Management Network configuration. During installation, as part of NCN deployment. 3.4.3 Ability to change post-install Not recommended, and would be manual. 4 Customer Access - external/site access System access to site or external resources (such as the Internet, site DNS servers, and site LDAP servers) was previously provided over the CAN. By default this CAN access path remains, but for the CSM 1.2 release it is possible during installation to select system-to-site access over the CHN or CMN.\n4.1 Traffic separation and routing At installation time one of the following egress routes from the system to the site must be selected: CAN (default), CHN, CMN.\n4.2 Changes 4.2.1 Touchpoints: effects and changes Installer customization changes: Management Network changes possibly for routing, but new ACLs may be necessary. NCNs will require specific site routes to prioritize selected path over the system default (CAN). Dependent on CHN and CMN work. 4.2.2 When configuration occurs During installation, as part of site configuration. 4.2.3 Ability to change post-install No "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/bican_technical_summary/",
	"title": "BICAN Summary",
	"tags": [],
	"description": "",
	"content": "BICAN Summary Bifurcated CAN was designed to separate administrative network traffic and user network traffic.\nBICAN terminology BICAN – Bifurcated Customer Access Network CAN – Customer Access Network CMN – Customer Management Network CHN – Customer High Speed Network NMN – Node Management Network HMN – Hardware Management Network BICAN features Bifurcation or splitting of the Customer Access Network (CAN) enables customization of customer traffic to and from the system. Customization will be performed during installation. In CSM 1.2, as part of the process to split the existing monolithic CAN, two new customer access networks are introduced: High Speed Customer Access - CHN: This feature adds the ability to connect to Application Nodes (UAN), UAI, Compute Nodes, and Kubernetes API endpoints from the customer site via the High Speed Network (HSN). Management Customer Access - CMN: Using a new VLAN on the Management Network, this feature allows system administrative access from the customer site. Administrative access was previously available on the original CAN; this feature provides a traffic path and access split. "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/known_issues/",
	"title": "known issues",
	"tags": [],
	"description": "",
	"content": "known issues Topics:\nGigabyte BMC Missing Redfish Data Hang Listing BOS Sessions Multiple Console Node Pods on the Same Worker Nexus Fails Authentication with Keycloak Users SLS Not Working During Node Rebuild Known Issue admin-client-auth Not Found SAT/HSM/CAPMC Component Power State Mismatch Cray CLI 403 Forbidden Errors HMS Discovery Job Not Creating RedfishEndpoints In Hardware State Manager Etcd Cluster Backup Fails Due to Timeout Known Issue Logging into the Gitea web UI requires logging in twice HPE iLO dropping event subscriptions and not properly transitioning power state in CSM software Known Issue initrd.img.xz Not Found Kafka Failure after CSM 1.2 Upgrade Kubernetes Master or Worker node\u0026rsquo;s root filesystem is out of space Mellanox lacp-individual Limitations Common Platform CA Issues Spire database connection pool configuration in an air-gapped environment Spire Database Cluster DNS Lookup Failure wait for unbound hang "
},
{
	"uri": "/docs-csm/en-12/troubleshooting/kubernetes/",
	"title": "kubernetes",
	"tags": [],
	"description": "",
	"content": "kubernetes Topics:\nKubernetes Log File Locations Kubernetes Troubleshooting Information Troubleshoot Kubernetes Master or Worker node in NotReady state Troubleshoot Kubernetes Pods Not Starting Troubleshoot Liveliness or Readiness Probe Failures Troubleshoot Unresponsive kubectl Commands "
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/resource_material/k8s/",
	"title": "k8s",
	"tags": [],
	"description": "",
	"content": "k8s Topics:\nWorker-Specific Manual Steps "
},
{
	"uri": "/docs-csm/en-12/operations/preinstall/",
	"title": "preinstall",
	"tags": [],
	"description": "",
	"content": "preinstall Topics:\nFresh Install Setting NodeBMC and RouterBMC Redfish Credentials Change Credentials on ServerTech PDUs Pre-Install Steps "
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/scripts/",
	"title": "scripts",
	"tags": [],
	"description": "",
	"content": "scripts Topics:\nsls upgrade "
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/scripts/sls/",
	"title": "sls",
	"tags": [],
	"description": "",
	"content": "sls Topics:\nUpgrade SLS Offline from CSM 1.0.x to CSM 1.2 SLS Updates Expert mode sls updater.py Technical Details sls utils "
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/resource_material/storage/",
	"title": "storage",
	"tags": [],
	"description": "",
	"content": "storage Topics:\nCEPHADM "
},
{
	"uri": "/docs-csm/en-12/upgrade/1.2/scripts/upgrade/",
	"title": "upgrade",
	"tags": [],
	"description": "",
	"content": "upgrade Topics:\nUpgrade Automation "
},
{
	"uri": "/docs-csm/en-12/operations/csm_product_management/",
	"title": "CSM product management",
	"tags": [],
	"description": "",
	"content": "CSM product management Topics:\nSecurity Hardening Change Passwords and Credentials Configure CSM packages with CFS Configure Keycloak Account Configure Non-Compute Nodes with CFS Perform NCN Personalization Post-Install Customizations Redeploying a Chart Remove Artifacts from Product Installations Validate Signed RPMs "
},
{
	"uri": "/docs-csm/en-12/operations/system_layout_service/",
	"title": "system layout service",
	"tags": [],
	"description": "",
	"content": "system layout service Topics:\nAdd Liquid-Cooled Cabinets to SLS Add UAN CAN IP Addresses to SLS Add an alias to a service Create a Backup of the SLS Postgres Database Dump SLS Information Load SLS Database with Dump File Restore SLS Postgres Database from Backup Restore SLS Postgres without an Existing Backup System Layout Service (SLS) Update SLS with UAN Aliases "
},
{
	"uri": "/docs-csm/en-12/operations/image_management/",
	"title": "image management",
	"tags": [],
	"description": "",
	"content": "image management Topics:\nBuild a New UAN Image Using the Default Recipe Build an Image Using IMS REST Service Configure IMS to Validate RPMs Convert TGZ Archives to SquashFS Images Create UAN Boot Images Customize an Image Root Using IMS Delete or Recover Deleted IMS Content Image Management Image Management Workflows Import an External Image to IMS Update IMS Job Access Network Upload and Register an Image Recipe "
},
{
	"uri": "/docs-csm/en-12/operations/system_management_health/",
	"title": "system management health",
	"tags": [],
	"description": "",
	"content": "system management health Topics:\nAccess System Management Health Services Configure Prometheus Email Alert Notifications Grafana Dashboards by Component Grafterm Remove Kiali System Management Health System Management Health Checks and Alerts Troubleshoot Grafana Dashboard Troubleshoot Prometheus Alerts "
},
{
	"uri": "/docs-csm/en-12/operations/configuration_management/",
	"title": "configuration management",
	"tags": [],
	"description": "",
	"content": "configuration management Topics:\nAnsible Execution Environments Ansible Inventory Automatic Session Deletion with sessionTTL CFS Flow CFS Global Options CFS Key Management and Permission Denied Errors Change the Ansible Verbosity Logs Configuration Layers Configuration Management Configuration Management of System Components Configuration Management with the CFS Batcher Configuration Sessions Create a CFS Configuration Create a CFS Session with Dynamic Inventory Create an Image Customization CFS Session Create and Populate a VCS Configuration Repository Customize Configuration Values Delete CFS Sessions Enable Ansible Profiling Git Operations Manage Multiple Inventories in a Single Location NCN Worker Image Customization Set Limits for a Configuration Session Set the ansible.cfg for a Session Specifying Hosts and Groups Target Ansible Tasks for Image Customization Track the Status of a Session Troubleshoot Ansible Play Failures in CFS Sessions Troubleshoot CFS Session Failing to Complete Troubleshoot CFS Sessions Failing to Start Update a CFS Configuration Update the Privacy Settings for Gitea Configuration Content Repositories Use a Custom ansible.cfg File Use a Specific Inventory in a Configuration Session VCS Branching Strategy Version Control Service (VCS) View Configuration Session Logs Write Ansible Code for CFS "
},
{
	"uri": "/docs-csm/en-12/operations/hpe_pdu/",
	"title": "hpe pdu",
	"tags": [],
	"description": "",
	"content": "hpe pdu Topics:\nHPE PDU Admin Procedures "
},
{
	"uri": "/docs-csm/en-12/operations/system_configuration_service/",
	"title": "system configuration service",
	"tags": [],
	"description": "",
	"content": "system configuration service Topics:\nConfigure BMC and Controller Parameters with SCSD Manage Parameters with the scsd Service Set BMC Credentials System Configuration Service "
},
{
	"uri": "/docs-csm/en-12/operations/network/dhcp/",
	"title": "dhcp",
	"tags": [],
	"description": "",
	"content": "dhcp Topics:\nDHCP Troubleshoot DHCP Issues "
},
{
	"uri": "/docs-csm/en-12/operations/network/dns/",
	"title": "dns",
	"tags": [],
	"description": "",
	"content": "dns Topics:\nDomain Name Service (DNS) Overview Enable ncsd on UANs Manage the DNS Unbound Resolver PowerDNS Configuration PowerDNS Migration Guide Troubleshoot Common DNS Issues Troubleshoot PowerDNS "
},
{
	"uri": "/docs-csm/en-12/operations/network/external_dns/",
	"title": "external dns",
	"tags": [],
	"description": "",
	"content": "external dns Topics:\nExternal DNS External DNS Failing to Discover Services Workaround External DNS CSI Input Values Ingress Routing Troubleshoot DNS Configuration Issues Troubleshoot Connectivity to Services with External IP addresses Update the cmn-external-dns value post-installation "
},
{
	"uri": "/docs-csm/en-12/operations/network/management_network/firmware/",
	"title": "firmware",
	"tags": [],
	"description": "",
	"content": "firmware Topics:\nUpdate Management Network Firmware "
},
{
	"uri": "/docs-csm/en-12/operations/network/metallb_bgp/",
	"title": "metallb bgp",
	"tags": [],
	"description": "",
	"content": "metallb bgp Topics:\nCheck BGP Status and Reset Sessions MetalLB Configuration MetalLB in BGP-Mode Troubleshoot BGP not Accepting Routes from MetalLB Troubleshoot Services without an Allocated IP Address "
},
{
	"uri": "/docs-csm/en-12/operations/network/customer_accessible_networks/",
	"title": "customer accessible networks",
	"tags": [],
	"description": "",
	"content": "customer accessible networks Topics:\nConnect to the CMN and CAN Customer Accessible Networks CAN/CMN with Dual-Spine Configuration Externally Exposed Services Troubleshoot CMN issues BI-CAN Aruba/Arista Configuration MetalLB Peering with Arista Edge Router "
},
{
	"uri": "/docs-csm/en-12/operations/network/",
	"title": "network",
	"tags": [],
	"description": "",
	"content": "network Topics:\nAccess to System Management Services Connect to the HPE Cray EX Environment Create a CSM Configuration Upgrade Plan Default IP Address Ranges Network customer accessible networks dhcp dns external dns Gateway Testing management network metallb bgp "
},
{
	"uri": "/docs-csm/en-12/operations/kubernetes/",
	"title": "kubernetes",
	"tags": [],
	"description": "",
	"content": "kubernetes Topics:\nAbout Kubernetes Taints and Labels About Postgres About etcd About kubectl Backups for etcd-operator Clusters Kubernetes and Bare Metal EtcD Certificate Renewal Check for and Clear etcd Cluster Alarms Check the Health and Balance of etcd Clusters Clear Space in an etcd Cluster Database Configure kubectl Credentials to Access the Kubernetes APIs containerd Create a Manual Backup of a Healthy etcd Cluster Determine if Pods are Hitting Resource Limits Disaster Recovery for Postgres Increase Kafka Pod Resource Limits Increase Pod Resource Limits Kubernetes Kubernetes Networking Kubernetes Storage Configure Kubernetes API Audit Log Maximum Backups Pod Resource Limits Rebalance Healthy etcd Clusters Rebuild Unhealthy etcd Clusters Recover from Postgres WAL Event Repopulate Data in etcd Clusters When Rebuilding Them Report the Endpoint Status for etcd Clusters Restore Bare-Metal etcd Clusters from an S3 Snapshot Restore Postgres Restore an etcd Cluster from a Backup Retrieve Cluster Health Information Using Kubernetes TDS Lower CPU Requests Troubleshoot Intermittent HTTP 503 Code Failures Troubleshoot Postgres Database View Postgres Information for System Databases "
},
{
	"uri": "/docs-csm/en-12/operations/conman/",
	"title": "conman",
	"tags": [],
	"description": "",
	"content": "conman Topics:\nAccess Compute Node Logs Access Console Log Data Via the System Monitoring Framework (SMF) ConMan Disable ConMan After the System Software Installation Establish a Serial Connection to NCNs Log in to a Node Using ConMan Manage Node Consoles Troubleshoot ConMan Asking for Password on SSH Connection Troubleshoot ConMan Blocking Access to a Node BMC Troubleshoot ConMan Failing to Connect to a Console "
},
{
	"uri": "/docs-csm/en-12/operations/hardware_state_manager/",
	"title": "hardware state manager",
	"tags": [],
	"description": "",
	"content": "hardware state manager Topics:\nAdd a Switch to the HSM Database Add an NCN to the HSM Database Component Group Members Component Groups and Partitions Component Memberships Component Partition Members Create a Backup of the HSM Postgres Database HSM Roles and Subroles Hardware Management Services (HMS) Locking API Hardware State Manager (HSM) Hardware State Manager (HSM) State and Flag Fields Lock and Unlock Management Nodes Manage Component Groups Manage Component Partitions Manage HMS Locks Restore Hardware State Manager (HSM) Postgres Database from Backup Restore Hardware State Manager (HSM) Postgres without an Existing Backup Set BMC Management Roles "
},
{
	"uri": "/docs-csm/en-12/operations/boot_orchestration/",
	"title": "boot orchestration",
	"tags": [],
	"description": "",
	"content": "boot orchestration Topics:\nBOS Workflows Compute Node Boot Issue Symptom Node Console or Logs Indicate that the Server Response has Timed Out Boot Issue Symptom Node HSN Interface Does Not Appear or Show Detected Links Detected Boot Orchestration Boot UANs Check the Progress of BOS Session Operations Clean Up After a BOS/BOA Job is Completed or Cancelled Clean Up Logs After a BOA Kubernetes Job Compute Node Boot Issue Symptom Duplicate Address Warnings and Declined DHCP Offers in Logs Compute Node Boot Issue Symptom Message About Invalid EEPROM Checksum in Node Console or Log Compute Node Boot Issue Symptom Node is Not Able to Download the Required Artifacts Compute Node Boot Sequence Configure the BOS Timeout When Booting Compute Nodes Create a Session Template to Boot Compute Nodes with CPS Edit the iPXE Embedded Boot Script Healthy Compute Node Boot Process Kernel Boot Parameters Limit the Scope of a BOS Session BOS Limitations for Gigabyte BMC Hardware Log File Locations and Ports Used in Compute Node Boot Troubleshooting Manage a BOS Session Manage a Session Template Node Boot Root Cause Analysis Redeploy the iPXE and TFTP Services BOS Session Templates BOS Sessions Stage Changes Without BOS Tools for Resolving Compute Node Boot Issues Troubleshoot Booting Nodes with Hardware Issues Troubleshoot Compute Node Boot Issues Related to Dynamic Host Configuration Protocol (DHCP) Troubleshoot Compute Node Boot Issues Related to Slow Boot Times Troubleshoot Compute Node Boot Issues Related to Trivial File Transfer Protocol (TFTP) Troubleshoot Compute Node Boot Issues Related to Unified Extensible Firmware Interface (UEFI) Troubleshoot Compute Node Boot Issues Related to the Boot Script Service (BSS) Troubleshoot Compute Node Boot Issues Using Kubernetes Troubleshoot UAN Boot Issues Upload Node Boot Information to Boot Script Service (BSS) View the Status of a BOS Session "
},
{
	"uri": "/docs-csm/en-12/operations/security_and_authentication/",
	"title": "security and authentication",
	"tags": [],
	"description": "",
	"content": "security and authentication Topics:\nAPI Authorization Access the Keycloak User Management UI Add LDAP User Federation Add Root Service Account for Gigabyte Controllers Audit Logs Authenticate an Account with the Command Line Backup and Restore Vault Clusters Certificate Types Change Air-Cooled Node BMC Credentials Change Credentials on ServerTech PDUs Change Cray EX Liquid-Cooled Cabinet Global Default Password Change the Keycloak Token Lifetime Set NCN Image Root Password, SSH Keys, and Timezone Set NCN Image Root Password, SSH Keys, and Timezone on PIT Node Change Root Passwords for Compute Nodes Change SNMP Credentials on Leaf-BMC Switches Change the Keycloak Admin Password Change the LDAP Server IP Address for Existing LDAP Server Content Change the LDAP Server IP Address for New LDAP Server Content Configure Keycloak for LDAP/AD authentication Configure the RSA Plugin in Keycloak Create Internal Groups in the Keycloak Shasta Realm Create Internal User Accounts in the Keycloak Shasta Realm Create a Backup of the Keycloak Postgres Database Create a Service Account in Keycloak Default Keycloak Realms, Accounts, and Clients Delete Internal User Accounts in the Keycloak Shasta Realm Get a Long-Lived Token for a Service Account HashiCorp Vault Keycloak Operations Keycloak User Localization Keycloak User Management with kcadm.sh Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster Manage Sealed Secrets Manage System Passwords PKI Certificate Authority (CA) PKI Services Preserve Username Capitalization for Users Exported from Keycloak Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials Public Key Infrastructure (PKI) Recovering from Mismatched BMC Credentials Remove Internal Groups from the Keycloak Shasta Realm Remove the Email Mapper from the LDAP User Federation Remove the LDAP User Federation from Keycloak Restrict Network Access to the ncn-images S3 Bucket Re-Sync Keycloak Users to Compute Nodes Retrieve an Authentication Token Retrieve the Client Secret for Service Accounts Update NCN User SSH Keys System Security and Authentication Transport Layer Security (TLS) for Ingress Services Troubleshoot Common Vault Cluster Issues Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials Update Default ServerTech PDU Credentials used by the Redfish Translation Service (RTS) Set NCN User Passwords Updating the Liquid-Cooled EX Cabinet CEC with Default Credentials after a CEC Password Change "
},
{
	"uri": "/docs-csm/en-12/operations/spire/",
	"title": "spire",
	"tags": [],
	"description": "",
	"content": "spire Topics:\nCreate a Backup of the Spire Postgres Database Restore missing Spire metadata Restore Spire Postgres without an Existing Backup Troubleshoot Spire Failing to Start on NCNs Update Spire Intermediate CA Certificate "
},
{
	"uri": "/docs-csm/en-12/operations/compute_rolling_upgrades/",
	"title": "compute rolling upgrades",
	"tags": [],
	"description": "",
	"content": "compute rolling upgrades Topics:\nCRUS Workflow Compute Rolling Upgrades Troubleshoot Nodes Failing to Upgrade in a CRUS Session Troubleshoot a Failed CRUS Session Because of Bad Parameters Troubleshoot a Failed CRUS Session Because of Unmet Conditions Upgrade Compute Nodes with CRUS "
},
{
	"uri": "/docs-csm/en-12/operations/hmcollector/",
	"title": "hmcollector",
	"tags": [],
	"description": "",
	"content": "hmcollector Topics:\nAdjust HM Collector resource limits and requests "
},
{
	"uri": "/docs-csm/en-12/operations/package_repository_management/",
	"title": "package repository management",
	"tags": [],
	"description": "",
	"content": "package repository management Topics:\nManage Repositories with Nexus Nexus Configuration Nexus Deployment Nexus Export and Restore Nexus Space Cleanup Package Repository Management Package Repository Management with Nexus Repair Yum Repository Metadata Restrict Admin Privileges in Nexus Troubleshoot Nexus "
},
{
	"uri": "/docs-csm/en-12/operations/resiliency/",
	"title": "resiliency",
	"tags": [],
	"description": "",
	"content": "resiliency Topics:\nNTP Resiliency Recreate StatefulSet Pods on Another Node Resilience of System Management Services Resiliency Resiliency Testing Procedure Restore System Functionality if a Kubernetes Worker Node is Down "
},
{
	"uri": "/docs-csm/en-12/operations/firmware/",
	"title": "firmware",
	"tags": [],
	"description": "",
	"content": "firmware Topics:\nFAS Admin Procedures FAS CLI FAS Filters FAS Recipes FAS Use Cases Update Firmware with FAS Updating BMC Firmware and BIOS for ncn-m001 Updating BMC Firmware and BIOS for NCNs without FAS Upload BMC Recovery Firmware into TFTP Server "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/rebuild_ncns/",
	"title": "Rebuild NCNs",
	"tags": [],
	"description": "",
	"content": "Rebuild NCNs Topics:\nFinal Validation Steps Identify Nodes and Update Metadata Post Rebuild Storage Node Validation Power Cycle and Rebuild Nodes Prepare Storage Nodes Re-Add a Storage Node to Ceph Rebuild NCNs Validate Boot Loader Wipe Drives "
},
{
	"uri": "/docs-csm/en-12/operations/utility_storage/",
	"title": "utility storage",
	"tags": [],
	"description": "",
	"content": "utility storage Topics:\nAdding a Ceph Node to the Ceph Cluster Add Ceph OSDs Adjust Ceph Pool Quotas Alternate Storage Pools Ceph Daemon Memory Profiling Ceph Deep Scrubs Ceph Health States Ceph Orchestrator Usage Ceph Service Check Script Usage Ceph Storage Types Cephadm Reference Material Collect Information about the Ceph Cluster Dump Ceph Crash Data Identify Ceph Latency Issues Manage Ceph Services Shrink the Ceph Cluster Restore Nexus Data After Data Corruption Shrink Ceph OSDs Troubleshoot Ceph-Mon Processes Stopping and Exceeding Max Restarts Troubleshoot Ceph MDS Client Connectivity Issues Troubleshooting Ceph MDS Reporting Slow Requests and Failure on Client Troubleshoot Ceph OSDs Reporting Full Troubleshoot Ceph Services Not Starting After a Server Crash Troubleshoot Failure to Get Ceph Health Troubleshoot Insufficient Standby MDS Daemons Available Troubleshoot Large Object Map Objects in Ceph Health Troubleshoot Pods Failing to Restart on Other Worker Nodes Troubleshoot if RGW Health Check Fails Troubleshoot S3FS Mount Issues Troubleshoot System Clock Skew Troubleshoot a Down OSD Troubleshoot an Unresponsive Rados-Gateway (radosgw) S3 Endpoint Utility Storage "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/add_remove_replace_ncns/",
	"title": "Add Remove Replace NCNs",
	"tags": [],
	"description": "",
	"content": "Add Remove Replace NCNs Topics:\nAdd NCN Data Alpha Framework to Add, Remove, Replace, or Move NCNs Add Switch Configuration for NCN Allocate NCN IP Addresses Boot NCN Collect NCN MAC Addresses Redeploy Services Impacted by Adding or Permanently Removing Storage Nodes Remove NCN Data Remove NCN from Role Remove Switch Configuration for NCN Update Firmware Validate Health Validate Added NCN "
},
{
	"uri": "/docs-csm/en-12/operations/node_management/",
	"title": "node management",
	"tags": [],
	"description": "",
	"content": "node management Topics:\nAccess and Update Settings for Replacement NCNs Add Remove Replace NCNs Add TLS Certificates to BMCs Add a Standard Rack Node Add Additional Liquid-Cooled Cabinets to a System Adding a Liquid-cooled Blade to a System Build NCN Images Locally Change Java Security Settings Change Settings for HMS Collector Polling of Air-Cooled Nodes Change Settings in the Bond Check and Set the metal.no-wipe Setting on NCNs Check the BMC Failover Mode Clear Space in Root File System on Worker Nodes Configuration of NCN Bonding Configure NTP on NCNs Disable Nodes Dump a Non-Compute Node Enable Nodes Enable Passwordless Connections to Liquid Cooled Node BMCs Find Node Type and Manufacturer Launch a Virtual KVM on Gigabyte Servers Launch a Virtual KVM on Intel Servers Move a Standard Rack Node Move a Standard Rack Node (Same Rack/Same HSN Ports) Move a liquid-cooled blade within a System NCN Drive Identification Node Management Node Management Workflows Reboot NCNs Rebuild NCNs Removing a Liquid-cooled blade from a System Replace a Compute Blade Reset Credentials on Redfish Devices S3FS Usage and Guidelines for Shasta Swap a Compute Blade with a Different System TLS Certificates for Redfish BMCs Troubleshoot Interfaces with IP Address Issues Troubleshoot Issues with Redfish Endpoint Discovery Troubleshoot Loss of Console Connections and Logs on Gigabyte Nodes Update Compute Node Mellanox HSN NIC Firmware Update the Gigabyte Node BIOS Time Updating Cabinet Routes on Management NCNs Use the Physical KVM Verify Node Removal View BIOS Logs for Liquid-Cooled Nodes Customize PCIe Hardware Customize PCIe Hardware "
},
{
	"uri": "/docs-csm/en-12/scripts/operations/node_management/add_remove_replace_ncns/",
	"title": "Add Remove Replace NCNs",
	"tags": [],
	"description": "",
	"content": "Add Remove Replace NCNs Topics:\nsls utils "
},
{
	"uri": "/docs-csm/en-12/operations/artifact_management/",
	"title": "artifact management",
	"tags": [],
	"description": "",
	"content": "artifact management Topics:\nArtifact Management Generate Temporary S3 Credentials Manage Artifacts with the Cray CLI Use S3 Libraries and Clients "
},
{
	"uri": "/docs-csm/en-12/scripts/operations/node_management/",
	"title": "node management",
	"tags": [],
	"description": "",
	"content": "node management Topics:\nAdd Remove Replace NCNs "
},
{
	"uri": "/docs-csm/en-12/scripts/operations/",
	"title": "operations",
	"tags": [],
	"description": "",
	"content": "operations Topics:\nnode management "
},
{
	"uri": "/docs-csm/en-12/operations/power_management/",
	"title": "power management",
	"tags": [],
	"description": "",
	"content": "power management Topics:\nCray Advanced Platform Monitoring and Control (CAPMC) Ignore Nodes with CAPMC Liquid Cooled Node Power Management Power Off Compute and IO Cabinets Power Off the External Lustre File System Power On Compute and IO Cabinets Power On and Boot Compute and User Access Nodes Power On and Start the Management Kubernetes Cluster Power On the External Lustre File System Prepare the System for Power Off Recover from a Liquid Cooled Cabinet EPO Event Save Management Network Switch Configuration Settings Set the Turbo Boost Limit Shut Down and Power Off Compute and User Access Nodes Shut Down and Power Off the Management Kubernetes Cluster Standard Rack Node Power Management System Power Off Procedures System Power On Procedures User Access to Compute Node Power Data Worker Node COS Power Up Configuration Power Management "
},
{
	"uri": "/docs-csm/en-12/scripts/",
	"title": "scripts",
	"tags": [],
	"description": "",
	"content": "scripts Topics:\noperations workarounds "
},
{
	"uri": "/docs-csm/en-12/scripts/workarounds/",
	"title": "workarounds",
	"tags": [],
	"description": "",
	"content": "workarounds Topics:\nboot-order kdump "
},
{
	"uri": "/docs-csm/en-12/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/docs-csm/en-12/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]