<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rebuild NCNs on Cray System Management (CSM)</title>
    <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/</link>
    <description>Recent content in Rebuild NCNs on Cray System Management (CSM)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-12</language>
    <lastBuildDate>Thu, 24 Oct 2024 03:38:46 +0000</lastBuildDate>
    <atom:link href="/docs-csm/en-12/operations/node_management/rebuild_ncns/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Final Validation Steps</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/final_validation_steps/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:45 +0000</pubDate>
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/final_validation_steps/</guid>
      <description>Final Validation Steps Use this procedure to finish validating the success of rebuilt NCNs.&#xA;Procedure Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig after rebooting the node.&#xA;NOTE: The following command will indicate if a CFS job is currently in progress for this node.&#xA;IMPORTANT: The following command assumes that the variables from the prerequisites section have been set.&#xA;ncn# cray cfs components describe $XNAME --format json Example output:</description>
    </item>
    <item>
      <title>Identify Nodes and Update Metadata</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/identify_nodes_and_update_metadata/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:45 +0000</pubDate>
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/identify_nodes_and_update_metadata/</guid>
      <description>Identify Nodes and Update Metadata Use the following procedure to inspect and modify the Boot Script Service (BSS) boot parameters JSON file.&#xA;This section applies to all node types. The commands in this section assume the variables from the prerequisites section have been set.&#xA;Procedure Generate the BSS boot parameters JSON file.&#xA;Run the following commands from a node that has cray CLI initialized:&#xA;ncn# cray bss bootparameters list --name $XNAME --format=json | jq .</description>
    </item>
    <item>
      <title>Post Rebuild Storage Node Validation</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/post_rebuild_storage_node_validation/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:46 +0000</pubDate>
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/post_rebuild_storage_node_validation/</guid>
      <description>Post Rebuild Storage Node Validation Validate the storage node rebuilt successfully.&#xA;Skip this section if a master or worker node was rebuilt.&#xA;Procedure Verify there are 3 mons, 3 mds, 3 mgr processes, and rgws.&#xA;ncn-m# ceph -s Example output:&#xA;cluster: id: 22d01fcd-a75b-4bfc-b286-2ed8645be2b5 health: HEALTH_OK services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 4m) mgr: ncn-s001(active, since 19h), standbys: ncn-s002, ncn-s003 mds: cephfs:1 {0=ncn-s001=up:active} 2 up:standby osd: 12 osds: 12 up (since 2m), 12 in (since 2m) rgw: 3 daemons active (ncn-s001.</description>
    </item>
    <item>
      <title>Power Cycle and Rebuild Nodes</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/power_cycle_and_rebuild_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:46 +0000</pubDate>
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/power_cycle_and_rebuild_nodes/</guid>
      <description>Power Cycle and Rebuild Nodes This section applies to all node types. The commands in this section assume the variables from the prerequisites section have been set.&#xA;Procedure Open and watch the console for the node being rebuilt.&#xA;Log in to a second session to use it to watch the console using the instructions at the link below:&#xA;Open this link in a new tab or page Log in to a Node Using ConMan</description>
    </item>
    <item>
      <title>Prepare Storage Nodes</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/prepare_storage_nodes/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:46 +0000</pubDate>
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/prepare_storage_nodes/</guid>
      <description>Prepare Storage Nodes Prepare a storage node before rebuilding it.&#xA;IMPORTANT: All of the output examples may not reflect the cluster status where this operation is being performed. For example, if this is a rebuild in place, then Ceph components will not be reporting down, in contrast to a failed node rebuild.&#xA;Prerequisites Procedure Next step Prerequisites Ensure that the latest CSM documentation RPM is installed on ncn-m001.&#xA;See Check for Latest Documentation.</description>
    </item>
    <item>
      <title>Re-Add a Storage Node to Ceph</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/re-add_storage_node_to_ceph/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:46 +0000</pubDate>
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/re-add_storage_node_to_ceph/</guid>
      <description>Re-Add a Storage Node to Ceph Use the following procedure to re-add a Ceph node to the Ceph cluster.&#xA;NOTE: This operation can be done to add more than one node at the same time.&#xA;Add Join Script Copy and paste the below script into /srv/cray/scripts/common/join_ceph_cluster.sh.&#xA;NOTE: This script may also available in the /usr/share/doc/csm/scripts directory where the latest docs-csm RPM is installed. If so, it can be copied from that node to the new storage node being rebuilt and skip to step 2.</description>
    </item>
    <item>
      <title>Rebuild NCNs</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/rebuild_ncns/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:46 +0000</pubDate>
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/rebuild_ncns/</guid>
      <description>Rebuild NCNs Rebuild a master, worker, or storage non-compute node (NCN). Use this procedure in the event that a node has a hardware failure, or some other issue with the node has occurred that warrants rebuilding the node.&#xA;Prerequisites Procedure Validation Prerequisites The system is fully installed and has transitioned off of the LiveCD.&#xA;Variables set with the name of the node being rebuilt and its component name (xname) are required.</description>
    </item>
    <item>
      <title>Validate Boot Loader</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/validate_boot_loader/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:46 +0000</pubDate>
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/validate_boot_loader/</guid>
      <description>Validate Boot Loader Perform the following steps on ncn-m001 (where the latest docs-csm is installed).&#xA;Run the script to ensure the local BOOTRAID has a valid kernel and initrd.&#xA;/usr/share/doc/csm/scripts/check_bootloader_all_ncns.sh Workaround: CASMINST-2015 As a result of rebuilding any NCN(s), remove any dynamically assigned interface IP addresses that did not get released automatically by running the CASMINST-2015 script:&#xA;/usr/share/doc/csm/scripts/CASMINST-2015.sh Once that is done only follow the steps in the section for the node type that was rebuilt:</description>
    </item>
    <item>
      <title>Wipe Drives</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/wipe_drives/</link>
      <pubDate>Thu, 24 Oct 2024 03:38:46 +0000</pubDate>
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/wipe_drives/</guid>
      <description>Wipe Drives WARNING: This is the point of no return. Once the disks are wiped, the node must be rebuilt.&#xA;All commands in this section must be run on the node being rebuilt (unless otherwise indicated). These commands can be done from the ConMan console window.&#xA;Only follow the steps in the section for the node type that is being rebuilt:&#xA;Wipe Disks: Master Wipe Disks: Worker Node Wipe Disks: Utility Storage Node Wipe Disks: Master Unmount the etcd volume and remove the volume group.</description>
    </item>
  </channel>
</rss>
