[
{
	"uri": "/docs-csm/en-09/upgrade/0.9/csm-0.9.5/operations/node_management/check_and_set_the_metalno-wipe_setting_on_ncns/",
	"title": "Check and Set the metal.no-wipe Setting on NCNs",
	"tags": [],
	"description": "",
	"content": "Check and Set the metal.no-wipe Setting on NCNs Configure the metal.no-wipe setting on non-compute nodes (NCNs) to preserve data on the nodes before doing an NCN reboot.\nRun the ncnGetXnames.shscript to view the metal.no-wipe settings for each NCN. The xname and metal.no-wipe settings are also dumped out when executing the ncnHealthChecks.sh script.\nPrerequisites This procedure requires administrative privileges.\nIt also requires that the CSM_SCRIPTDIR variable was previously defined as part of the execution of the steps in the csm-0.9.5 upgrade README. You can verify that it is set by running echo $CSM_SCRIPTDIR on the ncn-m001 cli. If that returns nothing, re-execute the setting of that variable from the csm-0.9.5 README file.\nProcedure Run the ${CSM_SCRIPTDIR}/ncnGetXnames.sh script.\nThe output will include a listing of all of the NCNs, their xnames, and what the metal.no-wipe setting is for each.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnGetXnames.sh\u0026#34; ncn-m001: x3000c0s1b0n0 - metal.no-wipe=1 ncn-m002: x3000c0s2b0n0 - metal.no-wipe=1 ncn-m003: x3000c0s3b0n0 - metal.no-wipe=1 ncn-w001: x3000c0s4b0n0 - metal.no-wipe=1 ncn-w002: x3000c0s5b0n0 - metal.no-wipe=1 ncn-w003: x3000c0s6b0n0 - metal.no-wipe=1 ncn-s001: x3000c0s7b0n0 - metal.no-wipe=1 ncn-s002: x3000c0s8b0n0 - metal.no-wipe=1 ncn-s003: x3000c0s9b0n0 - metal.no-wipe=1 The metal.no-wipe setting must be set to 1 (metal.no-wipe=1) if doing a reboot of an NCN to preserve the current data on it. If it is not set to 1 when the NCN is rebooted, it will be completely wiped and will subsequently have to be rebuilt. If the metal.no-wipe status for one or more NCNs is not returned, re-run the ncnGetXnames.sh script.\nReset the metal.no-wipe settings for any NCN where it is set to 0.\nThis step can be skipped if the metal.no-wipe is already set to 1 for any NCNs being rebooted.\nGenerate a token from any master or worker NCN.\nncn-m001# export TOKEN=$(curl -k -s -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; \\ | base64 -d` https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Update the metal.no-wipe settings.\nncn-m001# csi handoff bss-update-param --set metal.no-wipe=1 Run the ${CSM_SCRIPTDIR}/ncnGetXnames.sh script again to verify the no-wipe settings have been reset as expected.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnGetXnames.sh\u0026#34; ncn-m001: x3000c0s1b0n0 - metal.no-wipe=1 ncn-m002: x3000c0s2b0n0 - metal.no-wipe=1 ncn-m003: x3000c0s3b0n0 - metal.no-wipe=1 ncn-w001: x3000c0s4b0n0 - metal.no-wipe=1 ncn-w002: x3000c0s5b0n0 - metal.no-wipe=1 ncn-w003: x3000c0s6b0n0 - metal.no-wipe=1 ncn-s001: x3000c0s7b0n0 - metal.no-wipe=1 ncn-s002: x3000c0s8b0n0 - metal.no-wipe=1 ncn-s003: x3000c0s9b0n0 - metal.no-wipe=1 "
},
{
	"uri": "/docs-csm/en-09/upgrade/0.9/csm-0.9.6/",
	"title": "CSM 0.9.6 Patch Installation Instructions",
	"tags": [],
	"description": "",
	"content": "CSM 0.9.6 Patch Installation Instructions Content Non-Deterministic Unbound DNS Results Patch\nThis procedure covers applying a new version of the cray-dns-unbound Helm chart to enable this setting in the configmap:\nrrset-roundrobin: no Unbound back in April 2020 changed the default of this setting to be yes which had the effect of randomizing the records returned from it if more than one entry corresponded (as would be the case for PTR records, for example):\n21 April 2020: George - Change default value for \u0026#39;rrset-roundrobin\u0026#39; to yes. - Fix tests for new rrset-roundrobin default. Some software is especially sensitive to this and thus requires this setting to be no.\nUpdate the cray-sysmgmt-health helm chart to address multiple alerts\nInstall/Update node_exporter on storage nodes\nUpdate cray-hms-hmnfd helm chart to include timestamp fix\nUpdate the cray-hms-hmcollector helm chart to include fix to prevent crashing, also its resource limits and requests can be overridden via customizations.yaml.\nProcedures Preparation Run Validation Checks (Pre-Upgrade) Apply cray-hms-hmcollector scale changes Setup Nexus Update NCNs Update BSS metadata Upgrade Services Rollout Deployment Restart Verification Run Validation Checks (Post-Upgrade) Exit Typescript Preparation Start a typescript to capture the commands and output from this procedure.\nncn-m001# script -af csm-update.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; NOTE: Installed CSM versions may be listed from the product catalog using:\nncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r -j - | jq -r \u0026#39;to_entries[] | .key\u0026#39; | sort -V 0.9.2 0.9.3 0.9.4 0.9.5 Set CSM_DISTDIR to the directory of the extracted release distribution for CSM 0.9.6:\nNOTE: Use --no-same-owner and --no-same-permissions options to tar when extracting a CSM release distribution as root to ensure the current umask value.\nIf using a release distribution:\nncn-m001# tar --no-same-owner --no-same-permissions -zxvf csm-0.9.6.tar.gz ncn-m001# export CSM_DISTDIR=\u0026#34;$(pwd)/csm-0.9.6\u0026#34; Set CSM_RELEASE_VERSION to the version reported by ${CSM_DISTDIR}/lib/version.sh:\nncn-m001# CSM_RELEASE_VERSION=\u0026#34;$(${CSM_DISTDIR}/lib/version.sh --version)\u0026#34; ncn-m001# echo $CSM_RELEASE_VERSION Download and install/upgrade the latest documentation RPM. If this machine does not have direct internet access these RPMs will need to be externally downloaded and then copied to be installed.\nncn-m001# rpm -Uvh https://storage.googleapis.com/csm-release-public/shasta-1.4/docs-csm/docs-csm-latest.noarch.rpm Run Validation Checks (Pre-Upgrade) It is important to first verify a healthy starting state. To do this, run the CSM validation checks. If any problems are found, correct them and verify the appropriate validation checks before proceeding.\nApply cray-hms-hmcollector scale changes If no scaling changes are desired to be made against the cray-hms-hmcollector deployment or if they have have not been previously applied, then this section can be skipped and proceed onto the Setup Nexus section.\nBefore upgrading services, customizations.yaml in the site-init secret in the loftsman namespace must be updated to apply or re-apply any manual scaling changes made to the cray-hms-hmcollector deployment. Follow the Adjust HM Collector resource limits and requests procedure for information about tuning and updating the resource limits used by the cray-hms-hmcollector deployment. The section Redeploy cray-hms-hmcollector with new resource limits and request of the referenced procedure can be skipped, as the upgrade.sh script will re-deploy the collector with the new resource limit changes.\nSetup Nexus Run lib/setup-nexus.sh to configure Nexus and upload new CSM RPM repositories, container images, and Helm charts:\nncn-m001# cd \u0026#34;$CSM_DISTDIR\u0026#34; ncn-m001# ./lib/setup-nexus.sh On success, setup-nexus.sh will output OK on stderr and exit with status code 0, e.g.:\nncn-m001# ./lib/setup-nexus.sh ... + Nexus setup complete setup-nexus.sh: OK ncn-m001# echo $? 0 In the event of an error, consult the known issues from the install documentation to resolve potential problems and then try running setup-nexus.sh again. Note that subsequent runs of setup-nexus.sh may report FAIL when uploading duplicate assets. This is ok as long as setup-nexus.sh outputs setup-nexus.sh: OK and exits with status code 0.\nUpdate NCNs Set CSM_SCRIPTDIR to the scripts directory included in the docs-csm RPM for the CSM 0.9.6 upgrade:\nncn-m001# CSM_SCRIPTDIR=/usr/share/doc/metal/upgrade/0.9/csm-0.9.6/scripts Execute the following script from the scripts directory determined in the previous step to update NCN.\nncn-m001# cd \u0026#34;$CSM_SCRIPTDIR\u0026#34; ncn-m001# ./update-ncns.sh Update BSS metadata Execute the following script from the scripts directory determined above to update BSS metadata.\nncn-m001# cd \u0026#34;$CSM_SCRIPTDIR\u0026#34; ncn-m001# ./update-bss-metadata.sh Upgrade Services Run upgrade.sh to deploy upgraded CSM applications and services:\nncn-m001# cd \u0026#34;$CSM_DISTDIR\u0026#34; ncn-m001# ./upgrade.sh Rollout Deployment Restart Instruct Kubernetes to gracefully restart the Unbound pods:\nncn-m001:~ # kubectl -n services rollout restart deployment cray-dns-unbound deployment.apps/cray-dns-unbound restarted ncn-m001:~ # kubectl -n services rollout status deployment cray-dns-unbound Waiting for deployment \u0026#34;cray-dns-unbound\u0026#34; rollout to finish: 0 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-dns-unbound\u0026#34; rollout to finish: 3 old replicas are pending termination... Waiting for deployment \u0026#34;cray-dns-unbound\u0026#34; rollout to finish: 3 old replicas are pending termination... Waiting for deployment \u0026#34;cray-dns-unbound\u0026#34; rollout to finish: 3 old replicas are pending termination... Waiting for deployment \u0026#34;cray-dns-unbound\u0026#34; rollout to finish: 2 old replicas are pending termination... Waiting for deployment \u0026#34;cray-dns-unbound\u0026#34; rollout to finish: 2 old replicas are pending termination... Waiting for deployment \u0026#34;cray-dns-unbound\u0026#34; rollout to finish: 2 old replicas are pending termination... Waiting for deployment \u0026#34;cray-dns-unbound\u0026#34; rollout to finish: 1 old replicas are pending termination... Waiting for deployment \u0026#34;cray-dns-unbound\u0026#34; rollout to finish: 1 old replicas are pending termination... deployment \u0026#34;cray-dns-unbound\u0026#34; successfully rolled out Verification Verify CSM Version in Product Catalog: Verify the CSM version has been updated in the product catalog. Verify that the following command includes version 0.9.6:\nncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r -j - | jq -r \u0026#39;to_entries[] | .key\u0026#39; | sort -V 0.9.2 0.9.3 0.9.4 0.9.5 0.9.6 Confirm the import_date reflects the timestamp of the upgrade:\nncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r - \u0026#39;\u0026#34;0.9.6\u0026#34;.configuration.import_date\u0026#39; Verify updated images for CVE-2021-3711: Execute the following script from the scripts directory to make sure correct images for CVE-2021-3711 are updated:\nncn-m001# cd \u0026#34;$CSM_SCRIPTDIR\u0026#34; ncn-m001# ./validate_versions.sh Verify cray-sysmgmt-health changes: Confirm node-exporter is running on each storage node. This command can be run from a master node. Validate that the result contains go_goroutines (replace ncn-s001 below with each storage node):\ncurl -s http://ncn-s001:9100/metrics |grep go_goroutines|grep -v \u0026#34;#\u0026#34; go_goroutines 8 Confirm manifests were updated on each master node (repeat on each master node):\nncn-m# grep bind /etc/kubernetes/manifests/* kube-controller-manager.yaml: - --bind-address=0.0.0.0 kube-scheduler.yaml: - --bind-address=0.0.0.0 Confirm updated sysmgmt-health chart was deployed. This command can be executed on a master node \u0026ndash; confirm the cray-sysmgmt-health-0.12.6 chart version:\nncn-m# helm ls -n sysmgmt-health NAME NAMESPACE REVISION\tUPDATED STATUS CHART APP VERSION cray-sysmgmt-health\tsysmgmt-health\t2 2021-09-10 16:45:12.00113666 +0000 UTC\tdeployed\tcray-sysmgmt-health-0.12.6 8.15.4 Confirm updates to BSS for cloud-init runcmd\nIMPORTANT: Ensure you replace XNAME with the correct xname in the below examples (executing the /opt/cray/platform-utils/getXnames.sh script on a master node will display xnames):\nExample for a master node \u0026ndash; this should be checked for each master node. Validate the three sed commands are returned in the output.\nncn-m# cray bss bootparameters list --name XNAME --format=json | jq \u0026#39;.[]|.\u0026#34;cloud-init\u0026#34;.\u0026#34;user-data\u0026#34;\u0026#39; { \u0026#34;hostname\u0026#34;: \u0026#34;ncn-m001\u0026#34;, \u0026#34;local_hostname\u0026#34;: \u0026#34;ncn-m001\u0026#34;, \u0026#34;mac0\u0026#34;: { \u0026#34;gateway\u0026#34;: \u0026#34;10.252.0.1\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;mask\u0026#34;: \u0026#34;10.252.2.0/23\u0026#34; }, \u0026#34;runcmd\u0026#34;: [ \u0026#34;/srv/cray/scripts/metal/install-bootloader.sh\u0026#34;, \u0026#34;/srv/cray/scripts/metal/set-host-records.sh\u0026#34;, \u0026#34;/srv/cray/scripts/metal/set-dhcp-to-static.sh\u0026#34;, \u0026#34;/srv/cray/scripts/metal/set-dns-config.sh\u0026#34;, \u0026#34;/srv/cray/scripts/metal/set-ntp-config.sh\u0026#34;, \u0026#34;/srv/cray/scripts/metal/set-bmc-bbs.sh\u0026#34;, \u0026#34;/srv/cray/scripts/metal/disable-cloud-init.sh\u0026#34;, \u0026#34;/srv/cray/scripts/common/update_ca_certs.py\u0026#34;, \u0026#34;/srv/cray/scripts/common/kubernetes-cloudinit.sh\u0026#34;, \u0026#34;sed -i \u0026#39;s/--bind-address=127.0.0.1/--bind-address=0.0.0.0/\u0026#39; /etc/kubernetes/manifests/kube-controller-manager.yaml\u0026#34;, \u0026#34;sed -i \u0026#39;/--port=0/d\u0026#39; /etc/kubernetes/manifests/kube-scheduler.yaml\u0026#34;, \u0026#34;sed -i \u0026#39;s/--bind-address=127.0.0.1/--bind-address=0.0.0.0/\u0026#39; /etc/kubernetes/manifests/kube-scheduler.yaml\u0026#34; ] } Example for a storage node \u0026ndash; this should be checked for each storage node. Validate the zypper command is returned in the output.\nncn-m001:~ # cray bss bootparameters list --name XNAME --format=json | jq \u0026#39;.[]|.\u0026#34;cloud-init\u0026#34;.\u0026#34;user-data\u0026#34;\u0026#39; { \u0026#34;hostname\u0026#34;: \u0026#34;ncn-s001\u0026#34;, \u0026#34;local_hostname\u0026#34;: \u0026#34;ncn-s001\u0026#34;, \u0026#34;mac0\u0026#34;: { \u0026#34;gateway\u0026#34;: \u0026#34;10.252.0.1\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;mask\u0026#34;: \u0026#34;10.252.2.0/23\u0026#34; }, \u0026#34;runcmd\u0026#34;: [ \u0026#34;/srv/cray/scripts/metal/install-bootloader.sh\u0026#34;, \u0026#34;/srv/cray/scripts/metal/set-host-records.sh\u0026#34;, \u0026#34;/srv/cray/scripts/metal/set-dhcp-to-static.sh\u0026#34;, \u0026#34;/srv/cray/scripts/metal/set-dns-config.sh\u0026#34;, \u0026#34;/srv/cray/scripts/metal/set-ntp-config.sh\u0026#34;, \u0026#34;/srv/cray/scripts/metal/set-bmc-bbs.sh\u0026#34;, \u0026#34;/srv/cray/scripts/metal/disable-cloud-init.sh\u0026#34;, \u0026#34;/srv/cray/scripts/common/update_ca_certs.py\u0026#34;, \u0026#34;zypper --no-gpg-checks in -y https://packages.local/repository/csm-sle-15sp2/x86_64/cray-node-exporter-1.2.2.1-1.x86_64.rpm\u0026#34; ] } Verify HMNFD timestamp fix: NOTE: The following verification steps require SMA and SAT to be installed.\nOnce the patch is installed the missing timestamp fix can be validated by taking the following steps:\nFind an instance of a cluster-kafka pod: kubectl -n sma get pods | grep kafka cluster-kafka-0 2/2 Running 1 30d cluster-kafka-1 2/2 Running 1 26d cluster-kafka-2 2/2 Running 0 73d Exec into one of those pods: kubectl -n sma exec -it \u0026lt;pod_id\u0026gt; /bin/bash cd to the \u0026lsquo;bin\u0026rsquo; directory in the kafka pod.\nExecute the following command in the kafka pod to run a kafka consumer app:\n./kafka-console-consumer.sh --bootstrap-server=localhost:9092 --topic=cray-hmsstatechange-notifications Find a compute node that is booted on the system: sat status | grep Compute | grep Ready ... | x1003c7s7b1n1 | Node | 2023 | Ready | OK | True | X86 | Mountain | Compute | Sling | NOTE: All examples below will use the node seen in the above example.\nSend an SCN to HMNFD for that node indicating that it is in the Ready state. Note that this will not affect anything since the node is already Ready. TOKEN=`curl -k -s -S -d grant_type=client_credentials -d client_id=admin-client -d client_secret=\\`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d\\` https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;` curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -d \u0026#39;{\u0026#34;Components\u0026#34;:[\u0026#34;x1003c7s7b1n1\u0026#34;],\u0026#34;State\u0026#34;:\u0026#34;Ready\u0026#34;}\u0026#39; https://api_gw_service.local/apis/hmnfd/hmi/v1/scn In the kafka-console-consumer.sh window there should be an SCN sent by HMNFD, which should include a Timestamp field: {\u0026#34;Components\u0026#34;:[\u0026#34;x1003c7s7b1n1\u0026#34;],\u0026#34;Flag\u0026#34;:\u0026#34;OK\u0026#34;,\u0026#34;State\u0026#34;:\u0026#34;Ready\u0026#34;,\u0026#34;Timestamp\u0026#34;:\u0026#34;2021-09-13T13:00:00\u0026#34;} Run Validation Checks (Post-Upgrade) IMPORTANT: Wait at least 15 minutes after upgrade.sh completes to let the various Kubernetes resources get initialized and started.\nRun the following validation checks to ensure that everything is still working properly after the upgrade:\nPlatform health checks Network health checks Other health checks may be run as desired.\nExit Typescript Remember to exit your typescript.\nncn-m001# exit It is recommended to save the typescript file for later reference.\n"
},
{
	"uri": "/docs-csm/en-09/troubleshooting/known_issues/cfs_sessions_stuck_in_pending/",
	"title": "CFS Sessions are Stuck in Pending State",
	"tags": [],
	"description": "",
	"content": "CFS Sessions are Stuck in Pending State In rare cases it is possible that a CFS session can be stuck in a pending state. Sessions should only enter the pending state briefly, for no more than a few seconds while the corresponding Kubernetes job is being scheduled. If any sessions are in this state for more than a minute, they can safely be deleted. If the sessions were created automatically and retires are enabled, the sessions should be recreated automatically.\nPending sessions can be found with the following command:\ncray cfs sessions list --format json | jq \u0026#39;.[] | select(.status.session.status==\u0026#34;pending\u0026#34;) | {name}\u0026#39; Stuck sessions that were created by the cfs-batcher can block further sessions from being scheduled against the same components. In the event that sessions are not being scheduled against some components, check the list of pending sessions to see if any are stuck and targeting the same component.\n"
},
{
	"uri": "/docs-csm/en-09/upgrade/0.9/csm-0.9.5/operations/network/metallb_bgp/check_bgp_status_and_reset_sessions/",
	"title": "Check BGP Status and Reset Sessions",
	"tags": [],
	"description": "",
	"content": "Check BGP Status and Reset Sessions Check the Border Gateway Protocol (BGP) status on the Aruba and Mellanox switches and verify that all sessions are in an Established state. If the state of any session in the table is Idle, the BGP sessions needs to be reset.\nPrerequisites This procedure requires administrative privileges.\nProcedure The following procedures will require knowing a list of switches that are BGP peers to connect to. You can obtain this list by running the following from an NCN node:\nncn-m001# kubectl get cm config -n metallb-system -o yaml | head -12 Expected output looks similar to the following:\napiVersion: v1 data: config: | peers: - peer-address: 10.252.0.2 peer-asn: 65533 my-asn: 65533 - peer-address: 10.252.0.3 peer-asn: 65533 my-asn: 65533 address-pools: - name: customer-access The switch IPs are the peer-address values.\nMELLANOX Verify that all BGP sessions are in an Established state for the Mellanox spine switches.\nSSH to each BGP peer switch to check the status of all BGP sessions.\nSSH to a BGP peer switch.\nFor example:\nncn-m001# ssh admin@10.252.0.2 View the status of the BGP sessions.\nsw-spine-001 [standalone: master] \u0026gt; enable sw-spine-001 [standalone: master] # show ip bgp summary VRF name : default BGP router identifier : 10.252.0.2 local AS number : 65533 BGP table version : 50 Main routing table version: 50 IPV4 Prefixes : 68 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.1.10 4 65533 3144 3564 50 0 0 1:01:50:41 ESTABLISHED/13 10.252.1.11 4 65533 3144 3569 50 0 0 1:01:50:40 ESTABLISHED/14 10.252.1.12 4 65533 3145 3576 50 0 0 1:01:50:41 ESTABLISHED/14 10.252.1.13 4 65533 3144 3568 50 0 0 1:01:50:41 ESTABLISHED/13 10.252.1.14 4 65533 3145 3572 50 0 0 1:01:50:41 ESTABLISHED/14 If any of the sessions are in an Idle state, proceed to the next step.\nReset BGP to re-establish the sessions.\nSSH to each BGP peer switch.\nFor example:\nncn-m001# ssh admin@10.252.0.2 Verify BGP is enabled.\nsw-spine-001 [standalone: master] \u0026gt; show protocols | include bgp bgp: enabled Clear the BGP sessions.\nsw-spine-001 [standalone: master] \u0026gt; enable sw-spine-001 [standalone: master] # clear ip bgp all Check the status of the BGP sessions to see if they are now Established.\nIt may take a few minutes for sessions to become Established.\nsw-spine-001 [standalone: master] # show ip bgp summary VRF name : default BGP router identifier : 10.252.0.2 local AS number : 65533 BGP table version : 50 Main routing table version: 50 IPV4 Prefixes : 68 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.1.10 4 65533 3144 3564 50 0 0 1:01:50:41 ESTABLISHED/13 10.252.1.11 4 65533 3144 3569 50 0 0 1:01:50:40 ESTABLISHED/14 10.252.1.12 4 65533 3145 3576 50 0 0 1:01:50:41 ESTABLISHED/14 10.252.1.13 4 65533 3144 3568 50 0 0 1:01:50:41 ESTABLISHED/13 10.252.1.14 4 65533 3145 3572 50 0 0 1:01:50:41 ESTABLISHED/14 Once all sessions are in an Established state, BGP reset is complete for the Mellanox switches.\nTroubleshooting: If some sessions remain Idle, re-run the Mellanox reset steps to clear and re-check status. The clear ip bgp all command may need to be be ran multiple times (up to 10 times). In between each clear command wait a few minutes before re-checking the BGP Sessions. If some sessions still remain Idle, proceed to reapply the cray-metallb helm chart, along with the BGP reset, to force the speaker pods to re-establish sessions with the switch.\nAruba Verify that all BGP sessions are in an Established state for the Aruba spine switches.\nSSH to each BGP peer switch to check the status of all BGP sessions.\nSSH to a BGP peer switch.\nncn-m001# ssh admin@10.252.0.2 View the status of the BGP sessions.\nsw-spine-001# show bgp ipv4 unicast summary VRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.2 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.3 65533 19704 19708 00m:01w:00d Established Up 10.252.1.10 65533 34455 39416 00m:01w:04d Established Up 10.252.1.11 65533 34458 39400 00m:01w:04d Established Up 10.252.1.12 65533 34448 39415 00m:01w:04d Established Up If any of the sessions are in an Idle state, proceed to the next step.\nReset BGP to re-establish the sessions.\nSSH to each BGP peer switch.\nFor example:\nncn-m001# ssh admin@10.252.0.2 Clear the BGP sessions.\nsw-spine-001# clear bgp * Check the status of the BGP sessions.\nIt may take a few minutes for sessions to become Established.\nsw-spine-001# show bgp ipv4 unicast summary VRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.2 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.3 65533 19704 19708 00m:01w:00d Established Up 10.252.1.10 65533 34455 39416 00m:01w:04d Established Up 10.252.1.11 65533 34458 39400 00m:01w:04d Established Up 10.252.1.12 65533 34448 39415 00m:01w:04d Established Up Once all sessions are in an Established state, BGP reset is complete for the Aruba switches.\nTroubleshooting: If some sessions remain Idle, re-run the Aruba reset steps to clear and re-check status. The clear bgp * command may need to be be ran multiple times (up to 10 times). In between each clear command wait a few minutes before re-checking the BGP Sessions. If some sessions still remain Idle, proceed to the next step to reapply the cray-metallb helm chart, along with the BGP reset to force the speaker pods to re-establish sessions with the switch.\nTroubleshooting Re-apply the cray-metallb Helm Chart Determine the cray-metallb chart version that is currently deployed.\nncn-m001# helm ls -A -a | grep cray-metallb cray-metallb metallb-system 1 2021-02-10 14:58:43.902752441 -0600 CST deployed cray-metallb-0.12.2 0.8.1 Create a manifest file that will be used to reapply the same chart version.\nncn-m001# cat \u0026lt;\u0026lt; EOF \u0026gt; ./metallb-manifest.yaml apiVersion: manifests/v1beta1 metadata: name: reapply-metallb spec: charts: - name: cray-metallb namespace: metallb-system values: imagesHost: dtr.dev.cray.com version: 0.12.2 EOF Open SSH sessions to all spine switches.\nDetermine the CSM_RELEASE version that is currently running and set an environment variable.\nFor example:\nncn-m001# CSM_RELEASE=0.8.0 Mount the PITDATA so that helm charts are available for the re-install (it might already be mounted) and verify that the chart with the expected version exists.\nncn-m001# mkdir -pv /mnt/pitdata ncn-m001# mount -L PITDATA /mnt/pitdata ncn-m001# ls /mnt/pitdata/csm-${CSM_RELEASE}/helm/cray-metallb* /mnt/pitdata/csm-0.8.0/helm/cray-metallb-0.12.2.tgz Uninstall the current cray-metallb chart.\nUntil the chart is reapplied, this will also effect unbound name resolution, and all BGP sessions will be Idle for all of the worker nodes.\nncn-m001# helm del cray-metallb -n metallb-system Use the open SSH sessions to the switches to clear the BGP sessions based on the above Mellanox or Aruba procedures.\nRefer to substeps 1-3 for Mellanox.\nRefer to substeps 1-2 for Aruba.\nReapply the cray-metallb chart based on the CSM_RELEASE.\nncn-m001# loftsman ship --manifest-path ./metallb-manifest.yaml \\ --charts-path /mnt/pitdata/csm-${CSM_RELEASE}/helm Check that the speaker pods are all running.\nThis may take a few minutes.\nncn-m001# kubectl get pods -n metallb-system NAME READY STATUS RESTARTS AGE cray-metallb-controller-6d545b5ccc-mm4qz 1/1 Running 0 79m cray-metallb-speaker-4nrzq 1/1 Running 0 76m cray-metallb-speaker-b5m2n 1/1 Running 0 79m cray-metallb-speaker-h7s7b 1/1 Running 0 79m Use the open SSH sessions to the switches to check the status of the BGP sessions.\nRefer to substeps 1-3 for Mellanox.\nRefer to substeps 1-2 for Aruba.\n"
},
{
	"uri": "/docs-csm/en-09/upgrade/0.9/",
	"title": "CSM 0.9 Patch Release Upgrade Procedures",
	"tags": [],
	"description": "",
	"content": "Copyright 2021 Hewlett Packard Enterprise Development LP\nCSM 0.9 Patch Release Upgrade Procedures CSM 0.9 patch release upgrade procedures assume systems have been upgraded to the previous release. It is recommended that systems be kept up-to-date as patches are released.\nCSM 0.9.0 was released with Shasta 1.4.0 and requires a fresh install.\nCSM 0.9.1 - Not released CSM 0.9.2 - Released with Shasta 1.4.1 CSM 0.9.3 - Released with Shasta 1.4.2 "
},
{
	"uri": "/docs-csm/en-09/upgrade/0.9/csm-0.9.4/",
	"title": "CSM 0.9.4 Patch Upgrade Guide",
	"tags": [],
	"description": "",
	"content": "Copyright 2021 Hewlett Packard Enterprise Development LP\nCSM 0.9.4 Patch Upgrade Guide This guide contains procedures for upgrading systems running CSM 0.9.3 to CSM 0.9.4. It is intended for system installers, system administrators, and network administrators. It assumes some familiarity with standard Linux and associated tooling.\nProcedures:\nPreparation Run Validation Checks (Pre-Upgrade) Update /etc/hosts on Workers Check For Manually Created Unbound PSP Remove zypper RIS services repositories Setup Nexus Backup VCS Content Upgrade Services Clean Up CFS Sessions Update NTP and DNS Servers on BMCs Fix Kubelet and Kube-Proxy Target Down Prometheus Alerts Install Prometheus Node-Exporter on Utility Storage Nodes Restore VCS Content Disable TPM Kernel Module Run Validation Checks (Post-Upgrade) Verify CSM Version in Product Catalog Update customizations.yaml Exit Typescript Changes See CHANGELOG.md in the root of a CSM release distribution for a summary of changes in each CSM release. This patch includes the following changes:\nCFS sessions stuck with no job. A race condition sometimes caused CFS sessions to never start a job, which could in turn block other sessions targeting the same nodes from starting. The fix is an updated cfs-operator image which will retry when this race condition is hit. Configure NTP and DNS for HPE NCN BMCs. Unbound no longer forwards requests to Shasta zones to site DNS. Add static entries for registry.local and packages.local to the /etc/hosts files on the worker nodes. Update Kea externTrafficPolicy from Cluster to Local. Prometheus can now to scrape kubelet/kube-proxy for metrics. Install node-exporter on storage nodes. BOS will now leave any nodes that it cannot communicate with behind. These nodes will not prolong a BOS session. A message describing how to relaunch BOS to pick up any failing nodes is output in the log for the BOA pod corresponding to the BOS session. Updates the VCS PVC name so that it can be found on system restart. Preparation For convenience, these procedures make use of environment variables. This section sets the expected environment variables to appropriate values.\nStart a typescript to capture the commands and output from this procedure.\nncn-m001# script -af csm-update.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Set CSM_SYSTEM_VERSION to 0.9.3:\nncn-m001# CSM_SYSTEM_VERSION=\u0026#34;0.9.3\u0026#34; NOTE: Installed CSM versions may be listed from the product catalog using:\nncn-m001# kubectl -n services get cm cray-product-catalog -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r -j - | jq -r \u0026#39;keys[]\u0026#39; | sed \u0026#39;/-/!{s/$/_/}\u0026#39; | sort -V | sed \u0026#39;s/_$//\u0026#39; Set CSM_DISTDIR to the directory of the extracted release distribution for CSM 0.9.4:\nNOTE: Use --no-same-owner and --no-same-permissions options to tar when extracting a CSM release distribution as root to ensure the extracted files are owned by root and have permissions based on the current umask value.\nIf using a release distribution:\nncn-m001# tar --no-same-owner --no-same-permissions -zxvf csm-0.9.4.tar.gz ncn-m001# CSM_DISTDIR=\u0026#34;$(pwd)/csm-0.9.4\u0026#34; Else if using a hotfix distribution:\nncn-m001# CSM_HOTFIX=\u0026#34;csm-0.9.4-hotfix-0.0.1\u0026#34; ncn-m001# tar --no-same-owner --no-same-permissions -zxvf ${CSM_HOTFIX}.tar.gz ncn-m001# CSM_DISTDIR=\u0026#34;$(pwd)/${CSM_HOTFIX}\u0026#34; ncn-m001# echo $CSM_DISTDIR Set CSM_RELEASE_VERSION to the version reported by ${CSM_DISTDIR}/lib/version.sh:\nncn-m001# CSM_RELEASE_VERSION=\u0026#34;$(${CSM_DISTDIR}/lib/version.sh --version)\u0026#34; ncn-m001# echo $CSM_RELEASE_VERSION Download and install/upgrade the latest workaround and documentation RPMs. If this machine does not have direct internet access these RPMs will need to be externally downloaded and then copied to be installed.\nncn-m001# rpm -Uvh https://storage.googleapis.com/csm-release-public/shasta-1.4/docs-csm/docs-csm-latest.noarch.rpm ncn-m001# rpm -Uvh https://storage.googleapis.com/csm-release-public/shasta-1.4/csm-install-workarounds/csm-install-workarounds-latest.noarch.rpm After completing the previous step, apply the workaround in the following directory, even if it has been previously applied on the system.\n/opt/cray/csm/workarounds/livecd-post-reboot/CASMINST-2689 See the README.md file in that directory for instructions on how to apply the workaround. It requires you to run a script.\nSet CSM_SCRIPTDIR to the scripts directory included in the docs-csm RPM for the CSM 0.9.4 upgrade:\nncn-m001# CSM_SCRIPTDIR=/usr/share/doc/metal/upgrade/0.9/csm-0.9.4/scripts Run Validation Checks (Pre-Upgrade) It is important to first verify a healthy starting state. To do this, run the CSM validation checks. If any problems are found, correct them and verify the appropriate validation checks before proceeding.\nUpdate /etc/hosts on Workers Run the update-host-records.sh script to update /etc/hosts on NCN workers:\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/update-host-records.sh\u0026#34; Check Unbound PSP Check for manually created unbound-psp and delete the psp. Helm will manage the psp during the upgrade.\nncn-m001# ${CSM_SCRIPTDIR}/check-unbound-psp.sh Remove zypper RIS services repositories Run lib/remove-service-repos.sh to remove repositories that are external to the system.\nncn-m001# ${CSM_SCRIPTDIR}/remove-service-repos.sh Setup Nexus Run lib/setup-nexus.sh to configure Nexus and upload new CSM RPM repositories, container images, and Helm charts:\nncn-m001# cd \u0026#34;$CSM_DISTDIR\u0026#34; ncn-m001# ./lib/setup-nexus.sh On success, setup-nexus.sh will output OK on stderr and exit with status code 0, e.g.:\nncn-m001# ./lib/setup-nexus.sh ... + Nexus setup complete setup-nexus.sh: OK ncn-m001# echo $? 0 In the event of an error, consult the known issues from the install documentation to resolve potential problems and then try running setup-nexus.sh again. Note that subsequent runs of setup-nexus.sh may report FAIL when uploading duplicate assets. This is ok as long as setup-nexus.sh outputs setup-nexus.sh: OK and exits with status code 0.\nBackup VCS Content Run the vcs-backup.sh script to backup all VCS content to a temporary location.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/vcs-backup.sh\u0026#34; Confirm the local tar file vcs.tar was created. It contains the Git repository data and will be needed in the restore step. Once upgrade.sh is run, the git data will not be recoverable if this step failed.\nIf vcs.tar was successfully created, run vcs-prep.sh. This will remove the existing pvc in preparation for the upgrade.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/vcs-prep.sh\u0026#34; It is also recommended to save the VCS password to a safe location prior to making changes to VCS. The current password can can be retrieved with:\nncn-m001# kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode; echo Update customizations.yaml If you manage customizations.yaml in an external Git repository (as recommended), then clone a local working tree, e.g.:\nncn-m001# git clone \u0026lt;URL\u0026gt; site-init ncn-m001# cd site-init Otherwise extract customizations.yaml from the site-init secret:\nncn-m001# cd /tmp ncn-m001# kubectl -n loftsman get secret site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d - \u0026gt; customizations.yaml Remove the Gitea PVC configuration from customizations.yaml:\nncn-m001# yq d -i customizations.yaml \u0026#39;spec.kubernetes.services.gitea.cray-service.persistentVolumeClaims\u0026#39; Update the site-init secret:\nncn-m001# kubectl delete secret -n loftsman site-init ncn-m001# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml Commit changes to customizations.yaml if using an external Git repository, e.g.:\nncn-m001# git add customizations.yaml ncn-m001# git commit -m \u0026#39;Remove Gitea PVC configuration from customizations.yaml\u0026#39; ncn-m001# git push Upgrade Services Run upgrade.sh to deploy upgraded CSM applications and services:\nncn-m001# cd \u0026#34;$CSM_DISTDIR\u0026#34; ncn-m001# ./upgrade.sh Note: If you have not already installed the workload manager product including slurm and munge, then the cray-crus pod is expected to be in the Init state. After running upgrade.sh, you may observe there are now two copies of the cray-crus pod in the Init state. This situation is benign and should resolve itself once the workload manager product is installed.\nClean Up CFS Sessions NOTE: This fix only applies to new sessions and will not correct sessions that are already in the stuck state.\nDelete all sessions that are in stuck:\nncn-m001# cray cfs sessions list --format json | jq -r \u0026#39;.[] | select(.status.session.startTime==null) | .name\u0026#39; | while read name ; do cray cfs sessions delete $name; done Update NTP and DNS servers on BMCs NOTE: For Gigabyte or Intel NCNs skip this section.\nDeploy the set-bmc-ntp-dns.sh script (and its helper script make_api_call.py) to each NCN except m001:\nncn-m001# for h in $( grep ncn /etc/hosts | grep nmn | grep -v m001 | awk \u0026#39;{print $2}\u0026#39; ); do ssh $h \u0026#34;mkdir -p /opt/cray/ncn\u0026#34; scp \u0026#34;${CSM_SCRIPTDIR}/make_api_call.py\u0026#34; \u0026#34;${CSM_SCRIPTDIR}/set-bmc-ntp-dns.sh\u0026#34; root@$h:/opt/cray/ncn/ ssh $h \u0026#34;chmod 755 /opt/cray/ncn/set-bmc-ntp-dns.sh\u0026#34; done Run the /opt/cray/ncn/set-bmc-ntp-dns.sh script on each NCN except m001.\nPass -h to see some examples and use the information below to run the script.\nThe following process can restore NTP and DNS server values after a firmware update to HPE NCNs. If you update the System ROM of an NCN, you will lose NTP and DNS server values. Correctly setting these also allows FAS to function properly.\nDetermine the HMN IP address for m001: ncn# M001_HMN_IP=$(cat /etc/hosts | grep m001.hmn | awk \u0026#39;{print $1}\u0026#39;) ncn# echo $M001_HMN_IP 10.254.1.4 Specify the name and credentials for the BMC: ncn# BMC=ncn-\u0026lt;NCN name\u0026gt;-mgmt # e.g. ncn-w003-mgmt ncn# export USERNAME=root ncn# export IPMI_PASSWORD=changeme View the existing DNS and NTP settings on the BMC: ncn# /opt/cray/ncn/set-bmc-ntp-dns.sh ilo -H $BMC -s Disable DHCP and set the NTP servers to point toward time-hmn and ncn-m001. ncn# /opt/cray/ncn/set-bmc-ntp-dns.sh ilo -H $BMC -S -N \u0026#34;time-hmn,$M001_HMN_IP\u0026#34; -n Set the DNS server to point toward Unbound and ncn-m001. ncn# /opt/cray/ncn/set-bmc-ntp-dns.sh ilo -H $BMC -D \u0026#34;10.94.100.225,$M001_HMN_IP\u0026#34; -d Fix Kubelet and Kube-Proxy Target Down Prometheus Alerts NOTE: These scripts should be run from a Kubernetes NCN (manager or worker). Also note it can take several minutes for the target down alerts to clear after the scripts have been executed.\nRun the fix-kube-proxy-target-down-alert.sh script to fix the kube-proxy alert.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/fix-kube-proxy-target-down-alert.sh\u0026#34; Run the fix-kubelet-target-down-alert.sh script to fix the kube-proxy alert.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/fix-kubelet-target-down-alert.sh\u0026#34; Install Prometheus Node-Exporter on Utility Storage Nodes Verify the zypper repository in nexus that contains the golang-github-prometheus-node_exporter RPM is enabled. Typically this is the SUSE-SLE-Module-Basesystem-15-SP1-x86_64-Updates repository. If not enabled, enable it (or the repository in nexus that contains the RPM) on all storage nodes. The easiest way to find the repository that contains this RPM is to login to the Nexus UI at https://nexus.SYSTEM-NAME.cray.com, click the search icon in the navigation pane on the left, and enter golang-github-prometheus-node_exporter as the keyword. Then click on the search result that has the latest version of the RPM, and on that screen the repository name to use is listed as the repository at the top.\nncn-m001# for h in $( cat /etc/hosts | grep ncn-s | grep nmn | awk \u0026#39;{print $2}\u0026#39; ); do ssh $h \u0026#34;zypper ar https://packages.local/repository/SUSE-SLE-Module-Basesystem-15-SP1-x86_64-Updates SUSE-SLE-Module-Basesystem-15-SP1-x86_64-Updates\u0026#34; done Copy the install-node-exporter-storage.sh script out to the storage nodes.\nncn-m001# for h in $( cat /etc/hosts | grep ncn-s | grep nmn | awk \u0026#39;{print $2}\u0026#39; ); do scp \u0026#34;${CSM_SCRIPTDIR}/install-node-exporter-storage.sh\u0026#34; root@$h:/tmp done Run the install-node-exporter-storage.sh script on each of the storage nodes to enable the node-exporter:\nNOTE: This script should be run on each storage node.\nncn-s# /tmp/install-node-exporter-storage.sh NOTE: While running install-node-exporter-storage.sh, you may see an error similar to the following:\nError building the cache: [SUSE-SLE-Module-Basesystem-15-SP1-x86_64-Updates|https://packages.local/repository/SUSE-SLE-Module-Basesystem-15-SP1-x86_64-Updates] Valid metadata not found at specified URL History: - [SUSE-SLE-Module-Basesystem-15-SP1-x86_64-Updates|https://packages.local/repository/SUSE-SLE-Module-Basesystem-15-SP1-x86_64-Updates] Repository type can\u0026#39;t be determined. Warning: Skipping repository \u0026#39;SUSE-SLE-Module-Basesystem-15-SP1-x86_64-Updates\u0026#39; because of the above error. This error can be safely ignored.\nThe following error may occur for air-gapped systems that do not have connectivity to the internet:\nRefreshing service \u0026#39;Public_Cloud_Module_15_SP2_x86_64\u0026#39;. Problem retrieving the repository index file for service \u0026#39;Public_Cloud_Module_15_SP2_x86_64\u0026#39;: Download (curl) error for \u0026#39;https://scc.suse.com/access/services/1973/repo/repoindex.xml?cookies=0\u0026amp;credentials=Public_Cloud_Module_15_SP2_x86_64\u0026#39;: Error code: Connection failed Error message: Failed to connect to scc.suse.com port 443: Connection timed out If this error is encountered, move files out of the following directory (for each storage node) and re-run the install-node-exporter-storage.sh script:\n/etc/zypp/services.d Restore VCS Content Run the vcs-restore.sh script to restore all VCS content. This should be run from the same directory that vcs-backup.sh was run from so that the tar file can be located. If successful, this script will list the data files that have been restored.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/vcs-restore.sh\u0026#34; Re-run the csm-config-import job pod if it exists and is in Error state. Find the csm-config-import job pod:\nncn-m001# kubectl get pods -n services | grep csm-config-import If the pod exists, confirm it is not in an Error state. If the pod is in Error state, then delete it:\nncn-m001# CSM_CONFIG_POD=$(kubectl get pods --no-headers -o custom-columns=\u0026#34;:metadata.name\u0026#34; -n services | grep csm-config-import) ncn-m001# echo $CSM_CONFIG_POD ncn-m001# kubectl delete pod -n services $CSM_CONFIG_POD Disable TPM Kernel Module Disable the TPM kernel module from being loaded by the GRUB bootloader.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/tpm-fix-install.sh\u0026#34; Run Validation Checks (Post-Upgrade) IMPORTANT: Wait at least 15 minutes after upgrade.sh completes to let the various Kubernetes resources get initialized and started.\nRun the following validation checks to ensure that everything is still working properly after the upgrade:\nPlatform health checks Network health checks Other health checks may be run as desired.\nCAUTION: The following HMS functional tests may fail because of locked components in HSM:\ntest_bss_bootscript_ncn-functional_remote-functional.tavern.yaml test_smd_components_ncn-functional_remote-functional.tavern.yaml Traceback (most recent call last): File \u0026#34;/usr/lib/python3.8/site-packages/tavern/schemas/files.py\u0026#34;, line 106, in verify_generic verifier.validate() File \u0026#34;/usr/lib/python3.8/site-packages/pykwalify/core.py\u0026#34;, line 166, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/0\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/5\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/6\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/7\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/8\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/9\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/10\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/11\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/12\u0026#39;.: Path: \u0026#39;/\u0026#39;\u0026gt; Failures of these tests because of locked components as shown above can be safely ignored.\nNOTE: If you plan to do any further CSM health validation, you should follow the validation procedures found in the CSM v1.0 documentation. Some of the information in the CSM v0.9 validation documentation is no longer accurate in CSM v1.0.\nVerify CSM Version in Product Catalog Verify the CSM version has been updated in the product catalog. Verify that the following command includes version 0.9.4:\nncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r -j - | jq -r \u0026#39;to_entries[] | .key\u0026#39; 0.9.4 0.9.3 Confirm the import_date reflects the timestamp of the upgrade:\nncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r - \u0026#39;\u0026#34;0.9.4\u0026#34;.configuration.import_date\u0026#39; Exit Typescript Remember to exit your typescript.\nncn-m001# exit It is recommended to save the typescript file for later reference.\n"
},
{
	"uri": "/docs-csm/en-09/troubleshooting/",
	"title": "CSM Troubleshooting Information",
	"tags": [],
	"description": "",
	"content": "CSM Troubleshooting Information This document provides troubleshooting information for services and functionality provided by CSM.\nTopics Known Issues CFS Sessions are Stuck in a Pending State Orphaned CFS Pods After Booting or Rebooting Known Issues Listing of known issues and procedures to workaround them in this CSM release.\n"
},
{
	"uri": "/docs-csm/en-09/upgrade/0.9/csm-0.9.5/",
	"title": "CVE-2021-22555 CVE-2021-33909",
	"tags": [],
	"description": "",
	"content": "CVE-2021-22555 CVE-2021-33909 This procedure covers patching CVE-2021-22555 and CVE-2021-33909 on Shasta V1.4.X (and upgrades CSM to v0.9.5). These special directions are only for Linux dependencies, such as the kernel and internal packages compiled against the kernel.\nA high-level overview of the procedure is as follows:\ninstall the new kernel directly to NCNs delete the existing artifacts from S3 upload the new artifacts to S3 reboot NCNs Procedures:\nPreparation Run Validation Checks (Pre-Upgrade) Remove zypper RIS services repositories Run CVE Patch Script Reboot NCNs Validate NCNs Running Patched Kernel Upgrade Services Run Validation Checks (Post-Upgrade) Verify CSM Version in Product Catalog Update UAS/UAI Exit Typescript Preparation Start a typescript to capture the commands and output from this procedure.\nncn-m001# script -af csm-update.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; NOTE: Installed CSM versions may be listed from the product catalog using:\nncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r -j - | jq -r \u0026#39;to_entries[] | .key\u0026#39; | sort -V 0.9.2 0.9.3 0.9.4 Set CSM_DISTDIR to the directory of the extracted release distribution for CSM 0.9.5:\nNOTE: Use --no-same-owner and --no-same-permissions options to tar when extracting a CSM release distribution as root to ensure the current umask value.\nIf using a release distribution:\nncn-m001# tar --no-same-owner --no-same-permissions -zxvf csm-0.9.5.tar.gz ncn-m001# export CSM_DISTDIR=\u0026#34;$(pwd)/csm-0.9.5\u0026#34; Set CSM_RELEASE_VERSION to the version reported by ${CSM_DISTDIR}/lib/version.sh:\nncn-m001# CSM_RELEASE_VERSION=\u0026#34;$(${CSM_DISTDIR}/lib/version.sh --version)\u0026#34; ncn-m001# echo $CSM_RELEASE_VERSION Install/upgrade CSI.\nlinux# rpm -Uvh --force ${CSM_DISTDIR}/rpm/cray/csm/sle-15sp2/x86_64/cray-site-init-*.x86_64.rpm When installing or upgrading CSI the following error may appear, it can be safely ignored.\nrm: cannot remove ‘/usr/bin/sic’: No such file or directory Download and install/upgrade the latest documentation RPM. If this machine does not have direct internet access these RPMs will need to be externally downloaded and then copied to be installed.\nncn-m001# rpm -Uvh https://storage.googleapis.com/csm-release-public/shasta-1.4/docs-csm/docs-csm-latest.noarch.rpm Set CSM_SCRIPTDIR to the scripts directory included in the docs-csm RPM for the CSM 0.9.5 patch:\nncn-m001# export CSM_SCRIPTDIR=/usr/share/doc/metal/upgrade/0.9/csm-0.9.5/scripts Run Validation Checks (Pre-Upgrade) It is important to first verify a healthy starting state. To do this, run the CSM validation checks. If any problems are found, correct them and verify the appropriate validation checks before proceeding.\nRemove zypper RIS services repositories Run lib/remove-service-repos.sh to remove repositories that are external to the system.\nncn-m001# ${CSM_SCRIPTDIR}/remove-service-repos.sh Run CVE Patch Script The run-patch.sh script expects that the TOKEN environment variable is set. Either set this to a valid token of your choosing or get a new one using the following:\nncn-m001# export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) The script also expects that the Cray CLI is configured and authenticated. Please see Initialize cray CLI for more information on how to do this.\nRun the run-patch.sh script. This does a few things:\nUpdates all the NCNs via zypper to have the latest patched packages.\nPatches the kernel/initrd/squash image to have the correctly patched assets.\nApplies a pod priority to essential deployments to ensure that they are scheduled when rebooting the NCNs.\nThis step requires the latest SUSE updates tarball has been extracted and installed (i.e., synced with Nexus).\nPlease see section, \u0026ldquo;Install SLE for V1.4.2A-security0821 Patch\u0026rdquo; in the main patch README if you have not already.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/run-patch.sh\u0026#34; DO NOT REBOOT\nThe zypper commands issued by the run-patch.sh script may indicate a reboot is needed at several points during the script run but this will happen in a later step so do not reboot the NCNs yet.\nReboot NCNs Reference the Reboot NCNs procedure.\nValidate NCNs Running Patched Kernel Start a typescript to capture the commands and output from this procedure.\nncn-m001# script -af csm-update-post-reboot.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Setup CSM_DISTDIR to point toward the location where the extracted csm-0.9.5 tarball.\nIf the tarball was not extracted to ~/csm-0.9.5, then provide the alternative path instead.\nncn-m001# CSM_DISTDIR=~/csm-0.9.5 Once a system has booted, verify the new kernel is running on each NCN. This should match 5.3.18-24.75-default, which is the version of the kernel that addresses the CVE.\nncn-m001# cd \u0026#34;$CSM_DISTDIR\u0026#34; ncn-m001# pdsh -w $(./lib/list-ncns.sh| paste -sd,) \u0026#34;uname -r\u0026#34; + Getting admin-client-auth secret + Obtaining access token + Querying SLS ncn-s003: 5.3.18-24.75-default ncn-s002: 5.3.18-24.75-default ncn-s001: 5.3.18-24.75-default ncn-m001: 5.3.18-24.75-default ncn-m002: 5.3.18-24.75-default ncn-m003: 5.3.18-24.75-default ncn-w003: 5.3.18-24.75-default ncn-w001: 5.3.18-24.75-default ncn-w002: 5.3.18-24.75-default Alternatively, login to each NCN and run the following command to get get currently running kernel version.\nncn# uname -r Setup Nexus Run lib/setup-nexus.sh to configure Nexus and upload new CSM RPM repositories, container images, and Helm charts:\nncn-m001# cd \u0026#34;$CSM_DISTDIR\u0026#34; ncn-m001# ./lib/setup-nexus.sh On success, setup-nexus.sh will output OK on stderr and exit with status code 0, e.g.:\nncn-m001# ./lib/setup-nexus.sh ... + Nexus setup complete setup-nexus.sh: OK ncn-m001# echo $? 0 In the event of an error, consult the known issues from the install documentation to resolve potential problems and then try running setup-nexus.sh again. Note that subsequent runs of setup-nexus.sh may report FAIL when uploading duplicate assets. This is ok as long as setup-nexus.sh outputs setup-nexus.sh: OK and exits with status code 0.\nUpgrade Services Run upgrade.sh to deploy upgraded CSM applications and services:\nncn-m001# cd \u0026#34;$CSM_DISTDIR\u0026#34; ncn-m001# ./upgrade.sh Update UAS / UAI This update includes a new basic UAI image and a new Broker UAI image. The HPE supplied basic UAI image, cray-uai-sles15sp1:latest simply needs to be updated by pulling it to the NCN worker nodes and restarting the UAI Kubernetes pods that are using it. The following commands ensure that the updated images are used for non-Broker and Broker UAIs:\nncn-m001:~ # pdsh -w ncn-w[000-999] crictl pull dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest 2\u0026gt;\u0026amp;1 | grep -v -e \u0026#34;Could not resolve hostname\u0026#34; -e \u0026#34;ssh exited with exit code 255\u0026#34; ncn-m001:~ # pdsh -w ncn-w[000-999] crictl pull dtr.dev.cray.com/cray/cray-uai-broker:latest 2\u0026gt;\u0026amp;1 | grep -v -e \u0026#34;Could not resolve hostname\u0026#34; -e \u0026#34;ssh exited with exit code 255\u0026#34; If you have any UAIs running, you will want to cause them to restart with the new images. If you get a non-empty list back from:\ncray uas admin uais list Then you have UAIs. If you are using Broker UAIs, there will be a mix of Broker and Non-Broker UAIs in the list. If not, you will only have non-Broker UAIs.\nThe following steps will interrupt any users who are working on UAIs (either through a broker or in legacy mode). To minimize surprise, make sure users are notified that you will be restarting UAIs before proceeding.\nTo refresh non-Broker UAIs (if you have them):\nncn-m001:~ # kubectl delete po -n user $(kubectl get po -n user | grep \u0026#34;^uai-\u0026#34; | awk \u0026#39;{ print $1 }\u0026#39;) To refresh Broker UAIs (if you have them):\nncn-m001:~ # kubectl delete po -n uas $(kubectl get po -n uas | grep \u0026#34;^uai-\u0026#34; | awk \u0026#39;{ print $1 }\u0026#39;) Finally, this update provides new Compute Node images. If your site uses UAI images built from the Compute Node Image, you will need to build new images and register the new images with UAS, then delete and recreate your running UAIs (if any).\nRun Validation Checks (Post-Upgrade) IMPORTANT: Wait at least 15 minutes after upgrade.sh completes to let the various Kubernetes resources get initialized and started.\nRun the following validation checks to ensure that everything is still working properly after the upgrade:\nPlatform health checks Network health checks Other health checks may be run as desired.\nVerify CSM Version in Product Catalog Verify the CSM version has been updated in the product catalog. Verify that the following command includes version 0.9.5:\nncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r -j - | jq -r \u0026#39;to_entries[] | .key\u0026#39; | sort -V 0.9.2 0.9.3 0.9.4 0.9.5 Confirm the import_date reflects the timestamp of the upgrade:\nncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r - \u0026#39;\u0026#34;0.9.5\u0026#34;.configuration.import_date\u0026#39; Exit Typescript Remember to exit your typescript.\nncn-m001# exit It is recommended to save the typescript file for later reference.\n"
},
{
	"uri": "/docs-csm/en-09/operations/system_management_health/troubleshoot_prometheus_alerts/",
	"title": "Troubleshoot Prometheus Alerts",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Prometheus Alerts General Prometheus Alert Troubleshooting Topics\nPostgresqlFollowerReplicationLagSMA PostgresqlHighRollbackRate PostgresqlInactiveReplicationSlot PostgresqlNotEnoughConnections CPUThrottlingHigh PostgresqlFollowerReplicationLagSMA Alerts for PostgresqlFollowerReplicationLagSMA on sma-postgres-cluster pods with slot_name=\u0026ldquo;permanent_physical_1\u0026rdquo; can be ignored. This slot_name is disabled and will be removed in a future release.\nPostgresqlHighRollbackRate Alerts for PostgresqlHighRollbackRate on spire-postgres pods can be ignored. This is caused by an idle session that requires a timeout. This will be fixed in a future release.\nPostgresqlInactiveReplicationSlot Alerts for PostgresqlInactiveReplicationSlot on sma-postgres-cluster pods with slot_name=\u0026ldquo;permanent_physical_1\u0026rdquo; can be ignored. This slot_name is disabled and will be removed in a future release.\nPostgresqlNotEnoughConnections Alerts for PostgresqlNotEnoughConnections for datname=\u0026ldquo;foo\u0026rdquo; and datname=\u0026ldquo;bar\u0026rdquo; can be ignored. These databases are not used and will be removed in a future release.\nCPUThrottlingHigh Alerts for CPUThrottlingHigh on gatekeeper-audit can be ignored. This pod is not utilized in this release.\nAlerts for CPUThrottlingHigh on CFS services such as cfs-batcher and cfs-trust can be ignored. Because CFS is idle most of the time these services have low CPU requests, and it is normal for CFS service resource usage to spike when it is in use.\n"
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/add_ldap_user_federation/",
	"title": "Add LDAP User Federation",
	"tags": [],
	"description": "",
	"content": "Add LDAP User Federation Add LDAP user federation using the Keycloak localization tool.\nPrerequisites LDAP user federation is not currently configured in Keycloak. For example, if it was not configured in Keycloak when the system was initially installed or the LDAP user federation was removed.\nProcedure Prepare to customize the customizations.yaml file.\nIf the customizations.yaml file is managed in an external Git repository (as recommended), then clone a local working tree. Replace the \u0026lt;URL\u0026gt; value in the following command before running it.\nncn-m001# git clone \u0026lt;URL\u0026gt; /root/site-init ncn-m001# cd /root/site-init If there is not a backup of site-init, perform the following steps to create a new one using the values stored in the Kubernetes cluster.\nCreate a new site-init directory using the CSM tarball.\nDetermine the location of the initial unpacked install tarball and set ${CSM_DISTDIR} accordingly.\nNOTE: If the unpacked set of CSM directories was copied, no untar action is required. If the tarball tgz file was copied, the command to unpack it is tar -zxvf CSM_RELEASE.tar.gz. Replace the CSM_RELEASE value before running the command to unpack the tarball.\nncn-m001# cp -r ${CSM_DISTDIR}/shasta-cfg/* /root/site-init ncn-m001# cd /root/site-init Extract customizations.yaml from the site-init secret.\nncn-m001# kubectl -n loftsman get secret site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d - \u0026gt; customizations.yaml Extract the certificate and key used to create the sealed secrets.\nncn-m001# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39; | base64 -d - \u0026gt; certs/sealed_secrets.crt ncn-m001# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.key}\u0026#39; | base64 -d - \u0026gt; certs/sealed_secrets.key NOTE: All subsequent steps of this procedure should be performed within the /root/site-init directory created in this step.\nRepopulate the keycloak_users_localize and cray-keycloak Sealed Secrets in the customizations.yaml file with the desired configuration.\nUpdate the LDAP settings with the desired configuration. LDAP connection information is stored in the keycloak-users-localize Secret in the customizations.yaml file.\nThe ldap_connection_url key is required and is set to an LDAP URL. The ldap_bind_dn and ldap_bind_credentials keys are optional. If the LDAP server requires authentication. then the bind DN and credentials are set in these keys respectively. For example:\ncray-keycloak: generate: name: keycloak-certs data: - type: static_b64 args: name: certs.jks value: /u3+7QAAAAIAAAAA5yXvSDt11bGXyBA9M2iy0/5i1Tg= keycloak_users_localize: generate: name: keycloak-users-localize data: - type: static args: name: ldap_connection_url value: \u0026#34;ldaps://my_ldap.my_org.test\u0026#34; - type: static args: name: ldap_bind_dn value: \u0026#34;cn=my_admin\u0026#34; - type: static args: name: ldap_bind_credentials value: \u0026#34;my_ldap_admin_password\u0026#34; The example above puts an empty certs.jks in the cray-keycloak Sealed Secret. The next step will generate certs.jks.\nOther LDAP configuration settings are set in the spec.kubernetes.services.cray-keycloak-users-localize field in the customizations.yaml file.\nThe fields are as follows:\n( Notes for the following table: format is * \u0026lt;cray-keycloak-users-localize chart option name\u0026gt; : \u0026lt;description\u0026gt; - default: \u0026lt;the default value if not overridden in customizations.yaml - type: \u0026lt;type that the value in customizations.yaml has to be. e.g., if type is string and a number is entered then you need to quote it\u0026gt; - allowed values: \u0026lt;if only certain values are allowed they are listed here\u0026gt; ) * ldapProviderId : The Keycloak provider ID for the component. This must be \u0026#34;ldap\u0026#34; - default: ldap - type: string * ldapFederationName : The name of the LDAP provider in Keycloak. If a provider with this name already exists then this tool will not create a new provider. - default: shasta-user-federation-ldap - type: string * ldapPriority : The priority of this provider when looking up users or adding a user. - default: 1 - type: string * ldapEditMode : If you want to be able to create or change users in Keycloak and have them created or modified in the LDAP server, and the LDAP server allows it, then this can be changed. - default: READ_ONLY - type: string - allowed values: READ_ONLY, WRITEABLE, or UNSYNCED * ldapSyncRegistrations : If true, then newly created users will be created in the LDAP server. - default: false - type: string - allowed values: true or false * ldapVendor: This determines some defaults for what mappers are created by default. - default: other - type: string - allowed values: Active Directory, Red Hat Directory Server, Tivoli, Novell eDirectory, or other * ldapUsernameLDAPAttribute: The LDAP attribute to map to the username in Keycloak. - default: uid - type: string * ldapRdnLDAPAttribute: The LDAP attribute being used as the users RDN. - default: uid - type: string * ldapUuidLDAPAttribute: The LDAP attribute being used as a unique ID. - default: uid - type: string * ldapUserObjectClasses: The object classes for user entries. - default: posixAccount - type: comma-separated string * ldapAuthType: Set to \u0026#34;none\u0026#34; if the LDAP server allows anonymous search for users and groups, otherwise set to \u0026#34;simple\u0026#34; to bind. - default: none - type: string - allowed values: none or simple * ldapSearchBase: The DN for the base entry to search for users and groups. - default: cn=default - type: string * ldapSearchScope: The search scope to use when searching for users or groups: 2 for subtree, 1 for onelevel - default: 2 - type: string - allowed values: 1 or 2 * ldapUseTruststoreSpi: Determines if the truststore is used to validate the server certificate when connecting to the server. - default: ldapsOnly - type: string - allowed values: ldapsOnly, always, never * ldapConnectionPooling: If true then Keycloak will use a connection pool of LDAP connections. - default: true - type: string - allowed values: true or false * ldapPagination: Set to true if the LDAP server supports or requires use of the paging extension. - default: true - type: string - allowed values: true or false * ldapAllowKerberosAuthentication: - Set to true to enable HTTP authentication of users with SPNEGO/Kerberos tokens. - default: false - type: string * ldapBatchSizeForSync: Count of LDAP users to be imported from LDAP to Keycloak in a single transaction. - default: 4000 - type: string * ldapFullSyncPeriod: If a positive number, this is the number of seconds between automatic full user synchronization operations; if negative then full user synchronization operations will not be done automatically. - default: -1 - type: string * ldapChangedSyncPeriod: f a positive number, this is the number of seconds between automatic changed user synchronization operations; if negative then changed user synchronization operations will not be done automatically. - default: -1 - type: string * ldapDebug: Set to true to enable extra logging of LDAP operations. - default: true - allowed values: true or false * ldapUserAttributeMappers: Extra attribute mappers to create so that users have attributes required by Shasta software. The Keycloak attribute that the LDAP attribute maps to will be the same. - default: [uidNumber, gidNumber, loginShell, homeDirectory] - type: list of strings * ldapUserAttributeMappersToRemove: These attribute mappers will be removed, to be used in the case where the default attribute mappers are not appropriate. For example, this could be used to remove the email mapper if email addresses are not unique. - default: [] - type: list of strings * ldapGroupNameLDAPAttribute: The LDAP attribute to map to the group name in Keycloak. - default: cn - type: string * ldapGroupObjectClass: The object classes for group entries. - default: posixGroup - type: comma-separated string * ldapPreserveGroupInheritance: Whether group inheritance should be propagated to Keycloak or not. - default: false - type: string - allowed values: true or false * ldapMembershipLDAPAttribute: Name of the LDAP attribute that refers to the group members. - default: memberUid - type: string * ldapMembershipAttributeType: If the member attribute contains the DN for the user, then set this to DN. If the member attribute is the UID of the entry then set this to UID. - default: UID - type: string - allowed values: UID or DN * ldapMembershipUserLDAPAttribute: If the ldapMembershipAttributeType is UID then this is the LDAP attribute containing the UID value, otherwise this is ignored. - default: uid - type: string * ldapGroupsLDAPFilter: Extra filter to include when searching for group entries. If this is not the empty string the value must start with the ( character and end with ). - default: \u0026#34;\u0026#34; - type: string * ldapUserRolesRetrieveStrategy: Defines how to retrieve groups for a user. - default: LOAD_GROUPS_BY_MEMBER_ATTRIBUTE - type: string - allowed values: LOAD_GROUPS_BY_MEMBER_ATTRIBUTE, GET_GROUPS_FROM_USER_MEMBEROF_ATTRIBUTE, LOAD_GROUPS_BY_MEMBER_ATTRIBUTE_RECURSIVELY * ldapMappedGroupAttributes: Attributes of the group that will be added as attributes of the user. Some Shasta REST API operations require the user to have a gidNumber and this adds that attribute from the LDAP group. - default: cn,gidNumber,memberUid - type: comma-separated string * ldapDropNonExistingGroupsDuringSync: If true, groups that are not in LDAP will be deleted when synchronizing. - default: false - type: string - allowed values: true or false * ldapDoFullSync: Tells the HPE Cray EX Keycloak localization tool to perform an immediate full user synchronization after configuring the LDAP integration. - default: true - type: string * ldapRoleMapperDn: If this is an empty string then a role mapper is not created, otherwise this the the DN used as the search base to find role entries. - default: \u0026#34;\u0026#34; - type: string * ldapRoleMapperRoleNameLDAPAttribute: The LDAP attribute to map to the role name in Keycloak. - default: cn - type: string * ldapRoleMapperRoleObjectClasses: The object classes for role entries. - default: groupOfNames - type: string * ldapRoleMapperLDAPAttribute: Name of the LDAP attribute that refers to the group members. - default: member - type: string * ldapRoleMapperMemberAttributeType: If the member attribute contains the DN for the user, then set this to DN. If the member attribute is the UID of the entry then set this to UID. - default: DN - type: string - allowed values: UID or DN * ldapRoleMapperUserLDAPAttribute: If the ldapRoleMapperMemberAttributeType is UID then this is the LDAP attribute containing the UID value, otherwise this is ignored. - default: sAMAccountName - type: string * ldapRoleMapperRolesLDAPFilter: Extra filter to include when searching for group entries. If this is not the empty string the value must start with the ( character and end with ). - default: \u0026#34;\u0026#34; - type: string * ldapRoleMapperMode: Specifies how to retrieve roles for the user. - default: READ_ONLY - allowed values: READ_ONLY, LDAP_ONLY, or IMPORT * ldapRoleMapperStrategy: Defines how to retrieve roles for a user. - default: LOAD_ROLES_BY_MEMBER_ATTRIBUTE - type: string - allowed values: LOAD_ROLES_BY_MEMBER_ATTRIBUTE, GET_ROLES_FROM_MEMBEROF_ATTRIBUTE, or LOAD_ROLES_BY_MEMBER_ATTRIBUTE_RECURSIVELY * ldapRoleMapperMemberOfLDAPAttribute: Only used when ldapRoleMapperStrategy is GET_ROLES_FROM_MEMBEROF_ATTRIBUTE where it is the LDAP attribute in the user entry that contains the roles that the user has. - default: memberOf - type: string * ldapRoleMapperUseRealmRolesMapping: If true then LDAP role mappings will be mapped to realm role mappings in Keycloak, otherwise the LDAP role mappings will be mapped to client role mappings. - default: false - type: string - allowed values: true or false * ldapRoleMapperClientId: If ldapRoleMapperUseRealmRolesMapping is false then this is the client ID to apply the roles to. - default: shasta - type: string (Optional) Add the LDAP CA certificate in the certs.jks section of customizations.yaml.\nIf LDAP requires TLS (recommended), update the cray-keycloak Sealed Secret value by supplying a base64 encoded Java KeyStore (JKS) that contains the CA certificate that signed the LDAP server\u0026rsquo;s host key. The password for the JKS file must be password.\nAdministrators may use the keytool command from the openjdk:11-jre-slim container image packaged with CSM to create a JKS file that includes a PEM-encoded CA certificate to verify the LDAP host(s).\nLoad the openjdk container image.\nNOTE: Requires a properly configured Docker or Podman environment.\nncn-m001# ${CSM_DISTDIR}/hack/load-container-image.sh dtr.dev.cray.com/library/openjdk:11-jre-slim Troubleshooting:\nIf the output shows the skopeo.tar file cannot be found, ensure that the $CSM_DISTDIR directory looks correct, and contains the dtr.dev.cray.com directory that includes the originally installed docker images.\nThe following is an example of the skopeo.tar file not being found:\n++ podman load -q -i ./hack/../vendor/skopeo.tar ++ sed -e \u0026#39;s/^.*: //\u0026#39; + SKOPEO_IMAGE= If the following overlay error is returned, it could be caused by an earlier podman invocation using a different configuration:\n\u0026#34;ERRO[0000] [graphdriver] prior storage driver overlay failed: \u0026#39;overlay\u0026#39; is not supported over overlayfs, a mount_program is required: backing file system is unsupported for this graph driver\u0026#34; To recover podman, move the overlay directories to a backup folder as follows:\nncn-m001# mkdir /var/lib/containers/storage/backup ncn-m001# mv /var/lib/containers/storage/overlay* /var/lib/containers/storage/backup This should allow load-container-images.sh to succeed.\nCreate (or update) cert.jks with the PEM-encoded CA certificate for an LDAP host.\nIMPORTANT: Replace \u0026lt;ca-cert.pem\u0026gt; and \u0026lt;alias\u0026gt; before running the command.\nncn-m001# podman run --rm -v \u0026#34;$(pwd):/data\u0026#34; dtr.dev.cray.com/library/openjdk:11-jre-slim keytool \\ -importcert -trustcacerts -file /data/\u0026lt;ca-cert.pem\u0026gt; -alias \u0026lt;alias\u0026gt; -keystore /data/certs.jks \\ -storepass password -noprompt Set variables for the LDAP server.\nIn the following example, the LDAP server has the hostname dcldap2.hpc.amslabs.hpecorp.net and is using the port 636.\nncn-m001# export LDAP=dcldap2.hpc.amslabs.hpecorp.net ncn-m001# export PORT=636 Get the issuer certificate for the LDAP server at port 636. Use openssl s_client to connect and show the certificate chain returned by the LDAP host.\nncn-m001# openssl s_client -showcerts -connect $LDAP:${PORT} \u0026lt;/dev/null Either manually extract (cut/paste) the issuer\u0026rsquo;s certificate into cacert.pem, or try the following commands to create it automatically.\nNOTE: The following commands were verified using OpenSSL version 1.1.1d and use the -nameopt RFC2253 option to ensure consistent formatting of distinguished names (DNs). Unfortunately, older versions of OpenSSL may not support -nameopt on the s_client command or may use a different default format. As a result, mileage may vary; however, administrators should be able to extract the issuer certificate manually from the output of the above openssl s_client example if the following commands are unsuccessful.\nObserve the issuer\u0026rsquo;s DN.\nFor example:\nncn-m001# openssl s_client -showcerts -nameopt RFC2253 -connect $LDAP:${PORT} \u0026lt;/dev/null 2\u0026gt;/dev/null | grep issuer= | sed -e \u0026#39;s/^issuer=//\u0026#39; emailAddress=dcops@hpe.com,CN=Data Center,OU=HPC/MCS,O=HPE,ST=WI,C=US Then, extract the issuer\u0026rsquo;s certificate using the awk command:\nNOTE: The issuer DN is properly escaped as part of the awk pattern below. If the value being used is different, be sure to escape it properly!\nncn-m001# openssl s_client -showcerts -nameopt RFC2253 -connect $LDAP:${PORT} \u0026lt;/dev/null 2\u0026gt;/dev/null | \\ awk \u0026#39;/s:emailAddress=dcops@hpe.com,CN=Data Center,OU=HPC\\/MCS,O=HPE,ST=WI,C=US/,/END CERTIFICATE/\u0026#39; | \\ awk \u0026#39;/BEGIN CERTIFICATE/,/END CERTIFICATE/\u0026#39; \u0026gt; cacert.pem Verify the issuer\u0026rsquo;s certificate was properly extracted and saved in cacert.pem.\nncn-m001# cat cacert.pem Expected output looks similar to the following:\n-----BEGIN CERTIFICATE----- MIIDvTCCAqWgAwIBAgIUYxrG/PrMcmIzDuJ+U1Gh8hpsU8cwDQYJKoZIhvcNAQEL BQAwbjELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAldJMQwwCgYDVQQKDANIUEUxEDAO BgNVBAsMB0hQQy9NQ1MxFDASBgNVBAMMC0RhdGEgQ2VudGVyMRwwGgYJKoZIhvcN AQkBFg1kY29wc0BocGUuY29tMB4XDTIwMTEyNDIwMzM0MVoXDTMwMTEyMjIwMzM0 MVowbjELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAldJMQwwCgYDVQQKDANIUEUxEDAO BgNVBAsMB0hQQy9NQ1MxFDASBgNVBAMMC0RhdGEgQ2VudGVyMRwwGgYJKoZIhvcN AQkBFg1kY29wc0BocGUuY29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC AQEAuBIZkKitHHVQHymtaQt4D8ZhG4qNJ0cTsLhODPMtVtBjPZp59e+PWzbc9Rj5 +wfjLGteK6/fNJsJctWlS/ar4jw/xBIPMk5pg0dnkMT2s7lkSCmyd9Uib7u6y6E8 yeGoGcb7I+4ZI+E3FQV7zPact6b17xmajNyKrzhBGEjYucYJUL5iTgZ6a7HOZU2O aQSXe7ctiHBxe7p7RhHCuKRrqJnxoohakloKwgHHzDLFQzX/5ADp1hdJcduWpaXY RMBu6b1mhmwo5vmc+fDnfUpl5/X4i109r9VN7JC7DQ5+JX8u9SHDGLggBWkrhpvl bNXMVCnwnSFfb/rnmGO7rdJSpwIDAQABo1MwUTAdBgNVHQ4EFgQUVg3VYExUAdn2 WE3e8Xc8HONy/+4wHwYDVR0jBBgwFoAUVg3VYExUAdn2WE3e8Xc8HONy/+4wDwYD VR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAWLDQLB6rrmK+gwUY+4B7 0USbQK0JkLWuc0tCfjTxNQTzFb75PeH+GH21QsjUI8VC6QOAAJ4uzIEV85VpOQPp qjz+LI/Ej1xXfz5ostZQu9rCMnPtVu7JT0B+NV7HvgqidTfa2M2dw9yUYS2surZO 8S0Dq3Bi6IEhtGU3T8ZpbAmAp+nNsaJWdUNjD4ECO5rAkyA/Vu+WyMz6F3ZDBmRr ipWM1B16vx8rSpQpygY+FNX4e1RqslKhoyuzXfUGzyXux5yhs/ufOaqORCw3rJIx v4sTWGsSBLXDsFM3lBgljSAHfmDuKdO+Qv7EqGzCRMpgSciZihnbQoRrPZkOHUxr NA== -----END CERTIFICATE----- Create certs.jks.\nncn-m001# podman run --rm -v \u0026#34;$(pwd):/data\u0026#34; dtr.dev.cray.com/library/openjdk:11-jre-slim keytool -importcert \\ -trustcacerts -file /data/cacert.pem -alias cray-data-center-ca -keystore /data/certs.jks \\ -storepass password -noprompt Create certs.jks.b64 by base-64 encoding certs.jks.\nncn-m001# base64 certs.jks \u0026gt; certs.jks.b64 Inject and encrypt certs.jks.b64 into customizations.yaml.\nncn-m001# cat \u0026lt;\u0026lt;EOF | yq w - \u0026#39;data.\u0026#34;certs.jks\u0026#34;\u0026#39; \u0026#34;$(\u0026lt;certs.jks.b64)\u0026#34; | \\ yq r -j - | /root/site-init/utils/secrets-encrypt.sh | \\ yq w -f - -i /root/site-init/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray-keycloak\u0026#39; { \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;keycloak-certs\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: {} } EOF Prepare to generate Sealed Secrets.\nSecrets are stored in customizations.yaml as SealedSecret resources (encrypted secrets), which are deployed by specific charts and decrypted by the Sealed Secrets operator. But first, those Secrets must be seeded, generated, and encrypted.\nncn-m001# ./utils/secrets-reencrypt.sh customizations.yaml ./certs/sealed_secrets.key ./certs/sealed_secrets.crt Encrypt the static values in the customizations.yaml file after making changes.\nThe following command must be run within the site-init directory.\nncn-m001# ./utils/secrets-seed-customizations.sh customizations.yaml Expected output looks similar to:\nCreating Sealed Secret keycloak-certs Generating type static_b64... Creating Sealed Secret keycloak-master-admin-auth Generating type static... Generating type static... Generating type randstr... Generating type static... Creating Sealed Secret cray_reds_credentials Generating type static... Generating type static... Creating Sealed Secret cray_meds_credentials Generating type static... Creating Sealed Secret cray_hms_rts_credentials Generating type static... Generating type static... Creating Sealed Secret vcs-user-credentials Generating type randstr... Generating type static... Creating Sealed Secret generated-platform-ca-1 Generating type platform_ca... Creating Sealed Secret pals-config Generating type zmq_curve... Generating type zmq_curve... Creating Sealed Secret munge-secret Generating type randstr... Creating Sealed Secret slurmdb-secret Generating type static... Generating type static... Generating type randstr... Generating type randstr... Creating Sealed Secret keycloak-users-localize Generating type static... Decrypt the Sealed Secret to verify it was generated correctly.\nncn-m001# ./utils/secrets-decrypt.sh keycloak_users_localize | jq -r \u0026#39;.data.ldap_connection_url\u0026#39; | base64 --decode ldaps://my_ldap.my_org.test Re-apply the cray-keycloak Helm chart with the updated customizations.yaml file.\nRetrieve the current platform.yaml manifest.\nncn-m001# kubectl -n loftsman get cm loftsman-platform -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; \u0026gt; platform.yaml Remove all charts from the platform.yaml except for cray-keycloak.\nEdit the platform.yaml file and delete all sections starting with -name: \u0026lt;chart_name\u0026gt;, except for the cray-keycloak section.\nThen, change the name of the manifest being deployed from platform to cray-keycloak:\nncn-m001:# sed -i \u0026#39;s/name: platform/name: cray-keycloak/\u0026#39; platform.yaml Populate the platform manifest with data from the customizations.yaml file.\nncn-m001# manifestgen -i platform.yaml -c customizations.yaml -o new-platform.yaml Re-apply the platform manifest with the updated cray-keycloak chart.\nncn-m001# loftsman ship --manifest-path ./new-platform.yaml --charts-repo https://packages.local/repository/charts Wait for the keycloak-certs secret to reflect the new cert.jks.\nRun the following command until there is a non-empty value in the secret (this can take a minute or two):\nncn-m001# kubectl get secret -n services keycloak-certs -o yaml | grep certs.jks certs.jks: \u0026lt;REDACTED\u0026gt; Restart the cray-keycloak-[012] pods.\nncn-m001# kubectl rollout restart statefulset -n services cray-keycloak Wait for the Keycloak pods to restart before moving on to the next step.\nOnce the cray-keycloak-[012] pods have restarted, proceed to the next step.\nncn-m001# kubectl get po -n services | grep cray-keycloak Re-apply the cray-keycloak-users-localize Helm chart with the updated customizations.yaml file.\nDetermine the cray-keycloak-users-localize chart version that is currently deployed.\nncn-m001# helm ls -A -a | grep cray-keycloak-users-localize | awk \u0026#39;{print $(NF-1)}\u0026#39; cray-keycloak-users-localize-1.5.6 Create a manifest file that will be used to reapply the same chart version.\nncn-m001# cat \u0026lt;\u0026lt; EOF \u0026gt; ./cray-keycloak-users-localize-manifest.yaml apiVersion: manifests/v1beta1 metadata: name: reapply-cray-keycloak-users-localize spec: charts: - name: cray-keycloak-users-localize namespace: services version: 1.5.6 EOF Uninstall the current cray-keycloak-users-localize chart.\nncn-m001# helm del cray-keycloak-users-localize -n services Populate the deployment manifest with data from the customizations.yaml file.\nncn-m001# manifestgen -i cray-keycloak-users-localize-manifest.yaml -c customizations.yaml -o deploy.yaml Reapply the cray-keycloak-users-localize chart.\nncn-m001# loftsman ship --manifest-path ./deploy.yaml \\ --charts-repo https://packages.local/repository/charts Watch the pod to check the status of the job.\nThe pod will go through the normal Kubernetes states. It will stay in a Running state for a while, and then it will go to Completed.\nncn-m001# kubectl get pods -n services | grep keycloak-users-localize keycloak-users-localize-1-sk2hn 0/2 Completed 0 2m35s Check the pod\u0026rsquo;s logs.\nReplace the KEYCLOAK_POD_NAME value with the pod name from the previous step.\nncn-m001# kubectl logs -n services KEYCLOAK_POD_NAME keycloak-localize \u0026lt;logs showing it has updated the \u0026#34;s3\u0026#34; objects and ConfigMaps\u0026gt; 2020-07-20 18:26:15,774 - INFO - keycloak_localize - keycloak-localize complete Sync the users and groups from Keycloak to the compute nodes.\nGet the crayvcs password for pushing the changes.\nncn-m001# kubectl get secret -n services vcs-user-credentials \\ --template={{.data.vcs_password}} | base64 --decode Checkout content from the cos-config-management VCS repository.\nncn-m001# git clone https://api-gw-service-nmn.local/vcs/cray/cos-config-management.git ncn-m001# cd cos-config-management ncn-m001# git checkout integration Create the group_vars/Compute/keycloak.yaml file.\nThe file should contain the following values:\n--- keycloak_config_computes: True Push the changes to VCS with the crayvcs username.\nncn-m001# git add group_vars/Compute/keycloak.yaml ncn-m001# git commit -m \u0026#34;Configure keycloak on computes\u0026#34; ncn-m001# git push origin integration Update the Configuration Framework Service (CFS) configuration.\nncn-m001# cray cfs configurations update configurations-example \\ --file ./configurations-example.json --format json { \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:30:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; } Reboot with the Boot Orchestration Service (BOS).\nncn-m001# cray bos session create --template-uuid BOS_TEMPLATE --operation reboot Validate that LDAP integration was added successfully.\n1. Retrieve the admin password for Keycloak. ```bash ncn-m001: # kubectl get secrets -n services keycloak-master-admin-auth -ojsonpath='{.data.password}' | base64 -d ``` 2. Login to the Keycloak UI using the `admin` user and the password obtained in the previous step. The Keycloak UI URL is typically similar to the following: ``` https://auth.\u0026lt;system_name\u0026gt;/keycloak ``` 3. Click on the \u0026quot;Users\u0026quot; tab in the navigation pane on the left. 4. Click on the \u0026quot;View all users\u0026quot; button and verify the LDAP users appear in the table. 5. Verify a token can be retrieved from Keycloak using an LDAP user/password. In the example below, replace myuser, mypass, and shasta in the cURL command with site-specific values. The shasta client is created during the SMS install process. In the following example, the `python -mjson.tool` is not required; it is simply used to format the output for readability. ```bash ncn-w001# curl -s \\ -d grant_type=password \\ -d client_id=shasta \\ -d username=myuser \\ -d password=mypass \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | python -mjson.tool ``` Expected output: ```bash { \u0026quot;access_token\u0026quot;: \u0026quot;ey...IA\u0026quot;, \u0026lt;\u0026lt;-- NOTE this value, used in the following step \u0026quot;expires_in\u0026quot;: 300, \u0026quot;not-before-policy\u0026quot;: 0, \u0026quot;refresh_expires_in\u0026quot;: 1800, \u0026quot;refresh_token\u0026quot;: \u0026quot;ey...qg\u0026quot;, \u0026quot;scope\u0026quot;: \u0026quot;profile email\u0026quot;, \u0026quot;session_state\u0026quot;: \u0026quot;10c7d2f7-8921-4652-ad1e-10138ec6fbc3\u0026quot;, \u0026quot;token_type\u0026quot;: \u0026quot;bearer\u0026quot; } ``` 6. Validate that the `access_token` looks correct. Copy the `access_token` from the previous step and open a browser window. Navigate to http://jwt.io, and paste the token in the \u0026quot;Encoded\u0026quot; field. Verify the `preferred_username` is the expected LDAP user and the role is `admin` (or other role based on the user). "
},
{
	"uri": "/docs-csm/en-09/operations/node_management/adding_a_liquid-cooled_blade_to_a_system/",
	"title": "Adding a Liquid-cooled blade to a System",
	"tags": [],
	"description": "",
	"content": "Adding a Liquid-cooled blade to a System This procedure will add a liquid-cooled blades from a HPE Cray EX system.\nPerquisites The Cray command line interface (CLI) tool is initialized and configured on the system.\nKnowledge of whether DVS is operating over the Node Management Network (NMN) or the High Speed Network (HSN).\nBlade is being added to a existing liquid-cooled cabinet in the system.\nThe Slingshot fabric must be configured with the desired topology for desired state of the blades in the system.\nThe System Layout Service (SLS) must have the desired HSN configuration.\nCheck the status of the high-speed network (HSN) and record link status before the procedure.\nReview the following command examples. The commands can be used to capture the required values from the HSM ethernetInterfaces table and write the values to a file. The file then can be used to automate subsequent commands in this procedure, for example:\nncn-m001# mkdir blade_swap_scripts; cd blade_swap_scripts ncn-m001# cat blade_query.sh #!/bin/bash BLADE=$1 OUTFILE=$2 BLADE_DOT=$BLADE. cray hsm inventory ethernetInterfaces list --format json | jq -c --arg BLADE \u0026#34;$BLADE_DOT\u0026#34; \u0026#39;map(select(.ComponentID|test($BLADE))) | map(select(.Description == \u0026#34;Node Maintenance Network\u0026#34;)) | .[] | {xname: .ComponentID, ID: .ID,MAC: .MACAddress, IP: .IPAddresses[0].IPAddress,Desc: .Description}\u0026#39; \u0026gt; $OUTFILE ncn-m001# ./blade_query.sh x1000c0s1 x1000c0s1.json ncn-m001# cat x1000c0s1.json {\u0026#34;xname\u0026#34;:\u0026#34;x1000c0s1b0n0\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a6836339\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:63:39\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.10\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} {\u0026#34;xname\u0026#34;:\u0026#34;x1000c0s1b0n1\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a683633a\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:63:3a\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.98\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} {\u0026#34;xname\u0026#34;:\u0026#34;x1000c0s1b1n0\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a68362e2\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:62:e2\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.123\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} {\u0026#34;xname\u0026#34;:\u0026#34;x1000c0s1b1n1\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a68362e3\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:62:e3\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.122\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} To delete anethernetInterfaces entry using curl:\nncn-m001# for ID in $(cat x1000c0s1.json | jq -r \u0026#39;.ID\u0026#39;); do cray hsm inventory ethernetInterfaces delete $ID; done To insert an ethernetInterfaces entry using curl:\nncn-m001# while read PAYLOAD ; do curl -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; -L -X POST \u0026#39;https://api-gw-service-nmn.local/apis/smd/hsm/v1/Inventory/EthernetInterfaces\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; --data-raw \u0026#34;$(echo $PAYLOAD | jq -c \u0026#39;{ComponentID: .xname,Description: .Desc,MACAddress: .MAC,IPAddress: .IP}\u0026#39;)\u0026#34;;sleep 5; done \u0026lt; x1000c0s1.json The blades must have the coolant drained and filled during the swap to minimize cross-contamination of cooling systems.\nReview procedures in HPE Cray EX Coolant Service Procedures H-6199 Review the HPE Cray EX Hand Pump User Guide H-6200 Procedure Suspend the hms-discovery cron job to disable it.\nncn-m001# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : true }}\u0026#39; Verify that the hms-discovery cron job has stopped.\nncn-m001# kubectl get cronjobs -n services hms-discovery Example output. Note the ACTIVE = 0 and is SUSPEND = True in the output indicating the job has been suspended:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * True 0 117s 15d Determine if the destination chassis slot is populated. This example is checking slot 0 in chassis 3 of cabinet x1005.\nncn-m001# cray hsm state components describe x1005c3s0 Example output:\nID = \u0026#34;x1005c3s0\u0026#34; Type = \u0026#34;ComputeModule\u0026#34; State = \u0026#34;Empty\u0026#34; Flag = \u0026#34;OK\u0026#34; Enabled = true NetType = \u0026#34;Sling\u0026#34; Arch = \u0026#34;X86\u0026#34; Class = \u0026#34;Mountain\u0026#34; If the state of the slot is On or Off, then the chassis slot is populated. If the state of the slot is Empty, then the chassis slot is not populated.\nSkip this step if the chassis slot is unpopulated. Verify the chassis slot is powered off.\nncn-m001# cray capmc get_xname_status create --xnames x1005c3s0 Example output:\ne = 0 err_msg = \u0026#34;\u0026#34; off = [ \u0026#34;x1005c3s0\u0026#34;,] If the slot is powered on, then power the chassis slot off.\nncn-m001# cray capmc xname_off create --xnames x1005c3s0 --recursive true Install the the blade into the system into the desired location.\nObtain an authentication token to access the API gateway.\nncn-m001# export TOKEN=$(curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Preserve node xname to IP address mapping Skip this step if DVS is operating over the HSN, otherwise proceed with this step. When DVS is operating over the NMN, and a blade is being replaced the mapping of node xname to node IP address must be preserved. Kea automatically adds entries to the HSM ethernetInterfaces table when DHCP lease is provided (about every 5 minutes). To prevent from Kea from automatically adding MAC entries to the HSM ethernetInterfaces table, use the following commands:\nCreate an eth_interfaces file that contains the interface IDs for the Node Maintenance Network entries for the destination blade location. If there has not been a blade previously in the destination location there may not be any Ethernet Interfaces to delete from HSM.\nThe blade_query.sh script from the perquisites section can help determine the IDs for the HSM Ethernet Interfaces associated with the blade if any. It is expected that if a blade has not been populated in the slot before that no HSM Ethernet Interfaces IDs would be found.\nncn-m001# cat eth_interfaces 0040a6836339 0040a683633a 0040a68362e2 0040a68362e3 Run the following commands in succession to remove the interfaces if any. Delete the cray-dhcp-kea pod to prevent the interfaces from being re-created.\nncn-m001# kubectl get pods -Ao wide | grep kea ncn-m001# kubectl delete -n services pod CRAY_DHCP_KEA_PODNAME ncn-m001# for ETH in $(cat eth_interfaces); do cray hsm inventory ethernetInterfaces delete $ETH --format json ; done Skip this step if the destination blade location has not been previously populated with a blade Add the MAC and IP addresses and also the Node Maintenance Network description to the interfaces. The ComponentID and IPAddress must be the values recorded from the blade previously in the destination location and the MACAddress must be the value recorded from the blade. These values were recorded if the blade was removed via the Removing a Liquid-cooled blade from a System procedure.\nValues recorded from the blade that was was previously in the slot.\nComponentID: \u0026#34;x1005c3s0b0n0\u0026#34; MACAddress: \u0026#34;00:40:a6:83:63:99\u0026#34; IPAddress: \u0026#34;10.10.0.123\u0026#34; ncn-m001# MAC=NEW_BLADE_MAC_ADDRESS ncn-m001# IP_ADDRESS=DESTLOCATION_IP_ADDRESS ncn-m001# XNAME=DESTLOCATION_XNAME ncn-m001# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -L -X POST \u0026#39;https://api-gw-service-nmn.local/apis/smd/hsm/v1/Inventory/EthernetInterfaces\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; --data-raw \u0026#34;{ \\\u0026#34;Description\\\u0026#34;: \\\u0026#34;Node Maintenance Network\\\u0026#34;, \\\u0026#34;MACAddress\\\u0026#34;: \\\u0026#34;$MAC\\\u0026#34;, \\\u0026#34;IPAddress\\\u0026#34;: \\\u0026#34;$IP_ADDRESS\\\u0026#34;, \\\u0026#34;ComponentID\\\u0026#34;: \\\u0026#34;$XNAME\\\u0026#34; }\u0026#34; Note: Kea may must be restarted when the curl command is issued.\nncn-m001# kubectl delete pods -n services -l app.kubernetes.io/name=cray-dhcp-kea To change or correct a curl command that has been entered, use a PATCH request, for example:\nncn-m001# curl -k -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; -L -X PATCH \\ \u0026#39;https://api-gw-service-nmn.local/apis/smd/hsm/v1/Inventory/EthernetInterfaces/0040a68350a4\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; --data-raw \u0026#39;{\u0026#34;MACAddress\u0026#34;:\u0026#34;xx:xx:xx:xx:xx:xx\u0026#34;,\u0026#34;IPAddress\u0026#34;:\u0026#34;10.xxx.xxx.xxx\u0026#34;,\u0026#34;ComponentID\u0026#34;:\u0026#34;XNAME\u0026#34;}\u0026#39; Repeat the preceding command for each node in the blade.\nRe-enable hms-discovery cron job Rediscover the ChassisBMC (the example shows cabinet 1005, chassis 3). Rediscovering the ChassisBMC will update HSM to become aware of the newly populated slot and allow CAPMC to perform power actions on the slot.\nncn-m001# cray hsm inventory discover create --xnames x1005c3b0 Verify that discovery of the ChassisBMC has completed (LastDiscoveryStatus = \u0026ldquo;DiscoverOK\u0026rdquo;).\nncn-m001# cray hsm inventory redfishEndpoints describe x1005c3b0 --format json Example output:\n{ \u0026#34;ID\u0026#34;: \u0026#34;x1005c3b0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;ChassisBMC\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;x1005c3b0\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x1005c3\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MACAddr\u0026#34;: \u0026#34;02:03:ED:03:00:00\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2020-09-03T19:03:47.989621Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;1.2.0\u0026#34; } } Unsuspend the hms-discovery cronjob to re-enable the hms-discovery job.\nncn-m001# kubectl -n services patch cronjobs hms-discovery \\ -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : false }}\u0026#39; Verify the hms-discovery job has been unsuspended:\nncn-m001# kubectl get cronjobs.batch -n services hms-discovery Example output. Note the ACTIVE = 1 and is SUSPEND = False in the output indicating the job has been unsuspended:\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * False 1 41s 33d Enable and power on the chassis slot Enable the chassis slot. The example enables slot 0, chassis 3, in cabinet 1005.\nncn-m001# cray hsm state components enabled update --enabled true x1005c3s0 Power on the chassis slot. The example powers on slot 0, chassis 3, in cabinet 1005.\nncn-m001# cray capmc xname_on create --xnames x1005c3s0 --recursive true Wait at least 3 minutes for the blade to power on and the node controllers (BMCs) to be discovered.\nncn-m001# sleep 180 Verify discovery has completed To verify the two Node BMCs in the blade have been discovered by the HSM, run this command for each BMC in the blade (x1005c3s0b0 and x1005c3s0b1).\nncn-m001# cray hsm inventory redfishEndpoints describe x1005c3s0b0 --format json { \u0026#34;ID\u0026#34;: \u0026#34;x1005c3s0b0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;NodeBMC\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;x1005c3s0b0\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x1005c3s0b0\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MACAddr\u0026#34;: \u0026#34;02:03:E8:00:31:00\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2021-06-10T18:01:59.920850Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;1.2.0\u0026#34; } } When LastDiscoveryStatus displays as DiscoverOK, the node BMC has been successfully discovered. If the last discovery state is DiscoveryStarted then the BMC is currently being inventoried by HSM. If the last discovery state is HTTPsGetFailed or ChildVerificationFailed, then an error has occurred during the discovery process. Troubleshooting:\nIf the redfish endpoint does not exist for a BMC verify the following:\nVerify the Node BMC is pingable:\nncn-m001# ping x1005c3s0b0 If the BMC is not pingable, verify the chassis slot has power.\nncn-m001# cray capmc get_xname_status create --xnames x1005c3s0 If the redfish endpoint is in HTTPSsGetFailed:\nVerify the Node BMC is pingable:\nncn-m001# ping x1005c3s0b0 If the BMC is pingable, verify the node BMC is configured with expected credentials.\nncn-m001# curl -k -u root:password https://x1005c3s0b0/redfish/v1/Managers Enable the nodes in the HSM database.\nFor a blade with four nodes per blade:\nncn-m001# cray hsm state components bulkEnabled update --enabled true --component-ids x1005c3s0b0n0,x1005c3s0b0n1,x1005c3s0b1n0,x1005c3s0b1n1 For a blade with two nodes per blade:\nncn-m001# cray hsm state components bulkEnabled update --enabled true --component-ids x1005c3s0b0n0,x1005c3s0b1n0 Verify that the nodes are enabled in the HSM.\nncn-m001# cray hsm state components query create --component-ids x1005c3s0b0n0,x1005c3s0b0n1,x1005c3s0b1n0,x1005c3s0b1n1 Example output:\n[[Components]] ID = x1005c3s0b0n0 Type = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;Off\u0026#34; . . . [[Components]] ID = x1005c3s0b1n1 Type = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;Off\u0026#34; . . . Power on and boot the nodes Use boot orchestration to power on and boot the nodes. Specify the appropriate BOS template for the node type.\nncn-m001# BOS_TEMPLATE=cos-2.0.30-slurm-healthy-compute ncn-m001# cray bos session create --template-uuid $BOS_TEMPLATE \\ --operation reboot --limit x1005c3s0b0n0,x1005c3s0b0n1,x1005c3s0b1n0,x1005c3s0b1n1 Check firmware Verify that the correct firmware versions for node BIOS, node controller (nC), NIC mezzanine card (NMC), GPUs, and so on. Review chapter 15 \u0026ldquo;Firmware Action Service (FAS)\u0026rdquo; in the HPE Cray EX System Administration Guide 1.4 S-8001 to perform a dry run using FAS to verify firmware versions.\nIf necessary update firmware with FAS. See section 15.1 \u0026ldquo;FAS Workflows\u0026rdquo; in the HPE Cray EX System Administration Guide 1.4 S-8001 for more information.\nCheck DVS There should be a cray-cps pod (the broker), three cray-cps-etcd pods and their waiter, and at least one cray-cps-cm-pm pod. Usually there are two cray-cps-cm-pm pods, one on ncn-w002 and one on ncn-w003 and other worker nodes\nCheck the cray-cps pods on worker nodes and verify they are Running.\nncn-m001# kubectl get pods -Ao wide | grep cps Example output:\nservices cray-cps-75cffc4b94-j9qzf 2/2 Running 0 42h 10.40.0.57 ncn-w001 services cray-cps-cm-pm-g6tjx 5/5 Running 21 41h 10.42.0.77 ncn-w003 services cray-cps-cm-pm-kss5k 5/5 Running 21 41h 10.39.0.80 ncn-w002 services cray-cps-etcd-knt45b8sjf 1/1 Running 0 42h 10.42.0.67 ncn-w003 services cray-cps-etcd-n76pmpbl5h 1/1 Running 0 42h 10.39.0.49 ncn-w002 services cray-cps-etcd-qwdn74rxmp 1/1 Running 0 42h 10.40.0.42 ncn-w001 services cray-cps-wait-for-etcd-jb95m 0/1 Completed SSH to each worker node running CPS/DVS, and run dmesg -T to ensure that there are no recurring \u0026quot;DVS: merge_one\u0026quot; error messages as shown. The error messages indicate that DVS is detecting an IP address change for one of the client nodes.\nncn-m001# dmesg -T | grep \u0026#34;DVS: merge_one\u0026#34; Example output:\n[Tue Jul 21 13:09:54 2020] DVS: merge_one#351: New node map entry does not match the existing entry [Tue Jul 21 13:09:54 2020] DVS: merge_one#353: nid: 8 -\u0026gt; 8 [Tue Jul 21 13:09:54 2020] DVS: merge_one#355: name: \u0026#39;x3000c0s19b1n0\u0026#39; -\u0026gt; \u0026#39;x3000c0s19b1n0\u0026#39; [Tue Jul 21 13:09:54 2020] DVS: merge_one#357: address: \u0026#39;10.252.0.26@tcp99\u0026#39; -\u0026gt; \u0026#39;10.252.0.33@tcp99\u0026#39; [Tue Jul 21 13:09:54 2020] DVS: merge_one#358: Ignoring. Make sure the Configuration Framework Service (CFS) finished successfully. Review HPE Cray EX DVS Administration Guide 1.4.1 S-8004.\nSSH to the node and check each DVS mount.\nnid# mount | grep dvs | head -1 Example output:\n/var/lib/cps-local/0dbb42538e05485de6f433a28c19e200 on /var/opt/cray/gpu/nvidia-squashfs-21.3 type dvs (ro,relatime,blksize=524288,statsfile=/sys/kernel/debug/dvs/mounts/1/stats,attrcache_timeout=14400,cache,nodatasync,noclosesync,retry,failover,userenv,noclusterfs,killprocess,noatomic,nodeferopens,no_distribute_create_ops,no_ro_cache,loadbalance,maxnodes=1,nnodes=6,nomagic,hash_on_nid,hash=modulo,nodefile=/sys/kernel/debug/dvs/mounts/1/nodenames,nodename=x3000c0s6b0n0:x3000c0s5b0n0:x3000c0s4b0n0:x3000c0s9b0n0:x3000c0s8b0n0:x3000c0s7b0n0) Check the HSN for the affected nodes Determine the pod name for the Slingshot fabric manager pod and check the status of the fabric.\nncn-m001# kubectl exec -it -n services \\ $(kubectl get pods --all-namespaces |grep slingshot | awk \u0026#39;{print $2}\u0026#39;) \\ -- fmn_status Check for duplicate IP address entries Check for duplicate IP address entries in the Hardware State Management Database (HSM). Duplicate entries will cause DNS operations to fail.\nVerify each node hostname resolves to one IP address.\nncn-m001# nslookup x1005c3s0b0n0 Example output with one IP address resolving:\nServer: 10.92.100.225 Address: 10.92.100.225#53 Name: x1005c3s0b0n0 Address: 10.100.0.26 Reload the KEA configuration.\nncn-m001# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;config-reload\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api-gw-service-nmn.local/apis/dhcp-kea |jq If there are no duplicate IP addresses within HSM the following response is expected:\n[ { \u0026#34;result\u0026#34;: 0, \u0026#34;text\u0026#34;: \u0026#34;Configuration successful.\u0026#34; } ] If there is a duplicate IP address an error message similar to the message below. This message indicates a duplicate IP address (10.100.0.105) in the HSM:\n[{\u0026#39;result\u0026#39;: 1, \u0026#39;text\u0026#39;: \u0026#34;Config reload failed: configuration error using file \u0026#39;/usr/local/kea/cray-dhcp-kea-dhcp4.conf\u0026#39;: failed to add new host using the HW address \u0026#39;00:40:a6:83:50:a4 and DUID \u0026#39;(null)\u0026#39; to the IPv4 subnet id \u0026#39;0\u0026#39; for the address 10.100.0.105: There\u0026#39;s already a reservation for this address\u0026#34;}] Use the following example curl command to check for active DHCP leases. If there are 0 DHCP leases, there is a configuration error.\nncn-m001# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api-gw-service-nmn.local/apis/dhcp-kea | jq Example output with no active DHCP leases:\n[ { \u0026#34;arguments\u0026#34;: { \u0026#34;leases\u0026#34;: [] }, \u0026#34;result\u0026#34;: 3, \u0026#34;text\u0026#34;: \u0026#34;0 IPv4 lease(s) found.\u0026#34; } ] If there are duplicate entries in the HSM as a result of this procedure, (10.100.0.105 in this example), delete the duplicate entry.\nShow the EthernetInterfaces for the duplicate IP address:\nncn-m001# cray hsm inventory ethernetInterfaces list --ip-address 10.100.0.105 --format json | jq Example output for an IP address that is associated with two MAC addresses:\n[ { \u0026#34;ID\u0026#34;: \u0026#34;0040a68350a4\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:50:a4\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.105\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-08-24T20:24:23.214023Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1005c3s0b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;0040a683639a\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:63:9a\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.105\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-08-27T19:15:53.697459Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1005c3s0b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34; } ] Delete the older entry.\nncn-m001# cray hsm inventory ethernetInterfaces delete 0040a68350a4 Check DNS using nslookup.\nncn-m001# nslookup 10.100.0.105 105.0.100.10.in-addr.arpa name = nid001032-nmn. 105.0.100.10.in-addr.arpa name = nid001032-nmn.local. 105.0.100.10.in-addr.arpa name = x1005c3s0b0n0. 105.0.100.10.in-addr.arpa name = x1005c3s0b0n0.local. Check SSH.\nncn-m001# ssh x1005c3s0b0n0 Example output:\nThe authenticity of host \u0026#39;x1005c3s0b0n0 (10.100.0.105)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:wttHXF5CaJcQGPTIq4zWp0whx3JTwT/tpx1dJNyyXkA. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;x1005c3s0b0n0\u0026#39; (ECDSA) to the list of known hosts. Last login: Tue Aug 31 10:45:49 2021 from 10.252.1.9 "
},
{
	"uri": "/docs-csm/en-09/operations/hmcollector/adjust_hmcollector_resource_limits_requests/",
	"title": "Adjust HM Collector resource limits and requests",
	"tags": [],
	"description": "",
	"content": "Adjust HM Collector resource limits and requests Resource Limit Tuning Guidance Customize cray-hms-hmcollector resource limits and requests in customizations.yaml Redeploy cray-hms-hmcollector with new resource limits and requests Resource Limit Tuning Guidance Inspect current resource usage in the cray-hms-hmcollector pod View resource usage of the containers in the cray-hms-hmcollector pod:\nncn-m001# kubectl -n services top pod -l app.kubernetes.io/name=cray-hms-hmcollector --containers POD NAME CPU(cores) MEMORY(bytes) cray-hms-hmcollector-7c5b797c5c-zxt67 istio-proxy 187m 275Mi cray-hms-hmcollector-7c5b797c5c-zxt67 cray-hms-hmcollector 4398m 296Mi The default resource limits for the cray-hms-hmcollector container are:\nCPU: 4 or 4000m Memory: 5Gi The default resource limits for the istio-proxy container are:\nCPU: 2 or 2000m Memory: 1Gi Inspect the cray-hms-hmcollector pod for OOMKilled events Describe the collector-hms-hmcollector pod to determine if it has been OOMKilled in the recent past:\nncn-m001# kubectl -n services describe pod -l app.kubernetes.io/name=cray-hms-hmcollector Look for the cray-hms-hmcollector container and check its Last State (if present) to see if the container has been perviously terminated due to it running out of memory:\n... Containers: cray-hms-hmcollector: Container ID: containerd://a35853bacdcea350e70c57fe1667b5b9d3c82d41e1e7c1f901832bae97b722fb Image: dtr.dev.cray.com/cray/hms-hmcollector:2.10.6 Image ID: dtr.dev.cray.com/cray/hms-hmcollector@sha256:b043617f83b9ff7e542e56af5bbf47f4ca35876f83b5eb07314054726c895b08 Ports: 80/TCP, 443/TCP Host Ports: 0/TCP, 0/TCP State: Running Started: Tue, 21 Sep 2021 20:52:13 +0000 Last State: Terminated Reason: OOMKilled Exit Code: 137 Started: Tue, 21 Sep 2021 20:51:08 +0000 Finished: Tue, 21 Sep 2021 20:52:12 +0000 ... In the above example output the cray-hms-hmcollector container was perviously OOMKilled, but the container is currently running.\nLook for the isitio-proxy container and check its Last State (if present) to see if the container has been perviously terminated due to it running out of memory:\n... istio-proxy: Container ID: containerd://f439317c16f7db43e87fbcec59b7d36a0254dabd57ab71865d9d7953d154bb1a Image: dtr.dev.cray.com/cray/proxyv2:1.7.8-cray1 Image ID: dtr.dev.cray.com/cray/proxyv2@sha256:8f2bccd346381e0399564142f9534c6c76d8d0b8bd637e9440d53bf96a9d86c7 Port: 15090/TCP Host Port: 0/TCP Args: proxy sidecar --domain $(POD_NAMESPACE).svc.cluster.local --serviceCluster cray-hms-hmcollector.services --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --trust-domain=cluster.local --concurrency 2 State: Running Started: Tue, 21 Sep 2021 20:51:09 +0000 Last State: Terminated Reason: OOMKilled Exit Code: 137 Started: Tue, 21 Sep 2021 20:51:08 +0000 Finished: Tue, 21 Sep 2021 20:52:12 +0000 ... In the above example output the istio-proxy container was perviously OOMKilled, but the container is currently running.\nHow to adjust CPU and Memory limits If the cray-hms-hmcollector container is hitting its CPU limit and memory usage is steadily increasing till it gets OOMKilled, then the CPU limit for the cray-hms-hmcollector should be increased. It can be increased in increments of 8 or 8000m This is a situation were the collector is unable to process events fast enough and they start to collect build up inside of it.\nIf the cray-hms-hmcollector container is consistency hitting its CPU limit, then its CPU limit should be increased. It can be increased in increments of 8 or 8000m.\nIf the cray-hms-hmcollector container is consistency hitting its memory limit, then its memory limit should be increased. It can be increased in increments of 5Gi.\nIf the istio-proxy container is getting OOMKilled, then its memory limit should be increased in increments of 5 Gigabytes (5Gi) at a time.\nOtherwise, if the cray-hms-hmcollector and istio-proxy containers are not hitting their CPU or memory limits\nFor reference, on a system with 4 fully populated liquid cooled cabinets the cray-hms-hmcollector was consuming ~5 or ~5000m of CPU and ~300Mi of memory.\nCustomize cray-hms-hmcollector resource limits and requests in customizations.yaml If the site-init repository is available as a remote repository then clone it on the host orchestrating the upgrade:\nncn-m001# git clone \u0026#34;$SITE_INIT_REPO_URL\u0026#34; site-init Otherwise, create a new site-init working tree:\nncn-m001# git init site-init Download customizations.yaml:\nncn-m001# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; site-init/customizations.yaml Review, add, and commit customizations.yaml to the local site-init repository as appropriate.\nNOTE: If site-init was cloned from a remote repository in step 1, there may not be any differences and hence nothing to commit. This is okay. If there are differences between what is in the repository and what was stored in the site-init, then it suggests settings were improperly changed at some point. If that is the case then be cautious, there may be dragons ahead.\nncn-m001# cd site-init ncn-m001# git diff ncn-m001# git add customizations.yaml ncn-m001# git commit -m \u0026#39;Add customizations.yaml from site-init secret\u0026#39; Update customizations.yaml with the existing cray-hms-hmcollector resource limits and requests settings:\nPersist resource requests and limits from the cray-hms-hmcollector deployment:\nncn-m001# kubectl -n services get deployments cray-hms-hmcollector \\ -o jsonpath=\u0026#39;{.spec.template.spec.containers[].resources}\u0026#39; | yq r -P - | \\ yq w -f - -i ./customizations.yaml spec.kubernetes.services.cray-hms-hmcollector.resources Persist annotations manually added to cray-hms-hmcollector deployment:\nncn-m001# kubectl -n services get deployments cray-hms-hmcollector \\ -o jsonpath=\u0026#39;{.spec.template.metadata.annotations}\u0026#39; | \\ yq d -P - \u0026#39;\u0026#34;traffic.sidecar.istio.io/excludeOutboundPorts\u0026#34;\u0026#39; | \\ yq w -f - -i ./customizations.yaml spec.kubernetes.services.cray-hms-hmcollector.podAnnotations View the updated overrides added to customizations.yaml. If the value overrides look different to the sample output below then the resource limits and requests have been manually modified in the past.\nncn-m001# yq r ./customizations.yaml spec.kubernetes.services.cray-hms-hmcollector hmcollector_external_ip: \u0026#39;{{ network.netstaticips.hmn_api_gw }}\u0026#39; resources: limits: cpu: \u0026#34;4\u0026#34; memory: 5Gi requests: cpu: 500m memory: 256Mi podAnnotations: {} If desired adjust the resource limits and requests for the cray-hms-hmcollector. Otherwise this step can be skipped. Refer to Resource Limit Tuning Guidance for information on how the resource limits could be adjusted.\nEdit customizations.yaml and the value overrides for the cray-hms-hmcollector Helm chart are defined at spec.kubernetes.services.cray-hms-hmcollector\nAdjust the resource limits and requests for the cray-hms-hmcollector deployment in customizations.yaml:\ncray-hms-hmcollector: hmcollector_external_ip: \u0026#39;{{ network.netstaticips.hmn_api_gw }}\u0026#39; resources: limits: cpu: \u0026#34;4\u0026#34; memory: 5Gi requests: cpu: 500m memory: 256Mi To specify a non-default memory limit for the Istio proxy used by the cray-hms-hmcollector to pod annotation sidecar.istio.io/proxyMemoryLimit can added under podAnnotations. By default the Istio proxy memory limit is 1Gi.\ncray-hms-hmcollector: podAnnotations: sidecar.istio.io/proxyMemoryLimit: 5Gi Review the changes to customizations.yaml and verify baseline system customizations and any customer-specific settings are correct.\nncn-m001# git diff Add and commit customizations.yaml if there are any changes:\nncn-m001# git add customizations.yaml ncn-m001# git commit -m \u0026#34;Update customizations.yaml consistent with CSM $CSM_RELEASE_VERSION\u0026#34; Update site-init sealed secret in loftsman namespace:\nncn-m001# kubectl delete secret -n loftsman site-init ncn-m001# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml Push to the remote repository as appropriate:\nncn-m001# git push If this document was referenced during an upgrade procure, then skip Otherwise, continue on to Redeploy cray-hms-hmcollector with new resource limits and requests for the the new resource limits and requests to take effect.\nRedeploy cray-hms-hmcollector with new resource limits and requests Determine the version of HM Collector:\nncn-m001# HMCOLLECTOR_VERSION=$(kubectl -n loftsman get cm loftsman-sysmgmt -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; | yq r - \u0026#39;spec.charts.(name==cray-hms-hmcollector).version\u0026#39;) ncn-m001# echo $HMCOLLECTOR_VERSION Create hmcollector-manifest.yaml:\nncn-m001# cat \u0026gt; hmcollector-manifest.yaml \u0026lt;\u0026lt; EOF apiVersion: manifests/v1beta1 metadata: name: hmcollector spec: charts: - name: cray-hms-hmcollector version: $HMCOLLECTOR_VERSION namespace: services EOF Acquire customizations.yaml:\nncn-m001# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; customizations.yaml Merge customizations.yaml with hmcollector-manifest.yaml:\nncn-m001# manifestgen -c customizations.yaml -i ./hmcollector-manifest.yaml \u0026gt; ./hmcollector-manifest.out.yaml Redeploy the HM Collector helm chart:\nncn-m001# loftsman ship \\ --charts-repo https://packages.local/repository/charts \\ --manifest-path hmcollector-manifest.out.yaml "
},
{
	"uri": "/docs-csm/en-09/operations/network/metallb_bgp/check_bgp_status_and_reset_sessions/",
	"title": "Check BGP Status and Reset Sessions",
	"tags": [],
	"description": "",
	"content": "Check BGP Status and Reset Sessions Check the Border Gateway Protocol (BGP) status on the Aruba and Mellanox switches and verify that all sessions are in an Established state. If the state of any session in the table is Idle, the BGP sessions needs to be reset.\nPrerequisites This procedure requires administrative privileges.\nProcedure The following procedures will require knowing a list of switches that are BGP peers to connect to. You can obtain this list by running the following from an NCN node:\nncn-m001# kubectl get cm config -n metallb-system -o yaml | head -12 Expected output looks similar to the following:\napiVersion: v1 data: config: | peers: - peer-address: 10.252.0.2 peer-asn: 65533 my-asn: 65533 - peer-address: 10.252.0.3 peer-asn: 65533 my-asn: 65533 address-pools: - name: customer-access The switch IPs are the peer-address values.\nMELLANOX Verify that all BGP sessions are in an Established state for the Mellanox spine switches.\nSSH to each BGP peer switch to check the status of all BGP sessions.\nSSH to a BGP peer switch.\nFor example:\nncn-m001# ssh admin@10.252.0.2 View the status of the BGP sessions.\nsw-spine-001 [standalone: master] \u0026gt; enable sw-spine-001 [standalone: master] # show ip bgp summary VRF name : default BGP router identifier : 10.252.0.2 local AS number : 65533 BGP table version : 50 Main routing table version: 50 IPV4 Prefixes : 68 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.1.10 4 65533 3144 3564 50 0 0 1:01:50:41 ESTABLISHED/13 10.252.1.11 4 65533 3144 3569 50 0 0 1:01:50:40 ESTABLISHED/14 10.252.1.12 4 65533 3145 3576 50 0 0 1:01:50:41 ESTABLISHED/14 10.252.1.13 4 65533 3144 3568 50 0 0 1:01:50:41 ESTABLISHED/13 10.252.1.14 4 65533 3145 3572 50 0 0 1:01:50:41 ESTABLISHED/14 If any of the sessions are in an Idle state, proceed to the next step.\nReset BGP to re-establish the sessions.\nSSH to each BGP peer switch.\nFor example:\nncn-m001# ssh admin@10.252.0.2 Verify BGP is enabled.\nsw-spine-001 [standalone: master] \u0026gt; show protocols | include bgp bgp: enabled Clear the BGP sessions.\nsw-spine-001 [standalone: master] \u0026gt; enable sw-spine-001 [standalone: master] # clear ip bgp all Check the status of the BGP sessions to see if they are now Established.\nIt may take a few minutes for sessions to become Established.\nsw-spine-001 [standalone: master] # show ip bgp summary VRF name : default BGP router identifier : 10.252.0.2 local AS number : 65533 BGP table version : 50 Main routing table version: 50 IPV4 Prefixes : 68 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.1.10 4 65533 3144 3564 50 0 0 1:01:50:41 ESTABLISHED/13 10.252.1.11 4 65533 3144 3569 50 0 0 1:01:50:40 ESTABLISHED/14 10.252.1.12 4 65533 3145 3576 50 0 0 1:01:50:41 ESTABLISHED/14 10.252.1.13 4 65533 3144 3568 50 0 0 1:01:50:41 ESTABLISHED/13 10.252.1.14 4 65533 3145 3572 50 0 0 1:01:50:41 ESTABLISHED/14 Once all sessions are in an Established state, BGP reset is complete for the Mellanox switches.\nTroubleshooting: If some sessions remain Idle, re-run the Mellanox reset steps to clear and re-check status. The clear ip bgp all command may need to be be ran multiple times (up to 10 times). In between each clear command wait a few minutes before re-checking the BGP Sessions. If some sessions still remain Idle, proceed to reapply the cray-metallb helm chart, along with the BGP reset, to force the speaker pods to re-establish sessions with the switch.\nAruba Verify that all BGP sessions are in an Established state for the Aruba spine switches.\nSSH to each BGP peer switch to check the status of all BGP sessions.\nSSH to a BGP peer switch.\nncn-m001# ssh admin@10.252.0.2 View the status of the BGP sessions.\nsw-spine-001# show bgp ipv4 unicast summary VRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.2 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.3 65533 19704 19708 00m:01w:00d Established Up 10.252.1.10 65533 34455 39416 00m:01w:04d Established Up 10.252.1.11 65533 34458 39400 00m:01w:04d Established Up 10.252.1.12 65533 34448 39415 00m:01w:04d Established Up If any of the sessions are in an Idle state, proceed to the next step.\nReset BGP to re-establish the sessions.\nSSH to each BGP peer switch.\nFor example:\nncn-m001# ssh admin@10.252.0.2 Clear the BGP sessions.\nsw-spine-001# clear bgp * Check the status of the BGP sessions.\nIt may take a few minutes for sessions to become Established.\nsw-spine-001# show bgp ipv4 unicast summary VRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.2 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.3 65533 19704 19708 00m:01w:00d Established Up 10.252.1.10 65533 34455 39416 00m:01w:04d Established Up 10.252.1.11 65533 34458 39400 00m:01w:04d Established Up 10.252.1.12 65533 34448 39415 00m:01w:04d Established Up Once all sessions are in an Established state, BGP reset is complete for the Aruba switches.\nTroubleshooting: If some sessions remain Idle, re-run the Aruba reset steps to clear and re-check status. The clear bgp * command may need to be be ran multiple times (up to 10 times). In between each clear command wait a few minutes before re-checking the BGP Sessions. If some sessions still remain Idle, proceed to the next step to reapply the cray-metallb helm chart, along with the BGP reset to force the speaker pods to re-establish sessions with the switch.\nTroubleshooting Re-apply the cray-metallb Helm Chart Determine the cray-metallb chart version that is currently deployed.\nncn-m001# helm ls -A -a | grep cray-metallb cray-metallb metallb-system 1 2021-02-10 14:58:43.902752441 -0600 CST deployed cray-metallb-0.12.2 0.8.1 Create a manifest file that will be used to reapply the same chart version.\nncn-m001# cat \u0026lt;\u0026lt; EOF \u0026gt; ./metallb-manifest.yaml apiVersion: manifests/v1beta1 metadata: name: reapply-metallb spec: charts: - name: cray-metallb namespace: metallb-system values: imagesHost: dtr.dev.cray.com version: 0.12.2 EOF Open SSH sessions to all spine switches.\nDetermine the CSM_RELEASE version that is currently running and set an environment variable.\nFor example:\nncn-m001# CSM_RELEASE=0.8.0 Mount the PITDATA so that helm charts are available for the re-install (it might already be mounted) and verify that the chart with the expected version exists.\nncn-m001# mkdir -pv /mnt/pitdata ncn-m001# mount -L PITDATA /mnt/pitdata ncn-m001# ls /mnt/pitdata/csm-${CSM_RELEASE}/helm/cray-metallb* /mnt/pitdata/csm-0.8.0/helm/cray-metallb-0.12.2.tgz Uninstall the current cray-metallb chart.\nUntil the chart is reapplied, this will also effect unbound name resolution, and all BGP sessions will be Idle for all of the worker nodes.\nncn-m001# helm del cray-metallb -n metallb-system Use the open SSH sessions to the switches to clear the BGP sessions based on the above Mellanox or Aruba procedures.\nRefer to substeps 1-3 for Mellanox.\nRefer to substeps 1-2 for Aruba.\nReapply the cray-metallb chart based on the CSM_RELEASE.\nncn-m001# loftsman ship --manifest-path ./metallb-manifest.yaml \\ --charts-path /mnt/pitdata/csm-${CSM_RELEASE}/helm Check that the speaker pods are all running.\nThis may take a few minutes.\nncn-m001# kubectl get pods -n metallb-system NAME READY STATUS RESTARTS AGE cray-metallb-controller-6d545b5ccc-mm4qz 1/1 Running 0 79m cray-metallb-speaker-4nrzq 1/1 Running 0 76m cray-metallb-speaker-b5m2n 1/1 Running 0 79m cray-metallb-speaker-h7s7b 1/1 Running 0 79m Use the open SSH sessions to the switches to check the status of the BGP sessions.\nRefer to substeps 1-3 for Mellanox.\nRefer to substeps 1-2 for Aruba.\n"
},
{
	"uri": "/docs-csm/en-09/operations/kubernetes/cert_renewal_for_kubernetes_and_bare_metal_etcd/",
	"title": "Kubernetes and Bare Metal EtcD Certificate Renewal",
	"tags": [],
	"description": "",
	"content": "Kubernetes and Bare Metal EtcD Certificate Renewal Scope As part of the installation, Kubernetes generates certificates for the required subcomponents. This document will help walk through the process of renewing the certificates.\nIMPORTANT: Depending on the version of Kubernetes, the command may or may not reside under the alpha category. Use kubectl certs --help and kubectl alpha certs --help to determine this. The overall command syntax should be the same and this is just whether or not the command structure will require alpha in it.\nIMPORTANT: When you pick your master node to renew the certificatess on, that is the node that will be referenced in this document as ncn-m.\nIMPORTANT: This document is based off a base hardware configuration of 3 masters and 3 workers (We are leaving off utility storage since they are not running Kubernetes). Please make sure to update any commands that run on multiple nodes accordingly.\nProcedures for Certificate Renewal:\nFile Locations Check Certificates Backup Existing Certificates Renew All Certificates Renew Etcd Certificate Update Client Secrets File Locations IMPORTANT: Master nodes will have certificates for both Kubernetes services and the Kubernetes client. Workers will only have the certificates for the Kubernetes client.\nServices (master nodes):\n/etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver-etcd-client.crt /etc/kubernetes/pki/apiserver-etcd-client.key /etc/kubernetes/pki/apiserver.key /etc/kubernetes/pki/apiserver-kubelet-client.crt /etc/kubernetes/pki/apiserver-kubelet-client.key /etc/kubernetes/pki/ca.crt /etc/kubernetes/pki/ca.key /etc/kubernetes/pki/front-proxy-ca.crt /etc/kubernetes/pki/front-proxy-ca.key /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/pki/front-proxy-client.key /etc/kubernetes/pki/sa.key /etc/kubernetes/pki/sa.pub /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/etcd/ca.key /etc/kubernetes/pki/etcd/healthcheck-client.crt /etc/kubernetes/pki/etcd/healthcheck-client.key /etc/kubernetes/pki/etcd/peer.crt /etc/kubernetes/pki/etcd/peer.key /etc/kubernetes/pki/etcd/server.crt /etc/kubernetes/pki/etcd/server.key Client (master and worker nodes):\n/var/lib/kubelet/pki/kubelet-client-2021-09-07-17-06-36.pem /var/lib/kubelet/pki/kubelet-client-current.pem /var/lib/kubelet/pki/kubelet.crt /var/lib/kubelet/pki/kubelet.key Check Certificates Check the expiration of the certificates.\nLog into a master node and run the following:\nncn-m# kubeadm alpha certs check-expiration --config /etc/kubernetes/kubeadmcfg.yaml WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Sep 24, 2021 15:21 UTC 14d no apiserver Sep 24, 2021 15:21 UTC 14d ca no apiserver-etcd-client Sep 24, 2021 15:20 UTC 14d ca no apiserver-kubelet-client Sep 24, 2021 15:21 UTC 14d ca no controller-manager.conf Sep 24, 2021 15:21 UTC 14d no etcd-healthcheck-client Sep 24, 2021 15:19 UTC 14d etcd-ca no etcd-peer Sep 24, 2021 15:19 UTC 14d etcd-ca no etcd-server Sep 24, 2021 15:19 UTC 14d etcd-ca no front-proxy-client Sep 24, 2021 15:21 UTC 14d front-proxy-ca no scheduler.conf Sep 24, 2021 15:21 UTC 14d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Sep 02, 2030 15:21 UTC 8y no etcd-ca Sep 02, 2030 15:19 UTC 8y no front-proxy-ca Sep 02, 2030 15:21 UTC 8y no Backup Existing Certificates Backup existing certificates.\nMaster Nodes:\nncn-m# pdsh -w ncn-m00[1-3] tar cvf /root/cert_backup.tar /etc/kubernetes/pki/ /var/lib/kubelet/pki/ ncn-m001: tar: Removing leading / from member names ncn-m001: /etc/kubernetes/pki/ ncn-m001: /etc/kubernetes/pki/front-proxy-client.key ncn-m001: tar: Removing leading / from hard link targets ncn-m001: /etc/kubernetes/pki/apiserver-etcd-client.key ncn-m001: /etc/kubernetes/pki/sa.key . . .. shortened output Worker Nodes:\nIMPORTANT: The range of nodes below should reflect the size of the environment. This should run on every worker node.\nncn-m# pdsh -w ncn-w00[1-3] tar cvf /root/cert_backup.tar /var/lib/kubelet/pki/ ncn-w003: tar: Removing leading / from member names ncn-w003: /var/lib/kubelet/pki/ ncn-w003: /var/lib/kubelet/pki/kubelet.key ncn-w003: /var/lib/kubelet/pki/kubelet-client-2021-09-07-17-06-36.pem ncn-w003: /var/lib/kubelet/pki/kubelet.crt . . .. shortened output Renew All Certificates On each master node Renew the Certificates.\nncn-m# kubeadm alpha certs renew all --config /etc/kubernetes/kubeadmcfg.yaml WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed certificate for serving the Kubernetes API renewed certificate the apiserver uses to access etcd renewed certificate for the API server to connect to kubelet renewed certificate embedded in the kubeconfig file for the controller manager to use renewed certificate for liveness probes to healthcheck etcd renewed certificate for etcd nodes to communicate with each other renewed certificate for serving etcd renewed certificate for the front proxy client renewed certificate embedded in the kubeconfig file for the scheduler manager to use renewed Check the new expiration.\nncn-m# kubeadm alpha certs check-expiration --config /etc/kubernetes/kubeadmcfg.yaml WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Sep 22, 2022 17:13 UTC 364d no apiserver Sep 22, 2022 17:13 UTC 364d ca no apiserver-etcd-client Sep 22, 2022 17:13 UTC 364d etcd-ca no apiserver-kubelet-client Sep 22, 2022 17:13 UTC 364d ca no controller-manager.conf Sep 22, 2022 17:13 UTC 364d no etcd-healthcheck-client Sep 22, 2022 17:13 UTC 364d etcd-ca no etcd-peer Sep 22, 2022 17:13 UTC 364d etcd-ca no etcd-server Sep 22, 2022 17:13 UTC 364d etcd-ca no front-proxy-client Sep 22, 2022 17:13 UTC 364d front-proxy-ca no scheduler.conf Sep 22, 2022 17:13 UTC 364d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Sep 02, 2030 15:21 UTC 8y no etcd-ca Sep 02, 2030 15:19 UTC 8y no front-proxy-ca Sep 02, 2030 15:21 UTC 8y no This command may have only updated some certificates.\nncn-m# ncn-m001:~ # ls -l /etc/kubernetes/pki -rw-r--r-- 1 root root 1249 Sep 22 17:13 apiserver.crt -rw-r--r-- 1 root root 1090 Sep 22 17:13 apiserver-etcd-client.crt -rw------- 1 root root 1675 Sep 22 17:13 apiserver-etcd-client.key -rw------- 1 root root 1679 Sep 22 17:13 apiserver.key -rw-r--r-- 1 root root 1099 Sep 22 17:13 apiserver-kubelet-client.crt -rw------- 1 root root 1679 Sep 22 17:13 apiserver-kubelet-client.key -rw------- 1 root root 1025 Sep 21 20:50 ca.crt -rw------- 1 root root 1679 Sep 21 20:50 ca.key drwxr-xr-x 2 root root 162 Sep 21 20:50 etcd -rw------- 1 root root 1038 Sep 21 20:50 front-proxy-ca.crt -rw------- 1 root root 1679 Sep 21 20:50 front-proxy-ca.key -rw-r--r-- 1 root root 1058 Sep 22 17:13 front-proxy-client.crt -rw------- 1 root root 1675 Sep 22 17:13 front-proxy-client.key -rw------- 1 root root 1675 Sep 21 20:50 sa.key -rw------- 1 root root 451 Sep 21 20:50 sa.pub ncn-m# ls -l /etc/kubernetes/pki/etcd -rw-r--r-- 1 root root 1017 Sep 21 20:50 ca.crt -rw-r--r-- 1 root root 1675 Sep 21 20:50 ca.key -rw-r--r-- 1 root root 1094 Sep 22 17:13 healthcheck-client.crt -rw------- 1 root root 1679 Sep 22 17:13 healthcheck-client.key -rw-r--r-- 1 root root 1139 Sep 22 17:13 peer.crt -rw------- 1 root root 1679 Sep 22 17:13 peer.key -rw-r--r-- 1 root root 1139 Sep 22 17:13 server.crt -rw------- 1 root root 1675 Sep 22 17:13 server.key As we can see not all the certificate files were updated.\nIMPORTANT: Some certificates were not updated because they have a distant expiration time and did not need to be updated. This is expected.\nCertificates most likely to not be updated due to a distant expiration:\nCERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Sep 02, 2030 15:21 UTC 8y no etcd-ca Sep 02, 2030 15:19 UTC 8y no front-proxy-ca Sep 02, 2030 15:21 UTC 8y no This means we can ignore the fact that our ca.crt/key, front-proxy-ca.crt/key, and etcd ca.crt/key were not updated.\nCheck the expiration of the certificates files that do not have a current date and are of the .crt or .pem format. See File Locations for the list of files.\nThis task is for each master node and below example checks each certificate in File Locations.\nfor i in $(ls /etc/kubernetes/pki/*.crt;ls /etc/kubernetes/pki/etcd/*.crt;ls /var/lib/kubelet/pki/*.crt;ls /var/lib/kubelet/pki/*.pem);do echo ${i}; openssl x509 -enddate -noout -in ${i};done /etc/kubernetes/pki/apiserver.crt notAfter=Sep 22 17:13:28 2022 GMT /etc/kubernetes/pki/apiserver-etcd-client.crt notAfter=Sep 22 17:13:28 2022 GMT /etc/kubernetes/pki/apiserver-kubelet-client.crt notAfter=Sep 22 17:13:28 2022 GMT /etc/kubernetes/pki/ca.crt notAfter=Sep 4 09:31:10 2031 GMT /etc/kubernetes/pki/front-proxy-ca.crt notAfter=Sep 4 09:31:11 2031 GMT /etc/kubernetes/pki/front-proxy-client.crt notAfter=Sep 22 17:13:29 2022 GMT /etc/kubernetes/pki/etcd/ca.crt notAfter=Sep 4 09:30:28 2031 GMT /etc/kubernetes/pki/etcd/healthcheck-client.crt notAfter=Sep 22 17:13:29 2022 GMT /etc/kubernetes/pki/etcd/peer.crt notAfter=Sep 22 17:13:29 2022 GMT /etc/kubernetes/pki/etcd/server.crt notAfter=Sep 22 17:13:29 2022 GMT /var/lib/kubelet/pki/kubelet.crt notAfter=Sep 21 19:50:16 2022 GMT /var/lib/kubelet/pki/kubelet-client-2021-09-07-17-06-36.pem notAfter=Sep 4 17:01:38 2022 GMT /var/lib/kubelet/pki/kubelet-client-current.pem notAfter=Sep 4 17:01:38 2022 GMT IMPORTANT: DO NOT forget to verify certificates in /etc/kubernetes/pki/etcd.\nAs noted in our above output all certificates including those for etcd were updated. Please note apiserver-etcd-client.crt is critical as it is the cert that allows the Kubernetes API server to talk to the bare-metal etcd cluster. Also the /var/lib/kubelet/pki/ certificates will be updated in the Kubernetes client section that follows. Restart etcd.\nOnce the steps to renew the needed certs have been completed on all the master nodes, then log into each master node one at a time and do:\nncn-m# systemctl restart etcd.service On master and worker nodes Restart kubelet.\nOn each Kubernetes node do:\nIMPORTANT: The below example will need to be adjusted to reflect the correct amount of master and worker nodes in your environment.\nncn-m# pdsh -w ncn-m00[1-3] -w ncn-w00[1-3] systemctl restart kubelet.service Fix kubectl command access.\nNOTE: Only if your certificates have expired will the following command respond with Unauthorized. In any case, the new client certificates will need to be distributed in the following steps.\nncn-m# kubectl get nodes error: You must be logged in to the server (Unauthorized) ncn-m# cp /etc/kubernetes/admin.conf /root/.kube/config ncn-m# # kubectl get nodes NAME STATUS ROLES AGE VERSION ncn-m001 Ready master 370d v1.18.6 ncn-m002 Ready master 370d v1.18.6 ncn-m003 Ready master 370d v1.18.6 ncn-w001 Ready \u0026lt;none\u0026gt; 370d v1.18.6 ncn-w002 Ready \u0026lt;none\u0026gt; 370d v1.18.6 ncn-w003 Ready \u0026lt;none\u0026gt; 370d v1.18.6 Distribute the client certificate to the rest of the cluster.\nNOTE: You may have errors copying files. The target may or may not exist depending on the version of Shasta.\nYou DO NOT need to copy this to the master node where you are performing this work. Shasta v1.3 and earlier copy /root/.kube/config to all master nodes and ncn-w001. Shasta v1.4 and later copy /etc/kubernetes/admin.conf to all master and worker nodes. If you attempt to copy to workers nodes other than ncn-w001 in a Shasta v1.3 or earlier system you will see this error pdcp@ncn-m001: ncn-w003: fatal: /root/.kube/: Is a directory and this is expected and can be ignored.\nClient access:\nNOTE: Please update the below command with the appropriate amount of worker nodes.\nFor Shasta v1.4 and later :\nncn-m# pdcp -w ncn-m00[2-3] -w ncn-w00[1-3] /etc/kubernetes/admin.conf /etc/kubernetes/ For Shasta v1.3 and earlier :\nncn-m# pdcp -w ncn-m00[2-3] -w ncn-w001 /root/.kube/config /root/.kube/ Regenerating kubelet .pem certificates Backup certificates for kubelet on each master and worker node:\nIMPORTANT: The below example will need to be adjusted to reflect the correct amount of master and worker nodes in your environment.\nncn-m# pdsh -w ncn-m00[1-3] -w ncn-w00[1-3] tar cvf /root/kubelet_certs.tar /etc/kubernetes/kubelet.conf /var/lib/kubelet/pki/ On the master node where you updated the other certificates do:\nGet your current apiserver-advertise-address.\nncn# kubectl config view|grep server server: https://10.252.120.2:6442 Using the IP address from the above output do:\nThe apiserver-advertise-address may vary, so make sure you are not copy and pasting without verifying. ncn-m# for node in $(kubectl get nodes -o json|jq -r \u0026#39;.items[].metadata.name\u0026#39;); do kubeadm alpha kubeconfig user --org system:nodes --client-name system:node:$node --apiserver-advertise-address 10.252.120.2 --apiserver-bind-port 6442 \u0026gt; /root/$node.kubelet.conf; done This will generate a new kubelet.conf file in the /root/ directory. There should be a new file per node running Kubernetes.\nCopy each file to the corresponding node shown in the filename.\nNOTE: Please update the below command with the appropriate amount of master and worker nodes.\nncn-m# for node in ncn-m00{1..3} ncn-w00{1..3}; do scp /root/$node.kubelet.conf $node:/etc/kubernetes/; done Log into each node one at a time and do the following.\nsystemctl stop kubelet.service rm /etc/kubernetes/kubelet.conf rm /var/lib/kubelet/pki/* cp /etc/kubernetes/\u0026lt;node\u0026gt;.kubelet.conf /etc/kubernetes/kubelet.conf systemctl start kubelet.service kubeadm init phase kubelet-finalize all \u0026ndash;cert-dir /var/lib/kubelet/pki/ Check the expiration of the kubectl certificates files. See File Locations for the list of files.\nThis task is for each master and worker node. The example checks each kubelet certificate in File Locations.\nfor i in $(ls /var/lib/kubelet/pki/*.crt;ls /var/lib/kubelet/pki/*.pem);do echo ${i}; openssl x509 -enddate -noout -in ${i};done /var/lib/kubelet/pki/kubelet.crt notAfter=Sep 22 17:37:30 2022 GMT /var/lib/kubelet/pki/kubelet-client-2021-09-22-18-37-30.pem notAfter=Sep 22 18:32:30 2022 GMT /var/lib/kubelet/pki/kubelet-client-current.pem notAfter=Sep 22 18:32:30 2022 GMT Perform a rolling reboot of master nodes.\nFor Shasta v1.4 and later :\nFollow the Reboot_NCNs process. For Shasta v1.3 and earlier :\nFollow the Reboot_NCNs process.\nNOTES:\nncn-w001 is the externally connected node. On Shasta v1.4 and later, ncn-m001 is the externally connected node. The ncnGetXnames.sh script is not available; The xname can be found in the file /etc/cray/xname on the specific node. IMPORTANT: Please ensure you are verifying pods are running on the master node that was rebooted before proceeding to the next node.\nPerform a rolling reboot of worker nodes.\nFor Shasta v1.4 and later :\nFollow the Reboot_NCNs process. For Shasta v1.3 and earlier :\nBefore rebooting any worker node, scale nexus replicas to 0.\nncn-m# kubectl scale deployment nexus -n nexus --replicas=0 Follow the Reboot_NCNs process.\nNOTES:\nncn-w001 is the externally connected node. On Shasta v1.4 and later, ncn-m001 is the externally connected node.\nThe failover-leader.sh, ncnGetXnames.sh and add_pod_priority.sh scripts are not available or required when rebooting worker nodes.\nAfter draining a worker, force delete any pod that fails to terminate due to Cannot evict pod as it would violate the pod's disruption budget.\nncn-m# kubectl delete pod \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; --force Reference the Shasta v1.3 Admin Guide for any steps related to checking system health.\nAfter rebooting all the worker nodes, scale nexus replicas back to 1 and verify nexus is running.\nncn-m# kubectl scale deployment nexus -n nexus --replicas=1 ncn-m# kubectl get pods -n nexus | grep nexus nexus-868d7b8466-gjnps 2/2 Running 0 5m For Shasta v1.3 and earlier, restart the sonar cronjobs and verify vault etcd is healthy.\nRestart the sonar cronjobs.\nncn-m# kubectl -n services get cronjob sonar-jobs-watcher -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; | kubectl replace --force -f - ncn-m# kubectl -n services get cronjob sonar-sync -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; | kubectl replace --force -f - After at least a minute, verify that the cronjobs have been scheduled.\nncn-m# # kubectl get cronjobs -n services sonar-jobs-watcher NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE sonar-jobs-watcher */1 * * * * False 1 23s 5m10s ncn-m# kubectl get cronjobs -n services sonar-sync NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE sonar-sync */1 * * * * False 1 32s 5m15s Check the health of vault etcd.\nncn-m# for pod in $(kubectl get pods -l app=etcd -n vault -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do echo \u0026#34;### ${pod} ###\u0026#34;; kubectl -n vault exec $pod -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl --cacert /etc/etcdtls/operator/etcd-tls/etcd-client-ca.crt --cert /etc/etcdtls/operator/etcd-tls/etcd-client.crt --key /etc/etcdtls/operator/etcd-tls/etcd-client.key --endpoints https://localhost:2379 endpoint health\u0026#34;; done If the above health of vault etcd reports any pods as unhealthy, backup the secret, delete the secret. The operator will create a new secret.\nncn-m# kubectl get secret -n vault cray-vault-etcd-tls -o yaml \u0026gt; /root/vault_sec.yaml ncn-m# kubectl delete secret -n vault cray-vault-etcd-tls Once the new secret has been created and the cray-vault-etcd pods are running, verify the health of vault etcd.\nncn-m# kubectl get secret -n vault cray-vault-etcd-tls NAME TYPE DATA AGE cray-vault-etcd-tls Opaque 9 5m ncn-m# kubectl get pods -l app=etcd -n vault NAME READY STATUS RESTARTS AGE cray-vault-etcd-stzjf6dqd5 1/1 Running 0 10m cray-vault-etcd-ws59fgssxt 1/1 Running 0 10m cray-vault-etcd-xmvfxz48vs 1/1 Running 0 10m ncn-m# for pod in $(kubectl get pods -l app=etcd -n vault -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do echo \u0026#34;### ${pod} ###\u0026#34;; kubectl -n vault exec $pod -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl --cacert /etc/etcdtls/operator/etcd-tls/etcd-client-ca.crt --cert /etc/etcdtls/operator/etcd-tls/etcd-client.crt --key /etc/etcdtls/operator/etcd-tls/etcd-client.key --endpoints https://localhost:2379 endpoint health\u0026#34;; done ### cray-vault-etcd-stzjf6dqd5 ### https://localhost:2379 is healthy: successfully committed proposal: took = 19.999618ms ### cray-vault-etcd-ws59fgssxt ### https://localhost:2379 is healthy: successfully committed proposal: took = 19.597736ms ### cray-vault-etcd-xmvfxz48vs ### https://localhost:2379 is healthy: successfully committed proposal: took = 19.81056ms NOTE:\nVault etcd errors such as tls: bad certificate.\u0026quot; Reconnecting can be ignored.\nncn-m# kubectl logs -l app=etcd -n vault | grep \u0026#34;bad certificate\\\u0026#34;. Reconnecting\u0026#34; WARNING: 2021/09/24 17:35:11 grpc: addrConn.createTransport failed to connect to {0.0.0.0:2379 0 \u0026lt;nil\u0026gt;}. Err :connection error: desc = \u0026#34;transport: authentication handshake failed: remote error: tls: bad certificate\u0026#34;. Reconnecting... Renew Etcd Certificate If Check Certificates indicates that only the apiserver-etcd-client need to be renewed, then the following can be used to renew just that one certificate. The full Renew All Certificates procedure will also renew this certificate.\nRun the following steps on each master node.\nRenew the Etcd certificate.\nkubeadm alpha certs renew apiserver-etcd-client --config /etc/kubernetes/kubeadmcfg.yaml systemctl restart etcd.service systemctl restart kubelet.service Update Client Secrets The client secrets can be updated independently from the Kubernetes certs.\nRun the following steps from a master node.\nUpdate the client certificate for kube-etcdbackup.\nUpdate the kube-etcdbackup-etcd secret.\nkubectl --namespace=kube-system create secret generic kube-etcdbackup-etcd \\ --from-file=/etc/kubernetes/pki/etcd/ca.crt \\ --from-file=tls.crt=/etc/kubernetes/pki/etcd/server.crt \\ --from-file=tls.key=/etc/kubernetes/pki/etcd/server.key \\ --save-config --dry-run=client -o yaml | kubectl apply -f - Check the certificate\u0026rsquo;s expiration date to verify that the certificate is not expired.\nkubectl get secret -n kube-system kube-etcdbackup-etcd -o json | jq -r \u0026#39;.data.\u0026#34;tls.crt\u0026#34; | @base64d\u0026#39; | openssl x509 -noout -enddate Example output:\nnotAfter=May 4 22:37:16 2023 GMT Check that the next kube-etcdbackup cronjob Completed. This cronjob runs every 10 minutes.\nkubectl get pod -l app.kubernetes.io/instance=cray-baremetal-etcd-backup -n kube-system Example output:\nNAME READY STATUS RESTARTS AGE kube-etcdbackup-1652201400-czh5p 0/1 Completed 0 107s Update the client certificate for etcd-client.\nUpdate the etcd-client-cert secret.\nkubectl --namespace=sysmgmt-health create secret generic etcd-client-cert \\ --from-file=etcd-client=/etc/kubernetes/pki/apiserver-etcd-client.crt \\ --from-file=etcd-client-key=/etc/kubernetes/pki/apiserver-etcd-client.key \\ --from-file=etcd-ca=/etc/kubernetes/pki/etcd/ca.crt \\ --save-config --dry-run=client -o yaml | kubectl apply -f - Check the certificates\u0026rsquo; expiration dates to verify that none of the certificate are expired.\nCheck the etcd-ca expiration date.\nkubectl get secret -n sysmgmt-health etcd-client-cert -o json | jq -r \u0026#39;.data.\u0026#34;etcd-ca\u0026#34; | @base64d\u0026#39; | openssl x509 -noout -enddate Example output:\nnotAfter=May 1 18:20:23 2032 GMT Check the etcd-client expiration date.\nkubectl get secret -n sysmgmt-health etcd-client-cert -o json | jq -r \u0026#39;.data.\u0026#34;etcd-client\u0026#34; | @base64d\u0026#39; | openssl x509 -noout -enddate Example output:\nnotAfter=May 4 18:20:24 2023 GMT Restart Prometheus.\nkubectl rollout restart -n sysmgmt-health statefulSet/prometheus-cray-sysmgmt-health-promet-prometheus kubectl rollout status -n sysmgmt-health statefulSet/prometheus-cray-sysmgmt-health-promet-prometheus Example output:\nWaiting for 1 pods to be ready... statefulset rolling update complete ... Check for any tls errors from the active Prometheus targets. No errors are expected.\nPROM_IP=$(kubectl get services -n sysmgmt-health cray-sysmgmt-health-promet-prometheus -o json | jq -r \u0026#39;.spec.clusterIP\u0026#39;) curl -s http://${PROM_IP}:9090/api/v1/targets | jq -r \u0026#39;.data.activeTargets[] | select(.\u0026#34;scrapePool\u0026#34; == \u0026#34;sysmgmt-health/cray-sysmgmt-health-promet-kube-etcd/0\u0026#34;)\u0026#39; | grep lastError | sort -u Example output:\n\u0026#34;lastError\u0026#34;: \u0026#34;\u0026#34;, "
},
{
	"uri": "/docs-csm/en-09/operations/configuration_management/configuration_layers/",
	"title": "Configuration Layers",
	"tags": [],
	"description": "",
	"content": "Configuration Layers The Configuration Framework Service (CFS) uses configuration layers to specify the location of configuration content that will be applied. Configurations may include one or more layers. Each layer is defined by a Git repository clone URL, a Git commit, a name (optional), and the path in the repository to an Ansible playbook to execute.\nConfigurations with a single layer are useful when testing out a new configuration on targets, or when configuring system components with one product at a time. To fully configure a node or boot image component with all of the software products required, multiple layers can be used to apply all configurations in a single CFS session. When applying layers in a session, CFS runs through the configuration layers serially in the order specified.\nUse Branches in Configuration Layers When defining a configuration layer, the branch or commit values can be used to reference a Git commit. The commit value is the recommended way to reference a Git commit. In the following example, when the configuration is created or updated, CFS will automatically check with VCS to get the commit at the head of the branch. Both the commit and the branch are then stored. The commit acts as normal, and the branch is stored to make future updates to the commit easier.\nncn-m001# cat configurations-example.json { \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34; } ] } ncn-m001# cray cfs configurations update configurations-example \\ --file ./configurations-example.json \\ --format json { \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:30:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; } If changes are made to a repository and branches are specified in the configuration, users can then use the --update-branches flag to update a configuration so that all commits reflect the latest commit on the branches specified.\nncn-m001# cray cfs configurations update configurations-example --update-branches { \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:30:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;latest git commit id\u0026gt;\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; } Manage Configurations Use the cray cfs configurations --help command to manage CFS configurations on the system. The following operations are available:\nlist: List all configurations. describe: Display info about a single configuration and its layer(s). update: Create a new configuration or modify an existing configuration. delete: Delete an existing configuration. "
},
{
	"uri": "/docs-csm/en-09/operations/csm_product_management/apply_security_hardening/",
	"title": "Security Hardening",
	"tags": [],
	"description": "",
	"content": "Security Hardening This is an overarching guide to further harden the security posture of a Cray System Management (CSM) system.\nIf a subset of the steps in this procedure were completed as a consequence of an install, upgrade, or other guidance, then it is safe to skip that subset following a review.\nPrerequisites None.\nProcedure Change passwords and credentials.\nPerform procedure(s) in Change Passwords and Credentials.\nRestrict access to ncn-images S3 Bucket.\nPerform procedure(s) in Restrict Access to ncn-images S3 Bucket.\n"
},
{
	"uri": "/docs-csm/en-09/operations/boot_orchestration/stage_changes_without_bos/",
	"title": "Stage Changes Without BOS",
	"tags": [],
	"description": "",
	"content": "Stage Changes Without BOS Sometimes there is a need to stages changes to take place on a reboot, without immediately rebooting a node. When this is called for, users can bypass BOS, and set boot artifacts or configuration that will only take place when a node is later booted, whether that occurs manually, or triggered by a task manager.\nStage Boot Artifacts For information on staging boot artifacts, see the section Upload Node Boot Information to Boot Script Service (BSS).\nStage a Configuration Disable CFS for all nodes receiving the staged configuration. Nodes will automatically re-enable configuration when they are rebooted and will be configured with any staged changes.\nncn-m001# cray cfs components update \u0026lt;xname\u0026gt; --enabled false Either set the new desired configuration, or update the existing configuration.\nIf an entirely new configuration is being used, or if no configuration was previously set for a component, update the configuration name with the following:\nncn-m001# cray cfs components update \u0026lt;xname\u0026gt; --configuration-name \u0026lt;configuration_name\u0026gt; If all nodes that share a configuration are being staged with an update, updating the shared configuration will stage the change for all relevant nodes. Be aware that if this step is taken and not all nodes that use the configuration are disabled in CFS, the configuration will automatically and immediately apply to all enabled nodes that are using it.\nncn-m001# cray cfs configurations update \u0026lt;configuration_name\u0026gt; --file \u0026lt;file_path\u0026gt; Users also have the option of specifying branches rather than commits in configurations. If this feature is used, the configuration can also be updated by telling CFS to update the commits for all layers of a configuration that specify branches. Like with updating the configuration from a file, this will automatically start configuration on any enabled nodes that are using this configuration. For information on using branches, see the section (Use Branches in Configuration Layers)\n"
},
{
	"uri": "/docs-csm/en-09/operations/fas_loader/fas_loader_fails/",
	"title": "TROUBLESHOOTING FAS LOADER FAILS",
	"tags": [],
	"description": "",
	"content": "TROUBLESHOOTING FAS LOADER FAILS NOTE: this procedure is only for csm-0.9.x releases.\nThe FAS loader may fail due to issues with the repomd.xml file in Nexus.\nThis will be indicated by the following message: CRITICAL: Failed to get repomd.xml from repo in the fas-loader job.\nIndications that the fas-loader job has failed include:\nExpected firmware is not present in the FAS image list (cray fas images list) after the FAS loader has been run. FAS reports failed to find file, trying again soon while running an update action. To view the FAS Loader logs: (The system only keeps completed job pods available for a short amount time, if the command returns no pods, rerun the FAS loader with the procedure below.)\nGet the fas-loader pod name:\nncn# kubectl get pods -n services | awk \u0026#39;NR == 1 || /fas-loader/\u0026#39; NAME READY STATUS RESTARTS AGE cray-fas-loader-1-pnn6c 2/2 Running 2 9m38s Check the logs using the pod name returned:\nncn# kubectl logs -n services cray-fas-loader-1-pnn6c -c cray-fas-loader Check for CRITICAL: Failed to get repomd.xml from repo message in the logs.\nTo rerun the FAS Loader:\nRetrieve the job name. In the following example, the returned job name is cray-fas-loader-1, which is the job to rerun in this scenario.\nncn# kubectl -n services get jobs | grep fas-loader cray-fas-loader-1 1/1 3m11s 3d7h Rerun the cray-fas-loader job. Note, after \u0026ldquo;-f\u0026rdquo; there is a \u0026ldquo;-\u0026rdquo;. Change cray-fas-loader-1 to the loader job name returned from the last command.\nncn# kubectl -n services get job cray-fas-loader-1 -o json | jq \u0026#39;del(.spec.selector)\u0026#39; \\ | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; \\ | kubectl replace --force -f - job.batch \u0026#34;cray-fas-loader-1\u0026#34; deleted job.batch/cray-fas-loader-1 replaced Make sure the FAS Loader job is complete. Depending on the number of images in FAS, this could take 5-7 minutes.\nncn# kubectl -n services get jobs | awk \u0026#39;NR == 1 || /fas-loader/\u0026#39; NAME COMPLETIONS DURATION AGE cray-fas-loader-1 1/1 7m35s 7m35s Check the logs:\nGet the fas-loader pod name:\nncn# kubectl get pods -n services | awk \u0026#39;NR == 1 || /fas-loader/\u0026#39; NAME READY STATUS RESTARTS AGE cray-fas-loader-1-pnn6c 2/2 Running 2 9m38s Check the logs using the pod name returned:\nncn# kubectl logs -n services cray-fas-loader-1-pnn6c -c cray-fas-loader Check for CRITICAL: Failed to get repomd.xml from repo message in the logs.\nSolution To correct the repomd.xml file issue, you will need to delete the shasta-firmware-0.9.3 repo from Nexus and rerun the install script.\nncn# curl -sfkSL -X DELETE https://packages.local/service/rest/beta/repositories/shasta-firmware-0.9.3 ncn# ./install.sh After the install script is completed, rerun the FAS Loader job using the commands above, checking the logs any errors.\n"
},
{
	"uri": "/docs-csm/en-09/000-info/",
	"title": "CRAY Guide Contribution",
	"tags": [],
	"description": "",
	"content": "CRAY Guide Contribution The documentation included here describes how to install the CSM software and various supporting administrative procedures. See Guides for the different scenarios to which this documentation can apply for installation.\nThe rest of this page describes the conventions used in the documentation:\n[Page naming or indexing] (#page-indexing) conventions [Annotations] (#annotations) for how we identify sections of the documentation that do not apply to all systems [Command Prompt Conventions] (#command-prompt-conventions) which describe the context for user, host, directory, chroot environment, or container environment Page Indexing / Naming The page name can be anything. This repo has a loose pattern to assist tab-completion and contextual heuristics:\n[XYZ]-[context]-[memo].md Examples:\n006-CSM-PLATFORM-INSTALL.md 250-FIRMWARE-NODE.md 407-MGMT-NET-SNMP-CONFIG.md Annotations This repository may change annotations, for now under the MarkDown governance these are the available annotations.\nYou must use these to denote the right steps to the right audience.\nThese are context clues for steps, if they contain these, and you are not in that context you ought to skip them.\nAIRGAP/OFFLINE USE\nThis tag should preface any block that is for offline install steps or procedures, where there is no online/internet connection.\nEXTERNAL USE\nThis tag should be used to highlight anything that an internal user should ignore or skip.\nINTERNAL USE\nThis tag should be used before any block of instruction or text that is only usable or recommended for internal HPE CRAY systems.\nExternal (GitHub or customer) should disregard these annotated blocks - they maybe contain useful information as an example but are not intended for their use.\nPREFERRED\nThis is the preferred path, but if not possible, there will be a MANUAL section which can be done instead\nMANUAL\nThis is a manual path that can be taken if the PREFERRED section is not possible in the given context.\nCommand Prompt Conventions Host name and account in command prompts The host name in a command prompt indicates where the command must be run. The account that must run the command is also indicated in the prompt.\nThe root or super-user account always has the # character at the end of the prompt Any non-root account is indicated with account@hostname. A non-privileged account is referred to as user. Node abbreviations The following list contains abbreviations for nodes used below\nCN - compute Node NCN - Non Compute Node AN - Application Node (special type of NCN) UAN - User Access Node (special type of AN) PIT - Pre-Install Toolkit (initial node used as the inception node during software installation booted from the LiveCD) Prompt Description ncn# Run the command as root on any NCN, except an NCN which is functioning as an Application Node (AN), such as a UAN. ncn-m# Run the command as root on any NCN-M (NCN which is a Kubernetes master node). ncn-m002# Run the command as root on the specific NCN-M (NCN which is a Kubernetes master node) which has this hostname (ncn-m002). ncn-w# Run the command as root on any NCN-W (NCN which is a Kubernetes worker node). ncn-w001# Run the command as root on the specific NCN-W (NCN which is a Kubernetes master node) which has this hostname (ncn-w001). ncn-s# Run the command as root on any NCN-S (NCN which is a Utility Storage node). ncn-s003# Run the command as root on the specific NCN-S (NCN which is a Utility Storage node) which has this hostname (ncn-s003). pit# Run the command as root on the PIT node. linux# Run the command as root on a linux host. uan# Run the command as root on any UAN. uan01# Run the command as root on any UAN. user@uan\u0026gt; Run the command as any non-root user on any UAN. cn# Run the command as root on any CN. Note that a CN will have a hostname of the form nid124356, that is \u0026ldquo;nid\u0026rdquo; and a six digit, zero padded number. hostname# Run the command as root on the specified hostname. user@hostname\u0026gt; Run the command as any non-root user son the specified hostname. Command prompt inside chroot If the chroot command is used, the prompt changes to indicate that it is inside a chroot environment on the system.\nhostname# chroot /path/to/chroot chroot-hostname# Command prompt inside Kubernetes pod If executing a shell inside a container of a Kubernetes pod where the pod name is $podName, the prompt changes to indicate that it is inside the pod. Not all shells are available within every pod, this is an example using a commonly available shell.\nncn# kubectl exec -it $podName /bin/sh pod# Command prompt inside image customization session If using SSH during an image customization session, the prompt changes to indicate that it is inside the image customization environment (pod). This example uses $PORT and $HOST as environment variables with specific settings. When using chroot in this context the prompt will be different than the above chroot example.\nhostname# ssh -p $PORT root@$HOST root@POD# chroot /mnt/image/image-root :/# Directory path in command prompt Example prompts do not include the directory path, because long paths can reduce the clarity of examples. Most of the time, the command can be executed from any directory. When it matters which directory the command is invoked within, the cd command is used to change into the directory, and the directory is referenced with a period (.) to indicate the current directory\nExamples of prompts as they appear on the system:\nhostname:~ # cd /etc hostname:/etc# cd /var/tmp hostname:/var/tmp# ls ./file hostname:/var/tmp# su - user user@hostname:~\u0026gt; cd /usr/bin user hostname:/usr/bin\u0026gt; ./command Examples of prompts as they appear in this publication:\nhostname # cd /etc hostname # cd /var/tmp hostname # ls ./file hostname # su - user user@hostname \u0026gt; cd /usr/bin user@hostname \u0026gt; ./command Command prompts for network switch configuration The prompts when doing network switch configuration can vary widely depending on which vendor switch is being configured and the context of the item being configured on that switch. There may be two levels of user privilege which have different commands available and a special command to enter configuration mode.\nExample of prompts as they appear in this publication:\nEnter \u0026ldquo;setup\u0026rdquo; mode for the switch make and model, for example:\nremote# ssh sw-leaf-001 sw-leaf-001\u0026gt; enable sw-leaf-001# configure terminal sw-leaf-001(conf)# Refer to the switch vendor OEM documentation for more information about configuring a specific switch.\n"
},
{
	"uri": "/docs-csm/en-09/upgrade/0.9/csm-0.9.5/operations/node_management/reboot_ncns/",
	"title": "Reboot NCNs",
	"tags": [],
	"description": "",
	"content": "Reboot NCNs The following is a high-level overview of the non-compute node (NCN) reboot workflow:\nRun the NCN pre-reboot checks and procedures: Ensure ncn-m001 is not running in \u0026ldquo;LiveCD\u0026rdquo; or install mode Check the metal.no-wipe settings for all NCNs Enable pod priorities Run all platform health checks, including checks on the Border Gateway Protocol (BGP) peering sessions Run the rolling NCN reboot procedure: Loop through reboots on storage NCNs, worker NCNs, and master NCNs, where each boot consists of the following workflow: - Establish console session with NCN to reboot Check the hostname of the NCN to be rebooted Execute a power off/on sequence to the NCN to allow it to boot up to completion Check the hosthame of the NCN after reboot and reset it if it is not correct Execute NCN/platform health checks and do not go on to reboot the next NCN until health has been ensured on the most recently rebooted NCN Disconnect console session with the NCN that was rebooted Re-run all platform health checks, including checks on BGP peering sessions The time duration for this procedure (if health checks are being executed in between each boot, as recommended) could take between two to four hours for a system with approximately nine NCNs.\nThis same procedure can be used to reboot a single NCN node as outlined above. Be sure to carry out the NCN pre-reboot checks and procedures before and after rebooting the node. Execute the rolling NCN reboot procedure steps for the particular node type being rebooted.\nPrerequisites This procedure requires that the kubectl command is installed.\nIt also requires that the CSM_SCRIPTDIR variable was previously defined as part of the execution of the steps in the csm-0.9.5 upgrade README. You can verify that it is set by running echo $CSM_SCRIPTDIR on the ncn-m001 cli. If that returns nothing, re-execute the setting of that variable from the csm-0.9.5 README file.\nProcedure NCN Pre-Reboot Health Checks Ensure that ncn-m001 is not running in \u0026ldquo;LiveCD\u0026rdquo; mode.\nThis mode should only be in effect during the initial product install. If the word \u0026ldquo;pit\u0026rdquo; is NOT in the hostname of ncn-m001, then it is not in the \u0026ldquo;LiveCD\u0026rdquo; mode.\nIf \u0026ldquo;pit\u0026rdquo; is in the hostname of ncn-m001, the system is not in normal operational mode and rebooting ncn-m001 may have unexpected results. This procedure assumes that the node is not running in the \u0026ldquo;LiveCD\u0026rdquo; mode that occurs during product install.\nCheck and set the metal.no-wipe setting on NCNs to ensure data on the node is preserved when rebooting.\nRefer to Check and Set the metal.no-wipe Setting on NCNs.\nRun the following script to enable a Kubernetes scheduling pod priority class for a set of critical pods.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/add_pod_priority.sh\u0026#34; After the add_pod_priority.sh script completes, wait five minutes for the changes to take effect.\nncn-m001# sleep 5m Run the platform health checks and analyze the results.\nRefer to the \u0026ldquo;Platform Health Checks\u0026rdquo; section in Validate CSM Health for an overview of the health checks.\nPlease note that though the CSM validation document references running the the HealthCheck scripts from /opt/cray/platform-utils, more recent versions of those scripts are referenced in the instructions below. Please ensure they are run from the location referenced below.\nRun the platform health scripts from ncn-m001:\nThe output of the following scripts will need to be referenced in the remaining sub-steps.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnHealthChecks.sh\u0026#34; ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnPostgresHealthChecks.sh\u0026#34; NOTE: If the ncnHealthChecks script output indicates any kube-multus-ds- pods are in a Terminating state, that can indicate a previous restart of these pods did not complete. In this case, it is safe to force delete these pods in order to let them properly restart by executing the kubectl delete po -n kube-system kube-multus-ds.. --force command. After executing this command, re-running the ncnHealthChecks script should indicate a new pod is in a Running state.\nCheck the status of the Kubernetes nodes.\nEnsure all Kubernetes nodes are in the Ready state.\nncn-m001# kubectl get nodes Troubleshooting: If the NCN that was rebooted is in a Not Ready state, run the following command to get more information.\nncn-m001# kubectl describe node NCN_HOSTNAME Verify the worker or master NCN is now in a Ready state:\nncn-m001# kubectl get nodes Check the status of the Kubernetes pods.\nThe bottom of the output returned after running the ${CSM_SCRIPTDIR}/ncnHealthChecks.sh script will show a list of pods that may be in a bad state. The following command can also be used to look for any pods that are not in a Running or Completed state:\nncn-m001# kubectl get pods -o wide -A | grep -Ev \u0026#39;Running|Completed\u0026#39; It is important to pay attention to that list, but it is equally important to note what pods are in that list before and after NCN reboots to determine if the reboot caused any new issues.\nThere are pods that may normally be in an Error, Not Ready, or Init state, and this may not indicate any problems caused by the NCN reboots. Error states can indicate that a job pod ran and ended in an Error. That means that there may be a problem with that job, but does not necessarily indicate that there is an overall health issue with the system. The key takeaway (for health purposes) is understanding the statuses of pods prior to doing an action like rebooting all of the NCNs. Comparing the pod statuses in between each NCN reboot will give a sense of what is new or different with respect to health.\nMonitor Ceph health continuously.\nIn a separate cli session, run the following command during NCN reboots:\nncn-m001# watch -n 10 \u0026#39;ceph -s\u0026#39; This window can be kept up throughout the reboot process to ensure Ceph remains healthy and to watch if Ceph goes into a WARN state when rebooting storage NCNs. It will be necessary to run it from an ssh session to an NCN that is not the one being rebooted.\nCheck the status of the slurmctld and slurmdbd pods to determine if they are starting:\nncn-m001# kubectl describe pod -n user -lapp=slurmctld ncn-m001# kubectl describe pod -n user -lapp=slurmdbd Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \u0026#34;314ca4285d0706ec3d76a9e953e412d4b0712da4d0cb8138162b53d807d07491\u0026#34;: Multus: Err in tearing down failed plugins: Multus: error in invoke Delegate add - \u0026#34;macvlan\u0026#34;: failed to allocate for range 0: no IP addresses available in range set: 10.252.2.4-10.252.2.4 Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox ... If the preceding error is displayed, then remove all files in the following directories on all worker nodes:\n/var/lib/cni/networks/macvlan-slurmctld-nmn-conf /var/lib/cni/networks/macvlan-slurmdbd-nmn-conf Check that the BGP peering sessions are established by using Check BGP Status and Reset Sessions.\nThis check will need to be run now and after all worker NCNs have been rebooted. Ensure that the checks have been run to check BGP peering sessions on the BGP peer switches (instructions will vary for Aruba and Mellanox switches).\nNCN Rolling Reboot Before rebooting NCNs:\nEnsure pre-reboot checks have been completed, including checking the metal.no-wipe setting for each NCN. Do not proceed if any of the NCN metal.no-wipe settings are zero. Utility Storage Nodes (Ceph) Reboot each of the NCN storage nodes one at a time going from the highest to the lowest number.\nNOTE: You are doing a single storage node at a time, so please keep track of what ncn-s0xx you are on for these steps.\nEstablish a console session to the NCN storage node that is going to be rebooted.\nUse the ${CSM_SCRIPTDIR}/ncnGetXnames.sh script to get the xnames for each of the NCNs. ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnGetXnames.sh\u0026#34; Use cray-conman to observe each node as it boots: ncn-m001# export CONMAN_POD=$(kubectl -n services get pods -l app.kubernetes.io/name=cray-conman -o json | jq -r .items[].metadata.name) ncn-m001# kubectl exec -it -n services $CONMAN_POD cray-conman -- /bin/bash cray-conman# conman -q cray-conman# conman -j XNAME NOTE: Exiting the connection to the console can be achieved with the \u0026amp;. command.\nCheck and take note of the hostname of the storage NCN by running the following command on the NCN which will be rebooted.\nncn-s# hostname Reboot the selected NCN (run this command on the NCN which needs to be rebooted).\nncn-s# shutdown -r now IMPORTANT: If the node does not shutdown after 5 mins, then proceed with the power reset below\nTo power off the node:\nncn-m001# hostname=\u0026lt;ncn being rebooted\u0026gt; # Example value: ncn-s003 ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power off ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nTo power back on the node:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power on ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as on. This may take 5-10 seconds for this to update.\nWatch on the console until the NCN has successfully booted and the login prompt is reached.\nIf the NCN fails to PXE boot, then it may be necessary to force the NCN to boot from disk.\nPower off the NCN:\nncn-m001# hostname=\u0026lt;ncn being rebooted\u0026gt; # Example value: ncn-s003 ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power off ncn-m001# sleep 10 ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Set the boot device for the next boot to disk:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus chassis bootdev disk Power on the NCN:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power on Continue to watch the console as the NCN boots.\nLogin to the storage NCN and ensure that the hostname matches what was being reported before the reboot.\nncn-s# hostname If the hostname after reboot does not match the hostname from before the reboot, the hostname will need to be reset followed by another reboot. The following command will need to be run on the cli for the NCN that has just been rebooted (and is incorrect).\nncn-s# hostnamectl set-hostname $hostname where $hostname is the original hostname from before reboot\nFollow the procedure outlined above to Reboot the selected NCN again and verify the hostname is correctly set, afterward.\nDisconnect from the console.\nRun the platform health checks from the Validate CSM Health procedure.\nRecall that updated copies of the two HealthCheck scripts referenced in the Platform Health Checks can be run from here:\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnHealthChecks.sh\u0026#34; ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnPostgresHealthChecks.sh\u0026#34; Repeat all of the sub-steps above for the remaining storage nodes, going from the highest to lowest number until all storage nodes have successfully rebooted.\nImportant: Ensure ceph -s shows that Ceph is healthy (HEALTH_OK) BEFORE MOVING ON to reboot the next storage node. Once Ceph has recovered the downed mon, it may take a several minutes for Ceph to resolve clock skew.\nNCN Worker Nodes Reboot each of the NCN worker nodes one at a time going from the highest to the lowest number.\nNOTE: You are doing a single worker at a time, so please keep track of what ncn-w0xx you are on for these steps.\nFailover any postgres leader that is running on the NCN worker node you are rebooting.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/failover-leader.sh\u0026#34; \u0026lt;node to be rebooted\u0026gt; Cordon and Drain the node.\nncn-m001# kubectl drain --timeout=300s --ignore-daemonsets=true --delete-local-data=true \u0026lt;node to be rebooted\u0026gt; If the command above exits with similar output to the following, then the drain command ran successfully amd you can proceed to the next step.\nerror: unable to drain node \u0026#34;ncn-w003\u0026#34;, aborting command... There are pending nodes to be drained: ncn-w003 error when evicting pod \u0026#34;cray-dns-unbound-7bb85f9b5b-fjs95\u0026#34;: global timeout reached: 5m0s error when evicting pod \u0026#34;cray-dns-unbound-7bb85f9b5b-kc72b\u0026#34;: global timeout reached: 5m0s Establish a console session to the NCN worker node you are rebooting.\nUse the ${CSM_SCRIPTDIR}/ncnGetXnames.sh script to get the xnames for each of the NCNs.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnGetXnames.sh\u0026#34; Wait for the cray-conman pod to become healthy before continue:\nncn-m001# kubectl -n services get pods -l app.kubernetes.io/name=cray-conman NAME READY STATUS RESTARTS AGE cray-conman-7f956fc9bc-npf7d 3/3 Running 0 5d13h Use cray-conman to observe each node as it boots:\nncn-m001# export CONMAN_POD=$(kubectl -n services get pods -l app.kubernetes.io/name=cray-conman -o json | jq -r .items[].metadata.name) ncn-m001# kubectl exec -it -n services $CONMAN_POD cray-conman -- /bin/bash cray-conman# conman -q cray-conman# conman -j XNAME NOTE: Exiting the connection to the console can be achieved with the \u0026amp;. command.\nCheck and take note of the hostname of the worker NCN by running the following command on the NCN which will be rebooted.\nncn-w# hostname Reboot the selected NCN (run this command on the NCN which needs to be rebooted).\nncn-w# shutdown -r now IMPORTANT: If the node does not shutdown after 5 mins, then proceed with the power reset below\nTo power off the node:\nncn-m001# hostname=\u0026lt;ncn being rebooted\u0026gt; # Example value: ncn-w003 ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power off ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nTo power back on the node:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power on ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as on. This may take 5-10 seconds for this to update.\nWatch on the console until the NCN has successfully booted and the login prompt is reached.\nIf the NCN fails to PXE boot, then it may be necessary to force the NCN to boot from disk.\nPower off the NCN:\nncn-m001# hostname=\u0026lt;ncn being rebooted\u0026gt; # Example value: ncn-w003 ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power off ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Set the boot device for the next boot to disk:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus chassis bootdev disk Power on the NCN:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power on ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Continue to watch the console as the NCN boots.\nLogin to the worker NCN and ensure that the hostname matches what was being reported before the reboot.\nncn-w# hostname If the hostname after reboot does not match the hostname from before the reboot, the hostname will need to be reset followed by another reboot. The following command will need to be run on the cli for the NCN that has just been rebooted (and is incorrect).\nncn-w# hostnamectl set-hostname $hostname where $hostname is the original hostname from before reboot\nFollow the procedure outlined above to Reboot the selected NCN again and verify the hostname is correctly set, afterward.\nDisconnect from the console.\nUncordon the node\nncn-m# kubectl uncordon \u0026lt;node you just rebooted\u0026gt; Run the platform health checks from the Validate CSM Health procedure. The BGP Peering Status and Reset procedure can be skipped, as a different procedure in step 12 will be used to verify the BGP peering status.\nRecall that updated copies of the two HealthCheck scripts referenced in the Platform Health Checks can be run from here:\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnHealthChecks.sh\u0026#34; ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnPostgresHealthChecks.sh\u0026#34; Verify that the Check if any \u0026quot;alarms\u0026quot; are set for any of the Etcd Clusters in the Services Namespace. check from the ncnHealthChecks.sh script reports no alarms set for any of the etcd pods. If an alarm similar to is reported, then wait a few minutes for the alarm to clear and try the ncnHealthChecks.sh script again.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2021-08-11T15:43:36.486Z\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;clientv3/retry_interceptor.go:62\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;retrying of unary invoker failed\u0026#34;,\u0026#34;target\u0026#34;:\u0026#34;endpoint://client-4d8f7712-2c91-4096-bbbe-fe2853cd6959/127.0.0.1:2379\u0026#34;,\u0026#34;attempt\u0026#34;:0,\u0026#34;error\u0026#34;:\u0026#34;rpc error: code = DeadlineExceeded desc = context deadline exceeded\u0026#34;} Verify that the Check the Health of the Etcd Clusters in the Services Namespace check from the ncnHealthChecks.sh script returns a healthy report for all members of each etcd cluster.\nIf pods are reported as Terminating, Init, or Pending when checking the status of the Kubernetes pods, wait for all pods to recover before proceeding.\nTroubleshooting: If the slurmctld and slurmdbd pods do not start after powering back up the node, check for the following error:\nncn-m001# kubectl describe pod -n user -lapp=slurmctld Warning FailedCreatePodSandBox 27m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \u0026#34;82c575cc978db00643b1bf84a4773c064c08dcb93dbd9741ba2e581bc7c5d545\u0026#34;: Multus: Err in tearing down failed plugins: Multus: error in invoke Delegate add - \u0026#34;macvlan\u0026#34;: failed to allocate for range 0: no IP addresses available in range set: 10.252.2.4-10.252.2.4 ncn-m001# kubectl describe pod -n user -lapp=slurmdbd Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \u0026#34;314ca4285d0706ec3d76a9e953e412d4b0712da4d0cb8138162b53d807d07491\u0026#34;: Multus: Err in tearing down failed plugins: Multus: error in invoke Delegate add - \u0026#34;macvlan\u0026#34;: failed to allocate for range 0: no IP addresses available in range set: 10.252.2.4-10.252.2.4 Remove the following files on every worker node to resolve the failure:\n/var/lib/cni/networks/macvlan-slurmctld-nmn-conf /var/lib/cni/networks/macvlan-slurmdbd-nmn-conf Ensure that BGP sessions are reset so that all BGP peering sessions with the spine switches are in an ESTABLISHED state.\nSee Check BGP Status and Reset Sessions.\nRepeat all of the sub-steps above for the remaining worker nodes, going from the highest to lowest number until all worker nodes have successfully rebooted.\nNCN Master Nodes Reboot each of the NCN master nodes one at a time except for ncn-m001 going from the highest to the lowest number.\nNOTE: You are doing a single master node at a time, so please keep track of what ncn-s0xx you are on for these steps.\nEstablish a console session to the NCN storage node that is going to be rebooted.\nUse the ${CSM_SCRIPTDIR}/ncnGetXnames.sh script to get the xnames for each of the NCNs.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnGetXnames.sh\u0026#34; Use cray-conman to observe each node as it boots:\nncn-m001# export CONMAN_POD=$(kubectl -n services get pods -l app.kubernetes.io/name=cray-conman -o json | jq -r .items[].metadata.name) ncn-m001# kubectl exec -it -n services $CONMAN_POD cray-conman -- /bin/bash cray-conman# conman -q cray-conman# conman -j XNAME NOTE: Exiting the connection to the console can be achieved with the \u0026amp;. command.\nCheck and take note of the hostname of the master NCN by running the command on the NCN that will be rebooted.\nncn-m# hostname Reboot the selected NCN (run this command on the NCN which needs to be rebooted).\nncn-m# shutdown -r now IMPORTANT: If the node does not shutdown after 5 mins, then proceed with the power reset below\nTo power off the node:\nncn-m001# hostname=\u0026lt;ncn being rebooted\u0026gt; # Example value: ncn-m003 ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power off ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nTo power back on the node:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power on ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as on. This may take 5-10 seconds for this to update.\nWatch on the console until the NCN has successfully booted and the login prompt is reached.\nIf the NCN fails to PXE boot, then it may be necessary to force the NCN to boot from disk.\nPower off the NCN:\nncn-m001# hostname=\u0026lt;ncn being rebooted\u0026gt; # Example value: ncn-m003 ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power off ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Set the boot device for the next boot to disk:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus chassis bootdev disk Power on the NCN:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power on ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Continue to watch the console as the NCN boots.\nLogin to the master NCN and ensure that the hostname matches what was being reported before the reboot.\nncn-m# hostname If the hostname after reboot does not match the hostname from before the reboot, the hostname will need to be reset followed by another reboot. The following command will need to be run on the cli for the NCN that has just been rebooted (and is incorrect).\nncn-m# hostnamectl set-hostname $hostname where $hostname is the original hostname from before reboot\nFollow the procedure outlined above to Reboot the selected NCN again and verify the hostname is correctly set, afterward.\nDisconnect from the console.\nRun the platform health checks from the Validate CSM Health procedure. The BGP Peering Status and Reset procedure can be skipped, as a different procedure in step 8 will be used to verify the BGP peering status.\nRecall that updated copies of the two HealthCheck scripts referenced in the Platform Health Checks can be run from here:\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnHealthChecks.sh\u0026#34; ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnPostgresHealthChecks.sh\u0026#34; Ensure that BGP sessions are reset so that all BGP peering sessions with the spine switches are in an ESTABLISHED state.\nSee Check BGP Status and Reset Sessions.\nRepeat all of the sub-steps above for the remaining master nodes (excluding ncn-m001), going from the highest to lowest number until all master nodes have successfully rebooted.\nReboot ncn-m001.\nDetermine the CAN IP address for one of the other NCNs in the system to establish an SSH session with that NCN.\nncn-m001# ssh ncn-m002 ncn-m002# ip a show vlan007 | grep inet Expected output looks similar to the following:\ninet 10.102.11.13/24 brd 10.102.11.255 scope global vlan007 inet6 fe80::1602:ecff:fed9:7820/64 scope link Now login from another machine to verify that IP is usable:\nexternal# ssh root@10.102.11.13 ncn-m002# Establish a console session to ncn-m001 from a remote system, as the BMC of ncn-m001 is the NCN that has an externally facing IP address.\nexternal# SYSTEM_NAME=eniac external# ipmitool -I lanplus -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt sol activate Check and take note of the hostname of the ncn-m001 NCN by running this command on it:\nncn-m001# hostname Reboot ncn-m001.\nncn-m001# shutdown -r now IMPORTANT: If the node does not shutdown after 5 mins, then proceed with the power reset below\nTo power off the node:\nexternal# SYSTEM_NAME=eniac external# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power off external# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power status Ensure the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nTo power back on the node:\nexternal# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power on external# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power status Ensure the power is reporting as on. This may take 5-10 seconds for this to update.\nWatch on the console until the NCN has successfully booted and the login prompt is reached.\nIf the NCN fails to PXE boot, then it may be necessary to force the NCN to boot from disk.\nPower off the NCN:\nexternal# SYSTEM_NAME=eniac external# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power off external# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power status Set the boot device for the next boot to disk:\nexternal# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanpluschassis bootdev disk Power on the NCN:\nexternal# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power on external# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power status Continue to watch the console as the NCN boots.\nLogin to ncn-m001 and ensure that the hostname matches what was being reported before the reboot.\nncn-m001# hostname If the hostname after reboot does not match the hostname from before the reboot, the hostname will need to be reset followed by another reboot. The following command will need to be run on the cli for the NCN that has just been rebooted (and is incorrect).\nncn-m001# hostname=ncn-m001 ncn-m001# hostnamectl set-hostname $hostname where $hostname is the original hostname from before reboot\nFollow the procedure outlined above to Power cycle the node again and verify the hostname is correctly set, afterward.\nDisconnect from the console.\nSet CSM_SCRIPTDIR to the scripts directory included in the docs-csm RPM for the CSM 0.9.5 patch:\nncn-m001# export CSM_SCRIPTDIR=/usr/share/doc/metal/upgrade/0.9/csm-0.9.5/scripts Run the platform health checks from the Validate CSM Health procedure. The BGP Peering Status and Reset procedure can be skipped, as a different procedure in the next step step 10 will be used to verify the BGP peering status.\nRecall that updated copies of the two HealthCheck scripts referenced in the Platform Health Checks can be run from here:\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnHealthChecks.sh\u0026#34; ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnPostgresHealthChecks.sh\u0026#34; Ensure that BGP sessions are reset so that all BGP peering sessions with the spine switches are in an ESTABLISHED state.\nSee Check BGP Status and Reset Sessions.\n"
},
{
	"uri": "/docs-csm/en-09/troubleshooting/known_issues/conman_pod_kubernetes_copy_fails/",
	"title": "Copying file from the cray-conman pod fails",
	"tags": [],
	"description": "",
	"content": "Copying file from the cray-conman pod fails The \u0026rsquo;tar\u0026rsquo; command is not installed in the pod image, so the usual kubernetes command to copy files from the cray-conman pod fails:\n$ kubectl -n services cp cray-conman-92a6cb7d2a:/var/log/conman/console.x3000c1s2b0n1 console.x3000c1s2b0n1 Defaulting container name to cray-conman. error: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec \u0026#34;e5054fd1452d04993a1e200435416168476621b8a44b8019a45a225fcb5c36f7\u0026#34;: OCI runtime exec failed: exec failed: container_linux.go:349: starting container process caused \u0026#34;exec: \\\u0026#34;tar\\\u0026#34;: executable file not found in $PATH\u0026#34;: unknown The files may still be copied by executing the \u0026lsquo;cat\u0026rsquo; command instead and redirecting the output to a file:\n$ kubectl -n services exec cray-conman-92a6cb7d2a -- cat /var/log/conman/console.x3000c1s2b0n1 \u0026gt; console.x3000c1s2b0n1 The console logs are also collected in SMF and may be accessed through the system monitoring tools.\n"
},
{
	"uri": "/docs-csm/en-09/upgrade/0.9/csm-0.9.2/",
	"title": "CSM 0.9.2 Patch Upgrade Guide",
	"tags": [],
	"description": "",
	"content": "Copyright 2021 Hewlett Packard Enterprise Development LP\nCSM 0.9.2 Patch Upgrade Guide This guide contains procedures for upgrading systems running CSM 0.9.0 to CSM 0.9.2. It is intended for system installers, system administrators, and network administrators. It assumes some familiarity with standard Linux and associated tooling.\nNOTE: CSM 0.9.1 was not officially released so these procedures start with CSM 0.9.0.\nSee CHANGELOG.md in the root of a CSM release distribution for a summary of changes in each CSM release.\nProcedures:\nPreparation Run Validation Checks (Pre-Upgrade) Update Customizations Setup Nexus Deploy Manifests Upgrade NCN Packages Switch VCS Configuration Repositories to Private Configure Prometheus Alert Notifications to Detect Postgres Replication Lag Run Validation Checks (Post-Upgrade) Update BGP Configuration Upgrade Firmware on Chassis Controllers Exit Typescript Preparation For convenience, these procedures make use of environment variables. This section sets the expected environment variables to the appropriate values.\nStart a typescript to capture the commands and output from this procedure.\nncn-m001# script -af csm-update.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Set CSM_DISTDIR to the directory of the extracted release distribution for CSM 0.9.2:\nNOTE: Use --no-same-owner and --no-same-permissions options to tar when extracting a CSM release distribution as root to ensure the extracted files are owned by root and have permissions based on the current umask value.\nncn-m001# tar --no-same-owner --no-same-permissions -zxvf csm-0.9.2.tar.gz ncn-m001# CSM_DISTDIR=\u0026#34;$(pwd)/csm-0.9.2\u0026#34; Set CSM_RELEASE_VERSION to the version reported by ${CSM_DISTDIR}/lib/version.sh:\nncn-m001# CSM_RELEASE_VERSION=\u0026#34;$(${CSM_DISTDIR}/lib/version.sh --version)\u0026#34; Set CSM_SYSTEM_VERSION to 0.9.0:\nncn-m001# CSM_SYSTEM_VERSION=\u0026#34;0.9.0\u0026#34; NOTE: Installed CSM versions may be listed from the product catalog using:\nncn-m001# kubectl -n services get cm cray-product-catalog -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r -j - | jq -r \u0026#39;keys[]\u0026#39; | sed \u0026#39;/-/!{s/$/_/}\u0026#39; | sort -V | sed \u0026#39;s/_$//\u0026#39; Run Validation Checks (Pre-Upgrade) It is important to first verify a healthy starting state. To do this, run the CSM validation checks. If any problems are found, correct them and verify the appropriate validation checks before proceeding.\nUpdate Customizations Before deploying upgraded manifests, customizations.yaml in the site-init secret in the loftsman namespace must be updated.\nIf the site-init repository is available as a remote repository then clone it on the host orchestrating the upgrade:\nncn-m001# git clone \u0026#34;$SITE_INIT_REPO_URL\u0026#34; site-init Otherwise, create a new site-init working tree:\nncn-m001# git init site-init Download customizations.yaml:\nncn-m001# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; site-init/customizations.yaml Review, add, and commit customizations.yaml to the local site-init repository as appropriate.\nNOTE: If site-init was cloned from a remote repository in step 1, there may not be any differences and hence nothing to commit. This is okay. If there are differences between what is in the repository and what was stored in the site-init, then it suggests settings were improperly changed at some point. If that is the case then be cautious, there may be dragons ahead.\nncn-m001# cd site-init ncn-m001# git diff ncn-m001# git add customizations.yaml ncn-m001# git commit -m \u0026#39;Add customizations.yaml from site-init secret\u0026#39; Update customizations.yaml.\nncn-m001# yq d -i customizations.yaml spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources Review the changes to customizations.yaml and verify baseline system customizations and any customer-specific settings are correct.\nncn-m001# git diff Add and commit customizations.yaml if there are any changes:\nncn-m001# git add customizations.yaml ncn-m001# git commit -m \u0026#34;Update customizations.yaml consistent with CSM $CSM_RELEASE_VERSION\u0026#34; Update site-init sealed secret in loftsman namespace:\nncn-m001# kubectl delete secret -n loftsman site-init ncn-m001# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml Push to the remote repository as appropriate:\nncn-m001# git push Setup Nexus Run lib/setup-nexus.sh to configure Nexus and upload new CSM RPM repositories, container images, and Helm charts:\nncn-m001# cd \u0026#34;$CSM_DISTDIR\u0026#34; ncn-m001# ./lib/setup-nexus.sh On success, setup-nexus.sh will output to OK on stderr and exit with status code 0, e.g.:\nncn-m001# ./lib/setup-nexus.sh ... + Nexus setup complete setup-nexus.sh: OK In the event of an error, consult the known issues from the install documentation to resolve potential problems and then try running setup-nexus.sh again. Note that subsequent runs of setup-nexus.sh may report FAIL when uploading duplicate assets. This is ok as long as setup-nexus.sh outputs setup-nexus.sh: OK and exits with status code 0.\nDeploy Manifests Run kubectl delete -n spire job spire-update-bss to allow the spire chart to be updated properly:\nncn-m001# kubectl delete -n spire job spire-update-bss Run upgrade.sh to deploy upgraded CSM applications and services:\nncn-m001# ./upgrade.sh Upgrade NCN Packages Upgrade packages on NCNs.\nGet the list of NCNs:\nncn-m001# ncns=\u0026#34;$(./lib/list-ncns.sh | paste -sd,)\u0026#34; Use zypper ms -d to disable the following zypper RIS services that configure repositories external to the system:\nBasesystem_Module_15_SP2_x86_64 Public_Cloud_Module_15_SP2_x86_64 SUSE_Linux_Enterprise_Server_15_SP2_x86_64 Server_Applications_Module_15_SP2_x86_64 ncn-m001# pdsh -w \u0026#34;$ncns\u0026#34; \u0026#39;zypper ms -d Basesystem_Module_15_SP2_x86_64\u0026#39; ncn-m001# pdsh -w \u0026#34;$ncns\u0026#34; \u0026#39;zypper ms -d Public_Cloud_Module_15_SP2_x86_64\u0026#39; ncn-m001# pdsh -w \u0026#34;$ncns\u0026#34; \u0026#39;zypper ms -d SUSE_Linux_Enterprise_Server_15_SP2_x86_64\u0026#39; ncn-m001# pdsh -w \u0026#34;$ncns\u0026#34; \u0026#39;zypper ms -d Server_Applications_Module_15_SP2_x86_64\u0026#39; NOTE: Field notice FN #6615a - Shasta V1.4 and V1.4.1 Install Issue with NCN Personalization for SMA included similar guidance as below. If these zypper services have been previously disabled, verify that they are in fact disabled:\nncn-m001# pdsh -w \u0026#34;$ncns\u0026#34; \u0026#39;zypper ls -u\u0026#39; Ensure the csm-sle-15sp2 repository is configured on every NCN:\nncn-m001# pdsh -w \u0026#34;$ncns\u0026#34; \u0026#39;zypper ar -fG https://packages.local/repository/csm-sle-15sp2/ csm-sle-15sp2\u0026#39; WARNING: If the csm-sle-15sp2 repository is already configured on a node zypper ar will error with e.g.:\nAdding repository \u0026#39;csm-sle-15sp2\u0026#39; [...error] Repository named \u0026#39;csm-sle-15sp2\u0026#39; already exists. Please use another alias. These errors may be ignored.\nInstall the hpe-csm-scripts package on each NCN:\nncn-m001# pdsh -w \u0026#34;$ncns\u0026#34; \u0026#39;zypper in -y hpe-csm-scripts\u0026#39; Switch VCS Configuration Repositories to Private Previous installs of CSM and other Cray products created git repositories in the VCS service which were set to be publicly visible. To enhance security, please follow the instructions in the Admin guide, chapter 12, \u0026ldquo;Version Control Service (VCS)\u0026rdquo; section to switch the visibility of all *-config-management repositories to private.\nFuture installations of configuration content into Gitea by CSM and other Cray products will create or patch repositories to private visibility automatically.\nAs a result of this change, git clone operations will now require credentials. CSM services that clone repositories have been upgraded to use the crayvcs user to clone repositories.\nConfigure Prometheus Alert Notifications to Detect Postgres Replication Lag Three new Prometheus alert definitions have been added in CSM 0.9.1 for monitoring replication across Postgres instances, which are used by some system management services. The new alerts are PostgresqlReplicationLagSMA (for Postgres instances in the sma namespace), PostgresqlReplicationLagServices (for Postgres instances in all other namespaces), and PostgresqlInactiveReplicationSlot.\nIn the event that a state of broken Postgres replication persists to the extent that space allocated for its WAL files fills-up, the affected database will likely shut down and create a state where it cannot be brought up again. This can impact the reliability of the related service and can require that it be redeployed with data re-population procedures.\nTo avoid this unexpected, but possible event, it is recommended that all administrators configure Prometheus alert notifications for the early detection of Postgres replication lag and, if notified, swiftly follow the suggested remediation actions (to avoid service down-time).\nPlease access the relevant sections of the 1.4 HPE Cray EX System Administration Guide for information about how to configure Prometheus Alert Notifications (\u0026ldquo;System Management Health Checks and Alerts\u0026rdquo; sub-section under \u0026ldquo;Monitor the System\u0026rdquo;) and how to re-initialize a Postgres cluster encountering signs of replication lag (\u0026ldquo;About Postgres\u0026rdquo; sub-section under \u0026ldquo;Kubernetes Architecture\u0026rdquo;).\nRun Validation Checks (Post-Upgrade) IMPORTANT: Wait at least 15 minutes after upgrade.sh completes to let the various Kubernetes resources get initialized and started.\nRun the following validation checks to ensure that everything is still working properly after the upgrade:\nPlatform health checks Network health checks Other health checks may be run as desired.\nCAUTION: The following HMS functional tests may fail because of locked components in HSM:\ntest_bss_bootscript_ncn-functional_remote-functional.tavern.yaml test_smd_components_ncn-functional_remote-functional.tavern.yaml Traceback (most recent call last): File \u0026#34;/usr/lib/python3.8/site-packages/tavern/schemas/files.py\u0026#34;, line 106, in verify_generic verifier.validate() File \u0026#34;/usr/lib/python3.8/site-packages/pykwalify/core.py\u0026#34;, line 166, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/0\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/5\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/6\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/7\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/8\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/9\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/10\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/11\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/12\u0026#39;.: Path: \u0026#39;/\u0026#39;\u0026gt; Failures of these tests because of locked components as shown above can be safely ignored.\nUpdate BGP Configuration IMPORTANT: This procedure applies to systems with Aruba management switches.\nIf your Shasta system is using Aruba management switches run the updated BGP script /opt/cray/csm/scripts/networking/BGP/Aruba_BGP_Peers.py.\nSet the SWITCH_IPS variable to an array containing the IP addresses of the switches.\nEXAMPLE:: The following can be used to determine the IP addresses of the switches running BGP:\nncn-m001# kubectl get cm config -n metallb-system -o yaml | head -12 apiVersion: v1 data: config: | peers: - peer-address: 10.252.0.2 peer-asn: 65533 my-asn: 65533 - peer-address: 10.252.0.3 peer-asn: 65533 my-asn: 65533 address-pools: - name: customer-access In the above output 10.252.0.2 and 10.252.0.3 are the switches running BGP. Set SWITCH_IPS as follows:\nncn-m001# SWITCH_IPS=( 10.252.0.2 10.252.0.3 ) Run:\nncn-m001# /opt/cray/csm/scripts/networking/BGP/Aruba_BGP_Peers.py \u0026#34;${SWITCH_IPS[@]}\u0026#34; Remove the static routes configured in LAYER3-CONFIG. Log into the switches running BGP (Spines/Aggs) and remove them:\nNote: To view the current static routes setup on the switch run the following\nsw-spine01# show ip route static Displaying ipv4 routes selected for forwarding \u0026#39;[x/y]\u0026#39; denotes [distance/metric] 0.0.0.0/0, vrf default via 10.103.15.161, [1/0], static 10.92.100.60/32, vrf default via 10.252.1.10, [1/0], static 10.94.100.60/32, vrf default via 10.252.1.10, [1/0], static In the above example the static routes that need to be removed point to 10.252.1.10\nsw-spine-001(config)# no ip route 10.92.100.60/32 10.252.1.10 sw-spine-001(config)# no ip route 10.94.100.60/32 10.252.1.10 Verify the BGP configuration.\nUpgrade Firmware on Chassis Controllers Check to see if firmware is loaded into FAS:\nncn-m001# cray fas images list | grep cc.1.4.19 If firmware not installed, rerun the FAS loader:\nncn-w001# kubectl -n services get jobs | grep fas-loader cray-fas-loader-1 1/1 8m57s 7d15h NOTE: In the above example, the returned job name is cray-fas-loader-1, hence that is the job to rerun.\nncn-m001# kubectl -n services get job cray-fas-loader-1 -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | kubectl replace --force -f - When completed, verify the firmware was loaded into FAS:\nncn-m001# cray fas images list | grep cc.1.4.19 Update the Chassis Controller BMC Firmware:\nPower off the chassis slots.\nDisable the hms-discovery job:\nncn-m001# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;suspend\u0026#34;:true}}\u0026#39; Power off all the components: for example, in chassis 0-7. cabinets 1000-1003:\nncn-m001# cray capmc xname_off create --xnames x[1000-1003]c[0-7] --recursive true --continue true Create an upgrade JSON file ccBMCupdate.json:\n{ \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;chassisBMC\u0026#34; ] }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Cray Chassis Controllers\u0026#34; } } Using the above JSON file run a dry-run with FAS:\nncn-w001# cray fas actions create ccBMCupdate.json Check the output from the dry-run with the command: cray fas actions describe {action-id} (where action-id was the actionId returned for the fas actions create command)\nIf dry-run succeeded with updates to version 1.4.19, change \u0026quot;overrideDryrun\u0026quot; in the above JSON file to true and update the description. Rerun FAS with the updated JSON file to do the actual updates.\nAfter firmware update completes, restart the hms-discovery cronjob:\nncn-m001 # kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : false }}\u0026#39; The hms-discovery cronjob will run within 5 minutes of being unsuspended and start powering on the chassis enclosures, switches, and compute blades. If components are not being powered back on, then power them on manually:\nncn-m001 # cray capmc xname_on create --xnames x[1000-1003]c[0-7]r[0-7],x[1000-1003]c[0-7]s[0-7] --prereq true --continue true The --prereq option ensures all required components are powered on first. The --continue option allows the command to complete in systems without fully populated hardware.\nAfter the components have powered on, boot the nodes using the Boot Orchestration Services (BOS).\nExit Typescript Remember to exit your typescript.\nncn-m001# exit It is recommended to save the typescript file for later reference.\n"
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/change_air-cooled_node_bmc_credentials/",
	"title": "Change Air-Cooled Node BMC Credentials",
	"tags": [],
	"description": "",
	"content": "Change Air-Cooled Node BMC Credentials This procedure will use the System Configuration Service (SCSD) to change all air-cooled Node BMCs in the system to the same global credential.\nLimitations All air-cooled and liquid-cooled BMCs share the same global credentials. The air-cooled Slingshot switch controllers (Router BMCs) must have the same credentials as the liquid-cooled Slingshot switch controllers.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. Procedure Create a SCSD payload file to change all air-cooled node BMCs to the same global credential:\nncn-m001# export NEW_BMC_CREDENTIAL=new.root.password ncn-m001# cat \u0026gt; bmc_creds_glb.json \u0026lt;\u0026lt;DATA { \u0026#34;Force\u0026#34;:false, \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;$NEW_BMC_CREDENTIAL\u0026#34;, \u0026#34;Targets\u0026#34;: $(cray hsm state components list --class River --type NodeBMC --format json | jq -r \u0026#39;[.Components[] | .ID]\u0026#39;) } DATA Inspect the generated SCSD payload file:\nncn-m001# cat bmc_creds_glb.json | jq Apply the new BMC credentials:\nncn-m001# cray scsd bmc globalcreds create ./bmc_creds_glb.json Troubleshooting: If the above command has any components that do not have the status of OK, they must be retried until they work, or the retries are exhausted and noted as failures. Failed modules need to be taken out of the system until they are fixed.\n"
},
{
	"uri": "/docs-csm/en-09/operations/node_management/check_and_set_the_metalno-wipe_setting_on_ncns/",
	"title": "Check and Set the metal.no-wipe Setting on NCNs",
	"tags": [],
	"description": "",
	"content": "Check and Set the metal.no-wipe Setting on NCNs Configure the metal.no-wipe setting on non-compute nodes (NCNs) to preserve data on the nodes before doing an NCN reboot.\nRun the ncnGetXnames.shscript to view the metal.no-wipe settings for each NCN. The xname and metal.no-wipe settings are also dumped out when executing the ncnHealthChecks.sh script.\nPrerequisites This procedure requires administrative privileges.\nIt also requires that the CSM_SCRIPTDIR variable is defined. Set this variable and verify that it is set.\nncn-m001# export CSM_SCRIPTDIR=/usr/share/doc/metal/scripts ncn-m001# echo $CSM_SCRIPTDIR Procedure Run the ${CSM_SCRIPTDIR}/ncnGetXnames.sh script.\nThe output will include a listing of all of the NCNs, their xnames, and what the metal.no-wipe setting is for each.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnGetXnames.sh\u0026#34; ncn-m001: x3000c0s1b0n0 - metal.no-wipe=1 ncn-m002: x3000c0s2b0n0 - metal.no-wipe=1 ncn-m003: x3000c0s3b0n0 - metal.no-wipe=1 ncn-w001: x3000c0s4b0n0 - metal.no-wipe=1 ncn-w002: x3000c0s5b0n0 - metal.no-wipe=1 ncn-w003: x3000c0s6b0n0 - metal.no-wipe=1 ncn-s001: x3000c0s7b0n0 - metal.no-wipe=1 ncn-s002: x3000c0s8b0n0 - metal.no-wipe=1 ncn-s003: x3000c0s9b0n0 - metal.no-wipe=1 The metal.no-wipe setting must be set to 1 (metal.no-wipe=1) if doing a reboot of an NCN to preserve the current data on it. If it is not set to 1 when the NCN is rebooted, it will be completely wiped and will subsequently have to be rebuilt. If the metal.no-wipe status for one or more NCNs is not returned, re-run the ncnGetXnames.sh script.\nReset the metal.no-wipe settings for any NCN where it is set to 0.\nThis step can be skipped if the metal.no-wipe is already set to 1 for any NCNs being rebooted.\nGenerate a token from any master or worker NCN.\nncn-m001# export TOKEN=$(curl -k -s -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; \\ | base64 -d` https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Update the metal.no-wipe settings.\nncn-m001# csi handoff bss-update-param --set metal.no-wipe=1 Run the ${CSM_SCRIPTDIR}/ncnGetXnames.sh script again to verify the no-wipe settings have been reset as expected.\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnGetXnames.sh\u0026#34; ncn-m001: x3000c0s1b0n0 - metal.no-wipe=1 ncn-m002: x3000c0s2b0n0 - metal.no-wipe=1 ncn-m003: x3000c0s3b0n0 - metal.no-wipe=1 ncn-w001: x3000c0s4b0n0 - metal.no-wipe=1 ncn-w002: x3000c0s5b0n0 - metal.no-wipe=1 ncn-w003: x3000c0s6b0n0 - metal.no-wipe=1 ncn-s001: x3000c0s7b0n0 - metal.no-wipe=1 ncn-s002: x3000c0s8b0n0 - metal.no-wipe=1 ncn-s003: x3000c0s9b0n0 - metal.no-wipe=1 "
},
{
	"uri": "/docs-csm/en-09/operations/csm_product_management/change_passwords_and_credentials/",
	"title": "Change Passwords and Credentials",
	"tags": [],
	"description": "",
	"content": "Change Passwords and Credentials This is an overarching procedure to change all credentials managed by Cray System Management (CSM) in HPE Cray EX system to new values.\nThere are many passwords and credentials used in different contexts to manage the system. These can be changed as needed. Their initial settings are documented, so it is recommended to change them during or soon after a CSM software installation.\nProcedure 1. Change Hardware Credentials Perform procedures in Change Cray EX Cabinet Global Default Password.\nPerform procedures in Update Default Air-Cooled BMC and Leaf Switch SNMP Credentials.\nPerform procedures in Change Air-Cooled Node BMC Credentials.\nPerform procedures in Change SMNP Credentials on Leaf Switches.\nPerform procedures in Update Default ServerTech PDU Credentials used by the Redfish Translation Service (RTS).\nPerform procedures in Change Credentials on ServerTech PDUs.\n2. Change Node Credentials Perform the procedure in in section 8.1.1 \u0026ldquo;Update NCN Passwords\u0026rdquo; in the HPE Cray EX System Administration Guide 1.4 S-8001.\nPerform the procedure in section 8.1.2 \u0026ldquo;Change Root Passwords for Compute Nodes\u0026rdquo; in the HPE Cray EX System Administration Guide 1.4 S-8001.\n3. Change Service Credentials Perform the procedure in section 8.5.2 \u0026ldquo;Change the Keycloak Admin Password\u0026rdquo; in the HPE Cray EX System Administration Guide 1.4 S-8001. "
},
{
	"uri": "/docs-csm/en-09/operations/boot_orchestration/upload_node_boot_information_to_boot_script_service_bss/",
	"title": "Upload Node Boot Information to Boot Script Service (BSS)",
	"tags": [],
	"description": "",
	"content": "Upload Node Boot Information to Boot Script Service (BSS) The following information must be uploaded to BSS as a prerequisite to booting a node via iPXE:\nThe location of an initrd image in the artifact repository The location of a kernel image in the artifact repository Kernel boot parameters The node(s) associated with that information, using either host name or NID BSS manages the iPXE boot scripts that coordinate the boot process for nodes, and it enables basic association of boot scripts with nodes. The boot scripts supply a booting node with a pointer to the necessary images (kernel and initrd) and a set of boot-time parameters.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. Boot Script Service (BSS) is running in containers on a non-compute node (NCN). An initrd image and kernel image for one or more nodes have been uploaded to the artifact repository. Procedure Because the parameters that must be specified in the PUT command are lengthy, this procedure shows a simple bash script (not to be confused with iPXE boot scripts) to enter the boot information into BSS. The first step creates a script that can use either node ID (NID) or host name to identify the node(s) with which to associate the boot information.\nCreate a bash script to enter the following boot information into BSS in preparation for booting one or more nodes identified by NID or host name.\nNCN = the host name of a non-compute node (NCN) that is a Kubernetes master node. This procedure uses api-gw-service-nmn.local, the API service name on the Node Management Network (NMN).\nKERNEL = the download URL of the kernel image artifact that was uploaded to S3, which is in the s3://s3_BUCKET/S3_OBJECT_KEY/kernel format.\nINITRD = the download URL of the initrd image artifact that was uploaded to S3, which is in the s3://s3_BUCKET/S3_OBJECT_KEY/initrd format.\nPARAMS = the boot kernel parameters.\nIMPORTANT: The PARAMS line must always include the substring crashkernel=360M. This enables node dumps, which are needed to troubleshoot node crashes.\nNIDS = a list of node IDs of the nodes to be booted.\nHOSTS = a list of strings identifying by host name the nodes to be booted.\nThe following script is generic. A script with specific values is below this one.\n#!/bin/bash NCN=api-gw-service-nmn.local KERNEL=s3://S3_BUCKET/S3_OBJECT_KEY/initrd INITRD=s3://S3_BUCKET/S3_OBJECT_KEY/kernel PARAMS=\u0026#34;STRING_WITH_BOOT_PARAMETERS crashkernel=360M\u0026#34; # # By NID NIDS=NID1,NID2,NID3 cray bss bootparameters create --nids $NIDS --kernel $KERNEL --initrd $INITRD --params $PARAMS # # By host name #HOSTS=\u0026#34;STRING_IDENTIFYING_HOST1\u0026#34;,\u0026#34;STRING_IDENTIFYING_HOST2\u0026#34; #cray bss bootparameters create --hosts $HOSTS --kernel $KERNEL --initrd $INITRD --params $PARAMS BSS supports a mechanism that allows for a default boot setup, rather than needing to specify boot details for each specific node. The HOSTS value should be set to \u0026ldquo;Default\u0026rdquo; in order to utilize the default boot setup. This feature is particular useful with larger systems.\nThe following script has specific values for the kernel/initrd image names, the kernel parameters, and the list of NIDS and hosts.\n#!/bin/bash NCN=api-gw-service-nmn.local KERNEL=s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/kernel INITRD=s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/initrd PARAMS=\u0026#34;console=ttyS0,115200n8 console=tty0 initrd=97b548b9-2ea9-45c9-95ba-dfc77e5522eb root=nfs:$NCN:/var/lib/nfsroot/cmp000001_image rw nofb selinux=0 rd.net.timeout.carrier=20 crashkernel=360M\u0026#34; PARAMS=\u0026#34;console=ttyS0,115200n8 console=tty0 initrd=${INITRD##*/} \\ root=nfs:10.2.0.1:$NFS_IMAGE_ROOT_DIR rw nofb selinux=0 rd.shell crashkernel=360M \\ ip=dhcp rd.neednet=1 htburl=https://10.2.100.50/apis/hbtd/hmi/v1/heartbeat\u0026#34; # # By NID NIDS=1 cray bss bootparameters create --nids $NIDS --kernel $KERNEL --initrd $INITRD --params $PARAMS # # By host name #HOSTS=\u0026#34;nid000001-nmn\u0026#34; #cray bss bootparameters create --hosts $HOSTS --kernel $KERNEL --initrd $INITRD --params $PARAMS Run the bash script to upload the boot information to BSS for the identified nodes.\nlocalhost# chmod +x script.sh \u0026amp;\u0026amp; ./script.sh View the boot script.\nThis will show the specific boot script that will be passed to a given node when requesting a boot script. This is useful for debugging boot problems and to verify that BSS is configured correctly.\nlocalhost# cray bss bootscript list --nid NODE_ID Confirm that the information has been uploaded to BSS.\nIf nodes identified by host name:\nlocalhost# cray bss bootparameters list --hosts HOST_NAME For example:\nlocalhost# cray bss bootparameters list --hosts Default If nodes identified by NID:\nlocalhost# cray bss bootparameters list --nids NODE_ID For example:\nlocalhost# cray bss bootparameters list --nids 1 View entire contents of BSS, if desired.\nlocalhost# cray bss dumpstate list To view the information retrieved from the HSM:\nlocalhost# cray bss hosts list To view the view the configured boot parameter information:\nlocalhost# cray bss bootparameters list Boot information has been added to BSS in preparation for iPXE booting all nodes in the list of host names or NIDs.\nAs part of power up the nodes in the host name or NID list, the next step is to reboot the nodes.\n"
},
{
	"uri": "/docs-csm/en-09/001-guides/",
	"title": "CSM Documentation Guide",
	"tags": [],
	"description": "",
	"content": "CSM Documentation Guide The installation of CSM software has three scenarios which are described in this documentation with many supporting procedures. Here is an overview of the workflow through the documentation to support these scenarios.\n[CSM Install] (002-CSM-INSTALL.md)\nInstallation prerequisites\nSatisfy the prerequisites for one of these three installation scenarios\nMigration from a Shasta v1.3.x system. How to collect information from a v1.3.x system to be used during the v1.4 installation, quiescing v1.3.x, checking and updating firmware to required versions, recabling site connection to shift from ncn-w001 to ncn-m001, adjust management NCNs to boot over PCIe instead of onboard NICs, shutting down Kubernetes, and powering off NCNs. Then you move to Starting an Installation. First time installation of Shasta software (a bare-metal install). This describes now to setup the LiveCD to be able to collect the configuration payload, configure network switches, update firmware for switches and nodes to required versions. Then you move to Starting an Installation. Reinstalling Shasta v1.4 software on a system which previously had Shasta v1.4 installed. These steps here are scaling down DHCP, wiping disks on the NCNs (except ncn-m001), power off NCNs, change management node BMCs to be DHCP not Static, and powering off the LiveCD or ncn-m001 (unless it will be used to prepare the LiveCD). Then you move to Starting an Installation. Once the installation prerequisites have been addressed, the installation is very similar for all of them. There are a few places where a comment will be made for how one of the scenarios needs to do something different.\nStarting an installation\nThe three separate scenarios (above) continue the same way at this point, with preparation and then booting from the LiveCD.\nThis version of the documentation supports booting from a [CSM USB LiveCD] (003-CSM-USB-LIVECD.md). A future version of this documentation will support booting from a virtual ISO method [CSM Remote LiveCD] (004-CSM-REMOTE-LIVECD.md). CSM USB LiveCD - Creation and Configuration Preparation of the LiveCD on a USB stick can be done from a Linux system such as a booted ncn-m001 node with either Shasta v1.3 or v1.4 or a laptop or desktop.\nDownload and Expand the CSM Release Create the Bootable Media Configuration Payload Generate Installation Files CSI Workarounds SHASTA-CFG Pre-Populate LiveCD Daemons Configuration and NCN Artifacts Boot the LiveCD First Login CSM Metal Install Now that ncn-m001 has been booted from the LiveCD, the other management NCNs will be deployed to create the management Kubernetes cluster.\nConfigure Bootstrap Registry to Proxy an Upstream Registry Tokens and IPMI Password Timing of Deployments NCN Deployment Apply NCN Pre-Boot Workarounds Ensure Time Is Accurate Before Deploying NCNs Start Deployment Workflow Deploy Apply NCN Post-Boot Workarounds LiveCD Cluster Authentication BGP Routing Validation Optional Validation Configure and Trim UEFI Entries CSM Platform Install Install all of the CSM applications and services into the management Kubernetes cluster.\nInitialize Bootstrap Registry Create Site-Init Secret Deploy Sealed Secret Decryption Key Deploy CSM Applications and Services Setup Nexus Set NCNs to use Unbound Validate CSM Install Reboot from the LiveCD to NCN Add Compute Cabinet Routing to NCNs Known Issues error: timed out waiting for the condition on jobs/cray-sls-init-load Error: not ready: https://packages.local Error initiating layer upload \u0026hellip; in registry.local: received unexpected HTTP status: 200 OK Error lookup registry.local: no such host CSM Install Validation and Health Checks The CSM installation validation and health checks can be run after install.sh finishes in this installation process, but can also be run at other times later.\nPlatform Health Checks ncnHealthChecks ncnPostgresHealthChecks BGP Peering Status and Reset Mellanox Switch Aruba Switch Network Health Checks Verify that KEA has active DHCP leases Verify ability to resolve external DNS Verify Spire Agent is Running on Kubernetes NCNs Verify the Vault Cluster is Healthy Automated Goss Testing Known Goss Test Issues Hardware Management Services Tests Test Execution Cray Management Services Validation Utility Usage Interpreting Results Checks To Run Booting CSM Barebones Image Locate the CSM Barebones Image in IMS Create a BOS Session Template for the CSM Barebones Image Find an available compute node and boot the session template Reboot node Verify console connections Connect to the node\u0026rsquo;s console and watch the boot UAS / UAI Tests Initialize and Authorize the CLI Stop Using the CRAY_CREDENTIALS Service Account Token Initialize the CLI Configuration Authorize the CLI for Your User Troubleshooting CLI issues Validate UAS and UAI Functionality Validate the Basic UAS Installation Validate UAI Creation Troubleshooting Authorization Issues UAS Cannot Access Keycloak UAI Images not in Registry Missing Volumes and other Container Startup Issues CSM Install Reboot - Final NCN Install The ncn-m001 node needs to reboot from the LiveCD to normal operation as a Kubernetes master node.\nRequired Services Notice of Danger LiveCD Pre-Reboot Workarounds Hand-Off Start Hand-Off Reboot Accessing USB Partitions After Reboot Accessing CSI from a USB or RemoteISO Enable NCN Disk Wiping Safeguard CSM Validation process Double-back\u0026hellip;\nThe CSM installation validation and health checks can be run again now that ncn-m001 has been rebooted to join the Kubernetes cluster.\nNCN/Management Node Locking The NCNs should be locked to prevent accidental firmware upgrades with FAS or power down operations and reset operations with CAPMC.\nWhy? When To Lock Management NCNs When To Unlock Management NCNs Locked Behavior How To Lock Management NCNs How To Unlock Management NCNs Security Hardening Review the security hardening guide, apply non-optional procedures, and review optional procedures.\nSecurity Hardening Guide Firmware Update the system with FAS The firmware versions of many components may need to be updated at this point in the installation process.\nPrerequisites Current Capabilities as of Shasta Release v1.4 Order Of Operations Hardware Precedence Order Next Steps The details of the process are outlined in 255-FIRMWARE-ACTION-SERVICE-FAS.md using recipes listed in 256-FIRMWARE-ACTION-SERVICE-FAS-RECIPES.md\nThen the administrator should install additional products following the procedures in the HPE Cray EX System Installation and Configuration Guide S-8000.\nNaming convention for files in CSM documentation These documentation files are grouped to keep similar pages together.\n000 - 001 INTRO : Information describing this book. 002 - 008 CSM INSTALL : Install Pages for CSM 009 - 049: Other install pages 050 - 099 PROCS : Procedures referenced by install; help guides, tricks/tips, etc. 100 - 150 NCN-META : Technical information for Non-Compute Nodes 250 - 300 Common : Technical information common to all nodes. 300 - 350 MFG/SVC : Procedures referenced by service teams. 400 - 499 NETWORK : Procedures for management network installation. "
},
{
	"uri": "/docs-csm/en-09/upgrade/0.9/csm-0.9.3/",
	"title": "CSM 0.9.3 Patch Upgrade Guide",
	"tags": [],
	"description": "",
	"content": "Copyright 2021 Hewlett Packard Enterprise Development LP\nCSM 0.9.3 Patch Upgrade Guide This guide contains procedures for upgrading systems running CSM 0.9.2 to CSM 0.9.3. It is intended for system installers, system administrators, and network administrators. It assumes some familiarity with standard Linux and associated tooling.\nSee CHANGELOG.md in the root of a CSM release distribution for a summary of changes in each CSM release.\nProcedures:\nPreparation Run Validation Checks (Pre-Upgrade) Setup Nexus Update Resources Increase Max pty on Workers Deploy Manifests Upgrade NCN Packages Enable PodSecurityPolicy Apply iSCSI Security Fix Configure LAG for CMMs Run Validation Checks (Post-Upgrade) Exit Typescript Preparation For convenience, these procedures make use of environment variables. This section sets the expected environment variables to the appropriate values.\nStart a typescript to capture the commands and output from this procedure.\nncn-m001# script -af csm-update.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Set CSM_DISTDIR to the directory of the extracted release distribution for CSM 0.9.3:\nNOTE: Use --no-same-owner and --no-same-permissions options to tar when extracting a CSM release distribution as root to ensure the extracted files are owned by root and have permissions based on the current umask value.\nncn-m001# tar --no-same-owner --no-same-permissions -zxvf csm-0.9.3.tar.gz ncn-m001# CSM_DISTDIR=\u0026#34;$(pwd)/csm-0.9.3\u0026#34; Download and install/upgrade the workaround and documentation RPMs. If this machine does not have direct internet access these RPMs will need to be externally downloaded and then copied to be installed.\nncn-m001# rpm -Uvh https://storage.googleapis.com/csm-release-public/shasta-1.4/docs-csm/docs-csm-latest.noarch.rpm ncn-m001# rpm -Uvh https://storage.googleapis.com/csm-release-public/shasta-1.4/csm-install-workarounds/csm-install-workarounds-latest.noarch.rpm Set CSM_RELEASE_VERSION to the version reported by ${CSM_DISTDIR}/lib/version.sh:\nncn-m001# CSM_RELEASE_VERSION=\u0026#34;$(${CSM_DISTDIR}/lib/version.sh --version)\u0026#34; Set CSM_SYSTEM_VERSION to 0.9.2:\nncn-m001# CSM_SYSTEM_VERSION=\u0026#34;0.9.2\u0026#34; NOTE: Installed CSM versions may be listed from the product catalog using:\nncn-m001# kubectl -n services get cm cray-product-catalog -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r -j - | jq -r \u0026#39;keys[]\u0026#39; | sed \u0026#39;/-/!{s/$/_/}\u0026#39; | sort -V | sed \u0026#39;s/_$//\u0026#39; Run Validation Checks (Pre-Upgrade) It is important to first verify a healthy starting state. To do this, run the CSM validation checks. If any problems are found, correct them and verify the appropriate validation checks before proceeding.\nSetup Nexus Run lib/setup-nexus.sh to configure Nexus and upload new CSM RPM repositories, container images, and Helm charts:\nncn-m001# cd \u0026#34;$CSM_DISTDIR\u0026#34; ncn-m001# ./lib/setup-nexus.sh On success, setup-nexus.sh will output to OK on stderr and exit with status code 0, e.g.:\nncn-m001# ./lib/setup-nexus.sh ... + Nexus setup complete setup-nexus.sh: OK In the event of an error, consult the known issues from the install documentation to resolve potential problems and then try running setup-nexus.sh again. Note that subsequent runs of setup-nexus.sh may report FAIL when uploading duplicate assets. This is ok as long as setup-nexus.sh outputs setup-nexus.sh: OK and exits with status code 0.\nUpdate Resources Update the coredns and kube-multus resources.\nRun lib/0.9.3/coredns-bump-resources.sh\nncn-m001# ./lib/0.9.3/coredns-bump-resources.sh Expected output looks similar to:\nApplying new resource limits to coredns pods Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply deployment.apps/coredns configured Verify that the pods restart with status Running:\nncn-m001# watch \u0026#34;kubectl get pods -n kube-system -l k8s-app=kube-dns\u0026#34; Run lib/0.9.3/multus-bump-resources.sh\nncn-m001# ./lib/0.9.3/multus-bump-resources.sh Expected output looks similar to:\nApplying new resource limits to kube-multus pods daemonset.apps/kube-multus-ds-amd64 configured Verify that the pods restart with status Running:\nncn-m001# watch \u0026#34;kubectl get pods -n kube-system -l app=multus\u0026#34; On success, the coredns and kube-multus pods should restart with a status of Running. If any kube-multus pods remain in Terminating status, force delete them so that the daemonset can restart them successfully.\nncn-m001# kubectl delete pod \u0026lt;pod-name\u0026gt; -n kube-system --force Increase Max pty on Workers ncn-m001# pdsh -w $(./lib/list-ncns.sh | grep ncn-w | paste -sd,) \u0026#34;echo kernel.pty.max=8196 \u0026gt; /etc/sysctl.d/991-maxpty.conf \u0026amp;\u0026amp; sysctl -p /etc/sysctl.d/991-maxpty.conf\u0026#34; Deploy Manifests Before deploying the manifests, the cray-product-catalog role in Kubernetes needs to be updated.\na. Display the role before changing it:\n```bash ncn-m001# kubectl get role -n services cray-product-catalog -o json| jq '.rules[0]' ``` Expected output looks like: ``` { \u0026quot;apiGroups\u0026quot;: [ \u0026quot;\u0026quot; ], \u0026quot;resources\u0026quot;: [ \u0026quot;configmaps\u0026quot; ], \u0026quot;verbs\u0026quot;: [ \u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot; ] } ``` b. Patch the role:\n```bash ncn-m001# kubectl patch role -n services cray-product-catalog --patch \\ '{\u0026quot;rules\u0026quot;: [{\u0026quot;apiGroups\u0026quot;: [\u0026quot;\u0026quot;],\u0026quot;resources\u0026quot;: [\u0026quot;configmaps\u0026quot;],\u0026quot;verbs\u0026quot;: [\u0026quot;create\u0026quot;,\u0026quot;get\u0026quot;,\u0026quot;list\u0026quot;,\u0026quot;update\u0026quot;,\u0026quot;patch\u0026quot;,\u0026quot;delete\u0026quot;]}]}' ``` On success, expected output looks like: ``` role.rbac.authorization.k8s.io/cray-product-catalog patched ``` c. Display the role after the patch:\n```bash ncn-m001# kubectl get role -n services cray-product-catalog -o json| jq '.rules[0]' ``` Expected output looks like: ``` { \u0026quot;apiGroups\u0026quot;: [ \u0026quot;\u0026quot; ], \u0026quot;resources\u0026quot;: [ \u0026quot;configmaps\u0026quot; ], \u0026quot;verbs\u0026quot;: [ \u0026quot;create\u0026quot;, \u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;, \u0026quot;delete\u0026quot; ] } ``` Add a ClusterRoleBinding for cray-unbound-coredns PodSecurityPolicies\na. create cray-unbound-coredns-psp.yaml with the following contents\n``` --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cray-unbound-coredns-psp roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: restricted-transition-net-raw-psp subjects: - kind: ServiceAccount name: cray-dns-unbound-manager namespace: services - kind: ServiceAccount name: cray-dns-unbound-coredns namespace: services ``` b. run kubectl apply -f on cray-unbound-coredns-psp.yaml\n```bash ncn-m001# kubectl apply -f cray-unbound-coredns-psp.yaml ``` Add a ClusterRoleBinding to update the PodSecurityPolicies used by the cray-hms-rts-init job.\nCreate cray-hms-rts-init-psp.yaml with the following contents:\n--- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cray-rts-vault-watcher-psp roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: restricted-transition-net-raw-psp subjects: - kind: ServiceAccount name: cray-rts-vault-watcher namespace: services Run kubectl apply -f on cray-hms-rts-init-psp.yaml:\nncn-m001# kubectl apply -f cray-hms-rts-init-psp.yaml Run kubectl delete -n spire job spire-update-bss to allow the spire chart to be updated properly:\nncn-m001# kubectl delete -n spire job spire-update-bss Run upgrade.sh to deploy upgraded CSM applications and services:\nncn-m001# ./upgrade.sh Note: If you have not already installed the workload manager product including slurm and munge, then the cray-crus pod is expected to be in the Init state. After running upgrade.sh, you may observe there are now two copies of the cray-crus pod in the Init state. This situation is benign and should resolve itself once the workload manager product is installed.\nUpgrade NCN Packages Upgrade CSM packages on NCNs.\nGet the list of NCNs:\nncn-m001# ncns=\u0026#34;$(./lib/list-ncns.sh | paste -sd,)\u0026#34; Use zypper ms -d to disable the following zypper RIS services that configure repositories external to the system:\nBasesystem_Module_15_SP2_x86_64 Public_Cloud_Module_15_SP2_x86_64 SUSE_Linux_Enterprise_Server_15_SP2_x86_64 Server_Applications_Module_15_SP2_x86_64 ncn-m001# pdsh -w \u0026#34;$ncns\u0026#34; \u0026#39;zypper ms -d Basesystem_Module_15_SP2_x86_64\u0026#39; ncn-m001# pdsh -w \u0026#34;$ncns\u0026#34; \u0026#39;zypper ms -d Public_Cloud_Module_15_SP2_x86_64\u0026#39; ncn-m001# pdsh -w \u0026#34;$ncns\u0026#34; \u0026#39;zypper ms -d SUSE_Linux_Enterprise_Server_15_SP2_x86_64\u0026#39; ncn-m001# pdsh -w \u0026#34;$ncns\u0026#34; \u0026#39;zypper ms -d Server_Applications_Module_15_SP2_x86_64\u0026#39; NOTE: Field notice FN #6615a - Shasta V1.4 and V1.4.1 Install Issue with NCN Personalization for SMA included similar guidance as below. If these zypper services have been previously disabled, verify that they are in fact disabled:\nncn-m001# pdsh -w \u0026#34;$ncns\u0026#34; \u0026#39;zypper ls -u\u0026#39; Ensure the csm-sle-15sp2 repository is configured on every NCN:\nncn-m001# pdsh -w \u0026#34;$ncns\u0026#34; \u0026#39;zypper ar -fG https://packages.local/repository/csm-sle-15sp2/ csm-sle-15sp2\u0026#39; WARNING: If the csm-sle-15sp2 repository is already configured on a node zypper ar will error with e.g.:\nAdding repository \u0026#39;csm-sle-15sp2\u0026#39; [...error] Repository named \u0026#39;csm-sle-15sp2\u0026#39; already exists. Please use another alias. These errors may be ignored.\nUse zypper up on each NCN to upgrade installed packages:\nncn-m001# pdsh -w \u0026#34;$ncns\u0026#34; \u0026#39;zypper up -y\u0026#39; Enable PodSecurityPolicy Run ./lib/0.9.3/enable-psp.sh to enable PodSecurityPolicy:\nncn-m001# ./lib/0.9.3/enable-psp.sh Apply iSCSI Security Fix Apply the workaround for the following CVEs: CVE-2021-27365, CVE-2021-27364, CVE-2021-27363.\nThe affected kernel modules are not typically loaded on Shasta NCNs. The following prevents them from ever being loaded.\nncn-m001# pdsh -w $(./lib/list-ncns.sh | paste -sd,) \u0026#34;echo \u0026#39;install libiscsi /bin/true\u0026#39; \u0026gt;\u0026gt; /etc/modprobe.d/disabled-modules.conf\u0026#34; Configure LAG for CMMs CRITICAL: Only perform the following procedure if $CSM_RELEASE_VERSION \u0026gt;= 0.9.3.\nIMPORTANT: This procedure applies to systems with CDU switches.\nIf your Shasta system is using CDU switches you will need to update the configuration going to the CMMs.\nThis requires updated CMM firmware. (version 1.4.20) See v1.4 Admin Guide for details on updating CMM firmware A static LAG will be configured on the CDU switches. The CDU switches have two cables (10Gb RJ45) connecting to each CMM. This configuration offers increased throughput and redundancy. The CEC will not need to be programmed in order to support the LAG configuration as it was required in previous versions. The updated firmware takes care of this. Aruba CDU switch configuration. This configuration is identical across CDU VSX pairs. The VLANs used here are generated from CSI.\nsw-cdu-001(config)# int lag 2 multi-chassis static sw-cdu-001(config-lag-if)# no shutdown sw-cdu-001(config-lag-if)# description CMM_CAB_1000 sw-cdu-001(config-lag-if)# no routing sw-cdu-001(config-lag-if)# vlan trunk native 2000 sw-cdu-001(config-lag-if)# vlan trunk allowed 2000,3000,4091 sw-cdu-001(config-lag-if)# exit sw-cdu-001(config)# int 1/1/2 sw-cdu-001(config-if)# no shutdown sw-cdu-001(config-if)# lag 2 sw-cdu-001(config-if)# exit Dell Dell CDU switch configuration. This configuration is identical across CDU VLT pairs. The VLANs used here are generated from CSI.\ninterface port-channel1 description CMM_CAB_1000 no shutdown switchport mode trunk switchport access vlan 2000 switchport trunk allowed vlan 3000,4091 mtu 9216 vlt-port-channel 1 interface ethernet1/1/1 description CMM_CAB_1000 no shutdown channel-group 1 mode on no switchport mtu 9216 flowcontrol receive on flowcontrol transmit on Run Validation Checks (Post-Upgrade) IMPORTANT: Wait at least 15 minutes after upgrade.sh completes to let the various Kubernetes resources get initialized and started.\nRun the following validation checks to ensure that everything is still working properly after the upgrade:\nPlatform health checks Network health checks Other health checks may be run as desired.\nCAUTION: The following HMS functional tests may fail because of locked components in HSM:\ntest_bss_bootscript_ncn-functional_remote-functional.tavern.yaml test_smd_components_ncn-functional_remote-functional.tavern.yaml Traceback (most recent call last): File \u0026#34;/usr/lib/python3.8/site-packages/tavern/schemas/files.py\u0026#34;, line 106, in verify_generic verifier.validate() File \u0026#34;/usr/lib/python3.8/site-packages/pykwalify/core.py\u0026#34;, line 166, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/0\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/5\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/6\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/7\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/8\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/9\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/10\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/11\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/12\u0026#39;.: Path: \u0026#39;/\u0026#39;\u0026gt; Failures of these tests because of locked components as shown above can be safely ignored.\nExit Typescript Remember to exit your typescript.\nncn-m001# exit It is recommended to save the typescript file for later reference.\n"
},
{
	"uri": "/docs-csm/en-09/troubleshooting/known_issues/orphaned_cfs_pods/",
	"title": "Orphaned CFS Pods After Booting or Rebooting",
	"tags": [],
	"description": "",
	"content": "Orphaned CFS Pods After Booting or Rebooting After a boot or reboot a few CFS Pods may continue running even after they\u0026rsquo;ve finished and never go away. The state of these Pod is that the only container still running in the Pod is istio-proxy and the Pod doesn\u0026rsquo;t have a metadata.ownerReference.\nIf kubectl get pods -n services | grep cfs is run after a boot or reboot, the orphaned CFS Pods look like this:\nservices cfs-257e0f3f-f677-4a0f-908a-aede6e6cc2fb-tbwgp 1/8 NotReady 0 24m services cfs-9818f756-8486-49f1-ab7c-0bea733bdbf8-mp296 1/8 NotReady 0 24m services cfs-e8e827c2-9cf0-4a52-9257-e93e275ec394-d8d9z 1/8 NotReady 0 24m The READY field is 1/8, the STATUS is NotReady, and the Pod will stay in this state for much longer than a couple of minutes.\nHaving a few of these orphaned CFS Pods on the system doesn\u0026rsquo;t cause a problem but a large number of these could cause problems with monitoring and eventually no more Pods will be able to be scheduled by the system since there\u0026rsquo;s a limit.\nThe orphaned CFS Pods can be cleaned up manually by deleting them, for example, using the pods above run the following command:\n# kubectl delete pods -n services cfs-257e0f3f-f677-4a0f-908a-aede6e6cc2fb-tbwgp \\ cfs-9818f756-8486-49f1-ab7c-0bea733bdbf8-mp296 \\ cfs-e8e827c2-9cf0-4a52-9257-e93e275ec394-d8d9z A fix will be provided in a follow-on release such that these orphaned CFS Pods are cleaned up automatically by the system.\n"
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/change_credentials_on_servertech_pdus/",
	"title": "Change Credentials on ServerTech PDUs",
	"tags": [],
	"description": "",
	"content": "Change Credentials on ServerTech PDUs This procedure changes password used by the admn user on ServerTech PDUs. Either a single PDU can be updated to a new credential, or update all ServerTech PDUs in the system to the same global credentials.\nNOTE: This procedure does not update the default credentials that RTS uses for new ServerTech PDUs added to a system. To change the default credentials, follow the Update default ServerTech PDU Credentials used by the Redfish Translation Service procedure.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. Procedure List the ServerTech PDUs currently discovered in the system:\nncn-m001# cray hsm inventory redfishEndpoints list --type CabinetPDUController --format json | jq -r \u0026#39;.RedfishEndpoints[] | select(.FQDN | contains(\u0026#34;rts\u0026#34;)).ID\u0026#39; Sample output:\nx3000m0 Specify the existing password for the admn user:\nncn-m001# OLD_PDU_PASSWORD=secret Specify the new desired password for the admn user. The new password must between 1 and 32 characters.\nncn-m001# NEW_PDU_PASSWORD=supersecret Change password for the admn user on the ServerTech PDU.\nEither change the credentials on a single PDU or change all ServerTech PDUs to the same global default value:\nTo update the password on a single ServerTech PDU in the system:\nncn-m001# PDU=x3000m0 ncn-m001# curl -i -k -u admn:$OLD_PDU_PASSWORD -X PATCH https://$PDU/jaws/config/users/local/admn \\ -d \u0026#34;{ \\\u0026#34;password\\\u0026#34;: \\\u0026#34;$NEW_PDU_PASSWORD\\\u0026#34; }\u0026#34; Expected output upon a successful password change:\nHTTP/1.1 204 No Content Content-Type: text/html Transfer-Encoding: chunked Server: ServerTech-AWS/v8.0p Set-Cookie: C5=1883488164; path=/ Connection: close Pragma: JAWS v1.01 To update all ServerTech PDUs in the system to the same password:\nncn-m001# for PDU in $(cray hsm inventory redfishEndpoints list --type CabinetPDUController --format json | jq -r \u0026#39;.RedfishEndpoints[] | select(.FQDN | contains(\u0026#34;rts\u0026#34;)).ID\u0026#39;); do echo \u0026#34;Updating password on $PDU\u0026#34; curl -i -k -u admn:$OLD_PDU_PASSWORD -X PATCH https://$PDU/jaws/config/users/local/admn \\ -d \u0026#34;{ \\\u0026#34;password\\\u0026#34;: \\\u0026#34;$NEW_PDU_PASSWORD\\\u0026#34; }\u0026#34; done Expected output upon a successful password change:\nUpdating password on x3000m0 HTTP/1.1 204 No Content Content-Type: text/html Transfer-Encoding: chunked Server: ServerTech-AWS/v8.0p Set-Cookie: C5=1883488164; path=/ Connection: close Pragma: JAWS v1.01 Updating password on x3001m0 HTTP/1.1 204 No Content Content-Type: text/html Transfer-Encoding: chunked Server: ServerTech-AWS/v8.0p Set-Cookie: C5=1883488164; path=/ Connection: close Pragma: JAWS v1.01 NOTE: After 5 minutes the previous credential should stop working, as the existing session timed out.\nUpdate the PDU credentials stored in Vault:\nncn-m001# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-m001# alias vault=\u0026#39;kubectl -n vault exec -i cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 VAULT_FORMAT=json vault\u0026#39; Either update the credentials in Vault for a single PDU or update Vault for all ServerTech PDUs to have same global default value:\nTo update Vault for a single PDU:\nncn-m001# PDU=x3000m0 ncn-m001# vault kv get secret/pdu-creds/$PDU | jq --arg PASSWORD \u0026#34;$NEW_PDU_PASSWORD\u0026#34; \u0026#39;.data | .Password=$PASSWORD\u0026#39; | vault kv put secret/pdu-creds/$PDU - To update Vault for all ServerTech PDUs in the system to the same password:\nncn-m001# for PDU in $(cray hsm inventory redfishEndpoints list --type CabinetPDUController --format json | jq -r \u0026#39;.RedfishEndpoints[] | select(.FQDN | contains(\u0026#34;rts\u0026#34;)).ID\u0026#39;); do echo \u0026#34;Updating password on $PDU\u0026#34; vault kv get secret/pdu-creds/$PDU | jq --arg PASSWORD \u0026#34;$NEW_PDU_PASSWORD\u0026#34; \u0026#39;.data | .Password=$PASSWORD\u0026#39; | vault kv put secret/pdu-creds/$PDU - done Restart the Redfish Translation Service (RTS) to pickup the new PDU credentials:\nncn-m001# kubectl -n services rollout restart deployment cray-hms-rts ncn-m001# kubectl -n services rollout status deployment cray-hms-rts Wait for RTS to initialize itself:\nncn-m001# sleep 3m Verify RTS was able to communicate with the PDUs with the updated credentials:\nncn-m001# kubectl -n services exec -it deployment/cray-hms-rts -c cray-hms-rts-redis -- redis-cli keys \u0026#39;*/redfish/v1/Managers\u0026#39; Expected output for a system with 2 PDUs.\n1) \u0026#34;x3000m0/redfish/v1/Managers\u0026#34; 2) \u0026#34;x3001m0/redfish/v1/Managers\u0026#34; "
},
{
	"uri": "/docs-csm/en-09/operations/node_management/enable_kdump/",
	"title": "Enable Kdump",
	"tags": [],
	"description": "",
	"content": "Enable Kdump CSM 0.9.x does not have kdump enabled by default. It is necessary to run the workaround script on each NCN when rebuilding them.\nncn-m001# /opt/cray/csm/workarounds/livecd-post-reboot/CASMINST-3341/CASMINST-3341.sh "
},
{
	"uri": "/docs-csm/en-09/002-csm-install/",
	"title": "CSM Install",
	"tags": [],
	"description": "",
	"content": "CSM Install This page will prepare you for a CSM install using the LiveCD in different scenarios.\nInstall Prerequisites\nStarting an Installation\nBoot the LiveCD Install Prerequisites The prerequisites for each install scenario are defined here. All prerequisites must be met before commencing an installation.\nAfter finishing any of these prerequisite guides, an administrator may move to Starting an Installation.\nAvailable Installation Paths Prerequisites for Shasta v1.4 Installations on Shasta v1.3 Systems Prerequisites for Shasta v1.4 Installations on Bare-metal Systems Prerequisites for Reinstalling Shasta v1.4 Prerequisites for Shasta v1.4 Installations on Shasta v1.3 Systems Each item below defines a prerequisite that must be completed on systems with existing Shasta v1.3 installations. Optional steps are noted as such.\nCollect Shasta v1.4 Config Payload Quiesce Shasta v1.3 System Upgrading BIOS and Firmware Re-cabling Site Connections PCIe Connections Shut Down Management Kubernetes Cluster Powering off NCNs Collect Shasta v1.4 Config Payload Although some configuration data can be saved from a Shasta v1.3 system, there are new configuration files needed for Shasta v1.4. Some of this data is easier to collect from a running Shasta v1.3 system.\nThere may be some operational data to be saved such as any nodes which are disabled or marked down in a workload manager. These nodes might need hardware or firmware actions to repair them. If not addressed, and the newer firmware in v1.4 does not improve their performance or operation, then these may need to be disabled with v1.4 as well.\nThere may be site modifications to the system from v1.3 which are desired in v1.4. They cannot be directly copied to v1.4, however, recommendation will be made about what to save. Some saved information from v1.3 may be referenced when making a similar site modification to v1.3.\nSee the Harvest Shasta v1.3 Information page for the data harvesting procedure.\nSee the service guides for information regarding the v1.4 configuration files.\nQuiesce Shasta v1.3 System Follow site processes to quiesce the system, such as draining workload manager queues, saving user data somewhere off the system, and limiting new logins to application nodes.\nCheck for running slurm jobs\nncn-w001# ssh nid001000 squeue -l Check for running PBS jobs\nncn-w001# ssh nid001000 qstat -Q ncn-w001# ssh nid001000 qstat -a Obtain the authorization key for SAT.\nSee System Security and Authentication, Authenticate an Account with the Command Line, SAT Authentication in the Cray Shasta Administration Guide 1.3 S-8001 for more information.\nv1.3.0: Use Rev C of the guide v1.3.2: Use Rev E or later\nCheck for running sessions from BOS, CFS, CRUS, FAS, and NMD.\nncn-w001# sat bootsys shutdown --stage session-checks Expected output will look something like this:\nChecking for active BOS sessions. Found no active BOS sessions. Checking for active CFS sessions. Found no active CFS sessions. Checking for active CRUS upgrades. Found no active CRUS upgrades. Checking for active FAS actions. Found no active FAS actions. Checking for active NMD dumps. Found no active NMD dumps. No active sessions exist. It is safe to proceed with the shutdown procedure. Coordinate amongst system administration staff to prevent new sessions from starting in the services listed.\nIn CLE release 1.3, there is no method to prevent new sessions from being created as long as the service APIs are accessible on the API gateway\nShut down and power off all compute nodes and application nodes.\nv1.3.2 use the \u0026ldquo;sat bootsys shutdown\u0026rdquo; command to do compute nodes and application nodes at the same time. See the Cray Shasta Administration Guide 1.3 S-8001 RevF (or later) in the section \u0026ldquo;Shut Down and Power Off Compute and User Access Nodes\u0026rdquo;\nv1.3.0 use the \u0026ldquo;cray bos\u0026rdquo; command to create a shutdown session for the compute nodes and then for the application nodes See the Cray Shasta Administration Guide 1.3 S-8001 RevC (or later) in the section \u0026ldquo;Shut Down and Power Off Compute and User Access Nodes\u0026rdquo;\nUpgrading BIOS and Firmware from 1.3 The management NCNs are expected to have certain minimum firmware installed for BMC, node BIOS, and PCIe card firmware.\nKnown issues for Shasta v1.3 systems include:\nGigabyte nodes should use the Gigabyte Node Firmware Update Guide (1.3.2) S-8010 while booted with Shasta v1.3.2. However, since v1.3 will never be booted again on this system, there is no need to ensure that the etcd clusters are healthy and that BGP Peering has been ESTABLISHED as recommended in that guide. Nodes with Mellanox ConnectX-4 and ConnectX-5 PCIe NICs need to update their firmware. This should be done while Shasta v1.3.2 is booted. The Mellanox ConnectX-4 cards will be enabled for PXE booting later. For minimum BIOS spec (required settings), see Node BIOS Preferences.\nFor minimum NCN firmware versions see Node Firmware.\nFor minimum Network switch firmware versions see Network Firmware.\nFor minimum Network switch configurations see Management Network Install.\nWARNING Skipping this on a system that is new to Shasta v1.4 (bare-metal or previously installed with Shasta v1.3 or earlier) can result in undesirable difficulties:\nMisnamed interfaces (missing hsn0) Malfunctioning bonds (bond0) Link failures (i.e. QLogic cards set to 10Gbps fixed) Malfunctioning or disabled Multi-Chassis LAGG Back-firing work-around scripts Re-cabling The Shasta v1.3 system needs to change a few connections.\nSite Connections PCIe Connections Site Connections Installs in Shasta v1.4 are based on ncn-m001, which functions as the PIT (Pre-Install-Toolkit) node booted from the LiveCD with Shasta v1.4, instead of ncn-w001, which was the BIS node used for installation with Shasta v1.3 software. Systems are required to change their cabling to match.\nSee moving site connections to complete this step.\nPCIe Connections This is strongly encouraged to prevent overhead when adding new NCNs that the existing NCNs are re-cabled to facilitate PCIe PXE booting and \u0026ldquo;keeping NCNs the same.\u0026rdquo;\nInstalls for NCNs support PCIe PXE booting for deployment. Previous installations of Shasta v1.3 and earlier used their onboard interfaces to start PXE, before pivoting to their faster PCIe ports for Linux install. Now, everything is over the PCIe network interface card.\nSee PCIe Net-boot and Recable for information on enabling PCIe card PXE boot.\nShut Down Management Kubernetes Cluster Shut down Ceph and the Kubernetes management cluster. This performs several actions to quiesce the management services and leaves each management NCN running Linux, but no other services.\nIf Shasta v1.3.2 or later is installed, use this command.\nShutdown platform services.\nncn-w001# sat bootsys shutdown --stage platform-services If Shasta v1.3.0 or v1.3.1 is installed, follow these three steps from Cray Shasta Administration Guide 1.3 S-8001 RevC in section 6.6 Shut Down and Power Off the Management Kubernetes Cluster.\nDetermine if patch 1.3.1 has been installed. Only do step 3 if patch 1.3.1 has been installed. Modify /opt/cray/crayctl/ansible_framework/main/roles/cri-ctl/tasks/main.yml file with the following lines. Run the platform-shutdown.yml playbook. ncn-w001# ansible-playbook \\ /opt/cray/crayctl/ansible_framework/main/platform-shutdown.yml Powering off NCNs The management NCNs need to be powered off to facilitate a 1.4 install. Wiping the node will avoid boot mistakes, making the only viable option the PXE option. Below, use Ansible for wiping and shutting down the NCNs.\nSince 1.3 installs used ncn-w001 as a place to run Ansible and host Ansible inventory, we will start by jumping from the manager node to ncn-w001.\n# jumpbox ncn-m001# ssh ncn-w001 ncn-w001# Wipe disks on all nodes:\nncn-w001# ansible ncn -m shell -a \u0026#39;wipefs --all --force /dev/sd*\u0026#39; For disks which have no labels, no output will be shown by the wipefs commands being run. If one or more disks have labels, output similar to the following is expected:\n/dev/sda: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa /dev/sdb: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sdb: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sdb: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa /dev/sdc: 6 bytes were erased at offset 0x00000000 (crypto_LUKS): 4c 55 4b 53 ba be /dev/sdc: 6 bytes were erased at offset 0x00004000 (crypto_LUKS): 53 4b 55 4c ba be The thing to verify is that there are no error messages in the output.\nPower off all other nodes except ncn-w001 and ncn-m001.\nncn-w001# ansible ncn -m shell --limit=\u0026#39;!ncn-w001:!ncn-m001\u0026#39; -a \u0026#39;ipmitool power off\u0026#39; Power off ncn-w001:\nncn-w001# ipmitool power off At this time all that is left powered on is ncn-m001. The final ipmitool power off command should disconnect the administrator from ncn-w001, leaving them on ncn-m001.\nIf the connection fails to disconnect, an administrator can escape and disconnect IPMI without exiting their SSH session by pressing ~. until ipmitool disconnects.\nExit typescript from prep.install. Save collected typescript and health check information off the system so it can be referenced later\nNext: Starting an Installation\nThe system is now ready for Starting an Installation.\nPrerequisites for Shasta v1.4 Installations on Bare-metal Systems Each item below defines a prerequisite necessary for a bare-metal installation to succeed.\nNOTE On bare-metal, the LiveCD tool will assist with these steps.\nLiveCD Setup Collect Config Payload Network Configuration and Firmware Upgrading BIOS and Firmware LiveCD Setup A 1TB USB3.0 USB stick will be required in order to create a bootable LiveCD.\nThe LiveCD itself can be used out-of-the-box, and with only a little configuration it can serve for the various bare-metal prerequisite tasks.\nExperimental - See LiveCD Quick Setup for either remote ISO path, this is useful for exploring a new system quickly. Other lab users may prefer a bootable USB stick to enable persistence, and for bringing artifacts for firmware updates.\nOnce you are booted into a LiveCD, proceed onto the next prerequisite steps for bare-metal.\nCollect Config Payload New configuration files are needed for the installation of Shasta v1.4.\nSee the Service Guides for information regarding the four files.\nNetwork Configuration and Firmware To complete this step, the network configuration needs to be applied. For information on bare configurations, firmware, and more, see Management network install.\nUpgrading BIOS and Firmware The management NCNs are expected to have certain minimum firmware installed for BMC, node BIOS, and PCIe card firmware. Where possible, the firmware should be updated prior to install. Some firmware can be updated during or after the Shasta v1.4 installation, but it is better to meet the minimum NCN firmware requirement before starting.\nFor minimum BIOS spec (required settings), see Node BIOS Preferences.\nFor minimum NCN firmware versions see Node Firmware.\nFor minimum Network switch firmware versions see Network Firmware.\nFor minimum Network switch configurations see Management Network Install.\nWARNING Skipping this on a system that is new to Shasta v1.4 (bare-metal or previously installed with Shasta v1.3 or earlier) can result in undesirable difficulties:\nMisnamed interfaces (missing hsn0) Malfunctioning bonds (bond0) Link failures (i.e. QLogic cards set to 10Gbps fixed) Malfunctioning or disabled Multi-Chassis LAGG Back-firing work-around scripts Next: Starting an Installation\nThe system is now ready for Starting an Installation.\nPrerequisites for Reinstalling Shasta v1.4 The following prerequisites must be completed in order to successfully reinstall Shasta v1.4.\nStanding Kubernetes Down Prepare the Non-Compute Nodes Standing Kubernetes Down Runtime DHCP services interfere with the LiveCD\u0026rsquo;s bootstrap nature to provide DHCP leases to BMCs. To remove edge-cases, disable the run-time cray-dhcp-kea pod.\nScale the deployment from either the LiveCD or any Kubernetes node\nncn# kubectl scale -n services --replicas=0 deployment cray-dhcp-kea Prepare the Non-Compute Nodes UANs and CNs do not need to be powered off.\nThe steps below detail how to prepare the NCNs.\nDegraded System Notice If the system is degraded; CRAY services are down, or the NCNs are in inconsistent states then a cleanslate should be performed. basic wipe from Disk Cleanslate\nREQUIRED For each NCN, excluding ncn-m001, login and wipe it (this step uses the basic wipe from Disk Cleanslate):\nNOTE Pending completion of CASMINST-1659, the auto-wipe is insufficient for masters and workers. All administrators must wipe their NCNs with this step.\nWipe NCN disks from LiveCD (pit)\npit# ncns=$(grep Bond0 /etc/dnsmasq.d/statics.conf | grep -v m001 | awk -F\u0026#39;,\u0026#39; \u0026#39;{print $6}\u0026#39;) pit# for h in $ncns; do read -r -p \u0026#34;Are you sure you want to wipe the disks on $h? [y/N] \u0026#34; response response=${response,,} if [[ \u0026#34;$response\u0026#34; =~ ^(yes|y)$ ]]; then ssh $h \u0026#39;wipefs --all --force /dev/sd* /dev/disk/by-label/*\u0026#39; fi done Wipe NCN disks from ncn-m001\nncn-m001# ncns=$(grep ncn /etc/hosts | grep nmn | grep -v m001 | awk \u0026#39;{print $3}\u0026#39;) ncn-m001# for h in $ncns; do read -r -p \u0026#34;Are you sure you want to wipe the disks on $h? [y/N] \u0026#34; response response=${response,,} if [[ \u0026#34;$response\u0026#34; =~ ^(yes|y)$ ]]; then ssh $h \u0026#39;wipefs --all --force /dev/sd* /dev/disk/by-label/*\u0026#39; fi done In either case, for disks which have no labels, no output will be shown. If one or more disks have labels, output similar to the following is expected:\n... Are you sure you want to wipe the disks on ncn-m003? [y/N] y /dev/sda: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa /dev/sdb: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sdb: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sdb: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa /dev/sdc: 6 bytes were erased at offset 0x00000000 (crypto_LUKS): 4c 55 4b 53 ba be /dev/sdc: 6 bytes were erased at offset 0x00004000 (crypto_LUKS): 53 4b 55 4c ba be ... The thing to verify is that there are no error messages in the output.\nPower each NCN off using ipmitool from ncn-m001 (or the booted LiveCD if reinstalling an incomplete install).\nShutdown from LiveCD (pit)\npit# export username=root pit# export IPMI_PASSWORD=changeme pit# conman -q | grep mgmt | grep -v m001 | xargs -t -i ipmitool -I lanplus -U $username -E -H {} power off Shutdown from ncn-m001\nncn-m001# export username=root ncn-m001# export IPMI_PASSWORD=changeme ncn-m001# grep ncn /etc/hosts | grep mgmt | grep -v m001 | sort -u | awk \u0026#39;{print $2}\u0026#39; | xargs -t -i ipmitool -I lanplus -U $username -E -H {} power off 3. Set the BMCs on the systems back to DHCP.\nNOTE During the install of the NCNs their BMCs get set to static IP addresses. The installation expects the that the NCN BMCs are set back to DHCP before proceeding.\nIf you have Intel nodes run:\n# export lan=3 Otherwise run:\n# export lan=1 from the LiveCD (pit):\nNOTE This step uses the old statics.conf on the system in case CSI changes IPs:\npit# export username=root pit# export IPMI_PASSWORD=changeme pit# for h in $( grep mgmt /etc/dnsmasq.d/statics.conf | grep -v m001 | awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; ) do ipmitool -U $username -I lanplus -H $h -E lan set $lan ipsrc dhcp done The timing of this change can vary based on the hardware, so if the IP can no longer be reached after running the above command, run these commands.\npit# for h in $( grep mgmt /etc/dnsmasq.d/statics.conf | grep -v m001 | awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; ) do ipmitool -U $username -I lanplus -H $h -E lan print $lan | grep Source done pit# for h in $( grep mgmt /etc/dnsmasq.d/statics.conf | grep -v m001 | awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; ) do ipmitool -U $username -I lanplus -H $h -E mc reset cold done from ncn-m001:\nNOTE This step uses to the /etc/hosts file on ncn-m001 to determine the IP addresses of the BMCs:\nncn-m001# export username=root ncn-m001# export IPMI_PASSWORD=changeme ncn-m001# for h in $( grep ncn /etc/hosts | grep mgmt | grep -v m001 | awk \u0026#39;{print $2}\u0026#39; ) do ipmitool -U $username -I lanplus -H $h -E lan set $lan ipsrc dhcp done The timing of this change can vary based on the hardware, so if the IP can no longer be reached after running the above command, run these commands.\nncn-m001# for h in $( grep ncn /etc/hosts | grep mgmt | grep -v m001 | awk \u0026#39;{print $2}\u0026#39; ) do ipmitool -U $username -I lanplus -H $h -E lan print $lan | grep Source done ncn-m001# for h in $( grep ncn /etc/hosts | grep mgmt | grep -v m001 | awk \u0026#39;{print $2}\u0026#39; ) do ipmitool -U $username -I lanplus -H $h -E mc reset cold done Powering Off LiveCD or ncn-m001 node Skip this step if you are planning to use this node as a staging area to create the LiveCD. Lastly, shutdown the LiveCD or ncn-m001 node.\nncn-m001# poweroff With the nodes off, the system is now ready for Starting an Installation.\nStarting an Installation After finishing the prerequisites an installation can be started one of two ways.\nBoot the LiveCD All installs may be done in full from a LiveCD of any supported medium.\nFor preloading on a laptop or Linux node and inserting into a CRAY, click here for starting an installation with the (persistent bootable) CSM USB LiveCD. Experimental\nFor installing through a remote console, click here for starting an installation with the ( non-persistent bootable) CSM Remote LiveCD. NOTICE the remote ISO runs entirely in the systems volatile memory.\nFor installs using the remote mounted LiveCD (no USB stick), pay attention to memory usage as artifacts are downloaded and subsequently extracted. When RAM is limited to less than 128GB, memory pressure may occur from increasing file-system usage. For instances where memory is scarce, an NFS/CIF or HTTP/S share can be mounted in-place of the USB\u0026rsquo;s data partition at /var/www/ephemeral. Using the same mount point as the USB data partition will help ward off mistakes when following along.\n"
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/change_ex_cabinet_global_default_password/",
	"title": "Change Cray EX Cabinet Global Default Password",
	"tags": [],
	"description": "",
	"content": "Change Cray EX Cabinet Global Default Password This procedure changes the global default credential on HPE Cray EX liquid-cooled cabinet embedded controllers (BMCs). The chassis management module (CMM) controller (cC), node controller (nC), and Slingshot switch controller (sC) are generically referred to as \u0026ldquo;BMCs\u0026rdquo; in these procedures.\nPrerequisites HPE Cray EX 1.4.2 software is installed and operating. The Cray command line interface (CLI) tool is initialized and configured on the system. See \u0026ldquo;Configure the Cray Command Line Interface (CLI)\u0026rdquo; in the HPE Cray EX System Administration Guide (1.4) S-8001 for more information. Review the procedures in section 8.1 \u0026ldquo;Manage System Passwords\u0026rdquo; the HPE Cray EX System Administration Guide (1.4) S-8001. Procedure If necessary, shut down compute nodes in each cabinet. Refer to \u0026ldquo;Shut Down and Power Off Compute and User Access Nodes\u0026rdquo; the HPE Cray EX System Administration Guide (1.4) S-8001 for detailed instructions.\nncn-m001# sat bootsys shutdown --stage bos-operations \\ --bos-templates COS_SESSION_TEMPLATE Perform procedures in \u0026ldquo;Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials.\u0026rdquo;\nPerform procedures in \u0026ldquo;Updating the Liquid-Cooled EX Cabinet Default Credentials after a CEC Password Change.\u0026rdquo;\nTo update Slingshot switch BMCs, refer to \u0026ldquo;Change Rosetta Login and Redfish API Credentials\u0026rdquo; in the Slingshot Operations Guide (\u0026gt;1.6.0).\n"
},
{
	"uri": "/docs-csm/en-09/operations/node_management/reboot_ncns/",
	"title": "Reboot NCNs",
	"tags": [],
	"description": "",
	"content": "Reboot NCNs The following is a high-level overview of the non-compute node (NCN) reboot workflow:\nRun the NCN pre-reboot checks and procedures: Ensure ncn-m001 is not running in \u0026ldquo;LiveCD\u0026rdquo; or install mode Check the metal.no-wipe settings for all NCNs Enable pod priorities Run all platform health checks, including checks on the Border Gateway Protocol (BGP) peering sessions Run the rolling NCN reboot procedure: Loop through reboots on storage NCNs, worker NCNs, and master NCNs, where each boot consists of the following workflow: - Establish console session with NCN to reboot Check the hostname of the NCN to be rebooted Execute a power off/on sequence to the NCN to allow it to boot up to completion Check the hosthame of the NCN after reboot and reset it if it is not correct Execute NCN/platform health checks and do not go on to reboot the next NCN until health has been ensured on the most recently rebooted NCN Disconnect console session with the NCN that was rebooted Re-run all platform health checks, including checks on BGP peering sessions The time duration for this procedure (if health checks are being executed in between each boot, as recommended) could take between two to four hours for a system with approximately nine NCNs.\nThis same procedure can be used to reboot a single NCN node as outlined above. Be sure to carry out the NCN pre-reboot checks and procedures before and after rebooting the node. Execute the rolling NCN reboot procedure steps for the particular node type being rebooted.\nPrerequisites This procedure requires that the kubectl command is installed.\nIt also requires that the CSM_SCRIPTDIR variable is defined. Set this variable and verify that it is set. Note that if execution of this procedure is on-and-off or if multiple command windows are used, in cases where the CSM_SCRIPTDIR variable is referenced, it may be necessary to return to this step to ensure that the variable is defined.\nncn-m001# export CSM_SCRIPTDIR=/usr/share/doc/metal/scripts ncn-m001# echo $CSM_SCRIPTDIR Procedure NCN Pre-Reboot Health Checks Ensure that ncn-m001 is not running in \u0026ldquo;LiveCD\u0026rdquo; mode.\nThis mode should only be in effect during the initial product install. If the word \u0026ldquo;pit\u0026rdquo; is NOT in the hostname of ncn-m001, then it is not in the \u0026ldquo;LiveCD\u0026rdquo; mode.\nIf \u0026ldquo;pit\u0026rdquo; is in the hostname of ncn-m001, the system is not in normal operational mode and rebooting ncn-m001 may have unexpected results. This procedure assumes that the node is not running in the \u0026ldquo;LiveCD\u0026rdquo; mode that occurs during product install.\nCheck and set the metal.no-wipe setting on NCNs to ensure data on the node is preserved when rebooting.\nRefer to Check and Set the metal.no-wipe Setting on NCNs.\nRun the following script to enable a Kubernetes scheduling pod priority class for a set of critical pods. Ensure CSM_SCRIPTDIR is defined and if not, see the Prerequisites step toward the top of this procedure.\nncn-m001# echo $CSM_SCRIPTDIR ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/add_pod_priority.sh\u0026#34; After the add_pod_priority.sh script completes, wait five minutes for the changes to take effect.\nncn-m001# sleep 5m Run the platform health checks and analyze the results.\nRefer to the \u0026ldquo;Platform Health Checks\u0026rdquo; section in Validate CSM Health for an overview of the health checks.\nPlease note that though the CSM validation document references running the the HealthCheck scripts from /opt/cray/platform-utils, more recent versions of those scripts are referenced in the instructions below. Please ensure they are run from the location referenced below. Ensure CSM_SCRIPTDIR is defined and if not, see the Prerequisites st ep toward the top of this procedure.\nRun the platform health scripts from ncn-m001:\nThe output of the following scripts will need to be referenced in the remaining sub-steps.\nncn-m001# echo $CSM_SCRIPTDIR ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnHealthChecks.sh\u0026#34; ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnPostgresHealthChecks.sh\u0026#34; NOTE: If the ncnHealthChecks script output indicates any kube-multus-ds- pods are in a Terminating state, that can indicate a previous restart of these pods did not complete. In this case, it is safe to force delete these pods in order to let them properly restart by executing the kubectl delete po -n kube-system kube-multus-ds.. --force command. After executing this command, re-running the ncnHealthChecks script should indicate a new pod is in a Running state.\nCheck the status of the Kubernetes nodes.\nEnsure all Kubernetes nodes are in the Ready state.\nncn-m001# kubectl get nodes Troubleshooting: If the NCN that was rebooted is in a Not Ready state, run the following command to get more information.\nncn-m001# kubectl describe node NCN_HOSTNAME Verify the worker or master NCN is now in a Ready state:\nncn-m001# kubectl get nodes Check the status of the Kubernetes pods.\nThe bottom of the output returned after running the ${CSM_SCRIPTDIR}/ncnHealthChecks.sh script will show a list of pods that may be in a bad state. The following command can also be used to look for any pods that are not in a Running or Completed state:\nncn-m001# kubectl get pods -o wide -A | grep -Ev \u0026#39;Running|Completed\u0026#39; It is important to pay attention to that list, but it is equally important to note what pods are in that list before and after NCN reboots to determine if the reboot caused any new issues.\nThere are pods that may normally be in an Error, Not Ready, or Init state, and this may not indicate any problems caused by the NCN reboots. Error states can indicate that a job pod ran and ended in an Error. That means that there may be a problem with that job, but does not necessarily indicate that there is an overall health issue with the system. The key takeaway (for health purposes) is understanding the statuses of pods prior to doing an action like rebooting all of the NCNs. Comparing the pod statuses in between each NCN reboot will give a sense of what is new or different with respect to health.\nMonitor Ceph health continuously.\nIn a separate cli session, run the following command during NCN reboots:\nncn-m001# watch -n 10 \u0026#39;ceph -s\u0026#39; This window can be kept up throughout the reboot process to ensure Ceph remains healthy and to watch if Ceph goes into a WARN state when rebooting storage NCNs. It will be necessary to run it from an ssh session to an NCN that is not the one being rebooted.\nCheck the status of the slurmctld and slurmdbd pods to determine if they are starting:\nncn-m001# kubectl describe pod -n user -lapp=slurmctld ncn-m001# kubectl describe pod -n user -lapp=slurmdbd Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \u0026#34;314ca4285d0706ec3d76a9e953e412d4b0712da4d0cb8138162b53d807d07491\u0026#34;: Multus: Err in tearing down failed plugins: Multus: error in invoke Delegate add - \u0026#34;macvlan\u0026#34;: failed to allocate for range 0: no IP addresses available in range set: 10.252.2.4-10.252.2.4 Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox ... If the preceding error is displayed, then remove all files in the following directories on all worker nodes:\n/var/lib/cni/networks/macvlan-slurmctld-nmn-conf /var/lib/cni/networks/macvlan-slurmdbd-nmn-conf Check that the BGP peering sessions are established by using Check BGP Status and Reset Sessions.\nThis check will need to be run now and after all worker NCNs have been rebooted. Ensure that the checks have been run to check BGP peering sessions on the BGP peer switches (instructions will vary for Aruba and Mellanox switches).\nNCN Rolling Reboot Before rebooting NCNs:\nEnsure pre-reboot checks have been completed, including checking the metal.no-wipe setting for each NCN. Do not proceed if any of the NCN metal.no-wipe settings are zero. Utility Storage Nodes (Ceph) Reboot each of the NCN storage nodes one at a time going from the highest to the lowest number.\nNOTE: You are doing a single storage node at a time, so please keep track of what ncn-s0xx you are on for these steps.\nEstablish a console session to the NCN storage node that is going to be rebooted.\nUse the ${CSM_SCRIPTDIR}/ncnGetXnames.sh script to get the xnames for each of the NCNs. Ensure CSM_SCRIPTDIR is defined and if not, see the Prerequisites st ep toward the top of this procedure. ncn-m001# echo $CSM_SCRIPTDIR ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnGetXnames.sh\u0026#34; Use cray-conman to observe each node as it boots: ncn-m001# export CONMAN_POD=$(kubectl -n services get pods -l app.kubernetes.io/name=cray-conman -o json | jq -r .items[].metadata.name) ncn-m001# kubectl exec -it -n services $CONMAN_POD cray-conman -- /bin/bash cray-conman# conman -q cray-conman# conman -j XNAME NOTE: Exiting the connection to the console can be achieved with the \u0026amp;. command.\nCheck and take note of the hostname of the storage NCN by running the following command on the NCN which will be rebooted.\nncn-s# hostname Reboot the selected NCN (run this command on the NCN which needs to be rebooted).\nncn-s# shutdown -r now IMPORTANT: If the node does not shutdown after 5 mins, then proceed with the power reset below\nTo power off the node:\nncn-m001# hostname=\u0026lt;ncn being rebooted\u0026gt; # Example value: ncn-s003 ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power off ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nTo power back on the node:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power on ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as on. This may take 5-10 seconds for this to update.\nWatch on the console until the NCN has successfully booted and the login prompt is reached.\nIf the NCN fails to PXE boot, then it may be necessary to force the NCN to boot from disk.\nPower off the NCN:\nncn-m001# hostname=\u0026lt;ncn being rebooted\u0026gt; # Example value: ncn-s003 ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power off ncn-m001# sleep 10 ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Set the boot device for the next boot to disk:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus chassis bootdev disk Power on the NCN:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power on Continue to watch the console as the NCN boots.\nLogin to the storage NCN and ensure that the hostname matches what was being reported before the reboot.\nncn-s# hostname If the hostname after reboot does not match the hostname from before the reboot, the hostname will need to be reset followed by another reboot. The following command will need to be run on the cli for the NCN that has just been rebooted (and is incorrect).\nncn-s# hostnamectl set-hostname $hostname where $hostname is the original hostname from before reboot\nFollow the procedure outlined above to Reboot the selected NCN again and verify the hostname is correctly set, afterward.\nDisconnect from the console.\nRun the platform health checks from the Validate CSM Health procedure.\nRecall that updated copies of the two HealthCheck scripts referenced in the Platform Health Checks can be run, as referenced below. Ensure CSM_SCRIPTDIR is defined and if not, see the Prerequisites step toward the top of this procedure.\nncn-m001# echo $CSM_SCRIPTDIR ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnHealthChecks.sh\u0026#34; ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnPostgresHealthChecks.sh\u0026#34; Repeat all of the sub-steps above for the remaining storage nodes, going from the highest to lowest number until all storage nodes have successfully rebooted.\nImportant: Ensure ceph -s shows that Ceph is healthy (HEALTH_OK) BEFORE MOVING ON to reboot the next storage node. Once Ceph has recovered the downed mon, it may take a several minutes for Ceph to resolve clock skew.\nNCN Worker Nodes Reboot each of the NCN worker nodes one at a time going from the highest to the lowest number.\nNOTE: You are doing a single worker at a time, so please keep track of what ncn-w0xx you are on for these steps.\nFailover any postgres leader that is running on the NCN worker node you are rebooting. Ensure CSM_SCRIPTDIR is defined and if not, see the Prerequisites st ep toward the top of this procedure.\nncn-m001# echo $CSM_SCRIPTDIR ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/failover-leader.sh\u0026#34; \u0026lt;node to be rebooted\u0026gt; Cordon and Drain the node.\nncn-m001# kubectl drain --timeout=300s --ignore-daemonsets=true --delete-local-data=true \u0026lt;node to be rebooted\u0026gt; If the command above exits with similar output to the following, then the drain command ran successfully amd you can proceed to the next step.\nerror: unable to drain node \u0026#34;ncn-w003\u0026#34;, aborting command... There are pending nodes to be drained: ncn-w003 error when evicting pod \u0026#34;cray-dns-unbound-7bb85f9b5b-fjs95\u0026#34;: global timeout reached: 5m0s error when evicting pod \u0026#34;cray-dns-unbound-7bb85f9b5b-kc72b\u0026#34;: global timeout reached: 5m0s Establish a console session to the NCN worker node you are rebooting. Ensure CSM_SCRIPTDIR is defined and if not, see the Prerequisites st ep toward the top of this procedure.\nUse the ${CSM_SCRIPTDIR}/ncnGetXnames.sh script to get the xnames for each of the NCNs.\nncn-m001# echo $CSM_SCRIPTDIR ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnGetXnames.sh\u0026#34; Wait for the cray-conman pod to become healthy before continue:\nncn-m001# kubectl -n services get pods -l app.kubernetes.io/name=cray-conman NAME READY STATUS RESTARTS AGE cray-conman-7f956fc9bc-npf7d 3/3 Running 0 5d13h Use cray-conman to observe each node as it boots:\nncn-m001# export CONMAN_POD=$(kubectl -n services get pods -l app.kubernetes.io/name=cray-conman -o json | jq -r .items[].metadata.name) ncn-m001# kubectl exec -it -n services $CONMAN_POD cray-conman -- /bin/bash cray-conman# conman -q cray-conman# conman -j XNAME NOTE: Exiting the connection to the console can be achieved with the \u0026amp;. command.\nCheck and take note of the hostname of the worker NCN by running the following command on the NCN which will be rebooted.\nncn-w# hostname Reboot the selected NCN (run this command on the NCN which needs to be rebooted).\nncn-w# shutdown -r now IMPORTANT: If the node does not shutdown after 5 mins, then proceed with the power reset below\nTo power off the node:\nncn-m001# hostname=\u0026lt;ncn being rebooted\u0026gt; # Example value: ncn-w003 ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power off ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nTo power back on the node:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power on ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as on. This may take 5-10 seconds for this to update.\nWatch on the console until the NCN has successfully booted and the login prompt is reached.\nIf the NCN fails to PXE boot, then it may be necessary to force the NCN to boot from disk.\nPower off the NCN:\nncn-m001# hostname=\u0026lt;ncn being rebooted\u0026gt; # Example value: ncn-w003 ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power off ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Set the boot device for the next boot to disk:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus chassis bootdev disk Power on the NCN:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power on ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Continue to watch the console as the NCN boots.\nLogin to the worker NCN and ensure that the hostname matches what was being reported before the reboot.\nncn-w# hostname If the hostname after reboot does not match the hostname from before the reboot, the hostname will need to be reset followed by another reboot. The following command will need to be run on the cli for the NCN that has just been rebooted (and is incorrect).\nncn-w# hostnamectl set-hostname $hostname where $hostname is the original hostname from before reboot\nFollow the procedure outlined above to Reboot the selected NCN again and verify the hostname is correctly set, afterward.\nDisconnect from the console.\nUncordon the node\nncn-m# kubectl uncordon \u0026lt;node you just rebooted\u0026gt; Run the platform health checks from the Validate CSM Health procedure. The BGP Peering Status and Reset procedure can be skipped, as a different procedure in step 12 will be used to verify the BGP peering status.\nRecall that updated copies of the two HealthCheck scripts referenced in the Platform Health Checks can be run as noted below. Ensure CSM_SCRIPTDIR is defined and if not, see the Prerequisites st ep toward the top of this procedure.\nncn-m001# echo $CSM_SCRIPTDIR ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnHealthChecks.sh\u0026#34; ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnPostgresHealthChecks.sh\u0026#34; Verify that the Check if any \u0026quot;alarms\u0026quot; are set for any of the Etcd Clusters in the Services Namespace. check from the ncnHealthChecks.sh script reports no alarms set for any of the etcd pods. If an alarm similar to is reported, then wait a few minutes for the alarm to clear and try the ncnHealthChecks.sh script again.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2021-08-11T15:43:36.486Z\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;clientv3/retry_interceptor.go:62\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;retrying of unary invoker failed\u0026#34;,\u0026#34;target\u0026#34;:\u0026#34;endpoint://client-4d8f7712-2c91-4096-bbbe-fe2853cd6959/127.0.0.1:2379\u0026#34;,\u0026#34;attempt\u0026#34;:0,\u0026#34;error\u0026#34;:\u0026#34;rpc error: code = DeadlineExceeded desc = context deadline exceeded\u0026#34;} Verify that the Check the Health of the Etcd Clusters in the Services Namespace check from the ncnHealthChecks.sh script returns a healthy report for all members of each etcd cluster.\nIf pods are reported as Terminating, Init, or Pending when checking the status of the Kubernetes pods, wait for all pods to recover before proceeding.\nTroubleshooting: If the slurmctld and slurmdbd pods do not start after powering back up the node, check for the following error:\nncn-m001# kubectl describe pod -n user -lapp=slurmctld Warning FailedCreatePodSandBox 27m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \u0026#34;82c575cc978db00643b1bf84a4773c064c08dcb93dbd9741ba2e581bc7c5d545\u0026#34;: Multus: Err in tearing down failed plugins: Multus: error in invoke Delegate add - \u0026#34;macvlan\u0026#34;: failed to allocate for range 0: no IP addresses available in range set: 10.252.2.4-10.252.2.4 ncn-m001# kubectl describe pod -n user -lapp=slurmdbd Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \u0026#34;314ca4285d0706ec3d76a9e953e412d4b0712da4d0cb8138162b53d807d07491\u0026#34;: Multus: Err in tearing down failed plugins: Multus: error in invoke Delegate add - \u0026#34;macvlan\u0026#34;: failed to allocate for range 0: no IP addresses available in range set: 10.252.2.4-10.252.2.4 Remove the following files on every worker node to resolve the failure:\n/var/lib/cni/networks/macvlan-slurmctld-nmn-conf /var/lib/cni/networks/macvlan-slurmdbd-nmn-conf Ensure that BGP sessions are reset so that all BGP peering sessions with the spine switches are in an ESTABLISHED state.\nSee Check BGP Status and Reset Sessions.\nRepeat all of the sub-steps above for the remaining worker nodes, going from the highest to lowest number until all worker nodes have successfully rebooted.\nNCN Master Nodes Reboot each of the NCN master nodes one at a time except for ncn-m001 going from the highest to the lowest number.\nNOTE: You are doing a single master node at a time, so please keep track of what ncn-s0xx you are on for these steps.\nEstablish a console session to the NCN storage node that is going to be rebooted.\nUse the ${CSM_SCRIPTDIR}/ncnGetXnames.sh script to get the xnames for each of the NCNs. Ensure CSM_SCRIPTDIR is defined and if not, see the Prerequisites st ep toward the top of this procedure.\nncn-m001# echo $CSM_SCRIPTDIR ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnGetXnames.sh\u0026#34; Use cray-conman to observe each node as it boots:\nncn-m001# export CONMAN_POD=$(kubectl -n services get pods -l app.kubernetes.io/name=cray-conman -o json | jq -r .items[].metadata.name) ncn-m001# kubectl exec -it -n services $CONMAN_POD cray-conman -- /bin/bash cray-conman# conman -q cray-conman# conman -j XNAME NOTE: Exiting the connection to the console can be achieved with the \u0026amp;. command.\nCheck and take note of the hostname of the master NCN by running the command on the NCN that will be rebooted.\nncn-m# hostname Reboot the selected NCN (run this command on the NCN which needs to be rebooted).\nncn-m# shutdown -r now IMPORTANT: If the node does not shutdown after 5 mins, then proceed with the power reset below\nTo power off the node:\nncn-m001# hostname=\u0026lt;ncn being rebooted\u0026gt; # Example value: ncn-m003 ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power off ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nTo power back on the node:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power on ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as on. This may take 5-10 seconds for this to update.\nWatch on the console until the NCN has successfully booted and the login prompt is reached.\nIf the NCN fails to PXE boot, then it may be necessary to force the NCN to boot from disk.\nPower off the NCN:\nncn-m001# hostname=\u0026lt;ncn being rebooted\u0026gt; # Example value: ncn-m003 ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power off ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Set the boot device for the next boot to disk:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus chassis bootdev disk Power on the NCN:\nncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power on ncn-m001# ipmitool -U root -P PASSWORD -H ${hostname}-mgmt -I lanplus power status Continue to watch the console as the NCN boots.\nLogin to the master NCN and ensure that the hostname matches what was being reported before the reboot.\nncn-m# hostname If the hostname after reboot does not match the hostname from before the reboot, the hostname will need to be reset followed by another reboot. The following command will need to be run on the cli for the NCN that has just been rebooted (and is incorrect).\nncn-m# hostnamectl set-hostname $hostname where $hostname is the original hostname from before reboot\nFollow the procedure outlined above to Reboot the selected NCN again and verify the hostname is correctly set, afterward.\nDisconnect from the console.\nRun the platform health checks from the Validate CSM Health procedure. The BGP Peering Status and Reset procedure can be skipped, as a different procedure in step 8 will be used to verify the BGP peering status.\nRecall that updated copies of the two HealthCheck scripts referenced in the Platform Health Checks can be run as noted below. Ensure CSM_SCRIPTDIR is defined and if not, see the Prerequisites st ep toward the top of this procedure.\nncn-m001# echo $CSM_SCRIPTDIR ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnHealthChecks.sh\u0026#34; ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnPostgresHealthChecks.sh\u0026#34; Ensure that BGP sessions are reset so that all BGP peering sessions with the spine switches are in an ESTABLISHED state.\nSee Check BGP Status and Reset Sessions.\nRepeat all of the sub-steps above for the remaining master nodes (excluding ncn-m001), going from the highest to lowest number until all master nodes have successfully rebooted.\nReboot ncn-m001.\nDetermine the CAN IP address for one of the other NCNs in the system to establish an SSH session with that NCN.\nncn-m001# ssh ncn-m002 ncn-m002# ip a show vlan007 | grep inet Expected output looks similar to the following:\ninet 10.102.11.13/24 brd 10.102.11.255 scope global vlan007 inet6 fe80::1602:ecff:fed9:7820/64 scope link Now login from another machine to verify that IP is usable:\nexternal# ssh root@10.102.11.13 ncn-m002# Establish a console session to ncn-m001 from a remote system, as the BMC of ncn-m001 is the NCN that has an externally facing IP address.\nexternal# SYSTEM_NAME=eniac external# ipmitool -I lanplus -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt sol activate Check and take note of the hostname of the ncn-m001 NCN by running this command on it:\nncn-m001# hostname Reboot ncn-m001.\nncn-m001# shutdown -r now IMPORTANT: If the node does not shutdown after 5 mins, then proceed with the power reset below\nTo power off the node:\nexternal# SYSTEM_NAME=eniac external# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power off external# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power status Ensure the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nTo power back on the node:\nexternal# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power on external# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power status Ensure the power is reporting as on. This may take 5-10 seconds for this to update.\nWatch on the console until the NCN has successfully booted and the login prompt is reached.\nIf the NCN fails to PXE boot, then it may be necessary to force the NCN to boot from disk.\nPower off the NCN:\nexternal# SYSTEM_NAME=eniac external# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power off external# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power status Set the boot device for the next boot to disk:\nexternal# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanpluschassis bootdev disk Power on the NCN:\nexternal# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power on external# ipmitool -U root -P PASSWORD -H ${SYSTEM_NAME}-ncn-m001-mgmt -I lanplus power status Continue to watch the console as the NCN boots.\nLogin to ncn-m001 and ensure that the hostname matches what was being reported before the reboot.\nncn-m001# hostname If the hostname after reboot does not match the hostname from before the reboot, the hostname will need to be reset followed by another reboot. The following command will need to be run on the cli for the NCN that has just been rebooted (and is incorrect).\nncn-m001# hostname=ncn-m001 ncn-m001# hostnamectl set-hostname $hostname where $hostname is the original hostname from before reboot\nFollow the procedure outlined above to Power cycle the node again and verify the hostname is correctly set, afterward.\nDisconnect from the console.\nSet CSM_SCRIPTDIR to the scripts directory included in the docs-csm RPM and check that it is set:\nncn-m001# export CSM_SCRIPTDIR=/usr/share/doc/metal/scripts ncn-m001# echo $CSM_SCRIPTDIR Run the platform health checks from the Validate CSM Health procedure. The BGP Peering Status and Reset procedure can be skipped, as a different procedure in the next step step 10 will be used to verify the BGP peering status.\nRecall that updated copies of the two HealthCheck scripts referenced in the Platform Health Checks can be run from here:\nncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnHealthChecks.sh\u0026#34; ncn-m001# \u0026#34;${CSM_SCRIPTDIR}/ncnPostgresHealthChecks.sh\u0026#34; Ensure that BGP sessions are reset so that all BGP peering sessions with the spine switches are in an ESTABLISHED state.\nSee Check BGP Status and Reset Sessions.\n"
},
{
	"uri": "/docs-csm/en-09/003-csm-usb-livecd/",
	"title": "CSM USB LiveCD - Creation and Configuration",
	"tags": [],
	"description": "",
	"content": "CSM USB LiveCD - Creation and Configuration This page will guide an administrator through creating a USB stick from either their Shasta v1.3 ncn-m001 node or their own laptop or desktop.\nThere are 5 overall steps that provide a bootable USB with SSH enabled, capable of installing Shasta v1.4 (or higher).\nDownload and Expand the CSM Release Create the Bootable Media Configuration Payload Before Configuration Payload Workarounds Generate Installation Files CSI Workarounds SHASTA-CFG Pre-Populate LiveCD Daemons Configuration and NCN Artifacts Boot the LiveCD First Login Download and Expand the CSM Release Fetch the base installation CSM tarball and extract it, installing the contained CSI tool.\nStart a typescript to capture the commands and output from this installation. linux# script -af csm-usb-livecd.$(date +%Y-%m-%d).txt linux# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; INTERNAL USE The ENDPOINT URL below are for internal use. Customers do not need to download any additional artifacts, the CSM tarball is included along with the Shasta release.\nIf necessary, download the CSM software release to the Linux host which will be preparing the LiveCD.\nlinux# cd ~ linux# export ENDPOINT=https://arti.dev.cray.com/artifactory/shasta-distribution-stable-local/csm/ linux# export CSM_RELEASE=csm-x.y.z linux# wget ${ENDPOINT}/${CSM_RELEASE}.tar.gz Expand the CSM software release:\nIMPORTANT Before proceeding, refer to the \u0026ldquo;CSM Patch Assembly\u0026rdquo; section of the Shasta Install Guide to apply any needed patch content for CSM. It is critical to perform these steps to ensure that the correct CSM release artifacts are deployed.\nWARNING Ensure that the CSM_RELEASE environment variable is set to the version of the patched CSM release tarball. Applying the \u0026ldquo;CSM Patch Assembly\u0026rdquo; procedure will result in a different CSM version when compared to the pre-patched CSM release tarball.\nlinux# tar -zxvf ${CSM_RELEASE}.tar.gz linux# export CSM_PATH=$(pwd)/${CSM_RELEASE} linux# ls -l ${CSM_PATH} The ISO and other files are now available in the extracted CSM tar.\nRemove any previously-installed versions of CSI, the CSM install documentation, and the CSM install workarounds.\nIt is okay if this command reports that one or more of the packages are not installed.\nlinux# rpm -ev cray-site-init csm-install-workarounds docs-csm Install the CSI RPM.\nMake sure the CSM_PATH variable is set to the directory of your expanded CSM tarball.\nlinux# rpm -Uvh --force ${CSM_PATH}/rpm/cray/csm/sle-15sp2/x86_64/cray-site-init-*.x86_64.rpm Download and install the workaround and documentation RPMs. If this machine does not have direct internet access these RPMs will need to be externally downloaded and then copied to be installed.\nlinux# rpm -Uvh https://storage.googleapis.com/csm-release-public/shasta-1.4/docs-csm/docs-csm-latest.noarch.rpm linux# rpm -Uvh https://storage.googleapis.com/csm-release-public/shasta-1.4/csm-install-workarounds/csm-install-workarounds-latest.noarch.rpm Show the version of CSI installed.\nlinux# csi version Expected output looks similar to the following:\nCRAY-Site-Init build signature... Build Commit : b3ed3046a460d804eb545d21a362b3a5c7d517a3-release-shasta-1.4 Build Time : 2021-02-04T21:05:32Z Go Version : go1.14.9 Git Version : b3ed3046a460d804eb545d21a362b3a5c7d517a3 Platform : linux/amd64 App. Version : 1.5.18 Install podman or docker to support container tools required to generated sealed secrets.\nPodman RPMs are included in the embedded repository in the CSM release and may be installed in your pre-LiveCD environment using zypper as follows:\nAdd the embedded repository (if necessary):\nlinux# zypper ar -fG \u0026#34;${CSM_PATH}/rpm/embedded\u0026#34; \u0026#34;${CSM_RELEASE}-embedded\u0026#34; Install podman and podman-cni-config packages:\nlinux# zypper in --repo ${CSM_RELEASE}-embedded -y podman podman-cni-config Or you may use rpm -Uvh to install RPMs (and their dependencies) manually from the ${CSM_PATH}/rpm/embedded directory.\nInstall lsscsi to view attached storage devices.\nlsscsi RPMs are included in the embedded repository in the CSM release and may be installed in your pre-LiveCD environment using zypper as follows:\nAdd the embedded repository (if necessary):\nlinux# zypper ar -fG \u0026#34;${CSM_PATH}/rpm/embedded\u0026#34; \u0026#34;${CSM_RELEASE}-embedded\u0026#34; Install lsscsi package:\nlinux# zypper in --repo ${CSM_RELEASE}-embedded -y lsscsi Or you may use rpm -Uvh to install RPMs (and their dependencies) manually from the ${CSM_PATH}/rpm/embedded directory.\nAlthough not strictly required, the procedures for setting up the site-init directory recommend persisting site-init files in a Git repository.\nGit RPMs are included in the embedded repository in the CSM release and may be installed in your pre-LiveCD environment using zypper as follows:\nAdd the embedded repository (if necessary):\nlinux# zypper ar -fG \u0026#34;${CSM_PATH}/rpm/embedded\u0026#34; \u0026#34;${CSM_RELEASE}-embedded\u0026#34; Install git package:\nlinux# zypper in --repo ${CSM_RELEASE}-embedded -y git Or you may use rpm -Uvh to install RPMs (and their dependencies) manually from the ${CSM_PATH}/rpm/embedded directory.\nCreate the Bootable Media Cray Site Init will create the bootable LiveCD. Before creating the media, we need to identify which device that is.\nIdentify the USB device.\nThis example shows the USB device is /dev/sdd on the host.\nlinux# lsscsi Expected output looks similar to the following:\n[6:0:0:0] disk ATA SAMSUNG MZ7LH480 404Q /dev/sda [7:0:0:0] disk ATA SAMSUNG MZ7LH480 404Q /dev/sdb [8:0:0:0] disk ATA SAMSUNG MZ7LH480 404Q /dev/sdc [14:0:0:0] disk SanDisk Extreme SSD 1012 /dev/sdd [14:0:0:1] enclosu SanDisk SES Device 1012 - In the above example, we can see our internal disks as the ATA devices and our USB as the disk or enclosu device. Since the SanDisk fits the profile we are looking for, we are going to use /dev/sdd as our disk.\nSet a variable with your disk to avoid mistakes:\nlinux# export USB=/dev/sd\u0026lt;disk_letter\u0026gt; Format the USB device\nOn Linux using the CSI application:\nlinux# csi pit format $USB \u0026#34;${CSM_PATH}\u0026#34;/cray-pre-install-toolkit-*.iso 50000 On MacOS using the bash script:\nmacos# ./cray-site-init/write-livecd.sh $USB \u0026#34;${CSM_PATH}\u0026#34;/cray-pre-install-toolkit-*.iso 50000 NOTE: At this point the USB stick is usable in any server with an x86_64 architecture based CPU. The remaining steps help add the installation data and enable SSH on boot.\nMount the configuration and persistent data partition:\nlinux# mkdir -pv /mnt/{cow,pitdata} linux# mount -vL cow /mnt/cow \u0026amp;\u0026amp; mount -vL PITDATA /mnt/pitdata Copy and extract the tarball (compressed) into the USB:\nlinux# cp -rv \u0026#34;${CSM_PATH}.tar.gz\u0026#34; /mnt/pitdata/ linux# tar -zxvf \u0026#34;${CSM_PATH}.tar.gz\u0026#34; -C /mnt/pitdata/ The USB stick is now bootable and contains our artifacts. This may be useful for internal or quick usage. Administrators seeking a Shasta installation must continue onto the configuration payload.\nConfiguration Payload The SHASTA-CFG structure and other configuration files will be prepared, then csi will generate system-unique configuration payload used for the rest of the CSM installation on the USB stick.\nBefore Configuration Payload Workarounds Generate Installation Files CSI Workarounds SHASTA-CFG Before Configuration Payload Workarounds Check for workarounds in the /opt/cray/csm/workarounds/before-configuration-payload directory. If there are any workarounds in that directory, run those now. Each has its own instructions in their respective README.md files.\n# Example linux# ls /opt/cray/csm/workarounds/before-configuration-payload If there is a workaround here, the output looks similar to the following:\nCASMINST-999 Generate Installation Files Some files are needed for generating the configuration payload. New systems will need to create these files before continuing. Systems upgrading from Shasta v1.3 should prepare by gathering data from the existing system.\nNote: The USB stick is usable at this time, but without SSH enabled as well as core services. This means the stick could be used to boot the system now, and a user can return to this step at another time.\nPull these files into the current working directory:\napplication_node_config.yaml (optional - see below) cabinets.yaml (optional - see below) hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml (see below) The optional application_node_config.yaml file may be provided for further defining of settings relating to how application nodes will appear in HSM for roles and subroles. See the CSI usage for more information.\nThe optional cabinets.yaml file allows cabinet naming and numbering as well as some networking overrides (e.g. VLAN) which will allow systems on Shasta v1.3 to minimize changes to the existing system while migrating to Shasta v1.4. More information on this file can be found here.\nAfter gathering the files into the working directory, generate your configs:\nChange into the preparation directory:\nlinux# mkdir -pv /mnt/pitdata/prep linux# cd /mnt/pitdata/prep Generate the system configuration reusing a parameter file (see avoiding parameters) or skip this step.\nIf moving from a Shasta v1.3 system, the system_config.yaml file will not be available, so skip this step and continue with step 3.\nThe needed files should be in the current directory.\nlinux# ls -1 Expected output looks similar to the following:\napplication_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml Generate the system configuration.\nlinux# csi config init A new directory matching your --system-name argument will now exist in your working directory.\nSet an environment variable so this system name can be used in later commands.\nlinux# export SYSTEM_NAME=eniac Skip step 3 and continue with the CSI Workarounds\nGenerate the system configuration when a pre-existing parameter file is unavailable:\nIf moving from a Shasta v1.3 system, this step is required. If you did step 2 above, skip this step.\nThe needed files should be in the current directory. The application_node_config.yaml file is optional.\nlinux# ls -1 Expected output looks similar to the following:\napplication_node_config.yaml hmn_connections.json ncn_metadata.csv shasta_system_configs switch_metadata.csv Set an environment variable so this system name can be used in later commands.\nlinux# export SYSTEM_NAME=eniac Generate the system configuration. See below for an explanation of the command line parameters and some common settings.\nlinux# csi config init \\ --bootstrap-ncn-bmc-user root \\ --bootstrap-ncn-bmc-pass changeme \\ --system-name ${SYSTEM_NAME} \\ --mountain-cabinets 4 \\ --starting-mountain-cabinet 1000 \\ --hill-cabinets 0 \\ --river-cabinets 1 \\ --can-cidr 10.103.11.0/24 \\ --can-external-dns 10.103.11.113 \\ --can-gateway 10.103.11.1 \\ --can-static-pool 10.103.11.112/28 \\ --can-dynamic-pool 10.103.11.128/25 \\ --nmn-cidr 10.252.0.0/17 \\ --hmn-cidr 10.254.0.0/17 \\ --ntp-pool time.nist.gov \\ --site-domain dev.cray.com \\ --site-ip 172.30.53.79/20 \\ --site-gw 172.30.48.1 \\ --site-nic p1p2 \\ --site-dns 172.30.84.40 \\ --install-ncn-bond-members p1p1,p10p1 \\ --application-node-config-yaml application_node_config.yaml \\ --cabinets-yaml cabinets.yaml \\ --hmn-mtn-cidr 10.104.0.0/17 \\ --nmn-mtn-cidr 10.100.0.0/17 A new directory matching your --system-name argument will now exist in your working directory.\nAfter generating a configuration, particularly when upgrading from Shasta v1.3 a visual audit of the generated files for network data should be performed. Specifically, the /networks/HMN_MTN.yaml and /networks/NMN_MTN.yaml files should be viewed to ensure that cabinet names, subnets and VLANs have been preserved for an upgrade to Shasta v1.4. Failure of these parameters to match will likely mean a re-installation or reprogramming of CDU switches and CMM VLANs.\nRun the command \u0026ldquo;csi config init \u0026ndash;help\u0026rdquo; to get more information about the parameters mentioned in the example command above and others which are available.\nNotes about parameters to \u0026ldquo;csi config init\u0026rdquo;:\nThe application_node_config.yaml file is optional, but if you have one describing the mapping between prefixes in hmn_connections.csv that should be mapped to HSM subroles, you need to include a command line option to have it used. The bootstrap-ncn-bmc-user and bootstrap-ncn-bmc-pass must match what is used for the BMC account and its password for the management NCNs. Set site parameters (site-domain, site-ip, site-gw, site-nic, site-dns) for the information which connects the ncn-m001 (PIT) node to the site. The site-nic is the interface on this node connected to the site. If coming from Shasta v1.3, the information for all of these site parameters was collected. There are other interfaces possible, but the install-ncn-bond-members are typically: p1p1,p10p1 for HPE nodes; p1p1,p1p2 for Gigabyte nodes; and p801p1,p801p2 for Intel nodes. If coming from Shasta v1.3, this information was collected for ncn-m001. Set the three cabinet parameters (mountain-cabinets, hill-cabinets, and river-cabinets) to the number of each cabinet which are part of this system. The starting cabinet number for each type of cabinet (for example, starting-mountain-cabinet) has a default that can be overridden. See the \u0026ldquo;csi config init \u0026ndash;help\u0026rdquo; For systems that use non-sequential cabinet id numbers, use cabinets-yaml to include the cabinets.yaml file. This file can include information about the starting ID for each cabinet type and number of cabinets which have separate command line options, but is a way to explicitly specify the id of every cabinet in the system. This process is described here. An override to default cabinet IPv4 subnets can be made with the hmn-mtn-cidr and nmn-mtn-cidr parameters. These are also used to maintain existing configuration in a Shasta v1.3 system. Several parameters (can-gateway, can-cidr, can-static-pool, can-dynamic-pool) describe the CAN (Customer Access network). The can-gateway is the common gateway IP used for both spine switches and commonly referred to as the Virtual IP for the CAN. The can-cidr is the IP subnet for the CAN assigned to this system. The can-static-pool and can-dynamic-pool are the MetalLB address static and dynamic pools for the CAN. The can-external-dns is the static IP assigned to the DNS instance running in the cluster to which requests the cluster subdomain will be forwarded. The can-external-dns IP must be within the can-static-pool range. Set ntp-pool to a reachable NTP server These warnings from \u0026ldquo;csi config init\u0026rdquo; for issues in hmn_connections.json can be ignored.\nThe node with the external connection (ncn-m001) will have a warning similar to this because its BMC is connected to the site and not the HMN like the other management NCNs. It can be ignored. \u0026#34;Couldn\u0026#39;t find switch port for NCN: x3000c0s1b0\u0026#34; An unexpected component may have this message. If this component is an application node with an unusual prefix, it should be added to the application_node_config.yaml file and then rerun \u0026ldquo;csi config init\u0026rdquo;. See the procedure to create the application_node_config.yaml {\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1610405168.8705149,\u0026#34;msg\u0026#34;:\u0026#34;Found unknown source prefix! If this is expected to be an Application node, please update application_node_config.yaml\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u33\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3002\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u48\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;}} If a cooling door is found in hmn_connections.json, there may be a message like the following. It can be safely ignored. {\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1612552159.2962296,\u0026#34;msg\u0026#34;:\u0026#34;Cooling door found, but xname does not yet exist for cooling doors!\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;}} Continue with the CSI Workarounds\nCSI Workarounds Check for workarounds in the /opt/cray/csm/workarounds/csi-config directory. If there are any workarounds in that directory, run those now. Each has its own instructions in their respective README.md files.\n# Example linux# ls /opt/cray/csm/workarounds/csi-config If there is a workaround here, the output looks similar to the following:\nCASMINST-999 SHASTA-CFG Now execute the procedures in 067-SHASTA-CFG.md to prepare the site-init directory for your system. Do not skip this step.\nPre-Populate LiveCD Daemons Configuration and NCN Artifacts Now that the configuration is generated, we can populate the LiveCD with the generated files.\nThis will enable SSH, and other services when the LiveCD starts.\nSet system name and enter prep directory\nlinux# export SYSTEM_NAME=eniac linux# cd /mnt/pitdata/prep Use CSI to populate the LiveCD, provide both the mount point and the CSI generated config dir.\nlinux# csi pit populate cow /mnt/cow/ ${SYSTEM_NAME}/ Expected output looks similar to the following:\nconfig------------------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/config...OK ifcfg-bond0-------------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-bond0...OK ifcfg-lan0--------------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-lan0...OK ifcfg-vlan002-----------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-vlan002...OK ifcfg-vlan004-----------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-vlan004...OK ifcfg-vlan007-----------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-vlan007...OK ifroute-lan0------------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifroute-lan0...OK ifroute-vlan002---------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifroute-vlan002...OK CAN.conf----------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/CAN.conf...OK HMN.conf----------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/HMN.conf...OK NMN.conf----------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/NMN.conf...OK mtl.conf----------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/mtl.conf...OK statics.conf------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/statics.conf...OK conman.conf-------------------\u0026gt; /mnt/cow/rw/etc/conman.conf...OK Optionally set the hostname, print it into the hostname file.\nDo not confuse other admins and name the LiveCD \u0026ldquo;ncn-m001\u0026rdquo;, please append the \u0026ldquo;-pit\u0026rdquo; suffix which will indicate that the node is booted from the LiveCD.\nlinux# echo \u0026#34;${SYSTEM_NAME}-ncn-m001-pit\u0026#34; \u0026gt;/mnt/cow/rw/etc/hostname Unmount the Overlay, we are done with it\nlinux# umount /mnt/cow Make directories needed for basecamp (cloud-init) and the squashFS images\nlinux# mkdir -pv /mnt/pitdata/configs/ linux# mkdir -pv /mnt/pitdata/data/{k8s,ceph}/ Copy basecamp data\nlinux# csi pit populate pitdata ${SYSTEM_NAME} /mnt/pitdata/configs -b Expected output looks similar to the following:\ndata.json---------------------\u0026gt; /mnt/pitdata/configs/data.json...OK Update CA Cert on the copied data.json file. Provide the path to the data.json, the path to our customizations.yaml, and finally the sealed_secrets.key\nlinux# csi patch ca \\ --cloud-init-seed-file /mnt/pitdata/configs/data.json \\ --customizations-file /mnt/pitdata/prep/site-init/customizations.yaml \\ --sealed-secret-key-file /mnt/pitdata/prep/site-init/certs/sealed_secrets.key Copy k8s artifacts:\nMake sure the CSM_PATH variable is set to the directory of your expanded CSM tarball.\nlinux# csi pit populate pitdata \u0026#34;${CSM_PATH}/images/kubernetes/\u0026#34; /mnt/pitdata/data/k8s/ -kiK Expected output looks similar to the following:\n5.3.18-24.37-default-0.0.6.kernel-----------------\u0026gt; /mnt/pitdata/data/k8s/...OK initrd.img-0.0.6.xz-------------------------------\u0026gt; /mnt/pitdata/data/k8s/...OK kubernetes-0.0.6.squashfs-------------------------\u0026gt; /mnt/pitdata/data/k8s/...OK Copy ceph/storage artifacts:\nMake sure the CSM_PATH variable is set to the directory of your expanded CSM tarball.\nlinux# csi pit populate pitdata \u0026#34;${CSM_PATH}/images/storage-ceph/\u0026#34; /mnt/pitdata/data/ceph/ -kiC Expected output looks similar to the following:\n5.3.18-24.37-default-0.0.5.kernel-----------------\u0026gt; /mnt/pitdata/data/ceph/...OK initrd.img-0.0.5.xz-------------------------------\u0026gt; /mnt/pitdata/data/ceph/...OK storage-ceph-0.0.5.squashfs-----------------------\u0026gt; /mnt/pitdata/data/ceph/...OK Unmount the data partition:\nlinux# cd; umount /mnt/pitdata Quit the typescript session with the exit command and copy the file (csm-usb-livecd..txt) to a location on another server for reference later.\nNow the USB stick may be reattached to the CRAY, or if it was made on the CRAY then its server can now reboot into the LiveCD.\nBoot the LiveCD Some systems will boot the USB stick automatically if no other OS exists (bare-metal). Otherwise the administrator may need to use the BIOS Boot Selection menu to choose the USB stick.\nIf an administrator is rebooting a node into the LiveCD, vs booting a bare-metal or wiped node, then efibootmgr will deterministically set the boot order. See the set boot order page for more information on this topic..\nUEFI booting must be enabled to find the USB sticks EFI bootloader.\nStart a typescript on an external system, such as a laptop or Linux system, to record this section of activities done on the console of ncn-m001 via IPMI.\nexternal# script -a boot.livecd.$(date +%Y-%m-%d).txt external# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Confirm that the IPMI credentials work for the BMC by checking the power status.\nexternal# export SYSTEM_NAME=eniac external# export username=root external# export password=changeme external# ipmitool -I lanplus -U $username -P $password -H ${SYSTEM_NAME}-ncn-m001-mgmt chassis power status Connect to the IPMI console.\nexternal# ipmitool -I lanplus -U $username -P $password -H ${SYSTEM_NAME}-ncn-m001-mgmt sol activate ncn-m001# Reboot\nncn-m001# reboot Watch the shutdown and boot from the ipmitool session to the console terminal.\nAn integrity check runs before Linux starts by default, it can be skipped by selecting \u0026ldquo;OK\u0026rdquo; in its prompt.\nFirst Login On first login (over SSH or at local console) the LiveCD will prompt the administrator to change the password.\nThe initial password is empty; set the username of root and press return twice:\npit login: root Expected output looks similar to the following:\nPassword: \u0026lt;-------just press Enter here for a blank password You are required to change your password immediately (administrator enforced) Changing password for root. Current password: \u0026lt;------- press Enter here, again, for a blank password New password: \u0026lt;------- type new password Retype new password:\u0026lt;------- retype new password Welcome to the CRAY Prenstall Toolkit (LiveOS) Offline CSM documentation can be found at /usr/share/doc/metal (version: rpm -q docs-csm) NOTE If this password is forgotten, it can be reset by mounting the USB stick on another computer. See LiveCD Troubleshooting for information on clearing the password.\nDisconnect from IPMI console.\nOnce the network is up so that SSH to the node works, disconnect from the IPMI console.\nYou can disconnect from the IPMI console by using the \u0026ldquo;~.\u0026rdquo;, that is, the tilde character followed by a period character.\nLogin via ssh to the node as root.\nexternal# ssh root@${SYSTEM_NAME}-ncn-m001 pit# Note: The hostname should be similar to eniac-ncn-m001-pit when booted from the LiveCD, but it will be shown as \u0026ldquo;pit#\u0026rdquo; in the command prompts from this point onward.\nStart a typescript to record this section of activities done on ncn-m001 while booted from the LiveCD.\npit# script -af booted-csm-livecd.$(date +%Y-%m-%d).txt pit# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Download and install/upgrade the workaround and documentation RPMs. If this machine does not have direct internet access these RPMs will need to be externally downloaded and then copied to be installed.\npit# rpm -Uvh --force https://storage.googleapis.com/csm-release-public/shasta-1.4/docs-csm/docs-csm-latest.noarch.rpm pit# rpm -Uvh --force https://storage.googleapis.com/csm-release-public/shasta-1.4/csm-install-workarounds/csm-install-workarounds-latest.noarch.rpm Check the pit-release version.\npit# cat /etc/pit-release Expected output looks similar to the following:\nVERSION=1.2.2 TIMESTAMP=20210121044136 HASH=75e6c4a Mount the data partition\nThe data partition is set to fsopt=noauto to facilitate LiveCDs over virtual-ISO mount. USB installations need to mount this manually.\npit# mount -L PITDATA First login workarounds\nCheck for workarounds in the /opt/cray/csm/workarounds/first-livecd-login directory. If there are any workarounds in that directory, run those now. Each has its own instructions in their respective README.md files.\n# Example pit# ls /opt/cray/csm/workarounds/first-livecd-login If there is a workaround here, the output looks similar to the following:\nCASMINST-999 Start services\npit# systemctl start nexus pit# systemctl start basecamp pit# systemctl start conman Verify the system:\npit# csi pit validate --network pit# csi pit validate --services If dnsmasq is dead, restart it with systemctl restart dnsmasq. In addition, the final output from validating the services should have information about the nexus and basecamp containers/images similar this example.\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ff7c22c6c6cb dtr.dev.cray.com/sonatype/nexus3:3.25.0 sh -c ${SONATYPE_... 3 minutes ago Up 3 minutes ago nexus c7638b573b93 dtr.dev.cray.com/cray/metal-basecamp:1.1.0-1de4aa6 5 minutes ago Up 5 minutes ago basecamp Follow the output\u0026rsquo;s directions for failed validations before moving on.\nIf this is a Shasta v1.3.x migration scenario, then the Dell and Mellanox switches can be reconfigured now with their new names, new IP addresses, and new configuration for v1.4.\nSee Management Network Dell And Mellanox Upgrades.\nAfter successfully validating the LiveCD USB environment, the administrator may start the CSM Metal Install.\n"
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/change_ncn_image_root_password_and_ssh_keys/",
	"title": "Change NCN Image Root Password and SSH Keys",
	"tags": [],
	"description": "",
	"content": "Change NCN Image Root Password and SSH Keys The default SSH keys in the NCN image must be removed. The default password for the root user must be changed. Customize the NCN images by changing the root password or adding different SSH keys for the root account. This procedure shows this process being done any time after the first time installation of the CSM software has been completed and the PIT node is booted as a regular master node. To change the NCN image during an installation while the PIT node is booted as the PIT node, see Change NCN Image Root Password and SSH Keys PIT.\nThere is some common preparation before making the Kubernetes image for master nodes and worker nodes, making the Ceph image for utility storage nodes, and then some common cleanup afterwards.\nNote: This procedure can only be done after the PIT node is rebuilt to become a normal master node. Note: The NCNs must be rebuilt for the changes to take effect. This is covered in the last step.\nCommon preparation Kubernetes image Ceph image Common cleanup Deploy changes Common preparation Prepare new SSH keys for the root account in advance. The same key information will be added to both k8s-image and Ceph image.\nEither replace the root public and private SSH keys with your own previously generated keys or generate a new pair with ssh-keygen(1). By default ssh-keygen will create an RSA key, but other types could be chosen and different filenames would need to be substituted in later steps.\nNote: CSM only supports key pairs with empty passphrases (ssh-keygen -N\u0026quot;\u0026quot;, or enter an empty passphrase when prompted).\nncn-mw# mkdir /root/.ssh ncn-mw# ssh-keygen -f /root/.ssh/id_rsa -t rsa ncn-mw# ls -l /root/.ssh/id_rsa* ncn-mw# chmod 600 /root/.ssh/id_rsa Change to a working directory with enough space to hold the images once they have been expanded.\nncn-mw# cd /run/initramfs/overlayfs ncn-mw# mkdir workingarea ncn-mw# cd workingarea Kubernetes image The Kubernetes image k8s-image is used by the master and worker nodes.\nDecide which k8s-image to modify.\nncn-mw# cray artifacts list ncn-images --format json | jq \u0026#39;.artifacts[] .Key\u0026#39; | grep k8s | grep squashfs Example output:\n\u0026#34;k8s-filesystem.squashfs\u0026#34; \u0026#34;k8s/0.0.47/filesystem.squashfs\u0026#34; \u0026#34;k8s/0.0.46/filesystem.squashfs\u0026#34; \u0026#34;k8s/0.0.38/filesystem.squashfs\u0026#34; This example uses k8s/0.0.47 for the current version and adds a suffix for the new version.\nncn-mw# export K8SVERSION=0.0.47 ncn-mw# export K8SNEW=0.0.47-2 Make a temporary directory for the k8s-image using the current version string.\nncn-mw# mkdir -p k8s/${K8SVERSION} Get the image.\nncn-mw# cray artifacts get ncn-images k8s/${K8SVERSION}/filesystem.squashfs k8s/${K8SVERSION}/filesystem.squashfs.orig Open the image.\nncn-mw# unsquashfs -d k8s/${K8SVERSION}/filesystem.squashfs k8s/${K8SVERSION}/filesystem.squashfs.orig If the image being modified contains the default SSH keys for the root user and/or the default SSH host keys, remove them now. If the defaults were removed during initial system install or in a subsequent rotation, then this step can be safely skipped.\nncn-mw# rm -rf k8s/${K8SVERSION}/filesystem.squashfs/root/.ssh ncn-mw# rm -f k8s/${K8SVERSION}/filesystem.squashfs/etc/ssh/*key* Copy the generated public and private SSH keys for the root account into the image.\nThis example assumes that an RSA key was generated.\nncn-mw# mkdir -m 0700 k8s/${K8SVERSION}/filesystem.squashfs/root/.ssh ncn-mw# cp -p /root/.ssh/id_rsa /root/.ssh/id_rsa.pub k8s/${K8SVERSION}/filesystem.squashfs/root/.ssh Replace the public SSH key for the root account in authorized_keys.\nThis example assumes that an RSA key was generated so it adds the id_rsa.pub file to authorized_keys. It also removes any previously authorized keys. Feel free to manage this differently to retain additional keys if desired.\nncn-mw# cat /root/.ssh/id_rsa.pub \u0026gt; k8s/${K8SVERSION}/filesystem.squashfs/root/.ssh/authorized_keys ncn-mw# chmod 640 k8s/${K8SVERSION}/filesystem.squashfs/root/.ssh/authorized_keys Change into the image root.\nncn-mw# chroot k8s/${K8SVERSION}/filesystem.squashfs Change the password.\nchroot-ncn-mw# passwd (Optional) If there are any other things to be changed in the image, then they could also be done at this point.\n(Optional) Set default timezone on management nodes.\nCheck whether TZ variable is already set in /etc/environment. The setting for NEWTZ must be a valid timezone from the set under /usr/share/zoneinfo.\nchroot-ncn-mw# NEWTZ=US/Pacific chroot-ncn-mw# grep TZ /etc/environment Add only if TZ is not present.\nchroot-ncn-mw# echo TZ=${NEWTZ} \u0026gt;\u0026gt; /etc/environment Check for utc setting.\nchroot-ncn-mw# grep -i utc /srv/cray/scripts/metal/ntp-upgrade-config.sh Change only if the grep command shows these lines set to UTC.\nchroot-ncn-mw# sed -i \u0026#34;s#^timedatectl set-timezone UTC#timedatectl set-timezone $NEWTZ#\u0026#34; /srv/cray/scripts/metal/ntp-upgrade-config.sh chroot-ncn-mw# sed -i \u0026#39;s/--utc/--localtime/\u0026#39; /srv/cray/scripts/metal/ntp-upgrade-config.sh Create the new SquashFS artifact.\nchroot-ncn-mw# /srv/cray/scripts/common/create-kis-artifacts.sh Exit the chroot environment.\nchroot-ncn-mw# exit Clean up the SquashFS creation.\nncn-mw# umount -v k8s/${K8SVERSION}/filesystem.squashfs/mnt/squashfs Move new SquashFS image, kernel, and initrd into place.\nncn-mw# mkdir k8s/${K8SNEW} ncn-mw# mv -v k8s/${K8SVERSION}/filesystem.squashfs/squashfs/* k8s/${K8SNEW} Update file permissions on initrd.\nncn-mw# chmod -v 644 k8s/${K8SNEW}/initrd.img.xz Put the new squashfs, kernel, and initrd into S3.\nncn-mw# cd k8s/${K8SNEW} ncn-mw# /usr/share/doc/csm/scripts/ceph-upload-file-public-read.py --bucket-name ncn-images --key-name k8s/${K8SNEW}/filesystem.squashfs --file-name filesystem.squashfs ncn-mw# /usr/share/doc/csm/scripts/ceph-upload-file-public-read.py --bucket-name ncn-images --key-name k8s/${K8SNEW}/initrd --file-name initrd.img.xz ncn-mw# /usr/share/doc/csm/scripts/ceph-upload-file-public-read.py --bucket-name ncn-images --key-name k8s/${K8SNEW}/kernel --file-name 5.3.18-24.75-default.kernel ncn-mw# cd ../.. The Kubernetes image now has the image changes.\nUpdate BSS with the new image for the master nodes and worker nodes.\nWARNING: If doing a CSM software upgrade, then skip this section and proceed to Ceph image.\nIf not doing a CSM software upgrade, this process will update the entries in BSS for the master nodes and worker nodes to use the new k8s-image.\nSet all master nodes and worker nodes to use newly created k8s-image.\nThis will use the K8SVERSION and K8SNEW variables defined earlier.\nncn-mw# for node in $(grep -oP \u0026#34;(ncn-[mw]\\w+)\u0026#34; /etc/hosts | sort -u) do echo $node xname=$(ssh $node cat /etc/cray/xname) echo $xname cray bss bootparameters list --name $xname --format json \u0026gt; bss_$xname.json sed -i.$(date +%Y%m%d_%H%M%S%N).orig \u0026#34;s@/k8s/${K8SVERSION}\\([\\\u0026#34;/[:space:]]\\)@/k8s/${K8SNEW}\\1@g\u0026#34; bss_$xname.json kernel=$(cat bss_$xname.json | jq \u0026#39;.[] .kernel\u0026#39;) initrd=$(cat bss_$xname.json | jq \u0026#39;.[] .initrd\u0026#39;) params=$(cat bss_$xname.json | jq \u0026#39;.[] .params\u0026#39;) cray bss bootparameters update --initrd $initrd --kernel $kernel --params \u0026#34;$params\u0026#34; --hosts $xname --format json done BSS will be updated to use the new versions when /etc/cray/upgrade/csm/myenv is manually updated. See Stage 0.9 - Modify NCN Imagesfor more information.\nCeph image The Ceph image is used by the utility storage nodes.\nDecide which Ceph image to modify.\nncn-mw# cray artifacts list ncn-images --format json | jq \u0026#39;.artifacts[] .Key\u0026#39; | grep ceph | grep squashfs Example output:\n\u0026#34;ceph-filesystem.squashfs\u0026#34; \u0026#34;ceph/0.0.47/filesystem.squashfs\u0026#34; \u0026#34;ceph/0.0.46/filesystem.squashfs\u0026#34; \u0026#34;ceph/0.0.38/filesystem.squashfs\u0026#34; This example uses ceph/0.1.113 for the current version and adds a suffix for the new version.\nncn-mw# export CEPHVERSION=0.1.113 ncn-mw# export CEPHNEW=0.1.113-2 Make a temporary directory for the Ceph image using the current version string.\nncn-mw# mkdir -p ceph/${CEPHVERSION} Get the image.\nncn-mw# cray artifacts get ncn-images ceph/${CEPHVERSION}/filesystem.squashfs ceph/${CEPHVERSION}/filesystem.squashfs.orig Open the image.\nncn-mw# unsquashfs -d ceph/${CEPHVERSION}/filesystem.squashfs ceph/${CEPHVERSION}/filesystem.squashfs.orig Copy the generated public and private SSH keys for the root account into the image.\nThis example assumes that an RSA key was generated.\nncn-mw# cp -p /root/.ssh/id_rsa /root/.ssh/id_rsa.pub ceph/${CEPHVERSION}/filesystem.squashfs/root/.ssh Replace the public SSH key for the root account in authorized_keys.\nThis example assumes that an RSA key was generated so it adds the id_rsa.pub file to authorized_keys. It also removes any previously authorized keys. Feel free to manage this differently to retain additional keys if desired.\nncn-mw# cat /root/.ssh/id_rsa.pub \u0026gt; ceph/${CEPHVERSION}/filesystem.squashfs/root/.ssh/authorized_keys ncn-mw# chmod 640 ceph/${CEPHVERSION}/filesystem.squashfs/root/.ssh/authorized_keys Change into the image root.\nncn-mw# chroot ceph/${CEPHVERSION}/filesystem.squashfs Change the password.\nchroot-ncn-mw# passwd (Optional) If there are any other things to be changed in the image, then they could also be done at this point.\n(Optional) Set default timezone on management nodes.\nCheck whether TZ variable is already set in /etc/environment. The setting for NEWTZ must be a valid timezone from the set under /usr/share/zoneinfo.\nchroot-ncn-mw# NEWTZ=US/Pacific chroot-ncn-mw# grep TZ /etc/environment Add only if TZ is not present.\nchroot-ncn-mw# echo TZ=${NEWTZ} \u0026gt;\u0026gt; /etc/environment Check for utc setting.\nchroot-ncn-mw# grep -i utc /srv/cray/scripts/metal/ntp-upgrade-config.sh Change only if the grep command shows these lines set to UTC.\nchroot-ncn-mw# sed -i \u0026#34;s#^timedatectl set-timezone UTC#timedatectl set-timezone $NEWTZ#\u0026#34; /srv/cray/scripts/metal/ntp-upgrade-config.sh chroot-ncn-mw# sed -i \u0026#39;s/--utc/--localtime/\u0026#39; /srv/cray/scripts/metal/ntp-upgrade-config.sh Create the new SquashFS artifact.\nchroot-ncn-mw# /srv/cray/scripts/common/create-kis-artifacts.sh Exit the chroot environment.\nchroot-ncn-mw# exit Clean up the SquashFS creation.\nncn-mw# umount -v ceph/${CEPHVERSION}/filesystem.squashfs/mnt/squashfs Move the new SquashFS image, kernel, and initrd into place.\nncn-mw# mkdir ceph/$CEPHNEW ncn-mw# mv -v ceph/$CEPHVERSION/filesystem.squashfs/squashfs/* ceph/$CEPHNEW Update file permissions on initrd.\nncn-mw# chmod -v 644 ceph/${CEPHNEW}/initrd.img.xz Put the new initrd.img.xz, kernel, and SquashFS into S3.\nNote: The version string for the kernel file may be different.\nncn-mw# cd ceph/${CEPHNEW} ncn-mw# /usr/share/doc/csm/scripts/ceph-upload-file-public-read.py --bucket-name ncn-images --key-name ceph/${CEPHNEW}/filesystem.squashfs --file-name filesystem.squashfs ncn-mw# /usr/share/doc/csm/scripts/ceph-upload-file-public-read.py --bucket-name ncn-images --key-name ceph/${CEPHNEW}/initrd --file-name initrd.img.xz ncn-mw# /usr/share/doc/csm/scripts/ceph-upload-file-public-read.py --bucket-name ncn-images --key-name ceph/${CEPHNEW}/kernel --file-name 5.3.18-24.75-default.kernel ncn-mw# cd ../.. The Ceph image now has the image changes.\nUpdate BSS with the new image for utility storage nodes.\nWARNING: If doing a CSM software upgrade, then skip this section and proceed to Common cleanup.\nIf not doing a CSM software upgrade, this process will update the entries in BSS for the utility storage nodes to use the new Ceph image.\nSet all utility storage nodes to use newly created Ceph image.\nThis will use the CEPHVERSION and CEPHNEW variables defined earlier.\nncn-mw# for node in $(grep -oP \u0026#34;(ncn-s\\w+)\u0026#34; /etc/hosts | sort -u) do echo $node xname=$(ssh $node cat /etc/cray/xname) echo $xname cray bss bootparameters list --name $xname --format json \u0026gt; bss_$xname.json sed -i.$(date +%Y%m%d_%H%M%S%N).orig \u0026#34;s@/ceph/${CEPHVERSION}\\([\\\u0026#34;/[:space:]]\\)@/ceph/${CEPHNEW}\\1@g\u0026#34; bss_$xname.json kernel=$(cat bss_$xname.json | jq \u0026#39;.[] .kernel\u0026#39;) initrd=$(cat bss_$xname.json | jq \u0026#39;.[] .initrd\u0026#39;) params=$(cat bss_$xname.json | jq \u0026#39;.[] .params\u0026#39;) cray bss bootparameters update --initrd $initrd --kernel $kernel --params \u0026#34;$params\u0026#34; --hosts $xname --format json done Common cleanup Remove the work area so the space can be reused.\nncn-mw# rm -rf /run/initramfs/overlayfs/workingarea Deploy changes Rebuild nodes.\nWARNING: If doing a CSM software upgrade, then skip this step because the upgrade process does a rolling rebuild with some additional steps.\nIf not doing a CSM software upgrade, then follow the procedure to do a Rolling Rebuild of all management nodes.\n"
},
{
	"uri": "/docs-csm/en-09/operations/node_management/removing_a_liquid-cooled_blade_from_a_system/",
	"title": "Removing a Liquid-cooled blade from a System",
	"tags": [],
	"description": "",
	"content": "Removing a Liquid-cooled blade from a System This procedure will remove a liquid-cooled blades from a HPE Cray EX system.\nPerquisites The Cray command line interface (CLI) tool is initialized and configured on the system.\nKnowledge of whether DVS is operating over the Node Management Network (NMN) or the High Speed Network (HSN).\nThe Slingshot fabric must be configured with the desired topology for desired state of the blades in the system.\nThe System Layout Service (SLS) must have the desired HSN configuration.\nCheck the status of the high-speed network (HSN) and record link status before the procedure.\nThe blades must have the coolant drained and filled during the swap to minimize cross-contamination of cooling systems.\nReview procedures in HPE Cray EX Coolant Service Procedures H-6199 Review the HPE Cray EX Hand Pump User Guide H-6200 Procedure Prepare the source system blade for removal Using the work load manager (WLM), drain running jobs from the affected nodes on the blade. Refer to the vendor documentation for the WLM for more information.\nUse Boot Orchestration Services (BOS) to shut down the affected nodes in the source blade (in this example, x9000c3s0). Specify the appropriate xname and BOS template for the node type in the following command.\nncn-m001# BOS_TEMPLATE=cos-2.0.30-slurm-healthy-compute ncn-m001# cray bos session create --template-uuid $BOS_TEMPLATE --operation shutdown --limit x9000c3s0b0n0,x9000c3s0b0n1,x9000c3s0b1n0,x9000c3s0b1n1 Disable the Redfish endpoints for the nodes Temporarily disable the Redfish endpoints for NodeBMCs present in the blade.\nncn-m001# cray hsm inventory redfishEndpoints update --enabled false x9000c3s0b0 ncn-m001# cray hsm inventory redfishEndpoints update --enabled false x9000c3s0b1 Clear the node controller settings Remove the system specific settings from each node controller on the blade.\nncn-m001# curl -k -u root:PASSWORD -X POST -H \\ \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;ResetType\u0026#34;:\u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://x9000c3s0b0/redfish/v1/Managers/BMC/Actions/Manager.Reset ncn-m001# curl -k -u root:PASSWORD -X POST -H \\ \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;ResetType\u0026#34;:\u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://x9000c3s0b1/redfish/v1/Managers/BMC/Actions/Manager.Reset Use Ctrl-C to return to the prompt if command does not return.\nPower off the chassis slot Suspend the hms-discovery cron job.\nncn-m001# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : true }}\u0026#39; Verify that the hms-discovery cron job has stopped (ACTIVE = 0 and SUSPEND = True).\nncn-m001# kubectl get cronjobs -n services hms-discovery NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * True 0 117s 15d Power off the chassis slot. This examples powers off slot 0, chassis 3, in cabinet 9000.\nncn-m001# cray capmc xname_off create --xnames x9000c3s0 --recursive true Disable the chassis slot Disable the chassis slot. Disabling the slot prevents hms-discovery from automatically powering on the slot. This example disables slot 0, chassis 3, in cabinet 9000.\nncn-m001# cray hsm state components enabled update --enabled false x9000c3s0 Record MAC and IP addresses for nodes IMPORTANT: Record the node management network (NMN) MAC and IP addresses for each node in the blade (labeled Node Maintenance Network). To prevent disruption in the data virtualization service (DVS) when over operating the NMN, these addresses must be maintained in the HSM when the blade is swapped and discovered.\nThe NodeBMC MAC and IP addresses are assigned algorithmically and must not be deleted from the HSM.\nSkip this step if DVS is operating over the HSN, otherwise proceed with this step. Query HSM to determine the ComponentID, MAC, and IP addresses for each node in the blade. The prerequisites show an example of how to gather HSM values and store them to a file.\nncn-m001# cray hsm inventory ethernetInterfaces list --component-id x9000c3s0b0n0 --format json [ { \u0026#34;ID\u0026#34;: \u0026#34;0040a6836339\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:63:39\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-04-09T21:51:04.662063Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x9000c3s0b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;IPAddresses\u0026#34;: [ { \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.10\u0026#34; } ] } ] Record the following values for the blade:\n`ComponentID: \u0026#34;x9000c3s0b0n0\u0026#34;` `MACAddress: \u0026#34;00:40:a6:83:63:39\u0026#34;` `IPAddress: \u0026#34;10.100.0.10\u0026#34;` Repeat the command to record the ComponentID, MAC, and IP addresses for the Node Maintenance Network the other nodes in the blade.\nCleanup Hardware State Manager Set environment corresponding the chassis slot of the blade.\nncn-m001# export CHASSIS_SLOT=x9000c3s0 Delete the Redfish endpoints for each node.\nncn-m001# for xname in $(cray hsm inventory redfishEndpoints list --format json | jq -r --arg CHASSIS_SLOT $CHASSIS_SLOT \u0026#39;.RedfishEndpoints[] | select(.ID | startswith($CHASSIS_SLOT)) | .ID\u0026#39;); do echo \u0026#34;Removing $xname from HSM Inventory RedfishEndpoints\u0026#34; cray hsm inventory redfishEndpoints delete \u0026#34;$xname\u0026#34; done Remove entries from state components.\nfor xname in $(cray hsm state components list --class Mountain --format json | jq -r --arg CHASSIS_SLOT $CHASSIS_SLOT \u0026#39;.Components[] | select((.ID | startswith($CHASSIS_SLOT)) and (.ID != $CHASSIS_SLOT)) | .ID\u0026#39; ); do echo \u0026#34;Removing $xname from HSM State components\u0026#34; cray hsm state components delete \u0026#34;$xname\u0026#34; done Delete the NMN MAC and IP addresses each node in the blade from the HSM. Do not delete the MAC and IP addresses for the node BMC.\nfor mac in $(cray hsm inventory ethernetInterfaces list --type Node --format json | jq -r --arg CHASSIS_SLOT $CHASSIS_SLOT \u0026#39;.[] | select(.ComponentID | startswith($CHASSIS_SLOT)) | .ID\u0026#39;); do echo \u0026#34;Removing $mac from HSM Inventory EthernetInterfaces\u0026#34; cray hsm inventory ethernetInterfaces delete \u0026#34;$mac\u0026#34; done Restart KEA.\nncn-m001# kubectl delete pods -n services -l app.kubernetes.io/name=cray-dhcp-kea Remove the blade Remove the blade from the source location. - Review the Remove a Compute Blade Using the Lift procedure in HPE Cray EX Hardware Replacement Procedures H-6173 for detailed instructions for replacing liquid-cooled blades (https://internal.support.hpe.com/).\nDrain the coolant from the blade and fill with fresh coolant to minimize cross-contamination of cooling systems. - Review HPE Cray EX Coolant Service Procedures H-6199. If using the hand pump, review procedures in the HPE Cray EX Hand Pump User Guide H-6200 (https://internal.support.hpe.com/).\nInstall the blade from the source system in a storage rack or leave it on the cart.\nUn-suspend the hms-discovery cron job if no more liquid-cooled blades are planned to be removed from the system.\nncn-m001# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : false }}\u0026#39; Verify that the hms-discovery cron job has stopped (ACTIVE = 0 and SUSPEND = False).\nncn-m001# kubectl get cronjobs -n services hms-discovery NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * False 1 46s 15d "
},
{
	"uri": "/docs-csm/en-09/004-csm-remote-livecd/",
	"title": "LiveCD Remote ISO Install",
	"tags": [],
	"description": "",
	"content": "LiveCD Remote ISO Install This page will assist you with configuring and activating your booted LiveCD through a remote KVM.\nLiveCD Setup LiveCD Interfaces Setup the Site-Link Connection(s) Setup Internal Connections Download and Install the Workaround and Documentation RPMs Customization Hostname SHASTA-CFG Cray Site Init CA Certificate Validate the LiveCD platform. LiveCD Services Configure NTP Validate the LiveCD Services Verify Outside Name Resolution Attaching the ISO to the node varies by the vendor:\nHPE uses iLO Gigabyte Intel For information on how-to remote attach an ISO, see LiveCD ISO Boot.\nLiveCD Setup LiveCD Interfaces Set up variables for lan0 configuration\npit# site_ip=172.30.XXX.YYY/20 pit# site_gw=172.30.48.1 pit# site_dns=172.30.84.40 pit# site_nic=p1p2 site_nic The interface that is directly attached to the site network on ncn-m001. This should not be lan0. site_ip The IP address and netmask in CIDR notation that is assigned to the site connection on ncn-m001. NOTE: This is NOT just the network, but also the IP address. site_gw The gateway address for the site network. This will be used to set up the default gateway route on ncn-m001. site_dns ONE of the site DNS servers. The script does not currently handle setting more than one IP address here. Setup Site-Link Connection(s) External, direct access.\nPREFERRED use the generated files from your system inputs\u0026hellip;\npit# system_name=bigbird pit# cp /var/www/ephemeral/prep/${system_name}/cpt-files/ /etc/sysconfig/network/ pit# wicked ifreload lan0 pit# /root/bin/csi-set-hostname.sh MANUAL without CPT files generated by CSI\u0026hellip;\npit# /root/bin/csi-setup-lan0.sh $site_ip $site_gw $site_dns $site_nic pit# /root/bin/csi-set-hostname.sh If there is an IP showing for ip a s lan0 then you could exit your CONSOLE and return with an SSH connection (if you prefer).\nSetup Internal Connections Now reload the other configurations:\nPREFERRED use the generated files from your system inputs:\npit# wicked ifreload all MANUAL without CPT files generated by CSI: NOTE: Be sure to set the nmn_cidr, hmn_cidr, and can_cidr variables first.\npit# /root/bin/csi-setup-vlan002.sh $nmn_cidr pit# /root/bin/csi-setup-vlan004.sh $hmn_cidr pit# /root/bin/csi-setup-vlan007.sh $can_cidr Download and Install the Workaround and Documentation RPMs If this machine does not have direct internet access these RPMs will need to be externally downloaded and then copied to be installed.\npit# rpm -Uvh https://storage.googleapis.com/csm-release-public/shasta-1.4/docs-csm/docs-csm-latest.noarch.rpm pit# rpm -Uvh https://storage.googleapis.com/csm-release-public/shasta-1.4/csm-install-workarounds/csm-install-workarounds-latest.noarch.rpm Customization Hostname To prevent mistakes, naming the LiveCD can be a useful visual aide.\nNOTE do not confuse other administrators by neglecting the \u0026ldquo;-pit\u0026rdquo; suffix.\nSet the hostname with hostnamectl:\npit# hostnamectl set-hostname bigbird-ncn-m001-pit Shasta-CFG Follow the procedures in 067-SHASTA-CFG.md to prepare the site-init directory for your system.\nCray-Site-Init For csi usage and options, please see csi --help output\nCA Certificate Update CA Cert on the copied data.json file. Provide the path to the data.json, the path to our customizations.yaml, and finally the sealed_secrets.key\npit# csi patch ca \\ --cloud-init-seed-file /var/www/ephemeral/configs/data.json \\ --customizations-file /var/www/ephemeral/prep/site-init/customizations.yaml \\ --sealed-secret-key-file /var/www/ephemeral/prep/site-init/certs/sealed_secrets.key Validate the LiveCD platform. Check that IPs are set for each interface:\npit# csi pit validate --network LiveCD Services Move onto Configure NTP.\nCopy the config files generated earlier by csi config init into /etc/dnsmasq.d and /etc/conman.conf.\npit# cp /var/www/ephemeral/prep/${system_name}/dnsmasq.d/* /etc/dnsmasq.d pit# cp /var/www/ephemeral/prep/${system_name}/conman.conf /etc/conman.conf pit# cp /var/www/ephemeral/prep/${system_name}/basecamp/* /var/www/ephemeral/configs/ pit# systemctl restart conman pit# systemctl restart dnsmasq pit# systemctl start basecamp pit# systemctl start nexus Configure NTP Start and configure NTP on the LiveCD for a fallback/recovery server:\npit# /root/bin/configure-ntp.sh Validate the LiveCD Services Now verify service health:\ndnsmasq, basecamp, and nexus should report HEALTHY and running. No podman container(s) should be dead. pit# csi pit validate --services If basecamp is dead, restart it with systemctl restart basecamp. If dnsmasq is dead, restart it with systemctl restart dnsmasq. If nexus is dead, restart it with systemctl restart nexus. You should see two containers: nexus and basecamp\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 496a2ce806d8 dtr.dev.cray.com/metal/cloud-basecamp:latest 4 days ago Up 4 days ago basecamp 6fcdf2bfb58f docker.io/sonatype/nexus3:3.25.0 sh -c ${SONATYPE_... 4 days ago Up 4 days ago nexus Verify Outside Name Resolution SKIP IF AIRGAP/OFFLINE - offline installs should skip this check entirely.\nVerify you can ping quad9, or Google\u0026rsquo;s, or your IT/site\u0026rsquo;s DNS servers:\npit# ping 9.9.9.9 pit# ping 8.8.8.8 Now is a good time to also verify your local site docker registry, and RPM repository connectivity as well.\nNow you can now pass GO, collect $200, and begin the CSM Metal Install page\u0026hellip;\n"
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/change_ncn_image_root_password_and_ssh_keys_on_pit_node/",
	"title": "Change NCN Image Root Password and SSH Keys on PIT Node",
	"tags": [],
	"description": "",
	"content": "Change NCN Image Root Password and SSH Keys on PIT Node The default SSH keys in the NCN image must be removed. The default password for the root user must be changed. Customize the NCN images by changing the root password and adding different SSH keys for the root account. This procedure shows this process being done on the PIT node during a first time installation of the CSM software.\nThere is some common preparation before making the Kubernetes image for master nodes and worker nodes, making the Ceph image for utility storage nodes, and then some common cleanup afterwards.\nNote: This procedure can only be done before the PIT node is rebuilt to become a normal master node.\nCommon Preparation Prepare new SSH keys on the PIT node for the root account in advance. The same key information will be added to both k8s-image and ceph-image.\nEither replace the root public and private SSH keys with your own previously generated keys or generate a new pair with ssh-keygen(1). By default ssh-keygen will create an RSA key, but other types could be chosen and different filenames would need to be substituted in later steps.\nNote: CSM only supports key pairs with empty passphrases (ssh-keygen -N\u0026quot;\u0026quot;, or enter an empty passphrase when prompted).\npit# mkdir /root/.ssh pit# ssh-keygen -f /root/.ssh/id_rsa -t rsa pit# ls -l /root/.ssh/id_rsa* pit# chmod 600 /root/.ssh/id_rsa Kubernetes Image The Kubernetes image is used by the master and worker nodes.\nChange to the working directory for the Kubernetes image.\npit# cd /var/www/ephemeral/data/k8s Open the image.\nThe Kubernetes image will be of the form kubernetes-0.0.57.squashfs in /var/www/ephemeral/data/k8s, but the version number may be different.\npit# unsquashfs kubernetes-0.0.57.squashfs Remove default SSH keys\npit# rm -rf squashfs-root/root/.ssh pit# rm -f /etc/ssh/*key* Copy the generated public and private SSH keys for the root account into the image.\nThis example assumes that an RSA key was generated.\npit# mkdir -m 0700 squashfs-root/root/.ssh pit# cp -p /root/.ssh/id_rsa /root/.ssh/id_rsa.pub squashfs-root/root/.ssh Add the public SSH key for the root account to authorized_keys.\nThis example assumes that an RSA key was generated so it adds the id_rsa.pub file to authorized_keys. Note that authorized_keys is being overwritten, not appended.\npit# cat /root/.ssh/id_rsa.pub \u0026gt; squashfs-root/root/.ssh/authorized_keys pit# chmod 640 squashfs-root/root/.ssh/authorized_keys Change into the image root.\npit# chroot ./squashfs-root Change the password.\nchroot-pit# passwd (Optional) If there are any other things to be changed in the image, they could also be done at this point.\n(Optional) Set default timezone on management nodes.\nCheck whether TZ variable is already set in /etc/environment. The setting for NEWTZ must be a valid timezone from the set under /usr/share/zoneinfo.\nchroot-pit# NEWTZ=US/Pacific chroot-pit# grep TZ /etc/environment Add only if TZ is not present.\nchroot-pit# echo TZ=${NEWTZ} \u0026gt;\u0026gt; /etc/environment Check for utc setting.\nchroot-pit# grep -i utc /srv/cray/scripts/metal/ntp-upgrade-config.sh Change only if the grep command shows these lines set to UTC.\nchroot-pit# sed -i \u0026#34;s#^timedatectl set-timezone UTC#timedatectl set-timezone $NEWTZ#\u0026#34; /srv/cray/scripts/metal/ntp-upgrade-config.sh chroot-pit# sed -i \u0026#39;s/--utc/--localtime/\u0026#39; /srv/cray/scripts/metal/ntp-upgrade-config.sh Create the new SquashFS artifact.\nchroot-pit# /srv/cray/scripts/common/create-kis-artifacts.sh Exit the chroot environment.\nchroot-pit# exit Clean up the SquashFS creation.\nThe Kubernetes image directory is /var/www/ephemeral/data/k8s.\npit# umount -v /var/www/ephemeral/data/k8s/squashfs-root/mnt/squashfs Move new SquashFS image, kernel, and initrd into place.\npit# mv -v squashfs-root/squashfs/* . Update file permissions on initrd.\npit# chmod -v 644 initrd.img.xz Rename the new SquashFS, kernel, and initrd to include a new version string.\nIf the old name of the SquashFS was kubernetes-0.0.57.squashfs, then its version was \u0026lsquo;0.0.57\u0026rsquo;, so the newly created version should be renamed to include a version of \u0026lsquo;0.0.57-1\u0026rsquo; with an additional dash and a build iteration number of 1. This will help to track what base version was used.\npit# ls -l old/*squashfs -rw-r--r-- 1 root root 5135859712 Aug 19 19:10 kubernetes-0.0.57.squashfs Set the VERSION variable based on the version string displayed by the above command with an incremented suffix added to show a build iteration.\npit# export VERSION=0.0.57-1 pit# mv filesystem.squashfs kubernetes-${VERSION}.squashfs pit# mv initrd.img.xz initrd.img-${VERSION}.xz The kernel file will have a name with the kernel version but not this new $VERSION.\npit# ls -l *kernel -rw-r--r-- 1 root root 8552768 Aug 19 19:09 5.3.18-24.75-default.kernel Rename it to include the version string.\npit# mv 5.3.18-24.75-default.kernel 5.3.18-24.75-default-${VERSION}.kernel Set the boot links. Skip this step if proceeding to the Ceph Image section below.\npit# cd pit# set-sqfs-links.sh The Kubernetes image will have the image changes for the next boot.\nCeph Image The Ceph image is used by the utility storage nodes.\nChange to the working directory for the Ceph image.\npit# cd /var/www/ephemeral/data/ceph Open the image.\nThe Ceph image will be of the form storage-ceph-0.0.47.squashfs in /var/www/ephemeral/data/ceph, but the version number may be different.\npit# unsquashfs storage-ceph-0.0.47.squashfs Save the old SquashFS image, kernel, and initrd.\npit# mkdir -v old pit# mv -v *squashfs *kernel initrd* old Copy the generated public and private SSH keys for the root account into the image.\nThis example assumes that an RSA key was generated.\npit# cp -p /root/.ssh/id_rsa /root/.ssh/id_rsa.pub squashfs-root/root/.ssh Add the public SSH key for the root account to authorized_keys.\nThis example assumes that an RSA key was generated so it adds the id_rsa.pub file to authorized_keys.\nNote that authorized_keys is being overwritten, not appended.\npit# cat /root/.ssh/id_rsa.pub \u0026gt; squashfs-root/root/.ssh/authorized_keys pit# chmod 640 squashfs-root/root/.ssh/authorized_keys Change into the image root.\npit# chroot ./squashfs-root Change the password.\nchroot-pit# passwd (Optional) If there are any other things to be changed in the image, they could also be done at this point.\n(Optional) Set default timezone on management nodes.\nCheck whether TZ variable is already set in /etc/environment. The setting for NEWTZ must be a valid timezone from the set under /usr/share/zoneinfo.\nchroot-pit# NEWTZ=US/Pacific chroot-pit# grep TZ /etc/environment Add only if TZ is not present.\nchroot-pit# echo TZ=${NEWTZ} \u0026gt;\u0026gt; /etc/environment Check for utc setting.\nchroot-pit# grep -i utc /srv/cray/scripts/metal/ntp-upgrade-config.sh Change only if the grep command shows these lines set to UTC.\nchroot-pit# sed -i \u0026#34;s#^timedatectl set-timezone UTC#timedatectl set-timezone $NEWTZ#\u0026#34; /srv/cray/scripts/metal/ntp-upgrade-config.sh chroot-pit# sed -i \u0026#39;s/--utc/--localtime/\u0026#39; /srv/cray/scripts/metal/ntp-upgrade-config.sh Create the new SquashFS artifact.\nchroot-pit# /srv/cray/scripts/common/create-kis-artifacts.sh Exit the chroot environment.\nchroot-pit# exit Clean up the SquashFS creation.\nThe Ceph image directory is /var/www/ephemeral/data/ceph.\npit# umount -v /var/www/ephemeral/data/ceph/squashfs-root/mnt/squashfs Save old SquashFS image.\npit# mkdir -v old pit# mv -v *squashfs old Move new SquashFS image, kernel, and initrd into place.\npit# mv -v squashfs-root/squashfs/* . Update file permissions on initrd.\npit# chmod -v 644 initrd.img.xz Rename the new SquashFS, kernel, and initrd to include a new version string.\nIf the old name of the SquashFS was storage-ceph-0.0.47.squashfs, then its version was \u0026lsquo;0.0.47\u0026rsquo;, so the newly created version should be renamed to include a version of \u0026lsquo;0.0.47-1\u0026rsquo; with an additional dash and a build iteration number of 1. This will help to track what base version was used.\npit# ls -l old/*squashfs -rw-r--r-- 1 root root 5135859712 Aug 19 19:10 storage-ceph-0.0.47.squashfs Set the VERSION variable based on the version string displayed by the above command with an incremented suffix added to show a build iteration.\npit# VERSION=0.0.47-1 pit# mv filesystem.squashfs storage-ceph-${VERSION}.squashfs pit# mv initrd.img.xz initrd.img-${VERSION}.xz The kernel file will have a name with the kernel version but not this new $VERSION.\npit# ls -l *kernel -rw-r--r-- 1 root root 8552768 Aug 19 19:09 5.3.18-24.75-default.kernel Rename it to include the version string.\npit# mv 5.3.18-24.75-default.kernel 5.3.18-24.75-default-${VERSION}.kernel Set the boot links.\npit# cd pit# set-sqfs-links.sh The Ceph image will have the image changes for the next boot.\nCommon Cleanup Clean up temporary storage used to prepare images.\nThese could be removed now or after verification that the nodes are able to boot successfully with the new images.\npit# cd /var/www/ephemeral/data pit# rm -rf ceph/old k8s/old "
},
{
	"uri": "/docs-csm/en-09/005-csm-metal-install/",
	"title": "CSM Metal Install",
	"tags": [],
	"description": "",
	"content": "CSM Metal Install WARNING: Gigabyte NCNs running firmware version C20 can become unusable when Shasta 1.4 is installed. This is a result of a bug in the Gigabyte firmware that ships with Shasta 1.4. This bug has not been observed in firmware version C17.\nA key symptom of this bug is that the NCN will not PXE boot and will instead fall through to the boot menu, despite being configure to PXE boot. This behavior will persist until the failing node\u0026rsquo;s CMOS is cleared.\nA procedure is available in 254-NCN-FIRMWARE-GB.md.\nThis document specifies the procedures for deploying the non-compute nodes (NCNs).\nConfigure Bootstrap Registry to Proxy an Upstream Registry Tokens and IPMI Password Timing of Deployments NCN Deployment Apply NCN Pre-Boot Workarounds Ensure Time Is Accurate Before Deploying NCNs Start Deployment Workflow Deploy Check for Unused Drives on Utility Storage Nodes Apply NCN Post-Boot Workarounds LiveCD Cluster Authentication BGP Routing Validation Manual LVM Check Procedure Optional Validation Configure and Trim UEFI Entries Configure Bootstrap Registry to Proxy an Upstream Registry INTERNAL USE \u0026ndash; This section is only relevant for Cray/HPE internal systems.\nSKIP IF AIRGAP/OFFLINE - Do NOT reconfigure the bootstrap registry to proxy an upstream registry if performing an airgap/offline install.\nBy default, the bootstrap registry is a type: hosted Nexus repository to support airgap/offline installs, which requires container images to be imported prior to platform installation. However, it may be reconfigured to proxy container images from an upstream registry in order to support online installs as follows:\nStop Nexus:\npit# systemctl stop nexus Remove nexus container:\npit# podman container exists nexus \u0026amp;\u0026amp; podman container rm nexus Remove nexus-data volume:\npit# podman volume rm nexus-data Add the corresponding URL to the ExecStartPost script in /usr/lib/systemd/system/nexus.service.\nINTERNAL USE Cray internal systems may want to proxy to https://dtr.dev.cray.com as follows:\npit# URL=https://dtr.dev.cray.com pit# sed -e \u0026#34;s,^\\(ExecStartPost=/usr/sbin/nexus-setup.sh\\).*$,\\1 $URL,\u0026#34; -i /usr/lib/systemd/system/nexus.service Restart Nexus\npit# systemctl daemon-reload pit# systemctl start nexus Tokens and IPMI Password These tokens will assist an administrator as they follow this page. Copy these into the shell environment Notice that one of them is the IPMI_PASSWORD\nThese exist as an avoidance measure for hard-codes, so these may be used in various system contexts.\npit# \\ export mtoken=\u0026#39;ncn-m(?!001)\\w+-mgmt\u0026#39; export stoken=\u0026#39;ncn-s\\w+-mgmt\u0026#39; export wtoken=\u0026#39;ncn-w\\w+-mgmt\u0026#39; export username=root # Replace \u0026#34;changeme\u0026#34; with the real root password. export IPMI_PASSWORD=changeme Throughout the guide, simple one-liners can be used to query status of expected nodes. If the shell or environment is terminated, these environment variables should be re-exported.\nExamples:\n# Power status of all expected NCNs: pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | xargs -t -i ipmitool -I lanplus -U $username -E -H {} power status # Power off all expected NCNs: pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | xargs -t -i ipmitool -I lanplus -U $username -E -H {} power off Timing of Deployments The timing of each set of boots varies based on hardware, some manufacturers will POST faster than others or vary based on BIOS setting. After powering a set of nodes on, an administrator can expect a healthy boot-session to take about 60 minutes depending on the number of storage and worker nodes.\nNCN Deployment This section will walk an administrator through NCN deployment.\nGrab the Tokens to facilitate commands if loading this page from a bookmark.\nApply NCN Pre-Boot Workarounds There will be post-boot workarounds as well.\nCheck for workarounds in the /opt/cray/csm/workarounds/before-ncn-boot directory within the CSM tar. If there are any workarounds in that directory, run those now. Each has its own instructions in their respective README.md files.\n# Example pit# ls /opt/cray/csm/workarounds/before-ncn-boot If there is a workaround here, the output looks similar to the following:\nCASMINST-980 Ensure Time Is Accurate Before Deploying NCNs NOTE: If you wish to use a timezone other than UTC, instead of step 1 below, follow this procedure for setting a local timezone, then proceed to step 2.\nEnsure that the PIT node has the current and correct time.\nThis step should not be skipped\nCheck the current time to see if it matches the current time:\npit# date \u0026#34;+%Y-%m-%d %H:%M:%S.%6N%z\u0026#34; The time can be inaccurate if the system has been off for a long time, or, for example, the CMOS was cleared. If needed, set the time manually as close as possible.\npit# timedatectl set-time \u0026#34;2019-11-15 00:00:00\u0026#34; Then finally run the NTP script:\npit# /root/bin/configure-ntp.sh This ensures that the PIT is configured with an accurate date/time, which will be properly propagated to the NCNs during boot.\nEnsure the current time is set in BIOS for all management NCNs.\nIf each NCN is booted to the BIOS menu, you can check and set the current UTC time.\npit# export username=root pit# export IPMI_PASSWORD=changeme Repeat the following process for each NCN.\nStart an IPMI console session to the NCN.\npit# bmc=ncn-w001-mgmt # Change this to be each node in turn. pit# conman -j $bmc In another terminal boot the node to BIOS.\npit# bmc=ncn-w001-mgmt # Change this to be each node in turn. pit# ipmitool -I lanplus -U $username -E -H $bmc chassis bootdev bios pit# ipmitool -I lanplus -U $username -E -H $bmc chassis power off pit# sleep 10 pit# ipmitool -I lanplus -U $username -E -H $bmc chassis power on For HPE NCNs the above process will boot the nodes to their BIOS, but the menu is unavailable through conman as the node is booted into a graphical BIOS menu.\nTo access the serial version of the BIOS setup. Perform the ipmitool steps above to boot the node. Then in conman press ESC+9 key combination to when you see the following messages in the console, this will open you to a menu that can be used to enter the BIOS via conman.\nFor access via BIOS Serial Console: Press \u0026#39;ESC+9\u0026#39; for System Utilities Press \u0026#39;ESC+0\u0026#39; for Intelligent Provisioning Press \u0026#39;ESC+!\u0026#39; for One-Time Boot Menu Press \u0026#39;ESC+@\u0026#39; for Network Boot For HPE NCNs the date configuration menu can be found at the following path: System Configuration -\u0026gt; BIOS/Platform Configuration (RBSU) -\u0026gt; Date and Time\nAlternatively for HPE NCNs you can login to the BMC\u0026rsquo;s web interface and access the HTML5 console for the node to interact with the graphical BIOS. From the administrators own machine create a SSH tunnel (-L creates the tunnel, and -N prevents a shell and stubs the connection):\n# Change this to be each node in turn. linux# bmc=ncn-w001-mgmt linux# ssh -L 9443:$bmc:443 -N root@eniac-ncn-m001 Opening a web browser to https://localhost:9443 will give access to the BMC\u0026rsquo;s web interface.\nWhen the node boots, you will be able to use the conman session to see the BIOS menu to check and set the time to current UTC time. The process varies depending on the vendor of the NCN.\nRepeat this process for each NCN.\nStart Deployment Deployment of the nodes starts with booting the storage nodes first, then the master nodes and worker nodes together. After the operating system boots on each node there are some configuration actions which take place. Watching the console or the console log for certain nodes can help to understand what happens and when. When the process is complete for all nodes, the Ceph storage will have been initialized and the Kubernetes cluster will be created ready for a workload.\nWorkflow The configuration workflow described here is intended to help understand the expected path for booting and configuring. See the actual steps below for the commands to deploy these management NCNs.\nStart watching the consoles for ncn-s001 and at least one other storage node Boot all storage nodes at the same time The first storage node ncn-s001 will boot and then starts a loop as ceph-ansible configuration waits for all other storage nodes to boot The other storage nodes boot and become passive. They will be fully configured when ceph-ansible runs to completion on ncn-s001 Once ncn-s001 notices that all other storage nodes have booted, ceph-ansible will begin Ceph configuration. This takes several minutes. Once ceph-ansible has finished on ncn-s001, then ncn-s001 waits for ncn-m002 to create /etc/kubernetes/admin.conf. Start watching the consoles for ncn-m002, ncn-m003 and at least one worker node Boot master nodes (ncn-m002 and ncn-m003) and all worker nodes at the same time The worker nodes will boot and wait for ncn-m002 to create the /etc/cray/kubernetes/join-command-control-plane so they can join Kubernetes The third master node ncn-m003 boots and waits for ncn-m002 to create the /etc/cray/kubernetes/join-command-control-plane so it can join Kubernetes The second master node ncn-m002 boots, runs the kubernetes-cloudinit.sh which will create /etc/kubernetes/admin.conf and /etc/cray/kubernetes/join-command-control-plan, then waits for the storage node to create etcd-backup-s3-credentials Once ncn-s001 notices that ncn-m002 has created /etc/kubernetes/admin.conf, then ncn-s001 waits for any worker node to become available. Once each worker node notices that ncn-m002 has created /etc/cray/kubernetes/join-command-control-plan, then it will join the Kubernetes cluster. Now ncn-s001 should notice this from any one of the worker nodes and move forward with creation of config maps and running the post-ceph playbooks (s3, OSD pools, quotas, etc.) Once ncn-s001 creates etcd-backup-s3-credentials during the benji-backups role which is one of the last roles after Ceph has been set up, then ncn-m001 notices this and moves forward Deploy Change the default root password and SSH keys\nIf you want to avoid using the default install root password and SSH keys for the NCNs, follow the NCN image customization steps in 110 NCN Image Customization.\nThis step is strongly encouraged for external/site deployments.\nCreate boot directories for any NCN in DNS:\nThis will create folders for each host in /var/www, allowing each host to have their own unique set of artifacts; kernel, initrd, SquashFS, and script.ipxe bootscript.\npit# /root/bin/set-sqfs-links.sh Customize boot scripts for any out-of-baseline NCNs\nkubernetes-workers with more than 2 small disks need to make adjustments to prevent bare-metal etcd creation A brief overview of what is expected is here, in disk plan of record / baseline Set each node to always UEFI Network Boot, and ensure they are powered off\npit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | xargs -t -i ipmitool -I lanplus -U $username -E -H {} chassis bootdev pxe options=efiboot,persistent pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | xargs -t -i ipmitool -I lanplus -U $username -E -H {} power off Note: some BMCs will \u0026ldquo;flake\u0026rdquo; and ignore the bootorder setting by ipmitool. As a fallback, cloud-init will correct the bootorder after NCNs complete their first boot. The first boot may need manual effort to set the boot order over the conman console. The NCN boot order is further explained in 101 NCN Booting.\nValidate that the LiveCD is ready for installing NCNs Observe the output of the checks and note any failures, then remediate them.\npit# csi pit validate --livecd-preflight Note: If your shell terminal is not echoing your input after running this, type \u0026ldquo;reset\u0026rdquo; and press enter to recover.\nNote: If you are not on an internal Cray/HPE system, or if you are on an offline/airgapped system, then you can ignore any errors about not being able resolve arti.dev.cray.com\nPrint the consoles available to you:\npit# conman -q Expected output looks similar to the following:\nncn-m001-mgmt ncn-m002-mgmt ncn-m003-mgmt ncn-s001-mgmt ncn-s002-mgmt ncn-s003-mgmt ncn-w001-mgmt ncn-w002-mgmt ncn-w003-mgmt IMPORTANT This is the administrators last chance to run NCN pre-boot workarounds.\nNOTE: All consoles are located at /var/log/conman/console*\nBoot the Storage Nodes\npit# grep -oP $stoken /etc/dnsmasq.d/statics.conf | xargs -t -i ipmitool -I lanplus -U $username -E -H {} power on Wait. Observe the installation through ncn-s001-mgmt\u0026rsquo;s console:\n# Print the console name pit# conman -q | grep s001 Expected output looks similar to the following:\nncn-s001-mgmt Then join the console:\n# Join the console pit# conman -j ncn-s001-mgmt From there an administrator can witness console-output for the cloud-init scripts.\nNOTE: Watch the storage node consoles carefully for error messages. If any are seen, consult 066-CEPH-CSI\nNOTE: If the nodes have pxe boot issues (e.g. getting pxe errors, not pulling the ipxe.efi binary) see PXE boot troubleshooting\nNOTE: If other issues arise, such as cloud-init (e.g. NCNs come up to linux with no hostname) see the CSM workarounds for fixes around mutual symptoms.\n# Example pit# ls /opt/cray/csm/workarounds/after-ncn-boot If there is a workaround here, the output looks similar to the following:\nCASMINST-1093 Once all storage nodes are up and ncn-s001 is running ceph-ansible, boot Kubernetes Managers and Workers\npit# grep -oP \u0026#34;($mtoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | xargs -t -i ipmitool -I lanplus -U $username -E -H {} power on Wait. Observe the installation through ncn-m002-mgmt\u0026rsquo;s console:\n# Print the console name pit# conman -q | grep m002 Expected output looks similar to the following:\nncn-m002-mgmt Then join the console:\n# Join the console pit# conman -j ncn-m002-mgmt NOTE: If the nodes have pxe boot issues (e.g. getting pxe errors, not pulling the ipxe.efi binary) see PXE boot troubleshooting\nNOTE: If one of the manager nodes seems hung waiting for the storage nodes to create a secret, check the storage node consoles for error messages. If any are found, consult 066-CEPH-CSI\nNOTE: If other issues arise, such as cloud-init (e.g. NCNs come up to linux with no hostname) see the CSM workarounds for fixes around mutual symptoms.\n# Example pit# ls /opt/cray/csm/workarounds/after-ncn-boot If there is a workaround here, the output looks similar to the following:\nCASMINST-1093 Refer to timing of deployments. It should not take more than 60 minutes for the kubectl get nodes command to return output indicating that all the managers and workers aside from the PIT node booted from the LiveCD are Ready:\npit# ssh ncn-m002 ncn-m002# kubectl get nodes -o wide Expected output looks similar to the following:\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ncn-m002 Ready master 14m v1.18.6 10.252.1.5 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.43-default containerd://1.3.4 ncn-m003 Ready master 13m v1.18.6 10.252.1.6 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.43-default containerd://1.3.4 ncn-w001 Ready \u0026lt;none\u0026gt; 6m30s v1.18.6 10.252.1.7 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.43-default containerd://1.3.4 ncn-w002 Ready \u0026lt;none\u0026gt; 6m16s v1.18.6 10.252.1.8 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.43-default containerd://1.3.4 ncn-w003 Ready \u0026lt;none\u0026gt; 5m58s v1.18.6 10.252.1.12 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.43-default containerd://1.3.4 Check for Unused Drives on Utility Storage Nodes IMPORTANT: Do the following if NCNs use Gigabyte hardware.\nLog into each ncn-s node and check for unused drives\nncn-s# ceph-volume inventory The field \u0026ldquo;available\u0026rdquo; would be true if Ceph sees the drive as empty and can be used, e.g.:\nDevice Path Size rotates available Model name /dev/sda 447.13 GB False False SAMSUNG MZ7LH480 /dev/sdb 447.13 GB False False SAMSUNG MZ7LH480 /dev/sdc 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sdd 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sde 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sdf 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sdg 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sdh 3.49 TB False False SAMSUNG MZ7LH3T8 Alternatively, just dump the paths of available drives:\nncn-s# ceph-volume inventory --format json-pretty | jq -r \u0026#39;.[]|select(.available==true)|.path\u0026#39; Add unused drives\nncn-s# ceph-volume lvm create --data /dev/sd\u0026lt;drive to add\u0026gt; --bluestore Apply NCN Post-Boot Workarounds Check for workarounds in the /opt/cray/csm/workarounds/after-ncn-boot directory. If there are any workarounds in that directory, run those now. Instructions are in the README files.\n# Example pit# ls /opt/cray/csm/workarounds/after-ncn-boot If there is a workaround here, the output looks similar to the following:\nCASMINST-12345 LiveCD Cluster Authentication The LiveCD needs to authenticate with the cluster to facilitate the rest of the CSM installation.\nCopy the Kubernetes config to the LiveCD to be able to use kubectl as cluster administrator.\nThis will always be whatever node is the first-master-hostname in your /var/www/ephemeral/configs/data.json | jq file. If you are provisioning your CRAY from ncn-m001 then you can expect to fetch these from ncn-m002.\npit# mkdir ~/.kube pit# scp ncn-m002.nmn:/etc/kubernetes/admin.conf ~/.kube/config BGP Routing After the NCNs are booted, the BGP peers will need to be checked and updated if the neighbor IPs are incorrect on the switches. See the doc to Check and Update BGP Neighbors.\nNote: If migrating from Shasta v1.3.x, the worker nodes have different IP addresses, so the scripts below must be run to correct the spine switch configuration to the Shasta v1.4 IP addresses for the worker nodes.\nMake sure you clear the BGP sessions here.\nAruba:clear bgp * Mellanox: enable then clear ip bgp all NOTE: At this point all but possibly one of the peering sessions with the BGP neighbors should be in IDLE or CONNECT state and not ESTABLISHED state. If the switch is an Aruba, you will have one peering session established with the other switch. You should check that all of the neighbor IPs are correct.\nIf needed, the following helper scripts are available for the various switch types:\npit# ls -1 /usr/bin/*peer*py Expected output looks similar to the following:\n/usr/bin/aruba_set_bgp_peers.py /usr/bin/mellanox_set_bgp_peers.py Validation The following commands will run a series of remote tests on the other nodes to validate they are healthy and configured correctly.\nObserve the output of the checks and note any failures, then remediate them.\nCheck Ceph\npit# csi pit validate --ceph Note: Throughout the output there are multiple lines of test totals; be sure to check all of them and not just the final one.\nNote: Please refer to the Utility Storage section of the Admin guide to help resolve any failed tests.\nCheck Kubernetes\npit# csi pit validate --k8s WARNING If test failures for /dev/sdc are observed, the Manual LVM Check Procedure must be carried out to determine if they are true failures.\nEnsure that weave has not split-brained\nRun the following command on each member of the Kubernetes cluster (master nodes and worker nodes) to ensure that weave is operating as a single cluster:\nncn# weave --local status connections | grep failed If you see messages like \u0026lsquo;IP allocation was seeded by different peers\u0026rsquo; then weave looks to have split-brained. At this point it is necessary to wipe the ncns and start the pxe boot again:\nWipe the ncns using the \u0026lsquo;Basic Wipe\u0026rsquo; section of DISK CLEANSLATE. Return to the \u0026lsquo;Boot the Storage Nodes\u0026rsquo; step of Start Deployment section above. Manual LVM Check Procedure If an automated test reports a failure relating to /dev/sdc on a master or worker NCN, this manual procedure must be followed to determine whether or not there is a real error.\nTo manually validate the ephemeral disks on a master node, run the following command:\nncn-m# blkid -L ETCDLVM To manually validate the ephemeral disks on a worker node, run the following commands:\nncn-w# blkid -L CONLIB ncn-w# blkid -L CONRUN ncn-w# blkid -L K8SLET The validation is considered successful if each of the commands returns the name of any device (e.g. /dev/sdd, /dev/sdb1, etc). The name of the device does not matter \u0026ndash; each command just needs to output the name of some device.\nIf any nodes fail the validation, then the problem must be resolved before continuing with the install.\nIf any master node has the problem, then you must wipe and redeploy all of the NCNs before continuing the installation:\nWipe each worker node using the \u0026lsquo;Basic Wipe\u0026rsquo; section of DISK CLEANSLATE. Wipe each master node (except ncn-m001 because it is the PIT node) using the \u0026lsquo;Basic Wipe\u0026rsquo; section of DISK CLEANSLATE. Wipe each storage node using the \u0026lsquo;Full Wipe\u0026rsquo; section of DISK CLEANSLATE. Return to the Set each node to always UEFI Network Boot, and ensure they are powered off step of the Deploy section above. If only worker nodes have the problem, then you must wipe and redeploy the affected worker nodes before continuing the installation:\nWipe each affected worker node using the \u0026lsquo;Basic Wipe\u0026rsquo; section of DISK CLEANSLATE. Power off each affected worker node. Return to the Boot the Master and Worker Nodes step of the Deploy section above. Note: The ipmitool command will give errors trying to power on the unaffected nodes, since they are already powered on \u0026ndash; this is expected and not a problem. Optional Validation These tests are for sanity checking. These exist as software reaches maturity, or as tests are worked and added into the installation repertoire.\nAll validation should be taken care of by the CSI validate commands. The following checks can be done for sanity-checking:\nImportant common issues should be checked by tests, new pains in these areas should entail requests for new tests.\nVerify all nodes have joined the cluster Verify etcd is running outside Kubernetes on master nodes Verify that all the pods in the kube-system namespace are running Verify that the ceph-csi requirements are in place (see CEPH CSI) Configure and Trim UEFI Entries Do the following two steps outlined in Fixing Boot-Order for all NCNs except the PIT node.\nSetting Order Trimming Now move to the CSM Platform Install page to continue the CSM install.\n"
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/change_smnp_credentials_on_leaf_switches/",
	"title": "Change SMNP Credentials on Leaf Switches",
	"tags": [],
	"description": "",
	"content": "Change SMNP Credentials on Leaf Switches This procedure changes the SNMP credentials on management leaf switches in the system. Either a single leaf switch can be updated to use new SNMP credentials, or update all leaf switches in the system to use the same global SNMP credentials.\nNOTE: This procedure will not update the default SNMP credentials used when new leaf switches are added to the system. To update the default SNMP credentials for new hardware, follow the Update Default Air-Cooled BMC and Leaf Switch SNMP Credentials procedure.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. Procedure List Leaf switches in system:\nncn-m001# cray sls search hardware list --type comptype_mgmt_switch --format json | jq -r \u0026#39;[\u0026#34;Xname\u0026#34;, \u0026#34;Brand\u0026#34;, \u0026#34;Alias\u0026#34;], (.[] | [.Xname, .ExtraProperties.Brand, .ExtraProperties.Aliases[0]]) | @tsv\u0026#39; | column -t Sample output for a system with 1 Aruba leaf switch:\nXname Brand Alias x3000c0w14 Aruba sw-leaf-001 Sample output for a system with 2 Dell leaf switches:\nXname Brand Alias x3000c0w14 Dell sw-leaf-002 x3000c0w13 Dell sw-leaf-001 Update SNMP credentials for the testuser user on each leaf switch in the system. The SNMP testuser user requires 2 password to be provided for the SNMP Authentication and Privacy protocol passwords. Both of these passwords must be 8 characters or longer. In the examples below, foobar01 is the new SNMP Authentication password, and foobar02 is the new SNMP Privacy password.\nConfigure the Aruba leaf switch:\nncn-m001# ssh admin@sw-leaf-001 sw-leaf-001# configure terminal sw-leaf-001(config)# snmpv3 user testuser auth md5 auth-pass plaintext foobar01 priv des priv-pass plaintext foobar02 sw-leaf-001(config)# exit sw-leaf-001# write memory sw-leaf-001# exit Configure the Dell leaf switch:\nncn-m001# ssh admin@sw-leaf-001 sw-leaf-001# configure terminal sw-leaf-001(config)# snmp-server user testuser cray-reds-group 3 auth md5 foobar01 priv des foobar02 sw-leaf-001(config)# exit sw-leaf-001# write memory sw-leaf-001# exit Update Vault with new SNMP credentials:\nncn-m001# SNMP_AUTH_PASS=foobar01 ncn-m001# SNMP_PRIV_PASS=foobar02 ncn-m001# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-m001# alias vault=\u0026#39;kubectl -n vault exec -i cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 VAULT_FORMAT=json vault\u0026#39; Either update the credentials in Vault for a single leaf switch or update Vault for all leaf switches to have same global default value:\nTo update Vault for a single leaf switch:\nncn-m001# XNAME=x3000c0w22 ncn-m001# vault kv get secret/hms-creds/$XNAME | jq --arg SNMP_AUTH_PASS \u0026#34;$SNMP_AUTH_PASS\u0026#34; --arg SNMP_PRIV_PASS \u0026#34;$SNMP_PRIV_PASS\u0026#34; \\ \u0026#39;.data | .SNMPAuthPass=$SNMP_AUTH_PASS | .SNMPPrivPass=$SNMP_PRIV_PASS\u0026#39; | vault kv put secret/hms-creds/$XNAME - To update Vault for all leaf switches in the system to the same password:\nfor XNAME in $(cray sls search hardware list --type comptype_mgmt_switch --format json | jq -r .[].Xname); do echo \u0026#34;Updating SNMP creds for $XNAME\u0026#34; vault kv get secret/hms-creds/$XNAME | jq --arg SNMP_AUTH_PASS \u0026#34;$SNMP_AUTH_PASS\u0026#34; --arg SNMP_PRIV_PASS \u0026#34;$SNMP_PRIV_PASS\u0026#34; \\ \u0026#39;.data | .SNMPAuthPass=$SNMP_AUTH_PASS | .SNMPPrivPass=$SNMP_PRIV_PASS\u0026#39; | vault kv put secret/hms-creds/$XNAME - done Restart the River Endpoint Discovery Service (REDS) to pickup the new SNMP credentials:\nncn-m001# kubectl -n services rollout restart deployment cray-reds ncn-m001# kubectl -n services rollout status deployment cray-reds Wait for REDS to initialize itself:\nncn-m001# sleep 2m Verify REDS was able to communicate with the leaf switches with the updated credentials:\nDetermine the name of the REDS pods:\nncn-m001# kubectl -n services get pods -l app.kubernetes.io/name=cray-reds NAME READY STATUS RESTARTS AGE cray-reds-6b99b9d5dc-c5g2t 2/2 Running 0 3m21s Check the logs of the REDS pod for SNMP communication issues. Replace CRAY_REDS_POD_NAME with the currently running pod for REDS:\nncn-m001# kubectl -n services logs CRAY_REDS_POD_NAME cray-reds | grep \u0026#34;Failed to get ifIndex\u0026lt;-\u0026gt;name map\u0026#34; If nothing is returned, then REDS is able to successfully communicate to the leaf switches in the system via SNMP.\nErrors like the following occur when SNMP credentials in Vault to not match what is configured on the leaf switch.\n2021/10/26 20:03:21 WARNING: Failed to get ifIndex\u0026lt;-\u0026gt;name map (1.3.6.1.2.1.31.1.1.1.1) for x3000c0w22: Received a report from the agent - UsmStatsWrongDigests(1.3.6.1.6.3.15.1.1.5.0) "
},
{
	"uri": "/docs-csm/en-09/006-csm-platform-install/",
	"title": "CSM Platform Install",
	"tags": [],
	"description": "",
	"content": "CSM Platform Install This page will go over how to install CSM applications and services (i.e., into the CSM Kubernetes cluster).\nInitialize Bootstrap Registry\nCreate Site-Init Secret\nDeploy Sealed Secret Decryption Key\nDeploy CSM Applications and Services\nSetup Nexus Set NCNs to use Unbound Initialize cray CLI Apply After Sysmgmt Manifest Workarounds Add Compute Cabinet Routing to NCNs Validate CSM Install\nReboot from the LiveCD to NCN\nKnown Issues\nerror: timed out waiting for the condition on jobs/cray-sls-init-load Error: not ready: https://packages.local Error initiating layer upload \u0026hellip; in registry.local: received unexpected HTTP status: 200 OK Error lookup registry.local: no such host Initialize Bootstrap Registry SKIP IF ONLINE - Online installs cannot upload container images to the bootstrap registry since it proxies an upstream source. DO NOT perform this procedure if the bootstrap registry was reconfigured to proxy from an upstream registry.\nVerify that Nexus is running:\npit# systemctl status nexus Verify that Nexus is ready. (Any HTTP response other than 200 OK indicates Nexus is not ready.)\npit# curl -sSif http://localhost:8081/service/rest/v1/status/writable Expected output looks similar to the following:\nHTTP/1.1 200 OK Date: Thu, 04 Feb 2021 05:27:44 GMT Server: Nexus/3.25.0-03 (OSS) X-Content-Type-Options: nosniff Content-Length: 0 Load the skopeo image installed by the cray-nexus RPM:\npit# podman load -i /var/lib/cray/container-images/cray-nexus/skopeo-stable.tar quay.io/skopeo/stable Use skopeo sync to upload container images from the CSM release:\npit# export CSM_RELEASE=csm-x.y.z pit# podman run --rm --network host -v /var/www/ephemeral/${CSM_RELEASE}/docker/dtr.dev.cray.com:/images:ro quay.io/skopeo/stable sync \\ --scoped --src dir --dest docker --dest-tls-verify=false --dest-creds admin:admin123 /images localhost:5000 Create Site-Init Secret The site-init secret in the loftsman namespace makes /var/www/ephemeral/prep/site-init/customizations.yaml available to product installers. The site-init secret should only be updated when the corresponding customizations.yaml data is changed, such as during system installation or upgrade. Create the site-init secret to contain /var/www/ephemeral/prep/site-init/customizations.yaml:\npit# kubectl create secret -n loftsman generic site-init --from-file=/var/www/ephemeral/prep/site-init/customizations.yaml Expected output looks similar to the following:\nsecret/site-init created NOTE If the site-init secret already exists then kubectl will error with a message similar to:\nError from server (AlreadyExists): secrets \u0026#34;site-init\u0026#34; already exists In this case, delete the site-init secret and recreate it.\nFirst delete it:\npit# kubectl delete secret -n loftsman site-init Expected output looks similar to the following:\nsecret \u0026#34;site-init\u0026#34; deleted Then recreate it:\npit# kubectl create secret -n loftsman generic site-init --from-file=/var/www/ephemeral/prep/site-init/customizations.yaml Expected output looks similar to the following:\nsecret/site-init created WARNING If for some reason the system customizations need to be modified to complete product installation, administrators must first update customizations.yaml in the site-init Git repository, which may no longer be mounted on any cluster node, and then delete and recreate the site-init secret as shown below.\nTo read customizations.yaml from the site-init secret:\nncn# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; customizations.yaml To delete the site-init secret:\nncn# kubectl -n loftsman delete secret site-init To recreate the site-init secret:\nncn# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml Deploy Sealed Secret Decryption Key Deploy the corresponding key necessary to decrypt sealed secrets:\npit# /var/www/ephemeral/prep/site-init/deploy/deploydecryptionkey.sh An error similar to the following may occur when deploying the key:\nError from server (NotFound): secrets \u0026#34;sealed-secrets-key\u0026#34; not found W0304 17:21:42.749101 29066 helpers.go:535] --dry-run is deprecated and can be replaced with --dry-run=client. secret/sealed-secrets-key created Restarting sealed-secrets to pick up new keys No resources found This is expected and can safely be ignored.\nDeploy CSM Applications and Services Run install.sh to deploy CSM applications services:\nNOTE install.sh requires various system configuration which are expected to be found in the locations used in proceeding documentation; however, it needs to know SYSTEM_NAME in order to find metallb.yaml and sls_input_file.json configuration files.\nSome commands will also need to have the CSM_RELEASE variable set.\npit# export SYSTEM_NAME=eniac pit# export CSM_RELEASE=csm-x.y.z pit# cd /var/www/ephemeral/$CSM_RELEASE pit# ./install.sh On success, install.sh will output OK to stderr and exit with status code 0, e.g.:\npit# ./install.sh ... + CSM applications and services deployed install.sh: OK In the event that install.sh does not complete successfully, consult the known issues below to resolve potential problems and then try running install.sh again.\nSetup Nexus Run ./lib/setup-nexus.sh to configure Nexus and upload CSM RPM repositories, container images, and Helm charts:\npit# ./lib/setup-nexus.sh On success, setup-nexus.sh will output to OK on stderr and exit with status code 0, e.g.:\npit# ./lib/setup-nexus.sh ... + Nexus setup complete setup-nexus.sh: OK In the event of an error, consult the known issues below to resolve potential problems and then try running setup-nexus.sh again. Note that subsequent runs of setup-nexus.sh may report FAIL when uploading duplicate assets. This is ok as long as setup-nexus.sh outputs setup-nexus.sh: OK and exits with status code 0.\nSet Management NCNs to use Unbound First, verify that SLS properly reports all management NCNs in the system:\npit# ./lib/list-ncns.sh On success, each management NCN will be output, e.g.:\npit# ./lib/list-ncns.sh + Getting admin-client-auth secret + Obtaining access token + Querying SLS ncn-m001 ncn-m002 ncn-m003 ncn-s001 ncn-s002 ncn-s003 ncn-w001 ncn-w002 ncn-w003 If any management NCNs are missing from the output, take corrective action before proceeding.\nNext, run lib/set-ncns-to-unbound.sh to SSH to each management NCN and update /etc/resolv.conf to use Unbound as the nameserver.\npit# ./lib/set-ncns-to-unbound.sh NOTE If passwordless SSH is not configured, the administrator will have to enter the corresponding password as the script attempts to connect to each NCN.\nOn success, the nameserver configuration in /etc/resolv.conf will be printed for each management NCN, e.g.,:\npit# ./lib/set-ncns-to-unbound.sh + Getting admin-client-auth secret + Obtaining access token + Querying SLS + Updating ncn-m001 Password: ncn-m001: nameserver 127.0.0.1 ncn-m001: nameserver 10.92.100.225 + Updating ncn-m002 Password: ncn-m002: nameserver 10.92.100.225 + Updating ncn-m003 Password: ncn-m003: nameserver 10.92.100.225 + Updating ncn-s001 Password: ncn-s001: nameserver 10.92.100.225 + Updating ncn-s002 Password: ncn-s002: nameserver 10.92.100.225 + Updating ncn-s003 Password: ncn-s003: nameserver 10.92.100.225 + Updating ncn-w001 Password: ncn-w001: nameserver 10.92.100.225 + Updating ncn-w002 Password: ncn-w002: nameserver 10.92.100.225 + Updating ncn-w003 Password: ncn-w003: nameserver 10.92.100.225 NOTE The script connects to ncn-m001 which will be the PIT node, whose password may be different from that of the other NCNs.\nInitialize cray CLI The cray command line interface (CLI) is a framework created to integrate all of the system management REST APIs into easily usable commands.\nLater procedures in the installation process use the \u0026lsquo;cray\u0026rsquo; CLI to interact with multiple services. The \u0026lsquo;cray\u0026rsquo; CLI configuration needs to be initialized and the user running the procedure needs to be authorized. This section describes how to initialize the \u0026lsquo;cray\u0026rsquo; CLI for use by a user and authorize that user.\nThe \u0026lsquo;cray\u0026rsquo; CLI only needs to be initialized once per user on a node.\nUnset CRAY_CREDENTIALS environment variable, if previously set.\nSome of the installation procedures leading up to this point use the CLI with a Kubernetes managed service account normally used for internal operations. There is a procedure for extracting the OAUTH token for this service account and assigning it to the CRAY_CREDENTIALS environment variable to permit simple CLI operations.\nncn# unset CRAY_CREDENTIALS Initialize the \u0026lsquo;cray\u0026rsquo; CLI for the root account.\nThe \u0026lsquo;cray\u0026rsquo; CLI needs to know what host to use to obtain authorization and what user is requesting authorization so it can obtain an OAUTH token to talk to the API Gateway. This is accomplished by initializing the CLI configuration. In this example, the vers username and its password are used.\nIf LDAP configuration has enabled, then use a valid account in LDAP instead of \u0026lsquo;vers\u0026rsquo;.\nIf LDAP configuration was not enabled, or is not working, then a keycloak local account could be created. See \u0026ldquo;Create a Service Account in Keycloak\u0026rdquo; in the HPE Cray EX System Administration Guide 1.4 S-80001.\nncn# cray init When prompted, remember to substitute your username instead of \u0026lsquo;vers\u0026rsquo;. Expected output (including your typed input) should look similar to the following:\nCray Hostname: api-gw-service-nmn.local Username: vers Password: Success! Initialization complete. Troubleshooting cray CLI If initialization fails in the above step, there are several common causes:\nDNS failure looking up api-gw-service-nmn.local may be preventing the CLI from reaching the API Gateway and Keycloak for authorization Network connectivity issues with the NMN may be preventing the CLI from reaching the API Gateway and Keycloak for authorization Certificate mismatch or trust issues may be preventing a secure connection to the API Gateway Istio failures may be preventing traffic from reaching Keycloak Keycloak may not yet be set up to authorize you as a user While resolving these issues is beyond the scope of this section, you may get clues to what is failing by adding -vvvvv to the cray init ... commands.\nApply After Sysmgmt Manifest Workarounds Check for workarounds in the /opt/cray/csm/workarounds/after-sysmgmt-manifest directory within the CSM tar. If there are any workarounds in that directory, run those now. Each has its own instructions in their respective README.md files.\n# Example pit# ls /opt/cray/csm/workarounds/after-sysmgmt-manifest If there is a workaround here, the output looks similar to the following:\nCASMCMS-6857 CASMNET-423 Add Compute Cabinet Routing to NCNs NCNs require additional routing to enable access to Mountain, Hill and River Compute cabinets.\nRequires:\nPlatform installation Running and configured SLS Can be run from PIT if passwordless SSH is set up to all NCNs, but should be run post ncn-m001 reboot. To apply the routing, run:\nncn# /opt/cray/csm/workarounds/livecd-post-reboot/CASMINST-1570/CASMINST-1570.sh NOTE Currently, there is no automated procedure to apply routing changes to all worker NCNs to support Mountain, Hill and River Compute Node Cabinets.\nValidate CSM Install The administrator should wait at least 15 minutes to let the various Kubernetes resources get initialized and started. Because there are a number of dependencies between them, some services are not expected to work immediately after the install script completes. After waiting, the administrator may start the CSM Validation process.\nReboot from the LiveCD to NCN Once the CSM services are deemed healthy the administrator may proceed to the final step of the CSM install Reboot from the LiveCD to NCN.\nKnown Issues The install.sh script changes cluster state and should not simply be rerun in the event of a failure without careful consideration of the specific error. It may be possible to resume installation from the last successful command executed by install.sh, but admins will need to appropriately modify install.sh to pick up where the previous run left off. (Note: The install.sh script runs with set -x, so each command will be printed to stderr prefixed with the expanded value of PS4, namely, + .)\nKnown potential issues with suggested fixes are listed below.\nerror: timed out waiting for the condition on jobs/cray-sls-init-load The following error may occur when running ./install.sh:\n+ /var/www/ephemeral/csm-0.8.11/lib/wait-for-unbound.sh + kubectl wait -n services job cray-sls-init-load --for=condition=complete --timeout=20m error: timed out waiting for the condition on jobs/cray-sls-init-load Determine the name and state of the SLS init loader job pod:\npit# kubectl -n services get pods -l app=cray-sls-init-load Expected output looks similar to the following:\nNAME READY STATUS RESTARTS AGE cray-sls-init-load-nh5k7 2/2 Running 0 21m If the state is Running after after the 20 minute timeout, this is likely that the SLS loader job is failing to ping the SLS S3 bucket due to a malformed URL. To verify this inspect the logs of the cray-sls-init-load pod:\npit# kubectl -n services logs -l app=cray-sls-init-load -c cray-sls-loader The symptom of this situation is the present of something similar to the following in the output of the previous command:\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1612296611.2630196,\u0026#34;caller\u0026#34;:\u0026#34;sls-s3-downloader/main.go:96\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Failed to ping bucket.\u0026#34;,\u0026#34;error\u0026#34;:\u0026#34;encountered error during head_bucket operation for bucket sls at https://: RequestError: send request failed\\ncaused by: Head \\\u0026#34;https:///sls\\\u0026#34;: http: no Host in request URL\u0026#34;} This error is most likely intermittent and and deleting the cray-sls-init-load pod is expected to resolve this issue. You may need to delete the loader pod multiple times until it succeeds.\npit# kubectl -n services delete pod cray-sls-init-load-nh5k7 Once the pod is deleted is deleted, verify the new pod started by k8s completes successfully. If it does not complete within a few minutes inspect the logs for the pod. If it is still failing to ping the S3 bucket, delete the pod again and try again.\npit# kubectl -n services get pods -l app=cray-sls-init-load If the pod has completed successfully, the output looks similar to the following:\nNAME READY STATUS RESTARTS AGE cray-sls-init-load-pbzxv 0/2 Completed 0 55m Since it can sometimes be required to repeat the above steps several times before the pod succeeds, the following script can be used to automate the retry process:\npit# while [ true ]; do POD=\u0026#34;\u0026#34; while [ -z \u0026#34;$POD\u0026#34; ]; do POD=$(kubectl get pods -n services --no-headers -o custom-columns=:metadata.name | grep \u0026#34;^cray-sls-init-load-\u0026#34;) done GOOD=0 while [ true ]; do [ \u0026#34;$(kubectl get pod -n services $POD --no-headers -o custom-columns=:.status.phase)\u0026#34; = Succeeded ] \u0026amp;\u0026amp; GOOD=1 \u0026amp;\u0026amp; echo \u0026#34;Success!\u0026#34; \u0026amp;\u0026amp; break kubectl logs -n services $POD --all-containers 2\u0026gt;/dev/null | grep -q \u0026#34;http: no Host in request URL\u0026#34; \u0026amp;\u0026amp; break sleep 1 done [ $GOOD -eq 1 ] \u0026amp;\u0026amp; break kubectl delete pod -n services $POD done Once the loader job has completed successfully running ./install.sh again is expected to succeed.\nError: not ready: https://packages.local The infamous error: not ready: https://packages.local indicates that from the caller\u0026rsquo;s perspective, Nexus not ready to receive writes. However, it most likely indicates that a Nexus setup utility was unable to connect to Nexus via the packages.local name. Since the install does not attempt to connect to packages.local until Nexus has been successfully deployed, the error does not usually indicate something is actually wrong with Nexus. Instead, it is most commonly a network issue with name resolution (i.e., DNS), IP routes from the PIT node, switch misconfiguration, or Istio ingress.\nVerify that packages.local resolves to ONLY the load balancer IP for the istio-ingressgateway service in the istio-system namespace, typically 10.92.100.71. If name resolution returns addresses on other networks (such as HMN) this must be corrected. Prior to DNS/DHCP hand-off to Unbound, these settings are controlled by dnsmasq. Unbound settings are based on SLS settings in sls_input_file.json and must be updated via the Unbound manager.\nIf packages.local resolves to the correct addresses, verify basic connectivity using ping. If ping packages.local is unsuccessful, verify the IP routes from the PIT node to the NMN load balancer network. The typical ip route configuration is 10.92.100.0/24 via 10.252.0.1 dev vlan002. If pings are successful, try checking the status of Nexus by running curl -sS https://packages.local/service/rest/v1/status/writable. If the connection times out, it indicates there is a more complex connection issue. Verify switches are configured properly and BGP peering is operating correctly, see docs/400-SWITCH-BGP-NEIGHBORS.md for more information. Lastly, check Istio and OPA logs to see if connections to packages.local are not reaching Nexus, perhaps due to an authorization issue.\nIf https://packages.local/service/rest/v1/status/writable returns an HTTP code other than 200 OK, it indicates there is an issue with Nexus. Verify that the loftsman ship deployment of the nexus.yaml manifest was successful. If helm status -n nexus cray-nexus indicates the status is NOT deployed, then something is most likely wrong with the Nexus deployment and additional diagnosis is required. In this case, the current Nexus deployment probably needs to be uninstalled and the nexus-data PVC removed before attempting to deploy again.\nError initiating layer upload \u0026hellip; in registry.local: received unexpected HTTP status: 200 OK The following error may occur when running ./lib/setup-nexus.sh:\ntime=\u0026#34;2021-02-07T20:25:22Z\u0026#34; level=info msg=\u0026#34;Copying image tag 97/144\u0026#34; from=\u0026#34;dir:/image/jettech/kube-webhook-certgen:v1.2.1\u0026#34; to=\u0026#34;docker://registry.local/jettech/kube-webhook-certgen:v1.2.1\u0026#34; Getting image source signatures Copying blob sha256:f6e131d355612c71742d71c817ec15e32190999275b57d5fe2cd2ae5ca940079 Copying blob sha256:b6c5e433df0f735257f6999b3e3b7e955bab4841ef6e90c5bb85f0d2810468a2 Copying blob sha256:ad2a53c3e5351543df45531a58d9a573791c83d21f90ccbc558a7d8d3673ccfa time=\u0026#34;2021-02-07T20:25:33Z\u0026#34; level=fatal msg=\u0026#34;Error copying tag \\\u0026#34;dir:/image/jettech/kube-webhook-certgen:v1.2.1\\\u0026#34;: Error writing blob: Error initiating layer upload to /v2/jettech/kube-webhook-certgen/blobs/uploads/ in registry.local: received unexpected HTTP status: 200 OK\u0026#34; + return This error is most likely intermittent and running ./lib/setup-nexus.sh again is expected to succeed.\nError lookup registry.local: no such host The following error may occur when running ./lib/setup-nexus.sh:\ntime=\u0026#34;2021-02-23T19:55:54Z\u0026#34; level=fatal msg=\u0026#34;Error copying tag \\\u0026#34;dir:/image/grafana/grafana:7.0.3\\\u0026#34;: Error writing blob: Head \\\u0026#34;https://registry.local/v2/grafana/grafana/blobs/sha256:cf254eb90de2dc62aa7cce9737ad7e143c679f5486c46b742a1b55b168a736d3\\\u0026#34;: dial tcp: lookup registry.local: no such host\u0026#34; + return Or a similar error:\ntime=\u0026#34;2021-03-04T22:45:07Z\u0026#34; level=fatal msg=\u0026#34;Error copying ref \\\u0026#34;dir:/image/cray/cray-ims-load-artifacts:1.0.4\\\u0026#34;: Error trying to reuse blob sha256:1ec886c351fa4c330217411b0095ccc933090aa2cd7ae7dcd33bb14b9f1fd217 at destination: Head \\\u0026#34;https://registry.local/v2/cray/cray-ims-load-artifacts/blobs/sha256:1ec886c351fa4c330217411b0095ccc933090aa2cd7ae7dcd33bb14b9f1fd217\\\u0026#34;: dial tcp: lookup registry.local: Temporary failure in name resolution\u0026#34; + return These errors are most likely intermittent and running ./lib/setup-nexus.sh again is expected to succeed.\n"
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/change_the_keycloak_admin_password/",
	"title": "Change the Keycloak Admin Password",
	"tags": [],
	"description": "",
	"content": "Change the Keycloak Admin Password Update the default password for the admin Keycloak account using the Keycloak user interface (UI). After updating the password in Keycloak, encrypt it on the system and verify that the change was made successfully.\nThis procedure uses SYSTEM_DOMAIN_NAME as an example for the DNS name of the non-compute node (NCN). Replace this name with the actual NCN\u0026rsquo;s DNS name while executing this procedure.\nProcedure Log in to Keycloak with the default admin credentials.\nPoint a browser at https://auth.SYSTEM_DOMAIN_NAME/keycloak/admin, replacing SYSTEM_DOMAIN_NAME with the actual NCN\u0026rsquo;s DNS name.\nThe following is an example URL for a system:\nauth.system1.us.cray.com/keycloak/admin Use the following admin login credentials:\nUsername: admin The password can be obtained with the following command: ncn-w001# kubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode Click the Admin drop-down menu in the upper-right corner of the page.\nSelect Manage Account.\nClick the Password tab on the left side of the page.\nEnter the existing password, new password and confirmation, and then click Save.\nLog on to ncn-w001.\ngit clone https://github.com/Cray-HPE/csm.git.\nSave a local copy of the customizations.yaml file.\nkubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; customizations.yaml Change the password in the customizations.yaml file.\nThe Keycloak master admin password is also stored in the keycloak-master-admin-auth Secret in the services namespace. This needs to be updated so that clients that need to make requests as the master admin can authenticate with the new password.\nIn the customizations.yaml file, set the values for the keycloak_master_admin_auth keys in the spec.kubernetes.sealed_secrets field. The value in the data element where the name is password needs to be changed to the new Keycloak master admin password. The section below will replace the existing sealed secret data in the customizations.yaml.\nFor example:\nkeycloak_master_admin_auth: generate: name: keycloak-master-admin-auth data: - type: static args: name: client-id value: admin-cli - type: static args: name: user value: admin - type: static args: name: password value: my_secret_password - type: static args: name: internal_token_url value: https://api-gw-service-nmn.local/keycloak/realms/master/protocol/openid-connect/token Encrypt the values after changing the customizations.yaml file.\n./utils/secrets-seed-customizations.sh customizations.yaml If the above command complains that it cannot find certs/sealed_secrets.crt then you can run the following commands to create it\nmkdir -p certs ./utils/bin/linux/kubeseal --controller-name sealed-secrets --fetch-cert \u0026gt; certs/sealed_secrets.crt Create a local copy of the platform.yaml file.\nkubectl get cm -n loftsman loftsman-platform -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; \u0026gt; platform.yaml Edit the platform.yaml to only include the cray-keycloak chart and all its current data.\nExample:\napiVersion: manifests/v1beta1 metadata: name: platform spec: charts: - name: cray-keycloak namespace: services source: csm-algol60 values: internalTokenUrl: https://api-gw-service-nmn.local/keycloak/realms/master/protocol/openid-connect/token sealedSecrets: - apiVersion: bitnami.com/v1alpha1 kind: SealedSecret metadata: annotations: sealedsecrets.bitnami.com/cluster-wide: \u0026#39;true\u0026#39; ... Generate the manifest that will be used to redeploy the chart with the modified resources.\nmanifestgen -c customizations.yaml -i platform.yaml -o manifest.yaml Re-apply the cray-keycloak Helm chart with the updated customizations.yaml file.\nThis will update the keycloak-master-admin-auth SealedSecret which will cause the SealedSecret controller to update the Secret.\nloftsman ship --charts-path ${PATH_TO_RELEASE}/helm --manifest-path ${PWD}/manifest.yaml Verify that the Secret has been updated.\nGive the SealedSecret controller a few seconds to update the Secret, then run the following command to see the current value of the Secret:\nkubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode Save an updated copy of customizations.yaml to the site-init secret in the loftsman kubernetes namespace.\nCUSTOMIZATIONS=$(base64 \u0026lt; customizations.yaml | tr -d \u0026#39;\\n\u0026#39;) kubectl get secrets -n loftsman site-init -o json | \\ jq \u0026#34;.data.\\\u0026#34;customizations.yaml\\\u0026#34; |= \\\u0026#34;$CUSTOMIZATIONS\\\u0026#34;\u0026#34; | kubectl apply -f - "
},
{
	"uri": "/docs-csm/en-09/007-csm-install-reboot/",
	"title": "CSM Install Reboot - Final NCN Install",
	"tags": [],
	"description": "",
	"content": "CSM Install Reboot - Final NCN Install This page describes rebooting and deploying the non-compute node that is currently hosting the LiveCD.\nRequired Services Notice of Danger LiveCD Pre-Reboot Workarounds Hand-Off Start Hand-Off Reboot Accessing USB Partitions After Reboot Accessing CSI from a USB or RemoteISO Enable NCN Disk Wiping Safeguard Required Services These services must be healthy in Kubernetes before the reboot of the LiveCD can take place.\nRequired Platform Services:\ncray-dhcp-kea cray-dns-unbound cray-bss cray-sls cray-s3 cray-ipxe cray-tftp Notice of Danger While the node is rebooting, it will be available only through Serial-over-LAN and local terminals.\nThis procedure entails deactivating the LiveCD, meaning the LiveCD and all of its resources will be unavailable.\nLiveCD Pre-Reboot Workarounds Check for workarounds in the /opt/cray/csm/workarounds/livecd-pre-reboot directory. If there are any workarounds in that directory, run those when the workaround instructs. Timing is critical to ensure properly loaded data so run them only when indicated. Instructions are in the README files.\n# Example pit# ls /opt/cray/csm/workarounds/livecd-pre-reboot If there is a workaround here, the output looks similar to the following:\nCASMINST-435 Hand-Off The steps in this guide will ultimately walk an administrator through loading hand-off data and rebooting the node. This will assist with remote-console setup, for observing the reboot.\nAt the end of these steps, the LiveCD will be no longer active. The node it was using will join the Kubernetes cluster as the final of 3 masters forming a quorum.\nStart Hand-Off It is very important to run the livecd-pre-reboot workarounds. Ensure that the livecd-pre-reboot workarounds have all been run by the administrator before starting this stage.\nUpload SLS file.\nNote the system name environment variable SYSTEM_NAME must be set\npit# csi upload-sls-file --sls-file /var/www/ephemeral/prep/${SYSTEM_NAME}/sls_input_file.json Expected output looks similar to the following:\n2021/02/02 14:05:15 Retrieving S3 credentails ( sls-s3-credentials ) for SLS 2021/02/02 14:05:15 Uploading SLS file: /var/www/ephemeral/prep/eniac/sls_input_file.json 2021/02/02 14:05:15 Successfully uploaded SLS Input File. Get a token to use for authenticated communication with the gateway.\npit# export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Upload the same data.json file we used to BSS, our Kubernetes cloud-init DataSource. If you have made any changes to this file as a result of any customizations or workarounds use the path to that file instead. This step will prompt for the root password of the NCNs.\npit# csi handoff bss-metadata --data-file /var/www/ephemeral/configs/data.json Patch the metadata for the CEPH nodes to have the correct run commands:\npit# python3 /usr/share/doc/metal/scripts/patch-ceph-runcmd.py Perform csi handoff, uploading NCN boot artifacts into S3.\nSet variables\nIMPORTANT: The variables you set depend on whether or not you followed the steps in 108-NCN-NTP.md#setting-a-local-timezone. The two paths forward are listed below:\nIf you customized the timezones, set the following variables:\npit# export artdir=/var/www/ephemeral/data pit# export k8sdir=$artdir/k8s pit# export cephdir=$artdir/ceph If you did not customize the timezones, set the following variables (this is the default path):\npit# export CSM_RELEASE=csm-x.y.z pit# export artdir=/var/www/ephemeral/${CSM_RELEASE}/images pit# export k8sdir=$artdir/kubernetes pit# export cephdir=$artdir/storage-ceph After setting the variables above per your situation, run:\npit# csi handoff ncn-images \\ --k8s-kernel-path $k8sdir/*.kernel \\ --k8s-initrd-path $k8sdir/initrd.img*.xz \\ --k8s-squashfs-path $k8sdir/*.squashfs \\ --ceph-kernel-path $cephdir/*.kernel \\ --ceph-initrd-path $cephdir/initrd.img*.xz \\ --ceph-squashfs-path $cephdir/*.squashfs List ipv4 boot options using efibootmgr:\npit# efibootmgr | grep -Ei \u0026#34;ip(v4|4)\u0026#34; Configure and trim UEFI entries on the PIT node.\nOn the PIT node, do the following two steps outlined in Fixing Boot-Order. For options that depend on the node type, treat it as a master node. 1. Setting Order 1. Trimming\nIdentify Port-1 of Riser-1 in efibootmgr output and set PXEPORT variable.\nWARNING Pay attention to the boot order, if does not show any of these examples please double-check that PCIe booting was enabled. Simply run-through the \u0026ldquo;Print Current UEFI and SR-IOV State\u0026rdquo; section, then fixup as needed with the \u0026ldquo;Setting Expected Values\u0026rdquo; section before returning here.\nExample 1:\nPossible output of efibootmgr command in the previous step:\nBoot0005* UEFI IPv4: Network 00 at Riser 02 Slot 01 Boot0007* UEFI IPv4: Network 01 at Riser 02 Slot 01 Boot000A* UEFI IPv4: Intel Network 00 at Baseboard Boot000C* UEFI IPv4: Intel Network 01 at Baseboard Looking at the above output, Network 00 at Riser 02 Slot 01 is reasonably our Port-1 of Riser-1, which would be Boot0005 in this case. Set PXEPORT to the numeric suffix of that name.\npit# export PXEPORT=0005 Example 2:\nPossible output of efibootmgr command in the previous step:\nBoot0013* OCP Slot 10 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - PXE (HTTP(S) IPv4) Boot0014* OCP Slot 10 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - PXE (PXE IPv4) Boot0017* Slot 1 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - PXE (HTTP(S) IPv4) Boot0018* Slot 1 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - PXE (PXE IPv4) In the above output, look for the non-OCP device that has \u0026ldquo;PXE IPv4\u0026rdquo; rather than \u0026ldquo;HTTP(S) IPv4\u0026rdquo;, which would be Boot0018 in this case. Set PXEPORT to the numeric suffix of that name.\npit# export PXEPORT=0018 Example 3:\nThe output below if what a Gigabyte machine may look like.\nBoot000B* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:C7:12:F2 Boot000C* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:C7:12:F3 Boot000D* UEFI: PXE IP6 Mellanox Network Adapter - B8:59:9F:C7:12:F2 Boot000E* UEFI: PXE IP6 Mellanox Network Adapter - B8:59:9F:C7:12:F3 Use efibootmgr to set next boot device to port selected in the previous step.\npit# efibootmgr -n $PXEPORT 2\u0026gt;\u0026amp;1 | grep -i BootNext If PXEPORT was set to 0005, the expected output looks similar to the following:\nBootNext: 0005 SKIP THIS STEP IF USING USB LIVECD The remote LiveCD will lose all changes and local data once it is rebooted.\nIt is advised to backup the prep directory for the LiveCD off of the CRAY before rebooting. This will facilitate setting the LiveCD up again in the event of a bad reboot. Follow the procedure in Virtual ISO Boot - Backing up the OverlayFS. After completing that, return here and proceed to the next step.\nOptionally setup conman or serial console if not already on one from any laptop\nexternal# script -a boot.livecd.$(date +%Y-%m-%d).txt external# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; external# SYSTEM_NAME=eniac external# username=root external# IPMI_PASSWORD=changeme external# ipmitool -I lanplus -U $username -E -H ${SYSTEM_NAME}-ncn-m001-mgmt chassis power status external# ipmitool -I lanplus -U $username -E -H ${SYSTEM_NAME}-ncn-m001-mgmt sol activate Collect the CAN IPs for logging into other NCNs while this happens. This is useful for interacting and debugging the Kubernetes cluster while the LiveCD is offline.\npit# ssh ncn-m002 ncn-m002# ip a show vlan007 | grep inet Expected output looks similar to the following:\ninet 10.102.11.13/24 brd 10.102.11.255 scope global vlan007 inet6 fe80::1602:ecff:fed9:7820/64 scope link Now log in from another machine to verify that the IP address is usable\nexternal# ssh root@10.102.11.13 ncn-m002# Keep this terminal active as it will enable kubectl commands during the bring-up of the new NCN. If the reboot successfully deploys the LiveCD, this terminal can be exited.\nWipe the node beneath the LiveCD, erasing the RAIDs labels will trigger a fresh partition table to deploy.\nWARNING Do not assume to wipe the first three disks (e.g. sda, sdb, and sdc), these could be any letter. Choosing the wrong ones may result in wiping the USB stick.\nSelect disks to wipe; SATA/NVME/SAS\npit# md_disks=\u0026#34;$(lsblk -l -o SIZE,NAME,TYPE,TRAN | grep -E \u0026#39;(sata|nvme|sas)\u0026#39; | sort -h | awk \u0026#39;{print \u0026#34;/dev/\u0026#34; $2}\u0026#39;)\u0026#34; Sanity check; print disks into typescript or console\npit# echo $md_disks Expected output looks similar to the following:\n/dev/sda /dev/sdb /dev/sdc Wipe. This is irreversible.\npit# wipefs --all --force $md_disks If any disks had labels present, output looks similar to the following:\n/dev/sda: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa /dev/sdb: 6 bytes were erased at offset 0x00000000 (crypto_LUKS): 4c 55 4b 53 ba be /dev/sdb: 6 bytes were erased at offset 0x00004000 (crypto_LUKS): 53 4b 55 4c ba be /dev/sdc: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sdc: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sdc: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa The thing to verify is that there are no error messages in the output.\nQuit the typescript session with the exit command and copy the file (booted-csm-livecd.\u0026lt;date\u0026gt;.txt) to a location on another server for reference later.\npit# exit Reboot Reboot the LiveCD.\npit# reboot The node should boot, acquire its hostname (i.e. ncn-m001), and run cloud-init.\nNOTE: If the node has PXE boot issues, such as getting PXE errors or not pulling the ipxe.efi binary, see PXE boot troubleshooting\nNOTE: If ncn-m001 booted without a hostname (as ncn) or it did not run all the cloud-init scripts, the following commands need to be run (but only in that circumstance).\nMake a mount point. ncn-m001# mkdir -pv /mnt/cow Mount the USB to that directory. ncn-m001# mount -vL cow /mnt/cow Copy the network configuration files. ncn-m001# cp -pv /mnt/cow/rw/etc/sysconfig/network/ifroute-vlan* /etc/sysconfig/network/ ncn-m001# cp -pv /mnt/cow/rw/etc/sysconfig/network/ifcfg-lan0 /etc/sysconfig/network/ Run the set-dhcp-to-static.sh script ncn-m001# /srv/cray/scripts/metal/set-dhcp-to-static.sh Network connectivity should be restored afterwards; the bond is up. Run the following commands: ncn-m001# cloud-init clean ncn-m001# cloud-init init ncn-m001# cloud-init modules -m init ncn-m001# cloud-init modules -m config ncn-m001# cloud-init modules -m final Once cloud-init has completed successfully, log in and start a typescript (the IP address used here is the one we noted for ncn-m002 in an earlier step).\nexternal# ssh root@10.102.11.13 ncn-m002# ssh ncn-m001 ncn-m001# script -a verify.csm.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Optionally change the root password on ncn-m001 to match the other management NCNs.\nThis step is optional and is only needed when the other management NCNs passwords were customized during the CSM Metal Install procedure. If the management NCNs still have the default password this step can be skipped.\nncn-m001# passwd Run kubectl get nodes to see the full Kubernetes cluster.\nNOTE If the new node fails to join the cluster after running other cloud-init items please refer to the handoff\nncn-m001# kubectl get nodes Expected output looks similar to the following:\nNAME STATUS ROLES AGE VERSION ncn-m001 Ready master 7s v1.18.6 ncn-m002 Ready master 4h40m v1.18.6 ncn-m003 Ready master 4h38m v1.18.6 ncn-w001 Ready \u0026lt;none\u0026gt; 4h39m v1.18.6 ncn-w002 Ready \u0026lt;none\u0026gt; 4h39m v1.18.6 ncn-w003 Ready \u0026lt;none\u0026gt; 4h39m v1.18.6 Follow the procedure defined in Accessing CSI from a USB or RemoteISO.\nRestore and verify the site link. It will be necessary to restore the ifcfg-lan0 and ifroute-vlan002 files from either the manual backup taken in step 6 or remount the USB and copy it from the prep directory to /etc/sysconfig/network/.\nncn-m001# export SYSTEM_NAME=eniac ncn-m001# cp -v /mnt/pitdata/prep/${SYSTEM_NAME}/pit-files/ifcfg-lan0 /etc/sysconfig/network/ ncn-m001# cp -v /mnt/pitdata/prep/${SYSTEM_NAME}/pit-files/ifroute-vlan002 /etc/sysconfig/network/ ncn-m001# wicked ifreload lan0 Run ip a to show our IPs, verify the site link.\nncn-m001# ip a show lan0 Run ip a to show our VLANs, verify they all have IPs.\nncn-m001# ip a show vlan002 ncn-m001# ip a show vlan004 ncn-m001# ip a show vlan007 Verify we do not have a metal bootstrap IP.\nncn-m001# ip a show bond0 Enable NCN Disk Wiping Safeguard to prevent destructive behavior from occurring during reboot.\nNOTE This safeguard needs to be removed to facilitate bare-metal deployments of new nodes. The linked Enable NCN Disk Wiping Safeguard procedure can be used to disable the safeguard by setting the value back to 0.\nDownload and install/upgrade the workaround and documentation RPMs. If this machine does not have direct internet access these RPMs will need to be externally downloaded and then copied to be installed.\nncn-m001# rpm -Uvh --force https://storage.googleapis.com/csm-release-public/shasta-1.4/docs-csm/docs-csm-latest.noarch.rpm ncn-m001# rpm -Uvh --force https://storage.googleapis.com/csm-release-public/shasta-1.4/csm-install-workarounds/csm-install-workarounds-latest.noarch.rpm Now check for workarounds in the /opt/cray/csm/workarounds/livecd-post-reboot directory. If there are any workarounds in that directory, run those now. Each has its own instructions in their respective README.md files.\nNote: The following command assumes that the data partition of the USB stick has been remounted at /mnt/pitdata\n# Example ncn-m001# ls /opt/cray/csm/workarounds/livecd-post-reboot If there are workarounds here, the output looks similar to the following:\nCASMINST-1309 CASMINST-1570 At this time, the NCN cluster is fully established. The administrator may now eject any mounted USB stick:\nncn-m001# umount -v /mnt/rootfs /mnt/sqfs /mnt/livecd /mnt/pitdata The administrator can continue onto CSM Validation to conclude the CSM product deployment.\nThere are some operational steps to be taken in NCN/Management Node Locking and then Firmware updates with FAS\nThen the administrator should install additional products following the procedures in the HPE Cray EX System Installation and Configuration Guide S-8000.\nAccessing USB Partitions After Reboot After deploying the LiveCD\u0026rsquo;s NCN, the LiveCD USB itself is unharmed and available to an administrator.\nMount and view the USB stick:\nncn-m001# mkdir -pv /mnt/{cow,pitdata} ncn-m001# mount -vL cow /mnt/cow ncn-m001# mount -vL PITDATA /mnt/pitdata ncn-m001# ls -ld /mnt/cow/rw/* Example output:\ndrwxr-xr-x 2 root root 4096 Jan 28 15:47 /mnt/cow/rw/boot drwxr-xr-x 8 root root 4096 Jan 29 07:25 /mnt/cow/rw/etc drwxr-xr-x 3 root root 4096 Feb 5 04:02 /mnt/cow/rw/mnt drwxr-xr-x 3 root root 4096 Jan 28 15:49 /mnt/cow/rw/opt drwx------ 10 root root 4096 Feb 5 03:59 /mnt/cow/rw/root drwxrwxrwt 13 root root 4096 Feb 5 04:03 /mnt/cow/rw/tmp drwxr-xr-x 7 root root 4096 Jan 28 15:40 /mnt/cow/rw/usr drwxr-xr-x 7 root root 4096 Jan 28 15:47 /mnt/cow/rw/var Look at the contents of /mnt/pitdata:\nncn-m001# ls -ld /mnt/pitdata/* Example output:\ndrwxr-xr-x 2 root root 4096 Feb 3 04:32 /mnt/pitdata/configs drwxr-xr-x 14 root root 4096 Feb 3 07:26 /mnt/pitdata/csm-0.7.29 -rw-r--r-- 1 root root 22159328586 Feb 2 22:18 /mnt/pitdata/csm-0.7.29.tar.gz drwxr-xr-x 4 root root 4096 Feb 3 04:25 /mnt/pitdata/data drwx------ 2 root root 16384 Jan 28 15:41 /mnt/pitdata/lost+found drwxr-xr-x 5 root root 4096 Feb 3 04:20 /mnt/pitdata/prep drwxr-xr-x 2 root root 4096 Jan 28 16:07 /mnt/pitdata/static Be kind, unmount the USB before ejecting it:\nncn-m001# umount -v /mnt/cow /mnt/pitdata Accessing CSI from a USB or RemoteISO CSI is not installed on the NCNs, however it is compiled against the same base architecture and OS. Therefore, it can be accessed by any LiveCD ISO file if not the one used for the original installation.\nSet the CSM Release\nncn# export CSM_RELEASE=csm-x.y.z Make directories.\nncn# mkdir -pv /mnt/livecd /mnt/rootfs /mnt/sqfs /mnt/pitdata Mount the rootfs.\nncn# mount -vL PITDATA /mnt/pitdata ncn# mount -v /mnt/pitdata/${CSM_RELEASE}/cray-pre-install-toolkit-*.iso /mnt/livecd/ ncn# mount -v /mnt/livecd/LiveOS/squashfs.img /mnt/sqfs/ ncn# mount -v /mnt/sqfs/LiveOS/rootfs.img /mnt/rootfs/ Invoke CSI usage to validate it runs and is ready for use:\nncn# /mnt/rootfs/usr/bin/csi --help Copy the CSI binary and CSM workaround documentation off to tmp/\nncn# cp -pv /mnt/rootfs/usr/bin/csi /tmp/csi Enable NCN Disk Wiping Safeguard For more information about the safeguard, see /usr/share/doc/metal-dracut/mdsquash on any NCN. (view $(rpm -qi --fileprovide dracut-metal-mdsquash | grep -i readme)).\nAfter all the NCNs have been installed, it is imperative to disable the automated wiping of disks so subsequent boots do not destroy any data unintentionally. First follow the procedure above to remount the assets and then get a new token:\nncn-m001# export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Followed by a call to CSI to update BSS:\nncn-m001# /tmp/csi handoff bss-update-param --set metal.no-wipe=1 NOTE /tmp/csi will delete itself on the next reboot since /tmp/ is mounted as tmpfs and does not persist no matter what.\n"
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/provisioning_a_liquid-cooled_ex_cabinet_cec_with_default_credentials/",
	"title": "Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials",
	"tags": [],
	"description": "",
	"content": "Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials This procedure provisions a Glibc compatible SHA-512 administrative password hash to a cabinet environmental controller (CEC). This password becomes the Redfish default global credential to access the CMM controllers and node controllers (BMCs).\nThis procedure does not provision Slingshot switch BMCs. Slingshot switch BMC default credentials must be changed using the procedures in the Slingshot product documentation. To update Slingshot switch BMCs, refer to \u0026ldquo;Change Rosetta Login and Redfish API Credentials\u0026rdquo; in the Slingshot Operations Guide (\u0026gt;1.6.0).\nPrerequisites The administrator must have physical access to the CEC LCD panel to enable privileged command mode. The CEC does not enable users to set, display, or clear the password hash in restricted command mode.\nAn laptop with terminal software such as Netcat nc, telnet, or PuTTY that supports 10/100 IPv6 Ethernet connectivity to the CEC Ethernet port is required.\nA customer-generated hash for the CEC credentials:\nThe passhash tool that is installed on the CMMs can be used to generate a SHA-512 password hash. This HPE tool is provided for convenience, but any tool that generates an SHA-512 hash that is compatible with glibc can be used. The salt portion must be between 8 and 16 chars inclusive. The CEC does not support the optional \u0026ldquo;rounds=\u0026rdquo; parameter in the hash. See the man 3 crypt page for a description: https://man7.org/linux/man-pages/man3/crypt.3.html remote# passhash PASSWORD $6$v5YlqxKB$scBci... Note: The password hash hash in this example is truncated to prevent setting the hash as shown. The generated password hash is a SHA-512 hash.\nProcedure Disconnect the CEC Ethernet cable from the Ethernet port.\nConnect an Ethernet cable from an Apple Mac or Linux laptop to the CEC Ethernet port. The CEC Ethernet PHY will auto negotiate to either 10/100Mb speed and it supports auto crossover functionality. Any standard Ethernet patch cord should work for this.\nUse the Right Arrow on the display controls to select the CEC Network Settings Menu. The IPv6 link local address is displayed on this menu.\nStart the terminal program and use Netcat (nc) or telnet to connect to CEC command shell and provide the CEC IPv6 link local address.\n# nc -t -6 \u0026#39;fe80::a1:3e8:0%en14\u0026#39; 23 # telnet fe80::a1:3e8:0%eth0 en14 and eth0 in these examples are the Ethernet interfaces for the laptop.\nEnter return a few times to start the connection.\nNOTE: If the network connection to the CEC is lost, or if a CEC command does not return to the prompt, it may be necessary to reboot the CEC. Use the Right Arrow on the CEC control panel to display the Action menu, select Reset CEC, and press the green checkmark button to reboot the CEC. Then re-establish the nc or telnet connection.\nFrom the CEC\u0026gt; prompt, enter help to view the list of commands.\nCEC\u0026gt; help CAUTION: Run only the CEC commands in this procedure. Do not change other CEC settings.\nFrom the CEC\u0026gt; prompt, generate an unlock token for the CEC. Use the enable command (alias for unlock command) without arguments to display a random unlock token on the CEC front panel.\nCEC\u0026gt; enable ab12903c Record the unlock token displayed on the CEC front panel.\nThe unlock code is valid as long as the remote shell connection is open to the CEC. If you enter the unlock token incorrectly, a new unlock token is displayed on the front panel.\nEnter the enable command again but supply the token as an argument to unlock the CEC and enter privileged command mode.\nCEC\u0026gt; enable AB12903C EXE\u0026gt; If the token code is typed in incorrectly a new one is generated on screen. When unlocked, the LCD screen displays UNLOCKED and the shell prompt changes to EXE\u0026gt;.\nDo not use the get_hash command to display the password hash. If there is no password hash set, this command will not return to the prompt and the connection will be lost.\nEnter set_hash and provide the password hash value as the argument.\nThe CEC validates the input syntax of the hash. Adding an extra char or omitting a character is flagged as an error. I a character is changed, the password entered in the serial console login shell or the Redfish root account will not work. If that happens, rerun the set_hash command on the CEC and reboot the CMMs.\nEXE\u0026gt; set_hash $6$v5YlqxKB$scBci... Note: The password has in this example has been truncated to prevent accidental setting of production password hash to example values. The password hash is a SHA-512 hash.\nExit privileged command mode.\nEXE\u0026gt; lock CEC\u0026gt; The CEC remains in privileged mode until it is reset with the lock command or if the X button on the CEC front panel is pressed. Typing exit or terminating the connection exits privileged mode. There is no connection timeout.\nUse the front panel Right Arrow to select the CEC Action menu.\nReset the CMMs 3, 2, 1, and 0.\nThe Reset CMM commands reboot either the even numbered, or odd numbered CMMs in the cabinet, depending on which CEC is issuing the commands.\nImportant!: Power cycle the compute blade slots in each chassis.\nIf Cray System Management (CSM) is provisioned, use CAPMC to power cycle the compute blade slots (example show cabinets 1000-1003). Note: If a chassis is not fully populated, specify each slot individually: ncn-m001# cray capmc xname_off create --xnames x[1000-1003]c[0-7]s[0-7] --format json Check the power status:\nncn-m001# cray capmc get_xname_status create --xnames x[1000-1003]c[0-7] --format json Power on the compute chassis slots:\nncn-m001# cray capmc xname_on create --xnames x[1000-1003]c[0-7]s[0-7] --format json If the cabinet has not been provisioned with CSM or other management software (bare-metal), the compute chassis slots are most likely powered off. To perform chassis power control operations, SSH to a CMM and and use the redfish -h command to display the power control commands:\n\u0026gt; ssh root@x9000c1 x9000c1:\u0026gt; redfish -h \u0026#34;redfish\u0026#34; -- redfish API debugging tool \u0026lt;snip\u0026gt; redfish chassis status redfish chassis power [on|off|forceoff] redfish [blade|perif] [0-7] [on|off|forceoff] redfish node status redfish node [0-1] [on|off|forceoff] \u0026lt;snip\u0026gt; x9000c1:\u0026gt; To test the password, connect to the CMM serial console though the CEC. The IPv6 address is the same, but the port numbers are different as described below.\n#!/bin/bash trap \u0026#34;stty sane \u0026amp;\u0026amp; echo \u0026#39;\u0026#39;\u0026#34; EXIT stty -icanon -echo nc -6 \u0026#39;fe80::a1:2328:0%en14\u0026#39; 50000 The even numbered CEC manages the CMM serial console for chassis 0, 2, 4, 6 on TCP port numbers 50000-50003 respectively. The odd numbered CEC manages the CMM serial console for chassis 1, 3, 5, 7 on TCP port numbers 50000-50003 respectively. If using the script shown in the example to connect to the CMM console, type exit to return to the CMM login prompt and enter ctrl-c to close the console connection. Perform this procedure for each CEC in all system cabinets.\nHPE Cray EX3000 and EX4000 cabinets have two CECs per cabinet. HPE Cray EX2000 cabinets have a single CEC per cabinet. "
},
{
	"uri": "/docs-csm/en-09/008-csm-validation/",
	"title": "CSM Install Validation and Health Checks",
	"tags": [],
	"description": "",
	"content": "CSM Install Validation and Health Checks This page lists available CSM install and health checks that can be executed to validate the CSM install.\nPlatform Health Checks ncnHealthChecks ncnPostgresHealthChecks Clock Skew BGP Peering Status and Reset Network Health Checks KEA / DHCP External DNS Spire Agent Vault Cluster Automated Goss Testing Hardware Management Services Tests Cray Management Services Validation Utility Booting CSM Barebones Image UAS/UAI Tests Examples of when you may wish to run them are:\nafter install.sh completes (not before) before and after NCN reboots after the system is brought back up any time there is unexpected behavior observed in order to provide relevant information to support tickets The areas should be tested in the order they are listed on this page. Errors in an earlier check may cause errors in later checks due to dependencies.\nPlatform Health Checks These scripts do not verify results. The script output includes analysis needed to determine pass/fail for each check. All platform health checks are expected to pass.\nPlatform health check scripts can be run:\nafter install.sh has completed successfully (not before) before and after an NCN reboots after the system or a single node goes down unexpectedly after the system is gracefully shut down and brought up any time there is unexpected behavior on the system, in order to get a baseline of data for CSM services and components. This can provide relevant information to include in support tickets that are opened Platform health check scripts can be found and run on any worker or master node from any directory.\nncnHealthChecks ncn-mw# /opt/cray/platform-utils/ncnHealthChecks.sh The ncnHealthChecks.sh script reports the following health information:\nKubernetes status for master and worker NCNs Ceph health status Health of etcd clusters Number of pods on each worker node for each etcd cluster List of automated etcd backups for the Boot Orchestration Service (BOS), Boot Script Service (BSS), Compute Rolling Upgrade Service (CRUS), and Domain Name Service (DNS) Management node uptimes Pods yet to reach the running state. Any pods reported here should be investigated, other than the exceptions listed in the notes below Execute the ncnHealthChecks.sh script and analyze the output of each individual check.\nNotes:\nThe cray-crus- pod is expected to be in the Init state until slurm and munge are installed. In particular, this will be the case if you are executing this as part of the validation after completing the CSM Platform Install. If in doubt, you can validate the CRUS service using the CMS Validation Tool. If the CRUS check passes using that tool, you do not need to worry about the cray-crus- pod state.\nIf the script output indicates any kube-multus-ds- pods are in a Terminating state, that can indicate a previous restart of these pods did not complete. In this case, it is safe to force delete these pods in order to let them properly restart by executing the kubectl delete po -n kube-system kube-multus-ds.. --force command. After executing this command, re-running the ncnHealthChecks script should indicate a new pod is in a Running state.\nncnPostgresHealthChecks ncn-mw# /opt/cray/platform-utils/ncnPostgresHealthChecks.sh For each postgres cluster, the ncnPostgresHealthChecks.sh script determines the leader pod and then reports the status of all postgres pods in the cluster.\nExecute the ncnPostgresHealthChecks.sh script. Verify the leader for each cluster and status of cluster members.\nFor a particular postgres cluster, the expected output is similar to the following:\n--- patronictl, version 1.6.5, list for services leader pod cray-sls-postgres-0 --- + Cluster: cray-sls-postgres (6938772644984361037) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +---------------------+------------+--------+---------+----+-----------+ | cray-sls-postgres-0 | 10.47.0.35 | Leader | running | 1 | | | cray-sls-postgres-1 | 10.36.0.33 | | running | 1 | 0 | | cray-sls-postgres-2 | 10.44.0.42 | | running | 1 | 0 | +---------------------+------------+--------+---------+----+-----------+ Check the leader pod\u0026rsquo;s log output for its status as the leader. Such as:\ni am the leader with the lock For example:\n--- Logs for services Leader Pod cray-sls-postgres-0 --- ERROR: get_cluster INFO: establishing a new patroni connection to the postgres cluster INFO: initialized a new cluster INFO: Lock owner: cray-sls-postgres-0; I am cray-sls-postgres-0 INFO: Lock owner: None; I am cray-sls-postgres-0 INFO: no action. i am the leader with the lock INFO: No PostgreSQL configuration items changed, nothing to reload. INFO: postmaster pid=87 INFO: running post_bootstrap INFO: trying to bootstrap a new cluster Errors reported previous to the lock status, such as ERROR: get_cluster can be ignored.\nIf any of the postgres clusters are exhibiting the following symptoms, then follow then \u0026ldquo;Troubleshoot Postgres Databases with the Patroni Tool\u0026rdquo; procedure in the HPE Cray EX System Administration Guide S-8001:\nThe cluster has no leader. The number of cluster members is not correct. There should be three cluster members (with the exception of sma-postgres-cluster where there should be only two cluster members). Cluster members are found to be in a non running state (such as start failed). Cluster members are found to have lag or lag is unknown. See see the About Postgres section in the HPE Cray EX System Administration Guide S-8001 for further information.\nClock Skew Verify that NCNs clocks are synced (these commands should be run on ncn-m001 unless otherwise noted).\nncn-m001# pdsh -b -S -w \u0026#34;$(grep -oP \u0026#39;ncn-\\w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;)\u0026#34; \u0026#39;date\u0026#39; If there is skew (output from the above command shows different times across the NCNs), the system may be suffering from a bug where ncn-m001 uses itself as its own upstream NTP server. The following will check if this is the case:\nncn-m001# grep server /etc/chrony.d/cray.conf If the output from the above command returns ncn-m001, correct the server in the command below (the example below assumes time.nist.gov is the correct server; substitute the appropriate NTP server for your environment, if it is different):\nncn-m001# upstream_ntp_server=time.nist.gov ncn-m001# sed -i \u0026#34;s/^\\(server ncn-m001\\).*/server $upstream_ntp_server iburst trust/\u0026#34; /etc/chrony.d/cray.conf Restart chronyd\nncn-m001# systemctl restart chronyd Check skew again\nncn-m001# pdsh -b -S -w \u0026#34;$(grep -oP \u0026#39;ncn-\\w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;)\u0026#34; \u0026#39;date\u0026#39; If the skew is large (more than a few seconds) it is usually recommended to allow the nodes to come back in sync gradually. However, you can opt to force them to sync by running this on the affected nodes:\nncn# chronyc burst 4/4 ; sleep 15 ncn# chronyc makestep After clocks have been synced, it may be necessary to restart Ceph services. Check Ceph health:\nncn-m001# ceph -s If the output from the above command notes a storage node has clock skew, restart the following services on the storage node mentioned in the output:\nncn-s# systemctl restart ceph-mon@$(hostname) \u0026amp;\u0026amp; systemctl restart ceph-mgr@$(hostname) \u0026amp;\u0026amp; systemctl restart ceph-mds@$(hostname) BGP Peering Status and Reset Verify that Border Gateway Protocol (BGP) peering sessions are established for each worker node on the system.\nCheck the Border Gateway Protocol (BGP) status on the Aruba/Mellanox switches. Verify that all sessions are in an Established state. If the state of any session in the table is Idle, reset the BGP sessions.\nOn an NCN, determine the IP addresses of switches:\nncn# kubectl get cm config -n metallb-system -o yaml | head -12 Expected output looks similar to the following:\napiVersion: v1 data: config: | peers: - peer-address: 10.252.0.2 peer-asn: 65533 my-asn: 65533 - peer-address: 10.252.0.3 peer-asn: 65533 my-asn: 65533 address-pools: - name: customer-access Using the first peer-address (10.252.0.2 here), ssh as admin to the first switch and note in the returned output if a Mellanox or Aruba switch is indicated.\nncn# ssh admin@10.252.0.2 On a Mellanox switch, you may see Mellanox Onyx Switch Management or Mellanox Switch after logging in to the switch with ssh. In this case, proceed to the Mellanox steps. On an Aruba switch, you may see Please register your products now at: https://asp.arubanetworks.com after logging in to the switch with ssh. In this case, proceed to the Aruba steps. Mellanox Switch Enable:\nsw-spine-001# enable Verify BGP is enabled:\nsw-spine-001# show protocols | include bgp Expected output looks similar to the following:\nbgp: enabled Check peering status:\nsw-spine-001# show ip bgp summary Expected output looks similar to the following:\nVRF name : default BGP router identifier : 10.252.0.2 local AS number : 65533 BGP table version : 3 Main routing table version: 3 IPV4 Prefixes : 59 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.1.10 4 65533 2945 3365 3 0 0 1:00:21:33 ESTABLISHED/20 10.252.1.11 4 65533 2942 3356 3 0 0 1:00:20:49 ESTABLISHED/19 10.252.1.12 4 65533 2945 3363 3 0 0 1:00:21:33 ESTABLISHED/20 If one or more BGP session is reported in an Idle state, reset BGP to re-establish the sessions:\nsw-spine-001# clear ip bgp all It may take several minutes for all sessions to become Established. Wait a minute, or so, and then verify that all sessions now are all reported as Established. If some sessions remain in an Idle state, re-run the clear ip bgp all command and check again.\nIf after several tries (around 10 attempts) one or more BGP session remains Idle, see Check BGP Status and Reset Sessions, in the HPE Cray EX Administration Guide S-8001.\nRepeat the above Mellanox procedure using the second peer-address (10.252.0.3 here)\nAruba Switch On an Aruba switch, the prompt may include sw-spine or sw-agg.\nCheck BGP peering status\nsw-agg01# show bgp ipv4 unicast summary Expected output looks similar to the following:\nVRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.4 Peers : 7 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.5 65533 19579 19588 20h:40m:30s Established Up 10.252.1.7 65533 34137 39074 20h:41m:53s Established Up 10.252.1.8 65533 34134 39036 20h:36m:44s Established Up 10.252.1.9 65533 34104 39072 00m:01w:04d Established Up 10.252.1.10 65533 34105 39029 00m:01w:04d Established Up 10.252.1.11 65533 34099 39042 00m:01w:04d Established Up 10.252.1.12 65533 34101 39012 00m:01w:04d Established Up If one or more BGP session is reported in a Idle state, reset BGP to re-establish the sessions:\nsw-agg01# clear bgp * It may take several minutes for all sessions to become Established. Wait a minute or so, and then verify that all sessions now are reported as Established. If some sessions remain in an Idle state, re-run the clear bgp * command and check again.\nIf after several tries one or more BGP session remains Idle, see Check BGP Status and Reset Sessions, in the HPE Cray EX Administration Guide S-8001.\nRepeat the above Aruba procedure using the second peer-address (10.252.0.5 in this example)\nNetwork Health Checks Verify that KEA has active DHCP leases Verify that KEA has active DHCP leases. Right after an fresh install of CSM it is important to verify that KEA is currently handing out DHCP leases on the system. The following commands can be run on any of the master or worker nodes.\nGet an API Token:\nncn-mw# export TOKEN=$(curl -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth \\ -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Retrieve all the Leases currently in KEA:\nncn-mw# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api_gw_service.local/apis/dhcp-kea | jq If there is an non-zero amount of DHCP leases for river hardware returned that is a good indication that KEA is working.\nVerify ability to resolve external DNS If you have configured unbound to resolve outside hostnames, then the following check should be performed. If you have not done this, then this check may be skipped.\nRun the following on one of the master or worker nodes (not the PIT node):\nncn# nslookup cray.com ; echo \u0026#34;Exit code is $?\u0026#34; Expected output looks similar to the following:\nServer: 10.92.100.225 Address: 10.92.100.225#53 Non-authoritative answer: Name: cray.com Address: 52.36.131.229 Exit code is 0 Verify that the command has exit code 0, reports no errors, and resolves the address.\nVerify Spire Agent is Running on Kubernetes NCNs Execute the following command on all Kubernetes NCNs (i.e. all master and worker nodes) excluding the PIT:\nncn-mw# goss -g /opt/cray/tests/install/ncn/tests/goss-spire-agent-service-running.yaml validate Known Issue: Verify spire-agent is enabled and running The spire-agent service may fail to start on Kubernetes NCNs. You can see symptoms of this in its logs using journalctl. It may log errors similar to join token does not exist or has already been used, or the last logs may contain multiple lines of systemd[1]: spire-agent.service: Start request repeated too quickly..\nDeleting the request-ncn-join-token daemonset pod running on the node may clear the issue. While the spire-agent systemctl service on the Kubernetes node should eventually restart cleanly, you may have to login to the impacted nodes and restart the service. The following recovery procedure can be run from any Kubernetes node in the cluster.\nSet NODE to the NCN which is experiencing the issue. In this example, ncn-w002.\nncn-mw# NODE=ncn-w002 Define the following function\nncn-mw# function renewncnjoin() { for pod in $(kubectl get pods -n spire |grep request-ncn-join-token | awk \u0026#39;{print $1}\u0026#39;); do if kubectl describe -n spire pods $pod | grep -q \u0026#34;Node:.*$1\u0026#34;; then echo \u0026#34;Restarting $pod running on $1\u0026#34;; kubectl delete -n spire pod \u0026#34;$pod\u0026#34; fi done } Run the function as follows:\nncn-mw# renewncnjoin $NODE Repeat the original goss test on the affected NCN to verify that it passes.\nIf the test still fails, it may be a case where the spire-agent service failed after an NCN was powered off for too long and its tokens expired. If this applies to the affected NCN, you may try the following:\nRun the following command on the affected NCN:\nncn-mw# rm -v /root/spire/agent_svid.der /root/spire/bundle.der /root/spire/data/svid.key Perform steps 1-4 again.\nVerify the Vault Cluster is Healthy Execute the following commands on ncn-m002:\nncn-m002# goss -g /opt/cray/tests/install/ncn/tests/goss-k8s-vault-cluster-health.yaml validate Check the output to verify no failures are reported:\nCount: 2, Failed: 0, Skipped: 0 Automated Goss Testing NOTE: The tests in this section must be run on the PIT node. Do not run them from any other node. If you have already redeployed your PIT node to ncn-m001, skip this section.\nThere are multiple Goss test suites available that cover a variety of sub-systems.\nYou can execute the general NCN test suite via:\npit# /opt/cray/tests/install/ncn/automated/ncn-run-time-checks And the Kubernetes test suite via:\npit# /opt/cray/tests/install/ncn/automated/ncn-kubernetes-checks Known Test Issues These tests can only reliably be executed from the PIT node. Should be addressed in a future release. K8S Test: Kubernetes Query BSS Cloud-init for ca-certs May fail immediately after platform install. Should pass after the TrustedCerts Operator has updated BSS (Global cloud-init meta) with CA certificates. K8S Test: Kubernetes Velero No Failed Backups Due to a known issue with Velero, a backup may be attempted immediately upon the deployment of a backup schedule (for example, vault). It may be necessary to use the velero command to delete backups from a Kubernetes node to clear this situation. K8S Test: sdc Drive Test There is a known test issue that can produce false failures of this test. If it fails, perform the Manual LVM Check Procedure. Hardware Management Services Tests Execute the HMS smoke and functional tests after the CSM install to confirm that the HMS services are running and operational.\nTest Execution These tests should be executed as root on any worker or master NCN (but not the PIT node).\nRun the HMS smoke tests.\nncn-mw# /opt/cray/tests/ncn-resources/hms/hms-test/hms_run_ct_smoke_tests_ncn-resources.sh The final lines of output should indicate if there were any test failures. If that is the case, the full output should be examined.\nIf no failures occur, then run the HMS functional tests.\nncn-mw# /opt/cray/tests/ncn-resources/hms/hms-test/hms_run_ct_functional_tests_ncn-resources.sh The final lines of output should indicate if there were any test failures. If that is the case, the full output should be examined.\nKnown Issues: HMS Functional Tests Erroneous HSM Warning Flags on Mountain BMCs (SDEVICE-3319) Locked HSM Components (CASMHMS-4664) Empty Drive Bays in HSM (CASMHMS-4693) Unexpected HSM Discovery Status (CASMHMS-4794) Previously Captured FAS Snapshots (CASMHMS-5065) BMCs Set to \u0026ldquo;On\u0026rdquo; State in HSM (CASMHMS-5239) Erroneous HSM Warning Flags on Mountain BMCs (SDEVICE-3319) The HMS functional tests include a check for unexpected flags that may be set in Hardware State Manager (HSM) for the BMCs on the system. There is a known issue SDEVICE-3319 that can cause Warning flags to be set erroneously in HSM for Mountain BMCs and result in test failures.\nThe following HMS functional test may fail due to this issue:\ntest_smd_components_ncn-functional_remote-functional.tavern.yaml The symptom of this issue is the test fails with error messages about Warning flags being set on one or more BMCs. It may look similar to the following in the test output:\n=================================== FAILURES =================================== _ /opt/cray/tests/ncn-functional/hms/hms-smd/test_smd_components_ncn-functional_remote-functional.tavern.yaml::Ensure that we can conduct a query for all Node BMCs in the Component collection _ Errors: E tavern.util.exceptions.TestFailError: Test \u0026#39;Verify the expected response fields for all NodeBMCs\u0026#39; failed: - Error calling validate function \u0026#39;\u0026lt;function validate_pykwalify at 0x7f44666179d0\u0026gt;\u0026#39;: Traceback (most recent call last): File \u0026#34;/usr/lib/python3.8/site-packages/tavern/schemas/files.py\u0026#34;, line 106, in verify_generic verifier.validate() File \u0026#34;/usr/lib/python3.8/site-packages/pykwalify/core.py\u0026#34;, line 166, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Enum \u0026#39;Warning\u0026#39; does not exist. Path: \u0026#39;/Components/9/Flag\u0026#39;. - Enum \u0026#39;Warning\u0026#39; does not exist. Path: \u0026#39;/Components/10/Flag\u0026#39;. - Enum \u0026#39;Warning\u0026#39; does not exist. Path: \u0026#39;/Components/11/Flag\u0026#39;. - Enum \u0026#39;Warning\u0026#39; does not exist. Path: \u0026#39;/Components/12/Flag\u0026#39;. - Enum \u0026#39;Warning\u0026#39; does not exist. Path: \u0026#39;/Components/13/Flag\u0026#39;. - Enum \u0026#39;Warning\u0026#39; does not exist. Path: \u0026#39;/Components/14/Flag\u0026#39;.: Path: \u0026#39;/\u0026#39;\u0026gt; If you see this, perform the following steps:\nRetrieve the xnames of all Mountain BMCs with Warning flags set in HSM:\nncn-mw# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://api-gw-service-nmn.local/apis/smd/hsm/v1/State/Components?Type=NodeBMC\\\u0026amp;Class=Mountain\\\u0026amp;Flag=Warning | jq \u0026#39;.Components[] | { ID: .ID, Flag: .Flag, Class: .Class }\u0026#39; -c | sort -V | jq -c {\u0026#34;ID\u0026#34;:\u0026#34;x5000c1s0b0\u0026#34;,\u0026#34;Flag\u0026#34;:\u0026#34;Warning\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;Mountain\u0026#34;} {\u0026#34;ID\u0026#34;:\u0026#34;x5000c1s0b1\u0026#34;,\u0026#34;Flag\u0026#34;:\u0026#34;Warning\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;Mountain\u0026#34;} {\u0026#34;ID\u0026#34;:\u0026#34;x5000c1s1b0\u0026#34;,\u0026#34;Flag\u0026#34;:\u0026#34;Warning\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;Mountain\u0026#34;} {\u0026#34;ID\u0026#34;:\u0026#34;x5000c1s1b1\u0026#34;,\u0026#34;Flag\u0026#34;:\u0026#34;Warning\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;Mountain\u0026#34;} {\u0026#34;ID\u0026#34;:\u0026#34;x5000c1s2b0\u0026#34;,\u0026#34;Flag\u0026#34;:\u0026#34;Warning\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;Mountain\u0026#34;} {\u0026#34;ID\u0026#34;:\u0026#34;x5000c1s2b1\u0026#34;,\u0026#34;Flag\u0026#34;:\u0026#34;Warning\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;Mountain\u0026#34;} For each Mountain BMC xname, check its Redfish BMC Manager status:\nncn-mw# curl -s -k -u root:${BMC_PASSWORD} https://x5000c1s0b0/redfish/v1/Managers/BMC | jq \u0026#39;.Status\u0026#39; { \u0026#34;Health\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;State\u0026#34;: \u0026#34;Online\u0026#34; } Test failures and HSM Warning flags for Mountain BMCs with the Redfish BMC Manager status shown above can be safely ignored.\nLocked HSM Components (CASMHMS-4664) The following HMS functional tests may fail due to known issue CASMHMS-4664 because of locked components in HSM:\ntest_bss_bootscript_ncn-functional_remote-functional.tavern.yaml test_smd_components_ncn-functional_remote-functional.tavern.yaml The symptom of this issue looks similar to:\nTraceback (most recent call last): File \u0026#34;/usr/lib/python3.8/site-packages/tavern/schemas/files.py\u0026#34;, line 106, in verify_generic verifier.validate() File \u0026#34;/usr/lib/python3.8/site-packages/pykwalify/core.py\u0026#34;, line 166, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/0\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/5\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/6\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/7\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/8\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/9\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/10\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/11\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/12\u0026#39;.: Path: \u0026#39;/\u0026#39;\u0026gt; Failures of these tests because of locked components as shown above can be safely ignored.\nEmpty Drive Bays in HSM (CASMHMS-4693) The following HMS functional test may fail due to known issue CASMHMS-4693 because of empty drive bays in HSM:\ntest_smd_hardware_ncn-functional_remote-functional.tavern.yaml This issue looks similar to the following in the test output:\nTraceback (most recent call last): File \u0026#34;/usr/lib/python3.8/site-packages/tavern/schemas/files.py\u0026#34;, line 106, in verify_generic verifier.validate() File \u0026#34;/usr/lib/python3.8/site-packages/pykwalify/core.py\u0026#34;, line 166, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Cannot find required key \u0026#39;PopulatedFRU\u0026#39;. Path: \u0026#39;/Nodes/0/Drives/3\u0026#39;. - Cannot find required key \u0026#39;PopulatedFRU\u0026#39;. Path: \u0026#39;/Nodes/0/Drives/4\u0026#39;. - Cannot find required key \u0026#39;PopulatedFRU\u0026#39;. Path: \u0026#39;/Nodes/0/Drives/5\u0026#39;. - Cannot find required key \u0026#39;PopulatedFRU\u0026#39;. Path: \u0026#39;/Nodes/0/Drives/6\u0026#39;. - Cannot find required key \u0026#39;PopulatedFRU\u0026#39;. Path: \u0026#39;/Nodes/0/Drives/7\u0026#39;.: Path: \u0026#39;/\u0026#39;\u0026gt; Failures of this test because of empty drive bays as shown above can be safely ignored.\nUnexpected HSM Discovery Status (CASMHMS-4794) The following HMS functional test may fail due to known issue CASMHMS-4794 because of an unexpected discovery status in HSM:\ntest_smd_discovery_status_ncn-functional_remote-functional.tavern.yaml This issue looks similar to the following in the test output:\nTraceback (most recent call last): File \u0026#34;/usr/lib/python3.8/site-packages/tavern/schemas/files.py\u0026#34;, line 106, in verify_generic verifier.validate() File \u0026#34;/usr/lib/python3.8/site-packages/pykwalify/core.py\u0026#34;, line 166, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Value \u0026#39;NotStarted\u0026#39; does not match pattern \u0026#39;Complete\u0026#39;. Path: \u0026#39;/0/Status\u0026#39;. - Key \u0026#39;Details\u0026#39; was not defined. Path: \u0026#39;/0\u0026#39;.: Path: \u0026#39;/\u0026#39;\u0026gt; Failures of this test because of an unexpected discovery status as shown above can be safely ignored.\nPreviously Captured FAS Snapshots (CASMHMS-5065) The following HMS functional test may fail due to known issue CASMHMS-5065 because of FAS snapshots that may have been previously captured on the system:\ntest_fas_snapshots_ncn-functional_remote-functional.tavern.yaml ERROR tavern.schemas.files:files.py:108 Error validating {\u0026#39;snapshots\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;fasTestSnapshot\u0026#39;, \u0026#39;captureTime\u0026#39;: \u0026#39;2021-10-25 21:29:00.139366661 +0000 UTC\u0026#39;, \u0026#39;ready\u0026#39;: True, \u0026#39;relatedActions\u0026#39;: [], \u0026#39;uniqueDeviceCount\u0026#39;: 68}, {\u0026#39;name\u0026#39;: \u0026#39;testSnap\u0026#39;, \u0026#39;captureTime\u0026#39;: \u0026#39;2021-05-04 17:16:33.050058886 +0000 UTC\u0026#39;, \u0026#39;ready\u0026#39;: True, \u0026#39;relatedActions\u0026#39;: [], \u0026#39;uniqueDeviceCount\u0026#39;: 68}, {\u0026#39;name\u0026#39;: \u0026#39;testSnap2\u0026#39;, \u0026#39;captureTime\u0026#39;: \u0026#39;2021-05-04 17:20:33.132276731 +0000 UTC\u0026#39;, \u0026#39;ready\u0026#39;: True, \u0026#39;relatedActions\u0026#39;: [], \u0026#39;uniqueDeviceCount\u0026#39;: 68}, {\u0026#39;name\u0026#39;: \u0026#39;x1000c5s5\u0026#39;, \u0026#39;captureTime\u0026#39;: \u0026#39;2021-08-16 20:09:20.641734739 +0000 UTC\u0026#39;, \u0026#39;expirationTime\u0026#39;: \u0026#39;2021-12-31 02:35:54 +0000 UTC\u0026#39;, \u0026#39;ready\u0026#39;: True, \u0026#39;relatedActions\u0026#39;: [], \u0026#39;uniqueDeviceCount\u0026#39;: 2}, {\u0026#39;name\u0026#39;: \u0026#39;x3000c0s20b4n0\u0026#39;, \u0026#39;captureTime\u0026#39;: \u0026#39;2021-07-23 04:26:43.456013148 +0000 UTC\u0026#39;, \u0026#39;expirationTime\u0026#39;: \u0026#39;2021-10-31 02:35:54 +0000 UTC\u0026#39;, \u0026#39;ready\u0026#39;: True, \u0026#39;relatedActions\u0026#39;: [], \u0026#39;uniqueDeviceCount\u0026#39;: 0}]} Failures of this test caused by snapshots other than \u0026lsquo;fasTestSnapshot\u0026rsquo; can be safely ignored.\nBMCs Set to \u0026ldquo;On\u0026rdquo; State in HSM (CASMHMS-5239) The following HMS functional test may fail due to known issue CASMHMS-5239 because of CMMs setting BMC states to \u0026ldquo;On\u0026rdquo; instead of \u0026ldquo;Ready\u0026rdquo; in HSM:\ntest_smd_components_ncn-functional_remote-functional.tavern.yaml This issue looks similar to the following in the test output:\nTraceback (most recent call last): verifier.validate() File \u0026#34;/usr/lib/python3.8/site-packages/pykwalify/core.py\u0026#34;, line 166, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Enum \u0026#39;On\u0026#39; does not exist. Path: \u0026#39;/Components/9/State\u0026#39;. - Enum \u0026#39;On\u0026#39; does not exist. Path: \u0026#39;/Components/10/State\u0026#39;.: Path: \u0026#39;/\u0026#39;\u0026gt; Failures of this test caused by BMCs in the \u0026ldquo;On\u0026rdquo; state can be safely ignored.\nCray Management Services Validation Utility Usage Interpreting Results Checks To Run Usage /usr/local/bin/cmsdev test [-q | -v] \u0026lt;shortcut\u0026gt;\nThe shortcut determines which component will be tested. See the table in the next section for the list of shortcuts. The tool logs to /opt/cray/tests/cmsdev.log The -q (quiet) and -v (verbose) flags can be used to decrease or increase the amount of information sent to the screen. The same amount of data is written to the log file in either case. Interpreting Results If a test passes: The last line of output from the tool reports SUCCESS The return code is 0. If a test fails: The last line of output from the tool reports FAILURE The return code is non-0. Unless the test was run in verbose mode, the log file will contain additional information about the execution. Checks To Run You should run a check for each of the following services after an install. These should be executed as root on any worker or master NCN (but not the PIT node).\nServices Shortcut BOS (Boot Orchestration Service) bos CFS (Configuration Framework Service) cfs ConMan (Console Manager) conman CRUS (Compute Rolling Upgrade Service) crus IMS (Image Management Service) ims iPXE, TFTP (Trivial File Transfer Protocol) ipxe* VCS (Version Control Service) vcs * The ipxe shortcut runs a check of both the iPXE service and the TFTP service.\nThe following is a convenient way to run all of the tests and see if they passed:\nncn-mw# for S in bos cfs conman crus ims ipxe vcs ; do LOG=/root/cmsdev-$S-$(date +\u0026#34;%Y%m%d_%H%M%S\u0026#34;).log echo -n \u0026#34;$(date) Starting $S check ... \u0026#34; START=$SECONDS /usr/local/bin/cmsdev test -v $S \u0026gt; $LOG 2\u0026gt;\u0026amp;1 rc=$? let DURATION=SECONDS-START if [ $rc -eq 0 ]; then echo \u0026#34;PASSED (duration: $DURATION seconds)\u0026#34; else echo \u0026#34;FAILED (duration: $DURATION seconds)\u0026#34; echo \u0026#34; # See $LOG for details\u0026#34; fi done Booting CSM Barebones Image Included with the Cray System Management (CSM) release is a pre-built node image that can be used to validate that core CSM services are available and responding as expected. The CSM barebones image contains only the minimal set of RPMs and configuration required to boot an image and is not suitable for production usage. To run production work loads, it is suggested that an image from the Cray OS (COS) product, or similar, be used.\nNOTES\nThe CSM Barebones image included with the Shasta 1.4 release will not successfully complete the beyond the dracut stage of the boot process. However, if the dracut stage is reached the boot can be considered successful and shows that the necessary CSM services needed to boot a node are up and available. This inability to fully boot the barebones image will be resolved in future releases of the CSM product. In addition to the CSM Barebones image, the Shasta 1.4 release also includes an IMS Recipe that can be used to build the CSM Barebones image. However, the CSM Barebones recipe currently requires RPMs that are not installed with the CSM product. The CSM Barebones recipe can be built after the Cray OS (COS) product stream is also installed on to the system. In future releases of the CSM product, work will be undertaken to resolve these dependency issues. You will need to use the CLI in order to complete these tasks. If needed, see the Initialize and Authorize the CLI section. Locate Image in IMS Create BOS Session Template Find Compute Node Reboot Node Verify Consoles Watch Boot on Console Locate the CSM Barebones Image in IMS Locate the CSM Barebones image and note the path to the image\u0026rsquo;s manifest.json in S3.\nncn# cray ims images list --format json | jq \u0026#39;.[] | select(.name | contains(\u0026#34;barebones\u0026#34;))\u0026#39; Expected output is similar to the following:\n{ \u0026#34;created\u0026#34;: \u0026#34;2021-01-14T03:15:55.146962+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;293b1e9c-2bc4-4225-b235-147d1d611eef\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;6d04c3a4546888ee740d7149eaecea68\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/293b1e9c-2bc4-4225-b235-147d1d611eef/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;cray-shasta-csm-sles15sp1-barebones.x86_64-shasta-1.4\u0026#34; } Create a BOS Session Template for the CSM Barebones Image The session template below can be copied and used as the basis for the BOS Session Template. As noted below, make sure the S3 path for the manifest matches the S3 path shown in the Image Management Service (IMS).\nCreate sessiontemplate.json\nncn# vi sessiontemplate.json The session template should contain the following:\n{ \u0026#34;boot_sets\u0026#34;: { \u0026#34;compute\u0026#34;: { \u0026#34;boot_ordinal\u0026#34;: 2, \u0026#34;etag\u0026#34;: \u0026#34;etag_value_from_cray_ims_command\u0026#34;, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=340M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell turbo_boost_limit=999 spire_join_token=${SPIRE_JOIN_TOKEN}\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;node_roles_groups\u0026#34;: [ \u0026#34;Compute\u0026#34; ], \u0026#34;path\u0026#34;: \u0026#34;path_value_from_cray_ims_command\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:nmn0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;cfs\u0026#34;: { \u0026#34;configuration\u0026#34;: \u0026#34;cos-integ-config-1.4.0\u0026#34; }, \u0026#34;enable_cfs\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;shasta-1.4-csm-bare-bones-image\u0026#34; } NOTE: Be sure to replace the values of the etag and path fields with the ones you noted earlier in the cray ims images list command.\nCreate the BOS session template using this file as input:\nncn# cray bos sessiontemplate create --file sessiontemplate.json --name shasta-1.4-csm-bare-bones-image The expected output is:\n/sessionTemplate/shasta-1.4-csm-bare-bones-image Find an available compute node ncn# cray hsm state components list --role Compute --enabled true Example output:\n[[Components]] ID = \u0026#34;x3000c0s17b1n0\u0026#34; Type = \u0026#34;Node\u0026#34; State = \u0026#34;On\u0026#34; Flag = \u0026#34;OK\u0026#34; Enabled = true Role = \u0026#34;Compute\u0026#34; NID = 1 NetType = \u0026#34;Sling\u0026#34; Arch = \u0026#34;X86\u0026#34; Class = \u0026#34;River\u0026#34; [[Components]] ID = \u0026#34;x3000c0s17b2n0\u0026#34; Type = \u0026#34;Node\u0026#34; State = \u0026#34;On\u0026#34; Flag = \u0026#34;OK\u0026#34; Enabled = true Role = \u0026#34;Compute\u0026#34; NID = 2 NetType = \u0026#34;Sling\u0026#34; Arch = \u0026#34;X86\u0026#34; Class = \u0026#34;River\u0026#34; Choose a node from those listed and set XNAME to its ID. In this example, x3000c0s17b2n0:\nncn# export XNAME=x3000c0s17b2n0 Reboot the node using your BOS session template Create a BOS session to reboot the chosen node using the BOS session template that you created:\nncn# cray bos session create --template-uuid shasta-1.4-csm-bare-bones-image --operation reboot --limit $XNAME Expected output looks similar to the following:\nlimit = \u0026#34;x3000c0s17b2n0\u0026#34; operation = \u0026#34;reboot\u0026#34; templateUuid = \u0026#34;shasta-1.4-csm-bare-bones-image\u0026#34; [[links]] href = \u0026#34;/v1/session/8f2fc013-7817-4fe2-8e6f-c2136a5e3bd1\u0026#34; jobId = \u0026#34;boa-8f2fc013-7817-4fe2-8e6f-c2136a5e3bd1\u0026#34; rel = \u0026#34;session\u0026#34; type = \u0026#34;GET\u0026#34; [[links]] href = \u0026#34;/v1/session/8f2fc013-7817-4fe2-8e6f-c2136a5e3bd1/status\u0026#34; rel = \u0026#34;status\u0026#34; type = \u0026#34;GET\u0026#34; Verify console connections Sometimes the compute nodes and UAN are not up yet when cray-conman is initialized, and consequently will not be monitored. This is a good time to verify that all nodes are being monitored for console logging and re-initialize cray-conman if needed.\nThis procedure can be run from any member of the Kubernetes cluster.\nIdentify the cray-conman pod:\nncn# kubectl get pods -n services | grep \u0026#34;^cray-conman-\u0026#34; Expected output looks similar to the following:\ncray-conman-b69748645-qtfxj 3/3 Running 0 16m Set the PODNAME variable accordingly:\nncn# export PODNAME=cray-conman-b69748645-qtfxj Log into the cray-conman container in this pod:\nncn# kubectl exec -n services -it $PODNAME -c cray-conman -- bash cray-conman# Check the existing list of nodes being monitored.\ncray-conman# conman -q Output looks similar to the following:\nx9000c0s1b0n0 x9000c0s20b0n0 x9000c0s22b0n0 x9000c0s24b0n0 x9000c0s27b1n0 x9000c0s27b2n0 x9000c0s27b3n0 If any compute nodes or UANs are not included in this list, the conman process can be re-initialized by killing the conmand process.\nIdentify the command process\ncray-conman# ps -ax | grep conmand | grep -v grep Output will look similar to:\n13 ? Sl 0:45 conmand -F -v -c /etc/conman.conf Set CONPID to the process ID from the previous command output:\ncray-conman# export CONPID=13 Kill the process:\ncray-conman# kill $CONPID This will regenerate the conman configuration file and restart the conmand process. Repeat the previous steps to verify that it now includes all nodes that are included in state manager.\nConnect to the node\u0026rsquo;s console and watch the boot Run conman from inside the conman pod to access the console. The boot will fail, but should reach the dracut stage. If the dracut stage is reached, the boot can be considered successful and shows that the necessary CSM services needed to boot a node are up and available.\ncray-conman-b69748645-qtfxj:/ # conman -j x9000c1s7b0n1 The boot is considered successful if the console output ends with something similar to the following:\n[ 7.876909] dracut: FATAL: Don\u0026#39;t know how to handle \u0026#39;root=craycps-s3:s3://boot-images/e3ba09d7-e3c2-4b80-9d86-0ee2c48c2214/rootfs:c77c0097bb6d488a5d1e4a2503969ac0-27:dvs:api-gw-service-nmn.local:300:nmn0\u0026#39; [ 7.898169] dracut: Refusing to continue [ 7.952291] systemd-shutdow: 13 output lines suppressed due to ratelimiting [ 7.959842] systemd-shutdown[1]: Sending SIGTERM to remaining processes... [ 7.975211] systemd-journald[1022]: Received SIGTERM from PID 1 (systemd-shutdow). [ 7.982625] systemd-shutdown[1]: Sending SIGKILL to remaining processes... [ 7.999281] systemd-shutdown[1]: Unmounting file systems. [ 8.006767] systemd-shutdown[1]: Remounting \u0026#39;/\u0026#39; read-only with options \u0026#39;\u0026#39;. [ 8.013552] systemd-shutdown[1]: Remounting \u0026#39;/\u0026#39; read-only with options \u0026#39;\u0026#39;. [ 8.019715] systemd-shutdown[1]: All filesystems unmounted. [ 8.024697] systemd-shutdown[1]: Deactivating swaps. [ 8.029496] systemd-shutdown[1]: All swaps deactivated. [ 8.036504] systemd-shutdown[1]: Detaching loop devices. [ 8.043612] systemd-shutdown[1]: All loop devices detached. [ 8.059239] reboot: System halted UAS / UAI Tests Initialize and Authorize CLI Stop Using CRAY_CREDENTIALS Initialize Authorize Troubleshooting Validate UAS and UAI Validate Basic UAS Installation Validate UAI Creation Troubleshooting Authorization Issues Keycloak Issues Registry Issues Missing Volumes and Other Container Startup Issues Initialize and Authorize the CLI The procedures below use the CLI as an authorized user and run on two separate node types. The first part runs on the LiveCD node while the second part runs on a non-LiveCD Kubernetes master or worker node. When using the CLI on either node, the CLI configuration needs to be initialized and the user running the procedure needs to be authorized. This section describes how to initialize the CLI for use by a user and authorize the CLI as a user to run the procedures on any given node. The procedures will need to be repeated in both stages of the validation procedure.\nStop Using the CRAY_CREDENTIALS Service Account Token Installation procedures leading up to production mode on Shasta use the CLI with a Kubernetes managed service account normally used for internal operations. There is a procedure for extracting the OAUTH token for this service account and assigning it to the CRAY_CREDENTIALS environment variable to permit simple CLI operations. The UAS / UAI validation procedure runs as a post-installation procedure and requires an actual user with Linux credentials, not this service account. Prior to running any of the steps below you must unset the CRAY_CREDENTIALS environment variable:\nncn# unset CRAY_CREDENTIALS Initialize the CLI Configuration The CLI needs to know what host to use to obtain authorization and what user is requesting authorization so it can obtain an OAUTH token to talk to the API Gateway. This is accomplished by initializing the CLI configuration. In this example, I am using the vers username. In practice, vers and the response to the password: prompt should be replaced with the username and password of the administrator running the validation procedure.\nTo check whether the CLI needs initialization, run:\nncn# cray config describe The cray config describe output may look similar to this:\n# cray config describe Your active configuration is: default [core] hostname = \u0026#34;https://api-gw-service-nmn.local\u0026#34; [auth.login] username = \u0026#34;vers\u0026#34; This means the CLI is initialized and logged in as vers. * If you are not vers you will want to authorize yourself using your username and password in the next section. * If you are vers you are ready to move on to the validation procedure on that node.\nThe cray config describe output may instead look like this:\nUsage: cray config describe [OPTIONS] Error: No configuration exists. Run `cray init` This means the CLI needs to be initialized. To do so, run the following:\nncn# cray init When prompted, remember to substitute your username instead of \u0026lsquo;vers\u0026rsquo;. Expected output (including your typed input) should look similar to the following:\nCray Hostname: api-gw-service-nmn.local Username: vers Password: Success! Initialization complete. Authorize the CLI for Your User If, when you check for an initialized CLI you find it is initialized but authorized for a user different from you, you will want to authorize the CLI for your self. Do this with the following (remembering to substitute your username and password for vers):\nncn# cray auth login Verify that the output of the command reports success. You are now authorized to use the CLI.\nTroubleshooting If initialization or authorization fails in one of the above steps, there are several common causes:\nDNS failure looking up api-gw-service-nmn.local may be preventing the CLI from reaching the API Gateway and Keycloak for authorization Network connectivity issues with the NMN may be preventing the CLI from reaching the API Gateway and Keycloak for authorization Certificate mismatch or trust issues may be preventing a secure connection to the API Gateway Istio failures may be preventing traffic from reaching Keycloak Keycloak may not yet be set up to authorize you as a user While resolving these issues is beyond the scope of this section, you may get clues to what is failing by adding -vvvvv to the cray auth... or cray init ... commands.\nValidate UAS and UAI Functionality The following procedures run on separate nodes of the Shasta system. They are, therefore, separated into separate sub-sections.\nValidate the Basic UAS Installation Make sure you are running on a booted NCN node and have initialized and authorized yourself in the CLI as described above.\nIf the cray CLI was initialized on the LiveCD PIT node, then following commands will also work on the PIT node.\nBasic UAS installation is validated using the following: 1.\nncn# cray uas mgr-info list Expected output looks similar to the following:\nservice_name = \u0026#34;cray-uas-mgr\u0026#34; version = \u0026#34;1.11.5\u0026#34; In this example output, it shows that UAS is installed and running the 1.11.5 version. 1.\nncn# cray uas list Expected output looks similar to the following:\nresults = [] This example output shows that there are no currently running UAIs. It is possible, if someone else has been using the UAS, that there could be UAIs in the list. That is acceptable too from a validation standpoint.\nVerify that the pre-made UAI images are registered with UAS\nncn# cray uas images list Expected output looks similar to the following:\ndefault_image = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; image_list = [ \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34;,] This example output shows that the pre-made end-user UAI image (cray/cray-uai-sles15sp1:latest) is registered with UAS. This does not necessarily mean this image is installed in the container image registry, but it is configured for use. If other UAI images have been created and registered, they may also show up here \u0026ndash; that is acceptable.\nValidate UAI Creation This procedure must run on a master or worker node (and not ncn-w001) on the Shasta system (or from an external host, but the procedure for that is not covered here). It requires that the CLI be initialized and authorized as you.\nIn this procedure, we will show the steps being run on ncn-w003\nVerify that you can create a UAI:\nncn-w003# cray uas create --publickey ~/.ssh/id_rsa.pub Expected output looks similar to the following:\nuai_connect_string = \u0026#34;ssh vers@10.16.234.10\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.16.234.10\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-a00fb46b\u0026#34; uai_status = \u0026#34;Pending\u0026#34; username = \u0026#34;vers\u0026#34; [uai_portmap] This has created the UAI and the UAI is currently in the process of initializing and running.\nSet UAINAME to the value of the uai_name field in the previous command output (uai-vers-a00fb46b in our example):\nncn-w003# export UAINAME=uai-vers-a00fb46b Check the current status of the UAI:\nncn-w003# cray uas list Expected output looks similar to the following:\n[[results]] uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.16.234.10\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.16.234.10\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-a00fb46b\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; If the uai_status field is Running: Ready, proceed to the next step. Otherwise, wait and repeat this command until that is the case. It normally should not take more than a minute or two.\nThe UAI is ready for use. You can then log into it using the command in the uai_connect_string field in the previous command output:\nncn-w003# ssh vers@10.16.234.10 vers@uai-vers-a00fb46b-6889b666db-4dfvn:~\u0026gt; Run a command on the UAI:\nvers@uai-vers-a00fb46b-6889b666db-4dfvn:~\u0026gt; ps -afe Expected output looks similar to the following:\nUID PID PPID C STIME TTY TIME CMD root 1 0 0 18:51 ? 00:00:00 /bin/bash /usr/bin/uai-ssh.sh munge 36 1 0 18:51 ? 00:00:00 /usr/sbin/munged root 54 1 0 18:51 ? 00:00:00 su vers -c /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 55 54 0 18:51 ? 00:00:00 /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 62 55 0 18:51 ? 00:00:00 sshd: vers [priv] vers 67 62 0 18:51 ? 00:00:00 sshd: vers@pts/0 vers 68 67 0 18:51 pts/0 00:00:00 -bash vers 120 68 0 18:52 pts/0 00:00:00 ps -afe Log out from the UAI\nvers@uai-vers-a00fb46b-6889b666db-4dfvn:~\u0026gt; exit ncn-w003# Clean up the UAI.\nncn-w003# cray uas delete --uai-list $UAINAME Expected output looks similar to the following:\nresults = [ \u0026#34;Successfully deleted uai-vers-a00fb46b\u0026#34;,] If you got this far with similar results, then the basic functionality of the UAS and UAI is working.\nTroubleshooting Here are some common failure modes seen with UAS / UAI operations and how to resolve them.\nAuthorization Issues If you are not logged in as a valid Keycloak user or are accidentally using the CRAY_CREDENTIALS environment variable (i.e. the variable is set regardless of whether you are logged in as you), you will see an error when running CLI commands.\nFor example:\nncn# cray uas list The symptom of this problem is output similar to the following:\nUsage: cray uas list [OPTIONS] Try \u0026#39;cray uas list --help\u0026#39; for help. Error: Bad Request: Token not valid for UAS. Attributes missing: [\u0026#39;gidNumber\u0026#39;, \u0026#39;loginShell\u0026#39;, \u0026#39;homeDirectory\u0026#39;, \u0026#39;uidNumber\u0026#39;, \u0026#39;name\u0026#39;] Fix this by logging in as a real user (someone with actual Linux credentials) and making sure that CRAY_CREDENTIALS is unset.\nUAS Cannot Access Keycloak When running CLI commands, you may get a Keycloak error.\nFor example:\nncn# cray uas list The symptom of this problem is output similar to the following:\nUsage: cray uas list [OPTIONS] Try \u0026#39;cray uas list --help\u0026#39; for help. Error: Internal Server Error: An error was encountered while accessing Keycloak You may be using the wrong hostname to reach the API gateway, re-run the CLI initialization steps above and try again to check that. There may also be a problem with the Istio service mesh inside of your Shasta system. Troubleshooting this is beyond the scope of this section, but you may find more useful information by looking at the UAS pod logs in Kubernetes. There are, generally, two UAS pods, so you may need to look at logs from both to find the specific failure. The logs tend to have a very large number of GET events listed as part of the liveness checking.\nThe following shows an example of looking at UAS logs effectively (this example shows only one UAS manager, normally there would be two):\nDetermine the pod name of the uas-mgr pod\nncn# kubectl get po -n services | grep \u0026#34;^cray-uas-mgr\u0026#34; | grep -v etcd Expected output looks similar to:\ncray-uas-mgr-6bbd584ccb-zg8vx 2/2 Running 0 12d Set PODNAME to the name of the manager pod whose logs you wish to view.\nncn# export PODNAME=cray-uas-mgr-6bbd584ccb-zg8vx View its last 25 log entries of the cray-uas-mgr container in that pod, excluding GET events:\nncn# kubectl logs -n services $PODNAME cray-uas-mgr | grep -v \u0026#39;GET \u0026#39; | tail -25 Example output:\n2021-02-08 15:32:41,211 - uas_mgr - INFO - getting deployment uai-vers-87a0ff6e in namespace user 2021-02-08 15:32:41,225 - uas_mgr - INFO - creating deployment uai-vers-87a0ff6e in namespace user 2021-02-08 15:32:41,241 - uas_mgr - INFO - creating the UAI service uai-vers-87a0ff6e-ssh 2021-02-08 15:32:41,241 - uas_mgr - INFO - getting service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:32:41,252 - uas_mgr - INFO - creating service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:32:41,267 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:32:41,360 - uas_mgr - INFO - No start time provided from pod 2021-02-08 15:32:41,361 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 127.0.0.1 - - [08/Feb/2021 15:32:41] \u0026#34;POST /v1/uas?imagename=registry.local%2Fcray%2Fno-image-registered%3Alatest HTTP/1.1\u0026#34; 200 - 2021-02-08 15:32:54,455 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:32:54,455 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:32:54,455 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:32:54,484 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:32:54,596 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:25,053 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:40:25,054 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:40:25,054 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:40:25,085 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:40:25,212 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:51,210 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:40:51,210 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:40:51,210 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:40:51,261 - uas_mgr - INFO - deleting service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:51,291 - uas_mgr - INFO - delete deployment uai-vers-87a0ff6e in namespace user 127.0.0.1 - - [08/Feb/2021 15:40:51] \u0026#34;DELETE /v1/uas?uai_list=uai-vers-87a0ff6e HTTP/1.1\u0026#34; 200 - UAI Images not in Registry When listing or describing your UAI, you may notice an error in the uai_msg field. For example:\nncn# cray uas list You may see something similar to the following output:\n[[results]] uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.103.13.172\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.103.13.172\u0026#34; uai_msg = \u0026#34;ErrImagePull\u0026#34; uai_name = \u0026#34;uai-vers-87a0ff6e\u0026#34; uai_status = \u0026#34;Waiting\u0026#34; username = \u0026#34;vers\u0026#34; This means the pre-made end-user UAI image is not in your local registry (or whatever registry it is being pulled from; see the uai_img value for details). To correct this, locate and push/import the image to the registry.\nMissing Volumes and other Container Startup Issues Various packages install volumes in the UAS configuration. All of those volumes must also have the underlying resources available, sometimes on the host node where the UAI is running sometimes from with Kubernetes. If your UAI gets stuck with a ContainerCreating uai_msg field for an extended time, this is a likely cause. UAIs run in the user Kubernetes namespace, and are pods that can be examined using kubectl describe. Use\nncn# kubectl get po -n user | grep \u0026lt;uai-name\u0026gt; to locate the pod, and then use\nncn# kubectl describe pod -n user \u0026lt;pod-name\u0026gt; to investigate the problem. If volumes are missing they will show up in the Events: section of the output. Other problems may show up there as well. The names of the missing volumes or other issues should indicate what needs to be fixed to make the UAI run.\n"
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/restrict_access_to_ncn_images_s3_bucket/",
	"title": "Restrict Network Access to the ncn-images S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Restrict Network Access to the ncn-images S3 Bucket The configuration documented in this procedure is intended to prevent user-facing dedicated nodes (UANs, Compute Nodes) from retrieving NCN image content from Ceph S3 services, as running on storage nodes.\nSpecifically, the controls enacted via this procedure should do the following:\nBlock HAProxy access to the ncn-images bucket if the client is not an NCN (NMN) or PXE booting from the MTL network. This via a HAProxy ACL on the storage servers. Enable access logging for HAProxy. Block Rados GW network access (port 8080) if the client is not an NCN (NMN) or originating from the HMN network. This via iptables rules on the storage servers. Limitations This is not designed to prevent UAIs (if in use) from retrieving NCN image content.\nIf a storage node is rebuilt, this procedure (for the rebuilt node) will need to be applied after the rebuild. The same is true if NCNs are added or removed from the system as it will change source IP ranges for clients.\nPrerequisites and scope Procedure should be executed after install or upgrade is otherwise complete, but prior to opening the system for user access.\nUnless otherwise noted, the procedure should be run from ncn-m001 (not PIT).\nThis procedure was back-ported from CSM 1.0 and was tested on a CSM 0.9.5 system.\nProcedure Test connectivity before applying the ACL.\nSave the following script to a file (for example, con_test.sh).\n#!/bin/bash SNCNS=\u0026#34;$(grep \u0026#39;ncn-s.*\\.nmn\u0026#39; /etc/hosts | awk \u0026#39;{print $NF;}\u0026#39; | xargs)\u0026#34; SCSNS_NMN=\u0026#34;$(echo $SNCNS | xargs -n 1 | sed -e \u0026#39;s/$/.nmn/g\u0026#39;)\u0026#34; SCSNS_HMN=\u0026#34;$(echo $SNCNS | xargs -n 1 | sed -e \u0026#39;s/$/.hmn/g\u0026#39;)\u0026#34; SCSNS_CAN=\u0026#34;$(echo $SNCNS | xargs -n 1 | sed -e \u0026#39;s/$/.can/g\u0026#39;)\u0026#34; RADOS_HTTP_PORT=\u0026#34;8080\u0026#34; HAPROXY_HTTP_PORT=\u0026#34;80\u0026#34; HAPROXY_HTTPSPORT=\u0026#34;443\u0026#34; PASS=\u0026#34;PASS\u0026#34; FAIL=\u0026#34;FAIL\u0026#34; function rados_test { NODES=\u0026#34;$1\u0026#34; MSG=\u0026#34;$2\u0026#34; TTYPE=\u0026#34;$3\u0026#34; echo \u0026#34;[i] $MSG\u0026#34; for n in $NODES do echo -n \u0026#34; RADOS $n: \u0026#34; if [ \u0026#34;$TTYPE\u0026#34; == \u0026#34;CONN_FAIL\u0026#34; ] then curl -sI --connect-timeout 2 http://${n}:${RADOS_HTTP_PORT}/ \u0026amp;\u0026gt; /dev/null rc=$? rc_pass=28 else curl -I --connect-timeout 2 http://${n}:${RADOS_HTTP_PORT}/ 2\u0026gt;/dev/null | grep -q \u0026#34;200 OK\u0026#34; rc=$? rc_pass=0 fi if [ $rc -eq $rc_pass ] then echo $PASS else echo $FAIL fi done } function haproxy_test { NODES=\u0026#34;$1\u0026#34; MSG=\u0026#34;$2\u0026#34; echo \u0026#34;[i] $MSG\u0026#34; for n in $NODES do echo -n \u0026#34; HAPROXY (CEPH) HTTP $n: \u0026#34; curl -I --connect-timeout 2 http://${n}:${HAPROXY_HTTP_PORT}/ncn-images/ 2\u0026gt;/dev/null | grep -q \u0026#34;x-amz-request-id\u0026#34; if [ $? -eq 0 ] then echo $PASS else echo $FAIL fi echo -n \u0026#34; HAPROXY (CEPH) HTTPS $n: \u0026#34; curl -kI --connect-timeout 2 https://${n}:${HAPROXY_HTTPS_PORT}/ncn-images/ 2\u0026gt;/dev/null | grep -q \u0026#34;x-amz-request-id\u0026#34; if [ $? -eq 0 ] then echo $PASS else echo $FAIL fi done } rados_test \u0026#34;$SCSNS_NMN\u0026#34; \u0026#34;MGMT RADOS over NMN\u0026#34; rados_test \u0026#34;$SCSNS_HMN\u0026#34; \u0026#34;MGMT RADOS over HMN\u0026#34; rados_test \u0026#34;$SCSNS_CAN\u0026#34; \u0026#34;MGMT RADOS over CAN\u0026#34; \u0026#34;CONN_FAIL\u0026#34; haproxy_test \u0026#34;$SCSNS_NMN\u0026#34; \u0026#34;MGMT HAProxy over NMN\u0026#34; Execute the script, if the ACLs have not been applied, results similar to the following will be returned (failures are expected):\n[i] MGMT RADOS over NMN RADOS ncn-s003.nmn: PASS RADOS ncn-s002.nmn: PASS RADOS ncn-s001.nmn: PASS [i] MGMT RADOS over HMN RADOS ncn-s003.hmn: PASS RADOS ncn-s002.hmn: PASS RADOS ncn-s001.hmn: PASS [i] MGMT RADOS over CAN RADOS ncn-s003.can: FAIL RADOS ncn-s002.can: FAIL RADOS ncn-s001.can: FAIL [i] MGMT HAProxy over NMN HAPROXY (CEPH) HTTP ncn-s003.nmn: PASS HAPROXY (CEPH) HTTPS ncn-s003.nmn: PASS HAPROXY (CEPH) HTTP ncn-s002.nmn: PASS HAPROXY (CEPH) HTTPS ncn-s002.nmn: PASS HAPROXY (CEPH) HTTP ncn-s001.nmn: PASS HAPROXY (CEPH) HTTPS ncn-s001.nmn: PASS Build an IP address list of NCNs on the NMN.\nCross-check to verify the count seems appropriate for the system in use.\nncn-m001# grep \u0026#39;ncn-[mws].*.nmn\u0026#39; /etc/hosts | awk \u0026#39;{print $1;}\u0026#39; | sed -e \u0026#39;s/\\./ /g\u0026#39; | sort -nk 4 | sed -e \u0026#39;s/ /\\./g\u0026#39; | tee allowed_ncns.lst 10.252.1.4 10.252.1.5 10.252.1.6 10.252.1.7 10.252.1.8 10.252.1.9 10.252.1.10 10.252.1.11 10.252.1.12 10.252.1.13 10.252.1.14 Add the MTL subnet (needed for network boots of NCNs).\nncn-m001# echo \u0026#39;10.1.0.0/16\u0026#39; \u0026gt;\u0026gt; allowed_ncns.lst Verify the allowed_ncns.lst contains contain NMN addresses for all management NCNs nodes and the MTL subnet (10.1.0.0/16).\nncn-m001# cat allowed_ncns.lst 10.252.1.4 10.252.1.5 10.252.1.6 10.252.1.7 10.252.1.8 10.252.1.9 10.252.1.10 10.252.1.11 10.252.1.12 10.252.1.13 10.252.1.14 10.1.0.0/16 Confirm HAProxy configurations are identical across storage nodes.\nAdjust the -w predicate to represent the full set of storage nodes for the system. Applies to this step and subsequent steps.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;cat /etc/haproxy/haproxy.cfg\u0026#34; | dshbak -c ---------------- ncn-s[001-004] ---------------- # Please do not change this file directly since it is managed by Ansible and will be overwritten global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 8000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats tune.ssl.default-dh-param 4096 ssl-default-bind-ciphers EECDH+AESGCM:EDH+AESGCM ssl-default-bind-options no-sslv3 no-tlsv10 no-tlsv11 no-tls-tickets defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 8000 frontend http-rgw-frontend bind *:80 default_backend rgw-backend frontend https-rgw-frontend bind *:443 ssl crt /etc/ceph/rgw.pem default_backend rgw-backend backend rgw-backend option forwardfor balance static-rr option httpchk GET / server server-ncn-s001-rgw0 10.252.1.7:8080 check weight 100 server server-ncn-s002-rgw0 10.252.1.6:8080 check weight 100 server server-ncn-s003-rgw0 10.252.1.5:8080 check weight 100 server server-ncn-s004-rgw0 10.252.1.4:8080 check weight 100 Create a backup of haproxy.cfg files on storage nodes.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg-dist\u0026#34; Grab a copy of haproxy.cfg to modify from a storage node, preserving permissions.\nncn-m001# scp -p ncn-s001:/etc/haproxy/haproxy.cfg . haproxy.cfg Edit the haproxy.cfg, adding in the following ACLs and log directives to each front-end (a diff shown to illustrate changes necessary).\nncn-m001# diff -Naur haproxy.cfg-dist haproxy.cfg --- haproxy.cfg-dist 2022-06-30 18:20:55.000000000 +0000 +++ haproxy.cfg 2022-07-07 16:56:40.000000000 +0000 @@ -1,6 +1,6 @@ # Please do not change this file directly since it is managed by Ansible and will be overwritten global - log 127.0.0.1 local2 + log 127.0.0.1:514 local0 info chroot /var/lib/haproxy pidfile /var/run/haproxy.pid @@ -31,12 +31,22 @@ maxconn 8000 frontend http-rgw-frontend + log global + option httplog bind *:80 default_backend rgw-backend + acl allow_ncns src -n -f /etc/haproxy/allowed_ncns.lst + acl restrict_ncn_images path_beg /ncn-images + http-request deny if restrict_ncn_images !allow_ncns frontend https-rgw-frontend + log global + option httplog bind *:443 ssl crt /etc/ceph/rgw.pem default_backend rgw-backend + acl allow_ncns src -n -f /etc/haproxy/allowed_ncns.lst + acl restrict_ncn_images path_beg /ncn-images + http-request deny if restrict_ncn_images !allow_ncns backend rgw-backend option forwardfor Create a new rsyslog configuration for HAProxy to have it listen to UDP 514 on the local host.\nWith the log directive additions to HAProxy, and allowing a local host UDP 514 socket, access logging should work properly. Set permissions to 640 on the file.\nncn-m001# cat haproxy.conf # Collect log with UDP $ModLoad imudp $UDPServerAddress 127.0.0.1 $UDPServerRun 514 ncn-m001# chmod 0640 haproxy.conf Make sure HAProxy is running on storage nodes.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;systemctl status haproxy\u0026#34; | grep \u0026#34;Active\u0026#34; ncn-s001: Active: active (running) since Thu 2022-07-07 17:38:49 UTC; 54min ago ncn-s003: Active: active (running) since Thu 2022-07-07 17:38:49 UTC; 54min ago ncn-s002: Active: active (running) since Thu 2022-07-07 17:38:49 UTC; 54min ago ncn-s004: Active: active (running) since Thu 2022-07-07 17:38:49 UTC; 54min ago Determine where the HAProxy VIP currently resides (for awareness in the event debug is necessary).\nncn-m001# host rgw-vip rgw-vip.nmn has address 10.252.1.3 ncn-m001# host rgw-vip.nmn rgw-vip.nmn has address 10.252.1.3 ncn-m001# host 10.252.1.3 3.1.252.10.in-addr.arpa domain name pointer rgw-vip. 3.1.252.10.in-addr.arpa domain name pointer rgw-vip.local. 3.1.252.10.in-addr.arpa domain name pointer rgw-vip.local.local. 3.1.252.10.in-addr.arpa domain name pointer rgw-vip.nmn. 3.1.252.10.in-addr.arpa domain name pointer rgw-vip.nmn.local. ncn-m001# ssh rgw-vip \u0026#39;hostname\u0026#39; ncn-s001 Propagate the rsyslog configuration out to all storage nodes.\nncn-m001# pdcp -w ncn-s00[1-4] haproxy.conf /etc/rsyslog.d/ Propagate the HAProxy configuration out to all storage nodes.\nncn-m001# pdcp -w ncn-s00[1-4] haproxy.cfg allowed_ncns.lst /etc/haproxy/ Verify the configurations are identical across storage nodes.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;cat /etc/haproxy/haproxy.cfg\u0026#34; | dshbak -c ---------------- ncn-s[001-004] ---------------- # Please do not change this file directly since it is managed by Ansible and will be overwritten global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 8000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats tune.ssl.default-dh-param 4096 ssl-default-bind-ciphers EECDH+AESGCM:EDH+AESGCM ssl-default-bind-options no-sslv3 no-tlsv10 no-tlsv11 no-tls-tickets defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 8000 frontend http-rgw-frontend bind *:80 default_backend rgw-backend acl allow_ncns src -n -f /etc/haproxy/allowed_ncns.lst acl restrict_ncn_images path_beg /ncn-images http-request deny if restrict_ncn_images !allow_ncns frontend https-rgw-frontend bind *:443 ssl crt /etc/ceph/rgw.pem default_backend rgw-backend acl allow_ncns src -n -f /etc/haproxy/allowed_ncns.lst acl restrict_ncn_images path_beg /ncn-images http-request deny if restrict_ncn_images !allow_ncns backend rgw-backend option forwardfor balance static-rr option httpchk GET / server server-ncn-s001-rgw0 10.252.1.7:8080 check weight 100 server server-ncn-s002-rgw0 10.252.1.6:8080 check weight 100 server server-ncn-s003-rgw0 10.252.1.5:8080 check weight 100 server server-ncn-s004-rgw0 10.252.1.4:8080 check weight 100 Restart rsyslog across all storage nodes.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;systemctl restart rsyslog\u0026#34; ncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;systemctl status rsyslog\u0026#34; | grep Active ncn-s001: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago ncn-s002: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago ncn-s003: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago ncn-s004: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago Restart HAProxy across all storage nodes.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;systemctl restart haproxy\u0026#34; ncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;systemctl status haproxy\u0026#34; | grep Active ncn-s001: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago ncn-s002: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago ncn-s003: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago ncn-s004: Active: active (running) since Thu 2022-07-07 13:50:39 UTC; 7s ago Apply server-side iptables rules to storage nodes.\nThis is needed to prevent direct access to the Ceph Rados GW Service (not through HAProxy).\nThe process is written to support change on individual nodes, but could be scripted after analysis of the running firewall rule set (notably with respect to local modifications, if they exist).\nThis process must be completed on each storage node (steps 18 - 21).\nDocument where Rados GW is running (port wise). It should be the same across all storage nodes.\nncn-s001# ss -tnpl | grep rados LISTEN 0 128 0.0.0.0:8080 0.0.0.0:* users:((\u0026#34;radosgw\u0026#34;,pid=25018,fd=77)) LISTEN 0 128 [::]:8080 [::]:* users:((\u0026#34;radosgw\u0026#34;,pid=25018,fd=78)) List existing iptables rules.\nncn-s001# iptables -L -nx -v Chain INPUT (policy ACCEPT 399480930 packets, 1051007801113 bytes) pkts bytes target prot opt in out source destination 0 0 DROP tcp -- * * 0.0.0.0/0 10.102.4.135 tcp dpt:22 Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 400599807 packets, 1035420933926 bytes) pkts bytes target prot opt in out source destination Run the following to add iptables rules for control.\nThe range should include all NMN NCN IP addresses generated for the HAProxy ACL step.\niptables -A INPUT -i vlan004 -p tcp --dport 8080 -j ACCEPT iptables -A INPUT -i lo -p tcp --dport 8080 -j ACCEPT iptables -A INPUT -p tcp --dport 8080 -m iprange --src-range 10.252.1.4-10.252.1.12 -j ACCEPT iptables -A INPUT -p tcp --dport 8080 -j LOG --log-prefix \u0026#34;RADOSGW-DROP\u0026#34; iptables -A INPUT -p tcp --dport 8080 -j DROP List iptables rules again, verify rules are in place.\nncn-s001# iptables -L -nx -v Chain INPUT (policy ACCEPT 22144 packets, 28721015 bytes) pkts bytes target prot opt in out source destination 0 0 DROP tcp -- * * 0.0.0.0/0 10.102.4.135 tcp dpt:22 0 0 ACCEPT tcp -- vlan004 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 85 4862 ACCEPT tcp -- lo * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 276 15438 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 source IP range 10.252.1.4-10.252.1.12 0 0 LOG tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 LOG flags 0 level 4 prefix \u0026#34;RADOSGW-DROP\u0026#34; 0 0 DROP tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 22099 packets, 30141111 bytes) pkts bytes target prot opt in out source destination Test connectivity after applying the ACL.\nRe-run the connectivity test. While the results will be similar, they should all now be passing:\nncn-m001# bash ./con_test.sh [i] MGMT RADOS over NMN RADOS ncn-s001.nmn: PASS RADOS ncn-s002.nmn: PASS RADOS ncn-s003.nmn: PASS [i] MGMT RADOS over HMN RADOS ncn-s001.hmn: PASS RADOS ncn-s002.hmn: PASS RADOS ncn-s003.hmn: PASS [i] MGMT RADOS over CAN RADOS ncn-s001.can: PASS RADOS ncn-s002.can: PASS RADOS ncn-s003.can: PASS [i] MGMT HAProxy over NMN HAPROXY (CEPH) HTTP ncn-s001.nmn: PASS HAPROXY (CEPH) HTTPS ncn-s001.nmn: PASS HAPROXY (CEPH) HTTP ncn-s002.nmn: PASS HAPROXY (CEPH) HTTPS ncn-s002.nmn: PASS HAPROXY (CEPH) HTTP ncn-s003.nmn: PASS HAPROXY (CEPH) HTTPS ncn-s003.nmn: PASS Validate no connection can be made to HAProxy for ncn-images or Ceph RADOS GW (at all) from compute nodes and UANs.\nUse rgw-vip as it will resolve to one of the storage nodes.\nnid000002# host rgw-vip rgw-vip has address 10.252.1.3 nid000002# curl http://rgw-vip/ncn-images/ \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;403 Forbidden\u0026lt;/h1\u0026gt; Request forbidden by administrative rules. \u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; nid000002# curl -k https://rgw-vip/ncn-images/ \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;403 Forbidden\u0026lt;/h1\u0026gt; Request forbidden by administrative rules. \u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; nid000002# curl --connect-timeout 2 rgw-vip:8080 curl: (28) Connection timed out after 2001 milliseconds Look for a 403 response in the HAProxy logs:\nncn-m001# pdsh -N -w ncn-s00[1-4] \u0026#34;cd /var/log \u0026amp;\u0026amp; zgrep -h -i -E \u0026#39;haproxy.*frontend\u0026#39; messages || exit 0\u0026#34; | grep \u0026#34;ncn-images\u0026#34; 2022-07-13T13:57:08+00:00 xxx-ncn-s001.local haproxy[43591]: 10.252.1.13:50238 [13/Jul/2022:13:57:08.363] http-rgw-frontend http-rgw-frontend/\u0026lt;NOSRV\u0026gt; 0/-1/-1/-1/0 403 212 - - PR-- 1/1/0/0/0 0/0 \u0026#34;GET /ncn-images/ HTTP/1.1\u0026#34; 2022-07-13T14:01:11+00:00 xxx-ncn-s001.local haproxy[43591]: 10.252.1.13:50240 [13/Jul/2022:14:01:11.038] http-rgw-frontend http-rgw-frontend/\u0026lt;NOSRV\u0026gt; 0/-1/-1/-1/0 403 212 - - PR-- 1/1/0/0/0 0/0 \u0026#34;GET /ncn-images/ HTTP/1.1\u0026#34; ... In the firewall logs, the Ceph RADOS GW traffic will be dropped on the storage node. For example:\nncn-m001# pdsh -N -w ncn-s00[1-4] \u0026#34;grep RADOSGW /var/log/firewall\u0026#34; 2022-07-13T14:02:03.418750+00:00 xxx-ncn-s001 kernel: [4397628.546654] RADOSGW-DROPIN=vlan002 OUT= MAC=b8:59:9f:f9:1d:22:a4:bf:01:3f:6f:91:08:00 SRC=10.252.1.13 DST=10.252.1.3 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=9727 DF PROTO=TCP SPT=59278 DPT=8080 WINDOW=42340 RES=0x00 SYN URGP=0 ... For further validation, the following script can be saved to a UAN or compute node with a storage node count as an input.\nThis will test cross-network select access that should not be possible based on a correctly configured switch ACL posture, as well.\nnid000002# cat user_con_test.sh CURL_O=\u0026#34;--connect-timeout 2 -f\u0026#34; NODE_COUNT=\u0026#34;$1\u0026#34; function curl_rept { echo -n \u0026#34;[i] $1 -\u0026gt; \u0026#34; $1 \u0026amp;\u0026gt; /dev/null if [ $? -ne 0 ] then echo \u0026#34;PASS\u0026#34; else echo \u0026#34;FAIL\u0026#34; fi return } for n in `seq 1 $NODE_COUNT` do for t in nmn can hmn do curl_rept \u0026#34;curl $CURL_O ncn-s00${n}.${t}:8080\u0026#34; # ceph rados curl_rept \u0026#34;curl $CURL_O http://ncn-s00${n}.${t}/ncn-images/\u0026#34; # ncn images, http curl_rept \u0026#34;curl $CURL_O -k https://ncn-s00${n}.${t}/ncn-images/\u0026#34; # ncn images, https done done nid000002# bash ./user_con_test.sh 4 [i] curl --connect-timeout 2 -f ncn-s001.nmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s001.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s001.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s001.can:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s001.can/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s001.can/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s001.hmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s001.hmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s001.hmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s002.nmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s002.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s002.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s002.can:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s002.can/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s002.can/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s002.hmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s002.hmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s002.hmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s003.nmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s003.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s003.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s003.can:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s003.can/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s003.can/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s003.hmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s003.hmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s003.hmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s004.nmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s004.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s004.nmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s004.can:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s004.can/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s004.can/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f ncn-s004.hmn:8080 -\u0026gt; PASS [i] curl --connect-timeout 2 -f http://ncn-s004.hmn/ncn-images/ -\u0026gt; PASS [i] curl --connect-timeout 2 -f -k https://ncn-s004.hmn/ncn-images/ -\u0026gt; PASS Save the iptables rule set on all storage nodes and make it persistent across reboots.\nCreate a directory to hold the iptables configuration.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;mkdir --mode=750 /etc/iptables\u0026#34; Create a one-shot systemd service to load iptables on system boot.\nncn-m001# cat \u0026lt;\u0026lt; EOF \u0026gt; metal-iptables.service [Unit] Description=Loads Metal iptables config After=local-fs.target network.service [Service] Type=oneshot ExecStart=/usr/sbin/iptables-restore /etc/iptables/metal.conf Restart=no RemainAfterExit=no [Install] WantedBy=multi-user.target EOF ncn-m001# chmod 640 metal-iptables.service Distribute the one-shot systemd service to the storage nodes.\nncn-m001# pdcp -w ncn-s00[1-4] metal-iptables.service /usr/lib/systemd/system Enable the service.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;systemctl enable metal-iptables.service\u0026#34; Use iptables-save to commit running rules to the persistent configuration.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;iptables-save -f /etc/iptables/metal.conf\u0026#34; Execute the one-shot systemd service.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;systemctl start metal-iptables.service\u0026#34; Verify the rule set is consistent across nodes.\nncn-m001# pdsh -w ncn-s00[1-4] \u0026#34;cat /etc/iptables/metal.conf\u0026#34; | grep \u0026#34;8080\u0026#34; | dshbak -c ---------------- ncn-s[001-004] ---------------- -A INPUT -i vlan004 -p tcp -m tcp --dport 8080 -j ACCEPT -A INPUT -i lo -p tcp -m tcp --dport 8080 -j ACCEPT -A INPUT -p tcp -m tcp --dport 8080 -m iprange --src-range 10.252.1.4-10.252.1.14 -j ACCEPT -A INPUT -p tcp -m tcp --dport 8080 -j LOG --log-prefix RADOSGW-DROP -A INPUT -p tcp -m tcp --dport 8080 -j DROP Troubleshooting NOTE: If SMA log forwarders are not yet running, then it might be necessary to temporarily disable the /etc/rsyslog.d/01-cray-rsyslog.conf rule (for logs to flow to the local nodes without delay). Restart rsyslog if this action is required.\nLook for RADOSGW drops in /var/log/firewall on storage nodes, note that the connectivity test will attempt access on the CAN.\nncn-m001# pdsh -N -w ncn-s00[1-4] \u0026#34;grep RADOSGW /var/log/firewall\u0026#34; | grep vlan007 | head -3 2022-08-01T21:22:01.049443+00:00 ncn-s003 kernel: [13242021.397679] RADOSGW-DROPIN=vlan007 OUT= MAC=14:02:ec:d9:79:d0:94:40:c9:5f:9a:84:08:00 SRC=10.103.13.13 DST=10.103.13.5 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=35159 DF PROTO=TCP SPT=60482 DPT=8080 WINDOW=42340 RES=0x00 SYN URGP=0 2022-08-01T21:22:05.061945+00:00 ncn-s001 kernel: [13248180.144514] RADOSGW-DROPIN=vlan007 OUT= MAC=14:02:ec:da:bc:68:94:40:c9:5f:9a:84:08:00 SRC=10.103.13.13 DST=10.103.13.7 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=10604 DF PROTO=TCP SPT=43034 DPT=8080 WINDOW=42340 RES=0x00 SYN URGP=0 2022-08-01T21:22:02.047541+00:00 ncn-s003 kernel: [13242022.399499] RADOSGW-DROPIN=vlan007 OUT= MAC=14:02:ec:d9:79:d0:94:40:c9:5f:9a:84:08:00 SRC=10.103.13.13 DST=10.103.13.5 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=35160 DF PROTO=TCP SPT=60482 DPT=8080 WINDOW=42340 RES=0x00 SYN URGP=0 ... Look for HAProxy access logs in /var/log/messages on storage nodes that have HTTP 403 responses (or other responses depending upon context).\nncn-m001# pdsh -N -w ncn-s00[1-3] \u0026#34;cd /var/log \u0026amp;\u0026amp; zgrep -h \u0026#39;haproxy.*frontend\u0026#39; messages || exit 0\u0026#34; | grep \u0026#34; 403 \u0026#34; | sort -k 1 2022-08-01T21:36:28+00:00 localhost haproxy[20903]: 10.252.1.20:37248 [01/Aug/2022:21:36:28.679] http-rgw-frontend http-rgw-frontend/\u0026lt;NOSRV\u0026gt; 0/-1/-1/-1/0 403 212 - - PR-- 1/1/0/0/0 0/0 \u0026#34;GET /ncn-images/ HTTP/1.1\u0026#34; 2022-08-01T21:36:57+00:00 localhost haproxy[20903]: 10.252.1.20:53358 [01/Aug/2022:21:36:57.898] https-rgw-frontend~ https-rgw-frontend/\u0026lt;NOSRV\u0026gt; 0/-1/-1/-1/0 403 212 - - PR-- 1/1/0/0/0 0/0 \u0026#34;GET /ncn-images/ HTTP/1.1\u0026#34; 2022-08-01T21:39:35+00:00 localhost haproxy[20903]: 10.252.1.20:40400 [01/Aug/2022:21:39:35.141] https-rgw-frontend~ https-rgw-frontend/\u0026lt;NOSRV\u0026gt; 1/-1/-1/-1/1 403 212 - - PR-- 1/1/0/0/0 0/0 \u0026#34;GET /ncn-images/ HTTP/1.1\u0026#34; 2022-08-01T21:39:35+00:00 localhost haproxy[20903]: 10.252.1.20:57530 [01/Aug/2022:21:39:35.134] http-rgw-frontend http-rgw-frontend/\u0026lt;NOSRV\u0026gt; 0/-1/-1/-1/0 403 212 - - PR-- 1/1/0/0/0 0/0 \u0026#34;GET /ncn-images/ HTTP/1.1\u0026#34; 2022-08-01T21:39:37+00:00 localhost haproxy[20903]: 10.252.1.20:34828 [01/Aug/2022:21:39:37.152] http-rgw-frontend http-rgw-frontend/\u0026lt;NOSRV\u0026gt; 0/-1/-1/-1/0 403 212 - - PR-- 1/1/0/0/0 0/0 \u0026#34;GET /ncn-images/ HTTP/1.1\u0026#34; 2022-08-01T21:39:37+00:00 localhost haproxy[20903]: 10.252.1.20:57896 [01/Aug/2022:21:39:37.159] https-rgw-frontend~ https-rgw-frontend/\u0026lt;NOSRV\u0026gt; 0/-1/-1/-1/0 403 212 - - PR-- 1/1/0/0/0 0/0 \u0026#34;GET /ncn-images/ HTTP/1.1\u0026#34; "
},
{
	"uri": "/docs-csm/en-09/009-ncn-locking/",
	"title": "NCN/Management Node Locking",
	"tags": [],
	"description": "",
	"content": "NCN/Management Node Locking * [Why?](#why) * [When To Lock Management NCNs](#when-to-lock-management-ncns) * [When To Unlock Management NCNs](#when-to-unlock-management-ncns) * [Locked Behavior](#locked-behavior) * [START -\u0026gt; How To Lock Management NCNs](#how-to-lock-management-ncns) * [How To Unlock Management NCNs](#how-to-unlock-management-ncns) * [Next Steps](#next-steps) Why? In Shasta 1.4 NCN black listing is turned off by default for CAPMC and FAS. Also, please note that Management NCNs are NOT locked by default either.\nThus it is up to the administrator to properly lock NCNs to prevent things from accidentally being done to them, namely:\nFirmware upgrades Power down operations Reset operations Doing any of these by accident will take down an NCN. If the NCN is a Kubernetes master or worker node, this can have serious negative effects on system operation.\nIf a single node is taken down by mistake it is possible that things will recover; if all NCNs are taken down, or all Kubernetes worker nodes are taken down by mistake, the system is dead and has to be completely restarted.\nWhen To Lock Management NCNs If NCNs are to be locked, it should be done as early as possible in the install/ upgrade cycle. The later in the process, the more risk of accidentally taking down a critical node.\nNCN locking must be done after Kubernetes is running and all HMS Hardware State Manager service is operational.\nThis can be checked thusly:\nlinux# kubectl -n services get pods | grep smd cray-smd-848bcc875c-6wqsh 2/2 Running 0 9d cray-smd-848bcc875c-hznqj 2/2 Running 0 9d cray-smd-848bcc875c-tp6gf 2/2 Running 0 6d22h cray-smd-init-2tnnq 0/2 Completed 0 9d cray-smd-postgres-0 2/2 Running 0 19d cray-smd-postgres-1 2/2 Running 0 6d21h cray-smd-postgres-2 2/2 Running 0 19d cray-smd-wait-for-postgres-4-7c78j 0/3 Completed 0 9d Note that the cray-smd-xxx pods are in the Running state.\nWhen To Unlock Management NCNs Any time a management NCN has to be power cycled, reset, or have its firmware updated it will first need to be unlocked.\nAfter the operation is complete the targeted nodes should once again be locked. See below for instructions and examples.\nLocked Behavior Once critical nodes are locked, then no power/reset (CAPMC) or firmware (FAS) operations can be done to them unless they are first unlocked. Any node included in a list of nodes to reset, for example, which are locked, will result in a failure.\nSTART-\u0026gt; How To Lock Management NCNs Use the standard CLI to perform locking. The simplest command will lock all nodes with a Management role. The processing-model rigid parameter means that the operation must succeed on all target nodes or the entire operation will fail.\nExample:\nlinux# cray hsm locks lock create --role Management --processing-model rigid Failure = [] [Counts] Total = 8 Success = 8 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x3000c0s5b0n0\u0026#34;, \u0026#34;x3000c0s4b0n0\u0026#34;, \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;x3000c0s6b0n0\u0026#34;, \u0026#34;x3000c0s3b0n0\u0026#34;, \u0026#34;x3000c0s2b0n0\u0026#34;, \u0026#34;x3000c0s9b0n0\u0026#34;, \u0026#34;x3000c0s8b0n0\u0026#34;,] Single nodes or lists of specific nodes can also be locked:\nlinux# cray hsm locks lock create --role Management --component-ids x3000c0s6b0n0 --processing-model rigid Failure = [] [Counts] Total = 1 Success = 1 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x3000c0s6b0n0\u0026#34;,] How To Unlock Management NCNs linux# cray hsm locks unlock create --role Management --processing-model rigid Failure = [] [Counts] Total = 8 Success = 8 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;x3000c0s6b0n0\u0026#34;, \u0026#34;x3000c0s3b0n0\u0026#34;, \u0026#34;x3000c0s2b0n0\u0026#34;, \u0026#34;x3000c0s9b0n0\u0026#34;, \u0026#34;x3000c0s8b0n0\u0026#34;, \u0026#34;x3000c0s5b0n0\u0026#34;, \u0026#34;x3000c0s4b0n0\u0026#34;,] Single nodes or lists of specific nodes can also be locked:\nlinux# cray hsm locks unlock create --role Management --component-ids x3000c0s6b0n0 --processing-model rigid Failure = [] [Counts] Total = 1 Success = 1 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x3000c0s6b0n0\u0026#34;,] Next Steps Check and update firmware next. See Firmware updates with FAS\n"
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/update_default_air-cooled_bmc_and_leaf_switch_snmp_credentials/",
	"title": "Update Default Air-Cooled BMC and Leaf Switch SNMP Credentials",
	"tags": [],
	"description": "",
	"content": "Update Default Air-Cooled BMC and Leaf Switch SNMP Credentials This procedure updates the default credentials used when new air-cooled hardware is discovered for the first time. This includes the default Redfish credentials used for new air-cooled NodeBMCs and Slingshot switch BMCs (RouterBMCs), and SNMP credentials for new management leaf switches.\nImportant: After this procedure is completed going forward all future air-cooled hardware added to the system will be assumed to be already configured with the new global default credential when getting added to the system.\nNOTE: This procedure will not update the Redfish or SNMP credentials for existing air-cooled devices. To change the credentials on existing air-cooled hardware follow the Change Air-Cooled Node BMC Credentials and Change SMNP Credentials on Leaf Switches procedures.\nLimitation The default global credentials used for liquid-cooled BMCs in the Change Cray EX Liquid-Cooled Cabinet Global Default Password procedure needs to be the same as the one used in this procedure for air-cooled BMCs river hardware.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. Procedure 1.1 Acquire site-init. Before redeploying the River Endpoint Discovery Service (REDS), update the customizations.yaml file in the site-init secret in the loftsman namespace.\nIf the site-init repository is available as a remote repository as described here, then clone it to ncn-m001. Otherwise, ensure that the site-init repository is available on ncn-m001.\nncn-m001# git clone \u0026#34;$SITE_INIT_REPO_URL\u0026#34; site-init Acquire customizations.yaml from the currently running system:\nncn-m001# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; site-init/customizations.yaml Review, add, and commit customizations.yaml to the local site-init repository as appropriate.\nNOTE: If site-init was cloned from a remote repository in step 1, there may not be any differences and hence nothing to commit. This is okay. If there are differences between what is in the repository and what was stored in the site-init, then it suggests settings were changed at some point.\nncn-m001# cd site-init ncn-m001# git diff ncn-m001# git add customizations.yaml ncn-m001# git commit -m \u0026#39;Add customizations.yaml from site-init secret\u0026#39; Acquire sealed secret keys:\nncn-m001# mkdir -p certs ncn-m001# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39; | base64 -d \u0026gt; certs/sealed_secrets.crt ncn-m001# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.key}\u0026#39; | base64 -d \u0026gt; certs/sealed_secrets.key 1.2 Modify REDS sealed secret to use new global default credentials. Inspect the original default Redfish credentials used by REDS and HMS Discovery:\nncn-m001# ./utils/secrets-decrypt.sh cray_reds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_redfish_defaults -r | base64 -d | jq { \u0026#34;Cray\u0026#34;: { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foo\u0026#34; } } Inspect the original default switch SNMP credentials used by REDS and HMS Discovery:\nncn-m001# ./utils/secrets-decrypt.sh cray_reds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_switch_defaults -r | base64 -d | jq { \u0026#34;SNMPUsername\u0026#34;: \u0026#34;testuser\u0026#34;, \u0026#34;SNMPAuthPassword\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;SNMPPrivPassword\u0026#34;: \u0026#34;bar\u0026#34; } Update the default credentials in customizations.yaml for REDS and HMS Discovery to work with:\nSpecify the desired default Redfish credentials:\nncn-m001# echo \u0026#39;{\u0026#34;Cray\u0026#34;:{\u0026#34;Username\u0026#34;:\u0026#34;root\u0026#34;,\u0026#34;Password\u0026#34;:\u0026#34;foobar\u0026#34;}}\u0026#39; | base64 \u0026gt; reds.redfish.creds.json.b64 Specify the desired default SNMP credentials:\nncn-m001# echo \u0026#39;{\u0026#34;SNMPUsername\u0026#34;:\u0026#34;testuser\u0026#34;,\u0026#34;SNMPAuthPassword\u0026#34;:\u0026#34;foo1\u0026#34;,\u0026#34;SNMPPrivPassword\u0026#34;:\u0026#34;bar2\u0026#34;}\u0026#39; | base64 \u0026gt; reds.switch.creds.json.b64 Update and regenerate cray_reds_credentials sealed secret:\nncn-m001# cat \u0026lt;\u0026lt; EOF | yq w - \u0026#39;data.vault_redfish_defaults\u0026#39; \u0026#34;$(\u0026lt;reds.redfish.creds.json.b64)\u0026#34; | yq w - \u0026#39;data.vault_switch_defaults\u0026#39; \u0026#34;$(\u0026lt;reds.switch.creds.json.b64)\u0026#34; | yq r -j - | ./utils/secrets-encrypt.sh | yq w -f - -i ./customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_reds_credentials\u0026#39; { \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cray-reds-credentials\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: {} } EOF Decrypt generated secret for review.\nDefault Redfish credentials:\nncn-m001# ./utils/secrets-decrypt.sh cray_reds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_redfish_defaults -r | base64 -d | jq { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foobar\u0026#34; } Default Switch SNMP credentials:\nncn-m001# ./utils/secrets-decrypt.sh cray_reds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_switch_defaults -r | base64 -d | jq { \u0026#34;SNMPUsername\u0026#34;: \u0026#34;testuser\u0026#34;, \u0026#34;SNMPAuthPassword\u0026#34;: \u0026#34;foo1\u0026#34;, \u0026#34;SNMPPrivPassword\u0026#34;: \u0026#34;bar2\u0026#34; } Update the site-init secret for the system:\nncn-m001# kubectl delete secret -n loftsman site-init ncn-m001# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml 1.3 Redeploy REDS to pick up the new sealed secret and push credentials into vault. Determine the version of REDS:\nncn-m001# REDS_VERSION=$(kubectl -n loftsman get cm loftsman-core-services -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; | yq r - \u0026#39;spec.charts.(name==cray-hms-reds).version\u0026#39;) ncn-m001# echo $REDS_VERSION Create reds-manifest.yaml:\nncn-m001# cat \u0026gt; reds-manifest.yaml \u0026lt;\u0026lt; EOF apiVersion: manifests/v1beta1 metadata: name: reds spec: charts: - name: cray-hms-reds version: $REDS_VERSION namespace: services EOF Merge customizations.yaml with reds-manifest.yaml:\nncn-m001# manifestgen -c customizations.yaml -i ./reds-manifest.yaml \u0026gt; ./reds-manifest.out.yaml Redeploy the REDS helm chart:\nncn-m001# loftsman ship \\ --charts-repo https://packages.local/repository/charts \\ --manifest-path reds-manifest.out.yaml Wait for the REDS Vault loader job to run to completion:\nncn-m001# kubectl -n services wait job cray-reds-vault-loader --for=condition=complete --timeout=5m Verify the default Redfish credentials have updated in Vault:\nncn-m001# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-m001# kubectl -n vault exec -it cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 vault kv get secret/reds-creds/defaults Expected output:\n==== Data ==== Key Value --- ----- Cray map[password:foobar username:root] Verify the default SNMP credentials have updated in Vault:\nncn-m001# kubectl -n vault exec -it cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 vault kv get secret/reds-creds/switch_defaults Expected output:\n========== Data ========== Key Value --- ----- SNMPAuthPassword foo1 SNMPPrivPassword bar2 SNMPUsername testuser "
},
{
	"uri": "/docs-csm/en-09/010-firmware-update-with-fas/",
	"title": "Firmware Update the system with FAS",
	"tags": [],
	"description": "",
	"content": "Firmware Update the system with FAS Prerequisites Current Capabilities as of Shasta Release v1.4 Order Of Operations Hardware Precedence Order Next steps Prerequisites 001-008 have been completed; CSM has been installed and HSM is running with discovered nodes. Firmware has been loaded into FAS as part of the CSM install 009 has been applied and the NCNs are locked. Identify the type and manufacturers of hardware in your system. If you do not have Gigabyte nodes, do not update them! WARNING: Non-compute nodes (NCNs) should be locked with the HSM locking API to ensure they are not unintentionally updated by FAS. See 009-NCN-LOCKING.md for more information. Failure to lock the NCNs could result in unintentional update of the NCNs if FAS is not used correctly; this will lead to system instability problems.\nUsing the process outlined in 255-FIRMWARE-ACTION-SERVICE-FAS.md follow the process to update the system. We recommend that you use the \u0026lsquo;recipes\u0026rsquo; listed in 256-FIRMWARE-ACTION-SERVICE-FAS-RECIPES.md to update each supported type.\nNOTE: each system is different and may not have all hardware options.\nCurrent Capabilities as of Shasta Release v1.4 The following table describes the hardware items that can have their firmware updated via FAS.\nTable 1. Upgradable Firmware Items\nManufacturer Type Target New in Release 1.4 Cray nodeBMC BMC, Node0.BIOS, Node1.BIOS, Recovery, Node1.AccFPGA0, Node0.AccFPGA0 Node1.AccFPGA0 and Node0.AccFPGA0 targets Cray chassisBMC BMC, Recovery Cray routerBMC BMC, Recovery Gigabyte nodeBMC BMC, BIOS HPE nodeBMC iLO 5 (BMC aka 1 ), System ROM(BIOS aka 2) ,Redundant System ROM iLO 5 and System ROM targets Order Of Operations For each item in the Hardware Precedence Order:\nComplete a dry run\ncray fas actions create {jsonfile} Note the ActionID! Poll the status of the action until the action state is completed: cray fas actions describe {actionID} --format json Interpret the outcome of the dryrun; look at the counts and determine if the dryrun identified any hardware to update\nFor the steps below, the following returned messages will help determine if a firmware update is needed. The following are end states for operations. The Firmware action itself should be in completed once all operations have finished.\nNoOp: Nothing to do, already at version. NoSol: No viable image is available; this will not be updated. succeeded: *\tIF dryrun: The operation should succeed if performed as a live update. succeeded means that FAS identified that it COULD update an xname + target with the declared strategy. *\tIF live update: the operation succeeded, and has updated the xname + target to the identified version. failed: *\tIF dryrun : There is something that FAS could do, but it likely would fail; most likely because the file is missing. *\tIF live update : the operation failed, the identified version could not be put on the xname + target. If succeeded count \u0026gt; 0, now perform a live update\nUpdate the JSON file overrideDryrun to true\ncray fas actions create {jsonfile} Note the ActionID! Poll the status of the action until the action state is completed: cray fas actions describe {actionID} --format json Interpret the outcome of the live update; proceed to next type of hardware\nHardware Precedence Order After you identify which hardware you have; start with the top most item on this list to update. If you do not have the hardware, skip it.\nIMPORTANT: This process does not communicate the SAFE way to update NCNs. If you have not locked NCNs, or blindly use FAS to update NCNs without following the correct process, then YOU WILL VIOLATE THE STABILITY OF THE SYSTEM\nIMPORTANT : Read the corresponding recipes! There are sometimes ancillary actions that must be completed in order to ensure update integrity.\nCray 2. RouterBMC 2. ChassisBMC 4. NodeBMC 4. BMC 5. NodeBIOS 6. Redstone FPGA Gigabyte 6. BMC 7. BIOS HPE BMC (iLO5) BIOS (System ROM) Next Steps Next the administrator should install additional products following the procedures in the HPE Cray EX System Installation and Configuration Guide S-8000.\n"
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/update_default_servertech_pdu_credentials_used_by_the_redfish_translation_service/",
	"title": "Update Default ServerTech PDU Credentials used by the Redfish Translation Service (RTS)",
	"tags": [],
	"description": "",
	"content": "Update Default ServerTech PDU Credentials used by the Redfish Translation Service (RTS) This procedure updates the default credentials used by the Redfish Translation Service (RTS) for when new ServerTech PDUs are discovered in a system.\nThe Redfish Translation Service provides a Redfish interface that the Hardware State Manager (HSM) and Cray Advanced Platform Monitoring and Control (CAPMC) services can use interact with ServerTech PDUs which do not natively support Redfish.\nThere are two sets of default credentials that are required for RTS to function:\nThe default credentials to use when new ServerTech PDUs are discovered in the system. The global default credential that RTS uses for its Redfish interface with other CSM services. Important:: After this procedure is completed going forward all future ServerTech PDUs added to the system will be assumed to be already configured with the new global default credential when getting added to the system.\nNOTE: This procedure will not change the credentials on existing ServerTech PDUs in a system. To change the credential on existing air-cooled hardware, follow the Change Credentials on ServerTech PDUs procedure. However, this procedure will update the global default credential that RTS uses for its Redfish interface to other CSM services.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. Procedure 1.1 Acquire site-init. Before redeploying RTS, update the customizations.yaml file in the site-init secret in the loftsman namespace.\nIf the site-init repository is available as a remote repository as described here, then clone it to ncn-m001. Otherwise, ensure that the site-init repository is available on ncn-m001.\nncn-m001# git clone \u0026#34;$SITE_INIT_REPO_URL\u0026#34; site-init Acquire customizations.yaml from the currently running system:\nncn-m001# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; site-init/customizations.yaml Review, add, and commit customizations.yaml to the local site-init repository as appropriate.\nNOTE: If site-init was cloned from a remote repository in step 1, there may not be any differences and hence nothing to commit. This is okay. If there are differences between what is in the repository and what was stored in the site-init, then it suggests settings were changed at some point.\nncn-m001# cd site-init ncn-m001# git diff ncn-m001# git add customizations.yaml ncn-m001# git commit -m \u0026#39;Add customizations.yaml from site-init secret\u0026#39; Acquire sealed secret keys:\nncn-m001# mkdir -p certs ncn-m001# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39; | base64 -d \u0026gt; certs/sealed_secrets.crt ncn-m001# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.key}\u0026#39; | base64 -d \u0026gt; certs/sealed_secrets.key 1.2 Modify RTS sealed secret to use new global default credentials. Inspect the original default ServerTech PDU credentials:\nncn-m001# ./utils/secrets-decrypt.sh cray_hms_rts_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_pdu_defaults -r | base64 -d | jq { \u0026#34;Username\u0026#34;: \u0026#34;admn\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foo\u0026#34; } Inspect the original default RTS Redfish Interface credentials:\nncn-m001# ./utils/secrets-decrypt.sh cray_hms_rts_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_rts_defaults -r | base64 -d | jq { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;secret\u0026#34; } Update the default credentials in customizations.yaml for RTS:\nSpecify the desired default ServerTech PDU credentials:\nncn-m001# echo \u0026#39;{\u0026#34;Username\u0026#34;:\u0026#34;admn\u0026#34;, \u0026#34;Password\u0026#34;:\u0026#34;foobar\u0026#34;}\u0026#39; | base64 \u0026gt; rts.pdu.creds.json.b64 Specify the desired default RTS Redfish interface credentials:\nncn-m001# echo \u0026#39;{\u0026#34;Username\u0026#34;:\u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;:\u0026#34;supersecert\u0026#34;}\u0026#39; | base64 \u0026gt; rts.redfish.creds.json.b64 Update and regenerate cray_hms_rts_credentials sealed secret:\nncn-m001# cat \u0026lt;\u0026lt; EOF | yq w - \u0026#39;data.vault_pdu_defaults\u0026#39; \u0026#34;$(\u0026lt;rts.pdu.creds.json.b64)\u0026#34; | yq w - \u0026#39;data.vault_rts_defaults\u0026#39; \u0026#34;$(\u0026lt;rts.redfish.creds.json.b64)\u0026#34; | yq r -j - | ./utils/secrets-encrypt.sh | yq w -f - -i ./customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_hms_rts_credentials\u0026#39; { \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cray-hms-rts-credentials\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: {} } EOF Decrypt generated secret for review.\nDefault ServerTech PDU credentials:\nncn-m001# ./utils/secrets-decrypt.sh cray_hms_rts_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_pdu_defaults -r | base64 -d | jq { \u0026#34;Username\u0026#34;: \u0026#34;admn\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foobar\u0026#34; } Default RTS Redfish interface credentials:\nncn-m001# ./utils/secrets-decrypt.sh cray_hms_rts_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_rts_defaults -r | base64 -d | jq { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;supersecert\u0026#34; } Update the site-init secret for the system:\nncn-m001# kubectl delete secret -n loftsman site-init ncn-m001# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml 1.3 Redeploy RTS to pick up the new sealed secret and push credentials into vault. Determine the version of RTS:\nncn-m001# RTS_VERSION=$(kubectl -n loftsman get cm loftsman-core-services -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; | yq r - \u0026#39;spec.charts.(name==cray-hms-rts).version\u0026#39;) ncn-m001# echo $RTS_VERSION Create rts-manifest.yaml:\nncn-m001# cat \u0026gt; rts-manifest.yaml \u0026lt;\u0026lt; EOF apiVersion: manifests/v1beta1 metadata: name: rts spec: charts: - name: cray-hms-rts version: $RTS_VERSION namespace: services EOF Merge customizations.yaml with rts-manifest.yaml:\nncn-m001# manifestgen -c customizations.yaml -i ./rts-manifest.yaml \u0026gt; ./rts-manifest.out.yaml Redeploy the RTS helm chart:\nncn-m001# loftsman ship \\ --charts-repo https://packages.local/repository/charts \\ --manifest-path rts-manifest.out.yaml Wait for the RTS Init job to run to completion:\nncn-m001# kubectl -n services wait job cray-hms-rts-init --for=condition=complete --timeout=5m Verify the default ServerTech PDU credentials have updated in Vault:\nncn-m001# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-m001# kubectl -n vault exec -it cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 vault kv get secret/pdu-creds/global/pdu Expected output:\n====== Data ====== Key Value --- ----- Password foobar Username admn Verify that default RTS Redfish interface credential has updated in Vault:\nncn-m001# kubectl -n vault exec -it cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 vault kv get secret/pdu-creds/global/rts Expected output:\n====== Data ====== Key Value --- ----- Password supersecret Username root "
},
{
	"uri": "/docs-csm/en-09/051-disk-cleanslate/",
	"title": "Disk Cleanslate",
	"tags": [],
	"description": "",
	"content": "Disk Cleanslate Use Cases Basic Wipe Advanced Wipe Full Wipe This page will detail how disks are wiped and workarounds for wedged disks.\nAny process covered on this page will be covered by the installer.\nEverything in this guide should be considered DESTRUCTIVE.\nAfter following these procedures an NCN can be rebooted and redeployed.\nUse Cases Ideally the Basic Wipe is enough, and should be tried first. All of these procedures may be ran from Linux or an initramFS/initrd emergency shell.\nAdding a node that is not bare. Adopting new disks that are not bare. Fresh-installing. Basic Wipe These basic wipe instructions can be executed on any NCN nodes (master, worker and storage).\nWipe Magic Bits # Print off the disks for verification: ncn# ls -1 /dev/sd* /dev/disk/by-label/* # Wipe the disks and the RAIDs: ncn# wipefs --all --force /dev/sd* /dev/disk/by-label/* If any disks had labels present, output looks similar to the following:\n/dev/sda: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa /dev/sdb: 6 bytes were erased at offset 0x00000000 (crypto_LUKS): 4c 55 4b 53 ba be /dev/sdb: 6 bytes were erased at offset 0x00004000 (crypto_LUKS): 53 4b 55 4c ba be /dev/sdc: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sdc: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sdc: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa The thing to verify is that there are no error messages in the output.\nAdvanced Wipe This section is specific to storage nodes.\nClear Ceph Wipe Magic Bits # Delete CEPH Volumes ncn-s# systemctl stop ceph-osd.target # Make sure the OSDs (if any) are not running ncn-s# ls -1 /dev/sd* /dev/disk/by-label/* ncn-s# vgremove -f --select \u0026#39;vg_name=~ceph*\u0026#39; # Wipe the disks and RAIDs: ncn# wipefs --all --force /dev/sd* /dev/disk/by-label/* See Basic Wipe section for expected output from the wipefs command.\nFull-Wipe This section is also specific to storage nodes.\nClear Ceph Wipe Magic Bits Zero disks Stop RAIDs # Delete CEPH Volumes ncn-s# systemctl stop ceph-osd.target # Make sure the OSDs (if any) are not running ncn-s# ls -1 /dev/sd* /dev/disk/by-label/* ncn-s# vgremove -f --select \u0026#39;vg_name=~ceph*\u0026#39; # Nicely stop the RAIDs, or try. ncn# for md in /dev/md/*; do mdadm -S $md || echo nope ; done # Wipe the disks and RAIDs: ncn# sgdisk --zap-all /dev/sd* ncn# wipefs --all --force /dev/sd* /dev/disk/by-label/* See Basic Wipe section for expected output from the wipefs command.\n"
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/update_ncn_passwords/",
	"title": "Update NCN Passwords",
	"tags": [],
	"description": "",
	"content": "Update NCN Passwords The management nodes deploy with a default password in the image, so it is a recommended best practice for system security to change the root password in the image so that it is not the documented default password. In addition to the root password in the image, NCN personalization should be used to change the password as part of post-boot CFS. The password in the image should be used when console access is desired during the network boot of a management node that is being rebuilt, but this password should be different than the one stored in Vault that is applied by CFS during post-boot NCN personalization to change the on-disk password. Once NCN personalization has been run, then the password in Vault should be used for console access.\nUse one of these methods to change the root password in the image.\nIf the PIT node is booted, see Change NCN Image Root Password and SSH Keys on PIT Node for more information.\nIf the PIT node is not booted, see Change NCN Image Root Password and SSH Keys for more information.\nThe rest of this procedure describes how to change the root password stored in the HashiCorp Vault instance and then apply it immediately to management nodes with the csm.password Ansible role via a CFS session. The same root password from Vault will be applied anytime that the NCN personalization including the CSM layer is run.\nProcedure: Configure root password in Vault Generate a new password hash for the root user.\nThis script uses read -s to prevent the password from being echoed to the screen or saved in the shell history. It unsets the plaintext password variables at the end, so that they cannot be viewed later.\nncn# read -r -s -p \u0026#34;New root password for NCN images: \u0026#34; PW1 ; echo ; if [[ -z ${PW1} ]]; then echo \u0026#34;ERROR: Password cannot be blank\u0026#34; else read -r -s -p \u0026#34;Enter again: \u0026#34; PW2 echo if [[ ${PW1} != ${PW2} ]]; then echo \u0026#34;ERROR: Passwords do not match\u0026#34; else echo -n \u0026#34;${PW1}\u0026#34; | openssl passwd -6 -salt $(\u0026lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c4) --stdin fi fi ; unset PW1 PW2 Get the HashiCorp Vault root token:\nncn-mw# kubectl get secrets -n vault cray-vault-unseal-keys -o jsonpath=\u0026#39;{.data.vault-root}\u0026#39; | base64 -d; echo Write the password hash from step 1 to the HashiCorp Vault. The vault login command will request the token value from the output of step 2 above. The vault read command verifies the hash was stored correctly.\nNOTE: It is important to enclose the hash in single quotes to preserve any special characters.\nncn-mw# kubectl exec -itn vault cray-vault-0 -- sh cray-vault-0# export VAULT_ADDR=http://cray-vault:8200 cray-vault-0# vault login cray-vault-0# vault write secret/csm/management_nodes root_password=\u0026#39;HASH\u0026#39; cray-vault-0# vault read secret/csm/management_nodes cray-vault-0# exit Procedure: Apply root password to NCNs (standalone) Use the following procedure with the rotate-pw-mgmt-nodes.yml playbook to only change the root password on NCNs. This is a quick alternative to running a full NCN personalization, where passwords are also applied using the password stored in Vault set in the procedure above.\nCreate a CFS configuration layer to run the password change Ansible playbook.\nncn# cat ncn-password-update-config.json Example output:\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ncn-password-update\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;rotate-pw-mgmt-nodes.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;INSERT GIT COMMIT ID\u0026gt;\u0026#34; } ] } ncn# cray cfs configurations update ncn-password-update --file ./ncn-password-update-config.json Create a CFS configuration session to apply the password update.\nncn# cray cfs sessions create --name ncn-password-update-`date +%Y%m%d%H%M%S` --configuration-name ncn-password-update NOTE: Subsequent password changes need only update the password hash in HashiCorp Vault and create the CFS session as long as the commit in the CSM configuration management repository has not changed. If the commit has changed, repeat this procedure from the beginning.\n"
},
{
	"uri": "/docs-csm/en-09/052-network-stack/",
	"title": "Network Stack (Software and Hardware)",
	"tags": [],
	"description": "",
	"content": "Network Stack (Software and Hardware) This page will detail how to reload/reset interfaces within the network stack to fix wedged interfaces.\nAny process covered on this page will be covered by the installer.\nNetwork Interfaces The NCNs have network device names set during first boot. The names vary based on the available hardware. For more information, see NCN Networking.\nNetwork Stack There are a few daemons that makeup the SUSE network stack:\nSorted by safest to touch relative to keeping your SSH connection up.\nwickedd.service : The daemons handling each interface ; resetting this clears stale configuration. # Restarts Wicked daemons without reconfiguring the network interfaces. ncn# systemctl restart wickedd wicked.service : The overarching service for spawning daemons and manipulating interface configuration; resetting this reloads daemons and configuration. # Restarts Wicked; respawns daemons and (re)configures the network. ncn# systemctl restart wicked network.service : Responsible for network configuration per interface; does not reload Wicked. NOTE: Commonly the problem exists within Wicked, this is a last resort in the event the configuration is so bad Wicked cannot handle it.\n# Restart the network interface configuration, but leaves wicked daemons alone. ncn# systemctl restart network Use Cases for Resetting Services: Interfaces not showing up IP Addresses not applying Member/children interfaces not being included Command Reference Interface Information # Check interface status (up/down/broken) ncn# wicked ifstatus # Show routing and status for all devices ncn# wicked ifstatus --verbose all lo up link: #1, state up type: loopback control: persistent config: compat:suse:/etc/sysconfig/network/ifcfg-lo, uuid: 6ad37e59-72d7-5988-9675-93b8df96d9f6 leases: ipv4 static granted leases: ipv6 static granted addr: ipv4 127.0.0.1/8 scope host label lo [static] addr: ipv6 ::1/128 scope host [static] route: ipv6 ::1/128 type unicast table main scope universe protocol kernel priority 256 em1 device-unconfigured link: #2, state down, mtu 1500 type: ethernet, hwaddr a4:bf:01:48:1f:dc config: none em2 device-unconfigured link: #3, state down, mtu 1500 type: ethernet, hwaddr a4:bf:01:48:1f:dd config: none mgmt0 enslaved link: #4, state up, mtu 9000, master bond0 type: ethernet, hwaddr b8:59:9f:f9:1c:8e control: none config: compat:suse:/etc/sysconfig/network/ifcfg-mgmt0, uuid: 7175c041-ee2b-5ce2-a4d7-67fa6cb94a17 mgmt1 device-unconfigured link: #5, state up, mtu 9000, master bond0 type: ethernet, hwaddr b8:59:9f:f9:1c:8e config: none bond0 device-unconfigured link: #6, state up, mtu 9000 type: bond, mode ieee802-3ad, hwaddr b8:59:9f:f9:1c:8e config: none addr: ipv6 fe80::ba59:9fff:fef9:1c8e/64 scope link route: ipv6 fe80::/64 type unicast table main scope universe protocol kernel priority 256 vlan002 device-unconfigured link: #7, state up, mtu 9000 type: vlan bond0[2], hwaddr b8:59:9f:f9:1c:8e config: none addr: ipv4 10.252.2.2/17 brd 10.252.2.2 scope universe label vlan002 addr: ipv6 fe80::ba59:9fff:fef9:1c8e/64 scope link route: ipv4 0.0.0.0/0 via 10.252.1.1 dev vlan002 type unicast table main scope universe protocol boot route: ipv4 10.252.0.0/17 type unicast table main scope link protocol kernel pref-src 10.252.2.2 route: ipv6 fe80::/64 type unicast table main scope universe protocol kernel priority 256 vlan004 device-unconfigured link: #8, state up, mtu 9000 type: vlan bond0[4], hwaddr b8:59:9f:f9:1c:8e config: none addr: ipv4 10.254.2.2/17 brd 10.254.2.2 scope universe label vlan004 addr: ipv6 fe80::ba59:9fff:fef9:1c8e/64 scope link route: ipv4 10.254.0.0/17 type unicast table main scope link protocol kernel pref-src 10.254.2.2 route: ipv6 fe80::/64 type unicast table main scope universe protocol kernel priority 256 vlan007 device-unconfigured link: #9, state up, mtu 9000 type: vlan bond0[7], hwaddr b8:59:9f:f9:1c:8e config: none addr: ipv4 10.102.9.12/24 brd 10.102.9.12 scope universe label vlan007 addr: ipv6 fe80::ba59:9fff:fef9:1c8e/64 scope link route: ipv4 10.102.9.0/24 type unicast table main scope link protocol kernel pref-src 10.102.9.12 route: ipv6 fe80::/64 type unicast table main scope universe protocol kernel priority 256 eth0 no-device # Print real devices ( ignore no-device ) ncn# wicked show --verbose all Network Service # Shows the currently enabled network service (Wicked or Network Manager) ncn# systemctl show -p Id network.service Id=wicked.service "
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/updating_the_liquid-cooled_ex_cabinet_default_credentials_after_a_cec_password_change/",
	"title": "Updating the Liquid-Cooled EX Cabinet Default Credentials after a CEC Password Change",
	"tags": [],
	"description": "",
	"content": "Updating the Liquid-Cooled EX Cabinet Default Credentials after a CEC Password Change This procedure changes the credential for liquid-cooled EX cabinet chassis controllers and node controller (BMCs) used by CSM services after the CECs have been set to a new global default credential.\nNOTE: This procedure does not provision Slingshot switch BMCs (RouterBMCs). Slingshot switch BMC default credentials must be changed using the procedures in the Slingshot product documentation. To update Slingshot switch BMCs, refer to \u0026ldquo;Change Rosetta Login and Redfish API Credentials\u0026rdquo; in the Slingshot Operations Guide (\u0026gt;1.6.0).\nThis procedure provisions only the default Redfish root account passwords. It does not modify Redfish accounts that have been added after an initial system installation.\nPrerequisites Perform procedures in Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials on all CECs in the system. All of the CECs must be configured with the same global credential. The previous default global credential for liquid-cooled BMCs needs to be known. Procedure 1. Update the Default credentials used by MEDS for new hardware. The MEDS sealed secret contains the default global credential used by MEDS when it discovers new liquid-cooled EX cabinet hardware.\n1.1 Acquire site-init. Before redeploying MEDS, update the customizations.yaml file in the site-init secret in the loftsman namespace.\nIf the site-init repository is available as a remote repository as described here, then clone it to ncn-m001. Otherwise, ensure that the site-init repository is available on ncn-m001.\nncn-m001# git clone \u0026#34;$SITE_INIT_REPO_URL\u0026#34; site-init Acquire customizations.yaml from the currently running system:\nncn-m001# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; site-init/customizations.yaml Review, add, and commit customizations.yaml to the local site-init repository as appropriate.\nNOTE: If site-init was cloned from a remote repository in step 1, there may not be any differences and hence nothing to commit. This is okay. If there are differences between what is in the repository and what was stored in the site-init, then it suggests settings were changed at some point.\nncn-m001# cd site-init ncn-m001# git diff ncn-m001# git add customizations.yaml ncn-m001# git commit -m \u0026#39;Add customizations.yaml from site-init secret\u0026#39; Acquire sealed secret keys:\nncn-m001# mkdir -p certs ncn-m001# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39; | base64 -d \u0026gt; certs/sealed_secrets.crt ncn-m001# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.key}\u0026#39; | base64 -d \u0026gt; certs/sealed_secrets.key 1.2 Modify MEDS sealed secret to use new global default credential. Inspect the original default credentials for MEDS:\nncn-m001# ./utils/secrets-decrypt.sh cray_meds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_redfish_defaults -r | base64 -d | jq { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;bar\u0026#34; } Specify the desired default credentials for MEDS to use with new hardware:\nReplace foobar with the root password configured on the CEC(s).\nncn-m001# echo \u0026#39;{ \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foobar\u0026#34; }\u0026#39; | base64 \u0026gt; creds.json.b64 Update and regenerate the cray_meds_credentials sealed secret:\nncn-m001# cat \u0026lt;\u0026lt; EOF | yq w - \u0026#39;data.vault_redfish_defaults\u0026#39; \u0026#34;$(\u0026lt;creds.json.b64)\u0026#34; | yq r -j - | ./utils/secrets-encrypt.sh | yq w -f - -i ./customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_meds_credentials\u0026#39; { \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cray-meds-credentials\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: {} } EOF Decrypt updated sealed secret for review. The sealed secret should match the credentials set on the CEC.\nncn-m001# ./utils/secrets-decrypt.sh cray_meds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_redfish_defaults -r | base64 -d | jq { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foobar\u0026#34; } Update the site-init secret containing customizations.yaml for the system:\nncn-m001# kubectl delete secret -n loftsman site-init ncn-m001# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml Check in changes made to customizations.yaml\nncn-m001# git diff ncn-m001# git add customizations.yaml ncn-m001# git commit -m \u0026#39;Update customizations.yaml with global default credential for MEDS\u0026#39; Push to the remote repository as appropriate:\nncn-m001# git push 1.3 Redeploy MEDS to pick up the new sealed secret and push credentials into vault. Determine the version of MEDS:\nncn-m001# MEDS_VERSION=$(kubectl -n loftsman get cm loftsman-core-services -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; | yq r - \u0026#39;spec.charts.(name==cray-hms-meds).version\u0026#39;) ncn-m001# echo $MEDS_VERSION Create meds-manifest.yaml:\nncn-m001# cat \u0026gt; meds-manifest.yaml \u0026lt;\u0026lt; EOF apiVersion: manifests/v1beta1 metadata: name: meds spec: charts: - name: cray-hms-meds version: $MEDS_VERSION namespace: services EOF Merge customizations.yaml with meds-manifest.yaml:\nncn-m001# manifestgen -c customizations.yaml -i ./meds-manifest.yaml \u0026gt; ./meds-manifest.out.yaml Redeploy the MEDS helm chart:\nncn-m001# loftsman ship \\ --charts-repo https://packages.local/repository/charts \\ --manifest-path meds-manifest.out.yaml Wait for the MEDS Vault loader job to run to completion:\nncn-m001# kubectl wait -n services job cray-meds-vault-loader --for=condition=complete --timeout=5m Verify the default credentials have changed in Vault:\nncn-m001# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-m001# kubectl -n vault exec -it cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 vault kv get secret/meds-cred/global/ipmi ====== Data ====== Key Value --- ----- Password foobar Username root 2. Update credentials for existing EX hardware in the system. Set CRED_PASSWORD to the new updated password:\nncn-m001# CRED_PASSWORD=foobar Update the credentials used by CSM services for all previously discovered EX cabinet BMCs to the new global default:\nncn-m001# \\ REDFISH_ENDPOINTS=$(cray hsm inventory redfishEndpoints list --type \u0026#39;!RouterBMC\u0026#39; --format json | jq .RedfishEndpoints[].ID -r | sort -V ) cray hsm state components list --format json \u0026gt; /tmp/components.json for RF in $REDFISH_ENDPOINTS; do echo \u0026#34;$RF: Checking...\u0026#34; CLASS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.Components[] | select(.ID == $XNAME).Class\u0026#39; /tmp/components.json) if [[ \u0026#34;$CLASS\u0026#34; != \u0026#34;Mountain\u0026#34; ]]; then echo \u0026#34;$RF is not Mountain, skipping...\u0026#34; continue fi echo \u0026#34;$RF: Updating credentials\u0026#34; cray hsm inventory redfishEndpoints update ${RF} --user root --password ${CRED_PASSWORD} done It will take some time for the above bash script to run. It will take approximately 5 minutes to update all of the credentials for a single fully populated cabinet.\nAlternatively, use the following command on each BMC. Replace BMC_XNAME with the BMC xname to update the credentials:\nncn-m001# cray hsm inventory redfishEndpoints update BMC_XNAME --user root --password ${CRED_PASSWORD} Wait for HSM to re-discover the updated RedfishEndpoints:\nncn-m001# sleep 180 Wait for all updated Redfish endpoints to become DiscoverOK:\nThe following bash script will find all Redfish endpoints for the liquid-cooled BMCs that are not in DiscoverOK, and display their last Discovery Status.\nncn-m001# \\ cray hsm inventory redfishEndpoints list --laststatus \u0026#39;!DiscoverOK\u0026#39; --type \u0026#39;!RouterBMC\u0026#39; --format json \u0026gt; /tmp/redfishEndpoints.json cray hsm state components list --format json \u0026gt; /tmp/components.json REDFISH_ENDPOINTS=$(jq .RedfishEndpoints[].ID -r /tmp/redfishEndpoints.json | sort -V) for RF in $REDFISH_ENDPOINTS; do CLASS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.Components[] | select(.ID == $XNAME).Class\u0026#39; /tmp/components.json) if [[ \u0026#34;$CLASS\u0026#34; != \u0026#34;Mountain\u0026#34; ]]; then continue fi DISCOVERY_STATUS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.RedfishEndpoints[] | select(.ID == $XNAME).DiscoveryInfo.LastDiscoveryStatus\u0026#39; /tmp/redfishEndpoints.json) echo \u0026#34;$RF: $DISCOVERY_STATUS\u0026#34; done Example output:\nx1001c0r5b0: HTTPsGetFailed x1001c1s0b0: HTTPsGetFailed x1001c1s0b1: HTTPsGetFailed x1001c2s0b1: DiscoveryStarted For each Redfish endpoint that is reported use the following to troubleshoot why it is not DiscoverOK or DiscoveryStarted:\nIf the Redfish endpoint is DiscoveryStarted, then that BMC is currently in the process of being inventoried by HSM. Wait a few minutes and re-try the bash script above to re-check the current discovery status of the RedfishEndpoints. The hms-discovery cronjob (if enabled) will trigger a discover on BMCs that are not currently in DiscoverOK or DiscoveryStarted every 3 minutes. If the Redfish endpoint is HTTPsGetFailed, then HSM had issues contacting BMC, then perform the following steps: Verify that the BMC xname is resolvable and pingable.\nIf the BMC is a ChassisBMC, then the b0 in its xname needs to be removed to get its hostname. Otherwise, for NodeBMCs their xnames is their BMC hostname. For example, the ChassisBMC has the xname x1000c0b0, and its hostname is x1000c0.\nncn-m001# ping x1001c1s0b0 If a NodeBMC is not pingable, then verify that the slot powering the BMC is powered on. If this is a ChassisBMC, skip this step. For example, the NodeBMC x1001c1s0b0 is in slot x1001c1s0.\nncn-m001# cray capmc get_xname_status create --xnames x1001c1s0 e = 0 err_msg = \u0026#34;\u0026#34; on = [ \u0026#34;x1001c1s0b0\u0026#34;,] If the slot is off, power it on.\nncn-m001# cray capmc xname_on create --xnames x1001c1s0 If the BMC is reachable and in HTTPsGetFailed, then verify that the BMC is accessible with the new default global credential. Replace BMC_HOSTNAME with the hostname of the Redfish Endpoint. For a NodeBMC its hostname is its xname. For a ChassisBMC, the b0 part of the xname must be removed to get its hostname.\nFor example, the ChassisBMC has the xname x1000c0b0, and its hostname is x1000c0.\nncn-m001# curl -k -u root:$CRED_PASSWORD https://BMC_HOSTNAME/redfish/v1/Managers | jq If the error message below is returned, then the BMC requires a StatefulReset action. The StatefulReset action will clear user-defined credentials that are taking precedence over the CEC supplied credential. It will also clear NTP, Syslog, and SSH Key configurations on the BMC.\n{ \u0026#34;error\u0026#34;: { \u0026#34;@Message.ExtendedInfo\u0026#34;: [ { \u0026#34;@odata.type\u0026#34;: \u0026#34;#Message.v1_0_5.Message\u0026#34;, \u0026#34;Message\u0026#34;: \u0026#34;While attempting to establish a connection to /redfish/v1/Managers, the service was denied access.\u0026#34;, \u0026#34;MessageArgs\u0026#34;: [ \u0026#34;/redfish/v1/Managers\u0026#34; ], \u0026#34;MessageId\u0026#34;: \u0026#34;Security.1.0.AccessDenied\u0026#34;, \u0026#34;Resolution\u0026#34;: \u0026#34;Attempt to ensure that the URI is correct and that the service has the appropriate credentials.\u0026#34;, \u0026#34;Severity\u0026#34;: \u0026#34;Critical\u0026#34; } ], \u0026#34;code\u0026#34;: \u0026#34;Security.1.0.AccessDenied\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;While attempting to establish a connection to /redfish/v1/Managers, the service was denied access.\u0026#34; } } Perform a StatefulReset on the liquid-cooled BMC and replace BMC_HOSTNAME with the hostname of the BMC. The OLD_DEFAULT_PASSWORD must match the credential that was previously set on the BMC. This is mostly likely the previous global default credential for liquid-cooled BMCs.\nncn-m001# curl -k -u root:OLD_DEFAULT_PASSWORD -X POST -H \u0026#39;Content-Type: application/json\u0026#39; -d \\ \u0026#39;{\u0026#34;ResetType\u0026#34;: \u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://BMC_HOSTNAME/redfish/v1/Managers/BMC/Actions/Manager.Reset After the StatefulReset action has been issued, the BMC will be unreachable for a few minutes as it performs the StatefulReset.\nImportant!: If after the StatefulReset, the BMC is still using the old password, then power cycle the compute chassis slot(s).\nReapply BMC settings if a StatefulReset was performed on any BMC. This section must be performed only if any liquid-cooled Node or Chassis BMCs had a StatefulReset action.\nFor each liquid-cooled BMC that the StatefulReset action was applied, delete the BMC from HSM. Replace BMC_XNAME with the BMC xname to delete.\nncn-m001# cray hsm inventory redfishEndpoints delete BMC_XNAME Restart MEDS to re-setup the NTP and Syslog configuration the RedfishEndpoints:\nView Running MEDS pods:\nncn-m001# kubectl -n services get pods -l app.kubernetes.io/instance=cray-hms-meds NAME READY STATUS RESTARTS AGE cray-meds-6d8b5875bc-4jngc 2/2 Running 0 17d Restart MEDS:\nncn-m001# kubectl -n services rollout restart deployment cray-meds ncn-m001# kubectl -n services rollout status deployment cray-meds Wait for MEDS to re-discover the deleted RedfishEndpoints:\nncn-m001# sleep 300 Verify all expected hardware has been discovered:\nThe following bash script finds all Redfish endpoints for the liquid-cooled BMCs that are not in DiscoverOK, and displays their last Discovery Status.\nncn-m001# \\ cray hsm inventory redfishEndpoints list --laststatus \u0026#39;!DiscoverOK\u0026#39; --type \u0026#39;!RouterBMC\u0026#39; --format json \u0026gt; /tmp/redfishEndpoints.json cray hsm state components list --format json \u0026gt; /tmp/components.json REDFISH_ENDPOINTS=$(jq .RedfishEndpoints[].ID -r /tmp/redfishEndpoints.json | sort -V) for RF in $REDFISH_ENDPOINTS; do CLASS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.Components[] | select(.ID == $XNAME).Class\u0026#39; /tmp/components.json) if [[ \u0026#34;$CLASS\u0026#34; != \u0026#34;Mountain\u0026#34; ]]; then continue fi DISCOVERY_STATUS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.RedfishEndpoints[] | select(.ID == $XNAME).DiscoveryInfo.LastDiscoveryStatus\u0026#39; /tmp/redfishEndpoints.json) echo \u0026#34;$RF: $DISCOVERY_STATUS\u0026#34; done Restore SSH Keys configured by cray-conman on liquid-cooled Node BMCs.\nView the current status of the cray-conman pods:\nncn-m001# kubectl -n services get pods -l app.kubernetes.io/instance=cray-conman NAME READY STATUS RESTARTS AGE cray-conman-7f956fc9bc-97rx4 3/3 Running 0 47d Restart cray-conman deployment:\nncn-m001# kubectl -n services rollout restart deployment cray-conman ncn-m001# kubectl -n services rollout status deployment cray-conman To restore passwordless SSH connections to liquid-cooled Node BMCs that have had the StatefulReset action, follow the procedure in section 30.23 \u0026ldquo;Enable Passwordless Connections to Liquid Cooled Node BMCs\u0026rdquo; in the HPE Cray EX System Administration Guide 1.4 S-8001.\nWARNING: If an admin uses SCSD to update the SSHConsoleKey value outside of ConMan, it will disrupt the ConMan connection to the console and collection of console logs. Refer to \u0026ldquo;About the ConMan Containerized Service\u0026rdquo; in the HPE Cray EX System Administration Guide 1.4 S-8001.\nTo restore passwordless SSH connections to the liquid-cooled Chassis BMCs that have had a StatefulReset action, follow the steps below for each Chassis BMC that was reset:\nSave the public SSH key for the root user.\nncn-m001# export SSH_PUBLIC_KEY=$(cat /root/.ssh/id_rsa.pub | sed \u0026#39;s/[[:space:]]*$//\u0026#39;) Enable passwordless SSH to the root user of the BMCs. Skip this step if passwordless SSH to the root user is not desired. Replace BMC_HOSTNAME with the hostname name of the Chassis BMC. The hostname of a ChassisBMC is its xname with the ending b0 removed.\nncn-m001# curl -k -u root:$CRED_PASSWORD -X PATCH https://BMC_HOSTNAME/redfish/v1/Managers/BMC/NetworkProtocol \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#34;{\\\u0026#34;Oem\\\u0026#34;:{\\\u0026#34;SSHAdmin\\\u0026#34;:{\\\u0026#34;AuthorizedKeys\\\u0026#34;:\\\u0026#34;ssh-rsa $SSH_PUBLIC_KEY\\\u0026#34;}}}\u0026#34; Enable passwordless SSH to the consoles on the BMCs. Skip this step if passwordless SSH to the root user is not desired. Replace BMC_HOSTNAME with the hostname name of the Chassis BMC. The hostname of a ChassisBMC is its xname with the ending b0 removed.\nncn-m001# curl -k -u root:$CRED_PASSWORD -X PATCH https://BMC_HOSTNAME/redfish/v1/Managers/BMC/NetworkProtocol \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#34;{\\\u0026#34;Oem\\\u0026#34;:{\\\u0026#34;SSHConsole\\\u0026#34;:{\\\u0026#34;AuthorizedKeys\\\u0026#34;:\\\u0026#34;ssh-rsa $SSH_PUBLIC_KEY\\\u0026#34;}}}\u0026#34; "
},
{
	"uri": "/docs-csm/en-09/055-certificate-authority/",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": "Overview At install time, a PKI certificate authority (CA) can either be generated for a system, or a customer can opt to supply their own (intermediate) CA.\nOutside of a new installation, there is currently no supported method to rotate (change) the platform CA. The ability to rotate CAs is anticipated as part of a future release.\nSealed Secrets, part of shasta-cfg, are used by the installation process to inject CA material in an encrypted form. Vault (cray-vault instance) ultimately sources and stores the CA from a K8S secret (result of decrypting the corresponding Sealed Secret).\nThe resulting CA will be used to sign multiple workloads on the platform (Ingress, mTLS for PostgreSQL Clusters, Spire, \u0026hellip;).\nManagement of Sealed Secrets should ideally take place on a secure workstation.\nUsing Default Platform Generated CA In shasta-cfg, there is a Sealed Secret generator named platform_ca. By default, the customizations.yaml file will contain a generation template to use this generator, and will create a sealed secret named generated-platform-ca-1. The cray-vault overrides in customizations.yaml contain a) a templated reference to expand the generated-platform-ca-1 Sealed Secret and b) directives instructing vault to load the CA material on start-up \u0026ndash; ultimately initializing a Hashicorp Vault PKI Engine instance with the material.\nNote: the intermediate CA gets installed into Vault, not the root CA (as generated). Use of a root CA is not recommended.\nThe resulting default configuration (prior to seeding customizations) should look like the following customizations.yaml snippet:\nspec: ... kubernetes: sealed_secrets: ... gen_platform_ca_1: generate: name: generated-platform-ca-1 data: - type: platform_ca args: root_days: 3651 int_days: 3650 root_cn: \u0026#34;Platform CA\u0026#34; int_cn: \u0026#34;Platform CA - L1\u0026#34; services: ... cray-vault: sealedSecrets: - \u0026#34;{{ kubernetes.sealed_secrets.gen_platform_ca_1 | toYaml }}\u0026#34; pki: customCA: enabled: true secret: generated-platform-ca-1 private_key: int_ca.key certificate: int_ca.crt ca_bundle: root_ca.crt ... The platform_ca generator will produce RSA CAs with a 3072-bit modulus, using SHA256 as the base signature algorithm.\nCustomize Platform Generated CA The platform_ca generator inputs can be customized, if desired. Notably, the root_days, int_days, root_cn and int_cn fields can be modified. While the shasta-cfg documentation on the use of generators supplies additional detail, the *_days settings control the validity period and the *_cn settings control the common name value for the resulting CA certificates. Ensure the Sealed Secret name reference in spec.kubernetes.services.cray-vault.sealedSecrets is updated if you opt to use a different name.\nOutside of a new installation, there is currently no supported method to rotate (change) the platform CA. Please set validity periods accordingly. The ability to rotate CAs is anticipated as part of a future release.\nUse an External CA The static_platform_ca generator, part of shasta-cfg, can be used to supply an external CA private key, certificate, and associated upstream CAs that form the trust chain. The generator will attempt to prevent you from supplying a root CA. You must also supply the entire trust chain up to the root CA certificate.\nOutside of a new installation, there is currently no supported method to rotate (change) the platform CA. Please ensure validity periods are set accordingly for external CAs you use in this process. The ability to rotate CAs is anticipated as part of a future release.\nHere is an example customizations.yaml snippet illustrating the generator input to inject a static CA:\nspec: ... kubernetes: sealed_secrets: ... external_platform_ca_1: generate: name: external-platform-ca-1 data: - type: static_platform_ca args: key: |- -----BEGIN PRIVATE KEY----- MIIG/gIBADANBgkqhkiG9w0BAQEFAASCBugwggbkAgEAAoIBgQDvhzXCUmGalTDo uswnppXbM+E+OwU79xvaZBsiGEDPpERPZfizpSO3/6IWnYvCUCrb1V4rIhkSKGYq LLVMhmEkfiEImDnx+ksbZau3/w23ogP4qj+BpbTRF707//IOfXgRSD1Q+mVQ7MVo crOt8e/hR4DqZjbkWOrw9pdrfvV159o6x9RVpip33BkAtDzONYApY6ePhzS1BFmo I9R0zMGNeVpy7I2m47YUwpyGAWjRoof0P2BFHX7vdEoJE/TWAlbbiqlM9OHmR85J I/O0MwP63C2Eqn9HajbF1GPVw2IvGN6fE3THtmVDVwxD17cFsKxtVl8gMHljkw9V I+U5piuIfDPvaCoUIC3hlv7jsQs9j52LyZZF3sOKP3xsGG4a5ThqK08EKEgrFovg MYsQrt8aSx7o/7K6IzDOD9QVf7dmkFVxlbPGAjR6nlQ5aW7gFEOAr1CbbZFS+lKi KGjHGraIv93MTqqToE7yRJ6Sv0yP7U9clCi6MNi89AWFfZDkLAsCAwEAAQKCAYAW R61odeE+T8JM45M53PTzfs/kyfiiq0mb9tPPSBI/Pjhcak/H5gR8iPq6v8zQNkTG TgKEYJeUaM2X/rCefaFrk4/fDMnXCEEUO1DNvJu6CQf1iWB+3rsC+AJSImyRjHou oVmSvrfN3zg9ju3HsElv2wbSxs80TlEMOOO8zAJpBTf3X78QeHRa0c5BkoJVbASP 1QUxBJKSg+UTDsIkWydl0XPoXLiQXX4CUFfe3yKw3T1oKrz5sNSt0VNRpNmRToY3 s96Teuv2iBUnN4UciuFajgjlP0Wt2YvntWoYcwJ7mOjwo6Ru5IXdPMeLBx/xKeLF j2SnPiozSAg2OV8G+yffOIcV7598s2Jh9LpgEX0S2NWPdSrjp33IWM9clivzQXaV fFZtFcb3dkrXTt2jVuj6hQR5dsVMC/D/sfORPuAudejmUkAYmozTI9vgcOJpWw3h AT8KBZ6xR3ifr3/GwJk9eosFMeLCTnUprhgbMzM9sde31NOzgYPhiPrN4GJRp4EC gcEA+e3m7HNrSY766GOaiYwiVdzLftL7i6Ie0QTHqJLLESu2/XyxuoML6IRXc+Df A/HVtuwJMqxEe3APvOcwS/Qs6qnPhh0WNz9vJ+3D/uo7Om3cbIR8J6QlsQID9Kas /OAOqxcbtedkkiDSzVM1SPzNh+R85FBDK2xBM433Eu9xET0V8YZegT99SWg72l8+ M37/EhGvtyQpYpY8lYs8pI3Xj7IRLt+jkPKu59uDdATMvVntOMheddpTwYW7XdUI M67VAoHBAPVYodD9Hoe5AcUBrahM7trGzAw3z8fom5lf/wmzJ6Mow8lgH6tliwCs 4NS5PR45olONhK7o7vd/PXvzP1QSIHLNbInveCH29O0ZmBasDlF/eDT+Hcdzq0sw YWUR+9mX5kNS3DuZaWy6f2PDQC+mzPn1yxGmwL2yW0sY6ExfKjmFVSjqG7Mt/oMo BriKaANd3ctge3aRm2MHniXOPq+jC2Zq1rRopWgWIWDzchQsyl4e6iHs5s80nQsE R9nrC6CfXwKBwQDMlwLB7HmW7YRXV7HZhu1UfDnYx71CwKOZVuBaDlBM7gwN1VVn 6H6HCE7OfPYStJTN+MpOwNYOdd1sNZRDmM5sCjXnA0h8UWEcvnYC5ps1aVlXO9ym VqjEDXJPg2F4X7GiPHhin9ikBlqJ2eN0q/1TkKbr/wf9M9Dr8vqedYOJKQgdfnE+ PErDHKBiUjUI0pzanb/Jm8CFA5b0k9ZAnhwndQy74jZzITYsdnVVM9il6EdYhC1P LDoD4QVP+mOMa0ECgcEA0ZCKb4O1j0Kk000ysx47q53A7vLBRUVXmzOXGgbwZXpN efXkNze9+q6wQKOVI/sgv3OTEQAgFkGWGAjXYA03sDftbQiiOYjC/r8s3LjMZiqW V9VzREl11/yURIuO7vbDlV/yg+nvVhMa+vDtI4a7cQrVENe5rI7rUgMNcSacX5OX ASKu1GcGDaujyf9XBwEnkS9xZf7LllQMbshzXPzMoQfDK0hzeKvmiPSIzdjQZoLL hHzhTb3oIl/eq7IMNX/LAoHAYuVeWbSXROyXITXrYcYMwgtYjjUWThQmrLQImJjj HDUNMqq8w8OaQsV+JpZ0lwukeYst3d8vH8Eb4UczUaR+oJpBeEmXjXCGYG4Ec1EQ H72VrrZoJowoqORDSp88h+akcF6+vPJPuNC/Ea7+eAeiYqgxOX5nc2uLjZxBt4OC AhKMY5mnBN2pfAkGVpuyUw3dqGctTSCT0jnxvFPXpldgdAmXi2NTPqPd0IzmLKNG jja1TCeqn9XRTy+EArf1bYi+ -----END PRIVATE KEY----- cert: |- -----BEGIN CERTIFICATE----- MIIEZTCCAs2gAwIBAgIJAKnqv1FyMOp/MA0GCSqGSIb3DQEBCwUAMFsxDzANBgNV BAoMBlNoYXN0YTERMA8GA1UECwwIUGxhdGZvcm0xGjAYBgNVBAMMEVJvb3QgR2Vu ZXJhdGVkIENBMRkwFwYDVQQDDBBQbGF0Zm9ybSBSb290IENBMB4XDTIwMDcwMTIz MjU1MVoXDTIwMDcxMTIzMjU1MVowJDEPMA0GA1UECgwGU2hhc3RhMREwDwYDVQQL DAhQbGF0Zm9ybTCCAaIwDQYJKoZIhvcNAQEBBQADggGPADCCAYoCggGBAO+HNcJS YZqVMOi6zCemldsz4T47BTv3G9pkGyIYQM+kRE9l+LOlI7f/ohadi8JQKtvVXisi GRIoZiostUyGYSR+IQiYOfH6Sxtlq7f/DbeiA/iqP4GltNEXvTv/8g59eBFIPVD6 ZVDsxWhys63x7+FHgOpmNuRY6vD2l2t+9XXn2jrH1FWmKnfcGQC0PM41gCljp4+H NLUEWagj1HTMwY15WnLsjabjthTCnIYBaNGih/Q/YEUdfu90SgkT9NYCVtuKqUz0 4eZHzkkj87QzA/rcLYSqf0dqNsXUY9XDYi8Y3p8TdMe2ZUNXDEPXtwWwrG1WXyAw eWOTD1Uj5TmmK4h8M+9oKhQgLeGW/uOxCz2PnYvJlkXew4o/fGwYbhrlOGorTwQo SCsWi+AxixCu3xpLHuj/srojMM4P1BV/t2aQVXGVs8YCNHqeVDlpbuAUQ4CvUJtt kVL6UqIoaMcatoi/3cxOqpOgTvJEnpK/TI/tT1yUKLow2Lz0BYV9kOQsCwIDAQAB o2MwYTAPBgNVHRMBAf8EBTADAQH/MA4GA1UdDwEB/wQEAwIBBjAdBgNVHQ4EFgQU uNa6qcbJsHdxo6k8kaR5o53DNbIwHwYDVR0jBBgwFoAU/SFNwDBMcAYWBC2SCsDf OyZJbEMwDQYJKoZIhvcNAQELBQADggGBAD8O1Vg9WLFem0RZiZWjtXiNOTZmaksE +a49CE7yGqyETljlVOvbkTUTr4eJnzq2prYJUF8QavSBs38OahcxkTU2GOawZa09 hFc1aBiGSPAxTxJqdHV+G3QZcce1CG2e9VyrxqNudosNRNBEPMOsgg4LpvlRqMfm QhPEJcfvVaCopDZBFXLBPxqmt9BckWFmTSsK09xnrCE/40YD69hdUQ6USJaz9/cd UfNm0HIugRUMvFUP2ytdJmbV+1YQbfVsFrKU4aClrMg+ECX83od5N1TUNQwMePLh IizLGoGDF353eRVKxlzyI724Ni9W82rMW66TQdA7vU6liItHYrhDmcZ+mK2R0F5B ZuYjsLf/BCQ1uDv/bsVG40ogjH/eI/qfhRIzbgVVTF74uKG97pOakp2iQaG9USFd 9/s6ouQQXfkDZ2a/vzs8SBD4eIx7vmeABPRqlHTE8VzohxugxMbJNMdZRPGrEeH6 uddqVNpMH9ehQtsDdt0nmfVIy9/An3BKFw== -----END CERTIFICATE----- ca_bundle: |- -----BEGIN CERTIFICATE----- MIIEezCCAuOgAwIBAgIJAMjuQjQKUpUtMA0GCSqGSIb3DQEBCwUAMFsxDzANBgNV BAoMBlNoYXN0YTERMA8GA1UECwwIUGxhdGZvcm0xGjAYBgNVBAMMEVJvb3QgR2Vu ZXJhdGVkIENBMRkwFwYDVQQDDBBQbGF0Zm9ybSBSb290IENBMB4XDTIwMDcwMTIz MjU1MVoXDTIwMDcxMTIzMjU1MVowWzEPMA0GA1UECgwGU2hhc3RhMREwDwYDVQQL DAhQbGF0Zm9ybTEaMBgGA1UEAwwRUm9vdCBHZW5lcmF0ZWQgQ0ExGTAXBgNVBAMM EFBsYXRmb3JtIFJvb3QgQ0EwggGiMA0GCSqGSIb3DQEBAQUAA4IBjwAwggGKAoIB gQDQ0DTdZmqCOfrWb8KTXJ0hT1r2G51rRE5eAp8d/PoVCgV1gg5h1+jbiv3yYd2R BgM/CPZPvEJaL03wR1gO9NiGEXh1ALd8+yv1O1VRKNb6JuB5cPZFHE3Z8El6aGMc zrqN1ZekRPrZMM1W5Iw78olOMZvsxYw0ZIJqfKOWYB9jYUNM1KohHVj65f/HD/Em kC+9VFhepRV9z21q6fBU13bMz6/NlW19omvbTMwrVSPbYi2nSzqOfi00GXmVh/9Q WElBrAeiGLOsjWkeQ8sFF8ab4SSvzLAAilyQqkBhz2jIxB4L7iG+b9KEgVLeOoMH 1Rs7RhduOMEQypZGVA/vsu/86/5ctM1Cu60mZP+s5B7oT2rwypz0ihLiVCaDCcS5 lDK7PPT5GxZPD8TAqX0SgtaxJnSB/RzavGPSS7efFvlWXh18frwlwa+FgOnyCw1/ qR3BHarcZX9XZivBQSupxQAaUNPMlk0N4wYi6oWrmf21zwd7NtZAinxC2F98J1sn sK8CAwEAAaNCMEAwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwHQYD VR0OBBYEFP0hTcAwTHAGFgQtkgrA3zsmSWxDMA0GCSqGSIb3DQEBCwUAA4IBgQAp ApgLdQBK6fZ7CWlEWwXSKxcjv3akuSqf1NXfn/J9e1rAqqyYoTDE9DXG9dYHL9OA p78KLsLy9fQmrLMmjacXw49bpXDG6XN1WLJfhgQg3j7lXvOvXyxynOgKDtBlroiU nMoK+or9lF2lBIuY34GPyZCL/+vB8s1tu0dGBDgHMUL8/k5d27sdGZZUljC7CgcC k+ABrv19IygDpZpZ6m5N27xajnKpJSjXOfpMCPdhCuNRMgMTX6x8bxZzVAx9ogQ8 16ZzAziB4iMXeCggaY/+YnoEstzTDPXB8FuqeGEVt63Y9ZA7NgWYvVExtKFGGhOL lnEhCLjQyu6/LgOJNfNM9EofaE/IU+i0talgFA+ygSChmYdXzFJn4EfAY9XbwEwV Pw+NHbkpv82jIpc+mopuMRdDO5OyFb+IGkn7ITUFE9N+u97oz2PjD5nQ/Z5DGjBu y3sefnrlqaRanHYkmOnOBTwImPSq8RE8eJP2aRrnu+2YrnoACXxS+XWUXtNhXJ4= -----END CERTIFICATE----- services: ... cray-vault: sealedSecrets: - \u0026#34;{{ kubernetes.sealed_secrets.external_platform_ca_1 | toYaml }}\u0026#34; pki: customCA: enabled: true secret: external-platform-ca-1 private_key: int_ca.key certificate: int_ca.crt ca_bundle: ca_bundle.crt ... Only RSA-based CAs with 3072- or 4096-bit moduli, using RSA256 as a signature/digest algorithm have been tested/are supported. Also note, the generator does not support password-protected private keys.\n"
},
{
	"uri": "/docs-csm/en-09/058-livecd-troubleshooting/",
	"title": "LiveCD Recovery",
	"tags": [],
	"description": "",
	"content": "LiveCD Recovery This page will contain sections for recovering the LiveCD.\nRoot Password Root Password It may become desirable to clear the password on the LiveCD.\nThe root password is preserved within the COW partition at cow:rw/etc/shadow. This is the modified copy of the /etc/shadow file used by the operating system.\nIf a site/user needs to reset/clear the password for root, they can mount their USB on another machine and remove this file from the COW partition. When next booting from the USB it will reinitialize to an empty password for root, and again at next login it will require the password to be changed.\nClearing the password (macOS or Linux):\nmypc:~ \u0026gt; mount -L cow /mnt mypc:~ \u0026gt; sudo rm -f /mnt/rw/etc/shadow mypc:~ \u0026gt; umount /mnt "
},
{
	"uri": "/docs-csm/en-09/059-livecd-backup/",
	"title": "LiveCD Re-Installs",
	"tags": [],
	"description": "",
	"content": "LiveCD Re-Installs This page will go over how to setup a re-install on a node.\nBackup to the data partition:\npit# mkdir -pv /var/www/ephemeral/backup pit# pushd /var/www/ephemeral/backup pit# tar -czvf \u0026#34;dnsmasq-data-$(date \u0026#39;+%Y-%m-%d_%H-%M-%S\u0026#39;).tar.gz\u0026#34; /etc/dnsmasq.* pit# tar -czvf \u0026#34;network-data-$(date \u0026#39;+%Y-%m-%d_%H-%M-%S\u0026#39;).tar.gz\u0026#34; /etc/sysconfig/network/* pit# cp -pv /etc/hosts ./ pit# popd pit# umount /var/www/ephemeral Now the USB stick can be unplugged, it contains all the information we already loaded plus backups of initialized files.\nPlug the stick into a new machine, or make a backup on the booted NCN. Make a snapshot of the USB stick.\nmylinuxpc\u0026gt; mount /dev/disk/by-label/PITDATA /mnt mylinuxpc\u0026gt; tar -czvf --exclude *.squashfs \u0026#34;install-data-$(date \u0026#39;+%Y-%m-%d_%H-%M-%S\u0026#39;).tar.gz\u0026#34; /mnt/ mylinuxpc\u0026gt; umount /dev/disk/by-label/PITDATA The new tar.gz file you made can be stored anywhere, and can be used to reinit the liveCD. Follow the directions in 005-LIVECD-CREATION and then return here and move onto the next step.\nNow you can create a newer LiveCD on the same USB stick, clobbering whats there. Once created the install-data partition can be remounted and you can restore/extract the backup:\nmylinuxpc\u0026gt; mount /dev/disk/by-label/PITDATA /mnt mylinuxpc\u0026gt; tar -xzvf $(ls -ltR *.tar.gz | head -n 1) mylinuxpc\u0026gt; ls -R /mnt The tarball should have extracted everything into the install-data partition. You will need to re-fetch your squashFS artifacts, they can be fetched into (respectively):\n/mnt/var/www/ephemeral/k8s/ /mnt/var/www/ephemeral/ceph/ Once fetched, attach the USB to a CRAY non-compute node and reboot into the USB stick.\nOnce booted into the USB stick; restore network config, dnsmasq, and ensure pods are started.\n# STOP AND INSPECT ANY FAILURE IN ANY OF THESE COMMANDS # DO NOT PASS GO ; DO NOT COLLECT $200 pit# tar -xzvf /var/www/ephemeral/backup/dnsmasq*.tar.gz pit# tar -xzvf /var/www/ephemeral/backup/network*.tar.gz pit# systemctl restart wicked wickedd-nanny pit# systemctl restart dnsmasq pit# systemctl start basecamp nexus You now have a revitalized LiveCD with previous configuration.\n"
},
{
	"uri": "/docs-csm/en-09/060-livecd-1.3-rollback/",
	"title": "LiveCD 1.3 Rollback",
	"tags": [],
	"description": "",
	"content": "LiveCD 1.3 Rollback Reboot into the BIOS and change the boot order so the USB drive is not first Run ansible-playbook /opt/cray/crayctl/ansible_framework/main/enable-dns-conflict-hosts.yml -l ncn-w001 once you are booted back into the 1.3 install systemctl start dhcpd for i in ncn-{m,s}00{1..3}-mgmt ncn-w00{2..3}-mgmt; do echo \u0026quot;------$i--------\u0026quot;; ipmitool -I lanplus -U $username -P $password -H $i chassis power on; done systemctl stop dhcpd \u0026amp;\u0026amp; ansible-playbook /opt/cray/crayctl/ansible_framework/main/disable-dns-conflict-hosts.yml -l ncn-w001 Wait a bit ansible ncn* -m ping until all nodes are up Get a quick overview of how things look: kubectl get nodes kubectl get pods -A | grep Running | wc -l kubectl get pods -A | grep Completed | wc -l kubectl get pods -A | grep Crash | wc -l kubectl get pods -A | grep Image | wc -l ansible ncn-m* -m command -a \u0026#39;ceph health\u0026#39; Fixing Ceph and Kubernetes If things are not quite working, you can try starting these services back up on the affected nodes.\n# Restart dead OSDs for i in 0 3 6 9;do systemctl status ceph-osd@${i}.service | grep active;done for i in 0 3 6 9;do systemctl start ceph-osd@${i}.service | grep active;done # Do the same for mgr and mds services for i in mgr mds;do systemctl start ceph-${i}@ncn-s003.service | grep Active;done for i in mgr mds;do systemctl start ceph-${i}@ncn-s003.service | grep Active;done ceph status "
},
{
	"uri": "/docs-csm/en-09/061-livecd-metal-basecamp/",
	"title": "LiveCD - Metal Basecamp",
	"tags": [],
	"description": "",
	"content": "LiveCD - Metal Basecamp Metal Basecamp is a cloud-init DataSource available on the LiveCD. Basecamp\u0026rsquo;s configuration file offers many inputs for various cloud-init scripts baked into the NCN images.\nThis page details what those settings are.\nBasecamp Config Files Purging Basecamp CAN CEPH Certificate Authority RADOS Gateway Wiping DNS * Resolution Configuration * Static Fallback Kubernetes NTP Node Auditing Generally these settings are determined by the cray-site-init tool. See csi config --help for more information. Manual adjustments typically are for debug and development.\nBasecamp Config Files The cloud-init configuration file is located at /var/www/ephemeral/configs/data.json. The basecamp server configuration file is located at /var/www/ephemeral/configs/server.yaml The static artifact directory served by basecamp can be leveraged at /var/www/ephemeral/static NOTE The jq tool is provided on the LiveCD to facilitate viewing JSON files like these.\nPurging Basecamp If the desire to reset basecamp to defaults comes up, you can do so by following these commands.\npit# systemctl stop basecamp pit# podman rm basecamp pit# podman rmi basecamp pit# rm -f /var/www/ephemeral/configs/server.yaml pit# systemctl start basecamp Basecamp is now entirely fresh.\nCAN Customer Access Network.\nKey: can-gw\ndata:\n{ // ... \u0026#34;can-gw\u0026#34;: \u0026#34;10.102.9.20\u0026#34;, // ... } Key: can-if\ndata:\n{ // ... \u0026#34;can-if\u0026#34;: \u0026#34;vlan007\u0026#34;, // ... } CEPH Key: num_storage_nodes\ndata:\n{ // ... \u0026#34;num_storage_nodes\u0026#34;: \u0026#34;3\u0026#34;, // ... } Certificate Authority Key: ca-certs\ndata:\n{ // ... \u0026#34;ca-certs\u0026#34;: {\u0026#34;remove-defaults\u0026#34;:false,\u0026#34;trusted\u0026#34;:[\u0026#34;-----BEGIN CERTIFICATE-----\\nM,\u0026#34;]} // ... } RADOS Gateway Key: rgw-virtual-ip\ndata:\n{ // ... \u0026#34;rgw-virtual-ip\u0026#34;: \u0026#34;10.252.1.3\u0026#34;, // ... } Wiping Key: wipe-ceph-osds\ndata:\n{ // ... \u0026#34;k8s_virtual_ip\u0026#34;: \u0026#34;10.252.1.2\u0026#34;, // ... } DNS cloud-init modifications to DNS.\nResolution Configuration Paves over bootstrap provisions by adjusting /etc/sysconfig/network/config to match the dns-server value. Updates /etc/resolv.conf by invoking netconfig update -f.\nscript: /srv/cray/scripts/metal/set-dns-config.sh\nKey: dns-server\ndata:\n{ // ... \u0026#34;dns-server\u0026#34;: \u0026#34;10.92.100.225 10.252.1.4\u0026#34;, // ... } Key: domain\ndata:\n{ // ... \u0026#34;domain\u0026#34;: \u0026#34;nmn hmn\u0026#34;, // ... } Static Fallback Safety-net script for installing static-fallback resolution when Kubernetes is offline.\nscript: /srv/cray/scripts/metal/set-host-records.sh\nKey: host_records\ndata:\n{ // ... \u0026#34;host_records\u0026#34;: [ { \u0026#34;aliases\u0026#34;: [ \u0026#34;ncn-s003.nmn\u0026#34;, \u0026#34;ncn-s003\u0026#34; ], \u0026#34;ip\u0026#34;: \u0026#34;10.252.1.4\u0026#34; }, { \u0026#34;aliases\u0026#34;: [ \u0026#34;ncn-s003.mtl\u0026#34; ], \u0026#34;ip\u0026#34;: \u0026#34;10.1.1.2\u0026#34; }, { \u0026#34;aliases\u0026#34;: [ \u0026#34;ncn-s003.hmn\u0026#34;, // ... // ... } Kubernetes Key: k8s_virtual_ip\ndata:\n{ // ... \u0026#34;k8s_virtual_ip\u0026#34;: \u0026#34;10.252.1.2\u0026#34;, // ... } Key: first_master_hostname\ndata:\n{ // ... \u0026#34;first_master_hostname\u0026#34;: \u0026#34;ncn-m002\u0026#34;, // ... } NTP cloud-init modifications to NTP.\nscript: /srv/cray/scripts/metal/set-ntp-config.sh\nKey: ntp_peers\ndata:\n{ // ... \u0026#34;ntp_peers\u0026#34;: \u0026#34;ncn-m003 ncn-w001 ncn-s001 ncn-s002 ncn-s003 ncn-m002 ncn-w003 ncn-m001 ncn-w002\u0026#34;, // ... } Key: ntp_local_nets\ndata:\n{ // ... \u0026#34;ntp_local_nets\u0026#34;: \u0026#34;10.252.0.0/17 10.254.0.0/17\u0026#34;, // ... } Key: upstream_ntp_server\nWARNING at this time, multiple upstream-NTP servers can not be specified.\ndata:\n{ // ... \u0026#34;upstream_ntp_server\u0026#34;: \u0026#34;time.nist.gov\u0026#34;, // ... } Node Auditing Key: ncn-mgmt-node-auditing-enabled\ndata:\n{ // ... \u0026#34;ncn-mgmt-node-auditing-enabled\u0026#34;: false, // ... } --- "
},
{
	"uri": "/docs-csm/en-09/062-livecd-virtual-iso-boot/",
	"title": "LiveCD Virtual ISO Boot",
	"tags": [],
	"description": "",
	"content": "LiveCD Virtual ISO Boot This page will walk-through booting the LiveCD .iso file directly onto a BMC.\nRequirements BMCs\u0026rsquo; Virtual Mounts HPE iLO BMCs Gigabyte BMCs Intel BMCs Configuring Backing up the Overlay COW FS Restoring from an Overlay COW FS Backup Requirements A Cray Pre-Install Toolkit ISO is required for this process. This ISO can be obtained from:\nThe Cray Pre-Install Toolkit ISO included in a CSM release tar file. Internal endpoints (HPE Artifactory) BMCs\u0026rsquo; Virtual Mounts Most BMCs offer a Web Interface for controlling the node and for providing access to its BIOS and firmware.\nRefer to the following pages based on your node vendor for help mounting an ISO image:\nHPE iLO BMCs Gigabyte Intel HPE iLO BMCs iLO BMCs allow for booting directly from an HTTP-accessible ISO location.\nEnter the Virtual Media URL, select Boot on Next Reset, and click Insert Media.\nNext reboot by selecting Reset in the top right power menu.\nOpen the virtual terminal by choosing the HTML5 Console option when clicking the terminal image in the bottom left corner.\nNOTE It may appear that the boot is stalled at a line of EXT4-fs (loop1): mounted ... or Starting dracut pre-mount hook.... This is the step when it actually begins downloading the ISO\u0026rsquo;s squashfs root filesystem and can take a few minutes\nGigabyte BMCs Gigabyte BMCs allow for booting over NFS or HTTP.\nGo to the BMC settings and setup the remote ISO for your protocol and node.\nWeb Interface\nAccess your BMC\u0026rsquo;s web interface and navigate to Settings -\u0026gt; Media Redirection Settings -\u0026gt; General Settings.\nEnable Remote Media Support and Mount CD/DVD and then fill in the server IP or DNS name and the path to the server.\nNOTE The Gigabyte URL appears to not allow certain characters and has a limit on path length. You may need to move or rename the ISO to a location with a smaller file name.\nNext navigate to Image Redirection -\u0026gt; Remote Images and click on the Start button to start the Virtual ISO mount.\nFinally, reboot the node and select the Virtual CDROM option from the manual boot options.\nIntel BMCs Intel BMCs allow for booting with direct ISO mounts.\nGo to the Virtual Media menu.\nChoose your ISO file:\nClick mount to make it available.\nNOTE Do not close the Virtual Media window or the main BMC page until done.\nConfiguring Configuring Backing up the Overlay COW FS Restoring from an Overlay COW FS Backup After attaching and booting into the ISO, the password will need to be changed before using the booted OS.\nThe ISO boots with no password, requiring one to be set on first login. Enter blank as the password and follow the prompts. You can now use the LiveCD to look around, or continue preparing for a CSM installation. NOTE The root OS / directory is writable without persistence. This means that restarting the machine will result in all changes being lost. Before restarting, consider following Backing up the Overlay COW FS and the accompanying Restoring from an Overlay COW FS Backup section.\nBacking up the Overlay COW FS You can backup the writable overlay upper-dir so that your changes are not lost after a reboot or when updating your ISO.\nThis requires that you have a location to which you can scp a tar file as a backup.\ntar czf /run/overlay.tar.gz -C /run/overlayfs/rw . scp /run/overlay.tar.gz \u0026lt;somelocation\u0026gt; NOTE If you want to reduce the size of the backup you can also delete any squashfs files first, or exclude them in the tar command using --exclude='*.squashfs'. You will then need to re-populate those after you restore your backup\nRestoring from an Overlay COW FS Backup Restore a backed up tar file from the previous command with\nscp \u0026lt;somelocation\u0026gt; /run/overlay.tar.gz tar xf /run/overlay.tar.gz -C /run/overlayfs/rw mount -o remount / If you excluded the squashfs files from the backup you will also need to repopulate them following the configuration section.\n"
},
{
	"uri": "/docs-csm/en-09/063-csi-files/",
	"title": "Cray Site Init Files",
	"tags": [],
	"description": "",
	"content": "Cray Site Init Files This page describes administrative knowledge around CSI\u0026rsquo;s files.\nDetailed information for collecting certain files starts in Service Guides\nSave-File / Avoiding Parameters CSI hmn_connections.json Notes Save-File / Avoiding Parameters A system_config.yaml file may be provided by the administrator that will omit the need for specifying parameters on the command line.\nThis file is dumped in the generated configs after every csi config init call, the new dumped file serves as a fingerprint for re-generated the same configs.\nHere is an example file\nbgp-asn: \u0026#34;65533\u0026#34; bootstrap-ncn-bmc-pass: admin bootstrap-ncn-bmc-user: admin can-bootstrap-vlan: 7 can-cidr: 10.102.9.0/24 can-dynamic-pool: 10.102.9.128/25 can-gateway: 10.102.9.20 can-external-dns: 10.102.9.113 can-gw: 10.102.9.20 can-static-pool: 10.102.9.112/28 ceph-cephfs-image: dtr.dev.cray.com/cray/cray-cephfs-provisioner:0.1.0-nautilus-1.3 ceph-rbd-image: dtr.dev.cray.com/cray/cray-rbd-provisioner:0.1.0-nautilus-1.3 chart-repo: http://helmrepo.dev.cray.com:8080 docker-image-registry: dtr.dev.cray.com help: false hill-cabinets: 0 hmn-bootstrap-vlan: 4 hmn-cidr: 10.254.0.0/17 hmn-connections: hmn_connections.json hsn-cidr: 10.250.0.0/16 install-ncn: ncn-m001 install-ncn-bond-members: p801p1,p801p2 ipv4-resolvers: 8.8.8.8, 9.9.9.9 management-net-ips: 0 manifest-release: \u0026#34;\u0026#34; mountain-cabinets: 0 mtl-cidr: 10.1.1.0/16 ncn-metadata: ncn_metadata.csv nmn-bootstrap-vlan: 2 nmn-cidr: 10.252.0.0/17 ntp-pool: time.nist.gov river-cabinets: 1 rpm-repository: https://packages.nmn/repository/shasta-master site-dns: 172.30.84.40 site-domain: dev.cray.com site-gw: 172.30.48.1 site-ip: 172.30.53.153/20 site-nic: em1 starting-hill-cabinet: 9000 starting-mountain-cabinet: 5000 starting-mountain-nid: 1000 starting-river-cabinet: 3000 starting-river-nid: 1 supernet: true switch-metadata: switch_metadata.csv system-name: redbull upstream_ntp_server: time.nist.gov v2-registry: https://registry.nmn/ CSI hmn_connections.json Notes If you see warnings from csi config init that are similar to the warning messages below, it means that CSI encountered an unknown piece of hardware in the hmn_connections.json file. If you do not see this message you can move on to sub-step 2.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1610405168.8705149,\u0026#34;msg\u0026#34;:\u0026#34;Found unknown source prefix! If this is expected to be an Application node, please update application_node_config.yaml\u0026#34;,\u0026#34;row\u0026#34;:{\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u33\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3002\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u48\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;}} If the piece of hardware is expected to be an application node then follow the procedure to create the application_node_config.yaml file. The argument --application-node-config-yaml ./application_node_config.yaml can be given to csi config init to include the additional application node configuration. Due to systems having system-specific application node source names in hmn_connections.json (and the SHCD) the csi config init command will need to be given additional configuration file to properly include these nodes in SLS Input file.\n"
},
{
	"uri": "/docs-csm/en-09/065-csm-safeguards/",
	"title": "Safeguards",
	"tags": [],
	"description": "",
	"content": "Safeguards This page covers safe-guards for preventing destructive behaviors on NCNs.\nIf you are upgrading you should run through these safe-guards on a by-case basis:\nWhether or not CEPH should be preserved. Whether or not the RAIDs should be protected. Safeguard CEPH OSDs Edit /var/www/ephemeral/configs/data.json and align the following options:\n{ .. // Disables Ceph wipe: \u0026#34;wipe-ceph-osds\u0026#34;: \u0026#34;no\u0026#34; .. } { .. // Restores default behavior: \u0026#34;wipe-ceph-osds\u0026#34;: \u0026#34;yes\u0026#34; .. } pit# vi /var/www/ephemeral/configs/data.json\nQuickly toggle yes or no to the file:\n# set wipe-ceph-osds=no pit# sed -i \u0026#39;s/wipe-ceph-osds\u0026#34;: \u0026#34;yes\u0026#34;/wipe-ceph-osds\u0026#34;: \u0026#34;no\u0026#34;/g\u0026#39; /var/www/ephemeral/configs/data.json # set wipe-ceph-osds=yes pit# sed -i \u0026#39;s/wipe-ceph-osds\u0026#34;: \u0026#34;no\u0026#34;/wipe-ceph-osds\u0026#34;: \u0026#34;yes\u0026#34;/g\u0026#39; /var/www/ephemeral/configs/data.json Activate the new setting:\npit# systemctl restart basecamp Safeguard RAIDS / BOOTLOADERS / SquashFS / OverlayFS Edit /var/www/boot/script.ipxe and align the following options as you see them here:\nrd.live.overlay.reset=0 will prevent any overlayFS files from being cleared. metal.no-wipe=1 will guard against touching RAIDs, disks, and partitions. pit# vi /var/www/boot/script.ipxe\n"
},
{
	"uri": "/docs-csm/en-09/066-ceph-csi/",
	"title": "CEPH CSI Verification",
	"tags": [],
	"description": "",
	"content": "CEPH CSI Verification Verify that the ceph-csi requirements are in place\nVerify all post-Ceph install tasks have run Log in to ncn-s001 and check /etc/cray/ceph for completed task files.\nncn-s001# ls /etc/cray/ceph/ ceph_k8s_initialized csi_initialized installed kubernetes_nodes.txt tuned Check to see if k8s ceph-csi prerequisites have been created You can run this from any storage, manager, or worker node.\npit# kubectl get cm NAME DATA AGE ceph-csi-config 1 3h50m cephfs-csi-sc 1 3h50m kube-csi-sc 1 3h50m sma-csi-sc 1 3h50m sts-rados-config 1 4h pit# kubectl get secrets | grep csi csi-cephfs-secret Opaque 4 3h51m csi-kube-secret Opaque 2 3h51m csi-sma-secret Opaque 2 3h51m Check your results against the above examples.\nTroubleshooting If you are missing any components then you will want to re-run the storage node cloud-init script\nLog in to ncn-s001 Run the following bash ncn-s001# ceph -s If it returns a connection error then assume Ceph is not installed and you can re-run the cloud-init script Run the following: bash ncn-s001# ls /etc/cray/ceph If any files are there they will represent completed stages If you have a running cluster you will want to edit /srv/cray/scripts/common/storage-ceph-cloudinit.sh on ncn-s001 and comment out this section: #if [ -f \u0026#34;$ceph_installed_file\u0026#34; ]; then # echo \u0026#34;This ceph cluster has been initialized\u0026#34; #else # echo \u0026#34;Installing ceph\u0026#34; # init # mark_initialized $ceph_installed_file #fi Run the storage-ceph-cloudinit.sh script: ncn-s001# /srv/cray/scripts/common/storage-ceph-cloudinit.sh Configuring node auditing software Using generic auditing configuration This ceph cluster has been initialized This ceph cluster has already been tuned This ceph radosgw config and initial k8s integration already complete ceph-csi configuration has been already been completed If your output is like above then that means that all the steps ran. You can also locate file in /etc/cray/ceph that are created as each step completes If the script failed out then you will have more output for the tasks that are being run. "
},
{
	"uri": "/docs-csm/en-09/067-shasta-cfg/",
	"title": "Setup Site-Init From SHASTA-CFG",
	"tags": [],
	"description": "",
	"content": "Setup Site-Init From SHASTA-CFG These procedures guide administrators through setting up the site-init directory which contains important customizations for various products. The appendix is informational only; it does not include any default install procedures.\nBackground Create and Initialize Site-Init Directory Create Baseline System Customizations Generate Sealed Secrets Version Control Site-Init Files Push to a Remote Repository Customer-Specific Customizations Appendix Tracked Sealed Secrets Decrypting Sealed Secrets for Review Background The shasta-cfg directory included in CSM includes relatively static, installation-centric artifacts such as:\nCluster-wide network configuration settings required by Helm Charts deployed by product stream Loftsman Manifests Sealed Secrets Sealed Secret Generate Blocks \u0026ndash; an form of plain-text input that renders to a Sealed Secret Helm Chart value overrides that are merged into Loftsman Manifests by product stream installers Create and Initialize Site-Init Directory Create directory /mnt/pitdata/prep/site-init:\nlinux# mkdir -p /mnt/pitdata/prep/site-init linux# cd /mnt/pitdata/prep/site-init Initialize /mnt/pitdata/prep/site-init from CSM:\nlinux# /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/meta/init.sh /mnt/pitdata/prep/site-init The yq tool used in the following procedures is available under /mnt/pitdata/prep/site-init/utils/bin once the SHASTA-CFG repo has been cloned\nlinux# alias yq=\u0026#34;/mnt/pitdata/${CSM_RELEASE}/shasta-cfg/utils/bin/$(uname | awk \u0026#39;{print tolower($0)}\u0026#39;)/yq\u0026#34; Create Baseline System Customizations The following steps update /mnt/pitdata/prep/site-init/customizations.yaml with system-specific customizations.\nEnsure system-specific settings generated by CSI are merged into customizations.yaml:\nNote the system name environment variable SYSTEM_NAME must be set\nlinux# yq merge -xP -i /mnt/pitdata/prep/site-init/customizations.yaml \u0026lt;(yq prefix -P \u0026#34;/mnt/pitdata/prep/${SYSTEM_NAME}/customizations.yaml\u0026#34; spec) Set the cluster name:\nlinux# yq write -i /mnt/pitdata/prep/site-init/customizations.yaml spec.wlm.cluster_name \u0026#34;$SYSTEM_NAME\u0026#34; Make a backup copy of /mnt/pitdata/prep/site-init/customizations.yaml:\nlinux# cp /mnt/pitdata/prep/site-init/customizations.yaml /mnt/pitdata/prep/site-init/customizations.yaml.prepassword Review the configuration to generate these sealed secrets in customizations.yaml in the site-init directory:\nspec.kubernetes.sealed_secrets.cray_reds_credentials spec.kubernetes.sealed_secrets.cray_meds_credentials spec.kubernetes.sealed_secrets.cray_hms_rts_credentials Edit customizations.yaml, replacing the Password references with values appropriate for your system. See Decrypting Sealed Secrets for Review if you need to examine credentials from prior installs.\nReview the changes that you made:\nlinux# diff /mnt/pitdata/prep/site-init/customizations.yaml /mnt/pitdata/prep/site-init/customizations.yaml.prepassword Validate that REDS/MEDS/RTS sealed secrets contain valid JSON using jq:\n# Validate REDS credentials (used by the REDS and HMS discovery services, # targeting River Redfish BMC endpoints and management switches) linux# yq read /mnt/pitdata/prep/site-init/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_reds_credentials.generate.data[0].args.value\u0026#39; | jq linux# yq read /mnt/pitdata/prep/site-init/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_reds_credentials.generate.data[1].args.value\u0026#39; | jq # NOTE: For vault_redfish_defaults, the only entry used is \u0026#39;{\u0026#34;Cray\u0026#34;: {\u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;XXXX\u0026#34;}\u0026#39; # Make sure it is specified as shown, with the \u0026#39;Cray\u0026#39; key. This key is not # used in any of the other credential specifications. Make sure Username and # Password entries are correct. # Validate MEDS credentials (used by the MEDS service, targeting # Redfish BMC endpoints). Make sure Username and Password entries are correct. linux# yq read /mnt/pitdata/prep/site-init/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_meds_credentials.generate.data[0].args.value\u0026#39; | jq # Validate RTS credentials (used by the Redfish Translation Service, targeting # River Redfish BMC endpoints and PDU controllers). Make sure Username and # Password entries are correct. linux# yq read /mnt/pitdata/prep/site-init/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_hms_rts_credentials.generate.data[0].args.value\u0026#39; | jq linux# yq read /mnt/pitdata/prep/site-init/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_hms_rts_credentials.generate.data[1].args.value\u0026#39; | jq To customize the PKI Certificate Authority (CA) used by the platform, see Customizing the Platform CA.\nIMPORTANT The CA may not be modified after install.\nTo federate Keycloak with an upstream LDAP:\nINTERNAL ONLY On internal HPE systems, if the IP address for the ncn-m001 node is even then use ldaps://dcldap2.hpc.amslabs.hpecorp.net, otherwise use ldaps://dcldap3.us.cray.com. For example, ping fanta-ncn-m001.us.cray.com shows the IP address is 172.30.52.72, so fanta would use ldaps://dcldap2.hpc.amslabs.hpecorp.net as the ldap_connection_url. This just spreads the load over the two LDAP replicas.\nlinux# export LDAP=dcldap2.hpc.amslabs.hpecorp.net If LDAP requires TLS (recommended), update the cray-keycloak sealed secret value by supplying a base64 encoded Java KeyStore (JKS) that contains the CA certificate that signed the LDAP server\u0026rsquo;s host key. The password for the JKS file must be password. Admins may use the keytool command from the openjdk:11-jre-slim container image packaged with CSM to create a JKS file that includes a PEM-encoded CA certificate to verify the LDAP host(s) as follows:\nLoad the openjdk container image:\nNOTE Requires a properly configured Docker or Podman environment.\nlinux# /mnt/pitdata/${CSM_RELEASE}/hack/load-container-image.sh dtr.dev.cray.com/library/openjdk:11-jre-slim Create (or update) cert.jks with the PEM-encoded CA certificate for an LDAP host:\nIMPORTANT Replace \u0026lt;ca-cert.pem\u0026gt; and \u0026lt;alias\u0026gt; as appropriate.\nlinux# podman run --rm -v \u0026#34;$(pwd):/data\u0026#34; dtr.dev.cray.com/library/openjdk:11-jre-slim keytool \\ -importcert -trustcacerts -file /data/\u0026lt;ca-cert.pem\u0026gt; -alias \u0026lt;alias\u0026gt; \\ -keystore /data/certs.jks -storepass password -noprompt INTERNAL ONLY For example, on internal HPE systems, create the certs.jks.b64 file as follows:\nGet the issuer certificate for dcldap. Use openssl s_client to connect and show the certificate chain returned by the LDAP host:\nlinux# openssl s_client -showcerts -connect $LDAP:636 \u0026lt;/dev/null Either manually extract (i.e., cut/paste) the issuer\u0026rsquo;s certificate into cacert.pem or try the following commands to create it automatically.\nNOTE The following commands were verified using OpenSSL version 1.1.1d and use the -nameopt RFC2253 option to ensure consistent formatting of distinguished names (DNs). Unfortunately, older versions of OpenSSL may not support -nameopt on the s_client command or may use a different default format. As a result, your mileage may vary; however, you should be able to manually extract the issuer certificate from the output of the above openssl s_client example if the following commands are unsuccessful.\nObserve the issuer\u0026rsquo;s DN:\nlinux# openssl s_client -showcerts -nameopt RFC2253 -connect $LDAP:636 \u0026lt;/dev/null 2\u0026gt;/dev/null | grep issuer= | sed -e \u0026#39;s/^issuer=//\u0026#39; Expected output looks like:\nemailAddress=dcops@hpe.com,CN=Data Center,OU=HPC/MCS,O=HPE,ST=WI,C=US Extract the issuer\u0026rsquo;s certificate using awk:\nNOTE The issuer DN is properly escaped as part of the awk pattern below. If the value you are using is different, be sure to escape it properly!\nlinux# openssl s_client -showcerts -nameopt RFC2253 -connect $LDAP:636 \u0026lt;/dev/null 2\u0026gt;/dev/null | awk \u0026#39;/s:emailAddress=dcops@hpe.com,CN=Data Center,OU=HPC\\/MCS,O=HPE,ST=WI,C=US/,/END CERTIFICATE/\u0026#39; | awk \u0026#39;/BEGIN CERTIFICATE/,/END CERTIFICATE/\u0026#39; \u0026gt; cacert.pem Verify issuer\u0026rsquo;s certificate was properly extracted and saved in cacert.pem:\nlinux# cat cacert.pem Expected output looks like:\n-----BEGIN CERTIFICATE----- MIIDvTCCAqWgAwIBAgIUYxrG/PrMcmIzDuJ+U1Gh8hpsU8cwDQYJKoZIhvcNAQEL BQAwbjELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAldJMQwwCgYDVQQKDANIUEUxEDAO BgNVBAsMB0hQQy9NQ1MxFDASBgNVBAMMC0RhdGEgQ2VudGVyMRwwGgYJKoZIhvcN AQkBFg1kY29wc0BocGUuY29tMB4XDTIwMTEyNDIwMzM0MVoXDTMwMTEyMjIwMzM0 MVowbjELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAldJMQwwCgYDVQQKDANIUEUxEDAO BgNVBAsMB0hQQy9NQ1MxFDASBgNVBAMMC0RhdGEgQ2VudGVyMRwwGgYJKoZIhvcN AQkBFg1kY29wc0BocGUuY29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC AQEAuBIZkKitHHVQHymtaQt4D8ZhG4qNJ0cTsLhODPMtVtBjPZp59e+PWzbc9Rj5 +wfjLGteK6/fNJsJctWlS/ar4jw/xBIPMk5pg0dnkMT2s7lkSCmyd9Uib7u6y6E8 yeGoGcb7I+4ZI+E3FQV7zPact6b17xmajNyKrzhBGEjYucYJUL5iTgZ6a7HOZU2O aQSXe7ctiHBxe7p7RhHCuKRrqJnxoohakloKwgHHzDLFQzX/5ADp1hdJcduWpaXY RMBu6b1mhmwo5vmc+fDnfUpl5/X4i109r9VN7JC7DQ5+JX8u9SHDGLggBWkrhpvl bNXMVCnwnSFfb/rnmGO7rdJSpwIDAQABo1MwUTAdBgNVHQ4EFgQUVg3VYExUAdn2 WE3e8Xc8HONy/+4wHwYDVR0jBBgwFoAUVg3VYExUAdn2WE3e8Xc8HONy/+4wDwYD VR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAWLDQLB6rrmK+gwUY+4B7 0USbQK0JkLWuc0tCfjTxNQTzFb75PeH+GH21QsjUI8VC6QOAAJ4uzIEV85VpOQPp qjz+LI/Ej1xXfz5ostZQu9rCMnPtVu7JT0B+NV7HvgqidTfa2M2dw9yUYS2surZO 8S0Dq3Bi6IEhtGU3T8ZpbAmAp+nNsaJWdUNjD4ECO5rAkyA/Vu+WyMz6F3ZDBmRr ipWM1B16vx8rSpQpygY+FNX4e1RqslKhoyuzXfUGzyXux5yhs/ufOaqORCw3rJIx v4sTWGsSBLXDsFM3lBgljSAHfmDuKdO+Qv7EqGzCRMpgSciZihnbQoRrPZkOHUxr NA== -----END CERTIFICATE----- Create certs.jks:\nlinux# podman run --rm -v \u0026#34;$(pwd):/data\u0026#34; dtr.dev.cray.com/library/openjdk:11-jre-slim keytool -importcert \\ -trustcacerts -file /data/cacert.pem -alias cray-data-center-ca -keystore /data/certs.jks \\ -storepass password -noprompt Create certs.jks.b64 by base-64 encoding certs.jks:\nlinux# base64 certs.jks \u0026gt; certs.jks.b64 Then, inject and encrypt certs.jks.b64 into customizations.yaml:\nlinux# cat \u0026lt;\u0026lt;EOF | yq w - \u0026#39;data.\u0026#34;certs.jks\u0026#34;\u0026#39; \u0026#34;$(\u0026lt;certs.jks.b64)\u0026#34; | \\ yq r -j - | /mnt/pitdata/prep/site-init/utils/secrets-encrypt.sh | \\ yq w -f - -i /mnt/pitdata/prep/site-init/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray-keycloak\u0026#39; { \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;keycloak-certs\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: {} } EOF Update the keycloak_users_localize sealed secret with the appropriate value for ldap_connection_url.\nSet ldap_connection_url in customizations.yaml:\nIMPORTANT Replace \u0026lt;ldap-url\u0026gt; as appropriate.\nlinux# yq write -i /mnt/pitdata/prep/site-init/customizations.yaml \\ \u0026#39;spec.kubernetes.sealed_secrets.keycloak_users_localize.generate.data.(args.name==ldap_connection_url).args.value\u0026#39; \u0026#39;\u0026lt;ldap-url\u0026gt;\u0026#39; INTERNAL ONLY\nSet ldap_connection_url in customizations.yaml:\nlinux# yq write -i /mnt/pitdata/prep/site-init/customizations.yaml \\ \u0026#39;spec.kubernetes.sealed_secrets.keycloak_users_localize.generate.data.(args.name==ldap_connection_url).args.value\u0026#39; \u0026#34;ldaps://$LDAP\u0026#34; On success, review the keycloak_users_localize sealed secret:\nlinux# yq read /mnt/pitdata/prep/site-init/customizations.yaml spec.kubernetes.sealed_secrets.keycloak_users_localize Expected output is similar to:\ngenerate: name: keycloak-users-localize data: - type: static args: name: ldap_connection_url value: ldaps://dcldap2.hpc.amslabs.hpecorp.net Configure the ldapSearchBase and localRoleAssignments settings for the cray-keycloak-users-localize chart in customizations.yaml.\nSet ldapSearchBase in customizations.yaml:\nIMPORTANT Replace \u0026lt;search-base\u0026gt; as appropriate.\nlinux# yq write -i /mnt/pitdata/prep/site-init/customizations.yaml spec.kubernetes.services.cray-keycloak-users-localize.ldapSearchBase \u0026#39;\u0026lt;search-base\u0026gt;\u0026#39; Set localRoleAssignments that map to admin and/or user roles for both shasta and cray clients in customizations.yaml:\nIMPORTANT Replace \u0026lt;admin-group\u0026gt; and \u0026lt;user-group\u0026gt; as appropriate. Also add other assignments as desired.\nlinux# yq write -s - -i /mnt/pitdata/prep/site-init/customizations.yaml \u0026lt;\u0026lt;EOF - command: update path: spec.kubernetes.services.cray-keycloak-users-localize.localRoleAssignments value: - {\u0026#34;group\u0026#34;: \u0026#34;\u0026lt;admin-group\u0026gt;\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;\u0026lt;admin-group\u0026gt;\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;\u0026lt;user-group\u0026gt;\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;\u0026lt;user-group\u0026gt;\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} EOF INTERNAL ONLY For example, on internal HPE systems appropriate settings are:\nldapSearchBase: \u0026#34;dc=dcldap,dc=dit\u0026#34; localRoleAssignments: - {\u0026#34;group\u0026#34;: \u0026#34;criemp\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;criemp\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;craydev\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;craydev\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_admins\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_admins\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_users\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_users\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} Set ldapSearchBase in customizations.yaml:\nlinux# yq write -i /mnt/pitdata/prep/site-init/customizations.yaml spec.kubernetes.services.cray-keycloak-users-localize.ldapSearchBase \u0026#39;dc=dcldap,dc=dit\u0026#39; Set localRoleAssignments in customizations.yaml:\nlinux# yq write -s - -i /mnt/pitdata/prep/site-init/customizations.yaml \u0026lt;\u0026lt;EOF - command: update path: spec.kubernetes.services.cray-keycloak-users-localize.localRoleAssignments value: - {\u0026#34;group\u0026#34;: \u0026#34;criemp\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;criemp\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;craydev\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;craydev\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_admins\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_admins\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_users\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_users\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} EOF On success, review the cray-keycloak-users-localize values:\nlinux# yq read /mnt/pitdata/prep/site-init/customizations.yaml spec.kubernetes.services.cray-keycloak-users-localize Expected output looks similar to:\nsealedSecrets: - \u0026#39;{{ kubernetes.sealed_secrets.keycloak_users_localize | toYaml }}\u0026#39; localRoleAssignments: - {\u0026#34;group\u0026#34;: \u0026#34;criemp\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;criemp\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;craydev\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;craydev\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_admins\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_admins\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_users\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_users\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} ldapSearchBase: dc=dcldap,dc=dit If you need to resolve outside hostnames, you will need to configure forwarding in the cray-dns-unbound service. For example, if you are using a hostname and not an IP for the upstream LDAP server in step 4 above, you will need to be able to resolve that hostname.\nSet the localZones and forwardZones for the cray-dns-unbound service:\nlinux# yq write -s - -i /mnt/pitdata/prep/site-init/customizations.yaml \u0026lt;\u0026lt;EOF - command: update path: spec.kubernetes.services.cray-dns-unbound value: localZones: - localType: static name: \u0026#34;local\u0026#34; forwardZones: - name: \u0026#34;.\u0026#34; forwardIps: - \u0026#34;{{ network.netstaticips.system_to_site_lookups }}\u0026#34; EOF On success, review the cray-dns-unbound values.\nlinux# yq read /mnt/pitdata/prep/site-init/customizations.yaml spec.kubernetes.services.cray-dns-unbound Expected output looks similar to:\nlocalZones: - localType: static name: \u0026#34;local\u0026#34; forwardZones: - name: \u0026#34;.\u0026#34; forwardIps: - \u0026#34;{{ network.netstaticips.system_to_site_lookups }}\u0026#34; Review customizations.yaml in the site-init directory and replace remaining ~FIXME~ values with appropriate settings.\nCreate backup copy of customizations.yaml:\nlinux# cp /mnt/pitdata/prep/site-init/customizations.yaml /mnt/pitdata/prep/site-init/customizations.yaml-prefixme Edit customizations.yaml For the following ~FIXME~ values, use the example provided and just remove the ~FIXME~ e.g.\nsma-rsyslog-aggregator: cray-service: service: loadBalancerIP: ~FIXME~ e.g. 10.92.100.72 rsyslogAggregatorHmn: service: loadBalancerIP: ~FIXME~ e.g. 10.94.100.2 sma-rsyslog-aggregator-udp: cray-service: service: loadBalancerIP: ~FIXME~ e.g. 10.92.100.75 rsyslogAggregatorUdpHmn: service: loadBalancerIP: ~FIXME~ e.g. 10.94.100.3 Review your changes:\nlinux# diff /mnt/pitdata/prep/site-init/customizations.yaml /mnt/pitdata/prep/site-init/customizations.yaml-prefixme Generate Sealed Secrets Secrets are stored in customizations.yaml as SealedSecret resources (i.e., encrypted secrets) which are deployed by specific charts and decrypted by the Sealed Secrets operator. But first, those secrets must be seeded generated and encrypted.\nLoad the zeromq container image required by Sealed Secret Generators:\nNOTE Requires a properly configured Docker or Podman environment.\nlinux# /mnt/pitdata/${CSM_RELEASE}/hack/load-container-image.sh dtr.dev.cray.com/zeromq/zeromq:v4.0.5 Re-encrypt existing secrets:\nlinux# /mnt/pitdata/prep/site-init/utils/secrets-reencrypt.sh /mnt/pitdata/prep/site-init/customizations.yaml \\ /mnt/pitdata/prep/site-init/certs/sealed_secrets.key /mnt/pitdata/prep/site-init/certs/sealed_secrets.crt Generate secrets:\nlinux# /mnt/pitdata/prep/site-init/utils/secrets-seed-customizations.sh /mnt/pitdata/prep/site-init/customizations.yaml Expected output looks similar to:\nCreating Sealed Secret keycloak-certs Generating type static_b64... Creating Sealed Secret keycloak-master-admin-auth Generating type static... Generating type static... Generating type randstr... Generating type static... Creating Sealed Secret cray_reds_credentials Generating type static... Generating type static... Creating Sealed Secret cray_meds_credentials Generating type static... Creating Sealed Secret cray_hms_rts_credentials Generating type static... Generating type static... Creating Sealed Secret vcs-user-credentials Generating type randstr... Generating type static... Creating Sealed Secret generated-platform-ca-1 Generating type platform_ca... Creating Sealed Secret pals-config Generating type zmq_curve... Generating type zmq_curve... Creating Sealed Secret munge-secret Generating type randstr... Creating Sealed Secret slurmdb-secret Generating type static... Generating type static... Generating type randstr... Generating type randstr... Creating Sealed Secret keycloak-users-localize Generating type static... Version Control Site-Init Files Setup /mnt/pitdata/prep/site-init as a Git repository in order to manage the baseline configuration during initial system installation.\nInitialize /mnt/pitdata/prep/site-info as a Git repository:\nlinux# cd /mnt/pitdata/prep/site-init linux# git init . (Optional) WARNING If production system or operational security is a concern, do NOT store the sealed secret private key in Git; instead, store the sealed secret key outside of Git in a secure offline system. To ensure these sensitive keys are not accidentally committed, configure .gitignore to ignore files under the certs directory:\nlinux# echo \u0026#34;certs/\u0026#34; \u0026gt;\u0026gt; .gitignore Stage site-init files to be committed:\nlinux# git add -A Review what will be committed:\nlinux# git status Commit all the above changes as the baseline configuration:\nlinux# git commit -m \u0026#34;Baseline configuration for $(/mnt/pitdata/${CSM_RELEASE}/lib/version.sh)\u0026#34; Push to a Remote Repository It is strongly recommended to that the site-init repository be maintained off-cluster. Add a remote repository and push the baseline configuration on master branch to a corresponding remote branch.\nCustomer-Specific Customizations Customer-specific customizations are any changes on top of the baseline configuration to satisfy customer-specific requirements. It is recommended that customer-specific customizations be tracked on branches separate from the mainline in order to make them easier to manage.\nApply any customer-specific customizations by merging the corresponding branches into master branch of /mnt/pitdata/prep/site-init.\nWhen considering merges, and especially when resolving conflicts, carefully examine differences to ensure all changes are relevant. For example, when applying a customer-specific customization used in a prior version, be sure the change still makes sense. It is common for options to change as new features are introduced and bugs are fixed.\nAppendix Tracked Sealed Secrets Tracked sealed secrets are regenerated every time secrets are seeded (see the use of utils/secrets-seed-customizations.sh above). View currently tracked sealed secrets via:\nlinux# yq read /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/customizations.yaml spec.kubernetes.tracked_sealed_secrets Expected output looks similar to:\n- cray_reds_credentials - cray_meds_credentials - cray_hms_rts_credentials In order to prevent tracked sealed secrets from being regenerated, they MUST BE REMOVED from the spec.kubernetes.tracked_sealed_secrets list in /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/customizations.yaml prior to seeding. To retain the REDS/MEDS/RTS credentials, run:\nlinux# yq delete -i /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/customizations.yaml spec.kubernetes.tracked_sealed_secrets.cray_reds_credentials linux# yq delete -i /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/customizations.yaml spec.kubernetes.tracked_sealed_secrets.cray_meds_credentials linux# yq delete -i /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/customizations.yaml spec.kubernetes.tracked_sealed_secrets.cray_hms_rts_credentials Decrypting Sealed Secrets for Review For administrators that would like to decrypt and review previously encrypted sealed secrets, you can use the secrets-decrypt.sh utility in SHASTA-CFG.\nSyntax: secret-decrypt.sh SEALED-SECRET-NAME SEALED-SECRET-PRIVATE-KEY CUSTOMIZATIONS-YAML\nlinux:/mnt/pitdata/prep/site-init# ./utils/secrets-decrypt.sh cray_meds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_redfish_defaults | sed -e \u0026#39;s/\u0026#34;//g\u0026#39; | base64 -d; echo Expected output looks similar to:\n{\u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;...\u0026#34;} "
},
{
	"uri": "/docs-csm/en-09/068-harvest-13-config/",
	"title": "HARVEST 13 CONFIG",
	"tags": [],
	"description": "",
	"content": "HARVEST 13 CONFIG This procedure provides advice for information to collect from a healthy Shasta v1.3 system.\nCollect data needed to prepare Shasta v1.4 installation pre-config files Save operational information about which components are disabled and why Save site customizations to use as a guide for customizing a Shasta v1.4 system Although some configuration data can be saved from a Shasta v1.3 system, there are new configuration files needed for Shasta v1.4. Some of this data is easier to collect from a running Shasta v1.3 system.\nThere may be some operational data to be saved such as any nodes which are disabled or marked down in a workload manager. These nodes might need hardware or firmware actions to repair them. If not addressed, and the newer firmware in v1.4 does not improve their performance or operation, then these may need to be disabled with v1.4 as well.\nThere may be site modifications to the system from v1.3 which are desired in v1.4. They cannot be directly copied to v1.4, however, recommendation will be made about what to save. Some saved information from v1.3 may be referenced when making a similar site modification to v1.3.\nStart a prep.install typescript with timestamps and run commands to collect information.\nAlthough Some of the commands in this procedure will output to a file, others will output to stdout and be captured by this typescript file.\nuser@host\u0026gt; ssh ncn-w001 ncn-w001# mkdir -p ~/prep.install.1.4 ncn-w001# cd ~/prep.install.1.4 ncn-w001# script -af prep.install.1.4.$(date +%Y-%m-%d).txt ncn-w001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Confirm installed version of CLE software.\nSome of the steps below will be different for v1.3.0 versus v1.3.2. Any variation in commands will be marked.\nncn-w001# cat /opt/cray/etc/release/cle PRODUCT=\u0026#34;Cray\u0026#39;s Linux Environment\u0026#34; OS=SLES15SP1 ARCH=x86_64 VERSION=1.3.2 DATE=20201014142855 Obtain the user ID and passwords for system components:\na. Obtain user ID and passwords for all the system management network spine, leaf, CDU, and aggregation switches. For example:\nsw-spine01-mtl sw-spine02-mtl sw-leaf01-mtl sw-leaf02-mtl sw-cdu01-mtl sw-cdu02-mtl\nUser id: admin Password: PASSWORD\nb. If necessary, obtain the user ID and password for the ClusterStor primary management node. For example, cls01053n00.\nUser id: admin Password: PASSWORD\nc. If necessary, obtain the user ID and password for the Arista edge switches.\nConfirm BMC username/password for management NCNs.\nUser id: root Password: PASSWORD\nNote: The external connection for ncn-w001 (BMC and node) will be moved to ncn-m001 for installation of the v1.4 software in a later step. It is critical that you be able to connect to the ncn-m001 BMC using a valid administrative username and password.\nCheck switches are reachable.\nSubstitute the system switch names in the following command for the correct number of spine, leaf, CDU, and aggregation switches in this system.\nncn-w001# for switch in sw-leaf0{1,2}-mtl sw-spine0{1,2}-mtl sw-cdu0{1,2}-mtl; do while true; \\ do ping -c 1 $switch \u0026gt; /dev/null; if [[ $? == 0 ]]; then echo \u0026#34;switch $switch is up\u0026#34;; break; \\ else echo \u0026#34;switch $switch is not up\u0026#34;; fi; sleep 5; done; done switch sw-leaf01-mtl is up switch sw-leaf02-mtl is up switch sw-spine01-mtl is up switch sw-spine02-mtl is up switch sw-cdu01-mtl is up switch sw-cdu02-mtl is up Note: The IP addresses for these switches will need to be changed in a later step to align with v1.4 software.\nCollect the current IP addresses for the switches.\nncn-w001# grep sw /etc/hosts | grep mtl 10.1.0.1 sw-spine01-mtl.local sw-spine01-mtl #-label-10.1.0.1 10.1.0.2 sw-leaf01-mtl.local sw-leaf01-mtl #-label-10.1.0.2 10.1.0.3 sw-spine02-mtl.local sw-spine02-mtl #-label-10.1.0.3 10.1.0.4 sw-leaf02-mtl.local sw-leaf02-mtl #-label-10.1.0.4 10.1.0.5 sw-cdu01-mtl.local sw-cdu01-mtl #-label-10.1.0.5 10.1.0.6 sw-cdu02-mtl.local sw-cdu02-mtl #-label-10.1.0.6 Check firmware on all leaf, spine, CDU, and aggregation switches meets expected level for v1.4. If they need to be updated, remember that for later in this procedure.\nFor minimum Network switch versions see Network Firmware\nCheck the version for each leaf switch:\nncn-w001# ssh admin@sw-leaf01-mtl switch# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2P1 Build Version: 10.5.0.2P1.482 Build Time: 2019-10-29T19:58:10+0000 System Type: S3048-ON Architecture: x86_64 Up Time: 9 weeks 5 days 05:17:33 Check the version for each spine switch:\nncn-w001# ssh admin@spine01-mtl Mellanox Switch sw-spine01 [standalone: master] \u0026gt; show version Product name: Onyx Product release: 3.9.0300 Build ID: #1-dev Build date: 2020-02-26 19:25:24 Target arch: x86_64 Target hw: x86_64 Built by: jenkins@7c42130d8bd6 Version summary: X86_64 3.9.0300 2020-02-26 19:25:24 x86_64 Product model: x86onie Host ID: 506B4BF4FCB0 System serial num: MT1845X03127 System UUID: 7ce27170-e2ba-11e8-8000-98039befd600 Uptime: 68d 5h 21m 36.256s CPU load averages: 3.20 / 3.20 / 3.18 Number of CPUs: 2 System memory: 2763 MB used / 5026 MB free / 7789 MB total Swap: 0 MB used / 0 MB free / 0 MB total Check the version for each CDU switch:\nncn-w001# ssh admin@sw-cdu01-mtl switch# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2P1 Build Version: 10.5.0.2P1.482 Build Time: 2019-10-29T19:58:10+0000 System Type: S4148T-ON Architecture: x86_64 Up Time: 4 weeks 6 days 04:01:07 Check the version for each aggregation switch:\nncn-w001# ssh admin@sw-agg01-mtl switch# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2P1 Build Version: 10.5.0.2P1.482 Build Time: 2019-10-29T19:58:10+0000 System Type: S4148T-ON Architecture: x86_64 Up Time: 4 weeks 6 days 04:01:07 Check firmware on Gigabyte nodes. If they need to be updated, remember that for later in this procedure.\nFor minimum NCN firmware versions see Node Firmware\nR272 nodes (worker nodes, master nodes, storage nodes, some UANs and some Application Nodes) should use Gigabyte firmware package 21.00, which includes\nBIOS version C17 BMC version 12.84.09 CMC version 62.84.02 H262 (mostly compute nodes) and R282 nodes (most UAN and Application Nodes) should use Gigabyte firmware package 20.03, which includes\nBIOS version C20 BMC version 12.84.09 CMC version 62.84.02 Check which version of sh-svr rpms are installed. If they are less than 20.03.00, then the firmware update bundle has not been installed or applied to the Gigabyte nodes. You will need to have both the firmware release tarball and the Gigabyte Node Firmware Update Guide (1.3.2) S-8010 to upgrade the firmware on Gigabyte nodes when indicated later in this procedure.\nncn-w001# rpm -qa | grep sh-svr sh-svr-1264up-bios-20.02.00-20201022025951_2289a89.x86_64 sh-svr-3264-bios-crayctldeploy-0.0.14-20201022025951_2289a89.x86_64 sh-svr-3264-bios-20.02.00-20201022025951_2289a89.x86_64 sh-svr-5264-gpu-bios-20.02.00-20201022025951_2289a89.x86_64 sh-svr-5264-gpu-bios-crayctldeploy-0.0.14-20201022025951_2289a89.x86_64 sh-svr-1264up-bios-crayctldeploy-0.0.14-20201022025951_2289a89.x86_64 If the 20.03.00 rpms have been installed, then run this script to check whether the firmware update has been applied to the nodes. Information about how to interpret the output of this command and the procedures for updating Gigabyte compute node firmware or Gigabyte non-compute node firmware is in the Gigabyte Node Firmware Update Guide (1.3.2) S-8010\nCheck the BIOS, BMC, and CMC firmware versions on nodes.\nncn-w001# bash /opt/cray/FW/bios/sh-svr-1264up-bios/sh-svr-scripts/find_GBT_Summary_for_Shasta_v1.3_rack.sh \\ 2\u0026gt;\u0026amp;1 | tee /var/tmp/rvr-inv Determine which Boot Orchestration Service (BOS) templates to use to shutdown compute nodes and UANs.\nFor example:\nCompute nodes: cle-1.3.2 UANs: uan-1.3.2\nObtain the authorization key for SAT.\nSee System Security and Authentication, Authenticate an Account with the Command Line, SAT Authentication in the Cray Shasta Administration Guide 1.3 S-8001 for more information.\nv1.3.0: Use Rev C of the guide v1.3.2: Use Rev E or later Save the list of nodes that are disabled.\nncn-w001# sat status --filter Enabled=false \u0026gt; sat.status.disabled Save a list of nodes that are off.\nncn-w001# sat status --filter State=Off \u0026gt; sat.status.off Save the Slurm status on nodes.\nncn-w001# ssh nid001000 sinfo \u0026gt; sinfo Save the Slurm list of nodes down and the reason why they are down.\nncn-w001# ssh nid001000 sinfo --list-reasons \u0026gt; sinfo.reasons Get for PBS status on nodes (which are offline or down)\nncn-w001# ssh nid001000 pbsnodes -l \u0026gt; pbsnodes Check Slingshot port status.\nncn-w001# /opt/cray/bringup-fabric/status.sh \u0026gt; fabric.status Verify Rosetta switches are accessible and healthy\nncn-w001# /opt/cray/bringup-fabric/ssmgmt_sc_check.sh \u0026gt; fabric.ssmgmt_sc_check Check current firmware.\nncn-w001# sat firmware \u0026gt; sat.firmware There will be a point after the v1.4 software has been installed when firmware needs to be updated on many components. Those components which need a firmware update while v1.3 is booted, will be addressed later.\nCheck Lustre server health.\nncn-w001# ssh admin@cls01234n00.system.com admin@cls01234n00]$ cscli show_nodes Save information from HSM about any site-defined groups/labels applied to nodes.\nHSM groups might be used in BOS session templates, but they may be used in conjunction with Ansible plays in VCS to configure nodes. Saving this information now will make it easier to reload it after v1.4 has been installed.\nSave information about label, description, and members of each group in HSM.\nncn-w001# cray hsm groups list --format json \u0026gt; hsm.groups Here is a sample group from that output.\n{ \u0026#34;description\u0026#34;: \u0026#34;NCNs running Lustre\u0026#34;, \u0026#34;members\u0026#34;: { \u0026#34;ids\u0026#34;: [ \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;x3000c0s9b0n0\u0026#34;, \u0026#34;x3000c0s11b0n0\u0026#34;, \u0026#34;x3000c0s25b0n0\u0026#34;, \u0026#34;x3000c0s13b0n0\u0026#34; ] }, \u0026#34;label\u0026#34;: \u0026#34;lnet_ncn\u0026#34; } Save a copy of the master branch in VCS with all of the site modifications from v1.3.\nSee https://git-scm.com/book/en/v2/Git-Tools-Bundling for information about how to manipulate this bundle repo to extract information.\nNote: This data cannot be directly imported and used on v1.4, but can serve as a reference to site modifications done with v1.3 which might be similar to those needed with v1.4.\nncn-w001# git clone https://api-gw-service-nmn.local/vcs/cray/config-management.git Cloning into \u0026#39;config-management\u0026#39;... remote: Enumerating objects: 3148, done. remote: Counting objects: 100% (3148/3148), done. remote: Compressing objects: 100% (846/846), done. remote: Total 3148 (delta 1341), reused 2518 (delta 1049) Receiving objects: 100% (3148/3148), 10.35 MiB | 27.59 MiB/s, done. Resolving deltas: 100% (1341/1341), done. Make a git bundle which is transportable as a single file \u0026ldquo;VCS.bundle\u0026rdquo;.\nncn-w001# git bundle create VCS.bundle HEAD master Check for other branches. There may be other branches, in addition to master, which could be saved.\nncn-w001# git branch -a remotes/origin/HEAD -\u0026gt; origin/master remotes/origin/cray/cme-premium/1.3.0 remotes/origin/cray/cme-premium/1.3.2 remotes/origin/cray/cray-pe/20.08 remotes/origin/cray/cray-pe/20.09 remotes/origin/cray/cray-pe/20.11 remotes/origin/master remotes/origin/slurm remotes/origin/slurm2 Save a copy of any site-modified recipes in IMS.\nYou do not need to save any of the HPE-provided recipes, but you may want to use any modified recipes as a guide to how the v1.4 HPE-provided recipes may need to be modified to meet site needs.\nList all recipes in IMS\nncn-w001# cray ims recipes list --format json [ { \u0026ldquo;recipe_type\u0026rdquo;: \u0026ldquo;kiwi-ng\u0026rdquo;, \u0026ldquo;linux_distribution\u0026rdquo;: \u0026ldquo;sles15\u0026rdquo;, \u0026ldquo;created\u0026rdquo;: \u0026ldquo;2020-09-04T03:22:15.764123+00:00\u0026rdquo;, \u0026ldquo;link\u0026rdquo;: { \u0026ldquo;path\u0026rdquo;: \u0026ldquo;s3://ims/recipes/0bfcf98b-3b73-49ad-ab08-5b868ed3dda2/recipe.tar.gz\u0026rdquo;, \u0026ldquo;etag\u0026rdquo;: \u0026ldquo;dec4e4b7dd734a0a24dcf4b67e69c2f5\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;s3\u0026rdquo; }, \u0026ldquo;id\u0026rdquo;: \u0026ldquo;0bfcf98b-3b73-49ad-ab08-5b868ed3dda2\u0026rdquo;, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;cray-sles15sp1-barebones-0.1.4\u0026rdquo; }, { \u0026ldquo;recipe_type\u0026rdquo;: \u0026ldquo;kiwi-ng\u0026rdquo;, \u0026ldquo;linux_distribution\u0026rdquo;: \u0026ldquo;sles15\u0026rdquo;, \u0026ldquo;created\u0026rdquo;: \u0026ldquo;2020-09-04T03:38:07.259114+00:00\u0026rdquo;, \u0026ldquo;link\u0026rdquo;: { \u0026ldquo;path\u0026rdquo;: \u0026ldquo;s3://ims/recipes/b463fb84-ffaf-4e00-81f1-1682acae2f25/recipe.tar.gz\u0026rdquo;, \u0026ldquo;etag\u0026rdquo;: \u0026ldquo;42f7b0c58ef5db1828dd772405f376b7\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;s3\u0026rdquo; }, \u0026ldquo;id\u0026rdquo;: \u0026ldquo;b463fb84-ffaf-4e00-81f1-1682acae2f25\u0026rdquo;, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;cray-sles15sp1-cle\u0026rdquo; }, { \u0026ldquo;recipe_type\u0026rdquo;: \u0026ldquo;kiwi-ng\u0026rdquo;, \u0026ldquo;linux_distribution\u0026rdquo;: \u0026ldquo;sles15\u0026rdquo;, \u0026ldquo;created\u0026rdquo;: \u0026ldquo;2020-10-29T23:31:51.340962+00:00\u0026rdquo;, \u0026ldquo;link\u0026rdquo;: { \u0026ldquo;path\u0026rdquo;: \u0026ldquo;s3://ims/recipes/49c703e9-3b95-4409-804f-b9c0e790487b/recipe.tar.gz\u0026rdquo;, \u0026ldquo;etag\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;s3\u0026rdquo; }, \u0026ldquo;id\u0026rdquo;: \u0026ldquo;49c703e9-3b95-4409-804f-b9c0e790487b\u0026rdquo;, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;cray-sles15sp1-cle-1.3.26\u0026rdquo; } ]\nReview the list of IMS recipes to determine which you want to save. For each IMS recipe that you want to save, use `cray artifacts get ims \u0026lt;object\u0026gt; \u0026lt;filename\u0026gt;` where object is the S3 key for the recipe.tgz (from the recipe list and filename is the filename you want to save the object to. Once downloaded, uncompress the tgz file to view the files and directories that comprise the recipe. From the output above, IMS has this metadata about the cray-sles15sp1-barebones-0.1.4 recipe. { \u0026ldquo;recipe_type\u0026rdquo;: \u0026ldquo;kiwi-ng\u0026rdquo;, \u0026ldquo;linux_distribution\u0026rdquo;: \u0026ldquo;sles15\u0026rdquo;, \u0026ldquo;created\u0026rdquo;: \u0026ldquo;2020-09-04T03:22:15.764123+00:00\u0026rdquo;, \u0026ldquo;link\u0026rdquo;: { \u0026ldquo;path\u0026rdquo;: \u0026ldquo;s3://ims/recipes/0bfcf98b-3b73-49ad-ab08-5b868ed3dda2/recipe.tar.gz\u0026rdquo;, \u0026ldquo;etag\u0026rdquo;: \u0026ldquo;dec4e4b7dd734a0a24dcf4b67e69c2f5\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;s3\u0026rdquo; }, \u0026ldquo;id\u0026rdquo;: \u0026ldquo;0bfcf98b-3b73-49ad-ab08-5b868ed3dda2\u0026rdquo;, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;cray-sles15sp1-barebones-0.1.4\u0026rdquo; },\nThis command will extract the recipe from the IMS S3 bucket and place it into a filename which is based on the name from the IMS recipe. ```bash ncn-w001# cray artifacts get ims recipes/0bfcf98b-3b73-49ad-ab08-5b868ed3dda2/recipe.tar.gz cray-sles15sp1-barebones-0.1.4.tgz Save a copy of any BOS session templates of interest.\nThere might be organizational information, such as the names of BOS session templates or how they specify the nodes to be booted using a list of groups or list of nodes, but there might also be special kernel parameters added by the site.\nncn-w001# cray bos v1 sessiontemplate list --format \\ json \u0026gt; bos.sessiontemplate.list.json Save a copy of any IMS images which might have been directly customized.\nIf all images were built from an IMS recipe and then customized with CFS before booting, then this step can be skipped.\nList all IMS images.\nncn-w001# cray ims images list --format json [ { \u0026#34;link\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/9f32ed7b-9e1c-444d-8b63-40cefbf7e846/manifest.json\u0026#34;, \u0026#34;etag\u0026#34;: \u0026#34;db1b16d771a5a88cb320519e066348b8\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;id\u0026#34;: \u0026#34;9f32ed7b-9e1c-444d-8b63-40cefbf7e846\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cle_default_rootfs_cfs_2488e992-ee60-11ea-88f1-b42e993b710e\u0026#34;, \u0026#34;created\u0026#34;: \u0026#34;2020-09-04T03:42:51.474549+00:00\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/dc1a430f-82cb-4b9b-a1dd-7c8540721013/manifest.json\u0026#34;, \u0026#34;etag\u0026#34;: \u0026#34;5f243b60b4211c47e79b837df2271692\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;id\u0026#34;: \u0026#34;dc1a430f-82cb-4b9b-a1dd-7c8540721013\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cle_default_rootfs_cfs_test_20200914125841b\u0026#34;, \u0026#34;created\u0026#34;: \u0026#34;2020-09-14T20:46:51.367749+00:00\u0026#34; }, ] Review the list of IMS images to determine which you want to save.\nThis example uses the \u0026ldquo;cle_default_rootfs_cfs_test_20200914125841b\u0026rdquo; image from above.\nFor each image that you want to save perform the following steps, use cray artifacts get boot-images \u0026lt;object\u0026gt; \u0026lt;filename\u0026gt; where object is the S3 key for the IMS manifest file and filename is the filename you want to save the manifest file to.\nFor each artifact in the manifest file, use cray artifacts get boot-images \u0026lt;object\u0026gt; \u0026lt;filename\u0026gt; where object is the S3 key for the image artifact (from the manifest file) and filename is the filename you want to save the artifact to.\nGet the manifest file using the path to the S3 boot-images bucket and save as a local filename \u0026ldquo;manifest.json\u0026rdquo;.\nncn-w001# cray artifacts get boot-images fe4b8429-4ea7-4696-8018-2c7950a75e4b/manifest.json manifest.json Example manifest.json file for an IMS image\nncn-w001# cat manifest.json { \u0026#34;artifacts\u0026#34;: [ { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;1f9f88a65be7fdd8190008e06b1da2c0-175\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/dc1a430f-82cb-4b9b-a1dd-7c8540721013/rootfs\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;d402ea98531f995106918815e4d74cc4\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.rootfs.squashfs\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;d76adf4ee44b6229dad69103c38d49ca\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/dc1a430f-82cb-4b9b-a1dd-7c8540721013/kernel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;d76adf4ee44b6229dad69103c38d49ca\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.kernel\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;43b0f96a951590cc478ae7b311e3e413-4\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/dc1a430f-82cb-4b9b-a1dd-7c8540721013/initrd\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;9112b42d5e3da6902cc62158d6482552\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.initrd\u0026#34; } ], \u0026#34;created\u0026#34;: \u0026#34;2020-09-14 20:47:01.211752\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; } Changes might have been made in the initrd or in the rootfs or both. This example shows how to extract one of those files.\nExtract the rootfs (squashfs) file.\nncn-w001# cray artifacts get boot-images dc1a430f-82cb-4b9b-a1dd-7c8540721013/rootfs rootfs Dump the information from SLS.\nSome information from SLS can be extracted for use in the pre-config file, hmn_connections.json, and cabinets.yaml for the v1.4 installation. The Cray Shasta_administration Guide 1.3 S-8001 has a section \u0026ldquo;Dump SLS Information\u0026rdquo;.\nIf there is a fully current SHCD (Shasta Cabling Diagram, previously called CCD for Cray Cabling Diagram) spreadsheet file for this system, there is a way to extract information from it to create the hmn_connections.json file later in the v1.4 installation process. However, SLS data may be more current than the SHCD or there may not be a valid SHCD file for this system if cabling changes have not been recorded as updates to the SHCD file. Saving this SLS information while v1.3 is booted may provide a point of comparison with the data in the SHCD.\nThis procedure will create three files in the current directory (private_key.pem, public_key.pem, sls_dump.json). These files should be kept in a safe and secure place as the private key can decrypt the encrypted passwords stored in the SLS dump file.\nUse the get_token function to retrieve a token to validate requests to the API gateway.\nncn-w001# function get_token () { ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d) curl -s -d grant_type=client_credentials -d client_id=admin-client -d client_secret=$ADMIN_SECRET \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | python \\ -c \u0026#39;import sys, json; print json.load(sys.stdin)[\u0026#34;access_token\u0026#34;]\u0026#39; } Generate a private and public key pair.\nExecute the following commands to generate a private and public key to use for the dump.\nncn-w001# openssl genpkey -out private_key.pem -algorithm RSA -pkeyopt rsa_keygen_bits:2048 ncn-w001# openssl rsa -in private_key.pem -outform PEM -pubout -out public_key.pem The above commands will create two files the private key private_key.pem file and the public key public_key.pem file.\nMake sure to use a new private and public key pair for each dump operation, and do not reuse an existing private and public key pair. The private key should be treated securely because it will be required to decrypt the SLS dump file when the dump is loaded back into SLS. Once the private key is used to load state back into SLS, it should be considered insecure.\nPerform the SLS dump.\nThe SLS dump will be stored in the sls_dump.json file. The sls_dump.json and private_key.pem files are required to perform the SLS load state operation.\nncn-w001# curl -X POST \\ https://api-gw-service-nmn.local/apis/sls/v1/dumpstate \\ -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; \\ -F public_key=@public_key.pem \u0026gt; sls_dump.json % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 591k 0 590k 100 663 122k 137 0:00:04 0:00:04 --:--:-- 163k Save the contents of the /opt/cray/site-info directory.\nThe customizations.yaml and certs subdirectory have information which is needed for the v1.4 install.\nncn-w001# tar cf site-info.tar /opt/cray/site-info Collect MAC address information from the leaf switches for each management NCN and its BMC.\nThis will be used during the v1.4 install in the ncn_metadata.csv file.\nncn-w001# ssh admin@sw-leaf01-mtl Show vlan 4 which is the NMN. This shows the MAC addresses which will be used to boot the nodes.\nsw-leaf01# show mac address-table vlan 4 VlanId Mac Address Type Interface 4 00:0a:5c:90:1b:bf dynamic port-channel100 4 00:0a:9c:62:20:2e dynamic ethernet1/1/41 4 00:40:a6:82:f7:73 dynamic ethernet1/1/42 4 00:40:a6:83:07:ff dynamic ethernet1/1/43 4 00:40:a6:83:08:65 dynamic ethernet1/1/44 4 00:40:a6:83:08:8d dynamic ethernet1/1/45 4 3c:2c:30:67:c0:b5 dynamic port-channel100 4 50:6b:4b:9c:c6:48 dynamic port-channel100 4 50:9a:4c:e0:32:d1 dynamic port-channel100 4 50:9a:4c:e0:88:d1 dynamic port-channel100 4 98:03:9b:ef:d6:48 dynamic port-channel100 4 b4:2e:99:3b:70:28 dynamic ethernet1/1/46 4 b4:2e:99:3b:70:30 dynamic ethernet1/1/47 4 b4:2e:99:3b:70:50 dynamic ethernet1/1/34 4 b4:2e:99:3b:70:58 dynamic ethernet1/1/48 4 b4:2e:99:3b:70:c0 dynamic ethernet1/1/38 4 b4:2e:99:3b:70:c8 dynamic ethernet1/1/36 4 b4:2e:99:3b:70:d0 dynamic ethernet1/1/37 4 b4:2e:99:3b:70:d4 dynamic port-channel100 4 b4:2e:99:3b:70:d8 dynamic port-channel100 4 b4:2e:99:3e:82:66 dynamic ethernet1/1/33 4 b4:2e:99:a6:5d:df dynamic ethernet1/1/25 4 b8:59:9f:2b:2f:9e dynamic port-channel100 4 b8:59:9f:2b:30:fa dynamic port-channel100 4 b8:59:9f:34:88:be dynamic port-channel100 4 b8:59:9f:34:88:c6 dynamic port-channel100 4 b8:59:9f:34:89:3a dynamic port-channel100 4 b8:59:9f:34:89:46 dynamic port-channel100 4 b8:59:9f:34:89:4a dynamic port-channel100 4 b8:59:9f:f9:1b:e6 dynamic port-channel100 Show vlan 1 which is the HMN which has the node\u0026rsquo;s BMC MAC address. This is needed for DHCP of the node BMC.\nsw-leaf01# show mac address-table vlan 1 VlanId Mac Address Type Interface 1 3c:2c:30:67:c0:b5 dynamic port-channel100 1 50:6b:4b:9c:c6:20 dynamic port-channel100 1 50:6b:4b:9c:c6:48 dynamic port-channel100 1 50:9a:4c:e0:32:d1 dynamic port-channel100 1 50:9a:4c:e0:88:d1 dynamic port-channel100 1 98:03:9b:ef:d6:20 dynamic port-channel100 1 98:03:9b:ef:d6:48 dynamic port-channel100 1 b8:59:9f:2b:2e:aa dynamic port-channel100 1 b8:59:9f:2b:2e:b6 dynamic port-channel100 1 b8:59:9f:2b:2f:9e dynamic port-channel100 1 b8:59:9f:2b:30:82 dynamic port-channel100 1 b8:59:9f:2b:30:fa dynamic port-channel100 1 b8:59:9f:2b:31:0a dynamic port-channel100 1 b8:59:9f:34:88:be dynamic port-channel100 1 b8:59:9f:34:88:c6 dynamic port-channel100 1 b8:59:9f:34:89:3a dynamic port-channel100 1 b8:59:9f:34:89:46 dynamic port-channel100 1 b8:59:9f:34:89:4a dynamic port-channel100 1 b8:59:9f:f9:1b:e6 dynamic port-channel100 sw-leaf01# exit Another way to collect information about the BMC MAC address is this loop. Set the correct number of storage and worker nodes.\nncn-w001# nodes=\u0026#34;\u0026#34; ncn-w001# for name in ncn-m00{1,2,3} ncn-s00{1,2,3} ncn-w00{1,2,3,4,5}; do nodes=\u0026#34;$nodes $name\u0026#34;; done ncn-w001# echo $nodes ncn-m001 ncn-m002 ncn-m003 ncn-s001 ncn-s002 ncn-s003 ncn-w001 ncn-w002 ncn-w003 ncn-w004 ncn-w005 Use \u0026ldquo;ipmitool lan print\u0026rdquo; to determine the BMC MAC address.\nGigabyte nodes should use \u0026ldquo;lan print 1\u0026rdquo;. Intel nodes should use \u0026ldquo;lan print 3\u0026rdquo;. ncn-w001# for ncn in $nodes; do echo $ncn; ssh $ncn ipmitool lan print 1 | grep \u0026#34;MAC Address\u0026#34;; done ncn-m001 MAC Address : b4:2e:99:3b:70:c0 ncn-m002 MAC Address : b4:2e:99:3b:70:d0 ncn-m003 MAC Address : b4:2e:99:3b:70:c8 ncn-s001 MAC Address : b4:2e:99:3b:70:d8 ncn-s002 MAC Address : b4:2e:99:3b:70:d4 ncn-s003 MAC Address : b4:2e:99:3b:70:58 ncn-w001 MAC Address : b4:2e:99:3b:71:10 ncn-w002 MAC Address : b4:2e:99:3b:70:50 ncn-w003 MAC Address : b4:2e:99:3e:82:66 ncn-w004 MAC Address : b4:2e:99:a6:5d:df ncn-w005 MAC Address : b4:2e:99:3b:70:28 Collect output from \u0026ldquo;ip address\u0026rdquo; for all management NCNs.\nThis is another source of MAC address information, but also indicates the device names which will be used for the bonded interface bond0.\nncn-w001# for ncn in $nodes; do echo $ncn; ssh $ncn ip address ; done Check full BMC information for ncn-w001 because the connection will be moved to ncn-m001.\nThe IP address, subnet mask, and default gateway IP will be needed.\nncn-w001# ipmitool lan print Set in Progress : Set Complete Auth Type Support : NONE MD2 MD5 PASSWORD OEM Auth Type Enable : Callback : MD5 : User : MD5 : Operator : MD5 : Admin : MD5 : OEM : MD5 IP Address Source : Static Address IP Address : 172.30.56.3 Subnet Mask : 255.255.240.0 MAC Address : b4:2e:99:3b:71:10 SNMP Community String : AMI IP Header : TTL=0x40 Flags=0x40 Precedence=0x00 TOS=0x10 BMC ARP Control : ARP Responses Enabled, Gratuitous ARP Disabled Gratituous ARP Intrvl : 1.0 seconds Default Gateway IP : 172.30.48.1 Default Gateway MAC : 00:00:00:00:01:50 Backup Gateway IP : 0.0.0.0 Backup Gateway MAC : 00:00:00:00:00:00 802.1q VLAN ID : Disabled 802.1q VLAN Priority : 0 RMCP+ Cipher Suites : 0,1,2,3,6,7,8,11,12,15,16,17 Cipher Suite Priv Max : caaaaaaaaaaaXXX : X=Cipher Suite Unused : c=CALLBACK : u=USER : o=OPERATOR : a=ADMIN : O=OEM Bad Password Threshold : 0 Invalid password disable: no Attempt Count Reset Int.: 0 User Lockout Interval : 0 Check Mellanox firmware for the CX-4 and CX-5 on management NCNs.\nFor minimum NCN firmware versions see Node Firmware\nConfirm the version of the mft RPM installed on the management NCNs. The mft-4.14.0-105.x86_64 version is known to work to report the firmware version installed.\nncn-w001# for ncn in $nodes; do echo $ncn; ssh $ncn rpm -q mft; done Check version of firmware installed.\nncn-w001# for ncn in $nodes; do echo $ncn; ssh $ncn mlxfwmanager | egrep \u0026#34;FW|Device\u0026#34; ; done Output from each node will look like this. Compare this information with the versions in Node Firmware\nDevice #1: Device Type: ConnectX4 PCI Device Name: 0000:42:00.0 FW 12.26.4012 N/A Device #2: Device Type: ConnectX5 PCI Device Name: 0000:41:00.0 FW 16.28.4000 N/A Device #3: Device Type: ConnectX5 PCI Device Name: 0000:81:00.0 FW 16.28.4000 N/A When reinstalling a v1.3 system with v1.4 software, preserving the SDU configuration, plug-ins, and locally stored dump files may be desired. Run the following command and store the resulting tar file.\nncn-w001# tar czvf sdu-shastav1.3.tar.gz /opt/cray/sdu \\ /opt/cray/sdu-shasta-plugins/default \\ /etc/opt/cray/sdu /var/opt/cray/sdu SMA services are stateful and stateless. For stateful SMA services, the data is saved in PVs. The data comprises of configuration attributes, telemetry streams, logging info, etc. Upon an upgrade, the data in all SMA PVs must be saved. The enumeration of PVs for SMA services is displayed by running the following command. ncn-w001# kubectl -n sma get pvc LDMS, an SMA service, saves configuration data on the base OS for CNs and NCNs. The base OS directory /etc/sysconfig/ldms.d and all sub-directories under this path must be preserved, both on CNs and NCNs. Additionally, NCN configuration must be preserved in the locations as follows. Samplers: /etc/sysconfig/ldms.d dir and the entire tree must be preserved. Aggregator: The following PVs along with the data, must be preserved: sma/ldms-sms-aggr-pvc sma/ldms-sms-smpl-pvc sma/ldms-compute-aggr-pvc sma/ldms-compute-smpl-pvc The checks have all completed.\nFinish the typescript file\nncn-w001# exit Save the typescript file and the output from all of the above commands somewhere off the Shasta v1.3 system.\n"
},
{
	"uri": "/docs-csm/en-09/069-upgrade-workarounds-rpm/",
	"title": "Upgrade CSM Install Workarounds RPM",
	"tags": [],
	"description": "",
	"content": "Upgrade CSM Install Workarounds RPM Workarounds for CSM are built and can be distributed separately from the main CSM release. This allows for future alternations of workarounds without having to re-release all of CSM. If this happens, it is necessary to upgrade the RPM from the originally installed version to the newly distributed one to receive the latest workarounds.\nThe process for doing this is quite simple:\nDownload or copy the RPM to ncn-m001. Run: ncn-m001# rpm -Uhv /path/to/csm-install-workarounds-*.noarch.rpm "
},
{
	"uri": "/docs-csm/en-09/070-usp-install-workarounds/",
	"title": "Utility Storage Installation Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Utility Storage Installation Troubleshooting Occasionally we observe an installation failure during the ceph install. We will break these up into scenarios. Please match your scenario prior to executing any workarounds\nScenario 1 IMPORTANT (FOR NODE INSTALLS/REINSTALLS ONLY): If your Ceph install failed please check the following\nncn-s# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 83.83459 root default -5 27.94470 host ncn-s001 0 ssd 3.49309 osd.0 up 1.00000 1.00000 4 ssd 3.49309 osd.4 up 1.00000 1.00000 6 ssd 3.49309 osd.6 up 1.00000 1.00000 8 ssd 3.49309 osd.8 up 1.00000 1.00000 10 ssd 3.49309 osd.10 up 1.00000 1.00000 12 ssd 3.49309 osd.12 up 1.00000 1.00000 14 ssd 3.49309 osd.14 up 1.00000 1.00000 16 ssd 3.49309 osd.16 up 1.00000 1.00000 -3 27.94470 host ncn-s002 1 ssd 3.49309 osd.1 down 1.00000 1.00000 3 ssd 3.49309 osd.3 down 1.00000 1.00000 5 ssd 3.49309 osd.5 down 1.00000 1.00000 7 ssd 3.49309 osd.7 down 1.00000 1.00000 9 ssd 3.49309 osd.9 down 1.00000 1.00000 11 ssd 3.49309 osd.11 down 1.00000 1.00000 13 ssd 3.49309 osd.13 down 1.00000 1.00000 15 ssd 3.49309 osd.15 down 1.00000 1.00000 -7 27.94519 host ncn-s003 \u0026lt;--- node where our issue exists 2 ssd 27.94519 osd.2 down 1.00000 1.00000 \u0026lt;--- our problematic VG. SSH to our node(s) where the issue exists and do the following:\nncn-s# systemctl stop ceph-osd.target ncn-s# vgremove -f \u0026ndash;select \u0026lsquo;vg_name=~ceph*\u0026rsquo; This will take a little bit of time, so do not panic.* ncn-s# for i in {g..n}; do sgdisk \u0026ndash;zap-all /dev/sd$i; done. This will vary node to node and you should use lsblk to identify all drives available to Ceph\nManually create OSDs on the problematic nodes ncn-s# for i in {g..n}; do ceph-volume lvm create \u0026ndash;data /dev/sd$i \u0026ndash;bluestore; done\nALL THE BELOW WORK WILL BE RUN FROM NCN-S001\nVerify the /etc/cray/ceph directory is empty. If there are any files there then delete them Put in safeguard Edit /srv/cray/scripts/metal/lib.sh Comment out the below lines ```bash 22 if [ $wipe == 'yes' ]; then 23 ansible osds -m shell -a \u0026quot;vgremove -f --select 'vg_name=~ceph*'\u0026quot; 24 fi``` Run the cloud init script ncn-s001# /srv/cray/scripts/common/storage-ceph-cloudinit.sh\n"
},
{
	"uri": "/docs-csm/en-09/100-ncn-images/",
	"title": "Non-Compute Node Images",
	"tags": [],
	"description": "",
	"content": "Non-Compute Node Images There are several flavors of NCN images, each share a common base image. When booting NCNs an admin or user will need to choose between stable (Release) and unstable (pre-release/dev) images.\nFor details on how these images behave and inherit from the base and common images, see [node-image-docs][1].\nIn short, each application image (i.e. Kubernetes and storage-ceph) inherit from the non-compute-common layer. Operationally these are all that matter; the common layer, Kubernetes layer, Ceph layer, and any other new application images.\nTo boot an NCN, you need 3 artifacts for each node-type (kubernetes-manager/worker, ceph):\nThe Kubernetes SquashFS ([stable][4] or [unstable][5]) initrd-img-[RELEASE].xz $version-[RELEASE].kernel kubernetes-[RELEASE].squashfs The CEPH SquashFS ([stable][6] or [unstable][7]) initrd-img-[RELEASE].xz $version-[RELEASE].kernel storage-ceph-[RELEASE].squashfs For information on pulling and swapping other NCN images, see 107-NCN-DEVEL.\nLiveCD Server View the current ephemeral data payload:\npit# ls -l /var/www total 8 drwxr-xr-x 1 dnsmasq tftp 4096 Dec 17 21:20 boot drwxr-xr-x 7 root root 4096 Dec 2 04:45 ephemeral pit# ls -l /var/www/ephemeral/data/* /var/www/ephemeral/data/ceph: total 4 drwxr-xr-x 2 root root 4096 Dec 17 21:42 0.0.7 /var/www/ephemeral/data/k8s: total 4 drwxr-xr-x 2 root root 4096 Dec 17 21:26 0.0.8 Setup the \u0026ldquo;booting repos\u0026rdquo;:\npit# set-sqfs-links.sh Mismatching kernels! The discovered artifacts will deploy an undesirable stack. mkdir: created directory \u0026#39;ncn-m001\u0026#39; /var/www/ncn-m001 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-m002\u0026#39; /var/www/ncn-m002 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-m003\u0026#39; /var/www/ncn-m003 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-w002\u0026#39; /var/www/ncn-w002 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-w003\u0026#39; /var/www/ncn-w003 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-s001\u0026#39; /var/www/ncn-s001 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-s002\u0026#39; /var/www/ncn-s002 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-s003\u0026#39; /var/www/ncn-s003 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs\u0026#39; /var/www Viewing the currently set links\npit# ls -l /var/www/ncn-* boot: total 1552 -rw-r--r-- 1 root root 166634 Dec 17 13:21 graffiti.png -rw-r--r-- 1 dnsmasq tftp 700480 Dec 17 13:25 ipxe.efi -rw-r--r-- 1 dnsmasq tftp 700352 Dec 15 09:35 ipxe.efi.stable -rw-r--r-- 1 root root 6157 Dec 15 05:12 script.ipxe -rw-r--r-- 1 root root 6284 Dec 17 13:21 script.ipxe.rpmnew ephemeral: total 32 drwxr-xr-x 2 root root 4096 Dec 6 22:18 configs drwxr-xr-x 4 root root 4096 Dec 7 04:29 data drwx------ 2 root root 16384 Dec 2 04:25 lost+found drwxr-xr-x 4 root root 4096 Dec 3 02:31 prep drwxr-xr-x 2 root root 4096 Dec 2 04:45 static ncn-m001: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel ncn-m002: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel ncn-m003: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel ncn-s001: total 4 lrwxrwxrwx 1 root root 56 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs lrwxrwxrwx 1 root root 48 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz lrwxrwxrwx 1 root root 62 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel ncn-s002: total 4 lrwxrwxrwx 1 root root 56 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs lrwxrwxrwx 1 root root 48 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz lrwxrwxrwx 1 root root 62 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel ncn-s003: total 4 lrwxrwxrwx 1 root root 56 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs lrwxrwxrwx 1 root root 48 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz lrwxrwxrwx 1 root root 62 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel ncn-w002: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel ncn-w003: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel "
},
{
	"uri": "/docs-csm/en-09/101-ncn-booting/",
	"title": "NCN Booting",
	"tags": [],
	"description": "",
	"content": "NCN Booting Non-compute nodes boot two ways:\nNetwork/PXE booting Disk Booting Table of Contents:\nHow can I tell if I booted via disk or pxe? Set BMCs to DHCP Boot Order Setting Order Trimming Examples Reverting Changes Locating a USB Stick How can I tell if I booted via disk or pxe? Two ways, one may be easier depending on your env.\ncat /proc/cmdline if it starts with kernel then the node network booted. If it starts with BOOT_IMAGE=( then it disk booted. efibootmgr, see what it says for BootCurrent and match that value to the list beneath to see if it lines up with a networking option or a cray sd*) option for disk boots. Set BMCs to DHCP If you are reinstalling a system, the BMCs for the NCNs may be set to static. We check /var/lib/misc/dnsmasq.leases for setting up the symlinks for the artifacts each node needs to boot. So if your BMCs are set to static, those artifacts will not get setup correctly. You can set them back to DHCP by using a command as such:\nfor h in $( grep mgmt /etc/dnsmasq.d/statics.conf | grep -v m001 | awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; ) do ipmitool -U username -I lanplus -H $h -P password lan set 1 ipsrc dhcp done Some BMCs need a cold reset in order to fully pick up this change:\nfor h in $( grep mgmt /etc/dnsmasq.d/statics.conf | grep -v m001 | awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; ) do ipmitool -U username -I lanplus -H $h -P password mc reset cold done Set Boot Order ipmitool can set and edit boot order; works better on some vendors based on their BMC implementation efibootmgr speaks directly to systems UEFI; can only be ignored by new BIOS activity NOTE Cloud-init will set bootorder on boot, but this is bugged (CASMINST-1686) with certain vendors.\nSetting Order Create our boot-bios-selector file(s) based on the manufacturer: Gigabyte Technology Masters ncn-m# efibootmgr | grep -iP \u0026#39;(pxe ipv?4.*adapter)\u0026#39; | tee /tmp/bbs1 Boot0007* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:BE:8F:2E Boot0009* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:BE:8F:2F ncn-m# efibootmgr | grep cray | tee /tmp/bbs2 Boot0000* cray (sda1) Boot0002* cray (sdb1) Storage ncn-s# efibootmgr | grep -iP \u0026#39;(pxe ipv?4.*adapter)\u0026#39; | tee /tmp/bbs1 Boot0007* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:C7:11:FA Boot0009* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:C7:11:FB ncn-s# efibootmgr | grep cray | tee /tmp/bbs2 Boot0000* cray (sda1) Boot0002* cray (sdb1) Workers NOTE If more than 3 interfaces appear in /tmp/bbs1 the administrator may want to consider disabling PXE on their HSN cards. On the other hand, the rogue boot entry can be removed with a hand crafted efibootmgr -b \u0026lt;num\u0026gt; -B command.\nncn-w# efibootmgr | grep -iP \u0026#39;(pxe ipv?4.*adapter)\u0026#39; | tee /tmp/bbs1 Boot0007* UEFI: PXE IP4 Mellanox Network Adapter - 98:03:9B:AA:88:30 Boot0009* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:89:2A Boot000B* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:89:2B ncn-w# efibootmgr | grep cray | tee /tmp/bbs2 Boot0000* cray (sda1) Boot0002* cray (sdb1) Hewlett-Packard Enterprise Masters ncn-m# efibootmgr | grep -i \u0026#39;port 1\u0026#39; | grep -i \u0026#39;pxe ipv4\u0026#39; | tee /tmp/bbs1 Boot0014* OCP Slot 10 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - PXE (PXE IPv4) Boot0018* Slot 1 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - PXE (PXE IPv4) ncn-m# efibootmgr | grep cray | tee /tmp/bbs2 Boot0021* cray (sdb1) Boot0022* cray (sdc1) Storage ncn-s# efibootmgr | grep -i \u0026#39;port 1\u0026#39; | grep -i \u0026#39;pxe ipv4\u0026#39; | tee /tmp/bbs1 Boot001C* OCP Slot 10 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - PXE (PXE IPv4) Boot001D* Slot 1 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - PXE (PXE IPv4) ncn-s# efibootmgr | grep cray | tee /tmp/bbs2 Boot0002* cray (sdg1) Boot0020* cray (sdh1) Workers ncn-w# efibootmgr | grep -i \u0026#39;port 1\u0026#39; | grep -i \u0026#39;pxe ipv4\u0026#39; | tee /tmp/bbs1 Boot0012* OCP Slot 10 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - PXE (PXE IPv4) ncn-w# ncn-w# efibootmgr | grep cray | tee /tmp/bbs2 Boot0017* cray (sdb1) Boot0018* cray (sdc1) Intel Corporation Masters ncn-m# efibootmgr | grep -i \u0026#39;ipv4\u0026#39; | grep -iv \u0026#39;baseboard\u0026#39; | tee /tmp/bbs1 Boot000E* UEFI IPv4: Network 00 at Riser 02 Slot 01 Boot0014* UEFI IPv4: Network 01 at Riser 02 Slot 01 ncn-m# efibootmgr | grep -i \u0026#39;cray\u0026#39; | tee /tmp/bbs2 Boot0011* cray (sda1) Boot0012* cray (sdb1) Storage ncn-s# efibootmgr | grep -i \u0026#39;ipv4\u0026#39; | grep -iv \u0026#39;baseboard\u0026#39; | tee /tmp/bbs1 Boot000E* UEFI IPv4: Network 00 at Riser 02 Slot 01 Boot0012* UEFI IPv4: Network 01 at Riser 02 Slot 01 ncn-s# efibootmgr | grep -i \u0026#39;cray\u0026#39; | tee /tmp/bbs2 Boot0014* cray (sda1) Boot0015* cray (sdb1) Workers ncn-w# efibootmgr | grep -i \u0026#39;ipv4\u0026#39; | grep -iv \u0026#39;baseboard\u0026#39; | tee /tmp/bbs1 Boot0008* UEFI IPv4: Network 00 at Riser 02 Slot 01 Boot000C* UEFI IPv4: Network 01 at Riser 02 Slot 01 ncn-w# efibootmgr | grep -i \u0026#39;cray\u0026#39; | tee /tmp/bbs2 Boot0010* cray (sda1) Boot0011* cray (sdb1) Set Order (works universally; every vendor, every Shasta ncn-type): ncn# efibootmgr -o $(cat /tmp/bbs* | awk \u0026#39;!x[$0]++\u0026#39; | sed \u0026#39;s/^Boot//g\u0026#39; | tr -d \u0026#39;*\u0026#39; | awk \u0026#39;{print $1}\u0026#39; | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) | grep -i bootorder BootOrder: 000E,0014,0011,0012 Mark Active (works universally; every vendor, every Shasta ncn-type): ncn# cat /tmp/bbs* | awk \u0026#39;!x[$0]++\u0026#39; | sed \u0026#39;s/^Boot//g\u0026#39; | tr -d \u0026#39;*\u0026#39; | awk \u0026#39;{print $1}\u0026#39; | xargs -r -t -i efibootmgr -b {} -a After following these steps on a given NCN, that NCN will now use the desired Shasta boot order. Note that if USB Boot Priority (or similar) is enabled in BIOS, this can override the boot order set here.\nTrimming As for removing entries, this section will only advise on removing other PXE entries. There are too many vendor-specific entries beyond disks and NICs to cover in this section (e.g. BIOS entries, iLO entries, etc.).\nSimply run the reverse-pattern of the PXE commands from the setting boot order section:\nFind the other PXE entries: Gigabyte Technology: # on an NCN, or on the PIT node efibootmgr | grep -ivP \u0026#39;(pxe ipv?4.*)\u0026#39; | grep -iP \u0026#39;(adapter|connection|nvme|sata)\u0026#39; | tee /tmp/rbbs1 efibootmgr | grep -iP \u0026#39;(pxe ipv?4.*)\u0026#39; | grep -i connection | tee /tmp/rbbs2 Hewlett-Packard Enterprise NOTE This does not trim HSN Mellanox cards; these should disable their OpROMs using the high speed network snippet(s).\n# on an NCN, or on the PIT node efibootmgr | grep -vi \u0026#39;pxe ipv4\u0026#39; | grep -i adapter |tee /tmp/rbbs1 efibootmgr | grep -iP \u0026#39;(sata|nvme)\u0026#39; | tee /tmp/rbbs2 Intel Corporation # on an NCN, or on the PIT node efibootmgr | grep -vi \u0026#39;ipv4\u0026#39; | grep -iP \u0026#39;(sata|nvme|uefi)\u0026#39; | tee /tmp/rbbs1 efibootmgr | grep -i baseboard | tee /tmp/rbbs2 Remove them: ncn# cat /tmp/rbbs* | awk \u0026#39;!x[$0]++\u0026#39; | sed \u0026#39;s/^Boot//g\u0026#39; | awk \u0026#39;{print $1}\u0026#39; | tr -d \u0026#39;*\u0026#39; | xargs -t -i efibootmgr -b {} -B Your boot menu should be trimmed down to only contain relevant entries.\nExamples Master node (with onboards enabled):\nncn-m# efibootmgr BootCurrent: 0009 Timeout: 2 seconds BootOrder: 0004,0000,0007,0009,000B,000D,0012,0013,0002,0003,0001 Boot0000* cray (sda1) Boot0001* UEFI: Built-in EFI Shell Boot0002* UEFI OS Boot0003* UEFI OS Boot0004* cray (sdb1) Boot0007* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot0009* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:89:62 Boot000B* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:89:63 Boot000D* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot0012* UEFI: PNY USB 3.1 FD PMAP Boot0013* UEFI: PNY USB 3.1 FD PMAP, Partition 2 Storage node (with onboards enabled):\nncn-s# efibootmgr BootNext: 0005 BootCurrent: 0006 Timeout: 2 seconds BootOrder: 0007,0009,0000,0002 Boot0000* cray (sda1) Boot0001* UEFI: Built-in EFI Shell Boot0002* cray (sdb1) Boot0005* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot0007* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:88:76 Boot0009* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:88:77 Boot000B* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Worker node (with onboards enabled):\nncn-w# efibootmgr BootNext: 0005 BootCurrent: 0008 Timeout: 2 seconds BootOrder: 0007,0009,000B,0000,0002 Boot0000* cray (sda1) Boot0001* UEFI: Built-in EFI Shell Boot0002* cray (sdb1) Boot0005* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot0007* UEFI: PXE IP4 Mellanox Network Adapter - 98:03:9B:AA:88:30 Boot0009* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:89:2A Boot000B* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:89:2B Boot000D* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Reverting Changes Reset the BIOS.\nRefer to vendor documentation for resetting the BIOS.\nOptionally attempt to with ipmitool:\n# reset ipmitool chassis bootdev none options=clear-cmos # set boot order ipmitool chassis bootdev pxe options=efiboot,persistent # boot to BIOS for checkout ipmitool chassis bootdev bios options=efiboot NOTE ipmitool works against a machine remotely over TCP/IP, it requires more arguments:\nusername=root IPMI_PASSWORD= ipmitool -I lanplus -U $username -E -H \u0026lt;bmc-hostname\u0026gt; Locating a USB Stick Some systems very obviously print out which device is the USB, other systems (like Gigabyte based) do not.\nParsing the output of efibootmgr can be helpful in determining which device is your USB stick. One can and should use tools such as lsblk, blkid, or kernel (/proc) as well if one knows how. As an example, one can sometimes match up ls -l /dev/disk/by-partuuid with efibootmgr -v.\n# Print off the UEFI\u0026#39;s boot selections: ncn-m# efibootmgr BootCurrent: 0015 Timeout: 1 seconds BootOrder: 000E,000D,0011,0012,0007,0005,0006,0008,0009,0000,0001,0002,000A,000B,000C,0003,0004,000F,0010,0013,0014 Boot0000* Enter Setup Boot0001 Boot Device List Boot0002 Network Boot Boot0003* Launch EFI Shell Boot0004* UEFI HTTPv6: Network 00 at Riser 02 Slot 01 Boot0005* UEFI HTTPv6: Intel Network 00 at Baseboard Boot0006* UEFI HTTPv4: Intel Network 00 at Baseboard Boot0007* UEFI IPv4: Intel Network 00 at Baseboard Boot0008* UEFI IPv6: Intel Network 00 at Baseboard Boot0009* UEFI HTTPv6: Intel Network 01 at Baseboard Boot000A* UEFI HTTPv4: Intel Network 01 at Baseboard Boot000B* UEFI IPv4: Intel Network 01 at Baseboard Boot000C* UEFI IPv6: Intel Network 01 at Baseboard Boot000D* UEFI HTTPv4: Network 00 at Riser 02 Slot 01 Boot000E* UEFI IPv4: Network 00 at Riser 02 Slot 01 Boot000F* UEFI IPv6: Network 00 at Riser 02 Slot 01 Boot0010* UEFI HTTPv6: Network 01 at Riser 02 Slot 01 Boot0011* UEFI HTTPv4: Network 01 at Riser 02 Slot 01 Boot0012* UEFI IPv4: Network 01 at Riser 02 Slot 01 Boot0013* UEFI IPv6: Network 01 at Riser 02 Slot 01 Boot0014* UEFI Samsung Flash Drive 1100 Boot0015* UEFI Samsung Flash Drive 1100 Boot0018* UEFI SAMSUNG MZ7LH480HAHQ-00005 S45PNA0M838871 Boot1001* Enter Setup In the example above, our device is 0014 or 0015. We will guess it is the first one, and can correct this on-the-fly in POST Notice the lack of \u0026ldquo;Boot\u0026rdquo; in the ID number given, we want Boot0014 so we pass \u0026lsquo;0014\u0026rsquo; to efibootmgr:\nncn-m# efibootmgr -n 0014 # Verify the BootNext device is what you selected: ncn-m# efibootmgr | grep -i bootnext BootNext: 0014 Now the UEFI Samsung Flash Drive will boot next.\nNote there are duplicates in the list. During boot, the boot-manager will select the first one. If you find that the first one is false, false entries can be deleted with efibootmgr -b 0014 -d.\n"
},
{
	"uri": "/docs-csm/en-09/103-ncn-networking/",
	"title": "Networking",
	"tags": [],
	"description": "",
	"content": "Networking Non-computes and computes have different network interfaces, this page will talk about non-computes but in the context of a metal stack.\nName Type MTU mgmt0 Slot 1 on the SMNET card. 9000 mgmt1 Slot 2 on the SMNET card, or slot 1 on the 2nd SMNET card. 9000 bond0 LACP Link Agg. of mgmt0 and mgmt1, or mgmt0 and mgmt2 on dual-bonds (when bond1 is present). 9000 bond1 LACP Link Agg. of mgmt1 and mgmt3. 9000 lan0 Externally facing interface. 1500 lan1 Yet-another externally facing interface, or anything (unused). 1500 hsn0 High-speed network interface. 9000 hsnN+1 Yet-another high-speed network interface. 9000 vlan002 Virtual LAN for managing nodes 1500 vlan004 Virtual LAN for managing hardware 1500 vlan007 Virtual LAN for the customer access network 1500 These interfaces can be observed on a live NCN (using ip link on the command line).\nDevice Naming / udev The underlying naming relies on [BIOSDEVNAME][1], this helps conform device naming into a smaller set of possible names. It also helps show us when driver issues occur, if a non-BIOSDEVNAME interface appears then METAL can/should receive a triage report/bug.\nMAC Based udev rules during initial boot in iPXE. When a node boots, iPXE will dump the PCI busses and sort network interfaces into 3 buckets:\nmgmt: internal/management network connection hsn: high-speed connection lan: external/site-connection The source code for the rule generation is in [metal-ipxe][1], but for technical information on the PCI configuration/reading please read on.\nVendor and Bus ID Identification The initial boot of an NCN sets interface udev rules since it has no discovery method yet.\nThe information needed is:\nPCI Vendor IDs for devices/cards to be used on the Management network. PCI Device IDs for the devices/cards to be used on the High-Speed Network. The 16-bit Vendor ID is allocated by the PCI-SIG (Peripheral Component Interconnect Special Interest Group).\nThe information belongs to the first 4 bytes of the PCI header, and admin can obtain it using lspci or your preferred method for reading the PCI bus.\nCollection Example lspci | grep -i ethernet lspci | grep c6:00.0 Popular Vendor ID and Device ID Table These are commonly found in Cray computers.\nThe Device and Vendor IDs are used in iPXE for bootstrapping the nodes, this allows generators to swap IDs out for certain systems until smarter logic can be added to cloud-init.\nVendor Model Device ID Vendor ID Intel Corporation Ethernet Connection X722 37d2 8086 Intel Corporation 82576 1526 8086 Mellanox Technologies ConnectX-4 1013 15b3 Mellanox Technologies ConnectX-5 1017 15b3 Giga-Byte Intel Corporation I350 1521 8086 QLogic Corporation FastLinQ QL41000 8070 1077 "
},
{
	"uri": "/docs-csm/en-09/104-ncn-partitioning/",
	"title": "NCN Partitions",
	"tags": [],
	"description": "",
	"content": "NCN Partitions Shasta non-compute nodes use drive storage for persistence and block storage. This page outlines reference information for these disks, their partition tables, and their management.\nNCN Partitions What Controls Partitioning? Plan of Record / Baseline Problems When Above/Below Baseline Worker Nodes with ETCD Disable Luks Expand the RAID Disk Layout Quick-Reference Tables OverlayFS and Persistence Persistent Directories OverlayFS Example Layering - Upperdir and Lowerdir(s) Layering Real World Example OverlayFS Control Reset Toggles Reset On Next Boot Reset on Every Boot Re-sizing the Persistent Overlay Thin Overlay Feature SystemD MetalFS Old/Retired FS-Labels What Controls Partitioning? Partitioning is controlled by two aspects:\ndracut; this selects disks and builds their partition tables and/or LVM storage. cloud-init; this manages standalone partitions or volumes, as well as high-level object storage. Plan of Record / Baseline Node Type No. of \u0026ldquo;small\u0026rdquo; disks (0.5 TiB) No. of \u0026ldquo;large\u0026rdquo; disks (1.9 TiB) k8s-master nodes 3 0 k8s-worker nodes 2 1 ceph-storage nodes 2 3+ Disks are chosen by dracut. Kubernetes and storage nodes use different dracut modules.\nFirst, two disks for the OS are chosen from the pool of \u0026ldquo;small\u0026rdquo; disks Second, one disk is selected for the ephemeral data Problems When Above/Below Baseline NCN masters and workers use the same artifacts, and thus have the same dracut modules assimilating disks. Therefore, it is important to beware of:\nk8s-master nodes with 1 or more extra \u0026ldquo;large\u0026rdquo; disk(s); these disks help but are unnecessary ceph-storage nodes do not run the same dracut modules since they have different disk demands Worker Nodes with ETCD k8s-worker nodes with 1 or more extra \u0026ldquo;small\u0026rdquo; disk(s); these disks are confusing and unnecessary and can be disabled easily.\nDisable Luks NOTE This is broken, use the expand RAID option instead. (MTL-1309)\nAll NCNs (master/worker/storage) have the same kernel parameters, but are not always necessary. This method works by toggling the dependency for the metal ETCD module, disabling LUKs will disable ETCD bare-metal creation.\nDisable LUKs for each worker node, thus disabling the metal ETCD module:\nDuring Bootstrap (on the pit node): sed -i \u0026#39;s/disk-opts rd.luks /disk-opts rd.luks=0 /g\u0026#39; /var/www/ncn-w*/script.ipxe During runtime with csi: csi handoff bss-update-param rd.luks=0 Rebuild the node\nRun the basic wipe if the node was already booted (re)boot the node Expand the RAID This option simply expands the RAID to consume the extra disks, leaving none behind for the metal ETCD module to find.\nSet metal.disks equal to the number of \u0026ldquo;small\u0026rdquo; disks in the node(s), this will reserve them for the RAID and prevent any other partitioning from happening on them.\nDuring Bootstrap (on the pit node): sed -i \u0026#39;s/disk-opts /disk-opts metal.disks=3 /g\u0026#39; /var/www/ncn-w*/script.ipxe During runtime with csi: csi handoff bss-update-param metal.disks=3 Change the RAID type, or leave it as default (mirror)\nDuring Bootstrap (on the pit node): sed -i \u0026#39;s/disk-opts /disk-opts metal.md-level=stripe /g\u0026#39; /var/www/ncn-w*/script.ipxe During runtime with csi: csi handoff bss-update-param metal.md-level=stripe Rebuild the node\nRun the basic wipe if the node was already booted (re)boot the node Disk Layout Quick-Reference Tables The table below represents all recognizable FS labels on any given NCN, varying slightly by node-role (i.e. kubernetes-manager vs. kubernetes-worker).\nk8s-manager k8s-worker storage-ceph FS Label Partitions Device Partition Size OverlayFS Work Order(s) Memo ✅ ✅ ✅ BOOTRAID Not Mounted 2 small disks in RAID1 500 MiB ❌ Present since Shasta-Preview 1 ✅ ✅ ✅ SQFSRAID /run/initramfs/live 2 small disks in RAID1 100 GiB ✅ CASM-1885 squashfs should compress our images to about 1/3rd their uncompressed size. (20G → 6.6G) On pepsi\u0026rsquo;s ncn-w001, we are at about 20G of non-volatile data storage needed. ✅ ✅ ✅ ROOTRAID /run/initramfs/overlayfs 2 small disks in RAID1 Max/Remainder ✅ Present since Shasta-Preview 1 The persistent image file is loaded from this partition, when the image file is loaded the underlying drive is lazily unmounted (umount -l) so that when the overlay closes the disk follows suit. ❌ ✅ ❌ CONRUN /run/containerd Ephemeral 75 GiB ❌ MTL-916 On pepsi ncn-w001, we have less than 200G of operational storage for this. ❌ ✅ ❌ CONLIB /run/lib-containerd Ephemeral 25% ✅ MTL-892 CASMINST-255 ✅ ❌ ❌ ETCDK8S /run/lib-etcd Ephemeral 32 GiB ✅ CASMPET-338 ✅ ❌ ❌ K8SLET /var/lib/kubelet Ephemeral 25% ❌ MTL-892 CASMINST-255 The above table\u0026rsquo;s rows with overlayFS map their \u0026ldquo;Mount Paths\u0026rdquo; to the \u0026ldquo;Upper Directory\u0026rdquo; in the table below:\nThe \u0026ldquo;OverlayFS Name\u0026rdquo; is the name used in fstab and seen in the output of mount.\nOverlayFS Name Upper Directory Lower Directory (or more) etcd_overlayfs /run/lib-etcd /var/lib/etcd containerd_overlayfs /run/lib-containerd /var/lib/containerd For notes on previous/old labels, scroll to the bottom.\nOverlayFS and Persistence There are a few overlays used for NCN image boots. These enable two critical functions; changes to data and new data will persist between reboots, and RAM (memory) is freed because we are using our block-devices (SATA/PCIe).\nROOTRAID is the persistent root overlayFS, it commits and saves all changes made to the running OS and it stands on a RAID1 mirror. CONLIB is a persistent overlayFS for containerd, it commits and saves all new changes while allowing read-through to pre-existing (baked-in) data from the squashFS. ETCDK8S is a persistent overlayFS for etcd, it works like the CONLIB overlayFS however this exists in an encrypted LUKS2 partition. OverlayFS Example Helpful commands\u0026hellip; the overlayFS organization can be best viewed with these three commands:\nlsblk, lsblk -f will show how the RAIDs and disks are mounted losetup -a will show where the squashFS is mounted from mount | grep ' / ' will show you the overlay being layered atop the squashFS Let us pick apart the SQFSRAID and ROOTRAID overlays.\n/run/rootfsbase is the SquashFS image itself /run/initramfs/live is the squashFS\u0026rsquo;s storage array, where one or more squashFS can live /run/initramfs/overlayfs is the overlayFS storage array, where the persistent directories live /run/overlayfs and /run/ovlwork are symlinks to `/run/initramfs/overlayfs/overlayfs-SQFSRAID-$(blkid -s UUID -o value /dev/disk/by-label/SQFSRAID) and the neighboring work directory Admin note: The \u0026ldquo;work\u0026rdquo; directory is where the operating system processes data. It is the interim where data passes between RAM and persistent storage. Using the above bullets, one may be able to better understand the machine output below:\nncn-m002# mount | grep \u0026#39; / \u0026#39; LiveOS_rootfs on / type overlay (rw,relatime,lowerdir=/run/rootfsbase,upperdir=/run/overlayfs,workdir=/run/ovlwork) ^^^R/O^SQUASHFS IMAGE^^^|^^^ R/W PERSISTENCE ^^^|^^^^^^INTERIM^^^^^^ ^^^R/O^SQUASHFS IMAGE^^^|^^^ R/W PERSISTENCE ^^^|^^^^^^INTERIM^^^^^^ ^^^R/O^SQUASHFS IMAGE^^^|^^^ R/W PERSISTENCE ^^^|^^^^^^INTERIM^^^^^^ ncn-m002# losetup -a /dev/loop1: [0025]:74858 (/run/initramfs/thin-overlay/meta) /dev/loop2: [0025]:74859 (/run/initramfs/thin-overlay/data) /dev/loop0: [2430]:100 (/run/initramfs/live/LiveOS/filesystem.squashfs) The THIN OVERLAY is the transient space the system uses behind the scenes to allow data to live in RAM as it is written to disk. The THIN part of the overlay is the magic, using THIN overlays means the kernel will automatically clear free blocks.\nBelow is the layout of what a persistent system looks like. Note, this means that persistent capacity is there, but admins should beware of reset toggles on unfamiliar systems. There are toggles to reset overlays that are, by default, toggled off (so data persistence be default is safe but one should not assume).\nncn-m002# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 3.8G 1 loop /run/rootfsbase loop1 7:1 0 30G 0 loop └─live-overlay-pool 254:2 0 300G 0 dm loop2 7:2 0 300G 0 loop └─live-overlay-pool 254:2 0 300G 0 dm sda 8:0 1 447.1G 0 disk ├─sda1 8:1 1 476M 0 part │ └─md127 9:127 0 476M 0 raid1 ├─sda2 8:2 1 92.7G 0 part │ └─md126 9:126 0 92.6G 0 raid1 /run/initramfs/live └─sda3 8:3 1 279.4G 0 part └─md125 9:125 0 279.3G 0 raid1 /run/initramfs/overlayfs sdb 8:16 1 447.1G 0 disk ├─sdb1 8:17 1 476M 0 part │ └─md127 9:127 0 476M 0 raid1 ├─sdb2 8:18 1 92.7G 0 part │ └─md126 9:126 0 92.6G 0 raid1 /run/initramfs/live └─sdb3 8:19 1 279.4G 0 part └─md125 9:125 0 279.3G 0 raid1 /run/initramfs/overlayfs sdc 8:32 1 447.1G 0 disk └─ETCDLVM 254:0 0 447.1G 0 crypt └─etcdvg0-ETCDK8S 254:1 0 32G 0 lvm /run/lib-etcd Persistent Directories Not all directories are persistent!\nOnly the following directories are persistent by default:\netc home root srv tmp var /run/containerd /run/lib-containerd /run/lib-etcd /run/lib/kubelet More directories can be added, but mileage varies. The initial set is actually managed by dracut, when using a reset toggle the above list is \u0026ldquo;reset/cleared\u0026rdquo;. If more directories are added, they will be eradicated when enabling a reset toggle.\nThese are all provided through the Overlay from /run/overlayfs:\nncn-m001:/run/overlayfs # ls -l total 0 drwxr-xr-x 8 root root 290 Oct 15 22:41 etc drwxr-xr-x 3 root root 18 Oct 15 22:41 home drwx------ 3 root root 39 Oct 13 16:53 root drwxr-xr-x 3 root root 18 Oct 5 19:16 srv drwxrwxrwt 2 root root 85 Oct 16 14:50 tmp drwxr-xr-x 8 root root 76 Oct 13 16:52 var Remember: /run/overlayfs is a symbolic link to the real disk /run/initramfs/overlayfs/*.\nLayering - Upperdir and Lowerdir(s) The file-system the user is working on is really two layered file-systems (overlays).\nThe lower layer is the SquashFS image itself, read-only, which provides all that we need to run. The upper layer is the OverlayFS, read-write, which does a bit-wise xor with the lower-layer Anything in the upper-layer takes precedence by default. There are fancier options for overlays, such as multiple lower-layers, copy-up (lower-layer precedence), and opaque (removing a directory in the upper layer hides it in the lower layer). You can read more [here|https://www.kernel.org/doc/html/latest/filesystems/overlayfs.html#inode-properties]\nLayering Real World Example Let us take /root for example, we can see in the upper-dir (the overlay) we have these files:\nThe upper-dir has these files:\nncn-m001# ls -l /run/overlayfs/root/ total 4 -rw------- 1 root root 252 Nov 4 18:23 .bash_history drwxr-x--- 4 root root 37 Nov 4 04:35 .kube drwx------ 2 root root 29 Oct 21 21:57 .ssh Then in the squashFS image (lower-dir) we have these\u0026hellip;\nncn-m001# ls -l /run/rootfsbase/root/ total 1 -rw------- 1 root root 0 Oct 19 15:31 .bash_history drwxr-xr-x 2 root root 3 May 25 2018 bin drwx------ 3 root root 26 Oct 21 22:07 .cache drwx------ 2 root root 3 May 25 2018 .gnupg drwxr-xr-x 4 root root 57 Oct 19 15:23 inst-sys drwxr-xr-x 2 root root 33 Oct 19 15:33 .kbd drwxr-xr-x 5 root root 53 Oct 19 15:34 spire drwx------ 2 root root 70 Oct 21 21:57 .ssh -rw-r--r-- 1 root root 172 Oct 26 15:25 .wget-hsts Notice how the .bash_history file in the lower-dir is 0 bytes, but it is 252 bytes in the upperdir? Notice the .kube dir exists in the upper, but not the lower? Finally, looking at /root we see the magic:\nncn-m001# ls -l /root total 5 -rw------- 1 root root 252 Nov 4 18:23 .bash_history drwxr-xr-x 2 root root 3 May 25 2018 bin drwx------ 3 root root 26 Oct 21 22:07 .cache drwx------ 2 root root 3 May 25 2018 .gnupg drwxr-xr-x 4 root root 57 Oct 19 15:23 inst-sys drwxr-xr-x 2 root root 33 Oct 19 15:33 .kbd drwxr-x--- 4 root root 37 Nov 4 04:35 .kube drwxr-xr-x 5 root root 53 Oct 19 15:34 spire drwx------ 1 root root 29 Oct 21 21:57 .ssh -rw-r--r-- 1 root root 172 Oct 26 15:25 .wget-hsts Notice how .bash_history matches the upper-dir? Notice how .kube exists here? The take-away here is: any change done to /root/ will persist through /run/overlayfs/root and will take precedence to the squashFS image root.\nOverlayFS Control These features or toggles are passable on the kernel command line, and change the behavior of the overlayFS.\nReset Toggles The overlay FS provides a few reset toggles to clear out the persistence directories without reinstall.\nThe toggles require rebooting.\nReset On Next Boot The preferred way to reset persistent storage is to use the overlayFS reset toggle.\nModify the boot command line on the PXE server, adding this\n# Reset the overlay on boot rd.live.overlay.reset=1 Once reset, you may want to enable persistence again. Simply revert your change and the next reboot will persist.\n# Cease resetting the overlayFS rd.live.overlay.reset=0 Reset on Every Boot There are two options one can leave enabled to accomplish this:\nrd.live.overlay.reset=1 will eradicate/recreate the overlay every reboot. rd.live.overlayr.readonly=1 will clear the overlay on every reboot. For long-term usage, rd.live.overlay.readonly=1 should be added to the command line.\nThe reset=1 toggle is usually used to fix a side-ways overlay, if you want to completely refresh and purge the overlay then rd.live.overlay.reset is your friend.\n# Authorize METAL to purge metal.no-wipe=0 rd.live.overlay.reset=1 Note: metal.no-wipe=1 does not protect against rd.live.overlay.reset, metal.no-wipe is not a feature of dmsquash-live.\nRe-sizing the Persistent Overlay Default Size: 300 GiB File System: XFS The overlay can be resized to fit a variety of needs or use cases. The size is provided directly on the command line. Any value can be provided, but it must be in megabytes.\nIf you are resetting the overlay on a deployed node, you will need to also set rd.live.overlay.reset=1.\nIt is recommended to set the size before deployment. There is a linkage between the metal-dracut module and the live-module that makes this inflexible.\n# Use a 300 GiB overlayFS (default) rd.live.overlay.size=307200 # Use a 1 TiB overlayFS rd.live.overlay.size=1000000 Thin Overlay Feature The persistent overlayFS leverages newer, \u0026ldquo;thin\u0026rdquo; overlays that support discards and that will free blocks that are not claimed by the file system. This means that memory is free/released when the filesystem does not claim it anymore.\nThin overlays can be disabled, and instead classic DM Snapshots can be used to manage the overlay. This will use more RAM. It is not recommended, since dmraid is not included in the initrd.\n# Enable (default) rd.live.overlay.thin=1 # Disable (not recommended; undesirable RAM waste) rd.live.overlay.thin=0 SystemD MetalFS The metalfs systemd service will try to mount any metal created partitions.\nThis runs against the /run/initramfs/overlayfs/fstab.metal when it exists. This file is dynamically created by most metal dracut modules.\nThe service will continuously attempt to mount the partitions, if problems arise please stop the service:\nncn# systemctl stop metalfs Old/Retired FS-Labels Deprecated FS labels/partitions from Shasta 1.3.X (no longer in Shasta 1.4.0 and onwards).\nFS Label Partitions Nodes Device Size on Disk Work Order Memo K8SKUBE /var/lib/kubelet ncn-w001, ncn-w002 Ephemeral Max/Remainder CASMPET-338 CASMPET-342 No longer mounted/used in shasta-1.4 K8SEPH /var/lib/cray/k8s_ephemeral ncn-w001, ncn-w002 Ephemeral Max/Remainder CASMPET-338 CASMPET-342 No longer mounted/used in shasta-1.4 CRAYINSTALL /var/cray/vfat ncn-w001, ncn-w002 Ephemeral 12 GiB CASMPET-338 CASMPET-342 No longer mounted/used in shasta-1.4 CRAYVBIS /var/cray/vbis ncn-w001, ncn-w002 Ephemeral 900 GiB CASMPET-338 CASMPET-342 No longer mounted/used in shasta-1.4 CRAYNFS /var/lib/nfsroot/nmd ncn-w001, ncn-w002 Ephemeral 12 GiB CASMPET-338 CASMPET-342 No longer mounted/used in shasta-1.4 "
},
{
	"uri": "/docs-csm/en-09/105-ncn-packages/",
	"title": "NCN Packages",
	"tags": [],
	"description": "",
	"content": "NCN Packages These are defined elsewhere, but this list updates for quick-reference. These lists are generated on running nodes, but to account for drift there are collection commands listed for each image.\nKubernetes Images Collection ncn-w002# zypper --disable-repositories se --installed-only | grep i+ | tr -d \u0026#39;|\u0026#39; | awk \u0026#39;{print $2}\u0026#39; The List SLE_HPC SLE_HPC-release acpid apparmor-profiles arptables at autoyast2 autoyast2-installation base bc bind-utils biosdevname ceph-common cfs-state-reporter cloud-init conntrack-tools cpupower crash cray-cos-release cray-cps-utils cray-dvs-kmp-default cray-dvs-service cray-heartbeat cray-lustre-client cray-lustre-client-devel cray-lustre-client-kmp-default cray-network-config cray-node-identity cray-orca cray-power-button cray-sat-podman craycli-wrapper createrepo_c dnsmasq dosfstools dracut-kiwi-live dracut-metal-mdsquash dump e2fsprogs ebtables emacs ethtool expect fping gdb genders git-core glibc gnuplot gperftools gptfdisk grub2 haproxy hpe-csm-scripts hplip ipmitool iproute2-bash-completion ipset ipvsadm irqbalance java-1_8_0-ibm java-1_8_0-ibm-plugin kdump kdumpid keepalived kernel-mft-mlnx-kmp-default kernel-source kernel-syms kexec-tools kmod-bash-completion kubeadm kubectl kubelet less libcanberra-gtk0 libecpg6 libpq5 libunwind-devel ltrace lvm2 man mariadb mariadb-tools mdadm minicom mlocate nmap numactl open-lldp patterns-base-base pdsh perf perl-MailTools perl-doc pixz podman podman-cni-config postgresql postgresql-contrib postgresql-docs postgresql-server python python-urlgrabber python3-Jinja2 python3-MarkupSafe python3-click python3-colorama python3-curses python3-ipaddr python3-lxml python3-netaddr python3-pexpect python3-pip python3-rpm python3-sip rarpd rasdaemon rsync rsyslog rzsz screen slingshot-network-config smartmontools socat spire-agent squashfs squid strace sudo sysstat systemtap tar tcpdump unixODBC usbutils vim wget wireshark xfsdump yast2-add-on yast2-bootloader yast2-country yast2-network yast2-services-manager yast2-users yum-metadata-parser zip CEPH Collection ncn-s002# zypper --disable-repositories se --installed-only | grep i+ | tr -d \u0026#39;|\u0026#39; | awk \u0026#39;{print $2}\u0026#39; The List SLE_HPC SLE_HPC-release acpid apparmor-profiles arptables at autoyast2 autoyast2-installation base bc bind-utils biosdevname ceph-mds ceph-mgr ceph-mon ceph-osd ceph-radosgw cfs-state-reporter cloud-init cpupower crash cray-cos-release cray-dvs-kmp-default cray-dvs-service cray-heartbeat cray-lustre-client cray-lustre-client-devel cray-lustre-client-kmp-default cray-network-config cray-node-identity cray-power-button craycli-wrapper createrepo_c dnsmasq dosfstools dracut-kiwi-live dracut-metal-mdsquash dump e2fsprogs ebtables emacs ethtool expect fping gdb genders git-core glibc gnuplot gperftools gptfdisk grub2 haproxy hpe-csm-scripts hplip ipmitool iproute2-bash-completion ipset irqbalance java-1_8_0-ibm java-1_8_0-ibm-plugin kdump kdumpid keepalived kernel-mft-mlnx-kmp-default kernel-source kernel-syms kexec-tools kmod-bash-completion kubectl less libcanberra-gtk0 libecpg6 libpq5 libunwind-devel ltrace lvm2 man mariadb mariadb-tools mdadm minicom mlocate netcat-openbsd nmap numactl open-lldp patterns-base-base pdsh perf perl-MailTools perl-doc pixz podman podman-cni-config postgresql postgresql-contrib postgresql-docs postgresql-server python python-urlgrabber python3-Jinja2 python3-MarkupSafe python3-boto3 python3-click python3-colorama python3-curses python3-ipaddr python3-lxml python3-netaddr python3-pexpect python3-pip python3-rpm python3-sip rarpd rasdaemon rsync rsyslog rzsz screen slingshot-network-config smartmontools socat spire-agent squashfs squid strace sudo sysstat systemtap tar tcpdump unixODBC usbutils vim wget wireshark xfsdump yast2-add-on yast2-bootloader yast2-country yast2-network yast2-services-manager yast2-users yum-metadata-parser zip "
},
{
	"uri": "/docs-csm/en-09/106-ncn-sles/",
	"title": "NCN Operating System Release",
	"tags": [],
	"description": "",
	"content": "NCN Operating System Release The NCNs define their products per image layer:\nVSHASTA All images are SLES (SuSE Linux Enterprise Server) METAL Metal SquashFS are always SLE_HPC (SuSE High Performance Computing) Metal CEPH Storage Images are always SLE_HPC (SuSE High Performance Computing) with SES (SuSE Enterprise Storage) Release RPM Details The sles-release RPM is uninstalled for Metal, instead the sle_HPC-release RPM is installed. These both provide the same files, but differ for os-release and /etc/product.d/baseproduct.\nThe ses-release RPM is installed atop the sle_HPC-release RPM in our CEPH images.\nExample - On a Live System Below we can see the two product files for a CEPH node, in here we see we have a metal node that is capable of high performance computing and serving enterprise storage.\nncn-s# ls -l /etc/products.d/ total 5 lrwxrwxrwx 1 root root 12 Jan 1 06:43 baseproduct -\u0026gt; SLE_HPC.prod -rw-r--r-- 1 root root 1587 Oct 21 15:27 ses.prod -rw-r--r-- 1 root root 2956 Jun 10 2020 SLE_HPC.prod ncn-s# grep \u0026#39;\u0026lt;summary\u0026#39; /etc/products.d/*.prod /etc/products.d/ses.prod: \u0026lt;summary\u0026gt;SUSE Enterprise Storage 7\u0026lt;/summary\u0026gt; /etc/products.d/SLE_HPC.prod: \u0026lt;summary\u0026gt;SUSE Linux Enterprise High Performance Computing 15 SP2\u0026lt;/summary\u0026gt; Kubernetes nodes on the other hand will report SLES HPC only, this is reflective in kubectl output:\nRemember, vshasta will show SUSE Linux Enterprise Server instead.\nncn# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ncn-m002 Ready master 128m v1.18.6 10.252.1.14 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.37-default containerd://1.3.4 ncn-m003 Ready master 127m v1.18.6 10.252.1.13 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.37-default containerd://1.3.4 ncn-w001 Ready \u0026lt;none\u0026gt; 90m v1.18.6 10.252.1.12 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.37-default containerd://1.3.4 ncn-w002 Ready \u0026lt;none\u0026gt; 88m v1.18.6 10.252.1.11 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.37-default containerd://1.3.4 ncn-w003 Ready \u0026lt;none\u0026gt; 82m v1.18.6 10.252.1.10 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.37-default containerd://1.3.4 "
},
{
	"uri": "/docs-csm/en-09/107-ncn-devel/",
	"title": "NCN Development",
	"tags": [],
	"description": "",
	"content": "NCN Development This page will help you if you are trying to test new images on a metal system. Here you can find a basic flow for iterative boots.\nWe assume you are internally developing; these scripts are for internal use only.\nGet your Image ID\n-k for Kubernetes, -s for storage/ceph\npit# /root/bin/get-sqfs.sh -k 9683117-1609280754169 pit# /root/bin/get-sqfs.sh -s c46624e-1609524120402 Set your Image IDs\nThis finds the newest pair, so it will find the last downloaded set (i.e. your set of images).\npit# /root/bin/set-sqfs-links.sh (Re)boot the node(s) you want to test.\nOne can easily follow along using conman. Run conman -q to see available consoles.\n"
},
{
	"uri": "/docs-csm/en-09/108-ncn-ntp/",
	"title": "NTP On NCNs",
	"tags": [],
	"description": "",
	"content": "NTP On NCNs Internal use only See this epic for details: MTL-1182.\nThe NCNs serve NTP at stratum 3 and all NCNs peer with each other. Currently, the LiveCD does is not running NTP, but the other nodes are when they are booted.\nNTP is currently allowed on the NMN and HMN networks.\nThe NTP peers are set in data.json, which is normally created during an initial install. It is possible to edit this file at a later point, restart basecamp, and then reboot the nodes to apply the change.\nUpstream The upstream NTP server is also set in data.json. If left blank, the NCNs will simply peer with themselves. Until an upstream NTP server is configured, the time on the NCNs may not match the current time at the site, but they will stay in sync with each other.\nChanging the config If you need to adjust the config, you have three options:\nedit data.json, restart basecamp (systemctl restart basecamp), run the ntp script on each node (/srv/cray/scripts/metal/set-ntp-config.sh) edit data.json, restart basecamp, restart nodes so cloud-init runs on boot manually edit /etc/chrony.d/cray.conf and restart chrony (systemctl restart chronyd) on each node The first two options are not fully fleshed out yet as we have not done much testing around changing things. Cloud-init does cache data, so there could be inconsistent results until further testing is done.\nTroubleshooting chronyc can be used to gather information on the state of NTP.\nchronyc accheck HOST - checks if a host is allowed to use NTP from HOST\nExample:\nncn# chronyc accheck 10.252.0.7 208 Access allowed chronyc tracking displays system clock performance\nExample:\nncn# chronyc tracking Reference ID : 0AFC0104 (ncn-s003) Stratum : 4 Ref time (UTC) : Mon Nov 30 20:02:24 2020 System time : 0.000007622 seconds slow of NTP time Last offset : -0.000014609 seconds RMS offset : 0.000015776 seconds Frequency : 6.773 ppm fast Residual freq : -0.000 ppm Skew : 0.008 ppm Root delay : 0.000075896 seconds Root dispersion : 0.000484318 seconds Update interval : 513.7 seconds Leap status : Normal chronyc sourcestats show information on drift and offset\nExample:\nncn# chronyc sourcestats 210 Number of sources = 8 Name/IP Address NP NR Span Frequency Freq Skew Offset Std Dev ============================================================================== ncn-w001 6 3 42m -0.029 0.126 +4104ns 28us ncn-w002 6 6 42m -0.028 0.030 +44us 7278ns ncn-w003 12 7 23m -0.059 0.023 -35us 8359ns ncn-s002 36 17 213m -0.001 0.010 +5794ns 54us ncn-s003 36 17 212m -0.000 0.007 -178ns 40us ncn-m001 0 0 0 +0.000 2000.000 +0ns 4000ms ncn-m002 28 15 192m -0.007 0.009 +9942ns 49us ncn-m003 24 15 197m -0.005 0.009 +9442ns 46us chronyc sources shows the NTP servers, pools, peers\nExample:\nncn# chronyc sources 210 Number of sources = 8 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== =? ncn-w001 4 9 377 435 +162us[ +164us] +/- 679us =? ncn-w002 4 9 377 505 +118us[ +120us] +/- 277us =? ncn-w003 4 7 377 82 +850ns[+2686ns] +/- 504us =? ncn-s002 4 9 377 542 -38us[ -36us] +/- 892us =* ncn-s003 3 9 377 19 +13us[ +15us] +/- 110us =? ncn-m001 0 9 0 - +0ns[ +0ns] +/- 0ns =? ncn-m002 4 8 377 161 -47us[ -45us] +/- 408us =? ncn-m003 4 8 377 215 -11us[-9109ns] +/- 446us Log files Logs exist at /var/log/chrony/, which can be used for further troubleshooting.\nForcing a time sync You can step the clocks and force a sync of NTP. If Kubernetes is already up or other services, they do not always react well if there is a large jump, so ideally, you would do this as the node is booting (our images do this automatically now):\nchronyc burst 4/4 # wait about 15 seconds while NTP measurements are gathered # jump the clock manually chronyc makestep Customizing NTP Setting A Local Timezone This procedure needs to be completed before the NCNs are deployed\nConfigure NTP on PIT to your local timezone Shasta ships with UTC as the default time zone. To change this, you will need to set an environment variable, as well as chroot into the node images and change some files there. You can find a list of timezones to use in the commands below by running timedatectl list-timezones.\nRun the following commands, replacing them with your timezone as needed.\npit# export NEWTZ=America/Chicago pit# echo -e \u0026#34;\\nTZ=${NEWTZ}\u0026#34; \u0026gt;\u0026gt; /etc/environment pit# sed -i \u0026#34;s#^timedatectl set-timezone UTC#timedatectl set-timezone ${NEWTZ}#\u0026#34; /root/bin/configure-ntp.sh pit# sed -i \u0026#39;s/--utc/--localtime/\u0026#39; /root/bin/configure-ntp.sh pit# /root/bin/configure-ntp.sh You should see the output in your local timezone.\npit# /root/bin/configure-ntp.sh CURRENT TIME SETTINGS rtc: 2021-03-26 11:34:45.873331+00:00 sys: 2021-03-26 11:34:46.015647+0000 200 OK 200 OK NEW TIME SETTINGS rtc: 2021-03-26 06:35:16.576477-05:00 sys: 2021-03-26 06:35:17.004587-0500 You can verify as well by running timedatectl and hwclock --verbose.\npit# timedatectl Local time: Fri 2021-03-26 06:35:58 CDT Universal time: Fri 2021-03-26 11:35:58 UTC RTC time: Fri 2021-03-26 11:35:58 Time zone: America/Chicago (CDT, -0500) Network time on: no NTP synchronized: no RTC in local TZ: no pit# hwclock --verbose hwclock from util-linux 2.33.1 System Time: 1616758841.688220 Trying to open: /dev/rtc0 Using the rtc interface to the clock. Last drift adjustment done at 1616758836 seconds after 1969 Last calibration done at 1616758836 seconds after 1969 Hardware clock is on local time Assuming hardware clock is kept in local time. Waiting for clock tick... ...got clock tick Time read from Hardware Clock: 2021/03/26 06:40:42 Hw clock time : 2021/03/26 06:40:42 = 1616758842 seconds since 1969 Time since last adjustment is 6 seconds Calculated Hardware Clock drift is 0.000000 seconds 2021-03-26 06:40:41.685618-05:00 If the time is off and not accurate to your timezone, you will need to manually set the date and then run the NTP script again.\n# Set as close as possible to the real time pit# timedatectl set-time \u0026#34;2021-03-26 00:00:00\u0026#34; pit# /root/bin/configure-ntp.sh The PIT is now configured to your local timezone.\nConfigure NCN Images To Use Your Local Timezone You need to adjust the node images so that they also boot in the local timezone. This is accomplished by chrooting into the unsquashed images, making some modifications, and then squashing it back up and moving the new images into place.\nSet some variables pit# export NEWTZ=America/Chicago pit# export IMGTYPE=ceph pit# export IMGDIR=/var/www/ephemeral/data/${IMGTYPE} Go to the image directory and unsquash the image pit# cd ${IMGDIR} pit# unsquashfs *.squashfs Start a chroot session inside the unsquashed image. Your prompt may change to reflect that you are now in the root directory of the image. pit# chroot ./squashfs-root Inside the chroot session, you will modify a few files by running the following commands, and then exit from the chroot session. pit-chroot# echo TZ=${NEWTZ} \u0026gt;\u0026gt; /etc/environment pit-chroot# sed -i \u0026#34;s#^timedatectl set-timezone UTC#timedatectl set-timezone $NEWTZ#\u0026#34; /srv/cray/scripts/metal/set-ntp-config.sh pit-chroot# sed -i \u0026#39;s/--utc/--localtime/\u0026#39; /srv/cray/scripts/metal/set-ntp-config.sh pit-chroot# /srv/cray/scripts/common/create-kis-artifacts.sh pit-chroot# exit pit# Back outside the chroot session, you will now backup the original images and copy the new ones into place. pit# mkdir ${IMGDIR}/orig pit# mv *.kernel *.xz *.squashfs ${IMGDIR}/orig/ pit# cp squashfs-root/squashfs/* . pit# chmod 644 ${IMGDIR}/initrd.img.xz Unmount the squashfs mount (which was mounted by the earlier unsquashfs command) pit# umount ${IMGDIR}/squashfs-root/mnt/squashfs Repeat all of the previous steps, except in the first step, set the IMGTYPE variable as follows: pit# export IMGTYPE=k8s Be sure to also set the IMGDIR variable again, so it gets the new value of IMGTYPE Now link the new images so that the NCNs will get them from the LiveCD node when they boot pit# set-sqfs-links.sh Make a note that when performing the csi handoff of NCN boot artifacts in 007-CSM-INSTALL-REBOOT.md, you must be sure to specify these new images. Otherwise m001 will use the default timezone when it boots, and subsequent reboots of the other NCNs will also lose the customized timezone changes. "
},
{
	"uri": "/docs-csm/en-09/110-ncn-image-customization/",
	"title": "NCN Image Customization",
	"tags": [],
	"description": "",
	"content": "NCN Image Customization The LiveCD is equipped for \u0026ldquo;re-squashing\u0026rdquo; an SquashFS images.\nBoot Customization Set the Default Password Image Layer Pipeline Boot Customization Set the Default Password Customize the NCN images by changing the root password or adding different SSH keys for the root account.\nThis process should be done for the \u0026ldquo;Kubernetes\u0026rdquo; image used by master and worker nodes and then repeated for the Ceph image used by the utility storage nodes.\nOpen the image.\nThe Kubernetes image will be of the form \u0026ldquo;kubernetes-0.0.53.squashfs\u0026rdquo; in /var/www/ephemeral/data/k8s.\npit# cd /var/www/ephemeral/data/k8s pit# unsquashfs kubernetes-0.0.53.squashfs The Ceph image will be of the form \u0026ldquo;ceph-0.0.44.squashfs\u0026rdquo; in /var/www/ephemeral/data/ceph.\npit# cd /var/www/ephemeral/data/ceph pit# unsquashfs ceph-0.0.44.squashfs Change into the image root\npit# chroot ./squashfs-root Change the password\nchroot-pit# passwd Replace the SSH keys\nchroot-pit# cd root Replace the default root public and private SSH keys with your own or generate a new pair with ssh-keygen(1)\nCreate the new SquashFS artifact\nchroot-pit# /srv/cray/scripts/common/create-kis-artifacts.sh Exit the chroot\nchroot-pit# exit Cleanup the SquashFS creation\nThe Kubernetes image directory is /var/www/ephemeral/data/k8s.\npit# umount /var/www/ephemeral/data/k8s/squashfs-root/mnt/squashfs The Ceph image directory is /var/www/ephemeral/data/ceph.\npit# umount /var/www/ephemeral/data/ceph/squashfs-root/mnt/squashfs Save old SquashFS image.\npit# mkdir old pit# mv *squashfs old Move new SquashFS image, kernel, and initrd into place.\npit# mv squashfs-root/squashfs/* . Update file permissions on initrd\npit# chmod 644 initrd.img.xz 11. Repeat the preceding steps for the other image type. 12. Set the boot links. ```bash pit# cd pit# set-sqfs-links.sh The images will have the new password for the next boot.\nImage Layer Pipeline "
},
{
	"uri": "/docs-csm/en-09/200-ncn-bios-pref/",
	"title": "NCN BIOS Preferences",
	"tags": [],
	"description": "",
	"content": "NCN BIOS Preferences This page goes over desired NCN BIOS settings specifications.\nFor setting each one, please refer to the vendor manuals for the systems inventory.\nSpec. NOTE The table below declares desired settings; unlisted settings should remain at vendor-default. This table may be expanded as new settings are adjusted.\nCommon Name Common Value Memo Menu Location Intel® Hyper-Threading (e.g. HT) Enabled Enables two-threads per physical core. Within the Processor or the PCH Menu. Intel® Virtualization Technology (e.g. VT-x, VT) and AMD Virtualization Technology (e.g. AMD-V) Enabled Enables Virtual Machine extensions. Within the Processor or the PCH Menu. PXE Retry Count 1 or 2 (default: 1) Attempts done on a single boot-menu option (note: 2 should be set for systems with unsolved network congestion). Within the Networking Menu, and then under Network Boot. NOTE PCIe options can be found in PCIe : Setting Expected Values.\n"
},
{
	"uri": "/docs-csm/en-09/250-firmware/",
	"title": "Firmware Checkout",
	"tags": [],
	"description": "",
	"content": "Firmware Checkout This page will guide an administrator on 3 things:\nChecking Firmware Status Applying Firmware over the GUI (optionally) Applying firmware via Redfish Information below is sorted based on device type; complete each when directed to by the prerequisite page. On the other hand, if an administrator is using this guide ad-hoc then they must complete each of the listed guides in order.\n(required) Management Network Firmware Guide (required) NCN Firmware Guide for Bootstrap Guides for Runtime The following guide(s) can be done when the CRAY is operational (in runtime).\nThese are not required for an installation.\nNCN Firmware Installation Guide for FAS NCN Firmware Action Service (FAS) Guide NCN Firmware Action Service FAS Recipes WARNING: Non-compute nodes (NCNs) should be locked with the HSM locking API to ensure they are not unintentionally updated by FAS. Research 009-NCN-LOCKING for more information. Failure to lock the NCNs could result in an unintentional update of the NCNs if FAS is not used correctly; this will lead to system instability problems.\n"
},
{
	"uri": "/docs-csm/en-09/251-firmware-network/",
	"title": "Network Firmware",
	"tags": [],
	"description": "",
	"content": "Network Firmware This firmware is updated manually, and needs to be checked prior to an install.\nIMPORTANT NOTE systems with multi-chassis link-aggregations must meet this pages minimum specifications for optimal deployment.\nManagement Network Switches AIRGAP NOTE: This firmware is available off the LiveCD (http://pit/fw/network/), see more endpoints here.\nVendor Model Version Aruba 6300 ArubaOS-CX_6400-6300_10.06.0010 Aruba 8320 ArubaOS-CX_8320_10.06.0010 Aruba 8325 ArubaOS-CX_8325_10.06.0010 Aruba 8360 ArubaOS-CX_8360.06.0010 Dell S3048-ON 10.5.1.4 Dell S4148F-ON 10.5.1.4 Dell S4148T-ON 10.5.1.4 Mellanox MSN2100 3.9.1014 Mellanox MSN2700 3.9.1014 "
},
{
	"uri": "/docs-csm/en-09/252-firmware-ncn/",
	"title": "Node Firmware",
	"tags": [],
	"description": "",
	"content": "Node Firmware This page will walk an administrator through NCN BIOS and firmware checkout.\nTo complete firmware checkout, proceed through the below sections:\nConfirm BIOS and Firmware Inventory Identifying BIOS and Hardware Gigabyte Upgrades HPE (iLO) Upgrades Pre-Reqs GUI Redfish Component Firmware Checkout Marvell Upgrades Mellanox Upgrades Enable Tools Check Current Firmware Optional Online Update Confirm BIOS and Firmware Inventory CUSTOMER NOTE If there is doubt that the tar contains latest, the customer should check CrayPort for newer firmware.\nPrepare the inventory; the RPMs providing firmware need to be installed:\npit# export CSM_RELEASE=\u0026lt;insert the name of the CSM release folder\u0026gt; pit# find /var/www/ephemeral/${CSM_RELEASE}/firmware -name *.rpm -exec zypper -n in --auto-agree-with-licenses --allow-unsigned-rpm {} \\+ Hide the old firmware; cleanup the directory\nNOTE This step will be removed in later versions of Shasta; this is correcting the layout of the directory.\npit# mv /var/www/fw/river /var/www/fw/.river-old Set web-links for the new firmware:\npit# \\ mkdir -pv /var/www/fw/river/hpe find /opt/cray/fw -name *.flash -exec ln -snf {} /var/www/fw/river/hpe/ \\; find /opt/cray/fw -name *.bin -exec ln -snf {} /var/www/fw/river/hpe/ \\; mkdir -pv /var/www/fw/river/gb find /opt/cray/FW/bios -name sh-svr* -exec ln -snf {} /var/www/fw/river/gb/ \\; mkdir -pv /var/www/fw/mountain/cray find /opt/cray/FW/bios -mindepth 0 -maxdepth 1 -type f -exec ln -snf {} /var/www/fw/mountain/cray/ \\; Make a tftp symlink for Gigabyte nodes:\npit #ln -snf ../fw /var/www/boot/fw Identifying BIOS and Hardware Checkout BIOS and BMC firmware with ipmitool:\nFrom the NCN: ncn-m002# pdsh -b -w $(grep -oP \u0026#39;ncn-\\w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) \u0026#39; ipmitool fru | grep -i \u0026#34;board product\u0026#34; \u0026amp;\u0026amp; \\ ipmitool mc info | grep -i \u0026#34;firmware revision\u0026#34; \u0026amp;\u0026amp; \\ ipmitool fru | grep -i \u0026#34;product version\u0026#34; \u0026#39; | sort -u From the LiveCD pit# \\ export mtoken=\u0026#39;ncn-m(?!001)\\w+-mgmt\u0026#39; export stoken=\u0026#39;ncn-s\\w+-mgmt\u0026#39; export wtoken=\u0026#39;ncn-w\\w+-mgmt\u0026#39; export username=root export IPMI_PASSWORD=changeme grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | xargs -t -i ipmitool -I lanplus -U $username -E -H {} fru | grep -i \u0026#39;board product\u0026#39; grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | xargs -t -i ipmitool -I lanplus -U $username -E -H {} mc info | grep -i \u0026#39;firmware revision\u0026#39; Manufacturer Examples Gigabyte:\nNOTE On Gigabyte, the Product Version can be disregarded. It may become valuable at a later date.\nncn-m001: Board Product : MZ32-AR0-00 ncn-m001: Firmware Revision : 12.84 ncn-m001: Product Version : 0100 ncn-m002: Board Product : MZ32-AR0-00 ncn-m002: Firmware Revision : 12.84 ncn-m002: Product Version : 0100 ncn-m003: Board Product : MZ32-AR0-00 ncn-m003: Firmware Revision : 12.84 ncn-m003: Product Version : 0100 ncn-s001: Board Product : MZ32-AR0-00 ncn-s001: Firmware Revision : 12.84 ncn-s001: Product Version : 0100 ncn-s002: Board Product : MZ32-AR0-00 ncn-s002: Firmware Revision : 12.84 ncn-s002: Product Version : 0100 ncn-s003: Board Product : MZ32-AR0-00 ncn-s003: Firmware Revision : 12.84 ncn-s003: Product Version : 0100 ncn-w001: Board Product : MZ32-AR0-00 ncn-w001: Firmware Revision : 12.84 ncn-w001: Product Version : 0100 ncn-w002: Board Product : MZ32-AR0-00 ncn-w002: Firmware Revision : 12.84 ncn-w002: Product Version : 0100 ncn-w003: Board Product : MZ32-AR0-00 ncn-w003: Firmware Revision : 12.84 ncn-w003: Product Version : 0100 HPE:\nncn-m001: Board Product : Marvell 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter ncn-m001: Board Product : ProLiant DL325 Gen10 Plus ncn-m001: Firmware Revision : 2.33 ncn-m001: Product Version : ncn-m001: Product Version : 10/30/2020 ncn-m002: Board Product : Marvell 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter ncn-m002: Board Product : ProLiant DL325 Gen10 Plus ncn-m002: Firmware Revision : 2.33 ncn-m002: Product Version : ncn-m002: Product Version : 10/30/2020 ncn-m003: Board Product : Marvell 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter ncn-m003: Board Product : ProLiant DL325 Gen10 Plus ncn-m003: Firmware Revision : 2.33 ncn-m003: Product Version : ncn-m003: Product Version : 10/30/2020 ncn-s001: Board Product : Marvell 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter ncn-s001: Board Product : ProLiant DL325 Gen10 Plus ncn-s001: Firmware Revision : 2.33 ncn-s001: Product Version : ncn-s001: Product Version : 10/30/2020 ncn-s002: Board Product : Marvell 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter ncn-s002: Board Product : ProLiant DL325 Gen10 Plus ncn-s002: Firmware Revision : 2.33 ncn-s002: Product Version : ncn-s002: Product Version : 10/30/2020 ncn-s003: Board Product : Marvell 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter ncn-s003: Board Product : ProLiant DL325 Gen10 Plus ncn-s003: Firmware Revision : 2.33 ncn-s003: Product Version : ncn-s003: Product Version : 10/30/2020 ncn-w001: Board Product : Marvell 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter ncn-w001: Board Product : ProLiant DL325 Gen10 Plus ncn-w001: Firmware Revision : 2.33 ncn-w001: Product Version : ncn-w001: Product Version : 10/30/2020 ncn-w002: Board Product : Marvell 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter ncn-w002: Board Product : ProLiant DL325 Gen10 Plus ncn-w002: Firmware Revision : 2.33 ncn-w002: Product Version : ncn-w002: Product Version : 10/30/2020 ncn-w003: Board Product : Marvell 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter ncn-w003: Board Product : ProLiant DL325 Gen10 Plus ncn-w003: Firmware Revision : 2.33 ncn-w003: Product Version : ncn-w003: Product Version : 10/30/2020 Refer to the table below, and use the output from the previous step to map the following information:\nNCN Type: Board Product maps to the \u0026ldquo;Board Product\u0026rdquo; column NCN BIOS:Product Version : 10/30/2020 maps to the \u0026ldquo;Version\u0026rdquo; column (note: ignore this field for Gigabyte) NCN BMC Firmware: Firmware Revision maps to the rows for the BMC IMPORTANT NOTE Do not downgrade firmware unless directed to by the table(s) below. In the event that newer firmware is found, the administrator should return to their Shasta installation and consider this step completed. Only on rare circumstance will certain downgrades be required.\nBIOS and BMC Firmware Version Reference CRAY Node Type Manufacturer Board Product Device Type Version Downgrade (Y/n)? LiveCD Location NCN Gigabyte MZ32-AR0 BIOS 21.00.00 YES http://pit/fw/river/gb/sh-svr-1264up-bios/bios/RBU/image.RBU NCN Gigabyte MZ32-AR0 BMC 12.84.09 YES http://pit/fw/river/gb/sh-svr-1264up-bios/bmc/fw/128409.bin CN Gigabyte MZ62-HD0 BIOS 20.03.00 NO http://pit/fw/river/gb/sh-svr-3264-bios/bios/RBU/image.RBU CN Gigabyte MZ62-HD0 BMC 12.84.09 NO http://pit/fw/river/gb/sh-svr-3264-bios/bmc/fw/128409.bin CN Gigabyte MZ62-HD0 CMC 62.84.02 NO http://pit/fw/river/gb/sh-svr-3264-bios/bmc/fw/628402.bin UAN Gigabyte MZ92-FS0 BIOS 20.03.00 NO http://pit/fw/river/gb/sh-svr-5264-gpu-bios/bios/RBU/image.RBU UAN Gigabyte MZ92-FS0 BMC 12.84.09 NO http://pit/fw/river/gb/sh-svr-5264-gpu-bios/bmc/fw/128409.bin NCN HPE A42 ProLiant DL385 Gen10 Plus BIOS 10/30/2020 1.38 NO http://pit/fw/river/hpe/A42_1.38_10_30_2020.signed.flash NCN HPE A43 ProLiant DL325 Gen10 Plus BIOS 10/30/2020 1.38 NO http://pit/fw/river/hpe/A43_1.38_10_30_2020.signed.flash NCN HPE iLO5 BMC 2.33 NO http://pit/fw/river/hpe/ilo5_233.bin CN CRAY EX235n BIOS ex235n.bios-1.0.3 NO http://pit/fw/mountain/cray/ex235n.bios-1.0.3.tar.gz CN CRAY EX425 BIOS ex425.bios-1.4.3 NO http://pit/fw/mountain/cray/ex425.bios-1.4.3.tar.gz For each server that is lower than the items above (except for any downgrade exceptions), run through these guides to update them:\nGigabyte Upgrades HPE (iLO) Upgrades Gigabyte Upgrades For Gigabyte upgrades a tftp server needs to be referred to.\nGUI From the administrators own machine, SSH tunnel (-L creates the tunnel, and -N prevents a shell and stubs the connection). One at a time, or all together.\nssh -L 6443:ncn-m002-mgmt:443 -N $system_name-ncn-m001 ssh -L 7443:ncn-m003-mgmt:443 -N $system_name-ncn-m001 ssh -L 8443:ncn-w001-mgmt:443 -N $system_name-ncn-m001 ssh -L 9443:ncn-w002-mgmt:443 -N $system_name-ncn-m001 ssh -L 10443:ncn-w003-mgmt:443 -N $system_name-ncn-m001 ssh -L 11443:ncn-s001-mgmt:443 -N $system_name-ncn-m001 ssh -L 12443:ncn-s002-mgmt:443 -N $system_name-ncn-m001 ssh -L 13443:ncn-s003-mgmt:443 -N $system_name-ncn-m001 One at a time in (to prevent log-outs from duplicate SSL/CA) open each and run through the nested steps:\nhttps://127.0.0.1:6443 https://127.0.0.1:7443 https://127.0.0.1:8443 https://127.0.0.1:9443 https://127.0.0.1:10443 https://127.0.0.1:11443 https://127.0.0.1:12443 https://127.0.0.1:13443 Login with the default credentials.\nOn the Left, select \u0026ldquo;Maintenance\u0026rdquo;\nIn the new pane, select \u0026ldquo;Firmware Image Location\u0026rdquo; Configure the TFTP Server:\nServer Address: The HMN IP of the PIT node (ip a show vlan004) Image Name: The LiveCD Location from the above table, minus the base URL (e.g. /fw/river/gb/sh-svr-1264up-bios/bios/RBU/image.RBU) Press SAVE when done Go back to \u0026ldquo;Maintenance\u0026rdquo;, then select \u0026ldquo;Firmware Update\u0026rdquo;\nChange the selection to BIOS and then press \u0026ldquo;Flash\u0026rdquo; Next. Go back to the \u0026ldquo;Firmware Image Location\u0026rdquo; and modify it to fetch the BMC ROM: Press Proceed to Flash; ensure the Update Type is set to BMC\nIMPORTANT Make sure to check off \u0026ldquo;Preserve all configuration\u0026rdquo; otherwise network connectivity may be lost after reset.\nNow repeat this for m001, however for every location http://pit is used we need to use 127.0.0.1 instead.\nReboot the PIT node back into itself:\npit# bootcurrent=$(efibootmgr | grep -i bootcurrent | awk \u0026#39;{print $NF}\u0026#39;) pit# efibootmgr -n $bootcurrent pit# reboot You are now finished with FW updates.\nHPE (iLO) Upgrades Firmware is located on the LiveCD (versions 1.4.6 or higher).\nPre-Reqs BMCs are reachable; dnsmasq is setup and BMCs show in /var/lib/misc/dnsmasq.leases Servers can be off Static entries in dnsmasq are a bonus; helpful but unnecessary. GUI From the administrators own machine, SSH tunnel (-L creates the tunnel, and -N prevents a shell and stubs the connection). One at a time, or all together.\nssh -L 6443:ncn-m002-mgmt:443 -N $system_name-ncn-m001 ssh -L 7443:ncn-m003-mgmt:443 -N $system_name-ncn-m001 ssh -L 8443:ncn-w001-mgmt:443 -N $system_name-ncn-m001 ssh -L 9443:ncn-w002-mgmt:443 -N $system_name-ncn-m001 ssh -L 10443:ncn-w003-mgmt:443 -N $system_name-ncn-m001 ssh -L 11443:ncn-s001-mgmt:443 -N $system_name-ncn-m001 ssh -L 12443:ncn-s002-mgmt:443 -N $system_name-ncn-m001 ssh -L 13443:ncn-s003-mgmt:443 -N $system_name-ncn-m001 One at a time in (to prevent log-outs from duplicate SSL/CA) open each and run through the nested steps:\nhttps://127.0.0.1:6443 https://127.0.0.1:7443 https://127.0.0.1:8443 https://127.0.0.1:9443 https://127.0.0.1:10443 https://127.0.0.1:11443 https://127.0.0.1:12443 https://127.0.0.1:13443 Login with the default credentials. On the Left, select \u0026ldquo;Firmware \u0026amp; OS Software\u0026rdquo; On the Right, select \u0026ldquo;Upload Firmware\u0026rdquo; Select \u0026ldquo;Remote File\u0026rdquo; and \u0026ldquo;Confirm TPM override\u0026rdquo;, and then choose your firmware file: Remote File URL: Use the \u0026ldquo;LiveCD Location\u0026rdquo; value from the table above. Confirm TPM Override: Check this box to confirm the flash. Press Flash and wait for the upload and flash to complete. iLO may reboot after flash. Now grab the iLO5 Firmware the same way: On the Right, select \u0026ldquo;Upload Firmware\u0026rdquo; Select \u0026ldquo;Remote File\u0026rdquo; and \u0026ldquo;Confirm TPM override\u0026rdquo;, and then choose your firmware file: Press Flash and wait for the upload and flash to complete. iLO may reboot after flash. Cold boot the node, or momentarily press the button (GUI button) to power it on. After the other nodes are completed, the PIT node can be upgraded. (Alternatively this could be done first):\nRepeat the same process, using the external BMC URL for the PIT node\u0026rsquo;s BMC (e.g. https://system-ncn-m001-mgmt) For the Remote File URL use 127.0.0.1 instead of pit (e.g. http://127.0.0.1/fw/river/hpe/A42_1.38_10_30_2020.signed.flash) Before rebooting the node, save any work and set the BootNext to the current boot sessions device: pit# bootcurrent=$(efibootmgr | grep -i bootcurrent | awk \u0026#39;{print $NF}\u0026#39;) pit# efibootmgr -n $bootcurrent pit# reboot All NCNs are now updated via GUI.\nRedfish Not Ready This LiveCD bash script is broken, and will be fixed. It will allow remote BIOS and firmware updates and checkout from the PIT node.\nSet login vars for redfish™\nexport username=root export password=changeme Invoke mfw with the matching firmware (check ls 1 /var/www/fw/river/hpe for a list)\npit# /root/bin/mfw A43_1.30_07_18_2020.signed.flash Watch status:\npit# curl -sk -u $username:$password https://$1/redfish/v1/UpdateService | jq |grep -E \u0026#39;State|Progress|Status\u0026#39;\u0026#34; Component Firmware Checkout This covers PCIe devices.\nNote: The Mellanox firmware can be updated to minimum spec. using mlxfwmanager. The mlxfwmanager will fetch updates from online, or it can use a local file (or local web server such as http://pit/).\nFind more information for each vendor below:\nMarvell Upgrades Mellanox Upgrades Vendor Model PSID Version Downgrade (Y/n)? LiveCD Location Marvell QL41232HQCU-HC 08.50.78 NO unavailable Mellanox MCX416A-BCA* CRAY000000001 12.28.2006 NO http://pit/fw/pcie/images/CRAY000000001.bin Mellanox MCX515A-CCA* MT_0000000011 and MT_0000000591 16.28.2006 NO http://pit/fw/pcie/images/MT_0000000011.bin Marvell Upgrades There are no upgrades at this time for Marvell.\nMellanox Upgrades Shasta 1.4 NCNs are # Print name and current state; on an NCN or on the liveCD.\nEnable Tools MST needs to be started for the tools to work.\nlinux# mst status Starting MST (Mellanox Software Tools) driver set Loading MST PCI module - Success Loading MST PCI configuration module - Success Create devices Unloading MST PCI module (unused) - Success Check Current Firmware Some nodes will not have any Mellanox cards. If mlxfwmanager returns with no devices found after mst start was run then this section should be skipped for that NCN.\nStart and Run Mellanox Firmware services to check for firmware revision:\nlinux# mlxfwmanager Querying Mellanox devices firmware ... Device #1: ---------- Device Type: ConnectX5 Part Number: MCX515A-CCA_Ax_Bx Description: ConnectX-5 EN network interface card; 100GbE single-port QSFP28; PCIe3.0 x16; tall bracket; ROHS R6 PSID: MT_0000000011 PCI Device Name: /dev/mst/mt4119_pciconf1 Base GUID: 506b4b030028505c Base MAC: 506b4b28505c Versions: Current Available FW 16.28.4000 N/A PXE 3.6.0103 N/A UEFI 14.21.0021 N/A Status: No matching image found Device #2: ---------- Device Type: ConnectX5 Part Number: MCX515A-CCA_Ax_Bx Description: ConnectX-5 EN network interface card; 100GbE single-port QSFP28; PCIe3.0 x16; tall bracket; ROHS R6 PSID: MT_0000000011 PCI Device Name: /dev/mst/mt4119_pciconf0 Base GUID: 98039b03001eda3c Base MAC: 98039b1eda3c Versions: Current Available FW 16.28.4000 N/A PXE 3.6.0103 N/A UEFI 14.21.0021 N/A Status: No matching image found Download and run the update on the NCN\nncn# curl -O http://pit/fw/pcie/images/MT_0000000011.bin ncn# curl -O http://pit/fw/pcie/images/CRAY000000001.bin ncn# mlxfwmanager -u -i ./MT_0000000011.bin -y ncn# mlxfwmanager -u -i ./CRAY000000001.bin -y Update the PIT node:\nncn# mlxfwmanager -u -i /var/www/fw/pcie/images/MT_0000000011.bin -y ncn# mlxfwmanager -u -i /var/www/fw/pcie/images/CRAY000000001.bin -y Optional Online Update The non-HSN PCIe cards, like the NCN\u0026rsquo;s management PCIe cards, can obtain updates from the Internet:\nWhen firmware is not available on the LiveCD and Internet access is available to the NCN Simply run this to update the card, after finding the PCI Device Name in the mlxfwmanager output:\nncn# mlxfwmanager -u --online -d /dev/mst/\u0026lt;mst_device_id\u0026gt; "
},
{
	"uri": "/docs-csm/en-09/253-ncn-hw-swap/",
	"title": "NCN Hardware Swaps",
	"tags": [],
	"description": "",
	"content": "NCN Hardware Swaps This page will detail cases for various hardware changes.\nNode Components Nodes Rebooting Servers CEPH Kubernetes Safely Removing Nodes from Runtime Rebuilding CEPH NCNs Quorum Rebuilding K8s NCNs Master nodes Worker nodes Enable Kdump Node Components Malfunctioning or disabled hardware may need to be removed, or upgrades may want to be installed.\nFor either case, certain hardware requires that the node be shutdown prior to operations.\nComponent Server Off Rebuild Required cpu Yes No ram Yes No OS disks No No 1 Ephemeral disks No Yes gpu Yes No nic Yes Yes NOTE: These instructions only apply prior to booting off the LiveCD \u0026ndash; once that step is complete refer to the \u0026ldquo;Rebuild NCNs\u0026rdquo; section in the HPE Cray EX Hardware Management Administration Guide S-8015.\nRebooting Nodes For operations that do not require a rebuild, a power off and cold boot will suffice.\nCEPH STUB This is a stub that requires code snippets to step-wise replace a storage node (short of rebuilding everything).\nKubernetes If the node can be powered off nicely by issuing a poweroff command on the CLI, then it will evict its containers and unmount etcd. On power-up it will re-join.\nIf the node is unresponsive, you can alert the cluster that you will be rebooting it by evicting the node:\nlinux# kubectl drain ncn-w002 Then you can reboot, and nicely tell the node to add the node back.\nlinux# kubectl uncordon ncn-w002 Nodes Swapping a node for an entirely new node mandates a \u0026ldquo;rebuild\u0026rdquo; (or a \u0026ldquo;build\u0026rdquo; if this is the first use).\nSafely Removing Nodes from Runtime Rebuilding CEPH NCNs STUB This is a stub that requires code snippets to step-wise replace a storage node (short of rebuilding everything).\nQuorum STUB This is a stub that requires definition of constraints put on by the Ceph cluster when rebuilding nodes.\nRebuilding Kubernetes NCNs Master Nodes If etcd has met quorum, if there are 3 master nodes active, then etcd must expunge the node we are rebuilding.\nEvict etcd: STUB This is a stub that requires code snippets to search-and-destroy OOM.\nFollow the procedure for worker nodes. Worker Nodes It is dangerous to run with 2 worker nodes or less, work must be done with diligence or pod clean-up will be necessary. Kubernetes pods will begin to throw Out-Of-Memory error after some time.\nDrain the target worker node, issue the command from your laptop (if authenticated) or from an ingress node (such as ncn-m001):\nlinux# kubectl drain ncn-w002 linux# kubectl delete ncn-w002 Power down the node either in rack, or with ipmitool\nexport IPMI_PASSWORD=changeme export username=root ipmitool -I lanplus -U $username -E -H ncn-w002-mgmt power off Now commence the operations on the node.\nOnce ready, power the node on in the rack or with ipmitool\nexport IPMI_PASSWORD=changeme export username=root ipmitool -I lanplus -U $username -E -H ncn-w002-mgmt power on The node will netboot from sysmgmt services (kea/unbound/s3/bss).\nThe node will run cloud-init and will auto-join the cluster again. Monitor for status with:\nlinux# kubectl get nodes -w Once your node returns to the cluster, the procedure is done.\nEnable Kdump Enable kdump on the NCNs after they are rebuilt.\nCleaning up Out-Of-Memory Pods STUB This is a stub that requires code snippets to search-and-destroy OOM.\nIf replacing all OS disks then a rebuild is required.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"
},
{
	"uri": "/docs-csm/en-09/254-ncn-firmware-gb/",
	"title": "Gigabyte Firmware Bug",
	"tags": [],
	"description": "",
	"content": "Gigabyte Firmware Bug Due to a bug in the Gigabyte firmware, the Shasta 1.4 install may negatively impact Gigabyte motherboards when attempting to boot using bonded Mellanox network cards. The result is a board that is unusable until a CMOS clear is physically done via a jumper on the board itself.\nA patched firmware release is expected to be available for a future release of Shasta. It is recommended that Gigabyte users wait for this new firmware before attempting an installation of Shasta 1.4. The procedure to recover the boards is included below.\nClear BIOS settings by jumper Pull the power cables or blade server from the chassis, and open the system top cover. Move the Clear CMOS Jumper to 2-3, and wait 2 to 3 seconds. Move the Clear CMOS Jumper to 1-2. Motherboard MZ62-HD0-00/-YF for H262 chassis Motherboard MZ32-AR0-00/-YF for R272 chassis Motherboard MZ92-FS0-00/-YF for R282 chassis "
},
{
	"uri": "/docs-csm/en-09/255-firmware-action-service-fas/",
	"title": "FIRMWARE ACTION SERVICE (FAS) | User Procedures",
	"tags": [],
	"description": "",
	"content": "FIRMWARE ACTION SERVICE (FAS) | User Procedures Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system. See \u0026ldquo;Configure the Cray Command Line Interface (CLI)\u0026rdquo; in the HPE Cray EX System Administration Guide S-8001 for more information.\nIMPORTANT It may be necessary to explicitly set the power state of certain devices before performing a given firmware update. Refer to ()[] in order to understand the process for setting power state and controlling relevant settings.\nExecute an Action Objective Use the Firmware Action Service (FAS) to execute an action. An action produces a set of firmware operations. Each operation represents an xname + target on that xname that will be targeted for update. There are two of firmware action modes: : dryrun or liveupdate; the parameters used when creating either are completely identical except for the overrideDryrun setting. overrideDryrun will determine if feature to determine what firmware can be updated on the system. Dry-runs are enabled by default, and can be configured with the overrideDryrun parameter. A dry-run will create a query according to the filters requested by the admin. It will initiate an update sequence to determine what firmware is available, but will not actually change the state of the firmware\nWARNING: It is crucial that an admin is familiar with the release notes of any firmware. The release notes will indicate what new features the firmware provides and if there are any incompatibilities. FAS does not know about incompatibilities or dependencies between versions. The admin assumes full responsibility for this knowledge.\nWARNING: It is likely that when performing a firmware update, that the current version of firmware will not be available. This means that after successfully upgrading, the firmware cannot be reverted (i.e. downgraded to previous version).\nSteps This will cover the generic process for executing an action. For more specific examples and detailed explanations of options see the recipes.md file.\nIdentify the selection of filters you want to apply. Filters narrow the scope of FAS to target-specific xnames, manufacturers, targets, etc. For our purpose we will run FAS \u0026lsquo;wide open\u0026rsquo;, with no selection filters applied.\ncreate a JSON file {whole-system-dryrun.json}; to make this a live update set \u0026quot;overrideDryrun\u0026quot;: true\n{ \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;dryrun of full system\u0026#34; } } execute the dryrun # cray fas actions create {whole-system-dryrun.json} ... { \u0026#34;actionID\u0026#34;: \u0026#34;e0cdd7c2-32b1-4a25-9b2a-8e74217eafa7\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false } Note the returned actionID\nSee \u0026lsquo;interpreting an Action\u0026rsquo; for more information.\nAbort an Action Objective Firmware updates can be stopped if required. This is useful given only one action can be run at a time. This is to protect hardware from multiple actions trying to modify it at the same time.\nIMPORTANT: If a Redfish update is already in progress, the abort will not stop that process on the device. It is likely the device will update. If the device needs to be manually power cycled (needManualReboot), it is possible that the device will update, but not actually apply the update until its next reboot. Admins must verify the state of the system after an abort. Only perform an abort if truly necessary. The best way to check the state of the system is to do a snapshot or do a dry-run of an update.\nSteps Issue the abort command to the action # cray fas actions instance delete {actionID} Note: The action could take up to a minute to fully abort.\nDescribe an Action Objective There are several ways to get more information about a firmware update. An actionID and operationIDs are generated when an live update or dry-run is created. These values can be used to learn more about what is happening on the system during an update.\nInterpreting Output For the steps below, the following returned messages will help determine if a firmware update is needed. The following are end states for operations. The Firmware action itself should be in completed once all operations have finished.\nNoOp: Nothing to do, already at version. NoSol: No viable image is available; this will not be updated. succeeded: IF dryrun: The operation should succeed if performed as a live update. succeeded means that FAS identified that it COULD update an xname + target with the declared strategy. IF live update: the operation succeeded, and has updated the xname + target to the identified version. failed: IF dryrun : There is something that FAS could do, but it likely would fail; most likely because the file is missing. IF live update : the operation failed, the identified version could not be put on the xname + target. Data can be viewed at several levels of information:\nSteps Get High Level Summary To view counts of operations, what state they are in, the overall state of the action, and what parameters were used to create the action:\n# cray fas actions describe {actionID} blockedBy = [] state = \u0026#34;completed\u0026#34; actionID = \u0026#34;0a305f36-6d89-4cf8-b4a1-b9f199afaf3b\u0026#34; startTime = \u0026#34;2020-06-23 15:43:42.939100799 +0000 UTC\u0026#34; snapshotID = \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; endTime = \u0026#34;2020-06-23 15:48:59.586748151 +0000 UTC\u0026#34; [actions.command] description = \u0026#34;upgrade of x9000c1s3b1 Nodex.BIOS to WNC 1.1.2\u0026#34; tag = \u0026#34;default\u0026#34; restoreNotPossibleOverride = true timeLimit = 1000 version = \u0026#34;latest\u0026#34; overrideDryrun = false [actions.operationCounts] noOperation = 0 succeeded = 2 verifying = 0 unknown = 0 configured = 0 initial = 0 failed = 0 noSolution = 0 aborted = 0 needsVerified = 0 total = 2 inProgress = 0 blocked = 0 [[actions]] blockedBy = [] state = \u0026#34;completed\u0026#34; actionID = \u0026#34;0b9300d6-8f06-4019-a8fa-7b3ff65e5aa8\u0026#34; startTime = \u0026#34;2020-06-18 03:06:25.694573366 +0000 UTC\u0026#34; snapshotID = \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; endTime = \u0026#34;2020-06-18 03:11:06.806297546 +0000 UTC\u0026#34; NOTE : unless the action\u0026rsquo;s state is completed or aborted; then this action is still under progress.\nGet Details of Action # cray fas actions describe {actionID} --format json { \u0026#34;parameters\u0026#34;: { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;routerBMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;dryrun\u0026#34;: false, \u0026#34;description\u0026#34;: \u0026#34;upgrade of routerBMCs for cray\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;imageFilter\u0026#34;: { \u0026#34;imageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] } }, \u0026#34;blockedBy\u0026#34;: [], \u0026#34;state\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;command\u0026#34;: { \u0026#34;dryrun\u0026#34;: false, \u0026#34;description\u0026#34;: \u0026#34;upgrade of routerBMCs for cray\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;actionID\u0026#34;: \u0026#34;e0cdd7c2-32b1-4a25-9b2a-8e74217eafa7\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-06-26 20:03:37.316932354 +0000 UTC\u0026#34;, \u0026#34;snapshotID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2020-06-26 20:04:07.118243184 +0000 UTC\u0026#34;, \u0026#34;operationSummary\u0026#34;: { \u0026#34;succeeded\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;verifying\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;unknown\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;configured\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;initial\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;failed\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [ { \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x5000c1r7b0\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;0796eed0-e95d-45ea-bc71-8903d52cffde\u0026#34; }, ] }, \u0026#34;noSolution\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;aborted\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;needsVerified\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;noOperation\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;inProgress\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;blocked\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] } } } Get Details of Operation Using the operationID listed in the actions array we can see the full detail of the operation.\n# cray fas operations describe operationID --format json { \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fromTag\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fromImageURL\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2020-06-24 14:23:37.544814197 +0000 UTC\u0026#34;, \u0026#34;actionID\u0026#34;: \u0026#34;f48aabf1-1616-49ae-9761-a11edb38684d\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-06-24 14:19:15.10128214 +0000 UTC\u0026#34;, \u0026#34;fromSemanticFirmwareVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;toImageURL\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;WindomNodeCard_REV_D\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;24a5e5fb-5c4f-4848-bf4e-b071719c1850\u0026#34;, \u0026#34;fromImageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;toImageID\u0026#34;: \u0026#34;71c41a74-ab84-45b2-95bd-677f763af168\u0026#34;, \u0026#34;toSemanticFirmwareVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refreshTime\u0026#34;: \u0026#34;2020-06-24 14:23:37.544824938 +0000 UTC\u0026#34;, \u0026#34;blockedBy\u0026#34;: [], \u0026#34;toTag\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected nc.1.3.8-shasta-release.arm.2020-06-15T22:57:31+00:00.b7f0725 got: nc.1.2.25-shasta-release.arm.2020-05-15T17:27:16+00:00.0cf7f51\u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;expirationTime\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x9000c1s3b1\u0026#34;, \u0026#34;toFirmwareVersion\u0026#34;: \u0026#34;\u0026#34; } Override Image for Update If an update fails due to \u0026quot;No Image available\u0026quot;, it may be caused by FAS unable to match the data on the node to find an image in the image list.\nFind the available image in FAS using the command: (change TARGETNAME to the actual target you are looking for)\ncray fas images list --format json | jq \u0026#39;.[] | .[] | select(.target==\u0026#34;TARGETNAME\u0026#34;)\u0026#39; This command would display one or more images available for updates.\n{ \u0026#34;imageID\u0026#34;: \u0026#34;ff268e8a-8f73-414f-a9c7-737a34bb02fc\u0026#34;, \u0026#34;createTime\u0026#34;: \u0026#34;2021-02-24T02:25:03Z\u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;nodeBMC\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34;, \u0026#34;models\u0026#34;: [ \u0026#34;HPE Cray EX235n\u0026#34;, \u0026#34;GrizzlyPkNodeCard_REV_B\u0026#34; ], \u0026#34;softwareIds\u0026#34;: [ \u0026#34;fgpa:NVIDIA.HGX.A100.4.GPU:*:*\u0026#34; ], \u0026#34;target\u0026#34;: \u0026#34;Node0.AccFPGA0\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;default\u0026#34; ], \u0026#34;firmwareVersion\u0026#34;: \u0026#34;2.7\u0026#34;, \u0026#34;semanticFirmwareVersion\u0026#34;: \u0026#34;2.7.0\u0026#34;, \u0026#34;pollingSpeedSeconds\u0026#34;: 30, \u0026#34;s3URL\u0026#34;: \u0026#34;s3:/fw-update/80a62641764711ebabe28e2b78a05899/accfpga_nvidia_2.7.tar.gz\u0026#34; } If the firmwareVersion from the FAS image matches the fromFirmwareVersion from the FAS action, the firmware is at the latest version and no update is needed.\nUsing the imageID from the cray images list command above (in the example above it would be: ff268e8a-8f73-414f-a9c7-737a34bb02fc) add the following line to your action JSON file, replacing IMAGEID with the imageID:\n\u0026#34;imageFilter\u0026#34;: { \u0026#34;imageID\u0026#34;:\u0026#34;IMAGEID\u0026#34;, \u0026#34;overrideImage\u0026#34;:true } Example actions JSON file with imageFilter added:\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;:[\u0026#34;nodeBMC\u0026#34;] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;:\u0026#34;cray\u0026#34; }, \u0026#34;imageFilter\u0026#34;: { \u0026#34;imageID\u0026#34;:\u0026#34;ff268e8a-8f73-414f-a9c7-737a34bb02fc\u0026#34;, \u0026#34;overrideImage\u0026#34;:true }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;:[\u0026#34;Node0.AccFPGA0\u0026#34;,\u0026#34;Node1.AccFPGA0\u0026#34;] }, \u0026#34;command\u0026#34;: { \u0026#34;overrideDryrun\u0026#34;:false, \u0026#34;restoreNotPossibleOverride\u0026#34;:true, \u0026#34;overwriteSameImage\u0026#34;:false } } To be sure you grabbed the correct imageID, you can run the command:\ncray fas images describe imageID WARNING: FAS will force a flash of the device, using incorrect firmware may make it inoperable.\nRerun FAS actions command using the updated JSON file. It is strongly recommended you run a Dry Run (overrideDryrun=false) first and check the actions output.\n"
},
{
	"uri": "/docs-csm/en-09/256-firmware-action-service-fas-recipes/",
	"title": "Firmware Action Service (FAS) Administration Guide",
	"tags": [],
	"description": "",
	"content": "Firmware Action Service (FAS) Administration Guide Recipes Manufacturer : Cray Device Type : RouterBMC | Target : BMC Device Type : ChassisBMC | Target: BMC Device Type : NodeBMC | Target : BMC Device Type : NodeBMC | Target : NodeBIOS Device Type : NodeBMC | Target : Redstone FPGA Manufacturer : HPE Device Type : NodeBMC | Target : iLO 5 aka BMC Device Type : NodeBMC | Target : System ROM aka BIOS Manufacturer : Gigabyte Device Type : NodeBMC | Target : BMC Device Type : NodeBMC | Target : BIOS Special Note: updating NCNs FAS Filters for actions and snapshots FAS uses five primary filters to determine what operations to create. The filters are listed below:\nSelection Filters -\u0026gt; determine what operations will be created stateComponentFilter targetFilter inventoryHardwareFilter imageFilter Command Filters -\u0026gt; determine how the operations will be executed command All filters are logically connected with AND logic. Only the stateComponentFilter, targetFilter, and inventoryHardwareFilter are used for snapshots.\nSelection Filters stateComponentFilter The state component filter allows users to select hardware to update. Hardware can be selected individually with xnames, or in groups by leveraging the Hardware State Manager (HSM) groups and partitions features.\nParameters xnames - a list of xnames to target partitions - a partition to target groups- a group to target deviceTypes (NodeBMC, RouterBMC, or ChassisBMC \u0026ndash; these are the ONLY 3 allowed types and come from HSM) inventoryHardwareFilter The inventory hardware filter takes place after the state component filter has been applied. It will remove any devices that do not conform to the identified manufacturer or models determined by querying the Redfish endpoint.\nIMPORTANT: There can be a mismatch of hardware models. The model field is human-readable and is human-programmable. In some cases, there can be typos where the wrong model is programmed, which causes issues filtering. If this occurs, query the hardware, find the model name, and add it to the images repository on the desired image.\nParameters: manufacturer - (like Cray, HPE, Gigabyte) model - this is the Redfish reported model, you can specify this but we typically do not for the in-house updates we have done. imageFilter FAS applies images to xname/targets. The image filter is a way to specify an explicit image that should be used. When included with other filters, the image filter reduces the devices considered to only those devices where the image can be applied.\nFor example, if a user specifies an image that only applies to gigabyte, nodeBMCs, BIOS targets. If all hardware in the system is targeted with an empty stateComponentFilter, FAS would find all devices in the system that can be updated via Redfish, and then the image filter would remove all xname/ targets that this image could not be applied. In this example, FAS would remove any device that is not a gigabyte nodeBMC, as well as any target that is not BIOS.\nParameters imageID -\u0026gt; this is the id of the image you want to force onto the system; overrideImage - if this is combined with imageID; it will FORCE the selected image onto all hardware identified, even if it is not applicable. This may cause undesirable outcomes, but most hardware will prevent a bad image from being loaded. targetFilter The target filter selects targets that match against the list. For example, if the user specifies only the BIOS target, FAS will include only operations that explicitly have BIOS as a target. A Redfish device has potentially many targets (members). Targets for FAS are case sensitive and must match Redfish.\nParameters targets - these are the actual \u0026lsquo;members\u0026rsquo; that will be upgraded. Examples include, but are not limited to the following: BIOS BMC NIC Node0.BIOS Node1.BIOS Recovery Command Filters command The command group is the most important part of an action command and controls if the action is executed as dry-run or a live update.\nIt also determines whether or not to override an operation that would normally not be executed if there is no way to return the xname/target to the previous firmware version. This happens if an image does not exist in the image repository.\nThese filters are then applied; and then command parameter applies settings for the overall action: The swagger is a great reference, so I will include just the standards you should most likely use.\nParameters version - usually latest because we want to upgrade usually tag - usually default because we only care about the default image (this can be mostly ignored) overrideDryrun - This determines if this is a LIVE UPDATE or a DRYRUN; if you override; then it will provide a live update restoreNotPossibleOverride - this determines if an update (live or dry run) will be attempted if a restore cannot be performed. Typically we do not have enough firmware to be able to do a rollback; that means if you UPDATE away from a particular version, we probably cannot go back to a previous version. Given our context it is most likely that this value will ALWAYS need to be set true overwriteSameImage - this will cause a firmware update to be performed EVEN if the device is already at the identified, selected version. timeLimit - this is the amount of time in seconds that any operation should be allowed to execute. Most cray stuff can be completed in about 1000 seconds or less; but the gigabyte stuff will commonly take 1,500 seconds or greater. We recommend setting the value to 2000; this is just a stop gap to prevent the operation from never ending, should something get stuck. description- this is a human friendly description; use it! Recipes Below are some example json files that you may find useful when updating specific hardware components. In all of these examples the overrideDryrun field will be set to false; set them to true to perform a live update. We would recommend that when updating an entire system that you walk down the device hierarchy component type by component type, starting first with \u0026lsquo;Routers\u0026rsquo; aka switches, proceeding to Chassis, then finally Nodes. While this is not strictly necessary we have found that it helps eliminate confusion.\nManufacturer : Cray Device Type : RouterBMC | Target : BMC The BMC on the RouterBMC for a Cray includes the ASIC.\n{ \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;routerBMC\u0026#34; ] }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Columbia and/or Colorado router BMC\u0026#34; } } Device Type : ChassisBMC | Target: BMC IMPORTANT: Before updating a CMM, make sure all slot and rectifier power is off. The hms-discovery job must also be stopped before updates and restarted after updates are complete:\nStop hms-discovery job: kubectl -n services patch cronjobs hms-discovery -p '{\u0026quot;spec\u0026quot;:{\u0026quot;suspend\u0026quot;:true}}'\nStart hms-discovery job: kubectl -n services patch cronjobs hms-discovery -p '{\u0026quot;spec\u0026quot;:{\u0026quot;suspend\u0026quot;:false}}'\n{ \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;chassisBMC\u0026#34; ] }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Cray Chassis Controllers\u0026#34; } } Device Type : NodeBMC | Target : BMC { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Olympus node BMCs\u0026#34; } } Device Type : NodeBMC | Target : NodeBIOS IMPORTANT: The Nodes themselves must be powered off in order to update the BIOS on the nodes. The BMC will still have power and will perform the update.\nIMPORTANT: When the BMC is updated or rebooted after updating the Node0.BIOS and/or Node1.BIOS liquid-cooled nodes, the node BIOS version will not report the new version string until the nodes are powered back on. It is recommended that the Node0/1 BIOS be updated in a separate action, either before or after a BMC update and the nodes are powered back on after a BIOS update. The liquid-cooled nodes must be powered off for the BIOS to be updated.\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;Node0.BIOS\u0026#34;, \u0026#34;Node1.BIOS\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Node BIOS\u0026#34; } } NOTE: If this update does not work as expected please check the workaround 257-FIRMWARE-ACTION-SERVICE-CRAY-WINDOM-COMPUTE-NODE-BIOS-WORKAROUND.md.\nDevice Type : NodeBMC | Target : Redstone FPGA IMPORTANT: The Nodes themselves must be powered on in order to update the firmware of the Redstone FPGA on the nodes.\nNOTE: If updating FPGAs fail due to \u0026ldquo;No Image available\u0026rdquo;, you can update using the Override Image for Update procedure in 255-FIRMWARE-ACTIONS-SERVICE-FAS.md. You can find the imageID using the following command: cray fas images list --format json | jq '.[] | .[] | select(.target==\u0026quot;Node0.AccFPGA0\u0026quot;)'\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;Node0.AccFPGA0\u0026#34;, \u0026#34;Node1.AccFPGA0\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Node Redstone FPGA\u0026#34; } } Manufacturer : HPE Device Type : NodeBMC | Target : iLO 5 aka BMC \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;hpe\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;1\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of HPE node iLO 5\u0026#34; } } NOTE: You MUST use 1 as target to indicate iLO 5\nDevice Type : NodeBMC | Target : System ROM aka BIOS NOTE: Node should be powered on for System ROM update and will need to be rebooted to use the updated BIOS.\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;NodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;hpe\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;2\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of HPE node system rom\u0026#34; } } NOTE: You MUST use 2 as target to indicate System ROM\nNOTE: Because of an incorrect string in the image meta data in FAS. Update of System ROM may report as an error when it actually succeeded. You may have to manually check the update version.\nManufacturer : Gigabyte Device Type : NodeBMC | Target : BMC { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;gigabyte\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 2000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Gigabyte node BMCs\u0026#34; } } Note: The timeLimit is 2000 because the Gigabyte BMCs can take a lot longer to update.\nDevice Type : NodeBMC | Target : BIOS { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;gigabyte\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BIOS\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 2000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Gigabyte node BIOS\u0026#34; } } Special Note: updating NCNs NCNs are compute blades; we currently only have NCNs that are manufactured by Gigabyte or HPE. We recommend using the NodeBMC examples from above and including the xname param as part of the stateComponentFilter to target ONLY the xnames you have separately identified as an NCN. Updating more than one NCN at a time MAY cause system instability. Be sure to follow the correct process for updating NCN; FAS accepts no responsibility for updates that do not follow the correct process. Firmware updates have the capacity to harm the system; follow the appropriate guides!\n"
},
{
	"uri": "/docs-csm/en-09/257-firmware-action-service-cray-windom-compute-node-bios-workaround/",
	"title": "Compute Node BIOS workaround for WNC-rome aka HPE CRAY EX425",
	"tags": [],
	"description": "",
	"content": "Compute Node BIOS workaround for WNC-rome aka HPE CRAY EX425 Problem Identification The following conditions must be true in order to qualify for this problem:\nThe system running Shasta v1.4\nThe system has completed CSM installation\nan upgrade via FAS of Cray - Node1.BIOS/Node0.BIOS has been completed following the recipes in 256-FIRMWARE-ACTION-SERVICE-FAS-RECIPES.md\nThe result of the upgrade is that the NodeX.BIOS has failed as noSolution and the stateHelper field for the operation states: \u0026quot;No Image Available\u0026quot;\nThe BIOS in question is running a version \u0026lt;= 1.2.5 (as reported by Redfish; or by describing the noSolution operation in FAS).\nThe hardware model reported by Redfish is wnc-rome; this hardware\u0026rsquo;s marketing designation is HPE CRAY EX425. If your Redfish model is different (ignoring casing), it means the blade(s) in question are not Windom. In this case, please contact technical support.\nTo find the model reported by redfish, drill into the noSolution operation using FAS:\nlinux# cray fas operations describe {operationID} --format json { \u0026#34;operationID\u0026#34;:\u0026#34;102c949f-e662-4019-bc04-9e4b433ab45e\u0026#34;, \u0026#34;actionID\u0026#34;:\u0026#34;9088f9a2-953a-498d-8266-e2013ba2d15d\u0026#34;, \u0026#34;state\u0026#34;:\u0026#34;noSolution\u0026#34;, \u0026#34;stateHelper\u0026#34;:\u0026#34;No Image available\u0026#34;, \u0026#34;startTime\u0026#34;:\u0026#34;2021-03-08 13:13:14.688500503 +0000 UTC\u0026#34;, \u0026#34;endTime\u0026#34;:\u0026#34;2021-03-08 13:13:14.688508333 +0000 UTC\u0026#34;, \u0026#34;refreshTime\u0026#34;:\u0026#34;2021-03-08 13:13:14.722345901 +0000 UTC\u0026#34;, \u0026#34;expirationTime\u0026#34;:\u0026#34;2021-03-08 15:59:54.688500753 +0000 UTC\u0026#34;, \u0026#34;xname\u0026#34;:\u0026#34;x9000c1s0b0\u0026#34;, \u0026#34;deviceType\u0026#34;:\u0026#34;NodeBMC\u0026#34;, \u0026#34;target\u0026#34;:\u0026#34;Node1.BIOS\u0026#34;, \u0026#34;targetName\u0026#34;:\u0026#34;Node1.BIOS\u0026#34;, \u0026#34;manufacturer\u0026#34;:\u0026#34;cray\u0026#34;, \u0026#34;model\u0026#34;:\u0026#34;WNC-Rome\u0026#34;, \u0026#34;softwareId\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;fromImageID\u0026#34;:\u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;fromSemanticFirmwareVersion\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;:\u0026#34;wnc.bios-1.2.5\u0026#34;, \u0026#34;fromImageURL\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;fromTag\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;toImageID\u0026#34;:\u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;toSemanticFirmwareVersion\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;toFirmwareVersion\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;toImageURL\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;toTag\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;blockedBy\u0026#34;:[ ] } The model in this example is WNC-Rome and you can see the firmware version currently running is wnc.bios-1.2.5\nWorkaround Search for the FAS image records for a cray HPE CRAY EX425 Node1.BIOS.\nlinux# cray fas images list --format json | jq \u0026#39;.images[] | select(.manufacturer==\u0026#34;cray\u0026#34;) | select(.target==\u0026#34;Node1.BIOS\u0026#34;) | select(any(.models[]; contains(\u0026#34;EX425\u0026#34;)))\u0026#39; { \u0026#34;imageID\u0026#34;: \u0026#34;e23f5465-ed29-4b18-9389-f8cf0580ca60\u0026#34;, \u0026#34;createTime\u0026#34;: \u0026#34;2021-03-04T00:04:05Z\u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;nodeBMC\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34;, \u0026#34;models\u0026#34;: [ \u0026#34;HPE CRAY EX425\u0026#34; ], \u0026#34;softwareIds\u0026#34;: [ \u0026#34;bios.ex425.*.*\u0026#34; ], \u0026#34;target\u0026#34;: \u0026#34;Node1.BIOS\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;default\u0026#34; ], \u0026#34;firmwareVersion\u0026#34;: \u0026#34;ex425.bios-1.4.3\u0026#34;, \u0026#34;semanticFirmwareVersion\u0026#34;: \u0026#34;1.4.3\u0026#34;, \u0026#34;pollingSpeedSeconds\u0026#34;: 30, \u0026#34;s3URL\u0026#34;: \u0026#34;s3:/fw-update/2227040f7c7d11eb9fa00e2f2e08fd5d/ex425.bios-1.4.3.tar.gz\u0026#34; } Using the imageID from that record create an update command JSON file that will use the image override.\nNOTE YOU MUST CHANGE THE imageID to match your identified image ID\nUsing FAS as normal, launch the action referencing the new JSON file.\nAt this point you should use FAS as normal. The expectation would be that the operations should be succeeded after using the new JSON file.\n{ \u0026#34;stateComponentFilter\u0026#34;:{ \u0026#34;deviceTypes\u0026#34;:[ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;:{ \u0026#34;manufacturer\u0026#34;:\u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;:{ \u0026#34;targets\u0026#34;:[ \u0026#34;Node0.BIOS\u0026#34;, \u0026#34;Node1.BIOS\u0026#34; ] }, \u0026#34;imageFilter\u0026#34;:{ \u0026#34;imageID\u0026#34;:\u0026#34;e23f5465-ed29-4b18-9389-f8cf0580ca60\u0026#34;, \u0026#34;overrideImage\u0026#34;:true }, \u0026#34;command\u0026#34;:{ \u0026#34;version\u0026#34;:\u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;:\u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;:true, \u0026#34;restoreNotPossibleOverride\u0026#34;:true, \u0026#34;timeLimit\u0026#34;:1000, \u0026#34;description\u0026#34;:\u0026#34; upgrade of Node BIOS\u0026#34; } } "
},
{
	"uri": "/docs-csm/en-09/300-service-guides/",
	"title": "Service Guides",
	"tags": [],
	"description": "",
	"content": "Service Guides This guide centers around constructing bootstrap-files and contains various pre-install operations.\nThe remainder of this page provides important nomenclature, notes, and environment help.\nPre-Spring 2020 CRAY System Upgrade Notice Systems built before Sprint 2020 originally used onboard NICs for netbooting. The new topologies for Shasta cease using the onboard NICs. If your system is running Shasta v1.3, then it likely is using onboard NICs.\nIt is recommended to cease using these for Shasta v1.4, an admin would have one less MAC address to track and account for. The NCN networking becomes relatively simpler as a result from caring about one less NIC.\nThis guide may receive more installments for other files as time goes on.\nTable of Contents: Environments Nomenclature and Constraints Files ncn_metadata.csv switch_metadata.csv hmn_connections.json Environments These guides expect you to have access to either of the following things for working on a bare-metal system (assuming freshly racked, or fresh-installing an existing system).\nLiveCD (for more information, see LiveCD USB Boot) Linux and a Serial Console If you do not have the LiveCD, or any other local Linux environment, this data collection may be quicker the alternative method through the 303-NCN-METADATA-USB-SERIAL page.\nThere are 2 parts to the NCN metadata file:\nCollecting the MAC of the BMC Collecting the MAC(s) of the shasta-network interface(s) What is a \u0026ldquo;shasta-network interface\u0026rdquo;? This is not the High-Speed Network interface\nThis is the interface, one or more, that comprise the NCNs\u0026rsquo; LACP link-aggregation ports.\nLACP Bonding NCNs may have 1 or more bond interfaces, which may be comprised from one or more physical interfaces. The preferred default configuration is 2 physical network interfaces per bond. The number of bonds themselves depends on your systems network topology.\nFor example, systems with 4 network interfaces on a given node could configure either of these permutations (for redundancy minimums within Shasta cluster):\n1 bond with 4 interfaces (i.e. bond0) 2 bonds with 2 interfaces each (i.e. bond0 and bond1) For more information, see 103-NETWORKING page for NCNs.\nNomenclature and Constraints \u0026ldquo;PXE\u0026rdquo; or \u0026ldquo;BOOTSTRAP\u0026rdquo; MAC In general this refers to the interface to be used when the node attempts to PXE boot. This varies between vintages of systems; systems before \u0026ldquo;Spring 2020\u0026rdquo; often booted NCNs with onboard NICs, newer systems boot over their PCIe cards.\nIf the system is booting over PCIe than the \u0026ldquo;bootstrap MAC\u0026rdquo; and the \u0026ldquo;bond0 MAC 0\u0026rdquo; will be identical. If the system is booting over onboards then the \u0026ldquo;bootstrap MAC\u0026rdquo; and the \u0026ldquo;bond0 MAC 0\u0026rdquo; will be different.\nOther Nomenclature\n\u0026ldquo;BOND MACS\u0026rdquo; are the MAC addresses for the physical interfaces that your node will use for the various VLANs. \u0026ldquo;NMN MAC\u0026rdquo; is this is the same as the BOND MAC addresses, but with emphasis on the vlan-participation. Relationships \u0026hellip;\nBOND0 MAC0 and BOND0 MAC1 should not be on the same physical network card to establish redundancy for failed chips. On the other hand, if any nodes\u0026rsquo; capacity prevents it from being redundant, then MAC1 and MAC0 will still produce a valid configuration if they do reside on the same physical chip/card. The BMC MAC is the exclusive, dedicated LAN for the onboard BMC. It should not be swapped with any other device. Files Each paragraph here will denote which pre-reqs are needed and which pages to follow for data collection.\nncn_metadata.csv Unless your system is sans-onboards, meaning it does not use or does not have onboard NICs on the non-compute nodes, then these guides will be necessary before (re)constructing the ncn_metadata.csv file.\nRecabling from Shasta v1.3 for Shasta v1.4 (for machines still using ncn-w001 for BIS node) Enabling Network Boots over Spine Switches (for Shasta v1.3 machines) The following two guides will assist with (re)creating ncn_metadata.csv (an example file is below).\nCollecting BMC MAC Addresses Collecting NCN MAC Addresses The following are sample rows from a ncn_metadata.csv file:\nUse case: NCN with a single PCIe card (1 card with 2 ports): Notice how the MAC address for Bond0 MAC0 and Bond0 MAC1 are only off by 1, which indicates that they are on the same 2 port card.\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s6b0n0,Management,Worker,94:40:c9:37:77:b8,14:02:ec:da:bb:00,14:02:ec:da:bb:00,14:02:ec:da:bb:01 Use case: NCN with a dual PCIe cards (2 cards with 2 ports each for 4 ports total): Notice how the MAC address for Bond0 MAC0 and Bond0 MAC1 have a difference greater than 1, which indicates that they are on not on the same 2 port same card.\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Storage,94:40:c9:37:77:26,14:02:ec:d9:76:88,14:02:ec:d9:76:88,94:40:c9:5f:b6:92 Example ncn_metadata.csv file for a system that has been configured as follows:\nNCNs are configured to boot over the PCIe NICs Master and Storage nodes have two 2 port PCIe cards Worker nodes have one 2 port PCIe card Since the NCN have been configured to boot over their PCIe NICs the values for the columns Bootstrap MAC and Bond0 MAC0 have the same value.\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Storage,94:40:c9:37:77:26,14:02:ec:d9:76:88,14:02:ec:d9:76:88,94:40:c9:5f:b6:92 x3000c0s8b0n0,Management,Storage,94:40:c9:37:87:5a,14:02:ec:d9:7b:c8,14:02:ec:d9:7b:c8,94:40:c9:5f:b6:5c x3000c0s7b0n0,Management,Storage,94:40:c9:37:0a:2a,14:02:ec:d9:7c:88,14:02:ec:d9:7c:88,94:40:c9:5f:9a:a8 x3000c0s6b0n0,Management,Worker,94:40:c9:37:77:b8,14:02:ec:da:bb:00,14:02:ec:da:bb:00,14:02:ec:da:bb:01 x3000c0s5b0n0,Management,Worker,94:40:c9:35:03:06,14:02:ec:d9:76:b8,14:02:ec:d9:76:b8,14:02:ec:d9:76:b9 x3000c0s4b0n0,Management,Worker,94:40:c9:37:67:60,14:02:ec:d9:7c:40,14:02:ec:d9:7c:40,14:02:ec:d9:7c:41 x3000c0s3b0n0,Management,Master,94:40:c9:37:04:84,14:02:ec:d9:79:e8,14:02:ec:d9:79:e8,94:40:c9:5f:b5:cc x3000c0s2b0n0,Management,Master,94:40:c9:37:f9:b4,14:02:ec:da:b8:18,14:02:ec:da:b8:18,94:40:c9:5f:a3:a8 x3000c0s1b0n0,Management,Master,94:40:c9:37:87:32,14:02:ec:da:b9:98,14:02:ec:da:b9:98,14:02:ec:da:b9:99 switch_metadata.csv This file denotes your network topology devices, see Switch Metadata for directions about creating this file.\nuse case: 2 leaf switches and 2 spine switches\npit# cat example_switch_metadata.csv Switch Xname,Type,Brand x3000c0w38,Leaf,Dell x3000c0w36,Leaf,Dell x3000c0h33s1,Spine,Mellanox x3000c0h33s2,Spine,Mellanox use case: 2 CDU switches, 2 leaf switches, and 2 spines switches\npit# cat example_switch_metadata.csv Switch Xname,Type,Brand d0w1,CDU,Dell d0w2,CDU,Dell x3000c0w38,Leaf,Dell x3000c0w36,Leaf,Dell x3000c0h33s1,Spine,Mellanox x3000c0h34s1,Spine,Mellanox use case: 2 CDU Switches, 2 leaf switches, 4 aggregation switches, and 2 spine switches\npit# cat example_switch_metadata.csv Switch Xname,Type,Brand d0w1,CDU,Aruba d0w2,CDU,Aruba x3000c0w31,Leaf,Aruba x3000c0w32,Leaf,Aruba x3000c0h33s1,Aggregation,Aruba x3000c0h34s1,Aggregation,Aruba x3000c0h35s1,Aggregation,Aruba x3000c0h36s1,Aggregation,Aruba x3000c0h37s1,Spine,Aruba x3000c0h38s1,Spine,Aruba hmn_connections.json This file denotes your BMC interfaces and other hardware network topology devices, see HMN Connections for instructions creating this file.\n"
},
{
	"uri": "/docs-csm/en-09/301-ncn-metadata-bmc/",
	"title": "Collecting the BMC MAC Addresses",
	"tags": [],
	"description": "",
	"content": "Collecting the BMC MAC Addresses This guide will detail how to collect BMC MAC Addresses from a Shasta system with configured switches. The BMC MAC Address is the exclusive, dedicated LAN for the onboard BMC.\nIf you are here with an unconfigured switch, mileage may vary.\nRequirements Configured switch with SSH access or unconfigured with COM access (serial-over-lan/DB-9) Another file to record the collected BMC information. Procedure Establish an SSH or serial connection to the leaf switch.\nNote: These IPs are examples; 10.X.0.4 may or may not match your setup.\n# SSH over METAL MANAGEMENT pit# ssh admin@10.1.0.4 # SSH over NODE MANAGEMENT pit# ssh admin@10.252.0.4 # SSH over HARDWARE MANAGEMENT pit# ssh admin@10.254.0.4 # or.. serial (device name will vary). pit# minicom -b 115200 -D /dev/tty.USB1 If you know the ports of your BMCs, you can print out the MAC for those ports -or- if they exist on the same VLAN you should be able to dump the VLAN.\nSyntax is for Onyx and Dell EMC devices - please resort to your CLI usage (press ? or tab to assist on-the-fly).\nIf you know the VLAN ID:\n# DellOS 10 sw-leaf-001# show mac address-table vlan 4 | except 1/1/52 VlanId\tMac Address\tType\tInterface 4\t00:1e:67:98:fe:2c\tdynamic\tethernet1/1/11 4\ta4:bf:01:38:f0:b1\tdynamic\tethernet1/1/27 4\ta4:bf:01:38:f1:44\tdynamic\tethernet1/1/25 4\ta4:bf:01:48:1e:ac\tdynamic\tethernet1/1/28 4\ta4:bf:01:48:1f:70\tdynamic\tethernet1/1/31 4\ta4:bf:01:48:1f:e0\tdynamic\tethernet1/1/26 4\ta4:bf:01:48:20:03\tdynamic\tethernet1/1/30 4\ta4:bf:01:48:20:57\tdynamic\tethernet1/1/29 4\ta4:bf:01:4d:d9:9a\tdynamic\tethernet1/1/32 If you know the interface and trunk:\n# DellOS 10 sw-leaf-001# show mac address-table interface ethernet 1/1/32 VlanId\tMac Address\tType\tInterface 4\ta4:bf:01:4d:d9:9a\tdynamic\tethernet1/1/32 Print everything:\n# DellOS 10 sw-leaf-001# show mac address-table VlanId\tMac Address\tType\tInterface 4\ta4:bf:01:4d:d9:9a\tdynamic\tethernet1/1/32 .... # Onyx and Aruba sw-leaf-001# show mac-address-table In the output from the previous \u0026ldquo;show mac address-table\u0026rdquo; command, information will be available for all management NCNs which do not have an external connection for their BMC, such as ncn-m001. The information from these nodes is also needed in ncn_metadata.csv, but will have to be collected via another method, such as the \u0026ldquo;ipmitool lan print\u0026rdquo; command. All of the management NCNs should be present in the ncn_metadata.csv file.\nTip: Mind the index (3, 2, 1\u0026hellip;. ; not 1, 2, 3)\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Storage,94:40:c9:37:77:26,94:40:c9:5f:b5:de,94:40:c9:5f:b5:de,14:02:ec:da:b9:98 ^^^^^^^^^^^^^^^^^ The column heading must match that shown above for csi to correctly parse it. You can take the file you have started and move onto NCN Metadata BondX.\n"
},
{
	"uri": "/docs-csm/en-09/302-ncn-metadata-bondx/",
	"title": "Collecting NCN MAC Addresses",
	"tags": [],
	"description": "",
	"content": "Collecting NCN MAC Addresses This procedure will detail how to collect the NCN MAC addresses from a Shasta system. After completing this procedure, you will have the MAC addresses needed for the Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1 columns in ncn_metadata.csv.\nThe Bootstrap Mac address will be used for identification of this node during the early part of the PXE boot process before the bonded interface can be established. The Bond0 MAC0 and Bond0 MAC1 are the MAC addresses for the physical interfaces that your node will use for the various VLANs. The Bond0 MAC0 and Bond0 MAC1 should be on the different network cards to establish redundancy for a failed network card. On the other hand, if the node has only a single network card, then MAC1 and MAC0 will still produce a valid configuration if they do reside on the same physical card.\nSections Procedure: iPXE Requirements MAC Collection Procedure: Serial consoles Procedure: Recovering from an incorrect NCN metadata file The easy way to do this leverages the NIC-dump provided by the metal-ipxe package. This page will walk-through booting NCNs and collecting their MACs from the conman console logs.\nThe alternative is to use serial cables (or SSH) to collect the MACs from the switch ARP tables, this can become exponentially difficult for large systems. If this is the only way, please proceed to the bottom of this page.\nProcedure: iPXE Consoles This procedure is faster for those with the LiveCD (CRAY Pre-Install Toolkit) it can be used to quickly boot-check nodes to dump network device information without an OS. This works by accessing the PCI Configuration Space.\nRequirements If CSI does not work due to requiring a file, please file a bug. By default, dnsmasq and conman are already running on the LiveCD but bond0 needs to be configured, dnsmasq needs to serve/listen over bond0, and conman needs the BMC information.\nLiveCD dnsmasq is configured for the bond0/metal network (NMN/HMN/CAN do not matter) BMC MAC addresses already collected LiveCD conman is configured for each BMC (conman -q to see consoles) For help with either of those, see LiveCD Setup.\nMAC Collection (optional) shim the boot so nodes bail after dumping their netdevs. Removing the iPXE script will prevent network booting but beware of disk-boots. will prevent the nodes from continuing to boot and end in undesired states.\npit# mv /var/www/boot/script.ipxe /var/www/boot/script.ipxe.bak Verify consoles are active with conman -q,\npit# conman -q ncn-m002-mgmt ncn-m003-mgmt ncn-s001-mgmt ncn-s002-mgmt ncn-s003-mgmt ncn-w001-mgmt ncn-w002-mgmt ncn-w003-mgmt Now set the nodes to PXE boot and (re)start them.\npit# export username=root pit# export IPMI_PASSWORD= pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | xargs -t -i ipmitool -I lanplus -U $username -E -H {} chassis bootdev pxe options=efiboot,persistent pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | xargs -t -i ipmitool -I lanplus -U $username -E -H {} power off pit# sleep 10 pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | xargs -t -i ipmitool -I lanplus -U $username -E -H {} power on Now wait for the nodes to netboot. You can follow them with conman -j ncn-*id*-mgmt (use conman -q to see ). This takes less than 3 minutes, speed depends on how quickly your nodes POST.\nPrint off what has been found in the console logs, this snippet will omit duplicates from multiple boot attempts:\npit# for file in /var/log/conman/*; do echo $file grep -Eoh \u0026#39;(net[0-9] MAC .*)\u0026#39; $file | sort -u | grep PCI \u0026amp;\u0026amp; echo ----- done From the output you must fish out 2 MACs to use for bond0, and 2 more to use for bond1 based on your topology. The Bond0MAC0 must be the first port of the first PCIe card, specifically the port connecting the NCN to the lower spine (for example, if connected to spines01 and 02, this is going to sw-spine-001 - if connected to sw-spine-007 and sw-spine-008, then this is sw-spine-007). The 2nd MAC for bond0 is the first port of the 2nd PCIe card, or 2nd port of the first when only one card exists.\nExamine the output, you can use the table provided on NCN Networking for referencing commonly seen devices. Note that worker nodes also have the high-speed network cards. If you know these cards, you can filter their device IDs out from the above output using this snippet: pit# unset did # clear it if you used it. pit# did=1017 # ConnectX-5 example. pit# for file in /var/log/conman/*; do echo $file grep -Eoh \u0026#39;(net[0-9] MAC .*)\u0026#39; $file | sort -u | grep PCI | grep -Ev \u0026#34;$did\u0026#34; \u0026amp;\u0026amp; echo ----- done Note to filter out onboard NICs, or site-link cards, you can omit their device IDs as well. Use the above snippet but add the other IDs: this snippet prints out only mgmt MACs, the did is the HSN and onboard NICs that is being ignored. pit# unset did # clear it if you used it. pit# did=\u0026#39;(1017|8086|ffff)\u0026#39; pit# for file in /var/log/conman/*; do echo $file grep -Eoh \u0026#39;(net[0-9] MAC .*)\u0026#39; $file | sort -u | grep PCI | grep -Ev \u0026#34;$did\u0026#34; \u0026amp;\u0026amp; echo ----- done Examine the output from grep, use the lowest value MAC address per PCIe card.\nexample: 1 PCIe card with 2 ports for a total of 2 ports per node.\\\n----- /var/log/conman/console.ncn-w003-mt net2 MAC b8:59:9f:d9:9e:2c PCI.DeviceID 1013 PCI.VendorID 15b3 \u0026lt;-bond0-mac0 (0x2c \u0026lt; 0x2d) net3 MAC b8:59:9f:d9:9e:2d PCI.DeviceID 1013 PCI.VendorID 15b3 \u0026lt;-bond0-mac1 ----- example: 2 PCIe cards with 2 ports each for a total of 4 ports per node.\n----- /var/log/conman/console.ncn-w006-mgmt net0 MAC 94:40:c9:5f:b5:df PCI.DeviceID 8070 PCI.VendorID 1077 \u0026lt;-bond0-mac0 (0x38 \u0026lt; 0x39) net1 MAC 94:40:c9:5f:b5:e0 PCI.DeviceID 8070 PCI.VendorID 1077 (future use) net2 MAC 14:02:ec:da:b9:98 PCI.DeviceID 8070 PCI.VendorID 1077 \u0026lt;-bond0-mac1 (0x61f0 \u0026lt; 0x7104) net3 MAC 14:02:ec:da:b9:99 PCI.DeviceID 8070 PCI.VendorID 1077 (future use) ----- The above output identified MAC0 and MAC1 of the bond as 14:02:ec:df:9c:38 and 94:40:c9:c1:61:f0 respectively.\nTip: Mind the index (3, 2, 1\u0026hellip;. ; not 1, 2, 3)\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Worker,94:40:c9:37:77:26,14:02:ec:df:9c:38,14:02:ec:df:9c:38,94:40:c9:c1:61:f0 ^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^ Procedure: Serial Consoles For this, you will need to double-back to NCN Metadata BMC and pick out the MACs for your BOND from each the sw-spine-001 and sw-spine-002 switch.\nTip: A PCIe card with dual-heads may go to either spine switch, meaning MAC0 ought to be collected from spine-01. Please refer to your cabling diagram, or actual rack (in-person).\nFollow \u0026ldquo;Metadata BMC\u0026rdquo; on each spine switch that port1 and port2 of the bond is plugged into. Usually the 2nd/3rd/4th/Nth MAC on the PCIe card will be a 0x1 or 0x2 deviation from the first port. If you confirm this, then collection is quicker. Procedure: Recovering from an incorrect NCN metadata file If you are coming from 1.3 and happened to use the ncn_metadata.csv without modification, you will be unable to deploy the NCNs. This section details a recovery procedure in case that happens.\nRemove the incorrectly-generated configs. Before deleting the incorrectly-generated configs consider making a backup of them. In case they need to be examined at a later time. WARNING Ensure that the SYSTEM_NAME environment variable is correctly set. If SYSTEM_NAME is not set the command below could potentially remove the entire prep directory.\npit# export SYSTEM_NAME=eniac pit# rm -rf /var/www/ephemeral/prep/$SYSTEM_NAME Manually edit ncn_metadata.csv, replacing the bootstrap MAC address with Bond0Mac0 address for the afflicted nodes that failed to boot\nRe-run csi config init with the required flags\nCopy all the newly-generated files into place\npit# \\ cp -p /var/www/ephemeral/prep/$SYSTEM_NAME/dnsmasq.d/* /etc/dnsmasq.d/* cp -p /var/www/ephemeral/prep/$SYSTEM_NAME/basecamp/* /var/www/ephemeral/configs/ cp -p /var/www/ephemeral/prep/$SYSTEM_NAME/conman.conf /etc/ cp -p /var/www/ephemeral/prep/$SYSTEM_NAME/pit-files/* /etc/sysconfig/network/ Update CA Cert on the copied data.json file. Provide the path to the data.json, the path to our customizations.yaml, and finally the sealed_secrets.key pit# csi patch ca \\ --cloud-init-seed-file /var/www/ephemeral/configs/data.json \\ --customizations-file /var/www/ephemeral/prep/site-init/customizations.yaml \\ --sealed-secret-key-file /var/www/ephemeral/prep/site-init/certs/sealed_secrets.key Now restart everything to apply the new configs: pit# \\ wicked ifreload all systemctl restart dnsmasq conman basecamp systemctl restart nexus Apply any NCN Pre-Boot Workarounds. Check for workarounds in the /opt/cray/csm/workarounds/before-ncn-boot directory. If there are any workarounds in that directory, run those now. Each has its own instructions in their respective README.md files. pit# ls /opt/cray/csm/workarounds/before-ncn-boot If there is a workaround here, the output looks similar to the following:\nCASMINST-980 Before relaunching NCNs, be sure to wipe the disks first. To wipe all disks in all NCNs, refer to the disk wipe procedure in the Degraded System Notice section of 002-CSM-INSTALL.md. "
},
{
	"uri": "/docs-csm/en-09/303-ncn-metadata-usb-serial/",
	"title": "NCN Metadata over USB-Serial Cable",
	"tags": [],
	"description": "",
	"content": "NCN Metadata over USB-Serial Cable In the event that network plumbing is lacking, down, or unconfigured for procuring devices then it is recommended to use the Serial/COM ports on the spine and leaf switches.\nThis guide will instruct the user on procuring MAC addresses for the NCNs metadata files with the serial console.\nMileage may vary, as some obstacles such as BAUDRATE and terminal usage vary per manufacturer.\nCommon Manufacturers (click any for links to support/document portals)\nAruba Dell Mellanox Setup / Connection This will involve using a preferred utility (minicom, screen, or cu) for connecting to your switches console.\nAt this time it is assumed you have connected your USB-DB-9 or USB-RJ-45 cable between your switch and your NCN.\nscreen screen /dev/ttyUSB1 screen /dev/ttyUSB1 115200 minicom minicom -b 9600 -D /dev/ttyUSB1 minicom -b 115200 -D /dev/ttyUSB1 cu cu -l /dev/ttyUSB1 -s 115200 Debugging Connections Tip : Mellanox On Mellanox switches if the console is not responding when opened try holding CTRL + R (or control + R for macOS) to initiate a screen refresh. This should take 5-10 seconds.\nTip : No USB TTY Device If you cannot see your device in /dev/tty* then follow dmesg -w and try reseating your USB cable (unplug the end in the NCN, and plug it back in). Observe the dmesg -w output, does it show errors pertaining to USB? The cable may be bad, or you may need to reboot.\nAdditional External References USB-B to RJ-45 rs232 Cable USB-B to USB-C adapter "
},
{
	"uri": "/docs-csm/en-09/304-ncn-pcie-net-boot-and-re-cable/",
	"title": "Guide  Netboot an NCN from a Spine",
	"tags": [],
	"description": "",
	"content": "Guide : Netboot an NCN from a Spine This page details how to migrate NCNs from depending on their onboard NICs for PXE booting, and booting over the spine switches.\nEnabling UEFI PXE Mode Mellanox Print current UEFI and SR-IOV State Setting Expected Values High-Speed Network Obtaining Mellanox Tools QLogic FastLinq Kernel Modules Disabling/Removing On-Board Connections This applies to Newer systems (Spring 2020 or newer) where onboard NICs are still used.\nThis presents a need for migration for systems still using the legacy, preview topology. Specifically, systems with onboard connections to their leaf switches and NCNs need to disable/remove that connection.\nThis onboard NCN port came from before spine-switches were added to the shasta-network topology. The onboard connection was responsible for every network (MTL/NMN/HMN/CAN) and was the sole driver of PXE booting for. Now, NCNs use bond interfaces and spine switches for those networks, however some older systems still have this legacy connection to their leaf switches and solely use it for PXE booting. This NIC is not used during runtime, and NCNs in this state should enable PXE within their PCIe devices\u0026rsquo; OpROMs and disable/remove this onboard connection.\nEnabling UEFI PXE Mode Mellanox This uses the Mellanox CLI Tools for configuring UEFI PXE from the Linux command line.\nOn any NCN (using 0.0.10 k8s, or 0.0.8 Ceph anything built on ncn-0.0.21 or higher) can run this to begin interacting with Mellanox cards: If you are recovering NCNs with an earlier image without the mellanox tools, please refer to the section on the bottom of the Mellanox this segment.\nncn# mst start Now mst status and other commands like mlxfwmanager or mlxconfig will work, and devices required for these commands will be created in /dev/mst.\nPrint current UEFI and SR-IOV State UEFI: all boots are UEFI, this needs to be enabled for access to the UEFI OpROM for configuration and for usage of UEFI firmwares. SR_IOV: This is currently DISABLED because it can attribute to longer POSTs on HPE blades (Gen10+, i.e. DL325 or DL385) with Mellanox ConnectX-5 PCIe cards. The technology is not yet enabled for virtualization usage, but may be in the future.\nUse this snippet to print out device name and current UEFI PXE state.\nncn# mst status for MST in $(ls /dev/mst/*); do mlxconfig -d ${MST} q | egrep \u0026#34;(Device|EXP_ROM|SRIOV_EN)\u0026#34; done Setting Expected Values Use this snippet to enable and dump UEFI PXE state.\nfor MST in $(ls /dev/mst/*); do echo ${MST} mlxconfig -d ${MST} -y set EXP_ROM_UEFI_x86_ENABLE=1 mlxconfig -d ${MST} -y set EXP_ROM_PXE_ENABLE=1 mlxconfig -d ${MST} -y set SRIOV_EN=0 mlxconfig -d ${MST} q | egrep \u0026#34;EXP_ROM\u0026#34; done High-Speed Network For worker nodes with High-Speed network attachments, the PXE and SR-IOV features should be disabled.\nRun mlxfwmanager to probe and dump your Mellanox PCIe cards\nncn# mlxfwmanager Find the device path for the HSN card, assuming it is a ConnectX-5 or other 100GB card this should be easy to pick out.\nRun this, swapping the MST variable for your actual card path\n# Set UEFI to YES ncn# MST=/dev/mst/mt4119_pciconf1 ncn# mlxconfig -d ${MST} -y set EXP_ROM_UEFI_ARM_ENABLE=0 ncn# mlxconfig -d ${MST} -y set EXP_ROM_UEFI_x86_ENABLE=0 ncn# mlxconfig -d ${MST} -y set EXP_ROM_PXE_ENABLE=0 ncn# mlxconfig -d ${MST} -y set SRIOV_EN=0 ncn# mlxconfig -d ${MST} q | egrep \u0026#34;EXP_ROM\u0026#34; Your Mellanox HSN card is now neutralized, and will only be usable in a booted system.\nObtaining Mellanox Tools mft is installed in 1.4 NCN images, for 1.3 systems they will need to obtain the tools by hand:\nlinux# wget https://www.mellanox.com/downloads/MFT/mft-4.15.1-9-x86_64-rpm.tgz linux# tar -xzvf mft-4.15.1-9-x86_64-rpm.tgz linux# cd mft-4.15.1-9-x86_64-rpm/RPMS linux# rpm -ivh ./mft-4.15.1-9.x86_64.rpm linux# cd linux# mst start QLogic FastLinq These should already be configured for PXE booting.\nKernel Modules KMP modules for Qlogic are installed:\nqlgc-fastlinq-kmp-default qlgc-qla2xxx-kmp-default See [#casm-triage][2] if this is not the case.\nDisabling or Removing On-Board Connections The onboard connection can be disabled a few ways, short of removing the physical connection one may shutdown the switchport as well.\nIf you can remove the physical connection, this is preferred and can be done so after enabling PXE on the PCIe cards.\nIf you want to disable the connection, you will need to login to your respective leaf switch.\nConnect to the leaf switch using serial or SSH connections. Select one of the connection options below. The IP addresses and device names may vary in the commands below. ```bash # SSH over METAL MANAGEMENT pit# ssh admin@10.1.0.4 # SSH over NODE MANAGEMENT pit# ssh admin@10.252.0.4 # SSH over HARDWARE MANAGEMENT pit# ssh admin@10.254.0.4\n# or.. serial (device name will vary). pit# minicom -b 115200 -D /dev/tty.USB1 ``` Enter configuration mode sw-leaf-001\u0026gt; configure terminal sw-leaf-001(config)#\u0026gt; Disable the NCN interfaces. Check your SHCD for reference before continuing so that the interfaces which are connected to management NCNs are being changes. Ports 2 to 10 are commonly the master, worker, and storage nodes when there are 3 of each. Some systems may have more worker nodes or utility storage nodes, or may be racked and cabled differently. sw-leaf-001(config)#\u0026gt; interface range 1/1/2-1/1/10 sw-leaf-001(config)#\u0026gt; shutdown sw-leaf-001(config)#\u0026gt; write memory You can enable them again at anytime by switching the shutdown command out for no shutdown.\n"
},
{
	"uri": "/docs-csm/en-09/305-switch-metadata/",
	"title": "Switch Metadata",
	"tags": [],
	"description": "",
	"content": "Switch Metadata This page provides directions on constructing the switch_metadata.csv file.\nThis file is manually created to include information about all spine, leaf, CDU, and aggregation switches in the system. The file follows this format in ascending order for the switches of each type:\nSwitch Xname,Type,Brand d0w1,CDU,Dell d0w2,CDU,Dell x3000c0w38,Leaf,Dell x3000c0w36,Leaf,Dell x3000c0h33s1,Spine,Mellanox x3000c0h34s1,Spine,Mellanox The above file would lead to this pairing between component name and hostname:\nhostname component name sw-spine-001 x3000c0h33s1 sw-spine-002 x3000c0h34s1 sw-leaf-001 x3000c0w38 sw-leaf-002 x3000c0w36 sw-cdu-001 d0w1 sw-cdu-002 d0w2 Requirements For this you will need:\nThe SHCD for your system It is worthwhile to review the topic about component names from the HPE Cray EX Hardware Management Administration Guide 1.4 S-8015 while mapping names between the SHCD and your switch_metadata.csv file.\nINTERNAL USE HSS Naming Convention\nFormat Spine and aggregation switches use the format xXcChHsS. Leaf switches use xXcCwW. CDU switches use dDwW.\nDirections In your SHCD, identify your switches. Look for: The slot number(s) for the leaf switches (usually 48-port switches) In the below example this is x3000u22 The slot number(s) for the spine switches In the below example this is x3000u23R and x3000u23L (two side-by-side switches) Newer side-by-side switches use slot numbers of s1 and s2 instead of R and L Each spine or aggregation switch will follow this format: xXcChHsS This format also applies to CDU switches that are in a river cabinet that make connections to an adjacent hill cabinet.\nxX : where \u0026ldquo;X\u0026rdquo; is the river cabinet identification number (the figure above is \u0026ldquo;3000\u0026rdquo;) cC : where \u0026ldquo;C\u0026rdquo; is the cabinet identification number (the figure above is \u0026ldquo;0\u0026rdquo;) hH : where \u0026ldquo;H\u0026rdquo; is the slot number in the cabinet (height) sS : where \u0026ldquo;S\u0026rdquo; is the horizontal space number' Each leaf switch will follow this format: xXcCwW: xX : where \u0026ldquo;X\u0026rdquo; is the river cabinet identification number (the figure above is \u0026ldquo;3000\u0026rdquo;) cC : where \u0026ldquo;C\u0026rdquo; is the cabinet identification number (the figure above is \u0026ldquo;0\u0026rdquo;) wW : where \u0026ldquo;W\u0026rdquo; is the slot number in the cabinet (height) Each CDU switch will follow this format: dDwW: If a CDU switch is in a river cabinet, then follow the naming convention in step 2 instead.\ndD : where \u0026ldquo;D\u0026rdquo; is the Coolant Distribution Unit (CDU) wW : where \u0026ldquo;W\u0026rdquo; is the management switch in a CDU Each item in the file is either of type Aggregation, CDU, Leaf, or Spine. Each line in the file must denote the Brand, either Dell, Mellanox, or Aruba. Create the switch_metadata.csv file with this information. linux# vi switch_metadata.csv See the example files below for reference.\nExamples An example with Dell leaf switches and 2 Mellanox switches in the same slot number:\npit# cat example_switch_metadata.csv Switch Xname,Type,Brand x3000c0w38,Leaf,Dell x3000c0w36,Leaf,Dell x3000c0h33s1,Spine,Mellanox x3000c0h33s2,Spine,Mellanox An example with Aruba switches:\npit# cat example_switch_metadata.csv Switch Xname,Type,Brand x3000c0w14,Leaf,Aruba x3000c0h12s1,Spine,Aruba x3000c0h13s1,Spine,Aruba An example with Dell leaf and CDU switches and Mellanox spine switches:\nSwitch Xname,Type,Brand d0w1,CDU,Dell d0w2,CDU,Dell x3000c0w36,Leaf,Dell x3000c0w38,Leaf,Dell x3000c0h33s1,Spine,Mellanox x3000c0h34s1,Spine,Mellanox "
},
{
	"uri": "/docs-csm/en-09/306-sls-add-uan-alias/",
	"title": "Manually add UAN Aliases to SLS",
	"tags": [],
	"description": "",
	"content": "Manually add UAN Aliases to SLS Prerequisites SLS is up and running and has been populated with data. Access to the API gateway api-gw-service (legacy: api-gw-service-nmn.local) About this task This guide shows the process for manually adding aliases to UAN nodes in SLS. Steps 3 and 4 of this guide can be repeated for each UAN alias that needs to be added in SLS. This guide is intended to be run on any k8s node that has access to the API gateway api-gw-service (current/legacy: api-gw-service-nmn.local).\nProcedure Authenticate with Keycloak to obtain an API token:\nexport TOKEN=$(curl -k -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Find the xname of the UAN by searching through all Application nodes until found. bash curl -s -k -H \u0026quot;Authorization: Bearer ${TOKEN}\u0026quot; \u0026quot;https://api_gw_service.local/apis/sls/v1/search/hardware?extra_properties.Role=Application\u0026quot; | jq This will return an array of application nodes currently known in SLS: ```json ncn-w001# curl -s -k -H \u0026quot;Authorization: Bearer ${TOKEN}\u0026quot; \u0026quot;https://api_gw_service.local/apis/sls/v1/search/hardware?extra_properties.Role=Application\u0026quot; | jq [ { \u0026quot;Parent\u0026quot;: \u0026quot;x3000c0s19b0\u0026quot;, \u0026quot;Xname\u0026quot;: \u0026quot;x3000c0s19b0n0\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;comptype_node\u0026quot;, \u0026quot;Class\u0026quot;: \u0026quot;River\u0026quot;, \u0026quot;TypeString\u0026quot;: \u0026quot;Node\u0026quot;, \u0026quot;LastUpdated\u0026quot;: 1606332877, \u0026quot;LastUpdatedTime\u0026quot;: \u0026quot;2020-11-25 19:34:37.183293 +0000 +0000\u0026quot;, \u0026quot;ExtraProperties\u0026quot;: { \u0026quot;Role\u0026quot;: \u0026quot;Application\u0026quot;, \u0026quot;SubRole\u0026quot;: \u0026quot;UAN\u0026quot; } } ] ``` Update the UAN object in SLS by adding Aliases array with the UAN\u0026rsquo;s hostname.\ncurl -X PUT -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \u0026#34;https://api_gw_service.local/apis/sls/v1/hardware/\u0026lt;UAN_XNAME\u0026gt;\u0026#34; -d \u0026#39; { \u0026#34;Parent\u0026#34;: \u0026#34;\u0026lt;UAN_PARENT_XNAME\u0026gt;\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;\u0026lt;UAN_XNAME\u0026gt;\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Role\u0026#34;: \u0026#34;Application\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;UAN\u0026#34;, \u0026#34;Aliases\u0026#34;: [\u0026#34;\u0026lt;UAN_ALIAS\u0026gt;\u0026#34;] } }\u0026#39; Replace \u0026lt;UAN_XNAME\u0026gt; in the URL and JSON object with the UAN\u0026rsquo;s xname. Replace \u0026lt;UAN_PARENT_XNAME\u0026gt; in the JSON object with the UAN\u0026rsquo;s parent xname. Replace \u0026lt;UAN_ALIAS\u0026gt; in the Aliases array with the UAN\u0026rsquo;s hostname. The LastUpdated and LastUpdatedTime fields are not required to be in the PUT payload.\nUsing the response from Step 2, we can build the following command. The hostname for this uan is uan01, and this is reflected in the added Aliases field in the UAN\u0026rsquo;s ExtraProperties.\ncurl -X PUT -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \u0026#34;https://api_gw_service.local/apis/sls/v1/hardware/x3000c0s19b0n0\u0026#34; -d \u0026#39; { \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s19b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s19b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Role\u0026#34;: \u0026#34;Application\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;UAN\u0026#34;, \u0026#34;Aliases\u0026#34;: [\u0026#34;uan01\u0026#34;] } }\u0026#39; Example response:\nncn-w001# curl -X PUT -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \u0026#34;https://api_gw_service.local/apis/sls/v1/hardware/x3000c0s19b0n0\u0026#34; -d \u0026#39; \u0026gt; { \u0026gt; \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s19b0\u0026#34;, \u0026gt; \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s19b0n0\u0026#34;, \u0026gt; \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026gt; \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026gt; \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026gt; \u0026#34;ExtraProperties\u0026#34;: { \u0026gt; \u0026#34;Role\u0026#34;: \u0026#34;Application\u0026#34;, \u0026gt; \u0026#34;SubRole\u0026#34;: \u0026#34;UAN\u0026#34;, \u0026gt; \u0026#34;Aliases\u0026#34;: [\u0026#34;uan01\u0026#34;] \u0026gt; } \u0026gt; }\u0026#39; {\u0026#34;Parent\u0026#34;:\u0026#34;x3000c0s19b0\u0026#34;,\u0026#34;Xname\u0026#34;:\u0026#34;x3000c0s19b0n0\u0026#34;,\u0026#34;Type\u0026#34;:\u0026#34;comptype_node\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;River\u0026#34;,\u0026#34;TypeString\u0026#34;:\u0026#34;Node\u0026#34;,\u0026#34;LastUpdated\u0026#34;:1606332877,\u0026#34;LastUpdatedTime\u0026#34;:\u0026#34;2020-11-25 19:34:37.183293 +0000 +0000\u0026#34;,\u0026#34;ExtraProperties\u0026#34;:{\u0026#34;Aliases\u0026#34;:[\u0026#34;uan01\u0026#34;],\u0026#34;Role\u0026#34;:\u0026#34;Application\u0026#34;,\u0026#34;SubRole\u0026#34;:\u0026#34;UAN\u0026#34;}} After a few minutes the -mgmt name should begin resolving. Communication with the BMC should be available via the alias uan01-mgmt.\nConfirm that the BMC for the UAN is up and running at the aliased address.\nncn-w001# ping -c 4 uan01-mgmt PING uan01-mgmt (10.254.2.53) 56(84) bytes of data. 64 bytes from x3000c0s19b0 (10.254.2.53): icmp_seq=1 ttl=255 time=0.170 ms 64 bytes from x3000c0s19b0 (10.254.2.53): icmp_seq=2 ttl=255 time=0.228 ms 64 bytes from x3000c0s19b0 (10.254.2.53): icmp_seq=3 ttl=255 time=0.311 ms 64 bytes from x3000c0s19b0 (10.254.2.53): icmp_seq=4 ttl=255 time=0.240 ms --- uan01-mgmt ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3061ms rtt min/avg/max/mdev = 0.170/0.237/0.311/0.051 ms When this node boots, the DHCP request of its -nmn interface will cause the uan01 to be created and resolved.\nConfirm that the UAN is being monitored by the cray-conman service\nUse kubectl to exec into the running cray-conman pod.\nIdentify the cray-conman pod:\nncn# kubectl get pods -n services | grep \u0026#34;^cray-conman-\u0026#34; Expected output looks similar to the following:\ncray-conman-b69748645-qtfxj 3/3 Running 0 16m Set the PODNAME variable accordingly:\nncn# export PODNAME=cray-conman-b69748645-qtfxj Log into the cray-conman container in this pod:\nncn# kubectl exec -n services -it $PODNAME -c cray-conman -- bash cray-conman# Check the existing connections.\ncray-conman# conman -q | grep x3000c0s19b0 cray-conman# If the node is not being reported as connected to conman, the conman service will need to be re-initialized. This is done by killing the existing conmand process.\ncray-conman# ps -ax | grep conmand 13 ? Sl 0:45 conmand -F -v -c /etc/conman.conf 56704 pts/3 S+ 0:00 grep conmand cray-conman# kill 13 If the UAN has been successfully discovered by hsm, it should now be monitored by conman.\ncray-conman# conman -q | grep x3000c0s19b0 x3000c0s19b0 cray-conman# "
},
{
	"uri": "/docs-csm/en-09/307-hmn-connections/",
	"title": "HMN Connections File",
	"tags": [],
	"description": "",
	"content": "HMN Connections File About this task This guide shows the process for generating the hmn_connections.json from the system\u0026rsquo;s SHCD Excel document. This process is typically needed when generating the hmn_connections.json file for a new system, or regenerating it when system\u0026rsquo;s SHCD is changed (specifically the HMN tab). The hms-shcd-parser tool can be used to generate the hmn_connections.json file.\nEach system has its own directory in the repository. If this is a new system that does not yet have the hmn_connections.json file (or needs to be regenerated), then one will need to be generated from the SHCD (Cabling Diagram) for the system.\nIf you need to fetch the system\u0026rsquo;s SHCD, you can use your HPE login to fetch it from SharePoint. May need to request permission to access this SharePoint folder.\nPrerequisites SHCD Excel file for your system Podman or Docker running Note: Docker can be used instead of Podman if the system being used to prepare this file does not have Podman available. Podman is available on the CSM LiveCD, and is installed onto a NCN running Shasta v1.3 during the procedure to create the CSM USB LiveCD.\nProcedure If using Docker: Make sure that the docker service is running:\nlinux# systemctl status docker ● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled) Active: inactive (dead) Documentation: http://docs.docker.com If the service is not running start it:\nlinux# systemctl start docker Load the hms-shcd-parser docker image from the CSM release distribution. Only required if the CSM release distribution includes container images, otherwise this step can be skipped.\nINTERNAL USE When the version of the hms-shcd-parser image changes in the CSM release distribution the following section will need to be updated with the corrected tag\nNote: The load-container-image.sh script works with both Podman and Docker\nLoad the hms-shcd-parser docker image. This script will load the image into either Podman or Docker.\nlinux# ${CSM_RELEASE}/hack/load-container-image.sh dtr.dev.cray.com/cray/hms-shcd-parser:1.1.1 Set environment to point to the system\u0026rsquo;s SHCD Excel file:\nNote: Make sure to quote the SHCD file path if there are spaces in the document\u0026rsquo;s filename.\nlinux# export SHCD_FILE=\u0026#34;/path/to/systems/SHCD.xlsx\u0026#34; Generate the hmn_connections.json file from the SHCD. This will either create or overwrite the hmn_connections.json file in the current directory:\nINTERNAL USE When the version of the hms-shcd-parser image changes in the CSM release distribution the following section will need to be updated with the corrected tag\nIf using Podman:\nlinux# podman run --rm -it --name hms-shcd-parser -v \u0026#34;$(realpath \u0026#34;$SHCD_FILE\u0026#34;)\u0026#34;:/input/shcd_file.xlsx -v \u0026#34;$(pwd)\u0026#34;:/output dtr.dev.cray.com/cray/hms-shcd-parser:1.1.1 If using Docker:\nlinux# docker run --rm -it --name hms-shcd-parser -v \u0026#34;$(realpath \u0026#34;$SHCD_FILE\u0026#34;)\u0026#34;:/input/shcd_file.xlsx -v \u0026#34;$(pwd)\u0026#34;:/output dtr.dev.cray.com/cray/hms-shcd-parser:1.1.1 "
},
{
	"uri": "/docs-csm/en-09/308-application-node-config/",
	"title": "Application Node Config",
	"tags": [],
	"description": "",
	"content": "Application Node Config This page provides directions on constructing the application_node_config.yaml file. This file controls how the csi config init command finds and treats applications nodes discovered the hmn_connections.json file when building the SLS Input file.\nThe following hmn_connections.json file contains 4 application nodes. When the csi config init command is used without a application_node_config.yaml file, only the application node uan01 will be included the generated SLS input file. The other 3 application nodes will be ignored as they have unknown prefixes and will not be present in the SLS Input file.\n[ {\u0026#34;Source\u0026#34;:\u0026#34;uan01\u0026#34;, \u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;, \u0026#34;SourceLocation\u0026#34;:\u0026#34;u23\u0026#34;, \u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;, \u0026#34;DestinationLocation\u0026#34;:\u0026#34;u13\u0026#34;, \u0026#34;DestinationPort\u0026#34;:\u0026#34;j37\u0026#34;}, {\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;, \u0026#34;SourceRack\u0026#34;:\u0026#34;x3113\u0026#34;, \u0026#34;SourceLocation\u0026#34;:\u0026#34;u23\u0026#34;, \u0026#34;DestinationRack\u0026#34;:\u0026#34;x3113\u0026#34;, \u0026#34;DestinationLocation\u0026#34;:\u0026#34;u13\u0026#34;, \u0026#34;DestinationPort\u0026#34;:\u0026#34;j37\u0026#34;}, {\u0026#34;Source\u0026#34;:\u0026#34;vn02\u0026#34;, \u0026#34;SourceRack\u0026#34;:\u0026#34;x3114\u0026#34;, \u0026#34;SourceLocation\u0026#34;:\u0026#34;u23\u0026#34;, \u0026#34;DestinationRack\u0026#34;:\u0026#34;x3114\u0026#34;, \u0026#34;DestinationLocation\u0026#34;:\u0026#34;u13\u0026#34;, \u0026#34;DestinationPort\u0026#34;:\u0026#34;j37\u0026#34;}, {\u0026#34;Source\u0026#34;:\u0026#34;login02\u0026#34;, \u0026#34;SourceRack\u0026#34;:\u0026#34;x3115\u0026#34;, \u0026#34;SourceLocation\u0026#34;:\u0026#34;u23\u0026#34;, \u0026#34;DestinationRack\u0026#34;:\u0026#34;x3115\u0026#34;, \u0026#34;DestinationLocation\u0026#34;:\u0026#34;u13\u0026#34;, \u0026#34;DestinationPort\u0026#34;:\u0026#34;j37\u0026#34;} ] This file is manually created and follows this format. The 3 fields prefixes, prefix_hsm_subroles, and aliases are optional and do not need to be specified if not needed.\n--- # Additional application node prefixes to match on the Source field in the hmn_connections.json file # See step 1 for additional information prefixes: - gateway - vn # Additional HSM SubRole mappings # If a prefix does not have an HSM SubRole defined, the application node will not have a SubRole. # See step 2 for additional information prefix_hsm_subroles: gateway: Gateway vn: Visualization # Application Node aliases # One or more aliases can be specified for an application node # If an application does not have entry in this map, then it will not have any aliases defined in SLS # See step 3 for additional information aliases: x3113c0s23b0n0: [\u0026#34;gateway-01\u0026#34;] x3114c0s23b0n0: [\u0026#34;visualization-02\u0026#34;, \u0026#34;vn-02\u0026#34;] When the above application_node_config.yaml file is used 3 application nodes (uan01, gateway01, and vn02) will included in the generated SLS input file. The login02 application node will be ignored.\nThe following application node configuration does not add any additional prefixes, HSM subroles, or aliases:\n# Additional application node prefixes to match in the hmn_connections.json file prefixes: [] # Additional HSM SubRoles prefix_hsm_subroles: {} # Application Node aliases aliases: {} Requirements For this you will need:\nThe hmn_connections.json file for your system Background What is a source name?\nExample entry from the hmn_connections.json file. The source name is the Source field, and this name of the device that is being connected to the HMN network. From this source name the csi config init command can infer the type of hardware that is connected to the HMN network (Node, PDU, HSN Switch, etc\u0026hellip;).\n{ \u0026#34;Source\u0026#34;: \u0026#34;uan01\u0026#34;, \u0026#34;SourceRack\u0026#34;: \u0026#34;x3000\u0026#34;, \u0026#34;SourceLocation\u0026#34;: \u0026#34;u19\u0026#34;, \u0026#34;DestinationRack\u0026#34;: \u0026#34;x3000\u0026#34;, \u0026#34;DestinationLocation\u0026#34;: \u0026#34;u14\u0026#34;, \u0026#34;DestinationPort\u0026#34;: \u0026#34;j37\u0026#34; } Directions Add additional Application node Prefixes\nThe prefixes field is an array of strings, that augments the list of source name prefixes that are treated as application nodes. By default csi config init only looks for application nodes that have source names that start with uan, gn, and ln. If your system contains application nodes that fall outside of those source name prefixes you will need to add additional prefixes to application_node_config.yaml. These additional prefixes will used in addition to the default prefixes.\nNote: The command csi config init does a case insensitive check for whether a source name contains an application node prefix.\nTo add an additional prefix append a new string element to the prefixes array:\n--- prefixes: # Additional application node prefixes - gateway - vn - login # New prefix. Match source names that start with \u0026#34;login\u0026#34;, such as login02 Add HSM SubRoles for Application node prefixes\nThe prefix_hsm_subroles field mapping application node prefix (string) to the applicable Hardware State Manager (HSM) SubRole (string) for the application nodes. All applications nodes have the HSM Role of Application, and the SubRole value can be used to label what type of the application node it is (such as UAN, Gateway, etc\u0026hellip;).\nBy default, the csi config init command will use the following SubRoles for application nodes:\nPrefix HSM Subrole uan UAN ln UAN gn Gateway To add additional HSM SubRole for a given prefix add a new mapping under the prefix_hsm_subroles field. Where the key is the application node prefix and the value is the HSM SubRole.\n--- prefix_hsm_subroles: gateway: Gateway vn: Visualization login: UAN # Application nodes that have the non-default prefix \u0026#34;login\u0026#34; are assigned the HSM SubRole \u0026#34;UAN\u0026#34; Valid HSM subrole values are: Worker, Master, Storage, UAN, Gateway, LNETRouter, Visualization, and UserDefined.\nAdd Application node aliases The aliases field is an map of xnames (strings) to an array of aliases (strings).\nBy default, the csi config init command does not set the ExtraProperties.Alias field for application nodes in the SLS input file.\nInstead of manually adding the application node alias as described after the system is installed in this procedure the application node aliases can be included when the SLS Input file is built.\nTo add additional application node aliases, add a new mapping under the aliases field. Where the key is the xname of the application node, and the value is an array of aliases (strings) which allows for one or more aliases to be specified for an application node.\n--- aliases: # Application Node alias x3113c0s23b0n0: [\u0026#34;gateway-01\u0026#34;] x3114c0s23b0n0: [\u0026#34;visualization-02\u0026#34;, \u0026#34;vn-02\u0026#34;] x3115c0s23b0n0: [\u0026#34;uan-02\u0026#34;] # Added alias for the application node with the xname x3115c0s23b0n0 "
},
{
	"uri": "/docs-csm/en-09/309-move-site-connections/",
	"title": "Move Site Connections",
	"tags": [],
	"description": "",
	"content": "Move Site Connections In 1.4, the site connections that were previously connection to ncn-w001 will be moved to ncn-m001. This page will go over the process to make that change.\nNote: In Shasta v1.4, any number of master nodes may have external connections, before Shasta v1.4 this was strictly ncn-w001.\nMake request to DCHW to move the BMC/Host Connections and attach a USB storage device to ncn-m001.\nMake sure ncn-w001 is up and accessible via the NMN from ncn-m001.\nAlso request that the SHCD be updated with the following changes.\nSwap mn01 and wn01 cabling. mn01-j1 and mn01-j3 need the external/site-links and both wn01-j1 and wn01-j3 should be wired into the leaf switch.\nPlug a USB stick (250GB or larger) into mn01. If one is already in wn01 please move it to mn01, otherwise new USBs should have been ordered.\nCreate DNS records for ncn-m001 and its BMC.\n\u0026lt;system-name\u0026gt;-ncn-m001 \u0026lt;system-name\u0026gt;-ncn-m001-mgmt The old ncn-w001 DNS entries should remain/stay to prevent interruptions, if at all possible.\nSet the new host IP and default route.\nAfter the above changes have been made, go to the console of ncn-m001 via the new BMC address given by DHCW.\nusername=\u0026#39;\u0026#39; bmcaddr=\u0026#39;\u0026#39; export IPMI_PASSWORD=\u0026#39;\u0026#39; ipmitool -I lanplus -U $username -E -H $bmcaddr sol activate Set the new static em1 IP address in /etc/sysconfig/network/ifcfg-em1. Replace 172.30.XX.XX with the em1 IP for your system.\nBOOTPROTO=\u0026#39;static\u0026#39; STARTMODE=\u0026#39;auto\u0026#39; ONBOOT=\u0026#39;yes\u0026#39; IPADDR=\u0026#39;172.30.XX.XX\u0026#39; NETMASK=\u0026#39;255.255.240.0\u0026#39; Set the default route in /etc/sysconfig/network/ifroute-em1.\ndefault 172.30.48.1 255.255.240.0 em1 Bring up em1.\nwicked ifdown em1 wicked ifup em1 Confirm that the IP address and default were set.\nip addr show em1 ip route | grep default Exit out of the sol console.\nSet ncn-w001 BMC to dhcp\nDCHW may do this step for you. If not, do the following after the above changes have been made\na. SSH from your laptop to ncn-m001 via the new external connection.\nb. SSH from ncn-m001 to ncn-w001 over the NMN.\nc. Execute ipmitool lan print 1 and check to see if IP Address Source is set to Static or DHCP.\nd. If it is set to Static, run the command ipmitool lan set 1 ipsrc dhcp\ne. Execute ipmitool lan print 1 to verify that it is now set to DHCP and record if it has picked up an address. (This will depend on if Kea is still running on the 1.3 system. If it does not have an IP, we can get it later.)\nCapture bond0 MACs and ncn-w001 BMC MAC\nFor 1.4, we need to know the MAC address for the bond0 interface. We also need to know the BMC and em1 MACs for ncn-w001 since that is not capture in ncn_metadata.csv. You can capture all of this this from ncn-w001 by running the following commands.\nansible ncn -m shell -a \u0026#34;ip addr show bond0 | grep ether\u0026#34; \u0026gt; /root/macs.txt ipmitool lan print 1 | grep \u0026#34;MAC Address\u0026#34; \u0026gt;\u0026gt; /root/macs.txt scp this file to ncn-m001.\nscp /root/macs.txt ncn-m001:/root Log out of ncn-w001. You should now be back on ncn-m001.\nShutdown all of the nodes except for ncn-m001\nWe want to make sure all of the NCNs are shutdown before starting the 1.4 installation to avoid having multiple DHCP servers running. Because Kea is also serving the BMCs their addresses, we will want to do this all at approximately the same time before they lose their leases.\nUse ipmitool from ncn-m001 to shutdown all of the NCNs other than ncn-m001 and ncn-w001.\nusername=root export IPMI_PASSWORD= grep -oP $stoken /etc/dnsmasq.d/statics.conf | xargs -i ipmitool -I lanplus -U $username -E -H {} power off If you found an IP in step 3e, use ipmitool from ncn-m001 to power off ncn-w001.\nIf you did not find an IP in step 3e or you cannot access that IP, then SSH to ncn-w001 from ncn-m001 and execute shutdown -h now. Make sure you have moved anything you need from ncn-w001 because we will not have access to bring it back up until we bring up another DHCP server on the LiveCD.\nYou can now go back to LiveCD Creation\n"
},
{
	"uri": "/docs-csm/en-09/310-cabinets/",
	"title": "Cabinets",
	"tags": [],
	"description": "",
	"content": "Cabinets This page provides directions on constructing the optional \u0026ldquo;cabinets.yaml\u0026rdquo; file. This file lists cabinet ids for any systems with non-contiguous cabinet id numbers and controls how the \u0026ldquo;csi config init\u0026rdquo; command treats cabinet ids.\nThe cabinets.yaml file is particularly important for upgrades from Shasta v1.3 systems as it allows the preservation of cabinet names and network VLANs. An audit of the existing system will be required to gather the data needed to populate cabinets.yaml. In the example below the VLANs for cabinets 1000 and 1001 are overridden. This example can be used to preserve existing cabinet VLANs and prevent reconfiguring switches and CMMs. Similar cabinet numbering and preservation of VLANs can be used for Hill and River cabinets.\nUse for original cabinet data for v1.3 systems use SLS Dump data collected from the system. An excerpt of the data is shown below. Cabinet names and VLANs for comptype_cabinet should be collected from SLS data and used to populate cabinets.yaml for the system. Failure to do this will result in needing to change switch and CMM configurations.\n\u0026#34;x9000\u0026#34;: { \u0026#34;Parent\u0026#34;: \u0026#34;s0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x9000\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_cabinet\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;Hill\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Cabinet\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Networks\u0026#34;: { \u0026#34;cn\u0026#34;: { \u0026#34;HMN\u0026#34;: { \u0026#34;CIDR\u0026#34;: \u0026#34;10.104.0.0/22\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;10.104.0.1\u0026#34;, \u0026#34;VLan\u0026#34;: 999 }, \u0026#34;NMN\u0026#34;: { \u0026#34;CIDR\u0026#34;: \u0026#34;10.100.0.0/22\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;10.100.0.1\u0026#34;, \u0026#34;VLan\u0026#34;: 666 } } } } This file is manually created and follows this format. For each \u0026ldquo;type\u0026rdquo; of cabinet can have several fields: total_number of cabinets of this type, starting_id for this cabinet type, and a list of the ids.\n--- cabinets: - type: hill total_number: 2 starting_id: 9000 - type: mountain total_number: 4 starting_id: 1000 cabinets: - id: 1000 nmn-vlan: 2000 hmn-vlan: 3000 - id: 1001 nmn-vlan: 2001 hmn-vlan: 3001 - id: 1002 - id: 1003 - type: river total_number: 4 starting_id: 3000 In the above example file, there are 2 Hill cabinets that will be automatically numbered as 9000 and 9001. The Mountain cabinets appear in 3 groupings of four ids. The River cabinets are non-contiguous in 4 separated ids.\nA system will Hill cabinets can have 1 to 4 cabinet ids. There is no limit on the number of Mountain or River cabinets.\nWhen the above cabinets.yaml file is used, the fields for each type will take precedence over any command line argument to \u0026ldquo;csi config init\u0026rdquo; for starting-mountain-cabinet, starting-river-cabinet, starting-hill-cabinet, mountain-cabinets, river-cabinets, or hill-cabinets. If these command line arguments provide information which is not in the cabinets.yaml file, then the information will be merged from them with the information provided in cabinets.yaml.\n"
},
{
	"uri": "/docs-csm/en-09/400-switch-bgp-neighbors/",
	"title": "Verify and Update BGP neighbors",
	"tags": [],
	"description": "",
	"content": "Verify and Update BGP neighbors This page will detail how to manually configure and verify BGP neighbors on the management switches.\nYou will not have BGP peers until install.sh is ran. This is where MetalLB is deployed. How do I check the status of the BGP neighbors? Log into the spine switches and run show bgp ipv4 unicast summary for Aruba/HPE switches and show ip bgp summary for Mellanox. Are my Neighbors stuck in IDLE? running clear ip bgp all on the mellanox and clear bgp * on the Arubas will restart the BGP process, this process may need to be done when a system is reinstalled. If only some neighbors are showing ESTABLISHED you may need to run the command multiple times for all the BGP peers to come up. The BGP neighbors will be the worker NCN IPs on the NMN (node management network) (VLAN002). If your system is using HPE/Aruba, one of the neighbors will be the other spine switch. On the Aruba/HPE switches properly configured BGP will look like the following. Generate MetalLB configmap Depending on the network architecture of your system you may need to peer with switches other than the spines. CSI has a BGP peers argument that accepts \u0026lsquo;aggregation\u0026rsquo; as an option, if no option is defined it will default to the spines as being the MetalLB peers. CSI cli arguments with --bgp-peers aggregation\nlinux# ~/src/mtl/cray-site-init/bin/csi config init --bootstrap-ncn-bmc-user root --bootstrap-ncn-bmc-pass initial0 --ntp-pool cfntp-4-1.us.cray.com,cfntp-4-2.us.cray.com --can-external-dns 10.103.8.113 --can-gateway 10.103.8.1 --site-ip 172.30.56.2/24 --site-gw 172.30.48.1 --site-dns 172.30.84.40 --site-nic em1 --system-name odin --bgp-peers aggregation Automated Process There is an automated script to update the BGP configuration on both the Mellanox and Aruba switches. This script is installed into the $PATH by the metal-net-scripts package The scripts are named mellanox_set_bgp_peers.py and aruba_set_bgp_peers.py These scripts pull in data from CSI generated .yaml files. The files required are CAN.yaml, HMN.yaml, HMNLB.yaml, NMNLB.yaml, NMN.yaml, these exist in the networks/ subdirectory of the generated configs. In order for these scripts to work the following commands will need to be present on the switches. Aruba\nsw-spine-001(config)# https-server rest access-mode read-write Mellanox\nsw-spine-001 [standalone: master] \u0026gt; enable sw-spine-001 [standalone: master] # configure terminal sw-spine-001 [standalone: master] (config) # json-gw enable Script Usage\nUSAGE: - \u0026lt;Spine01/Agg01\u0026gt; \u0026lt;Spine02/Agg02\u0026gt; \u0026lt;Path to CSI generated network files\u0026gt; - The IPs used should be Node Management Network IPs (NMN), these IPs will be what is used for the BGP Router-ID. - The path must include CAN.yaml\u0026#39;, \u0026#39;HMN.yaml\u0026#39;, \u0026#39;HMNLB.yaml\u0026#39;, \u0026#39;NMNLB.yaml\u0026#39;, \u0026#39;NMN.yaml Example: ./aruba_set_bgp_peers.py 10.252.0.2 10.252.0.3 /var/www/ephemeral/prep/redbull/networks After this script is run you will need to verify the configuration and verify the BGP peers are ESTABLISHED Manual Process sw-spine-001# show bgp ipv4 unicast summary VRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.1 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.3 65533 31457 31474 00m:02w:04d Established Up 10.252.2.8 65533 54730 62906 00m:02w:04d Established Up 10.252.2.9 65533 54732 62927 00m:02w:04d Established Up 10.252.2.18 65533 54732 62911 00m:02w:04d Established Up On the Mellanox switches, first you must run the switch commands listed in the Automated section above. Then the output should look like the following.\nsw-spine-001 [standalone: master] # show ip bgp summary VRF name : default BGP router identifier : 10.252.0.1 local AS number : 65533 BGP table version : 308 Main routing table version: 308 IPV4 Prefixes : 261 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.0.7 4 65533 37421 42948 308 0 0 12:23:16:07 ESTABLISHED/53 10.252.0.8 4 65533 37421 42920 308 0 0 12:23:16:07 ESTABLISHED/51 10.252.0.9 4 65533 37420 42962 308 0 0 12:23:16:07 ESTABLISHED/51 If the BGP neighbors are not in the ESTABLISHED state make sure the IPs are correct for the route-map and BGP configuration. If IPs are incorrect you will have to update the configuration to match the IPs, the configuration below will need to be edited. You can get the NCN IPs from the CSI generated files (NMN.yaml, CAN.yaml, HMN.yaml), these IPs are also located in /etc/dnsmasq.d/statics.conf on the LiveCD/ncn-m001. pit# grep w00 /etc/dnsmasq.d/statics.conf | grep nmn host-record=ncn-w003,ncn-w003.nmn,10.252.1.13 host-record=ncn-w002,ncn-w002.nmn,10.252.1.14 host-record=ncn-w001,ncn-w001.nmn,10.252.1.15 The route-map configuration will require you to get the HMN, and CAN IPs as well. Note the Bond0 Mac0/Mac1 entry is for the NMN. pit# grep ncn-w /etc/dnsmasq.d/statics.conf | egrep \u0026#34;Bond0|HMN|CAN\u0026#34; | grep -v mgmt dhcp-host=50:6b:4b:08:d0:4a,10.252.1.13,ncn-w003,20m # Bond0 Mac0/Mac1 dhcp-host=50:6b:4b:08:d0:4a,10.254.1.20,ncn-w003,20m # HMN dhcp-host=50:6b:4b:08:d0:4a,10.102.4.12,ncn-w003,20m # CAN dhcp-host=98:03:9b:0f:39:4a,10.252.1.14,ncn-w002,20m # Bond0 Mac0/Mac1 dhcp-host=98:03:9b:0f:39:4a,10.254.1.22,ncn-w002,20m # HMN dhcp-host=98:03:9b:0f:39:4a,10.102.4.13,ncn-w002,20m # CAN dhcp-host=98:03:9b:bb:a9:94,10.252.1.15,ncn-w001,20m # Bond0 Mac0/Mac1 dhcp-host=98:03:9b:bb:a9:94,10.254.1.24,ncn-w001,20m # HMN dhcp-host=98:03:9b:bb:a9:94,10.102.4.14,ncn-w001,20m # CAN The Aruba configuration will require you to set the other peering switch as a BGP neighbor, the mellanox configuration does not require this. You will need to delete the previous route-map, and BGP configuration on both switches. Aruba delete commands. sw-spine-001# configure terminal sw-spine-001(config)# no router bgp 65533 This will delete all BGP configurations on this device. Continue (y/n)? y sw-spine-001(config)# no route-map ncn-w003 sw-spine-001(config)# no route-map ncn-w002 sw-spine-001(config)# no route-map ncn-w001 Aruba configuration example.\nroute-map rm-ncn-w001 permit seq 10 match ip address prefix-list pl-nmn set ip next-hop 10.252.2.8 route-map rm-ncn-w001 permit seq 20 match ip address prefix-list pl-hmn set ip next-hop 10.254.2.27 route-map rm-ncn-w001 permit seq 30 match ip address prefix-list pl-can set ip next-hop 10.103.10.10 route-map rm-ncn-w002 permit seq 10 match ip address prefix-list pl-nmn set ip next-hop 10.252.2.9 route-map rm-ncn-w002 permit seq 20 match ip address prefix-list pl-hmn set ip next-hop 10.254.2.25 route-map rm-ncn-w002 permit seq 30 match ip address prefix-list pl-can set ip next-hop 10.103.10.9 route-map rm-ncn-w003 permit seq 10 match ip address prefix-list pl-nmn set ip next-hop 10.252.2.18 route-map rm-ncn-w003 permit seq 20 match ip address prefix-list pl-hmn set ip next-hop 10.254.2.26 route-map rm-ncn-w003 permit seq 30 match ip address prefix-list pl-can set ip next-hop 10.103.10.11 ! router bgp 65533 bgp router-id 10.252.0.1 maximum-paths 8 neighbor 10.252.0.3 remote-as 65533 neighbor 10.252.2.8 remote-as 65533 neighbor 10.252.2.9 remote-as 65533 neighbor 10.252.2.18 remote-as 65533 address-family ipv4 unicast neighbor 10.252.0.3 activate neighbor 10.252.2.8 activate neighbor 10.252.2.8 route-map ncn-w001 in neighbor 10.252.2.9 activate neighbor 10.252.2.9 route-map ncn-w002 in neighbor 10.252.2.18 activate neighbor 10.252.2.18 route-map ncn-w003 in exit-address-family Mellanox delete commands.\nsw-spine-001 [standalone: master] # sw-spine-001 [standalone: master] # conf t sw-spine-001 [standalone: master] (config) # no router bgp 65533 sw-spine-001 [standalone: master] (config) # no route-map ncn-w001 sw-spine-001 [standalone: master] (config) # no route-map ncn-w002 sw-spine-001 [standalone: master] (config) # no route-map ncn-w003 Mellanox configuration example.\n## Route-maps configuration ## route-map rm-ncn-w001 permit 10 match ip address pl-nmn route-map rm-ncn-w001 permit 10 set ip next-hop 10.252.0.7 route-map rm-ncn-w001 permit 20 match ip address pl-hmn route-map rm-ncn-w001 permit 20 set ip next-hop 10.254.0.7 route-map rm-ncn-w001 permit 30 match ip address pl-can route-map rm-ncn-w001 permit 30 set ip next-hop 10.103.8.7 route-map rm-ncn-w002 permit 10 match ip address pl-nmn route-map rm-ncn-w002 permit 10 set ip next-hop 10.252.0.8 route-map rm-ncn-w002 permit 20 match ip address pl-hmn route-map rm-ncn-w002 permit 20 set ip next-hop 10.254.0.8 route-map rm-ncn-w002 permit 30 match ip address pl-can route-map rm-ncn-w002 permit 30 set ip next-hop 10.103.8.8 route-map rm-ncn-w003 permit 10 match ip address pl-nmn route-map rm-ncn-w003 permit 10 set ip next-hop 10.252.0.9 route-map rm-ncn-w003 permit 20 match ip address pl-hmn route-map rm-ncn-w003 permit 20 set ip next-hop 10.254.0.9 route-map rm-ncn-w003 permit 30 match ip address pl-can route-map rm-ncn-w003 permit 30 set ip next-hop 10.103.8.9 ## ## BGP configuration ## protocol bgp router bgp 65533 vrf default router bgp 65533 vrf default router-id 10.252.0.1 force router bgp 65533 vrf default maximum-paths ibgp 32 router bgp 65533 vrf default neighbor 10.252.0.7 remote-as 65533 router bgp 65533 vrf default neighbor 10.252.0.7 route-map ncn-w001 router bgp 65533 vrf default neighbor 10.252.0.8 remote-as 65533 router bgp 65533 vrf default neighbor 10.252.0.8 route-map ncn-w002 router bgp 65533 vrf default neighbor 10.252.0.9 remote-as 65533 router bgp 65533 vrf default neighbor 10.252.0.9 route-map ncn-w003 router bgp 65533 vrf default neighbor 10.252.0.10 remote-as 65533 Once the IPs are updated for the route-maps and BGP neighbors you may need to restart the BGP process on the switches, you do this by running clear ip bgp all on the mellanox and clear bgp * on the Arubas. (This may need to be done multiple times for all the peers to come up) When worker nodes are reinstalled, the BGP process will need to be restarted. If the BGP peers are still not coming up you should check the metallb.yaml config file for errors. The MetalLB config file should point to the NMN IPs of the switches configured. metallb.yaml configuration example.\nThe peer-address should be the IP of the switch that you are doing BGP peering with. --- apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | peers: - peer-address: 10.252.0.2 peer-asn: 65533 my-asn: 65533 - peer-address: 10.252.0.3 peer-asn: 65533 my-asn: 65533 address-pools: - name: customer-access protocol: bgp addresses: - 10.102.9.112/28 - name: customer-access-dynamic protocol: bgp addresses: - 10.102.9.128/25 - name: hardware-management protocol: bgp addresses: - 10.94.100.0/24 - name: node-management protocol: bgp addresses: - 10.92.100.0/24 "
},
{
	"uri": "/docs-csm/en-09/401-management-network-install/",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": "Overview New for Shasta v1.4 Networking Dell and Mellanox Changes for Shasta v1.3 to v1.4 Upgrades HPE Aruba Installation and Configuration Early Installation Access Master Node 1 (ncn-m001) and its iLO should already be completed (003-CSM-USB-LIVECD.md) Install Baseline Switch Configuration Update Firmware Configure all switches via IPv6 connection from ncn-m001 Layer 2 configuration: VLAN. MLAG and VSX pairs. iLO/BMC, CMM and Gateway Node port configuration. Switch uplink ports - ISL Layer 3 configuration: L3 interfaces. ACL. Dynamic Routing: OSPFv2 and BGP. SNMP Create the CAN. New for v1.4 The network architecture and configuration changes from v1.3 to v1.4 are fairly small. The biggest change is the introduction of HPE/Aruba switches, as well as Cray Site Init (CSI) generating switch IPs. HPE Aruba switch configuration is contained in separate documents as described in the index above. Dell and Mellanox changes to upgrade from v1.3 to v.14 Shasta releases are describe in separate document.\nNew Aruba/HPE switches. ACL configuration. Moving Site connection from ncn-w001 to ncn-m001. The IP-Helper will reside on the switches where the default gateway for the nodes, such as BMCs and computes, is configured. IP-Helper 10.92.100.222 applied on vlan 1, 2, and 7. IP-Helper 10.94.100.222 applied on vlan 4. Make sure 1.3.2 changes are applied, this includes flow-control and MAGP changes. Remove BPDUfilter on the Dell access ports. Add BPDUguard to the Dell access ports. "
},
{
	"uri": "/docs-csm/en-09/402-mgmt-net-base-config/",
	"title": "Management Network Base Configuration",
	"tags": [],
	"description": "",
	"content": "Management Network Base Configuration This page provides instructions on how to setup the base network configuration of the Shasta Management network.\nWith the base config applied you will be able to access all Management switches and apply the remaining configuration.\nRequirements Console access to all of the switches SHCD available Configuration Once you have console access to the switches you can begin by applying the base config. The purpose of this configuration is to have an IPv6 underlay that allows us to always be able to access the management switches. The base config is running OSPFv3 over IPv6 on VLAN 1, so the switches can dynamically form neighborship which allows us to gain remote access to them.\nThe admin username and password needs to be applied.\nswitch# conf t switch(config)# user admin group administrators password plaintext xxxxxxxx Set the hostname of the switch, you can use the name defined in the SHCD for this.\nswitch(config)# hostname sw-25g04 Every uplink or switch to switch link will need to be enabled and require the following configuration. You can determine the uplink ports from the SHCD.\nsw-25g04(config)# int 1/1/1 sw-25g04(config-if)# no routing sw-25g04(config-if)# vlan trunk native 1 sw-25g04(config-if)# no shut You will need to add an IPv6 interface to VLAN 1 and start the OSPv3 process. In addition to this, you will need a unique router-id, this is an IPv4 address that will only be used for identifying the router, this is not a routable address. You can increment this by 1 for each switch. You can use other IPs for router-IDs if desired.\nsw-25g04(config)# router ospfv3 1 sw-25g04(config-ospfv3-1)# area 0 sw-25g04(config-ospfv3-1)# router-id 172.16.0.1 sw-25g04(config-ospfv3-1)# exit Add VLAN 1 interface to OSPF area 0\nsw-25g04(config)# int vlan 1 sw-25g04(config-if-vlan)# ipv6 address autoconfig sw-25g04(config-if-vlan)# ipv6 ospfv3 1 area 0 sw-25g04(config-if-vlan)# exit Add a unique IPv6 Loopback address, this is the address that we will be remotely connecting to. You can increment this address by 1 for every switch you have.\nsw-25g04(config)# interface loopback 0 sw-25g04(config-loopback-if)# ipv6 address fd01::0/64 sw-25g04(config-loopback-if)# ipv6 ospfv3 1 area 0 The following commands will need to be applied to gain remote access via SSH/HTTPS/API\nsw-25g04(config)# ssh server vrf default sw-25g04(config)# ssh server vrf mgmt sw-25g04(config)# https-server vrf default sw-25g04(config)# https-server vrf mgmt sw-25g04(config)# https-server rest access-mode read-write The show run will look like the following\nsw-25g04(config)# show run Current configuration: ! !Version ArubaOS-CX Virtual.10.05.0020 !export-password: default hostname sw-25g04 user admin group administrators password ciphertext AQBapXDwaGq+GHwyLgj0Eu led locator on ! ! ! ! ssh server vrf default ssh server vrf mgmt vlan 1 interface mgmt no shutdown ip dhcp interface 1/1/1 no shutdown no routing vlan trunk native 1 vlan trunk allowed all interface loopback 0 ipv6 address fd01::1/64 ipv6 ospfv3 1 area 0.0.0.0 interface loopback 1 interface vlan 1 ipv6 address autoconfig ipv6 ospfv3 1 area 0.0.0.0 ! ! ! ! ! router ospfv3 1 router-id 192.168.100.1 area 0.0.0.0 https-server vrf default https-server vrf mgmt At this point you should see OSPFv3 neighbors.\nsw-25g04# show ipv6 ospfv3 neighbors OSPFv3 Process ID 1 VRF default ================================ Total Number of Neighbors: 1 Neighbor ID Priority State Interface ------------------------------------------------------- 192.168.100.2 1 FULL/BDR vlan1 Neighbor address fe80::800:901:8b4:e152 You can now connect to the neighbors via the IPv6 loopback that we set earlier.\nsw-25g03# ssh admin@fd01::1 "
},
{
	"uri": "/docs-csm/en-09/403-mgmt-net-vlan-config/",
	"title": "Management Network VLAN Configuration",
	"tags": [],
	"description": "",
	"content": "Management Network VLAN Configuration Requirements Access to all of the switches SHCD available Aruba Configuration At this point we should have access to the switches. We will start by adding all the VLANS required by the Shasta system. Cray Site Init (CSI) generates the IPs used by the system, below are samples only. Some switches will NOT need the CAN(VLAN7), most of the time this IP is only located on the Spine for external connectivity.\nThe River Cabinets will need the following VLANs\nVLAN Switch1 IP Switch2 IP Active Gateway Purpose 2 10.252.0.2/17 10.252.0.3/17 10.252.0.1 River Node Management 4 10.254.0.2/17 10.254.0.3/17 10.254.0.1 River Hardware Management 7 TBD TBD TBD Customer Access Network 10 10.11.0.2/17 10.11.0.3/17 10.11.0.1 Storage (future) The Mountain Cabinets will need the following VLANs, these are typically the CDU switches. The 2xxx and 3xxx VLANs are per cabinet, so with each additional cabinet you will increment the VLAN by 1 and add a new /22 subnet.\nVLAN Switch1 IP Switch2 IP Purpose 2 10.252.0.x/17 10.252.0.x/17 River Node Management 4 10.254.0.x/17 10.254.0.x/17 River Hardware Management 2000 10.100.0.2/22 10.100.0.3/22 Mountain Node Management 3000 10.104.0.2/22 10.104.0.3/22 Mountain Hardware Management Add the networks to each of the VSX pairs.\nsw-24g03(config)# vlan 2 sw-24g03(config-vlan-2)# name NMN sw-24g03(config-vlan-2)# vlan 4 sw-24g03(config-vlan-4)# name HMN sw-24g03(config-vlan-4)# vlan 7 sw-24g03(config-vlan-7)# name CAN sw-24g03(config-vlan-7)# vlan 10 sw-24g03(config-vlan-10)# name SUN sw-24g04(config)# vlan 2 sw-24g04(config-vlan-2)# name NMN sw-24g04(config-vlan-2)# vlan 4 sw-24g04(config-vlan-4)# name HMN sw-24g04(config-vlan-4)# vlan 7 sw-24g04(config-vlan-7)# name CAN sw-24g04(config-vlan-7)# vlan 10 sw-24g04(config-vlan-10)# name SUN Add the networks to the leaf switches or the switches to which the BMCs are connected.\nsw-leaf-001(config)# vlan 2 sw-leaf-001(config-vlan-2)# name NMN sw-leaf-001(config-vlan-2)# vlan 4 sw-leaf-001(config-vlan-4)# name HMN sw-leaf-001(config-vlan-4)# vlan 7 sw-leaf-001(config-vlan-7)# name CAN sw-leaf-001(config-vlan-7)# vlan 10 sw-leaf-001(config-vlan-10)# name SUN Add VLAN interfaces to the VLANs just created. Specific Addresses are provide CSI.\nsw-leaf-001(config)# int vlan 1 sw-leaf-001(config-if-vlan)# ip address 10.1.0.4/16 sw-leaf-001(config-if-vlan)# int vlan 2 sw-leaf-001(config-if-vlan)# ip address 10.252.0.4/17 sw-leaf-001(config-if-vlan)# int vlan 4 sw-leaf-001(config-if-vlan)# ip address 10.254.0.4/17 "
},
{
	"uri": "/docs-csm/en-09/404-mgmt-net-mlag-config/",
	"title": "Management Network MLAG Configuration",
	"tags": [],
	"description": "",
	"content": "Management Network MLAG Configuration This page describes how to setup a bonded configuration from the Non-Compute nodes (NCN) to the management network.\nRequirements Console Access to the switches participating in MLAG configuration. Two switches running the same firmware version. Three cables connected between the switches, two for the Inter Switch Link (ISL) and one for the Keepalive. Aruba Configuration Create the keepalive vrf on both switches. The following configuration will need to be done on both switches participating in VSX/MLAG, if there is a unique configuration it will be called out.\nsw-24g04(config)# vrf keepalive Create the ISL lag on both switches.\nsw-24g04(config)# interface lag 99 sw-24g04(config-lag-if)# no shutdown sw-24g04(config-lag-if)# description ISL link sw-24g04(config-lag-if)# no routing sw-24g04(config-lag-if)# vlan trunk native 1 tag sw-24g04(config-lag-if)# vlan trunk allowed all sw-24g04(config-lag-if)# lacp mode active Setup the keepalive link. This will require a unique IP on both switches.\nsw-24g04(config)# int 1/1/3 sw-24g04(config-if)# no shutdown sw-24g04(config-if)# mtu 9198 sw-24g04(config-if)# vrf attach keepalive sw-24g04(config-if)# description keepalive sw-24g04(config-if)# ip address 192.168.255.0/31 sw-24g03(config)# int 1/1/3 sw-24g03(config-if)# no shutdown sw-24g03(config-if)# mtu 9198 sw-24g03(config-if)# vrf attach keepalive sw-24g03(config-if)# description keepalive sw-24g03(config-if)# ip address 192.168.255.1/31 Add the ISL ports to the LAG, these are two of the ports connected between the switches.\nsw-24g04(config)# int 1/1/1-1/1/2 sw-24g04(config-if-\u0026lt;1/1/1-1/1/2\u0026gt;)# no shutdown sw-24g04(config-if-\u0026lt;1/1/1-1/1/2\u0026gt;)# mtu 9198 sw-24g04(config-if-\u0026lt;1/1/1-1/1/2\u0026gt;)# lag 99 Create the VSX instance and setup the keepalive link.\nsw-24g03(config)# vsx sw-24g03(config-vsx)# system-mac 02:01:00:00:01:00 sw-24g03(config-vsx)# inter-switch-link lag 99 sw-24g03(config-vsx)# role primary sw-24g03(config-vsx)# keepalive peer 192.168.255.0 source 192.168.255.1 vrf keepalive sw-24g03(config-vsx)# linkup-delay-timer 600 sw-24g03(config-vsx)# vsx-sync vsx-global sw-24g04(config)# vsx sw-24g04(config-vsx)# system-mac 02:01:00:00:01:00 sw-24g04(config-vsx)# inter-switch-link lag 99 sw-24g04(config-vsx)# role secondary sw-24g04(config-vsx)# keepalive peer 192.168.255.1 source 192.168.255.0 vrf keepalive sw-24g04(config-vsx)# linkup-delay-timer 600 sw-24g04(config-vsx)# vsx-sync vsx-global At this point you should have an Established VSX session\nsw-24g04(config-if-vlan)# show vsx brief ISL State : In-Sync Device State : Sync-Primary Keepalive State : Keepalive-Established Device Role : secondary Number of Multi-chassis LAG interfaces : 0 Create and setup high availability VLAN interfaces This should be done for all VLANS VLANs in the 2xxx and 3xxx range will be for CDU switches only Cray Site Init (CSI) generates the IPs used by the system, below are samples only.\nVLAN Switch1 IP Switch2 IP Active Gateway 2 10.252.0.2/17 10.252.0.3/17 10.252.0.1 4 10.254.0.2/17 10.254.0.3/17 10.254.0.1 7 TBD TBD TBD 10 10.11.0.2/17 10.11.0.3/17 10.11.0.1 2000 10.100.0.2/22 10.100.0.3/22 10.100.0.1/22 3000 10.104.0.2/22 10.104.0.3/22 10.104.0.1/22 sw-24g04(config)# vlan 2 sw-24g04(config-vlan-2)# interface vlan 2 sw-24g04(config-if-vlan)# vsx-sync active-gateways sw-24g04(config-if-vlan)# ip mtu 9198 sw-24g04(config-if-vlan)# ip address 10.252.0.3/17 sw-24g04(config-if-vlan)# active-gateway ip mac 12:01:00:00:01:00 sw-24g04(config-if-vlan)# active-gateway ip 10.252.0.1 sw-24g03(config)# vlan 2 sw-24g03(config-vlan-2)# interface vlan 2 sw-24g03(config-if-vlan)# vsx-sync active-gateways sw-24g03(config-if-vlan)# ip mtu 9198 sw-24g03(config-if-vlan)# ip address 10.252.0.2/17 sw-24g03(config-if-vlan)# active-gateway ip mac 12:01:00:00:01:00 sw-24g03(config-if-vlan)# active-gateway ip 10.252.0.1 Configure bonds on ports connecting to the NCNs, this information can be found on the SHCD.\nsw-24g04(config)# interface lag 1 multi-chassis sw-24g04(config-lag-if)# no shutdown sw-24g04(config-lag-if)# no routing sw-24g04(config-lag-if)# vlan trunk native 1 sw-24g04(config-lag-if)# vlan trunk allowed 1-2,4,7,10 sw-24g04(config-lag-if)# lacp mode active sw-24g04(config-lag-if)# lacp fallback sw-24g04(config)# interface 1/1/1 sw-24g04(config)# no shutdown sw-24g04(config)# mtu 9198 sw-24g04(config)# lag 1 "
},
{
	"uri": "/docs-csm/en-09/405-mgmt-net-port-config/",
	"title": "Management Network Access Port configurations.",
	"tags": [],
	"description": "",
	"content": "Management Network Access Port configurations. Requirements Access to switches SHCD Configuration This configuration describes the edge port configuration, you will find these in the NMN/HMN/Mountain-TDS Management Tab of the SHCD. Typically these are ports that are connected to iLOs (BMCs), gateway nodes, or compute nodes/CMM switches. sw-leaf-001(config)# interface 1/1/28 sw-leaf-001(config)# no shutdown sw-leaf-001(config)# mtu 9198 sw-leaf-001(config)# description HMN sw-leaf-001(config)# no routing sw-leaf-001(config)# vlan access 4 This configuration describes the ports that go to the Node Management Network (NMN/VLAN2). You can Identify these ports by referencing the NMN tab on the SHCD. sw-leaf-001(config)# interface 1/1/6 sw-leaf-001(config)# no shutdown sw-leaf-001(config)# mtu 9198 sw-leaf-001(config)# description NMN sw-leaf-001(config)# no routing sw-leaf-001(config)# vlan access 2 UAN port configuration UANs have the same network connections as Shasta v1.3. One connection will go to a NMN(VLAN2) access port, this is where the UAN will pxe boot and communicate with internal systems. (see SHCD for UAN cabling). One Bond (two connections) will be going to the MLAG/VSX pair of switches. This will be a TRUNK port for the CAN connection. Aruba UAN NMN Configuration\ninterface 1/1/16 no shutdown mtu 9198 no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit Aruba UAN CAN Configuration\nPort Configuration is the same on both switches.\ninterface lag 17 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 7 lacp mode active lacp fallback exit Gigabyte/Intel NCN Worker port configuration The cabling guidelines for all servers can be found here MGMT-NET-CABLING.\nMellanox Port Config\nsw-spine-002 [gamora-mlag-domain: master] # show run int ethernet 1/1 interface ethernet 1/1 speed 40G force interface ethernet 1/1 mtu 9216 force interface ethernet 1/1 mlag-channel-group 1 mode active Mellanox MLAG config\nsw-spine-002 [gamora-mlag-domain: master] # show run int mlag-port-channel 1 interface mlag-port-channel 1 interface mlag-port-channel 1 mtu 9216 force interface mlag-port-channel 1 switchport mode hybrid interface mlag-port-channel 1 no shutdown interface mlag-port-channel 1 switchport hybrid allowed-vlan add 2 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 4 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 7 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 10 HPE NCN Worker port configuration Aruba Port Config\nsw-spine02# show run int 1/1/7 interface 1/1/7 no shutdown mtu 9198 lag 4 exit Aruba LAG Configuration\nsw-spine02# show run int lag 4 interface lag 4 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback exit Gigabyte/Intel NCN Master port configuration Mellanox Port config\nsw-spine-002 [gamora-mlag-domain: master] # show run int ethernet 1/1 interface ethernet 1/1 speed 40G force interface ethernet 1/1 mtu 9216 force interface ethernet 1/1 mlag-channel-group 1 mode active Mellanox MLAG port config\nsw-spine-002 [gamora-mlag-domain: master] # show run int mlag-port-channel 1 interface mlag-port-channel 1 interface mlag-port-channel 1 mtu 9216 force interface mlag-port-channel 1 switchport mode hybrid interface mlag-port-channel 1 no shutdown interface mlag-port-channel 1 switchport hybrid allowed-vlan add 2 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 4 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 7 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 10 HPE NCN Master port configuration Aruba Port Config\nsw-spine02# show run int 1/1/7 interface 1/1/7 no shutdown mtu 9198 lag 4 exit Aruba LAG Configuration\nsw-spine02# show run int lag 4 interface lag 4 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback exit Gigabyte/Intel NCN Storage port configuration Mellanox Port config\nsw-spine-002 [gamora-mlag-domain: master] # show run int ethernet 1/7 interface ethernet 1/7 speed 40G force interface ethernet 1/7 mtu 9216 force interface ethernet 1/7 mlag-channel-group 7 mode active Mellanox MLAG port config\nsw-spine-002 [gamora-mlag-domain: master] # show run int mlag-port-channel 7 interface mlag-port-channel 7 interface mlag-port-channel 7 mtu 9216 force interface mlag-port-channel 7 switchport mode hybrid interface mlag-port-channel 7 no shutdown interface mlag-port-channel 7 switchport hybrid allowed-vlan add 2 interface mlag-port-channel 7 switchport hybrid allowed-vlan add 4 interface mlag-port-channel 7 switchport hybrid allowed-vlan add 7 interface mlag-port-channel 7 switchport hybrid allowed-vlan add HPE NCN Storage port configuration Aruba Port Config\nsw-spine02# show run int 1/1/7 interface 1/1/7 no shutdown mtu 9198 lag 4 exit Aruba LAG Configuration\nsw-spine02# show run int lag 4 interface lag 4 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback exit Aruba Storage port configuration (future use) These will be configured, but the ports will be shut down until needed.\nsw-spine02# show run int 1/1/7 interface 1/1/7 shutdown mtu 9198 lag 4 exit Aruba LAG Configuration\nsw-spine02# show run int lag 4 interface lag 4 multi-chassis shutdown no routing vlan access 10 lacp mode active lacp fallback exit CMM port configuration. This configuration describes the ports that go to the Mountain CMMs/Computes. This is for Aruba switches only. The CDU switches have two cables (10Gb RJ45) connecting to each CMM, we will setup a static MC-LAG. This LAG is currently used for improved throughput, redundancy is not yet offered. The CEC will need to be programmed to have the \u0026ldquo;switch LAG\u0026rdquo; setting to be set to \u0026ldquo;0001\u0026rdquo; First Aruba CDU configuration.\nsw-cdu-001(config)# int lag 2 multi-chassis static sw-cdu-001(config-lag-if)# vsx-sync vlans sw-cdu-001(config-lag-if)# no shutdown sw-cdu-001(config-lag-if)# description CMM_CAB_1000 sw-cdu-001(config-lag-if)# no routing sw-cdu-001(config-lag-if)# vlan trunk native 2000 sw-cdu-001(config-lag-if)# vlan trunk allowed 2000,3000,4091 sw-cdu-001(config-lag-if)# exit sw-cdu-001(config)# int 1/1/2 sw-cdu-001(config-if)# no shutdown sw-cdu-001(config-if)# lag 2 sw-cdu-001(config-if)# exit Second Aruba CDU configuration\nsw-cdu-002(config)# int lag 2 multi-chassis static sw-cdu-002(config-lag-if)# vsx-sync vlans sw-cdu-002(config-lag-if)# shutdown sw-cdu-002(config-lag-if)# description CMM_CAB_1000 sw-cdu-002(config-lag-if)# no routing sw-cdu-002(config-lag-if)# vlan trunk native 2000 sw-cdu-002(config-lag-if)# vlan trunk allowed 2000,3000,4091 sw-cdu-002(config-lag-if)# exit sw-cdu-002(config)# int 1/1/2 sw-cdu-002(config-if)# no shutdown sw-cdu-002(config-if)# lag 2 sw-cdu-002(config-if)# exit "
},
{
	"uri": "/docs-csm/en-09/406-mgmt-net-acl-config/",
	"title": "Management Network ACL configuration",
	"tags": [],
	"description": "",
	"content": "Management Network ACL configuration This page describes the purpose of the ACLs and how they are configured\nRequirements Access to the switches Aruba Configuration These ACLs are designed to block traffic from the node management network to and from the hardware management network. These need to be set where the Layer3 interface is located, this will most likely be a VSX pair of switches. These ACLs are required on both switches in the pair.\nThe first step is to create the access list, once it is created we have to apply it to a VLAN.\naccess-list ip nmn-hmn 10 deny any 10.252.0.0/255.255.128.0 10.254.0.0/255.255.128.0 20 deny any 10.252.0.0/255.255.128.0 10.104.0.0/255.252.0.0 30 deny any 10.254.0.0/255.255.128.0 10.252.0.0/255.255.128.0 40 deny any 10.254.0.0/255.255.128.0 10.100.0.0/255.252.0.0 50 deny any 10.100.0.0/255.252.0.0 10.254.0.0/255.255.128.0 60 deny any 10.100.0.0/255.252.0.0 10.104.0.0/255.252.0.0 70 deny any 10.104.0.0/255.252.0.0 10.252.0.0/255.255.128.0 80 deny any 10.104.0.0/255.252.0.0 10.100.0.0/255.252.0.0 90 permit any any any Apply ACL to a VLANs\nsw-24g03(config)# vlan 2 sw-s24g03(config-vlan-2)# apply access-list ip nmn-hmn in sw-s24g03(config-vlan-2)# apply access-list ip nmn-hmn out sw-24g03(config)# vlan 4 sw-s24g03(config-vlan-4)# apply access-list ip nmn-hmn in sw-s24g03(config-vlan-4)# apply access-list ip nmn-hmn out If you are on a CDU switch you will need to apply it to the 2xxx and 3xxx VLANs\nMellanox Configuration Create the nmn-hmn access-list and apply it to vlan 2 and vlan 4.\nsw-spine-001\u0026gt; enable sw-spine-001# configure terminal sw-spine-001(config) # ipv4 access-list nmn-hmn sw-spine-001(config ipv4 access-list nmn-hmn) # bind-point rif sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 10 deny ip 10.252.0.0 mask 255.255.128.0 10.254.0.0 mask 255.255.128.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 20 deny ip 10.252.0.0 mask 255.255.128.0 10.104.0.0 mask 255.252.0.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 30 deny ip 10.254.0.0 mask 255.255.128.0 10.252.0.0 mask 255.255.128.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 40 deny ip 10.254.0.0 mask 255.255.128.0 10.100.0.0 mask 255.252.0.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 50 deny ip 10.100.0.0 mask 255.252.0.0 10.254.0.0 mask 255.255.128.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 60 deny ip 10.100.0.0 mask 255.252.0.0 10.104.0.0 mask 255.252.0.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 70 deny ip 10.104.0.0 mask 255.252.0.0 10.252.0.0 mask 255.255.128.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 80 deny ip 10.104.0.0 mask 255.252.0.0 10.100.0.0 mask 255.252.0.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 90 permit ip any any sw-spine-001(config ipv4 access-list nmn-hmn) # exit sw-spine-001(config) # interface vlan 2 ipv4 port access-group nmn-hmn sw-spine-001(config) # interface vlan 4 ipv4 port access-group nmn-hmn sw-spine-001(config) # exit sw-spine-001# write memory "
},
{
	"uri": "/docs-csm/en-09/407-mgmt-net-snmp-config/",
	"title": "Management Network SNMP configuration",
	"tags": [],
	"description": "",
	"content": "Management Network SNMP configuration Requirements Access to switches\nThis configuration is required for hardware discovery of the Shasta system. It needs to be applied on all switches that are connected to BMCs\nAruba Configuration snmp-server vrf default snmpv3 user testuser auth md5 auth-pass plaintext testpass1 priv des priv-pass plaintext testpass2 Dell Configuration snmp-server group cray-reds-group 3 noauth read cray-reds-view snmp-server user testuser cray-reds-group 3 auth md5 testpass1 priv des testpass2 snmp-server view cray-reds-view 1.3.6.1.2 included Mellanox Configuration SNMP is not needed on Mellanox switches.\n"
},
{
	"uri": "/docs-csm/en-09/408-mgmt-net-can-config/",
	"title": "Management Network CAN setup",
	"tags": [],
	"description": "",
	"content": "Management Network CAN setup Access from the customer site to the system over shared networks is known as the Customer Access Network (CAN)\nRequirements Access to switches\nSHCD\nConfiguration To access the Shasta nodes and services from the customer network, there is minimal configuration needed on the spine switch and the customer switch connected upstream from the spine switch to allow the customer_access_network subnet to be routed to the Shasta system.\nThe customer\u0026rsquo;s switch must be connected to the spine switches with a p2p subnet for each switch. In the example below, these two p2p subnets are 10.11.15.148/30 and 10.101.15.152/30. The subnets used are up to the customer.\nThere are two routes configured on the customer\u0026rsquo;s switch to route traffic for the CAN subnet to each of the spine switches. ECMP will load balance the traffic across each of the switches when both links are up and only use the active link when one of the links goes down.\nThe CAN is connected between each spine switch and the NCNs through vlan 7 running over the physical connections between the spines and the port on the NCN. This is the same physical connection used for the NMN and HMN on the NCNs.\nThe two physical connections between the NCN and spines is MLAG\u0026rsquo;ed. MAGP/VSX is used to provide a single virtual router gateway that can be used as the default route on each of the NCNs.\nThis is an example of the p2p configuration on the spine switches. The IP address should be replaced with the IP chosen by the customer matching the customer\u0026rsquo;s switch configuration.\nMellanox\ninterface ethernet 1/11 speed auto force interface ethernet 1/11 description to-can interface ethernet 1/11 no switchport force interface ethernet 1/11 ip address 10.101.15.150/30 primary Aruba\ninterface 1/1/36 no shutdown description to-can ip address 10.101.15.150/30 exit There must then be two routes on the customer\u0026rsquo;s switch directing traffic for the customer_access_network subnet to the endpoint on the spine switch.\nThis is an example of the route configuration on the customer switch. These addresses/subnets are generated from CSI and can be found in CAN.yaml Example Snippet from CAN.yaml.\n- full_name: CAN Bootstrap DHCP Subnet cidr: ip: 10.101.8.0 mask: - 255 - 255 - 255 - 0 Customer switch example config.\nip route vrf default 10.101.8.0/24 10.101.15.150 ip route vrf default 10.101.8.0/24 10.101.15.154 Going the other direction, there must be a default route on each spine switch directing traffic not matching other routes to the endpoint on the customer\u0026rsquo;s switch.\nThis is an example of the route configuration on sw-spine-001. Mellanox\nip route vrf default 0.0.0.0/0 10.101.15.149 Aruba\nip route 0.0.0.0/0 10.101.15.149 The spine switch must also have the customer_access_gateway IP address assigned to the vlan 7 interface on the switch. This provides a gateway for the default route on the NCNs and UANs as well as a direct route to the customer_access_network from the spine switch. Mellanox\ninterface vlan 7 ip address 10.101.8.2/26 primary Aruba\nsw-spine-002(config)# int vlan 7 sw-spine-002(config-if-vlan)# ip address 10.102.11.3/24 Verification of CAN Configuration After completing this configuration you should be able to ping and log in to all of the NCNs at the external CAN IP address from a device on the customer network.\nexternal\u0026gt; ping 10.101.8.6 PING 10.101.8.6 (10.101.8.6): 56 data bytes 64 bytes from 10.101.8.6: icmp_seq=0 ttl=58 time=61.445 ms 64 bytes from 10.101.8.6: icmp_seq=1 ttl=58 time=70.263 ms 64 bytes from 10.101.8.6: icmp_seq=2 ttl=58 time=59.270 ms ^C --- 10.101.8.6 ping statistics --- 3 packets transmitted, 3 packets received, 0.0% packet loss round-trip min/avg/max/stddev = 59.270/63.659/70.263/4.753 ms external\u0026gt; ssh root@10.101.8.6 The authenticity of host \u0026#39;10.101.8.6 (10.101.8.6)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:jnMGZnMcdPQ9QleyJADbI9AQAvo4DfGz0SOYbe3lraI. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added \u0026#39;10.101.8.6\u0026#39; (ECDSA) to the list of known hosts. Password: ncn-w001# "
},
{
	"uri": "/docs-csm/en-09/409-mgmt-net-firmware-update/",
	"title": "Management Network Firmware Update",
	"tags": [],
	"description": "",
	"content": "Management Network Firmware Update This page describes how to update firmware on the Management network switches.\nRequirements Access to the switches from the liveCD/M001\nConfiguration All firmware will be located at /var/www/fw/network on the LiveCD It should contain the following files. ncn-m001-pit:/var/www/network/firmware # ls -lh total 2.7G -rw-r--r-- 1 root root 614M Jan 15 18:57 ArubaOS-CX_6400-6300_10_05_0040.stable.swi -rw-r--r-- 1 root root 368M Jan 15 19:09 ArubaOS-CX_8320_10_05_0040.stable.swi -rw-r--r-- 1 root root 406M Jan 15 18:59 ArubaOS-CX_8325_10_05_0040.stable.swi -rw-r--r-- 1 root root 729M Aug 26 17:11 onyx-X86_64-3.9.1014.stable.img -rw-r--r-- 1 root root 577M Oct 28 11:45 OS10_Enterprise_10.5.1.4.stable.tar 1.4 Switch firmware.\nVendor Model Version Aruba 6300 ArubaOS-CX_6400-6300_10.06.0010 Aruba 8320 ArubaOS-CX_8320_10.06.0010 Aruba 8325 ArubaOS-CX_8325_10.06.0010 Aruba 8360 ArubaOS-CX_8360_10.06.0010 Dell S3048-ON 10.5.1.4 Dell S4148F-ON 10.5.1.4 Dell S4148T-ON 10.5.1.4 Mellanox MSN2100 3.9.1014 Mellanox MSN2700 3.9.1014 Aruba Firmware Update - standalone SSH into the switch you want to upgrade.\nExample: the IP 10.252.1.12 used is the liveCD\nsw-leaf-001# copy sftp://root@10.252.1.12//var/www/ephemeral/data/network_images/ArubaOS-CX_6400-6300_10_06_0010.stable.swi primary sw-leaf-001# write mem Copying configuration: [Success] Once the upload is complete you can check the images\nsw-leaf-001# show image --------------------------------------------------------------------------- ArubaOS-CX Primary Image --------------------------------------------------------------------------- Version : FL.10.06.0010 Size : 643 MB Date : 2020-12-14 10:06:34 PST SHA-256 : 78dc27c5e521e92560a182ca44dc04b60d222b9609129c93c1e329940e1e11f9 After the firmware is uploaded you will need to boot the switch to the correct image.\nsw-leaf-001# boot system primary Once the reboot is complete check and make sure the firmware version is correct.\nsw-leaf-001# show version ----------------------------------------------------------------------------- ArubaOS-CX (c) Copyright 2017-2020 Hewlett Packard Enterprise Development LP ----------------------------------------------------------------------------- Version : FL.10.06.0010 Build Date : 2020-09-29 07:44:16 PDT Build ID : ArubaOS-CX:FL.10.06.0010:3cbfcce60961:202009291304 Build SHA : 3cbfcce609617b0cf84a6b941a2b36c43dfeb2cb Active Image : primary Service OS Version : FL.01.07.0002 BIOS Version : FL.01.0002 Mellanox Firmware Update SSH into the switch you want to upgrade\nFetch the image from ncn-m001.\nsw-spine-001 [standalone: master] # image fetch http://10.252.1.4/fw/network/onyx-X86_64-3.9.1014.stable.img Install the image.\nsw-spine-001 [standalone: master] # image install onyx-X86_64-3.9.1014.stable.img Select the image to boot next.\nsw-spine-001 [standalone: master] # image boot next Write memory and reload.\nsw-spine-001 [standalone: master] # write memory sw-spine-001 [standalone: master] # reload Once the switch is available, verify the image is installed.\nsw-spine-001 [standalone: master] # show images Installed images: Partition 1: version: X86_64 3.9.0300 2020-02-26 19:25:24 x86_64 Partition 2: version: X86_64 3.9.1014 2020-08-05 18:06:58 x86_64 Last boot partition: 2 Next boot partition: 1 Images available to be installed: 1: Image : onyx-X86_64-3.9.1014.stable.img Version: X86_64 3.9.1014 2020-08-05 18:06:58 x86_64 Dell Firmware Update SSH into the switch you want to upgrade\nFetch the image from ncn-m001.\nsw-leaf-001# image install http://10.252.1.4/fw/network/OS10_Enterprise_10.5.1.4.stable.tar Check the image upload status.\nsw-leaf-001# show image status Image Upgrade State: download ================================================== File Transfer State: download -------------------------------------------------- State Detail: In progress Task Start: 2021-02-08T21:24:14Z Task End: 0000-00-00T00:00:00Z Transfer Progress: 7 % Transfer Bytes: 40949640 bytes File Size: 604119040 bytes Transfer Rate: 869 kbps Once the image is uploaded all that is left is a reboot.\nsw-leaf-001# write memory sw-leaf-001# reload Once the switch is available, verify the image is installed.\nsw-leaf-001# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.4 Build Version: 10.5.1.4.249 "
},
{
	"uri": "/docs-csm/en-09/410-mgmt-net-uplink-config/",
	"title": "Management Network Uplink configuration",
	"tags": [],
	"description": "",
	"content": "Management Network Uplink configuration This page describes how to configure switch to switch connections or uplinks between switches.\nRequirements Console access Aruba VSX configured on a pair of switches. Configuration The configuration below shows how to configure a Multi-chassis LAG on a pair of VSX switches. These connections will go to other network switches.\nVSX Pair configuration Create the multi-chassis LAG on the first switch.\nsw-24g03(config)# interface lag 100 multi-chassis sw-24g03(config-lag-if)# no shutdown sw-24g03(config-lag-if)# no routing sw-24g03(config-lag-if)# vlan trunk native 1 sw-24g03(config-lag-if)# vlan trunk allowed all sw-24g03(config-lag-if)# lacp mode active Create the multi-chassis LAG on the second switch.\nsw-24g04(config)# interface lag 100 multi-chassis sw-24g04(config-lag-if)# no shutdown sw-24g04(config-lag-if)# no routing sw-24g04(config-lag-if)# vlan trunk native 1 sw-24g04(config-lag-if)# vlan trunk allowed all sw-24g04(config-lag-if)# lacp mode active After creating the multi-chassis LAG we will need to add ports to the LAG.\nsw-24g03(config)# interface 1/1/48 sw-24g03(config-if)# no shutdown sw-24g03(config-if)# mtu 9198 sw-24g03(config-if)# lag 100 sw-24g03(config-if)# sw-24g04(config)# interface 1/1/48 sw-24g04(config-if)# no shutdown sw-24g04(config-if)# mtu 9198 sw-24g04(config-if)# lag 100 sw-24g04(config-if)# This configuration shows the how to setup the LAG on the access switch connecting to the VSX pair.\nsw-leaf-001(config)# interface lag 1 sw-leaf-001(config)# no shutdown sw-leaf-001(config)# no routing sw-leaf-001(config)# vlan trunk native 1 sw-leaf-001(config)# vlan trunk allowed all sw-leaf-001(config)# lacp mode active sw-leaf-001(config)# interface 1/1/48 sw-leaf-001(config-if)# no shutdown sw-leaf-001(config-if)# mtu 9198 sw-leaf-001(config-if)# lag 100 sw-leaf-001(config)# interface 1/1/49 sw-leaf-001(config-if)# no shutdown sw-leaf-001(config-if)# mtu 9198 sw-leaf-001(config-if)# lag 100 "
},
{
	"uri": "/docs-csm/en-09/411-mgmt-net-layer3-config/",
	"title": "Management Network Layer3 Configuration",
	"tags": [],
	"description": "",
	"content": "Management Network Layer3 Configuration This page describes how to configure layer 3 routing for Hill and Mountain cabinets.\nMountain cabinets have their own \u0026ldquo;CDU\u0026rdquo; Switches.\nHill cabinets are connected to the leaf switches.\nRequirements Access to all of the switches SHCD available Aruba Configuration At this point you should be able to ping the CDU switches on their VLAN 2 and VLAN 4 interfaces. We will need to setup routing so the compute nodes can communicate with k8s.\nSpine/Agg switch configuration\nFirst step is to start the OSPF process, give the switch a router-id. This is typically the NMN IP. We will need to redistribute BGP into OSPF, this will allow devices to communicate with K8s router ospf 1 router-id 10.252.0.3 redistribute bgp area 0.0.0.2 area 0.0.0.4 The OSPF peering will happen over VLAN 2 and VLAN 4.\ninterface vlan 2 ip ospf 1 area 0.0.0.2 interface vlan 4 ip ospf 1 area 0.0.0.4 The BGP config will need to be changed on these switches to avoid routing loops.\nrouter bgp 65533 distance bgp 85 70 CDU/Leaf switch Layer3 configuration\nThe ipv6 configuration is needed for CEC communication.\nrouter ospf 1 router-id 10.252.0.6 area 0.0.0.2 area 0.0.0.4 interface vlan 2 ip ospf 1 area 0.0.0.2 interface vlan 4 ip ospf 1 area 0.0.0.4 interface vlan 2000 ip ospf 1 area 0.0.0.2 ip ospf passive ipv6 address autoconfig interface vlan 3000 ip ospf 1 area 0.0.0.4 ip ospf passive ipv6 address autoconfig Once this is complete you should be able to see OSPF neighbors on the CDU/Leaf switches.\nsw-cdu-002# show ip ospf neighbors OSPF Process ID 1 VRF default ============================== Total Number of Neighbors: 6 Neighbor ID Priority State Nbr Address Interface ------------------------------------------------------------------------- 10.252.0.2 1 FULL/DROther 10.252.0.2 vlan2 10.252.0.3 1 FULL/DROther 10.252.0.3 vlan2 10.252.0.5 1 FULL/BDR 10.252.0.5 vlan2 10.252.0.2 1 FULL/DROther 10.254.0.2 vlan4 10.252.0.3 1 FULL/DROther 10.254.0.3 vlan4 10.252.0.5 1 FULL/BDR 10.254.0.5 vlan4 Aruba Static route This route is needed for consistent PXE booting on Aruba switches. The second IP 10.252.1.10 will be a worker node. Here we are using worker 1.\nip route 10.92.100.60/32 10.252.1.10 ip route 10.94.100.60/32 10.252.1.10 "
},
{
	"uri": "/docs-csm/en-09/412-mgmt-net-dell-mellanox-upgrades/",
	"title": "Management Network Dell And Mellanox Upgrades",
	"tags": [],
	"description": "",
	"content": "Management Network Dell And Mellanox Upgrades The Dell and Mellanox switches have some changes which are needed when moving from Shasta v1.3 to Shasta v1.4. This page is a guide to walk through the steps of upgrading a network to 1.4.\n1. Firmware Upgrade With Shasta v1.4 we are using the following firmware, FIRMWARE\n2. IP Address and Hostname Changes CSI will generate the IPs for the switches on a Shasta v1.4 system, they will be located here \u0026ldquo;/var/www/ephemeral/prep/{system-name}/networks\u0026rdquo; when ncn-m001 is booted from the LiveCD.\nHere is a snippet from NMN.yaml with the IP addresses and hostnames of the switches.\nip_reservations: - ip_address: 10.252.0.2 name: sw-spine-001 comment: x3000c0h41s1 aliases: [] - ip_address: 10.252.0.3 name: sw-spine-002 comment: x3000c0h41s2 aliases: [] - ip_address: 10.252.0.4 name: sw-leaf-001 comment: x3000c0w40 aliases: [] On most Shasta v1.3.x systems the IP addresses and hostnames will be as shown below, which will require them to be updated.\nspine-01 10.252.0.1 leaf-01 10.252.0.2 spine-02 10.252.0.3 To make the hostname and IP address changes for all switches, follow this procedure Management Network Switch Rename\n3. Dell Changes to switch from bpdufilter to bpduguard Remove spanning-tree bpdufilter Add spanning-tree bpduguard Shasta v1.3 (old) config\n!v1.3 config interface ethernet1/1/3 no shutdown switchport mode trunk switchport access vlan 1 switchport trunk allowed vlan 2,4,7,10 mtu 9216 flowcontrol receive on flowcontrol transmit off spanning-tree bpdufilter enable spanning-tree port type edge Shasta v1.4 (new) config\n!v1.4 config interface ethernet1/1/2 no shutdown switchport mode trunk switchport access vlan 1 switchport trunk allowed vlan 2,4,7,10 mtu 9216 flowcontrol receive on flowcontrol transmit off spanning-tree bpduguard enable spanning-tree port type edge Shasta v1.3 to v1.4 Delta\n!v1.3 to v1.4 changes configure terminal interface ethernet 1/1/x no spanning-tree bpdufilter spanning-tree bpduguard enable exit write memory Dell CDU changes Add BPDUguard to ports going to CMMs\ninterface port-channel1 description CMM_CAB_1000 no shutdown switchport mode trunk switchport access vlan 2000 switchport trunk allowed vlan 3000,4091 mtu 9216 vlt-port-channel 1 spanning-tree bpduguard enable Mellanox Changes for MAGP MAGP MAGP setup for mellanox spine switches, this should be set for every VLAN interface (1,2,4,7,10) https://community.mellanox.com/s/article/howto-configure-magp-on-mellanox-switches\nprotocol magp interface vlan 1 magp 1 interface vlan 2 magp 2 interface vlan 4 magp 4 interface vlan 7 magp 7 interface vlan 10 magp 10 interface vlan 1 magp 1 ip virtual-router address 10.1.0.1 interface vlan 2 magp 2 ip virtual-router address 10.252.0.1 interface vlan 4 magp 4 ip virtual-router address 10.254.0.1 interface vlan 7 magp 7 ip virtual-router address 10.103.8.20 interface vlan 10 magp 10 ip virtual-router address 10.11.0.1 interface vlan 1 magp 1 ip virtual-router mac-address 00:00:5E:00:01:01 interface vlan 2 magp 2 ip virtual-router mac-address 00:00:5E:00:01:02 interface vlan 4 magp 4 ip virtual-router mac-address 00:00:5E:00:01:04 interface vlan 7 magp 7 ip virtual-router mac-address 00:00:5E:00:01:07 interface vlan 10 magp 10 ip virtual-router mac-address 00:00:5E:00:01:10 Output of a working MAGP\nsw-spine-001 [standalone: master] # show magp MAGP 1: Interface vlan: 1 Admin state : Enabled State : Master Virtual IP : 10.1.0.1 Virtual MAC : 00:00:5E:00:01:01 MAGP 2: Interface vlan: 2 Admin state : Enabled State : Master Virtual IP : 10.252.0.1 Virtual MAC : 00:00:5E:00:01:02 MAGP 4: Interface vlan: 4 Admin state : Enabled State : Master Virtual IP : 10.254.0.1 Virtual MAC : 00:00:5E:00:01:04 MAGP 7: Interface vlan: 7 Admin state : Enabled State : Master Virtual IP : 10.103.8.20 Virtual MAC : 00:00:5E:00:01:07 MAGP 10: Interface vlan: 10 Admin state : Enabled State : Master Virtual IP : 10.11.0.1 Virtual MAC : 00:00:5E:00:01:10 MLAG Check if MLAG is setup already.\nsw-spine-002 [standalone: master] # show mlag Admin status: Enabled Operational status: Up Reload-delay: 30 sec Keepalive-interval: 1 sec Upgrade-timeout: 60 min System-mac: 00:00:5E:00:01:01 MLAG Ports Configuration Summary: Configured: 15 Disabled: 0 Enabled: 15 MLAG Ports Status Summary: Inactive: 0 Active-partial: 1 Active-full: 14 MLAG IPLs Summary: ----------------------------------------------------------------------------------------------------------------------------------------------------------------- ID Group Vlan Operational Local Peer Up Time Toggle Counter Port-Channel Interface State IP address IP address ----------------------------------------------------------------------------------------------------------------------------------------------------------------- 1 Po100 4000 Up 192.168.255.253 192.168.255.254 19 days, 18:25:56 1 MLAG Members Summary: --------------------------------------------------------------------- System-id State Hostname --------------------------------------------------------------------- 50:6B:4B:9C:C6:48 Up \u0026lt;sw-spine-002\u0026gt; 98:03:9B:EF:D6:48 Up If output looks like the above, MLAG is already setup. If MLAG needs to be setup on the system follow these steps. Most 1.3 systems will have this already configured.\nSpine01 (config) # protocol mlag (config) # interface port-channel 100 (config) # interface ethernet 1/14 channel-group 100 mode active (config) # interface ethernet 1/13 channel-group 100 mode active (config) # interface ethernet 1/13 dcb priority-flow-control mode on force (config) # interface ethernet 1/14 dcb priority-flow-control mode on force (config) # vlan 4000 (config) # interface vlan 4000 (config) # interface port-channel 100 ipl 1 (config) # interface port-channel 100 dcb priority-flow-control mode on force (config interface vlan 4000) # ip address 192.168.255.254 255.255.255.252 (config interface vlan 4000) # ipl 1 peer-address 192.168.255.253 (config) # mlag system-mac 00:00:5E:00:01:5D (config) # no mlag shutdown Spine02 (config) # protocol mlag (config) # interface port-channel 100 (config) # interface ethernet 1/14 channel-group 100 mode active (config) # interface ethernet 1/13 channel-group 100 mode active (config) # interface ethernet 1/13 dcb priority-flow-control mode on force (config) # interface ethernet 1/14 dcb priority-flow-control mode on force (config) # vlan 4000 (config) # interface vlan 4000 (config) # interface port-channel 100 ipl 1 (config) # interface port-channel 100 dcb priority-flow-control mode on force (config interface vlan 4000) # ip address 192.168.255.253 255.255.255.252 (config interface vlan 4000) # ipl 1 peer-address 192.168.255.254 (config) # mlag system-mac 00:00:5E:00:01:5D (config) # no mlag shutdown Adding MLAG ports (these ports go to NCNs)\nSpine01 (config) # int mlag-port-channel 1 (config interface mlag-port-channel 1) # mtu 9216 force (config interface mlag-port-channel 1) # switchport mode hybrid (config interface mlag-port-channel 1) # no shutdown (config interface mlag-port-channel 1) # lacp-individual enable force (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 2 (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 4 (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 7 (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 10 Spine02 NOTE: \u0026rsquo;lacp fallback\u0026rsquo; is only on one of the Spines. Disable \u0026ldquo;lacp-individual enable force\u0026rdquo; on Spine02, if it was set previously.\n(config) # int mlag-port-channel 1 (config interface mlag-port-channel 1) # mtu 9216 force (config interface mlag-port-channel 1) # switchport mode hybrid (config interface mlag-port-channel 1) # no shutdown (config interface mlag-port-channel 1) # no lacp-individual enable force (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 2 (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 4 (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 7 (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 10 Once you create the MLAG you need to add ports to it.\n(config) # interface ethernet 1/1 (config interface ethernet 1/1) # mlag-channel-group 1 mode active (config interface ethernet 1/1) # interface ethernet 1/1 speed 40G force (config interface ethernet 1/1) # interface ethernet 1/1 mtu 9216 force Configuration with Recommended MLAG-VIP cable.\nThis is recommended by Mellanox but not required. Its purpose is to prevent \u0026ldquo;split brain\u0026rdquo; which is where both spines think they are the active gateway. It requires an RJ45 cable between the mgmt0 ports on both switches. https://community.mellanox.com/s/article/how-to-configure-mlag-on-mellanox-switches#jive_content_id_MLAG_VIP Spine01 no interface mgmt0 dhcp interface mgmt0 ip address 192.168.255.241 /29 no mlag shutdown mlag system-mac 00:00:5E:00:01:5D mlag-vip rocket-mlag-domain ip 192.168.255.242 /29 force Spine02 no interface mgmt0 dhcp interface mgmt0 ip address 192.168.255.243 /29 no mlag shutdown mlag system-mac 00:00:5E:00:01:5D mlag-vip rocket-mlag-domain ip 192.168.255.242 /29 force Verifying mlag-vip\nsw-spine-001 [rocket-mlag-domain: master] # show mlag-vip MLAG-VIP: MLAG group name: rocket-mlag-domain MLAG VIP address: 192.168.255.242/29 Active nodes: 2 ---------------------------------------------------------------------------------- Hostname VIP-State IP Address ---------------------------------------------------------------------------------- sw-spine-001 master 192.168.255.241 sw-spine-002 standby 192.168.255.243 Update SNMP configuration There have been no changes to SNMP in 1.4. You can verify the settings here. See SNMP\nUpdate CAN configuration Some systems may have had many switches on the CAN with Shasta v1.3. In Shasta v1.4, only the spine switches should be on the CAN. Other switches should remove their IP addresses on vlan7.\nSee CAN\nUpdate NTP configuration Some Shasta v1.3 systems may have set the switch ntp server to be the IP address of ncn-w001. With the switch rename, the old IP address for ncn-w001 may now be assigned to one of the switches. The Shasta v1.4 configuration sets the switches to have the first three worker nodes as their ntp servers.\nSee NTP\nVerify flow-control settings With Shasta v1.3.2, some changes were made for the flow-control settings which may not be on Shasta v1.3.0 systems. Verify that these are set correctly for Shasta v1.4. These changes for flow-control will also disable iSCSI on Dell Switches (Leaf, CDU, and Aggregation).\nSee Flow Control https://connect.us.cray.com/confluence/display/SSI/Management+Network+Changes+for+Shasta+1.3.2\nUpdate DHCP IP helper configuration With Shasta v1.3.2, some changes were made for the ip-helper settings which may not be on Shasta v1.3.0 systems. Verify that these are set correctly for Shasta v1.4. The IP-helpers are being moved for the switches that are doing the Layer3 Routing. For most systems this will be moving the helper from the leaf to the spine. Also the IP-helpers are being added on VLAN1 and VLAN7 to PXE boot NCNs.\nSee IP-Helper\nhttps://connect.us.cray.com/confluence/display/SSI/Management+Network+Changes+for+Shasta+1.3.2\nVerify Spanning-Tree settings Spanning tree configuration has not changed on Dell and Mellanox switches from 1.3 to 1.4\nSee Management Network Spanning-Tree Configuration.\nUpdate Management Network ACL settings These ACLs are designed to block traffic from the node management network to and from the hardware management network. See Management Network ACL configuration.\nVerify My Dell/Mellanox system is 1.4 compliant. Make sure firmware is up to date. Change IP addresses of switches accordingly. Verify Dell BPDU configuration. Verify MLAG is setup. Verify MAGP is setup for ALL vlans. Verify NTP configuration is updated. Verify flow-control settings. Verify DHCP IP-Helper settings. Verify Spanning-Tree settings. Verify ACL settings. "
},
{
	"uri": "/docs-csm/en-09/413-mgmt-net-example-config/",
	"title": "Management Network Example Config",
	"tags": [],
	"description": "",
	"content": "Management Network Example Config The following example configs are from a TDS system with one Hill cabinet.\nThis network architecture include two Aruba 8325s that are configured as VSX/MC-LAG pair, one Aruba 6300, and two Aruba 8360s for the CDU switches.\nFirst 8325\nsw-spine-001# show run Current configuration: ! !Version ArubaOS-CX GL.10.05.0020 !export-password: default hostname sw-spine-001 allow-unsupported-transceiver user admin group administrators password ciphertext AQBapa no ip icmp redirect debug bgp all vrf keepalive ntp server 10.254.0.8 ! ! ! ssh server vrf default ssh server vrf mgmt access-list ip nmn-hmn 10 deny any 10.252.0.0/255.255.128.0 10.254.0.0/255.255.128.0 20 deny any 10.252.0.0/255.255.128.0 10.104.0.0/255.252.0.0 30 deny any 10.254.0.0/255.255.128.0 10.252.0.0/255.255.128.0 40 deny any 10.254.0.0/255.255.128.0 10.100.0.0/255.252.0.0 50 deny any 10.100.0.0/255.252.0.0 10.254.0.0/255.255.128.0 60 deny any 10.100.0.0/255.252.0.0 10.104.0.0/255.252.0.0 70 deny any 10.104.0.0/255.252.0.0 10.252.0.0/255.255.128.0 80 deny any 10.104.0.0/255.252.0.0 10.100.0.0/255.252.0.0 90 permit any any any vlan 1 vlan 2 name RVR_NMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 4 name RVR_HMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 7 name CAN vlan 10 name SUN spanning-tree mode rpvst spanning-tree spanning-tree priority 7 spanning-tree bpdu-guard timeout 30 spanning-tree vlan 1,2,4,7,10 interface mgmt shutdown ip dhcp system interface-group 3 speed 10g !interface group 3 contains ports 1/1/25-1/1/36 interface lag 1 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 2 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 3 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 4 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 5 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 6 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 7 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 8 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 9 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 10 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 11 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 12 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 13 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 14 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 99 no shutdown description ISL link no routing vlan trunk native 1 tag vlan trunk allowed all lacp mode active interface lag 100 multi-chassis no shutdown description leaf-VSX-1 no routing vlan trunk native 1 vlan trunk allowed all lacp mode active interface lag 149 multi-chassis no shutdown description cdu0-vsx no routing vlan trunk native 1 vlan trunk allowed 1-2,4 lacp mode active interface 1/1/1 no shutdown mtu 9198 lag 1 interface 1/1/2 mtu 9198 interface 1/1/3 no shutdown mtu 9198 lag 2 interface 1/1/4 mtu 9198 interface 1/1/5 no shutdown mtu 9198 lag 3 interface 1/1/6 mtu 9198 interface 1/1/7 no shutdown mtu 9198 lag 4 interface 1/1/8 no shutdown mtu 9198 lag 5 interface 1/1/9 no shutdown mtu 9198 lag 6 interface 1/1/10 no shutdown mtu 9198 lag 7 interface 1/1/11 no shutdown mtu 9198 lag 8 interface 1/1/12 no shutdown mtu 9198 lag 9 interface 1/1/13 no shutdown mtu 9198 lag 10 interface 1/1/14 no shutdown mtu 9198 lag 11 interface 1/1/15 no shutdown mtu 9198 lag 12 interface 1/1/16 lag 13 interface 1/1/17 lag 14 interface 1/1/18 no routing vlan access 1 interface 1/1/19 no routing vlan access 1 interface 1/1/20 no routing vlan access 1 interface 1/1/21 no routing vlan access 1 interface 1/1/22 no routing vlan access 1 interface 1/1/23 no routing vlan access 1 interface 1/1/24 no routing vlan access 1 interface 1/1/25 no routing vlan access 1 interface 1/1/26 no routing vlan access 1 interface 1/1/27 no routing vlan access 1 interface 1/1/28 no routing vlan access 1 interface 1/1/29 no routing vlan access 1 interface 1/1/30 no routing vlan access 1 interface 1/1/31 no routing vlan access 1 interface 1/1/32 no routing vlan access 1 interface 1/1/33 no routing vlan access 1 interface 1/1/34 no routing vlan access 1 interface 1/1/35 no routing vlan access 1 interface 1/1/36 no shutdown ip address 10.102.255.78/30 interface 1/1/37 no routing vlan access 1 interface 1/1/38 no routing vlan access 1 interface 1/1/39 no routing vlan access 1 interface 1/1/40 no routing vlan access 1 interface 1/1/41 no routing vlan access 1 interface 1/1/42 no routing vlan access 1 interface 1/1/43 no routing vlan access 1 interface 1/1/44 no routing vlan access 1 interface 1/1/45 no routing vlan access 1 interface 1/1/46 no routing vlan access 1 interface 1/1/47 no shutdown mtu 9198 vrf attach keepalive description VSX keepalive ip address 192.168.255.0/31 interface 1/1/48 no shutdown mtu 9198 description ags01_49 lag 100 interface 1/1/49 no shutdown mtu 9198 description cdu0sw lag 149 interface 1/1/50 no shutdown mtu 9198 description cdu0sw lag 149 interface 1/1/51 no shutdown mtu 9198 lag 99 interface 1/1/52 no shutdown mtu 9198 lag 99 interface 1/1/53 no routing vlan access 1 interface 1/1/54 no routing vlan access 1 interface 1/1/55 no routing vlan access 1 interface 1/1/56 no routing vlan access 1 interface loopback 1 interface vlan 1 vsx-sync active-gateways ip mtu 9198 ip address 10.1.0.2/16 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.1.0.1 ip helper-address 10.92.100.222 interface vlan 2 vsx-sync active-gateways ip mtu 9198 ip address 10.252.0.2/17 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.252.0.1 ip ospf 1 area 0.0.0.2 interface vlan 4 vsx-sync active-gateways ip mtu 9198 ip address 10.254.0.2/17 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.254.0.1 ip ospf 1 area 0.0.0.4 interface vlan 7 vsx-sync active-gateways ip mtu 9198 ip address 10.102.11.1/24 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.102.11.111 ip helper-address 10.92.100.222 interface vlan 10 ip address 10.11.0.1/16 vsx system-mac 02:01:00:00:01:00 inter-switch-link lag 99 role primary keepalive peer 192.168.255.1 source 192.168.255.0 vrf keepalive linkup-delay-timer 600 vsx-sync vsx-global ip route 0.0.0.0/0 10.102.255.77 ip prefix-list pl-can seq 10 permit 10.102.11.0/24 ge 24 ip prefix-list pl-hmn seq 20 permit 10.94.100.0/24 ge 24 ip prefix-list pl-nmn seq 30 permit 10.92.100.0/24 ge 24 ! ! ! ! route-map ncn-w001 permit seq 10 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.9 route-map ncn-w001 permit seq 20 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.15 route-map ncn-w001 permit seq 30 match ip address prefix-list pl-can set ip next-hop 10.102.11.11 route-map ncn-w002 permit seq 10 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.8 route-map ncn-w002 permit seq 20 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.13 route-map ncn-w002 permit seq 30 match ip address prefix-list pl-can set ip next-hop 10.102.11.10 route-map ncn-w003 permit seq 10 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.7 route-map ncn-w003 permit seq 20 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.11 route-map ncn-w003 permit seq 30 match ip address prefix-list pl-can set ip next-hop 10.102.11.9 ! router ospf 1 router-id 10.252.0.2 redistribute bgp area 0.0.0.2 area 0.0.0.4 router bgp 65533 bgp router-id 10.252.0.2 maximum-paths 8 distance bgp 85 70 neighbor 10.252.0.3 remote-as 65533 neighbor 10.252.1.7 remote-as 65533 neighbor 10.252.1.8 remote-as 65533 neighbor 10.252.1.9 remote-as 65533 address-family ipv4 unicast neighbor 10.252.0.3 activate neighbor 10.252.1.7 activate neighbor 10.252.1.7 route-map ncn-w003 in neighbor 10.252.1.8 activate neighbor 10.252.1.8 route-map ncn-w002 in neighbor 10.252.1.9 activate neighbor 10.252.1.9 route-map ncn-w001 in exit-address-family ! https-server vrf default https-server vrf mgmt Second 8325\nsw-spine-002# show run Current configuration: ! !Version ArubaOS-CX GL.10.05.0020 !export-password: default hostname sw-spine-002 allow-unsupported-transceiver user admin group administrators password ciphertext AQBapWcbqh2GB9yAT6oln21BOY+3jKy2nth07vZLpzNwXNBVYgAAADGyXE3TJ7+ez0DzF/NNBCsaMXTyBJgqvtIvLd907Jr2JCIB9xgJ0R4qhp4Mf24L7aMJ0rXZ0DqDFS3vvz5aZ4Cj2wVu4h4kt/JV6RBpSk/j3QPSCCpj85BMUaSK11ECjXRM no ip icmp redirect debug lag all vrf keepalive ntp server 10.254.0.8 ! ! ! ssh server vrf default ssh server vrf mgmt access-list ip nmn-hmn 10 deny any 10.252.0.0/255.255.128.0 10.254.0.0/255.255.128.0 20 deny any 10.252.0.0/255.255.128.0 10.104.0.0/255.252.0.0 30 deny any 10.254.0.0/255.255.128.0 10.252.0.0/255.255.128.0 40 deny any 10.254.0.0/255.255.128.0 10.100.0.0/255.252.0.0 50 deny any 10.100.0.0/255.252.0.0 10.254.0.0/255.255.128.0 60 deny any 10.100.0.0/255.252.0.0 10.104.0.0/255.252.0.0 70 deny any 10.104.0.0/255.252.0.0 10.252.0.0/255.255.128.0 80 deny any 10.104.0.0/255.252.0.0 10.100.0.0/255.252.0.0 90 permit any any any vlan 1 vlan 2 name RVR_NMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 4 name RVR_HMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 7 name CAN vlan 10 name SUN spanning-tree mode rpvst spanning-tree spanning-tree priority 7 spanning-tree bpdu-guard timeout 30 spanning-tree vlan 1,2,4,7,10 interface mgmt shutdown ip dhcp system interface-group 3 speed 10g !interface group 3 contains ports 1/1/25-1/1/36 interface lag 1 multi-chassis no shutdown description CMM_CAB_1000 no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 2 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 3 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 4 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 5 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 6 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 7 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 8 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 9 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 10 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 11 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 12 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 13 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 14 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface lag 99 no shutdown description ISL trunk no routing vlan trunk native 1 tag vlan trunk allowed all lacp mode active interface lag 100 multi-chassis no shutdown description Leaf-VSX-1 no routing vlan trunk native 1 vlan trunk allowed all lacp mode active interface lag 149 multi-chassis no shutdown description cdu0-vsx no routing vlan trunk native 1 vlan trunk allowed 1-2,4 lacp mode active interface 1/1/1 no shutdown mtu 9198 lag 1 interface 1/1/2 mtu 9198 interface 1/1/3 no shutdown mtu 9198 lag 2 interface 1/1/4 mtu 9198 interface 1/1/5 no shutdown mtu 9198 lag 3 interface 1/1/6 mtu 9198 interface 1/1/7 no shutdown mtu 9198 lag 4 interface 1/1/8 no shutdown mtu 9198 lag 5 interface 1/1/9 no shutdown mtu 9198 lag 6 interface 1/1/10 no shutdown mtu 9198 lag 7 interface 1/1/11 no shutdown mtu 9198 lag 8 interface 1/1/12 no shutdown mtu 9198 lag 9 interface 1/1/13 no shutdown mtu 9198 lag 10 interface 1/1/14 no shutdown mtu 9198 lag 11 interface 1/1/15 no shutdown mtu 9198 lag 12 interface 1/1/16 lag 13 interface 1/1/17 lag 14 interface 1/1/18 no routing vlan access 1 interface 1/1/19 no routing vlan access 1 interface 1/1/20 no routing vlan access 1 interface 1/1/21 no routing vlan access 1 interface 1/1/22 no routing vlan access 1 interface 1/1/23 no routing vlan access 1 interface 1/1/24 no routing vlan access 1 interface 1/1/25 no routing vlan access 1 interface 1/1/26 no routing vlan access 1 interface 1/1/27 no routing vlan access 1 interface 1/1/28 no routing vlan access 1 interface 1/1/29 no routing vlan access 1 interface 1/1/30 no routing vlan access 1 interface 1/1/31 no routing vlan access 1 interface 1/1/32 no routing vlan access 1 interface 1/1/33 no routing vlan access 1 interface 1/1/34 no routing vlan access 1 interface 1/1/35 no routing vlan access 1 interface 1/1/36 no shutdown ip address 10.102.255.82/30 interface 1/1/37 no routing vlan access 1 interface 1/1/38 no routing vlan access 1 interface 1/1/39 no routing vlan access 1 interface 1/1/40 no routing vlan access 1 interface 1/1/41 no routing vlan access 1 interface 1/1/42 no routing vlan access 1 interface 1/1/43 no routing vlan access 1 interface 1/1/44 no routing vlan access 1 interface 1/1/45 no routing vlan access 1 interface 1/1/46 no routing vlan access 1 interface 1/1/47 no shutdown mtu 9198 vrf attach keepalive description VSX keepalive ip address 192.168.255.1/31 interface 1/1/48 no shutdown mtu 9198 description ags01_49 lag 100 interface 1/1/49 no shutdown mtu 9198 description cdu0sw lag 149 interface 1/1/50 no shutdown mtu 9198 description cdu0sw lag 149 interface 1/1/51 no shutdown mtu 9198 lag 99 interface 1/1/52 no shutdown mtu 9198 lag 99 interface 1/1/53 no routing vlan access 1 interface 1/1/54 no routing vlan access 1 interface 1/1/55 no routing vlan access 1 interface 1/1/56 no routing vlan access 1 interface loopback 1 interface vlan 1 vsx-sync active-gateways ip mtu 9198 ip address 10.1.0.3/16 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.1.0.1 interface vlan 2 vsx-sync active-gateways ip mtu 9198 ip address 10.252.0.3/17 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.252.0.1 ip ospf 1 area 0.0.0.2 interface vlan 4 vsx-sync active-gateways ip mtu 9198 ip address 10.254.0.3/17 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.254.0.1 ip ospf 1 area 0.0.0.4 interface vlan 7 vsx-sync active-gateways ip mtu 9198 ip address 10.102.11.3/24 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.102.11.111 vsx system-mac 02:01:00:00:01:00 inter-switch-link lag 99 role secondary keepalive peer 192.168.255.0 source 192.168.255.1 vrf keepalive linkup-delay-timer 600 vsx-sync vsx-global ip route 0.0.0.0/0 10.102.255.81 distance 5 ip prefix-list pl-can seq 10 permit 10.102.11.0/24 ge 24 ip prefix-list pl-hmn seq 20 permit 10.94.100.0/24 ge 24 ip prefix-list pl-nmn seq 30 permit 10.92.100.0/24 ge 24 ! ! ! ! route-map ncn-w001 permit seq 10 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.9 route-map ncn-w001 permit seq 20 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.15 route-map ncn-w001 permit seq 30 match ip address prefix-list pl-can set ip next-hop 10.102.11.11 route-map ncn-w002 permit seq 10 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.8 route-map ncn-w002 permit seq 20 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.13 route-map ncn-w002 permit seq 30 match ip address prefix-list pl-can set ip next-hop 10.102.11.10 route-map ncn-w003 permit seq 10 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.7 route-map ncn-w003 permit seq 20 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.11 route-map ncn-w003 permit seq 30 match ip address prefix-list pl-can set ip next-hop 10.102.11.9 ! router ospf 1 router-id 10.252.0.3 redistribute bgp area 0.0.0.2 area 0.0.0.4 router bgp 65533 bgp router-id 10.252.0.3 maximum-paths 8 distance bgp 85 70 neighbor 10.252.0.2 remote-as 65533 neighbor 10.252.1.7 remote-as 65533 neighbor 10.252.1.8 remote-as 65533 neighbor 10.252.1.9 remote-as 65533 address-family ipv4 unicast neighbor 10.252.0.2 activate neighbor 10.252.1.7 activate neighbor 10.252.1.7 route-map ncn-w003 in neighbor 10.252.1.8 activate neighbor 10.252.1.8 route-map ncn-w002 in neighbor 10.252.1.9 activate neighbor 10.252.1.9 route-map ncn-w001 in exit-address-family ! https-server vrf default https-server vrf mgmt The 6300\nsw-leaf-001# show run Current configuration: ! !Version ArubaOS-CX FL.10.05.0040 !export-password: default hostname sw-leaf-001 user admin group administrators password ciphertext AQBapTQZPv ntp server 10.254.0.8 ! ! ! ! ssh server vrf default ssh server vrf mgmt vsf member 1 type jl663a vlan 1 vlan 2 name RVR_NMN vlan 4 name RVR_HMN vlan 7 name CAN vlan 10 name SUN spanning-tree mode rpvst spanning-tree spanning-tree bpdu-guard timeout 30 spanning-tree vlan 1,2,4,7,10 interface mgmt no shutdown ip dhcp interface lag 1 no shutdown no routing vlan trunk native 1 vlan trunk allowed all lacp mode active interface 1/1/1 no shutdown mtu 9198 description UAN no routing vlan trunk native 2 vlan trunk allowed 7 interface 1/1/2 no shutdown mtu 9198 description NMN no routing vlan access 2 interface 1/1/3 no shutdown mtu 9198 description NMN no routing vlan access 2 interface 1/1/4 no shutdown mtu 9198 description NMN no routing vlan access 2 interface 1/1/5 no shutdown mtu 9198 description NMN no routing vlan access 2 interface 1/1/6 no shutdown mtu 9198 description NMN no routing vlan access 2 interface 1/1/7 no shutdown no routing vlan access 1 interface 1/1/8 no shutdown no routing vlan access 1 interface 1/1/9 no shutdown no routing vlan access 1 interface 1/1/10 no shutdown no routing vlan access 1 interface 1/1/11 no shutdown no routing vlan access 1 interface 1/1/12 no shutdown no routing vlan access 1 interface 1/1/13 no shutdown no routing vlan access 1 interface 1/1/14 no shutdown no routing vlan access 1 interface 1/1/15 no shutdown no routing vlan access 1 interface 1/1/16 no shutdown no routing vlan access 1 interface 1/1/17 no shutdown no routing vlan access 1 interface 1/1/18 no shutdown no routing vlan access 1 interface 1/1/19 no shutdown no routing vlan access 1 interface 1/1/20 no shutdown no routing vlan access 1 interface 1/1/21 no shutdown no routing vlan access 1 interface 1/1/22 no shutdown no routing vlan access 1 interface 1/1/23 no shutdown no routing vlan access 1 interface 1/1/24 no shutdown no routing vlan access 1 interface 1/1/25 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/26 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/27 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/28 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/29 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/30 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/31 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/32 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/33 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/34 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/35 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/36 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/37 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/38 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/39 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/40 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/41 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/42 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/43 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/44 no shutdown no routing vlan access 1 interface 1/1/45 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/46 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/47 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/48 no shutdown no routing vlan access 1 interface 1/1/49 no shutdown mtu 9198 description ncn-core1_48 lag 1 interface 1/1/50 no shutdown mtu 9198 description ncn-core2_48 lag 1 interface 1/1/51 shutdown no routing vlan access 1 interface 1/1/52 shutdown no routing vlan access 1 interface vlan 1 ip address 10.1.0.4/16 no ip dhcp interface vlan 2 description RIVER_NMN ip address 10.252.0.4/17 ip helper-address 10.92.100.222 interface vlan 4 description RIVER_HMN ip address 10.254.0.4/17 ip helper-address 10.94.100.222 interface vlan 7 description CAN ip address 10.102.11.2/24 snmp-server vrf default snmp-server system-contact \u0026#34;Contact Cray Global Technical Services (C.G.T.S.)\u0026#34; snmpv3 user testuser auth md5 auth-pass ciphertext AQBapflTKYh28GLx4x7Bp5XyAT0j2jnm9fDMNei1tR+BTyrqCQAAAITcQ4YsQX2noQ== priv des priv-pass ciphertext AQBapaNP67WbY49eqp0jL27tInN1FeAD9TjgkcbW31S85/SBCQAAAP6e+534mdJiaA== ip route 0.0.0.0/0 10.102.11.111 ! ! ! ! ! https-server vrf default https-server vrf mgmt First 8360 CDU switch\nsw-cdu-001# show run Current configuration: ! !Version ArubaOS-CX LL.10.06.0001 !export-password: default hostname sw-cdu-001 user admin group administrators password ciphertext AQBapT3gxulv3VzyyLcGaKF1fZMtSAMoJzls0b2ojfJ0k9srYgAAABWZvTs5PSgj1JZenpQTO+zoKnbHsI5UaT6QSNOws1+jMSIoDAgunDoWkaRtCLkC2jQKdruZo3il1ESdzS4JNy9JmZqT9jB4QJXSl0nTJEZywQFaii7xjPZwW3UdyZPPNfMP vrf keepalive ntp server 10.254.0.8 ntp enable ! ! ! ssh server vrf default ssh server vrf mgmt access-list ip nmn-hmn 10 deny any 10.252.0.0/255.255.128.0 10.254.0.0/255.255.128.0 20 deny any 10.252.0.0/255.255.128.0 10.104.0.0/255.252.0.0 30 deny any 10.254.0.0/255.255.128.0 10.252.0.0/255.255.128.0 40 deny any 10.254.0.0/255.255.128.0 10.100.0.0/255.252.0.0 50 deny any 10.100.0.0/255.252.0.0 10.254.0.0/255.255.128.0 60 deny any 10.100.0.0/255.252.0.0 10.104.0.0/255.252.0.0 70 deny any 10.104.0.0/255.252.0.0 10.252.0.0/255.255.128.0 80 deny any 10.104.0.0/255.252.0.0 10.100.0.0/255.252.0.0 90 permit any any any vlan 1 vlan 2 name RVR_NMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 4 name RVR_HMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 2000 name CAB_1000_MTN_NMN description CAB_1000_MTN_NMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 3000 name CAB_1000_MTN_HMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 4091 name CMM_RECOVERY spanning-tree mode rpvst spanning-tree spanning-tree priority 11 spanning-tree bpdu-guard timeout 30 spanning-tree vlan 1,2,4,2000,3000,4091 interface mgmt shutdown ip dhcp interface lag 2 multi-chassis vsx-sync vlans no shutdown description CMM_CAB_1000 no routing vlan trunk native 2000 vlan trunk allowed 2000,3000,4091 lacp mode active lacp fallback interface lag 3 multi-chassis vsx-sync vlans no shutdown description CMM_CAB_1000 no routing vlan trunk native 2000 vlan trunk allowed 2000,3000,4091 lacp mode active lacp fallback interface lag 99 no shutdown description ISL link no routing vlan trunk native 1 tag vlan trunk allowed all lacp mode active interface lag 149 multi-chassis no shutdown description sw-spine no routing vlan trunk native 1 vlan trunk allowed 1-2,4 lacp mode active interface 1/1/1 no shutdown mtu 9198 description cec1 no routing vlan access 3000 spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/2 no shutdown description cmm1 lag 2 interface 1/1/3 no shutdown description cmm3 lag 3 interface 1/1/48 no shutdown mtu 9198 vrf attach keepalive description VSX keepalive ip address 192.168.255.0/31 interface 1/1/49 no shutdown description sw-spine-001_49 lag 149 interface 1/1/50 no shutdown description sw-spine-002_49 lag 149 interface 1/1/51 no shutdown mtu 9198 lag 99 interface 1/1/52 no shutdown mtu 9198 lag 99 interface vlan 1 description MGMT ip mtu 9198 ip address 10.1.0.5/16 interface vlan 2 ip mtu 9198 ip address 10.252.0.5/17 ip ospf 1 area 0.0.0.2 interface vlan 4 ip mtu 9198 ip address 10.254.0.5/17 ip ospf 1 area 0.0.0.4 interface vlan 2000 vsx-sync active-gateways description CAB_1000_MTN_NMN ip address 10.100.3.252/22 active-gateway ip mac 02:01:00:00:01:02 active-gateway ip 10.100.3.254 ip helper-address 10.92.100.222 ip ospf 1 area 0.0.0.2 ip ospf passive interface vlan 3000 vsx-sync active-gateways description CAB_1000_MTN_HMN ip address 10.104.3.252/22 active-gateway ip mac 02:01:00:00:01:02 active-gateway ip 10.104.3.254 ip helper-address 10.94.100.222 ip ospf 1 area 0.0.0.4 ip ospf passive vsx system-mac 02:01:00:00:01:02 inter-switch-link lag 99 role primary keepalive peer 192.168.255.1 source 192.168.255.0 vrf keepalive linkup-delay-timer 600 vsx-sync vsx-global ! ! ! ! ! router ospf 1 router-id 10.252.0.5 area 0.0.0.2 area 0.0.0.4 https-server vrf default https-server vrf mgmt Second 8360 CDU switch\nsw-cdu02# show run Current configuration: ! !Version ArubaOS-CX LL.10.06.0001 !export-password: default hostname sw-cdu-002 user admin group administrators password ciphertext AQBap vrf keepalive ntp server 10.254.0.8 ntp enable ! ! ! ssh server vrf default ssh server vrf mgmt access-list ip nmn-hmn 10 deny any 10.252.0.0/255.255.128.0 10.254.0.0/255.255.128.0 20 deny any 10.252.0.0/255.255.128.0 10.104.0.0/255.252.0.0 30 deny any 10.254.0.0/255.255.128.0 10.252.0.0/255.255.128.0 40 deny any 10.254.0.0/255.255.128.0 10.100.0.0/255.252.0.0 50 deny any 10.100.0.0/255.252.0.0 10.254.0.0/255.255.128.0 60 deny any 10.100.0.0/255.252.0.0 10.104.0.0/255.252.0.0 70 deny any 10.104.0.0/255.252.0.0 10.252.0.0/255.255.128.0 80 deny any 10.104.0.0/255.252.0.0 10.100.0.0/255.252.0.0 90 permit any any any vlan 1 vlan 2 name RVR_NMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 4 name RVR_HMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 2000 name CAB_1000_MTN_NMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 3000 name CAB_1000_MTN_HMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 4091 name CMM_RECOVERY spanning-tree mode rpvst spanning-tree spanning-tree priority 11 spanning-tree bpdu-guard timeout 30 spanning-tree vlan 1,2,4,2000,3000,4091 interface mgmt shutdown ip dhcp interface lag 2 multi-chassis vsx-sync vlans no shutdown description CMM_CAB_1000 no routing vlan trunk native 2000 vlan trunk allowed 2000,3000,4091 lacp mode active lacp fallback interface lag 3 multi-chassis vsx-sync vlans no shutdown description CMM_CAB_1000 no routing vlan trunk native 2000 vlan trunk allowed 2000,3000,4091 lacp mode active lacp fallback interface lag 99 no shutdown description ISL link no routing vlan trunk native 1 tag vlan trunk allowed all lacp mode active interface lag 149 multi-chassis no shutdown description sw-spine no routing vlan trunk native 1 vlan trunk allowed 1-2,4 lacp mode active spanning-tree vlan 1 cost 203 spanning-tree vlan 2 cost 203 spanning-tree vlan 4 cost 203 interface 1/1/1 description cmm1 lag 2 interface 1/1/2 description cmm3 lag 3 interface 1/1/48 no shutdown mtu 9198 vrf attach keepalive description VSX keepalive ip address 192.168.255.1/31 interface 1/1/49 no shutdown description sw-spine-001_50 lag 149 interface 1/1/50 no shutdown description sw-spine-002_50 lag 149 interface 1/1/51 no shutdown mtu 9198 lag 99 interface 1/1/52 no shutdown mtu 9198 lag 99 interface vlan 1 description MGMT ip mtu 9198 ip address 10.1.0.6/16 interface vlan 2 ip mtu 9198 ip address 10.252.0.6/17 ip ospf 1 area 0.0.0.2 interface vlan 4 ip mtu 9198 ip address 10.254.0.6/17 ip ospf 1 area 0.0.0.4 interface vlan 2000 vsx-sync active-gateways description CAB_1000_MTN_HMN ip address 10.100.3.253/22 active-gateway ip mac 02:01:00:00:01:02 active-gateway ip 10.100.3.254 ip helper-address 10.92.100.222 ip ospf 1 area 0.0.0.2 ip ospf passive interface vlan 3000 vsx-sync active-gateways description CAB_1000_MTN_HMN ip address 10.104.3.253/22 active-gateway ip mac 02:01:00:00:01:02 active-gateway ip 10.104.3.254 ip helper-address 10.94.100.222 ip ospf 1 area 0.0.0.4 ip ospf passive interface vlan 4091 description CMM_RECOVERY vsx system-mac 02:01:00:00:01:02 inter-switch-link lag 99 role secondary keepalive peer 192.168.255.0 source 192.168.255.1 vrf keepalive linkup-delay-timer 600 vsx-sync vsx-global ! ! ! ! ! router ospf 1 router-id 10.252.0.6 area 0.0.0.2 area 0.0.0.4 https-server vrf default https-server vrf mgmt "
},
{
	"uri": "/docs-csm/en-09/414-mgmt-net-ntp-config/",
	"title": "Management Network NTP configuration",
	"tags": [],
	"description": "",
	"content": "Management Network NTP configuration This page describes how NTP is setup and configured on the management network switches.\nRequirements Access to switches CSI NMN.yaml file Configuration Our NTP servers will be the first 3 worker nodes. You can find these IPs from the CSI generated NMN.yaml file.\nAruba Get current NTP configuration.\nsw-spine-001(config)# show run | include ntp ntp server 10.252.1.7 ntp server 10.252.1.8 ntp server 10.252.1.9 ntp enable Delete current NTP configuration.\nsw-spine-001(config)# no ntp server 10.252.1.7 sw-spine-001(config)# no ntp server 10.252.1.8 sw-spine-001(config)# no ntp server 10.252.1.9 Add new NTP configuration.\nsw-spine-001(config)# ntp enable sw-spine-001(config)# ntp server 10.252.1.10 sw-spine-001(config)# ntp server 10.252.1.11 sw-spine-001(config)# ntp server 10.252.1.12 Verify NTP status.\nsw-spine-001(config)# show ntp associations ---------------------------------------------------------------------- ID NAME REMOTE REF-ID ST LAST POLL REACH ---------------------------------------------------------------------- 1 10.252.1.7 10.252.1.7 172.30.47.5 3 18 64 1 2 10.252.1.8 10.252.1.8 172.30.47.5 3 11 64 1 3 10.252.1.9 10.252.1.9 172.30.47.5 3 11 64 1 ---------------------------------------------------------------------- Dell Get current NTP configuration.\nsw-leaf-001# show running-configuration | grep ntp ntp server 10.252.1.12 ntp server 10.252.1.13 ntp server 10.252.1.14 prefer Delete current NTP configuration.\nsw-leaf-001# configure terminal sw-leaf-001(config)# no ntp server 10.252.1.12 sw-leaf-001(config)# no ntp server 10.252.1.13 sw-leaf-001(config)# no ntp server 10.252.1.14 Add new NTP server configuration.\nntp server 10.252.1.10 prefer ntp server 10.252.1.11 ntp server 10.252.1.12 ntp source vlan 2 Verify NTP status.\nsw-leaf-001# show ntp associations remote refid st t when poll reach delay offset jitter ============================================================================== *10.252.1.12 10.252.1.4 4 u 52 64 3 0.420 -0.262 0.023 10.252.1.13 10.252.1.4 4 u 51 64 3 0.387 -0.225 0.043 10.252.1.14 10.252.1.4 4 u 48 64 3 0.399 -0.222 0.050 * master (synced), # master (unsynced), + selected, - candidate, ~ configured Mellanox Get current NTP configuration.\nsw-spine-001 [standalone: master] (config) # show run | include ntp no ntp server 10.252.1.9 disable ntp server 10.252.1.9 keyID 0 no ntp server 10.252.1.9 trusted-enable ntp server 10.252.1.9 version 4 no ntp server 10.252.1.10 disable ntp server 10.252.1.10 keyID 0 no ntp server 10.252.1.10 trusted-enable ntp server 10.252.1.10 version 4 no ntp server 10.252.1.11 disable ntp server 10.252.1.11 keyID 0 no ntp server 10.252.1.11 trusted-enable ntp server 10.252.1.11 version 4 Delete current NTP configuration.\nsw-spine-001 [standalone: master] # conf t sw-spine-001 [standalone: master] (config) # no ntp server 10.252.1.9 sw-spine-001 [standalone: master] (config) # no ntp server 10.252.1.10 sw-spine-001 [standalone: master] (config) # no ntp server 10.252.1.11 Add New NTP configuration.\nsw-spine-001 [standalone: master] (config) # ntp server 10.252.1.12 sw-spine-001 [standalone: master] (config) # ntp server 10.252.1.13 sw-spine-001 [standalone: master] (config) # ntp server 10.252.1.14 Verify NTP configuration.\nsw-spine-001 [standalone: master] # show ntp NTP is administratively : enabled NTP Authentication administratively: disabled NTP server role : enabled Clock is synchronized: Reference: 10.252.1.14 Offset : -0.056 ms Active servers and peers: 10.252.1.12: Conf Type : serv Status : candidat(+) Stratum : 4 Offset(msec) : -0.119 Ref clock : 10.252.1.4 Poll Interval (sec): 128 Last Response (sec): 107 Auth state : none 10.252.1.13: Conf Type : serv Status : candidat(+) Stratum : 4 Offset(msec) : -0.059 Ref clock : 10.252.1.4 Poll Interval (sec): 128 Last Response (sec): 96 Auth state : none 10.252.1.14: Conf Type : serv Status : sys.peer(*) Stratum : 4 Offset(msec) : -0.056 Ref clock : 10.252.1.4 Poll Interval (sec): 128 Last Response (sec): 118 Auth state : none "
},
{
	"uri": "/docs-csm/en-09/415-mgmt-net-switch-rename/",
	"title": "Management Network Switch Rename",
	"tags": [],
	"description": "",
	"content": "Management Network Switch Rename Any system moving from Shasta v1.3 to Shasta v1.4 software needs to adjust the hostnames and IP addresses for all switches to match the new standard. There is now a virtual IP ending in .1 which is used by spine switches. In Shasta v1.3, the first spine switch used the .1 address. In Shasta v1.4, the ordering of the switches has changed with spine switches being grouped first. The hostname for switches has changed from two digits to a dash and then 3 digits. All IPv4 data for the switches and switch naming comes from Cray Site Init (CSI).\nFrom v1.3, this example system had these IP addresses and hostnames on the HMN network. Similar names and IP address numbers for the NMN and CAN networks as well. Also note that some systems may not have an agg or aggregation set of switches. This is ok. Simply follow the directions below, always paying attention to the CSI specified IPv4 addresses and skip the aggregation switch directions.\n10.1.0.1 sw-spine01 10.1.0.2 sw-leaf01 10.1.0.3 sw-spine02 10.1.0.4 sw-leaf02 10.1.0.5 sw-agg01 10.1.0.6 sw-agg02 10.1.0.7 sw-cdu01 10.1.0.8 sw-cdu02 The desired settings for the HMN network would be more like these.\n10.1.0.2 sw-spine-001 10.1.0.3 sw-spine-002 10.1.0.4 sw-agg-001 10.1.0.5 sw-agg-002 10.1.0.6 sw-leaf-001 10.1.0.7 sw-leaf-002 10.1.0.8 sw-cdu-001 10.1.0.9 sw-cdu-002 This system needs to do the renames in this order: do CDU switches (8 to 9, 7 to 8) and then leaf switches (4 to 7, but also 2 to 6), then aggregation switches (6 to 5, 5 to 4) and spine switches (3 to 3, and 1 to 2). These have IP address changes and name changes since we now have 3 digits instead of 2 for the switch hostname.\nCheck switch IP addresses, names, and component names in /var/www/ephemeral/prep/${SYSTEM_NAME}/networks when booted from the LiveCD on ncn-m001.\npit# export SYSTEM_NAME=eniac pit# cd /var/www/ephemeral/prep/${SYSTEM_NAME}/networks pit# vi NMN.yaml Excerpt from NMN.yaml\nip_reservations: - ip_address: 10.252.0.2 name: sw-spine-001 comment: x3000c0h33s1 aliases: [] - ip_address: 10.252.0.3 name: sw-spine-002 comment: x3000c0h34s1 aliases: [] - ip_address: 10.252.0.4 name: sw-cdu-001 comment: d0w1 aliases: [] - ip_address: 10.252.0.5 name: sw-cdu-002 comment: d0w2 aliases: [] - ip_address: 10.252.0.6 name: sw-leaf-001 comment: x3000c0w38 aliases: [] - ip_address: 10.252.0.7 name: sw-leaf-002 comment: x3000c0w36 aliases: [] pit# vi HMN.yaml Excerpt from HMN.yaml\nip_reservations: - ip_address: 10.254.0.2 name: sw-spine-001 comment: x3000c0h33s1 aliases: [] - ip_address: 10.254.0.3 name: sw-spine-002 comment: x3000c0h34s1 aliases: [] - ip_address: 10.254.0.4 name: sw-cdu-001 comment: d0w1 aliases: [] - ip_address: 10.254.0.5 name: sw-cdu-002 comment: d0w2 aliases: [] - ip_address: 10.254.0.6 name: sw-leaf-001 comment: x3000c0w38 aliases: [] - ip_address: 10.254.0.7 name: sw-leaf-002 comment: x3000c0w36 aliases: [] pit# vi MTL.yaml\nExcerpt from MTL.yaml ip_reservations:\nip_address: 10.1.0.2 name: sw-spine-001 comment: x3000c0h33s1 aliases: [] ip_address: 10.1.0.3 name: sw-spine-002 comment: x3000c0h34s1 aliases: [] ip_address: 10.1.0.4 name: sw-cdu-001 comment: d0w1 aliases: [] ip_address: 10.1.0.5 name: sw-cdu-002 comment: d0w2 aliases: [] ip_address: 10.1.0.6 name: sw-leaf-001 comment: x3000c0w38 aliases: [] ip_address: 10.1.0.7 name: sw-leaf-002 comment: x3000c0w36 aliases: [] pit# vi CAN.yaml\nExcerpt from CAN.yaml showing the two spine switches. Most v1.3 systems would have had these as ending in .1 and in .3. Note these switches are not named \u0026#34;spine\u0026#34; or \u0026#34;agg\u0026#34; since the SHCD may specify differing exit points, but with either option the IPv4 address is specified. ip_reservations:\nip_address: 10.103.8.2 name: can-switch-1 comment: \u0026quot;\u0026quot; aliases: [] ip_address: 10.103.8.3 name: can-switch-2 comment: \u0026quot;\u0026quot; aliases: [] Save the running-config from all switches before you start.\nNote: Mellanox switches require the \u0026ldquo;enable\u0026rdquo; command before doing \u0026ldquo;show running-config\u0026rdquo;\npit# ssh admin@10.1.0.1 switch# show running-config switch# exit\nSave this information in a text file for later evaluation and comparison after all changes have been made. pit# vi before.sw-spine01.txt\nRepeat this for all of the switches. The example system has switches up to 10.1.0.8.\nStart moves with the highest numbered switch. In this case, that is sw-cdu02. Said another way, if a switch is in a pair, start with the second half of the pair (i.e.. 2 of 2).\nMove sw-cdu02 to sw-cdu-002 and increase IP addresses as specified in CSI output. It is a Dell switch.\nNote: You can change many addresses in a single session, but not the one you used to connect. This first connection will skip vlan 1 and change all of the other vlans (vlan 2 and vlan 4 on a CDU switch.)\npit# ssh admin@10.1.0.6 sw-cdu02# configure terminal sw-cdu02(config)# hostname sw-cdu-002 sw-cdu-002(config)# interface vlan2 sw-cdu-002(conf-if-vl-2)# ip address 10.252.0.7/17 sw-cdu-002(conf-if-vl-2)# interface vlan4 sw-cdu-002(conf-if-vl-4)# ip address 10.254.0.7/17 sw-cdu-002(conf-if-vl-4)# router ospf 1 sw-cdu-002(config-router-ospf-1)# router-id 10.252.0.7 sw-cdu-002(config-router-ospf-1)# exit sw-cdu-002(config)# exit sw-cdu-002# write memory Logout of the switch and return using the new IP address for vlan 2 so that vlan 1 can be corrected.\nsw-cdu-002# exit pit# ssh admin@10.252.0.7 sw-cdu-002# configure terminal sw-cdu-002(config)# interface vlan1 sw-cdu-002(conf-if-vl-1)# ip address 10.1.0.7/16 sw-cdu-002(conf-if-vl-1)# exit sw-cdu-002(config)# exit sw-cdu-002# write memory sw-cdu-002# exit pit# Move sw-cdu01 to sw-cdu-001 and increase IP addresses as specified in CSI output. It is a Dell switch.\nNote: You can change many addresses in a single session, but not the one you used to connect. This first connection will skip vlan 1 and change all of the other vlans (vlan 2 and vlan 4 on a CDU switch.)\npit# ssh admin@10.1.0.5 sw-cdu01# configure terminal sw-cdu01(config)# hostname sw-cdu-001 sw-cdu-001(config)# interface vlan2 sw-cdu-001(conf-if-vl-2)# ip address 10.252.0.6/17 sw-cdu-001(conf-if-vl-2)# interface vlan4 sw-cdu-001(conf-if-vl-4)# ip address 10.254.0.6/17 sw-cdu-001(conf-if-vl-4)# router ospf 1 sw-cdu-001(config-router-ospf-1)# router-id 10.252.0.6 sw-cdu-001(config-router-ospf-1)# exit sw-cdu-001(config)# exit sw-cdu-001# write memory Logout of the switch and return using the new IP address for vlan 2 so that vlan 1 can be corrected.\nsw-cdu-001# exit pit# ssh admin@10.252.0.6 sw-cdu-001# configure terminal sw-cdu-001(config)# interface vlan1 sw-cdu-001(conf-if-vl-1)# ip address 10.1.0.6/16 sw-cdu-001(conf-if-vl-1)# exit sw-cdu-001(config)# exit sw-cdu-001# write memory sw-cdu-001# exit pit# Move sw-leaf02 to sw-leaf-002 and increase IP addresses as specified in CSI output. It is a Dell switch.\nNote: You can change many addresses in a single session, but not the one you used to connect. This first connection will skip vlan 1 and change all of the other vlans (vlan 2, vlan 4, vlan 7, vlan 10) on a leaf switch.\npit# ssh admin@10.1.0.4 sw-leaf02# configure terminal sw-leaf02(config)# hostname sw-leaf-002 sw-leaf-002(config)# interface vlan2 sw-leaf-002(conf-if-vl-2)# ip address 10.252.0.5/17 sw-leaf-002(conf-if-vl-2)# interface vlan4 sw-leaf-002(conf-if-vl-4)# ip address 10.254.0.5/17 sw-leaf-002(conf-if-vl-4)# interface vlan7 sw-leaf-002(conf-if-vl-7)# ip address 10.103.8.5/24 sw-leaf-002(conf-if-vl-7)# interface vlan10 sw-leaf-002(conf-if-vl-10)# ip address 10.11.0.5/16 sw-leaf-002(conf-if-vl-10)# router ospf 1 sw-leaf-002(config-router-ospf-1)# router-id 10.252.0.5 sw-leaf-002(config-router-ospf-1)# exit sw-leaf-002(config)# exit sw-leaf-002# write memory Logout of the switch and return using the new IP address for vlan 2 so that vlan 1 can be corrected.\nsw-leaf-002# exit pit# ssh admin@10.252.0.5 sw-leaf-002# configure terminal sw-leaf-002(config)# interface vlan1 sw-leaf-002(conf-if-vl-1)# ip address 10.1.0.5/16 sw-leaf-002(conf-if-vl-1)# exit sw-leaf-002(config)# exit sw-leaf-002# write memory sw-leaf-002# exit pit# Move sw-leaf01 to sw-leaf-001 and increase IP addresses as specified in CSI output. It is a Dell switch.\nNote: You can change many addresses in a single session, but not the one you used to connect. This first connection will skip vlan 1 and change all of the other vlans (vlan 2, vlan 4, vlan 7, vlan 10) on a leaf switch.\npit# ssh admin@10.1.0.2 sw-leaf01# configure terminal sw-leaf01(config)# hostname sw-leaf-001 sw-leaf-001(config)# interface vlan2 sw-leaf-001(conf-if-vl-2)# ip address 10.252.0.4/17 sw-leaf-001(conf-if-vl-2)# interface vlan4 sw-leaf-001(conf-if-vl-4)# ip address 10.254.0.4/17 sw-leaf-001(conf-if-vl-4)# interface vlan7 sw-leaf-001(conf-if-vl-7)# ip address 10.103.8.4/24 sw-leaf-001(conf-if-vl-7)# interface vlan10 sw-leaf-001(conf-if-vl-10)# ip address 10.11.0.4/16 sw-leaf-001(conf-if-vl-10)# router ospf 1 sw-leaf-001(config-router-ospf-1)# router-id 10.252.0.4 sw-leaf-001(config-router-ospf-1)# exit sw-leaf-001(config)# exit sw-leaf-001# write memory Logout of the switch and return using the new IP address for vlan 2 so that vlan 1 can be corrected.\nsw-leaf-001# exit pit# ssh admin@10.252.0.4 sw-leaf-001# configure terminal sw-leaf-001(config)# interface vlan1 sw-leaf-001(conf-if-vl-1)# ip address 10.1.0.4/16 sw-leaf-001(conf-if-vl-1)# exit sw-leaf-001(config)# exit sw-leaf-001# write memory sw-leaf-001# exit pit# Move sw-spine02 to sw-spine-002. It already has the .3 IP address so does not need to change. It is a Mellanox switch.\nNote: You can change many addresses in a single session, but not the one you used to connect. This first connection will skip vlan 1 and change all of the other vlans (vlan 2, vlan 4, vlan 7, vlan 10) on a leaf switch.\nNote: The addresses for the spine switches in CAN.yaml shown above (ip_address: 10.103.8.2, name: can-switch-1) and (ip_address: 10.103.8.3, name: can-switch-2) were shown on the 10.103.8.0 subnet so the virtual-router address on interface vlan7 magp 7 should be 10.103.8.1.\npit# ssh admin@10.1.0.3 sw-spine02\u0026gt; enable sw-spine02# configure terminal sw-spine02(config)# hostname sw-spine-002 sw-spine02(config)# no protocol magp sw-spine-002(config)# interface vlan 1 ip address 10.1.0.3/16 primary sw-spine-002(config)# interface vlan 2 ip address 10.252.0.3/17 primary sw-spine-002(config)# interface vlan 4 ip address 10.254.0.3/17 primary sw-spine-002(config)# interface vlan 7 ip address 10.103.8.3/24 primary sw-spine-002(config)# interface vlan 10 ip address 10.11.0.3/16 primary sw-spine-002(config)# router bgp 65533 vrf default router-id 10.252.0.3 force sw-spine-002(config)# router ospf 1 vrf default router-id 10.252.0.3 sw-spine-002(config)# exit sw-spine-002# write memory Logout of the switch and return using the new IP address for vlan 2 so that vlan 1 can be corrected.\nsw-spine-002# exit pit# ssh admin@10.252.0.3 sw-spine-002 [standalone: master] \u0026gt; enable sw-spine-002 [standalone: master] # configure terminal sw-spine-002 [standalone: master] (config) # no protocol magp sw-spine-002 [standalone: master] (config) # protocol magp sw-spine-002 [standalone: master] (config) # interface vlan 1 sw-spine-002 [standalone: master] (config interface vlan 1) # no ip address sw-spine-002 [standalone: master] (config interface vlan 1) # ip address 10.1.0.3/16 primary sw-spine-002 [standalone: master] (config interface vlan 1) # exit sw-spine-002 [standalone: master] (config) # interface vlan 1 magp 1 sw-spine-002 [standalone: master] (config interface vlan 1 magp 1) # ip virtual-router address 10.1.0.1 sw-spine-002 [standalone: master] (config interface vlan 1 magp 1) # ip virtual-router mac-address 00:00:5E:00:01:01 sw-spine-002 [standalone: master] (config interface vlan 1 magp) # exit sw-spine-002 [standalone: master] (config) # interface vlan 2 magp 2 sw-spine-002 [standalone: master] (config interface vlan 2 magp 2) # ip virtual-router address 10.252.0.1 sw-spine-002 [standalone: master] (config interface vlan 2 magp 2) # ip virtual-router mac-address 00:00:5E:00:01:02 sw-spine-002 [standalone: master] (config interface vlan 2 magp 2) # exit sw-spine-002 [standalone: master] (config) # interface vlan 4 magp 4 sw-spine-002 [standalone: master] (config interface vlan 4 magp 4) # ip virtual-router address 10.254.0.1 sw-spine-002 [standalone: master] (config interface vlan 4 magp 4) # ip virtual-router mac-address 00:00:5E:00:01:04 sw-spine-002 [standalone: master] (config interface vlan 4 magp 4) # exit sw-spine-002 [standalone: master] (config) # interface vlan 7 magp 7 sw-spine-002 [standalone: master] (config interface vlan 7 magp 7) # ip virtual-router address 10.103.8.1 sw-spine-002 [standalone: master] (config interface vlan 7 magp 7) # ip virtual-router mac-address 00:00:5E:00:01:07 sw-spine-002 [standalone: master] (config interface vlan 7 magp 7) # exit sw-spine-002 [standalone: master] (config) # interface vlan 10 magp 10 sw-spine-002 [standalone: master] (config interface vlan 10 magp 10) # ip virtual-router address 10.11.0.1 sw-spine-002 [standalone: master] (config interface vlan 10 magp 10) # ip virtual-router mac-address 00:00:5E:00:01:10 sw-spine-002 [standalone: master] (config interface vlan 10 magp 10) # exit sw-spine-002 [standalone: master] (config) # exit sw-spine-002 [standalone: master] # write memory sw-spine-002 [standalone: master] # exit pit# Move sw-spine01 to sw-spine-001 and increase IP addresses as specified in CSI output. It is a Mellanox switch.\nNote: You can change many addresses in a single session, but not the one you used to connect. This first connection will skip vlan 1 and change all of the other vlans (vlan 2, vlan 4, vlan 7, vlan 10) on a leaf switch.\nNote: The addresses for the spine switches in CAN.yaml shown above (ip_address: 10.103.8.2, name: can-switch-1) and (ip_address: 10.103.8.3, name: can-switch-2) were shown on the 10.103.8.0 subnet so the virtual-router address on interface vlan7 magp 7 should be 10.103.8.1.\npit# ssh admin@10.1.0.1 sw-spine01\u0026gt; enable sw-spine01# configure terminal sw-spine01(config)# hostname sw-spine-001 sw-spine-001(config)# no protocol magp sw-spine-001(config)# interface vlan 2 sw-spine-001 [standalone: master] (config interface vlan 2) # no ip address sw-spine-001 [standalone: master] (config interface vlan 2) # ip address 10.252.0.2/17 primary sw-spine-001 [standalone: master] (config interface vlan 2) # exit sw-spine-001 [standalone: master] (config) # interface vlan 4 sw-spine-001 [standalone: master] (config interface vlan 4) # no ip address sw-spine-001 [standalone: master] (config interface vlan 4) # ip address 10.254.0.2/17 primary sw-spine-001 [standalone: master] (config interface vlan 4) # exit sw-spine-001 [standalone: master] (config) # interface vlan 7 sw-spine-001 [standalone: master] (config interface vlan 7) # no ip address sw-spine-001 [standalone: master] (config interface vlan 7) # ip address 10.103.8.2/24 primary sw-spine-001 [standalone: master] (config interface vlan 7) # exit sw-spine-001 [standalone: master] (config) # interface vlan 10 sw-spine-001 [standalone: master] (config interface vlan 10) # no ip address sw-spine-001 [standalone: master] (config interface vlan 10) # ip address 10.11.0.2/16 primary sw-spine-001 [standalone: master] (config interface vlan 10) # exit sw-spine-001 [standalone: master] (config) # router bgp 65533 vrf default router-id 10.252.0.2 force sw-spine-001 [standalone: master] (config) # router ospf 1 vrf default router-id 10.252.0.2 sw-spine-001 [standalone: master] (config) # exit sw-spine-001 [standalone: master] # write memory Logout of the switch and return using the new IP address for vlan 2 so that vlan 1 can be corrected.\nsw-spine-001# exit pit# ssh admin@10.252.0.2 sw-spine-001 [standalone: master] \u0026gt; enable sw-spine-001 [standalone: master] # configure terminal sw-spine-001 [standalone: master] (config) # no protocol magp sw-spine-001 [standalone: master] (config) # protocol magp sw-spine-001 [standalone: master] (config) # interface vlan 1 sw-spine-001 [standalone: master] (config interface vlan 1) # no ip address sw-spine-001 [standalone: master] (config interface vlan 1) # ip address 10.1.0.2/16 primary sw-spine-001 [standalone: master] (config interface vlan 1) # exit sw-spine-001 [standalone: master] (config) # interface vlan 1 magp 1 sw-spine-001 [standalone: master] (config interface vlan 1 magp 1) # ip virtual-router address 10.1.0.1 sw-spine-001 [standalone: master] (config interface vlan 1 magp 1) # ip virtual-router mac-address 00:00:5E:00:01:01 sw-spine-001 [standalone: master] (config interface vlan 1 magp 1) # exit sw-spine-001 [standalone: master] (config) # interface vlan 2 magp 2 sw-spine-001 [standalone: master] (config interface vlan 2 magp 2) # ip virtual-router address 10.252.0.1 sw-spine-001 [standalone: master] (config interface vlan 2 magp 2) # ip virtual-router mac-address 00:00:5E:00:01:02 sw-spine-001 [standalone: master] (config interface vlan 2 magp 2) # exit sw-spine-001 [standalone: master] (config) # interface vlan 4 magp 4 sw-spine-001 [standalone: master] (config interface vlan 4 magp 4) # ip virtual-router address 10.254.0.1 sw-spine-001 [standalone: master] (config interface vlan 4 magp 4) # ip virtual-router mac-address 00:00:5E:00:01:04 sw-spine-001 [standalone: master] (config interface vlan 4 magp 4) # exit sw-spine-001 [standalone: master] (config) # interface vlan 7 magp 7 sw-spine-001 [standalone: master] (config interface vlan 7 magp 7) # ip virtual-router address 10.103.8.1 sw-spine-001 [standalone: master] (config interface vlan 7 magp 7) # ip virtual-router mac-address 00:00:5E:00:01:07 sw-spine-001 [standalone: master] (config interface vlan 7 magp 7) # exit sw-spine-001 [standalone: master] (config) # interface vlan 10 magp 10 sw-spine-001 [standalone: master] (config interface vlan 10 magp 10) # ip virtual-router address 10.11.0.1 sw-spine-001 [standalone: master] (config interface vlan 10 magp 10) # ip virtual-router mac-address 00:00:5E:00:01:10 sw-spine-001 [standalone: master] (config interface vlan 10 magp 10) # exit sw-spine-001 [standalone: master] (config) # exit sw-spine-001 [standalone: master] # write memory sw-spine-001 [standalone: master] # exit pit# Save the running-config from all switches after completion\nNote: Mellanox switches require the \u0026ldquo;enable\u0026rdquo; command before doing \u0026ldquo;show running-config\u0026rdquo;\npit# ssh admin@10.1.0.2 switch# show running-config switch# exit Save this information in a text file for comparison with the running-config saved before all changes were made. pit# vi after.sw-spine01.txt\nRepeat this for all of the switches. The example system has switches up to 10.1.0.7.\nThere are other changes needed, as described in Dell and Mellanox Changes for Shasta v1.3 to v1.4 Upgrades\n"
},
{
	"uri": "/docs-csm/en-09/416-mgmt-net-cabling/",
	"title": "Cabling",
	"tags": [],
	"description": "",
	"content": "Cabling HPE Hardware Gigabyte/Intel Hardware HPE Hardware HPE DL385 The OCP Slot is noted (number 7) in the image above. This is the bottom middle slot to the left of the VGA port. Ports are numbered left-to-right: the far left port is port 1. The PCIe Slot 1 is on the top left side of the image above (under number 1). Ports are numbered left-to-right: the far left port is port 1. HPE DL325 The OCP Slot is noted (number 9) in the image above. This is the slot on the bottom left of the node. Ports are numbered left-to-right: the far left port is port 1. The PCIE Slot 1 is on the top left side of the image above (under number 1). Ports are numbered left-to-right: the far left port is port 1. NCN Worker Cabling Server Port Management Network Port Speed Use / Configuration OCP port 1 spine or aggr pair, switch 1/2 25Gb Management Network NMN/HMN/CAN OCP port 2 NONE NONE NONE PCIe Slot 1 port 1 spine or aggr pair, switch 2/2 25Gb Management Network NMN/HMN/CAN PCIe Slot 1 port 2 NONE NONE NONE SHCD Example hostname Source Destination Destination wn01 x3000u04ocp-j1 x3000u12-j7 sw-25g01 wn01 x3000u04ocp-j2 x3000u13-j7 sw-25g02 NCN Master Cabling Single Card Installations Server Port Management Network Port Speed Use / Configuration OCP port 1 spine or aggr pair, switch 1/2 25Gb Management Network NMN/HMN/CAN OCP port 2 NONE NONE NONE PCIe Slot 1 port 1 spine or aggr pair, switch 2/2 25Gb Management Network NMN/HMN/CAN PCIe Slot 1 port 2 NONE (See note below for ncn-m001) NONE Site (See note below for ncn-m001) Dual Card Installations A dual card configuration is much less common, but can occur based on customer requirements. The table below describes the cabling of dual card configurations. Also read notes in this section to see other possible customer-based configurations.\nServer Port Management Network Port Speed Use / Configuration OCP port 1 spine or aggr pair, switch 1/2 25Gb Management Network NMN/HMN/CAN OCP port 2 NONE NONE NONE PCIe Slot 1 port 1 spine or aggr pair, switch 2/2 25Gb Management Network NMN/HMN/CAN PCIe Slot 1 port 2 NONE (See note below for ncn-m001) NONE Site (See note below for ncn-m001) SHCD Example hostname Source Destination Destination mn01 x3000u01ocp-j1 x3000u12-j1 sw-25g01 mn01 x3000u01s1-j1 x3000u13-j1 sw-25g02 NOTE: Master 1 (ncn-m001) is required to have a site connection for installation and non-CAN system access. This can have several configurations depending on customer requirements/equipment:\nDual 10/25Gb card configurations as described in the table above should use PCIe Slot 1, Port 2 as a site connection if the customer supports 10/25Gb. If the customer does not support 10/25Gb speeds (or connection type) and requires RJ45 copper or 1Gb, then a new and separate card will be installed on ncn-m001 and that card will provide site connectivity. Another possibility (non-HPE hardware mainly) is that a built-in 1Gb port will be used if available (similar to Shasta v1.3 PoR on Gigabyte hardware). NCN Storage Cabling Server Port Management Network Port Speed Use / Configuration OCP port 1 spine or aggr pair, switch 1/2 25Gb Management Network NMN/HMN/CAN OCP port 2 spine or aggr pair, switch 1/2 25Gb Storage SUN (future use) PCIe Slot 1 port 1 spine or aggr pair, switch 2/2 25Gb Management Network NMN/HMN/CAN PCIe Slot 1 port 2 spine or aggr pair, switch 2/2 25Gb Storage SUN (future use) SHCD Example hostname Source Destination Destination sn01 x3000u17s1-j2 x3000u34-j14 sw-25g02 sn01 x3000u17s1-j1 x3000u34-j8 sw-25g02 sn01 x3000u17ocp-j2 x3000u33-j14 sw-25g01 sn01 x3000u17ocp-j1 x3000u33-j8 sw-25g01 For systems that include 4 aggregation switches the cabling will look like the following.\nSHCD Example with four aggregation switches. hostname Source Destination Destination sn01 x3000u10ocp-j2 x3000u36-j5 sw-25g04 sn01 x3000u10s1-j2 x3000u35-j5 sw-25g03 sn01 x3000u10ocp-j1 x3000u34-j6 sw-25g02 sn01 x3000u10s1-j1 x3000u33-j6 sw-25g01 UAN Cabling Server Port Management Network Port Speed Use / Configuration OCP port 1 spine or aggr pair, switch 1/2 25Gb Management Network NMN OCP port 2 spine or aggr pair, switch 1/2 25Gb Management Network CAN bond PCIe Slot 1 port 1 spine or aggr pair, switch 2/2 25Gb NONE (Shasta v1.4) PCIe Slot 1 port 2 spine or aggr pair, switch 2/2 25Gb Management Network CAN bond SHCD Example hostname Source Destination Destination uan01 x3000u17s1-j2 x3000u34-j14 sw-25g02 uan01 x3000u17s1-j1 x3000u34-j8 sw-25g02 uan01 x3000u17ocp-j2 x3000u33-j14 sw-25g01 uan01 x3000u17ocp-j1 x3000u33-j8 sw-25g01 Gigabyte/Intel Hardware NCN Worker Cabling Server Port Management Network Port Speed Use / Configuration PCIe Slot 1 port 1 spine or aggr pair, switch 1/2 40Gb Management Network NMN/HMN/CAN PCIe Slot 1 port 2 spine or aggr pair, switch 2/2 40Gb Management Network NMN/HMN/CAN SHCD Example hostname Source Destination Destination wn01 x3000u07s1-j1 x3000u24L-j4 sw-smn02 wn01 x3000u07s1-j2 x3000u24R-j4 sw-smn03 NOTE: Cabling of ncn-w001 has changed in Shasta v1.4. Please see ncn-m001 note below.\nNCN Master Cabling Server Port Management Network Port Speed Use / Configuration PCIe Slot 1 port 1 spine or aggr pair, switch 1/2 40Gb Management Network NMN/HMN/CAN PCIe Slot 1 port 2 spine or aggr pair, switch 2/2 40Gb Management Network NMN/HMN/CAN LAN0 port 1 NONE (See note below for ncn-m001) NONE Site (See note below for ncn-m001) SHCD Example hostname Source Destination Destination mn01 x3000u01s1-j1 x3000u24L-j1 sw-smn02 mn01 x3000u01s1-j2 x3000u24R-j1 sw-smn03 NOTE: Master 1 (ncn-m001) is required to have a site connection for installation and non-CAN system access. In Shasta versions \u0026lt;=1.3 this connection was on ncn-w001. This can have several configurations depending on customer requirements/equipment:\nThe default configuration for Gigabyte systems uses the built-in 1Gb lan0 port for site connection on ncn-m001. If the customer requires connectivity greater than 1Gb (or a different connection type), then a new and separate card will be installed on ncn-m001 and that card will provide site connectivity. NCN Storage Cabling Server Port Management Network Port Speed Use / Configuration PCIe Slot 1 port 1 spine or aggr pair, switch 1/2 40Gb Management Network NMN/HMN/CAN PCIe Slot 1 port 2 spine or aggr pair, switch 2/2 40Gb Management Network NMN/HMN/CAN SHCD Example hostname Source Destination Destination sn01 x3000u13s1-j1 x3000u24L-j7 sw-smn02 sn01 x3000u13s1-j2 x3000u24R-j7 sw-smn03 UAN Cabling Server Port Management Network Port Speed Use / Configuration LAN0 port 1 leaf (see note) 1Gb Management Network NMN PCIe Slot 1 port 1 spine or aggr pair, switch 1/2 40Gb Management Network CAN bond PCIe Slot 1 port 2 spine or aggr pair, switch 2/2 40Gb Management Network CAN bond SHCD Example hostname Source Destination Destination uan01 x3000u27s1-j1 x3000u24L-j10 sw-smn02 uan01 x3000u27s1-j2 x3000u24R-j10 sw-smn03 NOTE that there are a couple configurations possible for LAN0:\nExisting Gigabyte systems on Dell and Mellanox network hardware will use the (existing) Dell leaf port. Any Gigabyte system on Aruba network hardware will use a Aruba 6300 (for the 1Gb port). Optionally a 10/25Gb card could be added in an Aruba hardware system to match the HPE UANs. "
},
{
	"uri": "/docs-csm/en-09/417-mgmt-net-flow-control/",
	"title": "Management Network Flow Control Settings",
	"tags": [],
	"description": "",
	"content": "Management Network Flow Control Settings This page is designed to go over all the flow control settings for Dell/Mellanox systems. These changes were introduced in 1.3.2. These changes are required for Shasta 1.4.\nLeaf Switch Node Connections For the node connections to a leaf switch, we want the transmit flowcontrol disabled, and receive flowcontrol enabled. The following commands will accomplish this.\nNOTE: If you have a TDS system involving a Hill cabinet, make sure to confirm that no CMM nor CEC components are connected to any leaf switches in your system. If these components are connected to the leaf, confirm to which ports they are connected, and modify the commands below to avoid modifying the flowcontrol settings of those ports.\nsw-leaf-001# configure terminal sw-leaf-001(config)# interface range ethernet 1/1/1-1/1/48 sw-leaf-001(conf-range-eth1/1/1-1/1/48)# flowcontrol receive on sw-leaf-001(conf-range-eth1/1/1-1/1/48)# flowcontrol transmit off sw-leaf-001(conf-range-eth1/1/1-1/1/48)# end sw-leaf-001# write memory Switch-to-Switch Connections We want to disable flowcontrol in both directions for all switch-to-switch connections: spine-to-leaf; spine-to-CDU; spine-to-aggregate; and aggregate-to-leaf. (It is unlikely that any system has every type of connection.) We will first cover how to identify which ports are part of a switch-to-switch connection, and then we will provide the commands to make the changes. We will provide the commands for each switch group separately, but it is strongly recommended to make the configuration changes for each end of the connection in short order; for example, for a spine-leaf connection, do not make the changes on the spine side, if you cannot also make the changes to the leaf switch end of the connection within a couple minutes. Repeat the above commands for each leaf switch in your system. These changes can be performed before the switch-to-switch connections, or concurrent with those changes.\nRecommended Order of Flow Control Changes for Switch-to-Switch Connections\nMake change on sw-spine-001 side of leaf/aggregate/CDU ISL connections. Make change on sw-spine-002 side of leaf/aggregate/CDU ISL connections. Make change on leaf/aggregate/CDU side of spine ISL connections. Make change on aggregate01/CDU01 side of VLT ISL connections. Make change on aggregate02/CDU02 side of VLT ISL connections. Repeat Steps 4 and 5 for each aggregate/CDU switch pair. Make change on aggregate side of leaf ISL connections. Make change on leaf side of aggregate ISL connections. Identify Switch-to-Switch Connections Leaf Switches Our standard for the configuration uses \u0026lsquo;port-channel 100\u0026rsquo; for the connection to the spine or aggregate switch. To get what ports are part of this composite interface, use this command:\nsw-leaf-001# show interface port-channel 100 summary LAG Mode Status Uptime Ports 100 L2-HYBRID up 2 weeks 5 days 01:2 Eth 1/1/51 (Up) Eth 1/1/52 (Up) Based on this example, we see that the physical ports are \u0026lsquo;1/1/51\u0026rsquo; and \u0026lsquo;1/1/52\u0026rsquo;. Record this information for each leaf switch.\nCDU Switches In order to get the ports involved in the connection to the spine switches, you can use the command shared for the leaf switch, above.\nIn addition to this, we also need the ports which connect the pair of CDU switches together. The best way to determine the ports involved is to run the following command:\nsw-cdu-001# show running-configuration | grep discovery discovery-interface ethernet1/1/25,1/1/29 Here, we can see ports 1/1/25 and 1/1/29 are being used as connections between the CDU switches. As with the connection to the spine, record the ports involved.\nNOTE: It is very important that the flowcontrol settings for the CMM and CEC devices connected to the CDU switches NOT be modified.\nAggregate Switches On large River systems, aggregate switches are situated between the leaf and spine switches. In general, we would expect every port which is up on these switches to either be a connection to the spine (as \u0026lsquo;port-channel 100\u0026rsquo;), a connection to a leaf, or a connection to its peer aggregate switch. To see which ports are currently up, run this:\nsw-10g01# show interface status -------------------------------------------------------------------------------------------------- Port Description Status Speed Duplex Mode Vlan Tagged-Vlans -------------------------------------------------------------------------------------------------- Eth 1/1/1 LEAF_CONN_1 up 10G full - Eth 1/1/2 LEAF_CONN_2 up 10G full - Eth 1/1/3 LEAF_CONN_3 up 10G full - Eth 1/1/4 LEAF_CONN_4 up 10G full - Eth 1/1/5 LEAF_CONN_5 up 10G full - Eth 1/1/6 LEAF_CONN_6 up 10G full - Eth 1/1/7 LEAF_CONN_7 up 10G full - Eth 1/1/8 LEAF_CONN_8 up 10G full - Eth 1/1/9 LEAF_CONN_9 up 10G full - Eth 1/1/10 down 0 full A 1 - Eth 1/1/11 down 0 full A 1 - Eth 1/1/12 down 0 full A 1 - Eth 1/1/13 down 0 full A 1 - Eth 1/1/14 down 0 full A 1 - Eth 1/1/15 down 0 full A 1 - Eth 1/1/16 down 0 full A 1 - Eth 1/1/17 down 0 full A 1 - Eth 1/1/18 down 0 full A 1 - Eth 1/1/19 down 0 full A 1 - Eth 1/1/20 down 0 full A 1 - Eth 1/1/21 down 0 full A 1 - Eth 1/1/22 down 0 full A 1 - Eth 1/1/23 down 0 full A 1 - Eth 1/1/24 down 0 full A 1 - Eth 1/1/25 up 100G full - Eth 1/1/26 down 0 full A 1 - Eth 1/1/27 up 40G full - Eth 1/1/28 up 40G full - Eth 1/1/29 up 100G full - Eth 1/1/30 down 0 full A 1 - Eth 1/1/31 down 0 full A 1 - Eth 1/1/32 down 0 full A 1 - Eth 1/1/33 down 0 full A 1 - Eth 1/1/34 down 0 full A 1 - Eth 1/1/35 down 0 full A 1 - Eth 1/1/36 down 0 full A 1 - Eth 1/1/37 down 0 full A 1 - Eth 1/1/38 down 0 full A 1 - Eth 1/1/39 down 0 full A 1 - Eth 1/1/40 down 0 full A 1 - Eth 1/1/41 down 0 full A 1 - Eth 1/1/42 down 0 full A 1 - Eth 1/1/43 down 0 full A 1 - Eth 1/1/44 down 0 full A 1 - Eth 1/1/45 down 0 full A 1 - Eth 1/1/46 down 0 full A 1 - Eth 1/1/47 down 0 full A 1 - Eth 1/1/48 down 0 full A 1 - Eth 1/1/49 down 0 full A 1 - Eth 1/1/50 down 0 full A 1 - Eth 1/1/51 down 0 full A 1 - Eth 1/1/52 down 0 full A 1 - Eth 1/1/53 down 0 full A 1 - Eth 1/1/54 down 0 full A 1 - -------------------------------------------------------------------------------------------------- From this output, we can see that ports 1/1/1 through 1/1/9, 1/1/25, and 1/1/27 through 1/1/29 are up. Record this information.\nSpine Switches The convenient way to identify the ports involved with connections to other switches is to look at the output from \u0026lsquo;show interface status\u0026rsquo;.\nsw-spine-001 [standalone: master] # show interfaces status -------------------------------------------------------------------------------------------------------------------------------------------------------- Port Operational state Admin Speed MTU Description -------------------------------------------------------------------------------------------------------------------------------------------------------- mgmt1 Down Enabled UNKNOWN 1500 - mgmt0 Down Enabled UNKNOWN 1500 - Po100 Up Enabled 9216 mlag-isl Mpo1 Up Enabled 9216 - Mpo2 Up Enabled 9216 - Mpo3 Up Enabled 9216 - Mpo4 Up Enabled 9216 - Mpo5 Up Enabled 9216 - Mpo6 Up Enabled 9216 - Mpo7 Up Enabled 9216 - Mpo8 Up Enabled 9216 - Mpo9 Up Enabled 9216 - Mpo10 Up Enabled 9216 - Mpo11 Up Enabled 9216 - Mpo17 Up Enabled 9216 - Mpo113 Up Enabled 9216 - Mpo151 Up Enabled 9216 - Mpo152 Up Enabled 9216 - Eth1/1 (Mpo1) Up Enabled 40G 9216 - Eth1/2 (Mpo2) Up Enabled 40G 9216 - Eth1/3 (Mpo3) Up Enabled 40G 9216 - Eth1/4 (Mpo4) Up Enabled 40G 9216 - Eth1/5 (Mpo5) Up Enabled 40G 9216 - Eth1/6 (Mpo6) Up Enabled 40G 9216 - Eth1/7 (Mpo7) Up Enabled 40G 9216 - Eth1/8 (Mpo8) Up Enabled 40G 9216 - Eth1/9 (Mpo9) Up Enabled 40G 9216 - Eth1/10 (Mpo10) Up Enabled 40G 9216 - Eth1/11 (Mpo11) Up Enabled 40G 9216 - Eth1/12 (Po100) Up Enabled 40G 9216 sw-spine-002-1/12 Eth1/13 (Mpo113) Up Enabled 40G 9216 - Eth1/14 (Mpo113) Up Enabled 40G 9216 - Eth1/15/1 (Mpo151) Up Enabled 10G 9216 - Eth1/15/2 (Mpo152) Up Enabled 10G 9216 - Eth1/15/3 Down Enabled Unknown 1500 - Eth1/15/4 Down Enabled Unknown 1500 - Eth1/17 (Mpo17) Up Enabled 40G 9216 - Eth1/18 (Po100) Up Enabled 40G 9216 sw-spine-002-1/18 Eth1/19 Up Enabled 10G 1500 - Eth1/20 Down Enabled Unknown 1500 - Eth1/21 Down Enabled Unknown 1500 - Eth1/22 Down Enabled Unknown 1500 - Eth1/23 Down Enabled Unknown 1500 - Eth1/24 Down Enabled Unknown 1500 - Eth1/25 Down Enabled Unknown 1500 - Eth1/26 Down Enabled Unknown 1500 - Eth1/27 Down Enabled Unknown 1500 - Eth1/28 Down Enabled Unknown 1500 - Eth1/29 Down Enabled Unknown 1500 - Eth1/30 Down Enabled Unknown 1500 - Eth1/31 Down Enabled Unknown 1500 - Eth1/32 Down Enabled Unknown 1500 The links between the 2 spines should be port-channel 100 (\u0026lsquo;Po100\u0026rsquo;). The \u0026lsquo;mlag-port-channel\u0026rsquo; interfaces which are connections to leaf, aggregate or CDU switches would be \u0026lsquo;Mpo\u0026rsquo; interfaces with indices greater than 100. So here, \u0026lsquo;Mpo1\u0026rsquo;-\u0026lsquo;Mpo11\u0026rsquo; and \u0026lsquo;Mpo17\u0026rsquo; are connections to NCN\u0026rsquo;s, whereas \u0026lsquo;Mpo113\u0026rsquo;, \u0026lsquo;Mpo151\u0026rsquo; and \u0026lsquo;Mpo152\u0026rsquo; are connections to other switches. So identifying the port-channel and mlag-port-channel devices, we look for the \u0026ldquo;Eth\u0026rdquo; rows which have one of these labels in parentheses next to it. In the example above, these are:\nEth1/12 Eth1/13 Eth1/14 Eth1/15/1 Eth1/15/2 Eth1/18 Record these ports AND the port-channel and mlag-port-channel interfaces, as we will need all of them.\nSpine Switch \u0026lsquo;flowcontrol\u0026rsquo; Configuration Change On the Mellanox spine switches, we need to modify the flowcontrol settings on the port-channel, mlag-port-channel, and Ethernet interfaces. The general form looks like this:\nsw-spine-001 [standalone: master] # configure terminal sw-spine-001 [standalone: master] (config) # interface port-channel \u0026lt;index\u0026gt; flowcontrol receive off force sw-spine-001 [standalone: master] (config) # interface port-channel \u0026lt;index\u0026gt; flowcontrol send off force sw-spine-001 [standalone: master] (config) # interface mlag-port-channel \u0026lt;index\u0026gt; flowcontrol receive off force sw-spine-001 [standalone: master] (config) # interface mlag-port-channel \u0026lt;index\u0026gt; flowcontrol send off force sw-spine-001 [standalone: master] (config) # interface ethernet \u0026lt;port\u0026gt; flowcontrol receive off force sw-spine-001 [standalone: master] (config) # interface ethernet \u0026lt;port\u0026gt; flowcontrol send off force sw-spine-001 [standalone: master] (config) # exit sw-spine-001 [standalone: master] # write memory \u0026ldquo;index\u0026rdquo; would just be the number after \u0026ldquo;Po\u0026rdquo; or \u0026ldquo;Mpo\u0026rdquo;, so \u0026ldquo;113\u0026rdquo; or \u0026ldquo;151\u0026rdquo;. \u0026ldquo;\u0026rdquo; would be the value after \u0026ldquo;Eth\u0026rdquo;, so \u0026ldquo;1/14\u0026rdquo; or \u0026ldquo;1/15/2\u0026rdquo;. Make sure to run the \u0026lsquo;flowcontrol\u0026rsquo; commands for each mlag-port-channel and Ethernet port.\nLeaf, CDU, and Aggregate Switch \u0026lsquo;flowcontrol\u0026rsquo; Configuration Change On the Dell switches, we only need to modify the Ethernet interface configurations. The general form looks like this:\nsw-leaf-001# configure terminal sw-leaf-001(config)# interface ethernet \u0026lt;port\u0026gt; sw-leaf-001(conf-if-eth\u0026lt;port\u0026gt;)# flowcontrol receive off sw-leaf-001(conf-if-eth\u0026lt;port\u0026gt;)# flowcontrol transmit off sw-leaf-001(conf-if-eth\u0026lt;port\u0026gt;)# end sw-leaf-001# write memory One would need to do this for each port. Alternatively, you can set it up to do multiple ports as one command. For instance, the common leaf switch would have ports 51 and 52 as connections to the spine. So in that case, these commands would work:\nsw-leaf-001# configure terminal sw-leaf-001(config)# interface range ethernet 1/1/51-1/1/52 sw-leaf-001(conf-if-eth1/1/51-1/1/52)# flowcontrol receive off sw-leaf-001(conf-if-eth1/1/51-1/1/52)# flowcontrol transmit off sw-leaf-001(conf-if-eth1/1/51-1/1/52)# end sw-leaf-001# write memory Alternatively, a typical CDU switch would have ports 27 and 28 as uplinks to the spine, with ports 25 and 29 as connections to the peer CDU switch. So in that case, we would use these commands:\nsw-cdu-001# configure terminal sw-cdu-001(config)# interface range ethernet 1/1/25,1/1/27-1/1/29 sw-cdu-001(conf-range-eth1/1/25,1/1/27-1/1/29)# flowcontrol receive off sw-cdu-001(conf-range-eth1/1/25,1/1/27-1/1/29)# flowcontrol transmit off sw-cdu-001(conf-range-eth1/1/25,1/1/27-1/1/29)# end sw-cdu-001# write memory Disable iSCSI on Dell Switches (Leaf, CDU, and Aggregate) The final configuration change needed on the Dell switches is to disable iSCSI in the configuration. This change ensures that all of the flowcontrol changes made above will persist through a reboot of the switch.\nRun the following commands on all Dell switches in your system:\nsw-leaf-001# configure terminal sw-leaf-001(config)# no iscsi enable sw-leaf-001(config)# exit sw-leaf-001# write memory "
},
{
	"uri": "/docs-csm/en-09/418-mgmt-net-ip-helper/",
	"title": "IP-Helper configuration",
	"tags": [],
	"description": "",
	"content": "IP-Helper configuration This page will describe how to setup IP-Helpers on Aruba, Dell, and Mellanox switches.\nIf you are migrating from a 1.3.2 system, the IP-helpers are being moved to the switches that are doing the Layer3 Routing. For most systems this will be moving the helper from the leaf to the spine.\nIP-Helpers will reside on VLANs 1,2,4,7,2xxx, and 3xxx.\nAruba Configuration On both switches participating in VSX we will need to add configuration to the VLAN interfaces. The IP-Helper is used to forward DCHP traffic from one network to a specified IP address.\nsw-24g03(config)# int vlan 1 sw-24g03(config-if-vlan)# ip helper-address 10.92.100.222 sw-24g03(config-if-vlan)# int vlan 2 sw-24g03(config-if-vlan)# ip helper-address 10.92.100.222 sw-24g03(config-if-vlan)# int vlan 4 sw-24g03(config-if-vlan)# ip helper-address 10.94.100.222 sw-24g03(config-if-vlan)# int vlan 7 sw-24g03(config-if-vlan)# ip helper-address 10.92.100.222 sw-24g04(config)# int vlan 1 sw-24g04(config-if-vlan)# ip helper-address 10.92.100.222 sw-24g04(config-if-vlan)# int vlan 2 sw-24g04(config-if-vlan)# ip helper-address 10.92.100.222 sw-24g04(config-if-vlan)# int vlan 4 sw-24g04(config-if-vlan)# ip helper-address 10.94.100.222 sw-24g04(config-if-vlan)# int vlan 7 sw-24g04(config-if-vlan)# ip helper-address 10.92.100.222 For CDU switches the IP helpers will look like the following. Any 2xxx VLANs will have 10.92.100.222 as the ip helper-address and any 3xxx VLANs will have 10.94.100.222 as the ip helper-address\ninterface vlan 2000 ip helper-address 10.92.100.222 interface vlan 3000 ip helper-address 10.94.100.222 Dell Configuration In Shasta v1.3.2 the IP-helpers for the NMN(VLAN2), and HMN(VLAN4) resided on the leafs, these are moving to the spines.\nRemove IP-Helper configuration from the leafs.\nsw-leaf-001(config)# interface vlan 2 sw-leaf-001(conf-if-vl-2)# no ip helper-address 10.92.100.222 sw-leaf-001(config)# interface vlan 4 sw-leaf-001(conf-if-vl-4)# no ip helper-address 10.94.100.222 On CDU switches the IP-Helpers need to be set accordingly. This is the same setting as 1.3. For 2xxx VLANS the config should look like the following.\nsw-cdu-001# show running-configuration interface vlan 2000 ! interface vlan2000 mode L3 description CAB_1000_MTN_NMN no shutdown ip address 10.100.0.2/22 ip access-group nmn-hmn in ip access-group nmn-hmn out ip ospf 1 area 0.0.0.2 ip ospf passive ip helper-address 10.92.100.222 For 3xxx VLANS the config should look like the following.\nsw-cdu-001# show running-configuration interface vlan 3000 ! interface vlan3000 mode L3 description CAB_1000_MTN_HMN no shutdown ip address 10.104.0.2/22 ip access-group nmn-hmn in ip access-group nmn-hmn out ip ospf 1 area 0.0.0.4 ip ospf passive ip helper-address 10.94.100.222 Mellanox Configuration Configuration for Mellanox switch. Notice there is a helper for vlan 1,2,4, and 7.\n## DHCP relay configuration ## ip dhcp relay instance 2 vrf default ip dhcp relay instance 4 vrf default ip dhcp relay instance 2 address 10.92.100.222 ip dhcp relay instance 4 address 10.94.100.222 interface vlan 1 ip dhcp relay instance 2 downstream interface vlan 2 ip dhcp relay instance 2 downstream interface vlan 4 ip dhcp relay instance 4 downstream interface vlan 7 ip dhcp relay instance 2 downstream Verify the configuration.\nsw-spine-002 [standalone: master] # show ip dhcp relay Instance ID 2: VRF Name: default DHCP Servers: 10.92.100.222 DHCP relay agent options: always-on : Disabled Information Option: Disabled UDP port : 67 Auto-helper : Disabled ------------------------------------------- Interface Label Mode ------------------------------------------- vlan1 N/A downstream vlan2 N/A downstream vlan7 N/A downstream Instance ID 4: VRF Name: default DHCP Servers: 10.94.100.222 DHCP relay agent options: always-on : Disabled Information Option: Disabled UDP port : 67 Auto-helper : Disabled ------------------------------------------- Interface Label Mode ------------------------------------------- vlan4 N/A downstream "
},
{
	"uri": "/docs-csm/en-09/419-mgmt-net-stp/",
	"title": "Management Network Spanning-Tree Configuration.",
	"tags": [],
	"description": "",
	"content": "Management Network Spanning-Tree Configuration. Spanning tree is used to protect the network against layer2 loops.\nIt is not recommended to add/adjust these settings while a system is running.\nAruba Configuration The following configuration is applied to each of the Spine VSX pairs, this config is identical.\nspanning-tree mode rpvst spanning-tree spanning-tree priority 7 spanning-tree vlan 1,2,4,7,10 Verify that each VSX pair is the root bridge for all VLANs configured on that switch.\nsw-spine-001# show spanning-tree summary root STP status : Enabled Protocol : RPVST System ID : 02:01:00:00:01:00 Root bridge for VLANs : 1,2,4,7,10 Root Hello Max Fwd VLAN Priority Root ID cost Time Age Dly Root Port -------- -------- ----------------- --------- ----- --- --- ------------ VLAN1 32768 02:01:00:00:01:00 0 2 20 15 0 VLAN2 32768 02:01:00:00:01:00 0 2 20 15 0 VLAN4 32768 02:01:00:00:01:00 0 2 20 15 0 VLAN7 32768 02:01:00:00:01:00 0 2 20 15 0 VLAN10 32768 02:01:00:00:01:00 0 2 20 15 0 The following configuration is applied to Aruba leaf/Aggregation switches.\nspanning-tree mode rpvst spanning-tree spanning-tree vlan 1,2,4,7,10 Verify Spanning tree is configured and we are not the Root Bridge for any VLANs.\nsw-leaf-001# show spanning-tree summary root STP status : Enabled Protocol : RPVST System ID : 88:3a:30:9f:24:80 Root bridge for VLANs : Root Hello Max Fwd VLAN Priority Root ID cost Time Age Dly Root Port -------- -------- ----------------- --------- ----- --- --- ------------ VLAN1 32768 02:01:00:00:01:00 800 2 20 15 lag1 VLAN2 32768 02:01:00:00:01:00 800 2 20 15 lag1 VLAN4 32768 02:01:00:00:01:00 800 2 20 15 lag1 VLAN7 32768 02:01:00:00:01:00 800 2 20 15 lag1 VLAN10 32768 02:01:00:00:01:00 800 2 20 15 lag1 The following config is applied to Aruba CDU switches. If there are more 2xxx or 3xxx VLANs you will add them to the spanning-tree vlan list\nspanning-tree mode rpvst spanning-tree spanning-tree vlan 1,2,4,2000,3000,4091 Verify that each CDU switch is the root bridge for VLANs 2xxx and 3xxx\nsw-cdu-002# show spanning-tree summary root STP status : Enabled Protocol : RPVST System ID : 02:01:00:00:01:02 Root bridge for VLANs : 2000,3000,4091 Root Hello Max Fwd VLAN Priority Root ID cost Time Age Dly Root Port -------- -------- ----------------- --------- ----- --- --- ------------ VLAN1 32768 02:01:00:00:01:00 200 2 20 15 lag149 VLAN2 32768 02:01:00:00:01:00 200 2 20 15 lag149 VLAN4 32768 02:01:00:00:01:00 200 2 20 15 lag149 VLAN2000 32768 02:01:00:00:01:02 0 2 20 15 0 VLAN3000 32768 02:01:00:00:01:02 0 2 20 15 0 VLAN4091 32768 02:01:00:00:01:02 0 2 20 15 0 Dell Configuration Spanning tree configuration has not changed on Dell switches from 1.3 to 1.4\nDell leaf configuration\nspanning-tree vlan 1-2,4,7,10 priority 61440 Dell CDU configuration\nspanning-tree vlan 1-2,4,4091 priority 61440 Mellanox Configuration Spanning tree will need to be applied to each MAGP pair. Spine01 will have a lower priority making it the root bridge. Spanning tree configuration has not changed from 1.3 to 1.4.\n## STP configuration ## spanning-tree mode rpvst spanning-tree port type edge default interface ethernet 1/13-1/14 spanning-tree port type network interface ethernet 1/15/1-1/15/2 spanning-tree port type network interface mlag-port-channel 113 spanning-tree port type network interface mlag-port-channel 151-152 spanning-tree port type network interface ethernet 1/13-1/14 spanning-tree guard root interface ethernet 1/15/1-1/15/2 spanning-tree guard root interface mlag-port-channel 113 spanning-tree guard root interface mlag-port-channel 151-152 spanning-tree guard root spanning-tree port type edge bpdufilter default spanning-tree port type edge bpduguard default spanning-tree vlan 1-2 priority 0 spanning-tree vlan 4 priority 0 spanning-tree vlan 7 priority 0 spanning-tree vlan 10 priority 0 "
},
{
	"uri": "/docs-csm/en-09/420-mgmt-net-pxe-tshoot/",
	"title": "PXE boot Troubleshooting",
	"tags": [],
	"description": "",
	"content": "PXE boot Troubleshooting This page is designed to cover various issues that arise when trying to pxe boot nodes in a Shasta system.\nIn order for PXE booting to successfully work, the MGMT switches need to be configured correctly.\nConfiguration required for PXE booting To successfully pxe boot nodes, the following is required.\nThe IP helper-address must be configured on VLAN 1,2,4,7. This will be where the layer 3 gateway exists (spine or agg) The virtual-IP/VSX/MAGP IP must be configured on VLAN 1,2,4,7. There must be a static route pointing to the TFTP server (Aruba Only). M001 needs an active gateway on VLAN1 this can be identified from MTL.yaml generated from CSI. M001 needs an IP helper-address on VLAN1 pointing to 10.92.100.222. snippet of MTL.yaml\nname: network_hardware net-name: MTL vlan_id: 0 comment: \u0026#34;\u0026#34; gateway: 10.1.0.1 Aruba Configuration Check the configuration for interface vlan x This configuration will be the same on BOTH Switches (except the ip address). You will see that there is an active-gateway and ip helper-address configured.\nsw-spine-002(config)# show run int vlan 1 interface vlan1 vsx-sync active-gateways ip address 10.1.0.3/16 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.1.0.1 ip mtu 9198 ip helper-address 10.92.100.222 exit sw-spine-002(config)# show run int vlan 2 interface vlan2 vsx-sync active-gateways ip address 10.252.0.3/17 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.252.0.1 ip mtu 9198 ip helper-address 10.92.100.222 exit sw-spine-002(config)# show run int vlan 4 interface vlan4 vsx-sync active-gateways ip address 10.254.0.3/17 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.254.0.1 ip mtu 9198 ip helper-address 10.94.100.222 exit sw-spine-002(config)# show run int vlan 7 interface vlan7 vsx-sync active-gateways ip address 10.103.13.3/24 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.103.13.111 ip mtu 9198 ip helper-address 10.92.100.222 exit If any of this configuration is missing, you will need to update it to BOTH switches.\nsw-spine-002# conf t sw-spine-002(config)# int vlan 1 sw-spine-002(config-if-vlan)# ip helper-address 10.92.100.222 sw-spine-002(config-if-vlan)# active-gateway ip mac 12:01:00:00:01:00 sw-spine-002(config-if-vlan)# active-gateway ip 10.1.0.1 sw-spine-002# conf t sw-spine-002(config)# int vlan 2 sw-spine-002(config-if-vlan)# ip helper-address 10.92.100.222 sw-spine-002(config-if-vlan)# active-gateway ip mac 12:01:00:00:01:00 sw-spine-002(config-if-vlan)# active-gateway ip 10.252.0.1 sw-spine-002# conf t sw-spine-002(config)# int vlan 4 sw-spine-002(config-if-vlan)# ip helper-address 10.94.100.222 sw-spine-002(config-if-vlan)# active-gateway ip mac 12:01:00:00:01:00 sw-spine-002# conf t sw-spine-002(config)# int vlan 7 sw-spine-002(config-if-vlan)# ip helper-address 10.92.100.222 sw-spine-002(config-if-vlan)# active-gateway ip mac 12:01:00:00:01:00 sw-spine-002(config-if-vlan)# active-gateway ip xxxxxxx sw-spine-002(config-if-vlan)# write mem Verify the route to the TFTP server is in place. This is a static route to get to the TFTP server via a worker node. You can get the worker node IP from NMN.yaml from CSI generated data.\n- ip_address: 10.252.1.9 name: ncn-w001 comment: x3000c0s4b0n0 aliases: sw-spine-002(config)# show ip route static Displaying ipv4 routes selected for forwarding \u0026#39;[x/y]\u0026#39; denotes [distance/metric] 0.0.0.0/0, vrf default via 10.103.15.209, [1/0], static 10.92.100.60/32, vrf default via 10.252.1.7, [1/0], static You can see that the route is 10.92.100.60/32 via 10.252.1.7 with 10.252.1.7 being the worker node.\nIf that static route is missing you will need to add it.\nsw-spine-001(config)# ip route 10.92.100.60/32 10.252.1.7 Mellanox Configuration Check the configuration for interface vlan 1 This configuration will be the same on BOTH Switches (except the ip address). You will see that there is magp and ip dhcp relay configured.\nsw-spine-001 [standalone: master] # show run int vlan 1 interface vlan 1 interface vlan 1 ip address 10.1.0.2/16 primary interface vlan 1 ip dhcp relay instance 2 downstream interface vlan 1 magp 1 interface vlan 1 magp 1 ip virtual-router address 10.1.0.1 interface vlan 1 magp 1 ip virtual-router mac-address 00:00:5E:00:01:01 If this configuration is missing, you will need to add it to BOTH switches.\nsw-spine-001 [standalone: master] # conf t sw-spine-001 [standalone: master] (config) # interface vlan 1 magp 1 sw-spine-001 [standalone: master] (config interface vlan 1 magp 1) # ip virtual-router address 10.1.0.1 sw-spine-001 [standalone: master] (config interface vlan 1 magp 1) # ip virtual-router mac-address 00:00:5E:00:01:01 sw-spine-001 [standalone: master] # conf t sw-spine-001 [standalone: master] (config) # ip dhcp relay instance 2 vrf default sw-spine-001 [standalone: master] (config) # ip dhcp relay instance 2 address 10.92.100.222 sw-spine-001 [standalone: master] (config) # interface vlan 2 ip dhcp relay instance 2 downstream You can then verify the VLAN 1 MAGP configuration.\nsw-spine-001 [standalone: master] # show magp 1 MAGP 1: Interface vlan: 1 Admin state : Enabled State : Master Virtual IP : 10.1.0.1 Virtual MAC : 00:00:5E:00:01:01 Verify the DHCP relay configuration\nsw-spine-001 [standalone: master] (config) # show ip dhcp relay instance 2 VRF Name: default DHCP Servers: 10.92.100.222 DHCP relay agent options: always-on : Disabled Information Option: Disabled UDP port : 67 Auto-helper : Disabled ------------------------------------------- Interface Label Mode ------------------------------------------- vlan1 N/A downstream vlan2 N/A downstream vlan7 N/A downstream Verify that the route to the TFTP server and the route for the ingress gateway are available.\nsw-spine-001 [standalone: master] # show ip route 10.92.100.60 Flags: F: Failed to install in H/W B: BFD protected (static route) i: BFD session initializing (static route) x: protecting BFD session failed (static route) c: consistent hashing p: partial programming in H/W VRF Name default: ------------------------------------------------------------------------------------------------------ Destination Mask Flag Gateway Interface Source AD/M ------------------------------------------------------------------------------------------------------ default 0.0.0.0 c 10.101.15.161 eth1/12 static 1/1 10.92.100.60 255.255.255.255 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 c 10.252.0.7 vlan2 bgp 200/0 sw-spine-001 [standalone: master] # show ip route 10.92.100.71 Flags: F: Failed to install in H/W B: BFD protected (static route) i: BFD session initializing (static route) x: protecting BFD session failed (static route) c: consistent hashing p: partial programming in H/W VRF Name default: ------------------------------------------------------------------------------------------------------ Destination Mask Flag Gateway Interface Source AD/M ------------------------------------------------------------------------------------------------------ default 0.0.0.0 c 10.101.15.161 eth1/12 static 1/1 10.92.100.71 255.255.255.255 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 c 10.252.0.7 vlan2 bgp 200/0 If these routes are missing please see the BGP page.\nNext steps If your configuration looks good, and you are still not able to pxe boot there are some other things to try.\nRestart BSS If while watching an NCN boot attempt you see the following output on the console during PXE (specifically the 404 error at the bottom):\nhttps://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript...X509 chain 0x6d35c548 added X509 0x6d360d68 \u0026#34;eniac.dev.cray.com\u0026#34; X509 chain 0x6d35c548 added X509 0x6d3d62e0 \u0026#34;Platform CA - L1 (a0b073c8-5c9c-4f89-b8a2-a44adce3cbdf)\u0026#34; X509 chain 0x6d35c548 added X509 0x6d3d6420 \u0026#34;Platform CA (a0b073c8-5c9c-4f89-b8a2-a44adce3cbdf)\u0026#34; EFITIME is 2021-02-26 21:55:04 HTTP 0x6d35da88 status 404 Not Found Rollout a restart of the BSS deployment from any other NCN (likely ncn-m002 if you are executing the ncn-m001 reboot):\nncn-m002# kubectl -n services rollout restart deployment cray-bss deployment.apps/cray-bss restarted Then wait for this command to return (it will block showing status as the pods are refreshed):\nncn-m002# # kubectl -n services rollout status deployment cray-bss Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 old replicas are pending termination... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 old replicas are pending termination... deployment \u0026#34;cray-bss\u0026#34; successfully rolled out Then reboot the NCN one more time.\nRestart KEA In some cases rebooting the KEA pod has resolved pxe issues.\nGet KEA pod\nncn-m002# kubectl get pods -n services | grep kea cray-dhcp-kea-6bd8cfc9c5-m6bgw 3/3 Running 0 20h Delete Pod\nncn-m002# kubectl delete pods -n services cray-dhcp-kea-6bd8cfc9c5-m6bgw "
},
{
	"uri": "/docs-csm/en-09/421-mgmt-net-cabling-checklist/",
	"title": "1.4 Management network cabling checklist",
	"tags": [],
	"description": "",
	"content": "1.4 Management network cabling checklist This page is designed to be a guide on how all nodes in a Shasta system are wired to the management network.\nPrerequisites System SHCD PoR Cabling Documentation MGMT-NET-CABLING Steps Open the SHCD\nGo to the Device Diagrams Tab.\nThere you will see what type of hardware is on the system. Take note of the hardware. go to the 25G_10G or 40G_10G tab, this will depend on the SHCD.\nwe will be looking at this part of the page. Based on the vendor of the nodes and the name in the first column we can determine how it is supposed to be cabled. We can use mn01 as an example, this is a gigabyte master node, we noted the vendor in the device diagrams tab. Once you have those two pieces of information you can open the MGMT-NET-CABLING page. Locate NCN Gigabyte Master and make sure that it is cabled correctly. Complete the previous steps for all nodes listed on the SHCD.\nUANs NCNs Workers NCNs Masters NCNs Storage Application nodes Checklist Hardware Type Step Complete? UAN/Application Node Open the SHCD from the system Go to the Device Diagrams tab, take note of the type of hardware on the system Depending on the hardware, open either the 25G_10G tab or the 40G_10G tab. Locate the nodes named mn Based on the vendor of the node and the name in the first column we can determine how it is supposed to be cabled. Check cabling against the MGMT-NET-CABLING page. If it is cabled correctly you are done. If it is cabled incorrectly, contact the team in charge of cabling and request a change. NCN-Master Open the SHCD from the system Go to the Device Diagrams tab, take note of the type of hardware on the system Depending on the hardware, open either the 25G_10G tab or the 40G_10G tab. Locate the nodes named mn0x Based on the vendor of the node and the name in the first column we can determine how it is supposed to be cabled. Check cabling against the MGMT-NET-CABLING page. If it is cabled correctly you are done. If it is cabled incorrectly, contact the team in charge of cabling and request a change. NCN-Worker Open the SHCD from the system Go to the Device Diagrams tab, take note of the type of hardware on the system Depending on the hardware, open either the 25G_10G tab or the 40G_10G tab. Locate the nodes named wn0x Based on the vendor of the node and the name in the first column we can determine how it is supposed to be cabled. Check cabling against the MGMT-NET-CABLING page. If it is cabled correctly you are done. If it is cabled incorrectly, contact the team in charge of cabling and request a change. NCN-Storage Open the SHCD from the system Go to the Device Diagrams tab, take note of the type of hardware on the system Depending on the hardware, open either the 25G_10G tab or the 40G_10G tab. Locate the nodes named sn0x Based on the vendor of the node and the name in the first column we can determine how it is supposed to be cabled. Check cabling against the MGMT-NET-CABLING page. If it is cabled correctly you are done. If it is cabled incorrectly, contact the team in charge of cabling and request a change. "
},
{
	"uri": "/docs-csm/en-09/500-uas-uai-admin-and-user-guide/",
	"title": "The Shasta User Access Service",
	"tags": [],
	"description": "",
	"content": "The Shasta User Access Service The Shasta User Access Service Concepts End-User UAIs Special Purpose UAIs Elements of a UAI UAI Host Nodes UAI Network Attachments (macvlans) UAI Host Node Selection Identifying UAI Host Nodes Specifying UAI Host Nodes Maintaining an HSM Group for UAI Host Nodes UAI Network Attachments CSI Localization Data Contents of customizations.yaml UAS Helm Chart UAI Network Attachment in Kubernetes UAS Configuration to Support UAI Creation UAI Images Listing Registered UAI Images Registering UAI Images Examining a UAI Image Registration Updating a UAI Image Registration Deleting a UAI Image Registration Volumes Listing UAS Volumes Adding a UAS Volume Examining a UAS Volume Updating a UAS Volume Deleting a UAS Volume Resource Specifications Listing Resource Specifications Adding a Resource Specification Examining a Resource Specification Updating a Resource Specification Deleting a Resource Specification UAI Classes Listing UAI Classes Adding a UAI Class Examining a UAI Class Updating a UAI Class Deleting a UAI Class UAI Management Administrative Management of UAIs Listing UAIs Creating UAIs Examining UAIs Deleting UAIs Legacy Mode UAI Management Configuring A Default UAI Class for Legacy Mode Example Minimal Default UAI Class Example Default UAI Class with Slurm Support Creating and Using Default UAIs in Legacy Mode Listing Available UAI Images in Legacy Mode Creating UAIs From Specific UAI Images in Legacy Mode The UAI Broker Based Mode Configuring End-User UAI Classes for Broker Mode Configuring a Broker UAI class An Example of Volumes to Connect broker UAIs to LDAP Starting a Broker UAI Logging In Through a Broker UAI UAI Images The Provided Broker UAI Image Customizing the Broker UAI Image Customizing the Broker UAI Entrypoint Script Customizing the Broker UAI SSH Configuration The Provided End-User UAI Image Custom End-User UAI Images Building a Custom End-User UAI Image Query BOS for a sessiontemplate ID Download a Compute Node squashfs Mount the squashfs and Create a tarball Create and Push the Container Image Register the New Container Image With UAS Cleanup the Mount Directory and tarball Troubleshooting Getting Log Output from UAS Getting Log Output from UAIs Stale Brokered UAIs Stuck UAIs Duplicate Mount Paths in a UAI Missing or Incorrect UAI Images Administrative Access to UAIs for Diagnosis Common Mistakes to Check for When Making a Custom End-User UAI Image Concepts The User Access Service (UAS) is responsible for managing User Access Instances (UAIs) and their associated configuration. A UAI is a lightweight, disposable platform that runs under Kubernetes orchestration. Most UAIs are reachable through SSH, and the most common use of UAIs is to facilitate short-term interactive logins for users of the Shasta system.\nEnd-User UAIs UAIs used for interactive logins are called end-user UAIs. End-user UAIs can be seen as lightweight User Access Nodes (UANs), but there are important differences between UAIs and UANs. First, end-user UAIs are not dedicated hardware like UANs. They are implemented as containers orchestrated by Kubernetes, which makes them subject to Kubernetes scheduling and resource management rules. One key element of Kubernetes orchestration is impermanence. While end-user UAIs are often long running, Kubernetes can re-schedule or re-create them as needed to meet resource and node availability constraints. UAIs can also be removed administratively. When either of these things happen, a new UAI may be created, but that new UAI reverts to its initial state, discarding any internal changes that might have been made in its previous incarnation. An administratively removed end-user UAI may or may not ever be re-created, and an end-user UAI that is preempted because of resource pressure may become unavailable for an extended time until the pressure is relieved.\nThe impermanence of end-user UAIs makes them suitable for tasks that are immediate and interactive over relatively short time frames, such as building and testing software or launching workloads. It makes them unsuitable for unattended activities like executing cron jobs or monitoring progress of a job in a logged-in shell unless those activities are built into the UAI image itself (more on custom UAI images later). These kinds of activities are more suited to UANs, which are more permanent and, unless they are re-installed, retain modified state through reboots and so forth.\nAnother way end-user UAIs differ from UANs is that any given end-user UAI is restricted to serving a single user. This protects users from interfering with each other within UAIs and means that any user who wants to use a UAI has to arrange for the UAI to be created and assigned. Once a user has an end-user UAI assigned, the user may initiate any number of SSH sessions to that UAI, but no other user will be recognized by the UAI when attempting to connect.\nSpecial Purpose UAIs While most UAIs are end-user UAIs, UAI classes make it possible to construct UAIs to serve special purposes that are not strictly end-user oriented. One kind of special purpose UAI is the broker UAI which provides on demand end-user UAI launch and management. While no other specialty UAI types currently exist, other applications are expected to arise and users are encouraged to innovate as needed.\nElements of a UAI All UAIs can have the following attributes associated with them:\nA required container image An optional set of volume An optional resource specification An optional collection of smaller configuration items The container image for a UAI (UAI image), defines the basic environment including the flavor of operating system, the installed packages, and so forth available to the user. UAI images can be customized by a site and added to the UAS configuration to be used in UAI creation. Any number of UAI images can be configured in the UAS, though only one will be used by any given UAI. The UAS comes with some pre-defined UAI images that make it possible to set up UAIs and run many common tasks without further customization. Later in this document there is a procedure for making custom UAIs as needed.\nThe volumes defined for a UAI provide for external access to data provided by the host system. Examples of this range from Kubernetes \u0026ldquo;configmaps\u0026rdquo; and \u0026ldquo;secrets\u0026rdquo; to external file systems used for persistent storage or external data access. Anything that can be defined as a volume in a Kubernetes pod specification can be configured in UAS as a volume and used within a UAI.\nResource requests and limits tell Kubernetes how much memory and CPU a given UAI wants all the time (request) and how much memory and CPU a UAI can ever be given (limit). Resource specifications configured into UAS contain resource requests and / or limits that can be associated with a UAI. Any resource request or limit that can be set up on a Kubernetes pod can be set up as a resource specification under UAS.\nThe smaller configuration items control things like whether the UAI can talk to compute nodes over the high-speed network (needed for workload management), whether the UAI presents a public facing or private facing IP address for SSH, Kubernetes scheduling priority and others.\nAll of the above can be customized on a given set of UAIs by defining a UAI class. UAI classes are templates used to create UAIs, and provide access to fine grained configuration and selection of image, volumes and resource specification. While an end-user UAI can be created by simply specifying its UAI image and the user\u0026rsquo;s public key, to make more precisely constructed UAIs a UAI class must be used.\nUAI Host Nodes UAIs run on Kubernetes worker nodes. There is a mechanism using Kubernetes labels to prevent UAIs from running on a specific worker node, however. Any Kubernetes node that is not labeled to prevent UAIs from running on it is considered to be a UAI host node. The administrator of a given site may control the set of UAI host nodes by labeling Kubernetes worker nodes appropriately.\nUAI Network Attachments (macvlans) UAIs need to be able to reach compute nodes across the node management network (NMN). When the compute node NMN is structured as multiple subnets, this requires routing form the UAIs to those subnets. The default route in a UAI goes to the public network through the customer access network (CAN) so that will not work for reaching compute nodes. To solve this problem, UAS installs Kubernetes network attachments within the Kubernetes user namespace, one of which is used by UAIs. The type of network attachment used on Shasta hardware for this purpose is a macvlan network attachment, so this is often referred to on Shasta systems as \u0026ldquo;macvlans\u0026rdquo;. This network attachment integrates the UAI into the NMN on the UAI host node where the UAI is running and assigns the UAI an IP address on that network. It also installs a set of routes in the UAI that are used to reach the compute node subnets on the NMN.\nUAI Host Node Selection When selecting UAI host nodes, it is a good idea to take into account the amount of combined load users and system services will bring to those nodes. UAIs run by default at a lower priority than system services on worker nodes which means that, if the combined load exceeds the capacity of the nodes, Kubernetes will eject UAIs and / or refuse to schedule them to protect system services. This can be disruptive or frustrating for users. This section explains how to identify the currently configured UAI host nodes and how to adjust that selection to meet the needs of users.\nIdentifying UAI Host Nodes Since UAI host node identification is an exclusive activity, not an inclusive one, it starts by identifying the nodes that could potentially be UAI host nodes by their Kubernetes role:\nncn-m001-pit# kubectl get nodes | grep -v master NAME STATUS ROLES AGE VERSION ncn-w001 Ready \u0026lt;none\u0026gt; 10d v1.18.6 ncn-w002 Ready \u0026lt;none\u0026gt; 25d v1.18.6 ncn-w003 Ready \u0026lt;none\u0026gt; 23d v1.18.6 On this system, there are 3 nodes known by Kubernetes that are not running as Kubernetes master nodes. These are all potential UAI host nodes. Next, identify the nodes that are excluded from eligibility as UAI host nodes:\nncn-m001-pit# kubectl get no -l uas=False NAME STATUS ROLES AGE VERSION ncn-w001 Ready \u0026lt;none\u0026gt; 10d v1.18.6 NOTE: given the fact that labels are textual not boolean, it is a good idea to try various common spellings of false. The ones that will prevent UAIs from running are 'False', 'false' and 'FALSE'. Repeat the above with all three options to be sure. Of the non-master nodes, there is one node that is configured to reject UAIs, ncn-w001. So, ncn-w002 and ncn-w003 are UAI host nodes.\nSpecifying UAI Host Nodes UAI host nodes are determined by tainting the nodes against UAIs, so the following:\nncn-m001-pit# kubectl label node ncn-w001 uas=False --overwrite Please note here that setting uas=True or any variant of that, while potentially useful for local book keeping purposes, does NOT transform the node into a UAS host node. With that setting the node will be a UAS node because the value of the uas flag is not in the list False, false or FALSE, but unless the node previously had one of the false values, it was a UAI node all along. Perhaps more to the point, removing the uas label from a node labeled uas=True does not take the node out of the list of UAI host nodes. The only way to make a non-master Kubernetes node not be a UAS host node is to explicitly set the label to False, false or FALSE.\nMaintaining an HSM Group for UAI Host Nodes When it comes to customizing non-compute node (NCN) contents for UAIs, it is useful to have a Hardware State Manager (HSM) node group containing the NCNs that are UAI hosts nodes. The hpe-csm-scripts package provides a script called make_node_groups that is useful for this purpose. This script is normally installed as /opt/cray/csm/scripts/node_management/make_node_groups. It can create and update node groups for management master nodes, storage nodes, management worker nodes, and UAI host nodes. The following summarizes its use:\nncn-m001# /opt/cray/csm/scripts/node_management/make_node_groups --help getopt: unrecognized option \u0026#39;--help\u0026#39; usage: make_node_groups [-m][-s][-u][w][-A][-R][-N] Where: -m - creates a node group for managment master nodes -s - creates a node group for management storage nodes -u - creates a node group for UAI worker nodes -w - creates a node group for management worker nodes -A - creates all of the above node groups -N - executes a dry run, showing commands not running them -R - deletes existing node group(s) before creating them Here is an example of a dry-run that will create or update a node group for UAI host nodes:\nncn-m001# /opt/cray/csm/scripts/node_management/make_node_groups -N -R -u (dry run)cray hsm groups delete uai (dry run)cray hsm groups create --label uai (dry run)cray hsm groups members create uai --id x3000c0s4b0n0 (dry run)cray hsm groups members create uai --id x3000c0s5b0n0 (dry run)cray hsm groups members create uai --id x3000c0s6b0n0 (dry run)cray hsm groups members create uai --id x3000c0s7b0n0 (dry run)cray hsm groups members create uai --id x3000c0s8b0n0 (dry run)cray hsm groups members create uai --id x3000c0s9b0n0 Notice that when run in dry-run (-N option) mode, the script only prints out the CLI commands it will execute without actually executing them. When run with the -R option, the script removes any existing node groups before recreating them, effectively updating the contents of the node group. The -u option tells the script to create or update only the node group for UAI host nodes. That node group is named uai in the HSM.\nSo, to create a new node group or replace an existing one, called uai, containing the list of UAI host nodes, use the following command:\n# /opt/cray/csm/scripts/node_management/make_node_groups -R -u UAI Network Attachments The UAI network attachment configuration flows from the CRAY Site Initializer (CSI) localization data through customizations.yaml into the UAS Helm chart and, ultimately, into Kubernetes in the form of a \u0026ldquo;network-attachment-definition\u0026rdquo;. This section describes the data at each of those stages to show how the final network attachment gets created.\nCSI Localization Data The details of CSI localization are beyond the scope of this guide, but here are the important settings, and the values used in the following examples:\nThe interface name on which the Kubernetes worker nodes reach their NMN subnet: vlan002 The network and CIDR configured on that interface: 10.252.0.0/17 The IP address of the gateway to other NMN subnets found on that network: 10.252.0.1 The subnets where compute nodes reside on this system: 10.92.100.0/24 10.106.0.0/17 10.104.0.0/17 Contents of customizations.yaml When CSI runs it produces the following data structure in the spec section of customizations.yaml:\nspec: ... wlm: ... macvlansetup: nmn_subnet: 10.252.2.0/23 nmn_supernet: 10.252.0.0/17 nmn_supernet_gateway: 10.252.0.1 nmn_vlan: vlan002 # NOTE: the term DHCP here is misleading, this is merely # a range of reserved IPs for UAIs that should not # be handed out to others because the network # attachment will hand them out to UAIs. nmn_dhcp_start: 10.252.2.10 nmn_dhcp_end: 10.252.3.254 routes: - dst: 10.92.100.0/24 gw: 10.252.0.1 - dst: 10.106.0.0/17 gw: 10.252.0.1 - dst: 10.104.0.0/17 gw: 10.252.0.1 The nmn_subnet value shown here is not of interest to this discussion.\nThese values, in turn, feed into the following translation to UAS Helm chart settings:\ncray-uas-mgr: uasConfig: uai_macvlan_interface: \u0026#39;{{ wlm.macvlansetup.nmn_vlan }}\u0026#39; uai_macvlan_network: \u0026#39;{{ wlm.macvlansetup.nmn_supernet }}\u0026#39; uai_macvlan_range_start: \u0026#39;{{ wlm.macvlansetup.nmn_dhcp_start }}\u0026#39; uai_macvlan_range_end: \u0026#39;{{ wlm.macvlansetup.nmn_dhcp_end }}\u0026#39; uai_macvlan_routes: \u0026#39;{{ wlm.macvlansetup.routes }}\u0026#39; UAS Helm Chart The inputs above tell the UAS Helm chart how to install the network attachment for UAIs. While the actual template used for this is more complex, here is a simplified view of the template used to generate the network attachment (if you are reading this document from the UAS source code, you can find the real template in the Helm chart there):\napiVersion: \u0026#34;k8s.cni.cncf.io/v1\u0026#34; kind: NetworkAttachmentDefinition ... spec: config: \u0026#39;{ \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;macvlan\u0026#34;, \u0026#34;master\u0026#34;: \u0026#34;{{ .Values.uasConfig.uai_macvlan_interface }}\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;{{ .Values.uasConfig.uai_macvlan_network }}\u0026#34;, \u0026#34;rangeStart\u0026#34;: \u0026#34;{{ .Values.uasConfig.uai_macvlan_range_start }}\u0026#34;, \u0026#34;rangeEnd\u0026#34;: \u0026#34;{{ .Values.uasConfig.uai_macvlan_range_end }}\u0026#34;, \u0026#34;routes\u0026#34;: [ {{- range $index, $route := .Values.uasConfig.uai_macvlan_routes }} {{- range $key, $value := $route }} { \u0026#34;{{ $key }}\u0026#34;: \u0026#34;{{ $value }}\u0026#34;, }, {{- end }} {{- end }} ] } }\u0026#39; The range templating in the routes section expands the routes from customizations.yaml into the network attachment routes.\nUAI Network Attachment in Kubernetes All of this produces a network attachment definition in Kubernetes called macvlan-uas-nmn-conf which is used by UAS. Here are the contents that would result from the above data:\napiVersion: v1 items: - apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition ... spec: config: \u0026#39;{ \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;macvlan\u0026#34;, \u0026#34;master\u0026#34;: \u0026#34;vlan002\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;10.252.0.0/17\u0026#34;, \u0026#34;rangeStart\u0026#34;: \u0026#34;10.252.124.10\u0026#34;, \u0026#34;rangeEnd\u0026#34;: \u0026#34;10.252.125.244\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;10.92.100.0/24\u0026#34;, \u0026#34;gw\u0026#34;: \u0026#34;10.252.0.1\u0026#34; }, { \u0026#34;dst\u0026#34;: \u0026#34;10.106.0.0/17\u0026#34;, \u0026#34;gw\u0026#34;: \u0026#34;10.252.0.1\u0026#34; }, { \u0026#34;dst\u0026#34;: \u0026#34;10.104.0.0/17\u0026#34;, \u0026#34;gw\u0026#34;: \u0026#34;10.252.0.1\u0026#34; } ] } }\u0026#39; ... The above tells Kubernetes to assign UAI IP addresses in the range 10.252.2.10 through 10.252.3.244 on the network attachment, and permits those UAIs to reach compute nodes on any of four possible NMN subnets:\ndirectly through the NMN subnet hosting the UAI host node itself (10.252.0.0/17 here) through the gateway in the local NMN subnet (10.252.0.1 here) to 10.92.100.0/24 10.106.0.0/17 10.104.0.0/17 UAS Configuration to Support UAI Creation Options for the elements of a UAI are maintained in the UAS configuration. The following can be configured in the UAS:\nUAI images Volumes Resource specifications UAI Classes To configure the UAS a user needs to be defined as an administrator in the Shasta system and logged in using the Shasta CLI (cray command). This can be done from a LiveCD node or from any system with the Shasta CLI installed that can reach the Shasta API Gateway. The following sections illustrate creating, updating, examining and removing configuration items from the UAS. More information on configuring and authenticating through the Shasta CLI can be found in the Shasta installation and administration guides.\nUAI Images UAS provides two stock UAI images when installed. The first is a standard end-user UAI Image that has the necessary software installed in it to support a basic Linux distribution login experience. This image also comes with with the Slurm and PBS Professional workload management client software installed, allowing users to take advantage of one or both of these if the underlying support is installed on the host system. The second image is a broker UAI image. Broker UAIs are a special type of UAIs used in the \u0026ldquo;broker based\u0026rdquo; operation model. Broker UAIs present a single SSH endpoint that responds to each SSH connection by locating or creating a suitable end-user UAI and redirecting the SSH session to that end-user UAI.\nUAS also permits creation and registration of custom UAI images.\nListing Registered UAI Images To see what UAI images have been registered with UAS, use the following Shasta CLI command:\nncn-m001-pit# cray uas admin config images list Here is a sample execution of that command:\nncn-m001-pit# cray uas admin config images list [[results]] default = true image_id = \u0026#34;08a04462-195a-4e66-aa31-08076072c9b3\u0026#34; imagename = \u0026#34;registry.local/cray/cray-uas-sles15:latest\u0026#34; [[results]] default = false image_id = \u0026#34;f8d5f4da-c910-421c-92b6-794ab8cc7e70\u0026#34; imagename = \u0026#34;registry.local/cray/cray-uai-broker:latest\u0026#34; [[results]] default = false image_id = \u0026#34;8fdf5d4a-c190-24c1-2b96-74ab98c7ec07\u0026#34; imagename = \u0026#34;registry.local/cray/custom-end-user-uai:latest\u0026#34; The output shown above shows three image registrations. Each has an imagename indicating the image to be used to construct a UAI.\nNOTE: simply registering a UAI image name does not make the image available. The image must also be created and stored in the container registry. This is covered in [???where???]. There is also a default flag. If this flag is true the image will be used whenever a UAI is created without specifying an image or UAI class as part of the creation. Finally, there is an image_id, which identifies this image registration for later inspection, update, or deletion and for linking the image to a UAI class.\nRegistering UAI Images To register a UAI image with UAS, here is the minimum required CLI command form:\nncn-m001-pit# cray uas admin config images create --imagename \u0026lt;image_name\u0026gt; To register the image registry.local/cray/custom-end-user-uai:latest, the stock end-user UAI image, use:\nncn-m001-pit# cray uas admin config images create --imagename registry.local/cray/custom-end-user-uai:latest In addition to registering an image UAS allows the image to be set as the default image. There can be at most one default image defined at any given time, and setting an image as default causes any previous default image to cease to be default. To register the above image as the default image, use:\nncn-m001-pit# cray uas admin config images create --imagename registry.local/cray/custom-end-user-uai:latest --default yes to register the image explicitly as non-default:\nncn-m001-pit# cray uas admin config images create --imagename registry.local/cray/custom-end-user-uai:latest --default no The last command is usually not necessary, because when the --default option is omitted, images are registered as non-default.\nExamining a UAI Image Registration Once an image is registered, it can be examined using a command of the form\ncray uas admin config images describe \u0026lt;image-id\u0026gt; For example:\ncray uas admin config images describe 8fdf5d4a-c190-24c1-2b96-74ab98c7ec07 [[results]] default = false image_id = \u0026#34;8fdf5d4a-c190-24c1-2b96-74ab98c7ec07\u0026#34; imagename = \u0026#34;registry.local/cray/custom-end-user-uai:latest\u0026#34; Updating a UAI Image Registration Once an image is registered, it can be updated using a command of the form\nncn-m001-pit# cray uas admin config images update [options] \u0026lt;image_id\u0026gt; Use the --default or --imagename options as specified when registering an image to update those specific elements of an existing image registration. For example, to make the registry.local/cray/custom-end-user-uai:latest image shown above the default image, use:\nncn-m001-pit# cray uas admin config images update --default yes 8fdf5d4a-c190-24c1-2b96-74ab98c7ec07 Deleting a UAI Image Registration To delete a UAS image registration, use the following form:\nncn-m001-pit# cray uas admin config images delete \u0026lt;image_id\u0026gt; For example, to delete the above image registration:\nncn-m001-pit# cray uas admin config images delete 8fdf5d4a-c190-24c1-2b96-74ab98c7ec07 Volumes Volumes provide a way to connect UAIs to external data, whether they be Kubernetes managed objects, external file systems or files, host node files and directories, or remote networked data to be used within the UAI. Some examples of how volumes are commonly used by UAIs are:\nTo connect UAIs to configuration files like /etc/localtime maintained by the host node To connect end-user UAIs to Slurm or PBS Professional Workload Manager configuration shared through Kubernetes To connect end-user UAIs to Programming Environment libraries and tools hosted on the UAI host nodes To connect end-user UAIs to Lustre or other external storage for user data To connect broker UAIs to a directory service or SSH configuration needed to authenticate and redirect user sessions. Any kind of volume recognized by the Kubernetes installation can be installed as a volume within UAS and will be used when creating UAIs. There is more information on Kubernetes volumes here.\nNOTE: as with UAI images, registering a volume with UAS creates the configuration that will be used to create a UAI. If the underlying object referred to by the volume does not exist at the time the UAI is created, the UAI will, in most cases, wait until the object becomes available before starting up. This will be visible in the UAI state which will eventually move to `waiting`. Listing UAS Volumes To list volumes in UAS, use the following Shasta CLI command:\nncn-m001-pit# cray uas admin config volumes list Here is an example of the output from this command:\nncn-m001-pit# cray uas admin config volumes list [[results]] mount_path = \u0026#34;/lus\u0026#34; volume_id = \u0026#34;2b23a260-e064-4f3e-bee5-3da8e3664f29\u0026#34; volumename = \u0026#34;lustre\u0026#34; [results.volume_description.host_path] path = \u0026#34;/lus\u0026#34; type = \u0026#34;DirectoryOrCreate\u0026#34; [[results]] mount_path = \u0026#34;/etc/slurm\u0026#34; volume_id = \u0026#34;53ea3f18-b202-455f-a8ec-79f9463aeb7b\u0026#34; volumename = \u0026#34;slurm-config\u0026#34; [results.volume_description.config_map] name = \u0026#34;slurm-map\u0026#34; [[results]] mount_path = \u0026#34;/root/slurm_config/munge\u0026#34; volume_id = \u0026#34;656aed94-fb5a-4b94-bcb7-19607bd8670f\u0026#34; volumename = \u0026#34;munge-key\u0026#34; [results.volume_description.secret] secret_name = \u0026#34;munge-secret\u0026#34; [[results]] mount_path = \u0026#34;/etc/pbs\u0026#34; volume_id = \u0026#34;7ee2bbe9-6428-43c8-b626-0d2316f3aff8\u0026#34; volumename = \u0026#34;pbs-config\u0026#34; [results.volume_description.config_map] name = \u0026#34;pbs-config\u0026#34; [[results]] mount_path = \u0026#34;/opt/forge_license\u0026#34; volume_id = \u0026#34;99e705a2-9bde-48cf-934d-ae721403d8fa\u0026#34; volumename = \u0026#34;optforgelicense\u0026#34; [results.volume_description.host_path] path = \u0026#34;/opt/forge_license\u0026#34; type = \u0026#34;DirectoryOrCreate\u0026#34; [[results]] mount_path = \u0026#34;/opt/forge\u0026#34; volume_id = \u0026#34;de224953-f5de-42f4-9d18-638855799dba\u0026#34; volumename = \u0026#34;opt-forge\u0026#34; [results.volume_description.host_path] path = \u0026#34;/opt/forge\u0026#34; type = \u0026#34;DirectoryOrCreate\u0026#34; [[results]] mount_path = \u0026#34;/etc/localtime\u0026#34; volume_id = \u0026#34;ef4be476-79c4-4b76-a9ba-e6dccf2a16db\u0026#34; volumename = \u0026#34;timezone\u0026#34; [results.volume_description.host_path] path = \u0026#34;/etc/localtime\u0026#34; type = \u0026#34;FileOrCreate\u0026#34; This TOML formatted output can be challenging to read, so it may be more reasonable to obtain the information in YAML format:\nncn-m001-pit# cray uas admin config volumes list --format yaml - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: 2b23a260-e064-4f3e-bee5-3da8e3664f29 volumename: lustre - mount_path: /etc/slurm volume_description: config_map: name: slurm-map volume_id: 53ea3f18-b202-455f-a8ec-79f9463aeb7b volumename: slurm-config - mount_path: /root/slurm_config/munge volume_description: secret: secret_name: munge-secret volume_id: 656aed94-fb5a-4b94-bcb7-19607bd8670f volumename: munge-key - mount_path: /etc/pbs volume_description: config_map: name: pbs-config volume_id: 7ee2bbe9-6428-43c8-b626-0d2316f3aff8 volumename: pbs-config - mount_path: /opt/forge_license volume_description: host_path: path: /opt/forge_license type: DirectoryOrCreate volume_id: 99e705a2-9bde-48cf-934d-ae721403d8fa volumename: optforgelicense - mount_path: /opt/forge volume_description: host_path: path: /opt/forge type: DirectoryOrCreate volume_id: de224953-f5de-42f4-9d18-638855799dba volumename: opt-forge - mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: ef4be476-79c4-4b76-a9ba-e6dccf2a16db volumename: timezone or JSON format:\nncn-m001-pit# cray uas admin config volumes list --format json [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;2b23a260-e064-4f3e-bee5-3da8e3664f29\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/slurm\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;config_map\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;slurm-map\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;53ea3f18-b202-455f-a8ec-79f9463aeb7b\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;slurm-config\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/root/slurm_config/munge\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;secret\u0026#34;: { \u0026#34;secret_name\u0026#34;: \u0026#34;munge-secret\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;656aed94-fb5a-4b94-bcb7-19607bd8670f\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;munge-key\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/pbs\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;config_map\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pbs-config\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;7ee2bbe9-6428-43c8-b626-0d2316f3aff8\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;pbs-config\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/opt/forge_license\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/opt/forge_license\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;99e705a2-9bde-48cf-934d-ae721403d8fa\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;optforgelicense\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/opt/forge\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/opt/forge\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;de224953-f5de-42f4-9d18-638855799dba\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;opt-forge\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;ef4be476-79c4-4b76-a9ba-e6dccf2a16db\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; } ] NOTE: The JSON format above will be useful in figuring out how to add or update a volume in the UAS configuration because that is the form in which the volume description is applied to the volume. Looking at the above output, each volume has a mount_path, volume_description, \u0026lsquo;volume_nameandvolume_identry. Themount_path` specifies where in the UAI the volume will be mounted.\nNOTE: While it is acceptable to have multiple volumes configured in UAS with the same `mount_path` any given UAI will fail creation if it has more than one volume specified for a given mount path. If multiple volumes with the same mount path exist in the UAS configuration all UAIs must be created using UAI classes that specify a workable subset of volumes. A UAI created without a UAI Class under such a UAS configuration will try to use all configured volumes and creation will fail. The volume_description is the JSON description of the volume, specified as a dictionary with one entry, whose key identifies the kind of Kubernetes volume is described (i.e. host_path, configmap, secret, etc.) whose value is another dictionary containing the Kubernetes volume description itself. See Kubernetes documentation for details on what goes in various kinds of volume descriptions.\nThe volumename is a string the creator of the volume may chose to describe or name the volume. It must be comprised of only lower case alphanumeric characters and dashes (\u0026rsquo;-\u0026rsquo;) and must begin and end with an alphanumeric character. It is used inside the UAI pod specification to identify the volume that is mounted in a given location in a container. It is required and administrators are free to use any name that meets the requirements. Volume names do need to be unique within any given UAI and are far more useful when searching for a volume if they are unique across the UAS configuration.\nThe volume_id is a unique identifier used to identify the UAS volume when examining, updating or deleting a volume and when linking a volume to a UAI class.\nAdding a UAS Volume To add a UAS volume use a command of the form:\nncn-m001-pit# cray uas admin config volumes create --mount-path \u0026lt;path in UAI\u0026gt; --volume-description \u0026#39;{\u0026#34;\u0026lt;volume-kind\u0026gt;\u0026#34;: \u0026lt;k8s-volume-description\u0026gt;}\u0026#39; --volumename \u0026#39;\u0026lt;string\u0026gt;\u0026#39; For example:\nncn-m001-pit# cray uas admin config volumes create --mount-path /host_files/host_passwd --volume-description \u0026#39;{\u0026#34;host_path\u0026#34;: {\u0026#34;path\u0026#34;: \u0026#34;/etc/passwd\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34;}}\u0026#39; --volumename \u0026#39;my-volume-with-passwd-from-the-host-node\u0026#39; will create a directory /host_files in every UAI configured to use this volume and mount the file /etc/passwd from the host node into that directory as a file named host_passwd. Notice the form of the --volume-description argument. It is a JSON string encapsulating an entire volume_description field as shown in the JSON output in the previous section.\nExamining a UAS Volume Once a UAS volume has been configured, it can be examined individually using a command of the form\nncn-m001-pit# cray uas admin config volumes describe \u0026lt;volume-id\u0026gt; The --format option can be used here to obtain formats other than TOML that may be easier to read. For example:\nncn-m001-pit# cray uas admin config volumes describe a0066f48-9867-4155-9268-d001a4430f5c --format json { \u0026#34;mount_path\u0026#34;: \u0026#34;/host_files/host_passwd\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/passwd\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;a0066f48-9867-4155-9268-d001a4430f5c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;my-volume-with-passwd-from-the-host-node\u0026#34; } Updating a UAS Volume Once a UAS volume has been configured, any part of it except for the volume_id can be updated with a command of the form\ncray uas admin config volumes update [options] \u0026lt;volume-id\u0026gt; For example:\nncn-m001-pit# cray uas admin config volumes update --volumename \u0026#39;my-example-volume\u0026#39; a0066f48-9867-4155-9268-d001a4430f5c The --volumename, --volume-description, and --mount-path options may be used in any combination to update the configuration of a given volume.\nDeleting a UAS Volume To delete a UAS Volume use a command of the form\nncn-m001-pit# cray uas admin config volumes delete \u0026lt;volume-id\u0026gt; For example:\nncn-m001-pit# cray uas admin config volumes delete a0066f48-9867-4155-9268-d001a4430f5c Resource Specifications Kubernetes uses resource limits and resource requests, to manage the system resources available to pods. Since UAIs run as pods under Kubernetes, UAS takes advantage of Kubernetes to manage the system resources available to UAIs. In the UAS configuration, resource specifications contain that configuration. A UAI that is assigned a resource specification will use that instead of the default resource limits / requests on the Kubernetes namespace containing the UAI. This can be used to fine-tune resources assigned to UAIs.\nListing Resource Specifications To list the available resource specifications, use the following:\nncn-m001-pit# cray uas admin config resources list for example:\nncn-m001-pit# cray uas admin config resources list [[results]] comment = \u0026#34;my first example resource specification\u0026#34; limit = \u0026#34;{\\\u0026#34;cpu\\\u0026#34;: \\\u0026#34;300m\\\u0026#34;, \\\u0026#34;memory\\\u0026#34;: \\\u0026#34;250Mi\\\u0026#34;}\u0026#34; request = \u0026#34;{\\\u0026#34;cpu\\\u0026#34;: \\\u0026#34;300m\\\u0026#34;, \\\u0026#34;memory\\\u0026#34;: \\\u0026#34;250Mi\\\u0026#34;}\u0026#34; resource_id = \u0026#34;85645ff3-1ce0-4f49-9c23-05b8a2d31849\u0026#34; [[results]] comment = \u0026#34;my second example resource specification\u0026#34; limit = \u0026#34;{\\\u0026#34;cpu\\\u0026#34;: \\\u0026#34;4\\\u0026#34;, \\\u0026#34;memory\\\u0026#34;: \\\u0026#34;1Gi\\\u0026#34;}\u0026#34; request = \u0026#34;{\\\u0026#34;cpu\\\u0026#34;: \\\u0026#34;4\\\u0026#34;, \\\u0026#34;memory\\\u0026#34;: \\\u0026#34;1Gi\\\u0026#34;}\u0026#34; resource_id = \u0026#34;eff9e1f2-3560-4ece-a9ce-8093e05e032d\u0026#34; There are three configurable parts to a resource specification, a limit which is a JSON string describing a Kubernetes resource limit, a request which is a JSON string describing a Kubernetes resource request and an optional comment which is a free form string containing any information an administrator might find useful about the resource specification. There is also a resource-id that can be used for examining, updating or deleting the resource specification as well as linking the resource specification into a UAI class.\nAdding a Resource Specification Add a new resource specification using a command of the form\nncn-m001-pit # cray uas admin config resources create [--limit \u0026lt;k8s-resource-limit\u0026gt;] [--request \u0026lt;k8s-resource-request\u0026gt;] [--comment \u0026#39;\u0026lt;string\u0026gt;\u0026#39;] For example:\nncn-m001-pit# cray uas admin config resources create --request \u0026#39;{\u0026#34;cpu\u0026#34;: \u0026#34;300m\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;250Mi\u0026#34;}\u0026#39; --limit \u0026#39;{\u0026#34;cpu\u0026#34;: \u0026#34;300m\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;250Mi\u0026#34;}\u0026#39; --comment \u0026#34;my first example resource specification\u0026#34; This specifies a request / limit pair that requests and is constrained to 300 mili-CPUs (0.3 CPUs) and 250 MiB of memory (250 * 1024 * 1024 bytes) for any UAI created with this limit specification. By keeping the request and the limit the same, this ensures that a host node will not be oversubscribed by UAIs. It is also legitimate to request less than the limit, though that risks over-subscription and is not recommended in most cases. If the request is greater than the limit, UAIs created with the request specification will never be scheduled because they will not be able to provide the requested resources.\nAll of the configurable parts are optional when adding a resource specification. If none are provided, an empty resource specification with only a resource_id will be created.\nExamining a Resource Specification To examine a particular resource specification use a command of the form\nncn-m001-pit# cray uas admin config resources describe \u0026lt;resource-id\u0026gt; for example:\nncn-m001-pit# cray uas admin config resources describe 85645ff3-1ce0-4f49-9c23-05b8a2d31849 comment = \u0026#34;my first example resource specification\u0026#34; limit = \u0026#34;{\\\u0026#34;cpu\\\u0026#34;: \\\u0026#34;300m\\\u0026#34;, \\\u0026#34;memory\\\u0026#34;: \\\u0026#34;250Mi\\\u0026#34;}\u0026#34; request = \u0026#34;{\\\u0026#34;cpu\\\u0026#34;: \\\u0026#34;300m\\\u0026#34;, \\\u0026#34;memory\\\u0026#34;: \\\u0026#34;250Mi\\\u0026#34;}\u0026#34; resource_id = \u0026#34;85645ff3-1ce0-4f49-9c23-05b8a2d31849\u0026#34; Updating a Resource Specification To modify an existing resource specification use a command of the form\ncray uas admin config resources update [options] \u0026lt;resource-id\u0026gt; for example:\nncn-m001-pit# cray uas admin config resources update --limit \u0026#39;{\u0026#34;cpu\u0026#34;: \u0026#34;100m\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;10Mi\u0026#34;}\u0026#39; 85645ff3-1ce0-4f49-9c23-05b8a2d31849 Deleting a Resource Specification To delete a resource specification use a command of the form\nncn-m001-pit# cray uas admin config resources delete \u0026lt;resource-id\u0026gt; for example:\nncn-m001-pit# cray uas admin config resources delete 7c78f5cf-ccf3-4d69-ae0b-a75648e5cddb UAI Classes A UAI class is a template from which UAIs can be created. At a minimum, a UAI class must contain the image_id on which UAIs of that class are constructed. The complete list of configuration that can be put in a UAI class is:\nThe UAI image to be used when creating UAIs of the class \u0026ndash; required configuration, no default A comment describing the purpose of the class \u0026ndash; no default A default flag indicating whether the class is the default UAI class \u0026ndash; default is false The name of the Kubernetes namespace in which UAIs of the class are created \u0026ndash; default is user A list of optional ports that will be opened in UAIs of the class \u0026ndash; default is an empty list A Kubernetes priority class name to be applied to UAIs of the class \u0026ndash; default is uai-priority A flag indicating whether UAIs of the class are reachable on a public IP address \u0026ndash; default is false A flag indicating whether UAIs of the class can reach the compute network \u0026ndash; default is true A resource identifier used for UAIs of the class \u0026ndash; not present by default, in which case the namespace resource limits and requests are used A list of volume identifiers used as volumes in UAIs of the class \u0026ndash; default is an empty list Listing UAI Classes To list available UAI classes, use the following command:\nncn-m001-pit# cray uas admin config classes list for example (using JSON format because it is a bit easier to read):\nncn-m001-pit# cray uas admin config classes list --format json [ { \u0026#34;class_id\u0026#34;: \u0026#34;05496a5f-7e35-435d-a802-882c6425e5b2\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;UAI Broker Class\u0026#34;, \u0026#34;default\u0026#34;: false, \u0026#34;namespace\u0026#34;: \u0026#34;uas\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: true, \u0026#34;resource_config\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: false, \u0026#34;uai_creation_class\u0026#34;: \u0026#34;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026#34;, \u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: false, \u0026#34;image_id\u0026#34;: \u0026#34;c5dcb261-5271-49b3-9347-afe7f3e31941\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-broker:latest\u0026#34; }, \u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; } ] }, { \u0026#34;class_id\u0026#34;: \u0026#34;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;UAI User Class\u0026#34;, \u0026#34;default\u0026#34;: false, \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: false, \u0026#34;resource_config\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: true, \u0026#34;uai_creation_class\u0026#34;: null, \u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: true, \u0026#34;image_id\u0026#34;: \u0026#34;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; }, \u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/root/slurm_config/munge\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;secret\u0026#34;: { \u0026#34;secret_name\u0026#34;: \u0026#34;munge-secret\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;munge-key\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/slurm\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;config_map\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;slurm-map\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;slurm-config\u0026#34; } ] }, { \u0026#34;class_id\u0026#34;: \u0026#34;bb28a35a-6cbc-4c30-84b0-6050314af76b\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;Non-Brokered UAI User Class\u0026#34;, \u0026#34;default\u0026#34;: false, \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: true, \u0026#34;resource_config\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: true, \u0026#34;uai_creation_class\u0026#34;: null, \u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: true, \u0026#34;image_id\u0026#34;: \u0026#34;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; }, \u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/root/slurm_config/munge\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;secret\u0026#34;: { \u0026#34;secret_name\u0026#34;: \u0026#34;munge-secret\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;munge-key\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/slurm\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;config_map\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;slurm-map\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;slurm-config\u0026#34; } ] } ] In the above output, there are three UAI classes:\nA UAI broker class A brokered end-user UAI class A non-brokered end-user UAI class Taking apart the non-brokered end-user UAI class, the first part is:\n\u0026#34;class_id\u0026#34;: \u0026#34;bb28a35a-6cbc-4c30-84b0-6050314af76b\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;Non-Brokered UAI User Class\u0026#34;, \u0026#34;default\u0026#34;: false, \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: true, \u0026#34;resource_config\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: true, \u0026#34;uai_creation_class\u0026#34;: null, The class_id field is the identifier used to refer to this class when examining, updating and deleting this class as well as when using this class with the\nncn-m001-pit# cray uas admin uais create command. The comment field is a free form string describing the UAI class. The default field is a flag indicating whether this class is the default class. The default class will be applied, overriding both the default UAI image and any specified image name, when the\nncn-m001-pit# cray uas create command is used to create an end-user UAI for a user. Setting a class to default gives the administrator fine grained control over the behavior of end-user UAIs that are created by authorized users in legacy mode. The namespace field specifies the Kubernetes namespace in which this UAI will run. It has the default setting of user here. The opt_ports field is an empty list of TCP port numbers that will be opened on the external IP address of the UAI when it runs. This controls whether services other than SSH can be run and reached publicly on the UAI. The priority_class_name \u0026quot;uai_priority\u0026quot; is the default Kubernetes priority class of UAIs. If it were a different class, it would affect both Kubernetes default resource limit / request assignments and Kubernetes scheduling priority for the UAI. The public_ip field is a flag that indicates whether the UAI should be given an external IP address LoadBalancer service so that clients outside the Kubernetes cluster can reach it, or only be given a Kubernetes Cluster-IP address. For the most part, this controls whether the UAI is reachable by SSH from external clients, but it also controls whether the ports in opt_ports are reachable as well. The resource_config field is not set, but could be set to a resource specification to override namespace defaults on Kubernetes resource requests / limits. The uai_compute_network flag indicates whether this UAI uses the macvlan mechanism to gain access to the Shasta compute node network. This needs to be true to support workload management. The uai_creation_class field is used by broker UAIs to tell the broker what kind of UAI to create when automatically generating a UAI.\nAfter all these individual items, we see the UAI Image to be used to create UAIs of this class:\n\u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: true, \u0026#34;image_id\u0026#34;: \u0026#34;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; }, Finally we see a list of volumes that will show up in UAIs created using this class:\n\u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/root/slurm_config/munge\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;secret\u0026#34;: { \u0026#34;secret_name\u0026#34;: \u0026#34;munge-secret\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;munge-key\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/slurm\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;config_map\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;slurm-map\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;slurm-config\u0026#34; } Here we are inheriting the timezone from the host node by importing /etc/localtime to the UAI. We are picking up access to the Lustre file system mounted on the host node as /lus and mounting that within the UAI at the same path. We then pick up two pieces of Slurm configuration, the munge key and the slurm configuration file, from Kubernetes and mount them as files at /root/slurm_config/munge and /etc/slurm respectively.\nAdding a UAI Class To add a new UAI class, use a command of the form:\nncn-m001-pit# cray uas admin config classes create --image-id \u0026lt;image-id\u0026gt; [options] where --image-id \u0026lt;image-id\u0026gt; specifies the UAI image identifier of the UAI image to be used in creating UAIs of the new class. Any number of classes using the same image id can be defined. Options available are:\n--image-id \u0026lt;image-id\u0026gt; set the UAI image to be used creating UAIs of this class (included here for completeness, this option is required for creation, not for updates) --volume-list '\u0026lt;volume-id\u0026gt;[,\u0026lt;volume-id[,...]]' set up the list of volumes mounted by UAIs of this class --resource-id \u0026lt;resource-id\u0026gt; set a resource specification to be used for UAIs of this class --uai-compute-network yes|no set the uai_compute_network flag described above in the UAI class --opt-ports '\u0026lt;port-number\u0026gt;[,\u0026lt;port-number[,...]]' sets up TCP ports in addition to SSH on which UAIs of this class will listen on their external IP address (i.e. the address SSH is listening on). -uai-creation-class \u0026lt;class-id\u0026gt; for broker UAIs only, the class of end-user UAIs the broker will create when handling a login --namespace '\u0026lt;namespace-name\u0026gt;' sets the Kubernetes namespace where UAIs of this class will run --priority-class-name '\u0026lt;priority-class-name\u0026gt;' set the Kubernetes priority class of UAIs created with this class --public-ip yes|no specify whether UAIs created with this class will listen on a public (LoadBalancer) IP address (yes) or a Kubernetes private (ClusterIP) IP address (no) --default yes|no specify whether this UAI class should be used as a default UAI class or not (see description in the previous section) --comment 'text' set a free-form text comment on the UAI class Only the --image-id option is required to create a UAI class. In that case, a UAI class with the specified UAI Image and no volumes will be created.\nExamining a UAI Class To examine an existing UAI class, use a command of the following form\nncn-m001-pit# cray uas admin config classes describe \u0026lt;class-id\u0026gt; For example:\nncn-m001-pit# cray uas admin config classes describe --format yaml bb28a35a-6cbc-4c30-84b0-6050314af76b class_id: bb28a35a-6cbc-4c30-84b0-6050314af76b comment: Non-Brokered UAI User Class default: false namespace: user opt_ports: [] priority_class_name: uai-priority public_ip: true resource_config: uai_compute_network: true uai_creation_class: uai_image: default: true image_id: ff86596e-9699-46e8-9d49-9cb20203df8c imagename: dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest volume_mounts: - mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: 55a02475-5770-4a77-b621-f92c5082475c volumename: timezone - mount_path: /root/slurm_config/munge volume_description: secret: secret_name: munge-secret volume_id: 7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad volumename: munge-key - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: 9fff2d24-77d9-467f-869a-235ddcd37ad7 volumename: lustre - mount_path: /etc/slurm volume_description: config_map: name: slurm-map volume_id: ea97325c-2b1d-418a-b3b5-3f6488f4a9e2 volumename: slurm-config Updating a UAI Class To update an existing UAI class use a command of the form\nncn-m001-pit# cray uas admin config classes update [options] \u0026lt;class-id\u0026gt; where [options] are the same options described in the previous section. Any change made using this command affects UAIs created using the class subsequent to the change. Existing UAIs using the class will not change. To update UAIs after updating a class delete and re-create the UAIs.\nFor example:\nncn-m001-pit# cray uas admin config classes update --comment \u0026#34;a new comment for my UAI class\u0026#34; bb28a35a-6cbc-4c30-84b0-6050314af76b would change the comment on the non-brokered UAI class examined above.\nDeleting a UAI Class To delete a UAI class use a command of the form\nncn-m001-pit# cray uas admin config classes delete \u0026lt;class-id\u0026gt; for example:\nncn-m001-pit# cray uas admin config classes bb28a35a-6cbc-4c30-84b0-6050314af76b deletes the non-brokered UAI class examined above.\nUAI Management UAS supports two manual methods and one automated method of UAI management. These are:\nDirect administrative UAI management Legacy mode user driven UAI management UAI broker mode UAI management Direct administrative UAI management is available mostly to allow administrators to set up UAI brokers for the UAI broker mode of UAI management and to control UAIs that are created under one of the other two methods. It is unlikely that a site will choose to create end-user UAIs this way, but it is possible to do. The administrative UAI management API provides an administrative way to list, create, examine and delete UAIs.\nThe legacy mode of UAI management gives users the authority to create, list and delete UAIs that belong to them. While this is a conceptually simple mode, it can lead to an unnecessary proliferation of UAIs belonging to a single user if the user is not careful to create UAIs only when needed. The legacy mode also cannot take advantage of UAI classes to create more than one kind of UAI for different users\u0026rsquo; needs.\nThe UAI broker mode creates / re-uses UAIs on demand when a user logs into a broker UAI using SSH. A site may run multiple broker UAIs, each configured to create UAIs of a different UAI class and each running with its own externally visible IP address. By choosing the correct IP address and logging into the broker, a user ultimately arrives in a UAI tailored for a given use case. Since the broker is responsible for managing the underlying end-user UAIs, users need not be given authority to create UAIs directly and, therefore, cannot cause a proliferation of unneeded UAIs. Since the broker UAIs each run separately on different IP addresses with, potentially, different user authorizations configured, a site can control which users are given access to which classes of end-user UAIs.\nAdministrative Management of UAIs Direct administration of UAIs includes listing, manual creation (rare), examination and deletion. Some of these activities have legacy mode user oriented analogs. This section focuses on administrative actions not user oriented actions.\nListing UAIs To list UAIs use a command of the form\nncn-m001-pit# cray uas admin uais list [options] where [options] include the following selection options:\n--owner '\u0026lt;user-name\u0026gt;' show only UAIs owned by the named user --class-id '\u0026lt;class-id' show only UAIs of the specified UAI class For example:\nncn-m001-pit# cray uas admin uais list --owner vers [[results]] uai_age = \u0026#34;6h22m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.28.212.166\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.28.212.166\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-715fa89d\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; Creating UAIs While it is rare that an an administrator would hand-craft a UAI in this way, it is possible. This is the mechanism used to create broker UAIs for the broker mode of UAI management. To create a UAI manually use a command of the form\ncray uas admin uais create [options] where options are:\n--class-id \u0026lt;class-id\u0026gt; the class of the UAI to be created. This option must be specified unless a default UAI class exists, in which case, it can be omitted and the default will be used --owner '\u0026lt;user-name\u0026gt;' create the UAI as owned by the specified user --passwd str '\u0026lt;passwd-string\u0026gt;' specify the /etc/password format string for the user who owns the UAI. This will be used to set up credentials within the UAI for the owner when the owner logs into the UAI --publickey-str '\u0026lt;public-ssh-key\u0026gt;' specify the SSH public key that will be used to authenticate with the UAI. The key should be, for example, the contents of an id_rsa.pub file used by SSH. Examining UAIs To examine an existing UAI use a command of the form\ncray uas admin uais describe \u0026lt;uai-name\u0026gt; for example:\nncn-m001-pit# cray uas admin uais describe uai-vers-715fa89d uai_age = \u0026#34;2d23h\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.28.212.166\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.28.212.166\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-715fa89d\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; [uai_portmap] Deleting UAIs To delete one or more UAIs use a command of the form\ncray uas admin uais delete [options] where options may be\n--uai-list '\u0026lt;list-of-uai-names\u0026gt;' - delete all the listed UAIs --owner \u0026lt;owner-name\u0026gt; - delete all UAIs owned by the named owner --class-id \u0026lt;uai-class-id\u0026gt; - delete all UAIs of the specified UAI class for example:\nncn-m001-pit# cray uas admin uais delete --uai-list \u0026#39;uai-vers-715fa89d,uai-ctuser-0aed4970\u0026#39; results = [ \u0026#34;Successfully deleted uai-vers-715fa89d\u0026#34;, \u0026#34;Successfully deleted uai-ctuser-0aed4970\u0026#34;,] Legacy Mode UAI Management In the legacy mode, users create and manage their own UAIs through the Shasta CLI. A user may create, list and delete only UAIs owned by the user. The user may not create a UAI for another user, nor may the user see or delete UAIs owned by another user. Once created, the information describing the UAI gives the user the information needed to reach the UAI using SSH and log into it.\nThe following diagram illustrates a system running with UAIs created in the legacy mode by four users, each of whom has created at least one end-user UAI. Notice that the user Pat has created two end-user UAIs:\nIn the simplest UAS configuration, there is some number of UAI images available for use in legacy mode and there is a set of volumes defined. In this configuration, when a UAI is created, the user may specify the UAI image to use as an option when creating the UAI, or may allow a default UAI image, if one is assigned, to be used. Every volume defined at the time the UAI is created will be mounted unconditionally in every newly created UAI if this approach is used. This can lead to problems with conflicting volume mount points and unresolvable volumes in some configurations of UAS. Unless UAI classes are used to make UAIs, care must be taken to ensure all volumes have unique mount-path settings and are accessible in the user Kubernetes namespace.\nA slightly more sophisticated configuration approach defines a default UAI Class that is always used by legacy mode UAI creation. When this approach is taken, the user can no longer specify the image to use, as it will be supplied by the UAI class, and the volumes mounted in any UAI created in legacy mode will be based on the specified UAI class. As long as volumes do not conflict within the list of volumes in a given UAI class, there is no need to avoid duplicate mount-path settings in the global list of volumes when this approach is used.\nConfiguring A Default UAI Class for Legacy Mode Using a default UAI class is optional but recommended for any site using the legacy UAI management mode that wants to have some control over UAIs created by users. UAI classes used for this purpose need to have certain minimum configuration in them:\nthe image_id field set to identify the image used to construct UAIs the volume_list field set to the list of volumes to mount in UAIs the public_ip field set to true the uai_compute_network flag set to true (if workload management will be used) the default flag set to true to make this the default UAI class To make UAIs useful, there is a minimum set of volumes that should be defined in the UAS configuration:\n/etc/localtime for default timezone information whatever directory on the host nodes holds persistent end-user storage, typically /lus. In addition to this, there may be volumes defined to support a workload manager (Slurm or PBS Professional) or the Cray PE or other packages the full extent of these volumes is outside the scope of this document, but whatever list of these other volumes is needed to get a suitable end-user UAI should be included in the default UAI class configuration.\nExample Minimal Default UAI Class For an example minimal system, here is an example set of volumes and an example creation of a UAI class that would use those volumes:\nncn-m001-pit# cray uas admin config volumes list --format json [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; } ] ncn-m001-pit# cray uas admin config images list [[results]] default = false image_id = \u0026#34;c5dcb261-5271-49b3-9347-afe7f3e31941\u0026#34; imagename = \u0026#34;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026#34; [[results]] default = false image_id = \u0026#34;c5f6377a-dfc0-41da-89c9-6c88c8a2cda8\u0026#34; imagename = \u0026#34;dtr.dev.cray.com/cray/cray-uas-sles15:latest\u0026#34; [[results]] default = true image_id = \u0026#34;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026#34; imagename = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; ncn-m001-pit# cray uas admin config classes create --image-id ff86596e-9699-46e8-9d49-9cb20203df8c --volume-list \u0026#39;55a02475-5770-4a77-b621-f92c5082475c,9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#39; --uai-compute-network yes --public-ip yes --comment \u0026#34;my default legacy mode uai class\u0026#34; --default yes class_id = \u0026#34;e2ea4845-5951-4c79-93d7-186ced8ce8ad\u0026#34; comment = \u0026#34;my default legacy mode uai class\u0026#34; default = true namespace = \u0026#34;user\u0026#34; opt_ports = [] priority_class_name = \u0026#34;uai-priority\u0026#34; public_ip = true uai_compute_network = true [[volume_mounts]] mount_path = \u0026#34;/etc/localtime\u0026#34; volume_id = \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34; volumename = \u0026#34;timezone\u0026#34; [volume_mounts.volume_description.host_path] path = \u0026#34;/etc/localtime\u0026#34; type = \u0026#34;FileOrCreate\u0026#34; [[volume_mounts]] mount_path = \u0026#34;/lus\u0026#34; volume_id = \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34; volumename = \u0026#34;lustre\u0026#34; [volume_mounts.volume_description.host_path] path = \u0026#34;/lus\u0026#34; type = \u0026#34;DirectoryOrCreate\u0026#34; [uai_image] default = true image_id = \u0026#34;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026#34; imagename = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; Example Default UAI Class with Slurm Support Here is an example of a default UAI class configured for Slurm support if Slurm has been installed on the host system:\nncn-m001-pit# cray uas admin config volumes list --format json [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/root/slurm_config/munge\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;secret\u0026#34;: { \u0026#34;secret_name\u0026#34;: \u0026#34;munge-secret\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;munge-key\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/slurm\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;config_map\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;slurm-map\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;slurm-config\u0026#34; } ] ncn-m001-pit# cray uas admin config images list [[results]] default = false image_id = \u0026#34;c5dcb261-5271-49b3-9347-afe7f3e31941\u0026#34; imagename = \u0026#34;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026#34; [[results]] default = false image_id = \u0026#34;c5f6377a-dfc0-41da-89c9-6c88c8a2cda8\u0026#34; imagename = \u0026#34;dtr.dev.cray.com/cray/cray-uas-sles15:latest\u0026#34; [[results]] default = true image_id = \u0026#34;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026#34; imagename = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; ncn-m001-pit# cray uas admin config classes create --image-id ff86596e-9699-46e8-9d49-9cb20203df8c --volume-list \u0026#39;55a02475-5770-4a77-b621-f92c5082475c,9fff2d24-77d9-467f-869a-235ddcd37ad7,7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad,ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026#39; --uai-compute-network yes --public-ip yes --comment \u0026#34;my default legacy mode uai class\u0026#34; --default yes class_id = \u0026#34;c0a6dfbc-f74c-4f2c-8c8e-e278ff0e14c6\u0026#34; comment = \u0026#34;my default legacy mode uai class\u0026#34; default = true namespace = \u0026#34;user\u0026#34; opt_ports = [] priority_class_name = \u0026#34;uai-priority\u0026#34; public_ip = true uai_compute_network = true [[volume_mounts]] mount_path = \u0026#34;/etc/localtime\u0026#34; volume_id = \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34; volumename = \u0026#34;timezone\u0026#34; [volume_mounts.volume_description.host_path] path = \u0026#34;/etc/localtime\u0026#34; type = \u0026#34;FileOrCreate\u0026#34; [[volume_mounts]] mount_path = \u0026#34;/lus\u0026#34; volume_id = \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34; volumename = \u0026#34;lustre\u0026#34; [volume_mounts.volume_description.host_path] path = \u0026#34;/lus\u0026#34; type = \u0026#34;DirectoryOrCreate\u0026#34; [[volume_mounts]] mount_path = \u0026#34;/root/slurm_config/munge\u0026#34; volume_id = \u0026#34;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026#34; volumename = \u0026#34;munge-key\u0026#34; [volume_mounts.volume_description.secret] secret_name = \u0026#34;munge-secret\u0026#34; [[volume_mounts]] mount_path = \u0026#34;/etc/slurm\u0026#34; volume_id = \u0026#34;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026#34; volumename = \u0026#34;slurm-config\u0026#34; [volume_mounts.volume_description.config_map] name = \u0026#34;slurm-map\u0026#34; [uai_image] default = true image_id = \u0026#34;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026#34; imagename = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; Creating and Using Default UAIs in Legacy Mode A user creates a UAI using the default UAI image or the default UAI class in legacy mode with a command of the following form\nuser\u0026gt; cray uas create --public-key \u0026#39;\u0026lt;path\u0026gt;\u0026#39; where \u0026lt;path\u0026gt; is the path to a file containing an SSH public-key matched to the SSH private key belonging to the user. The user can then use a command of the form\nuser\u0026gt; cray uas list to watch the UAI and see when it is ready for logins. The user logs into the UAI over SSH to do work in the UAI. When the user is finished with the UAI it can be deleted using a command of the form\nuser\u0026gt; cray uas delete --uai-list \u0026#39;\u0026lt;uai-list\u0026gt;\u0026#39; Here is a sample lifecycle of a UAI showing these steps:\nvers\u0026gt; cray auth login Username: vers Password: Success! vers\u0026gt; cray uas list results = [] vers\u0026gt; cray uas create --publickey ~/.ssh/id_rsa.pub uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.103.13.157\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.103.13.157\u0026#34; uai_msg = \u0026#34;ContainerCreating\u0026#34; uai_name = \u0026#34;uai-vers-8ee103bf\u0026#34; uai_status = \u0026#34;Waiting\u0026#34; username = \u0026#34;vers\u0026#34; [uai_portmap] vers\u0026gt; cray uas list [[results]] uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.103.13.157\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.103.13.157\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-8ee103bf\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; vers\u0026gt; ssh vers@10.103.13.157 The authenticity of host \u0026#39;10.103.13.157 (10.103.13.157)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:XQukF3V1q0Hh/aTiFmijhLMcaOzwAL+HjbM66YR4mAg. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;10.103.13.157\u0026#39; (ECDSA) to the list of known hosts. vers@uai-vers-8ee103bf-95b5d774-88ssd:/tmp\u0026gt; sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST workq* up infinite 4 comp nid[000001-000004] vers@uai-vers-8ee103bf-95b5d774-88ssd\u0026gt; srun -n 3 -N 3 hostname nid000001 nid000002 nid000003 vers@uai-vers-8ee103bf-95b5d774-88ssd\u0026gt; exit logout Connection to 10.103.13.157 closed. vers\u0026gt; cray uas delete --uai-list uai-vers-8ee103bf results = [ \u0026#34;Successfully deleted uai-vers-8ee103bf\u0026#34;,] In this example the user logs into the CLI using cray auth login and a user name and password matching that user\u0026rsquo;s credentials in Keycloak on Shasta. From there the user creates a UAI. The UAI starts out in a Pending or Waiting state as Kubernetes constructs its pod and starts its container running. Using cray uas list the user watches the UAI until it reaches a Running: Ready state. The UAI is now ready to accept SSH logins from the user, and the user then logs into the UAI to run a simple Slurm job, and logs out. Now finished with the UAI, the user deletes it with cray uas delete. If the user has more than one UAI to delete, the argument to the --uai-list option can be a comma separated list of UAI names.\nListing Available UAI Images in Legacy Mode A user can list the UAI images available for creating a UAI with a command of the form\nuser\u0026gt; cray uas images list For example:\nvers\u0026gt; cray uas images list default_image = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; image_list = [ \u0026#34;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026#34;, \u0026#34;dtr.dev.cray.com/cray/cray-uas-sles15:latest\u0026#34;, \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34;,] Creating UAIs From Specific UAI Images in Legacy Mode A user can create a UAI from a specific UAI image (assuming no default UAI class exists) using a command of the form\nuser\u0026gt; cray uas create --publickey \u0026lt;path\u0026gt; --imagename \u0026lt;image-name\u0026gt; where \u0026lt;image-name\u0026gt; is the name shown above in the list of UAI images. For example:\nvers\u0026gt; cray uas images list default_image = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; image_list = [ \u0026#34;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026#34;, \u0026#34;dtr.dev.cray.com/cray/cray-uas-sles15:latest\u0026#34;, \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34;,] vers\u0026gt; cray uas create --publickey ~/.ssh/id_rsa.pub --imagename dtr.dev.cray.com/cray/cray-uas-sles15:latest uai_connect_string = \u0026#34;ssh vers@10.103.13.160\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uas-sles15:latest\u0026#34; uai_ip = \u0026#34;10.103.13.160\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-b386d655\u0026#34; uai_status = \u0026#34;Pending\u0026#34; username = \u0026#34;vers\u0026#34; [uai_portmap] The UAI Broker Based Mode A UAI broker is a special kind of UAI whose job is not to host users directly but to field attempts to reach a UAI, locate or create a UAI for the user making the attempt, and then pass the connection on to the correct UAI. Multiple UAI brokers can be created, each serving a UAI of a different class, making it possible to set up UAIs for varying workflows and environments as needed. The following illustrates a system using the UAI broker mode of UAI management:\nNotice that, unlike in the legacy model, in this model users log into their UAIs through the UAI broker. After that, each user is assigned an end-user UAI by the broker and the SSH session is forwarded to the end-user UAI. This is seamless from the user\u0026rsquo;s perspective, as the SSH session is carried through the UAI broker and into the end-user UAI.\nTo make all of this work, the administrator must define at least one UAI class containing the configuration for the end-user UAIs to be created by the UAI broker and one UAI class containing the UAI broker configuration itself. The UAI broker should be configured by the site to permit authentication of users. This can be carried out using volumes to place configuration files as needed in the file system namespace of the broker UAI. Finally, once all of this is prepared, the administrator launches the broker UAI, and makes the IP address of the broker UAI available for users to log into.\nConfiguring End-User UAI Classes for Broker Mode Each UAI broker will create and manage a single class of end-user UAIs. Setting up UAI classes for this is similar to configuring a default UAI class for legacy mode with the following exceptions:\nthe public_ip flag for brokered UAI classes should be set to false the default flag for brokered UAI classes may be set to true or false but should, most likely, be set to false. Everything else should be the same as it would be for a legacy mode UAI class.\nConfiguring a Broker UAI class Configuring a broker UAI class consists of the following:\ncreate volumes to hold any site-specific authentication, SSH, or other configuration required choose the end-user UAI class for which the broker UAI will serve instances add the UAI class with (at a minimum) namespace set to uas default set to false volume_mounts set to the list of customization volume-ids created above public_ip set to true uai_compute_network set to false uai_creation_class set to the class-id of the end-user UAI class An Example of Volumes to Connect broker UAIs to LDAP Broker UAIs authenticate users in SSH and pass the SSH connection on to the selected / created end-user UAI. To do this authentication, they need an authentication source. For sites that use LDAP as a directory server for authentication, connecting broker UAIs to LDAP is simply a matter of replicating the LDAP configuration used by other nodes / systems at the site (UANs can be a good source of this configuration) inside the broker UAI. This section shows how to do that using volumes, which permits the standard broker UAI image to be used out of the box and reconfigured externally.\nWhile it would be possible to make the configuration available as files volume mounted from the host node of the broker UAI, this is difficult to set up and maintain because it means that the configuration files must be present and synchronized across all UAI host nodes. A more practical approach to this is to install the configuration files in Kubernetes as secrets and then mount them from Kubernetes directly. This ensures that no matter where a broker UAI runs, it has access to the configuration.\nThis example, uses Kubernetes secrets and assumes that the broker UAIs run in the uas Kubernetes namespace. If a different namespace is used, the creation of the configmaps is different but the contents are the same. Using a namespace other than uas for broker UAIs is not recommended and is beyond the scope of this document.\nTo configure LDAP, first determine which files need to be changed in the broker UAI and what their contents should be. In this example, the file is /etc/sssd/sssd.conf and its contents are (the contents have been sanitized, substitute appropriate contents in their place):\n[sssd] config_file_version = 2 services = nss, pam domains = My_DC [nss] filter_users = root filter_groups = root [pam] [domain/My_DC] ldap_search_base=dc=datacenter,dc=mydomain,dc=com ldap_uri=ldap://10.1.1.5,ldap://10.1.2.5 id_provider = ldap ldap_tls_reqcert = allow ldap_schema = rfc2307 cache_credentials = True entry_cache_timeout = 60 enumerate = False To add the above to a secret, first create a file with the contents:\nncn-m001-pit# cat \u0026lt;\u0026lt;EOF \u0026gt; sssd.conf [sssd] config_file_version = 2 services = nss, pam domains = My_DC [nss] filter_users = root filter_groups = root [pam] [domain/My_DC] ldap_search_base=dc=datacenter,dc=mydomain,dc=com ldap_uri=ldap://10.1.1.5,ldap://10.1.2.5 id_provider = ldap ldap_tls_reqcert = allow ldap_schema = rfc2307 cache_credentials = True entry_cache_timeout = 60 enumerate = False EOF Next make a secret from the file:\nncn-m001-pit# kubectl create secret generic -n uas broker-sssd-conf --from-file=sssd.conf Next make a volume for the secret in the UAS configuration:\nncn-m001-pit# cray uas admin config volumes create --mount-path /etc/sssd --volume-description \u0026#39;{\u0026#34;secret\u0026#34;: {\u0026#34;secret_name\u0026#34;: \u0026#34;broker-sssd-conf\u0026#34;, \u0026#34;default_mode\u0026#34;: 384}}\u0026#39; --volumename broker-sssd-config mount_path = \u0026#34;/etc/sssd\u0026#34; volume_id = \u0026#34;4dc6691e-e7d9-4af3-acde-fc6d308dd7b4\u0026#34; volumename = \u0026#34;broker-sssd-config\u0026#34; [volume_description.secret] default_mode = 384 secret_name = \u0026#34;broker-sssd-conf\u0026#34; Two important things to notice here are:\nThe secret is mounted on the directory /etc/sssd not the file /etc/sssd/sssd.conf because Kubernetes does not permit the replacement of an existing regular file with a volume but does allow overriding a directory The value 384 is used here for the default mode of the file instead of 0600, which would be easier to read, because JSON does not accept octal numbers in the leading zero form The last part that is needed is a UAI class for the broker UAI with the updated configuration in the volume list. For this we need the image-id of the broker UAI image, the volume-ids of the volumes to be added to the broker class and the class-id of the end-user UAI class managed by the broker:\nncn-m001-pit# cray uas admin config images list [[results]] default = false image_id = \u0026#34;c5dcb261-5271-49b3-9347-afe7f3e31941\u0026#34; imagename = \u0026#34;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026#34; [[results]] default = false image_id = \u0026#34;c5f6377a-dfc0-41da-89c9-6c88c8a2cda8\u0026#34; imagename = \u0026#34;dtr.dev.cray.com/cray/cray-uas-sles15:latest\u0026#34; [[results]] default = true image_id = \u0026#34;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026#34; imagename = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; ncn-m001-pit# cray uas admin config volumes list | grep -e volume_id -e volumename volume_id = \u0026#34;4dc6691e-e7d9-4af3-acde-fc6d308dd7b4\u0026#34; volumename = \u0026#34;broker-sssd-config\u0026#34; volume_id = \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34; volumename = \u0026#34;timezone\u0026#34; volume_id = \u0026#34;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026#34; volumename = \u0026#34;munge-key\u0026#34; volume_id = \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34; volumename = \u0026#34;lustre\u0026#34; volume_id = \u0026#34;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026#34; volumename = \u0026#34;slurm-config\u0026#34; ncn-m001-pit# cray uas admin config classes list | grep -e class_id -e comment class_id = \u0026#34;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026#34; comment = \u0026#34;UAI User Class\u0026#34; class_id = \u0026#34;bb28a35a-6cbc-4c30-84b0-6050314af76b\u0026#34; comment = \u0026#34;Non-Brokered UAI User Class\u0026#34; Using that information create the broker UAI class\nncn-m001-pit# cray uas admin config classes create --image-id c5dcb261-5271-49b3-9347-afe7f3e31941 --volume-list \u0026#39;4dc6691e-e7d9-4af3-acde-fc6d308dd7b4,55a02475-5770-4a77-b621-f92c5082475c,9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#39; --uai-compute-network no --public-ip yes --comment \u0026#34;UAI broker class\u0026#34; --uai-creation-class a623a04a-8ff0-425e-94cc-4409bdd49d9c --namespace uas class_id = \u0026#34;74970cdc-9f94-4d51-8f20-96326212b468\u0026#34; comment = \u0026#34;UAI broker class\u0026#34; default = false namespace = \u0026#34;uas\u0026#34; opt_ports = [] priority_class_name = \u0026#34;uai-priority\u0026#34; public_ip = true uai_compute_network = false uai_creation_class = \u0026#34;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026#34; [[volume_mounts]] mount_path = \u0026#34;/etc/sssd\u0026#34; volume_id = \u0026#34;4dc6691e-e7d9-4af3-acde-fc6d308dd7b4\u0026#34; volumename = \u0026#34;broker-sssd-config\u0026#34; [volume_mounts.volume_description.secret] default_mode = 384 secret_name = \u0026#34;broker-sssd-conf\u0026#34; [[volume_mounts]] mount_path = \u0026#34;/etc/localtime\u0026#34; volume_id = \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34; volumename = \u0026#34;timezone\u0026#34; [volume_mounts.volume_description.host_path] path = \u0026#34;/etc/localtime\u0026#34; type = \u0026#34;FileOrCreate\u0026#34; [[volume_mounts]] mount_path = \u0026#34;/lus\u0026#34; volume_id = \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34; volumename = \u0026#34;lustre\u0026#34; [volume_mounts.volume_description.host_path] path = \u0026#34;/lus\u0026#34; type = \u0026#34;DirectoryOrCreate\u0026#34; [uai_image] default = false image_id = \u0026#34;c5dcb261-5271-49b3-9347-afe7f3e31941\u0026#34; imagename = \u0026#34;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026#34; Starting a Broker UAI Once the broker UAI class has been set up, all that remains is to create the broker UAI using a command of the form\nncn-m001-pit# cray uas admin uais create --class-id \u0026lt;class-id\u0026gt; [--owner \u0026lt;name\u0026gt;] To make the broker obvious in the list of UAIs, giving it an owner name of broker is handy. The owner name on a broker is used for naming and listing, but nothing else, so this is a convenient convention. Here is an example using the class created above:\nncn-m001-pit# cray uas admin uais create --class-id 74970cdc-9f94-4d51-8f20-96326212b468 --owner broker uai_connect_string = \u0026#34;ssh broker@10.103.13.162\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026#34; uai_ip = \u0026#34;10.103.13.162\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-broker-11f36815\u0026#34; uai_status = \u0026#34;Pending\u0026#34; username = \u0026#34;broker\u0026#34; [uai_portmap] Logging In Through a Broker UAI With the broker UAI running, users can use SSH to log into it and reach their end-user UAIs on demand. Here is the first login by the user vers:\nvers\u0026gt; ssh vers@10.103.13.162 The authenticity of host \u0026#39;10.103.13.162 (10.103.13.162)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:k4ef6vTtJ1Dtb6H17cAFh5ljZYTl4IXtezR3fPVUKZI. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;10.103.13.162\u0026#39; (ECDSA) to the list of known hosts. Password: Creating a new UAI... The authenticity of host \u0026#39;10.21.138.52 (10.21.138.52)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:TX5DMAMQ8yQuL4YHo9qFEJWpKaaiqfeSs4ndYXOTjkU. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added \u0026#39;10.21.138.52\u0026#39; (ECDSA) to the list of known hosts. There are several things to notice here. First, the first time the user logs in the broker UAI\u0026rsquo;s SSH host key is unknown, as is normal for SSH. Next, the user is asked for a password in this example. If the user\u0026rsquo;s home directory, as defined in LDAP had been mounted in the broker UAI and a .ssh/authorized_keys entry had been present, there would not have been a password prompt. Home directory trees can be mounted as volumes just as any other directory can. Next, the broker mechanism in the broker UAI creates a new UAI since vers has never logged into this broker UAI before. Finally, there is a second prompt to acknowledge an unknown host which is, in this case, the end-user UAI itself. The broker UAI constructs a public/private key pair for the hidden SSH connection between the broker and the end-user UAI shown in the overview figure above.\nThe next time vers logs in, it looks more like this:\nvers\u0026gt; ssh vers@10.103.13.162 Password: vers@uai-vers-ee6f427e-6c7468cdb8-2rqtv\u0026gt; Only the password prompt appears now, since the hosts are all known and the end-user UAI exists but there is no .ssh/authorized_keys known yet by the broker UAI for vers.\nUAI Images There are three kinds of UAI images used by the UAS:\nA pre-packaged broker UAI image provided with the UAS A pre-packaged basic end-user UAI Image provided with the UAS Custom end-user UAI images created on site, usually based on compute node contents This section describes how to manage and customize these images.\nThe Provided Broker UAI Image The broker UAI image that comes with UAS is the image used to construct broker UAIs. The key pieces of the broker UAI image are:\nan entrypoint shell script that initializes the container and starts the SSH daemon running an SSH configuration that forces logged in users into the switchboard command which creates / selects end-user UAIs and redirects connections Customizing the Broker UAI Image The primary way to customize the broker UAI image is by defining volumes and connecting them to the broker UAI class for a given broker. An example of this is configuring the broker for LDAP shown above. Some customizations may require action that cannot be covered simply by using a volume. Those cases can be covered either by volume mounting a customized entrypoint script, or volume mounting a customized SSH configuration. Both of these are shown below.\nCustomizing the Broker UAI Entrypoint Script The broker UAI entrypoint script runs once every time the broker UAI starts. It resides at /app/broker/entrypoint.sh in the broker UAI image. The entrypoint script is the only file in that directory, so it can be overridden by creating a Kubernetes config-map in the uas namespace containing the modified script and creating a volume using that config-map with a mount point of /app/broker. There is critical content in the entrypoint script that should not be modified. Here is a tour the unmodified script:\n#!/bin/bash # Copyright 2020 Hewlett Packard Enterprise Development LP echo \u0026#34;Configure PAM to use sssd...\u0026#34; pam-config -a --sss --mkhomedir echo \u0026#34;Generating broker host keys...\u0026#34; ssh-keygen -A echo \u0026#34;Checking for UAI_CREATION_CLASS...\u0026#34; if ! [ -z $UAI_CREATION_CLASS ]; then echo UAI_CREATION_CLASS=$UAI_CREATION_CLASS \u0026gt;\u0026gt; /etc/environment fi echo \u0026#34;Starting sshd...\u0026#34; /usr/sbin/sshd -f /etc/switchboard/sshd_config echo \u0026#34;Starting sssd...\u0026#34; sssd sleep infinity Starting at the top, pam_config ... can be customized to set up PAM as needed. The configuration here assumes the broker is using SSSD to reach a directory server for authentication and that, if a home directory is not present for a user at login, one should be made on the broker. The ssh-keygen... part is needed to set up the SSH host key for the broker and should be left alone. The UAI_CREATION_CLASS code should be left alone, as it sets up information used by switchboard to create end-user UAIs. The /usr/sbin/sshd... part starts the SSH server on the broker and should be left alone. Configuration of SSH is covered in the next section and is done by replacing /etc/switchboard/sshd_config not by modifying this line. The sssd part assumes the broker is using SSSD to reach a directory server, it can be changed as needed. The sleep infinity prevents the script from exiting which keeps the broker UAI running. It should not be removed or altered. As long as the basic flow and contents described here are honored, other changes to this script should work without compromising the broker UAI\u0026rsquo;s function.\nHere is an example of replacing the entrypoint script with a new entrypoint script that changes the SSSD invocation to explicitly specify the sssd.conf file path (the standard path is used here, but a different path might make customizing SSSD for a given site simpler under some set of circumstances):\n# Notice special here document form to prevent variable substitution in the file ncn-m001-pit# cat \u0026lt;\u0026lt;-\u0026#34;EOF\u0026#34; \u0026gt; entrypoint.sh #!/bin/bash # Copyright 2020 Hewlett Packard Enterprise Development LP echo \u0026#34;Configure PAM to use sssd...\u0026#34; pam-config -a --sss --mkhomedir echo \u0026#34;Generating broker host keys...\u0026#34; ssh-keygen -A echo \u0026#34;Checking for UAI_CREATION_CLASS...\u0026#34; if ! [ -z $UAI_CREATION_CLASS ]; then echo UAI_CREATION_CLASS=$UAI_CREATION_CLASS \u0026gt;\u0026gt; /etc/environment fi echo \u0026#34;Starting sshd...\u0026#34; /usr/sbin/sshd -f /etc/switchboard/sshd_config echo \u0026#34;Starting sssd...\u0026#34; # LOCAL MODIFICATION # change the normal SSSD invocation # sssd # to specify the config file path sssd --config /etc/sssd/sssd.conf # END OF LOCAL MODIFICATION sleep infinity EOF ncn-m001-pit# kubectl create configmap -n uas broker-entrypoint --from-file=entrypoint.sh # Notice that the `default_mode` setting, which will set the mode on the file # /app/broker/entrypoint.sh is decimal 493 here instead of octal 0755. # The octal notation is not permitted in a JSON specification. Decimal # numbers have to be used. ncn-m001-pit# cray uas admin config volumes create --mount-path /app/broker --volume-description \u0026#39;{\u0026#34;config_map\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;broker-entrypoint\u0026#34;, \u0026#34;default_mode\u0026#34;: 493}}\u0026#39; --volumename broker-entrypoint mount_path = \u0026#34;/app/broker\u0026#34; volume_id = \u0026#34;1f3bde56-b2e7-4596-ab3a-6aa4327d29c7\u0026#34; volumename = \u0026#34;broker-entrypoint\u0026#34; [volume_description.config_map] default_mode = 493 name = \u0026#34;broker-entrypoint\u0026#34; ncn-m001-pit# cray uas admin config classes list | grep -e class_id -e comment class_id = \u0026#34;74970cdc-9f94-4d51-8f20-96326212b468\u0026#34; comment = \u0026#34;UAI broker class\u0026#34; class_id = \u0026#34;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026#34; comment = \u0026#34;UAI User Class\u0026#34; class_id = \u0026#34;bb28a35a-6cbc-4c30-84b0-6050314af76b\u0026#34; comment = \u0026#34;Non-Brokered UAI User Class\u0026#34; ncn-m001-pit# cray uas admin config classes describe 74970cdc-9f94-4d51-8f20-96326212b468 --format yaml class_id: 74970cdc-9f94-4d51-8f20-96326212b468 comment: UAI broker class default: false namespace: uas opt_ports: [] priority_class_name: uai-priority public_ip: true resource_config: uai_compute_network: false uai_creation_class: a623a04a-8ff0-425e-94cc-4409bdd49d9c uai_image: default: false image_id: c5dcb261-5271-49b3-9347-afe7f3e31941 imagename: dtr.dev.cray.com/cray/cray-uai-broker:latest volume_mounts: - mount_path: /etc/sssd volume_description: secret: default_mode: 384 secret_name: broker-sssd-conf volume_id: 4dc6691e-e7d9-4af3-acde-fc6d308dd7b4 volumename: broker-sssd-config - mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: 55a02475-5770-4a77-b621-f92c5082475c volumename: timezone - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: 9fff2d24-77d9-467f-869a-235ddcd37ad7 volumename: lustre ncn-m001-pit# cray uas admin config classes update --volume-list \u0026#39;4dc6691e-e7d9-4af3-acde-fc6d308dd7b4,55a02475-5770-4a77-b621-f92c5082475c,9fff2d24-77d9-467f-869a-235ddcd37ad7,1f3bde56-b2e7-4596-ab3a-6aa4327d29c7\u0026#39; --format yaml 74970cdc-9f94-4d51-8f20-96326212b468 class_id: 74970cdc-9f94-4d51-8f20-96326212b468 comment: UAI broker class default: false namespace: uas opt_ports: [] priority_class_name: uai-priority public_ip: true resource_config: uai_compute_network: false uai_creation_class: a623a04a-8ff0-425e-94cc-4409bdd49d9c uai_image: default: false image_id: c5dcb261-5271-49b3-9347-afe7f3e31941 imagename: dtr.dev.cray.com/cray/cray-uai-broker:latest volume_mounts: - mount_path: /etc/sssd volume_description: secret: default_mode: 384 secret_name: broker-sssd-conf volume_id: 4dc6691e-e7d9-4af3-acde-fc6d308dd7b4 volumename: broker-sssd-config - mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: 55a02475-5770-4a77-b621-f92c5082475c volumename: timezone - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: 9fff2d24-77d9-467f-869a-235ddcd37ad7 volumename: lustre - mount_path: /app/broker volume_description: config_map: default_mode: 493 name: broker-entrypoint volume_id: 1f3bde56-b2e7-4596-ab3a-6aa4327d29c7 volumename: broker-entrypoint With the broker UAI class updated, all that remains is to clear out any existing end-user UAIs (existing UAIs will not work with the new broker because the new broker will have a new key-pair shared with its UAIs) and the existing broker UAI (if any) and create a new broker UAI.\nNOTE: clearing out existing UAIs will terminate any user activity on those UAIs, make sure that users are warned of the disruption. ncn-m001-pit# cray uas admin uais delete --class-id a623a04a-8ff0-425e-94cc-4409bdd49d9c results = [ \u0026#34;Successfully deleted uai-vers-ee6f427e\u0026#34;,] ncn-m001-pit# cray uas admin uais delete --class-id 74970cdc-9f94-4d51-8f20-96326212b468 results = [ \u0026#34;Successfully deleted uai-broker-11f36815\u0026#34;,] ncn-m001-pit# cray uas admin uais create --class-id 74970cdc-9f94-4d51-8f20-96326212b468 --owner broker uai_connect_string = \u0026#34;ssh broker@10.103.13.162\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026#34; uai_ip = \u0026#34;10.103.13.162\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-broker-a50407d5\u0026#34; uai_status = \u0026#34;Pending\u0026#34; username = \u0026#34;broker\u0026#34; [uai_portmap] Customizing the Broker UAI SSH Configuration The SSH configuration used on broker UAIs resides in /etc/switchboard/sshd_config and contains the following:\nPort 30123 AuthorizedKeysFile\t.ssh/authorized_keys UsePAM yes X11Forwarding yes Subsystem\tsftp\t/usr/lib/ssh/sftp-server AcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES AcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT AcceptEnv LC_IDENTIFICATION LC_ALL AcceptEnv UAI_ONE_SHOT UseDNS no Match User !root,* PermitTTY yes ForceCommand /usr/bin/switchboard broker --class-id $UAI_CREATION_CLASS Important content here is as follows:\nPort 30123 tells sshd to listen on a port that can be reached through port forwarding by the publicly visible Kubernetes service, The UseDNS no avoids any DNS issues resulting from the broker UAI running in the Kubernetes network space. The permitTTY yes setting permits interactive UAI logins The ForceCommand ... statement ensures that users are always sent on to end-user UAIs or drop out of the broker UAI on failure, preventing users from directly accessing the broker UAI. The AcceptEnv UAI_ONE_SHOT setting is not required, but it allows a user to set the UAI_ONE_SHOT variable which instructs the broker to delete any created end-user UAI after the user logs out. These should be left unchanged. The rest of the configuration can be customized as needed.\nThe following is an example that follows on from the previous section and configures SSH to provide a pre-login banner. Both a new banner file and a new sshd_config are placed in a Kubernetes configmap and mounted over /etc/switchboard:\n# Notice special here document form to prevent variable substitution in the file ncn-m001-pit# cat \u0026lt;\u0026lt;-\u0026#34;EOF\u0026#34; \u0026gt; banner Here is a banner that will be displayed before login on the broker UAI EOF # Notice special here document form to prevent variable substitution in the file ncn-m001-pit# cat \u0026lt;\u0026lt;-\u0026#34;EOF\u0026#34; \u0026gt; sshd_conf Port 30123 AuthorizedKeysFile\t.ssh/authorized_keys UsePAM yes X11Forwarding yes Subsystem\tsftp\t/usr/lib/ssh/sftp-server AcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES AcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT AcceptEnv LC_IDENTIFICATION LC_ALL AcceptEnv UAI_ONE_SHOT UseDNS no Banner /etc/switchboard/banner Match User !root,* PermitTTY yes ForceCommand /usr/bin/switchboard broker --class-id $UAI_CREATION_CLASS EOF ncn-m001-pit# kubectl create configmap -n uas broker-sshd-conf --from-file sshd_config --from-file banner ncn-m001-pit# cray uas admin config volumes create --mount-path /etc/switchboard --volume-description \u0026#39;{\u0026#34;config_map\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;broker-sshd-conf\u0026#34;, \u0026#34;default_mode\u0026#34;: 384}}\u0026#39; --volumename broker-sshd-config mount_path = \u0026#34;/etc/switchboard\u0026#34; volume_id = \u0026#34;d5058121-c1b6-4360-824d-3c712371f042\u0026#34; volumename = \u0026#34;broker-sshd-config\u0026#34; [volume_description.config_map] default_mode = 384 name = \u0026#34;broker-sshd-conf\u0026#34; ncn-m001-pit# cray uas admin config classes update --volume-list \u0026#39;4dc6691e-e7d9-4af3-acde-fc6d308dd7b4,55a02475-5770-4a77-b621-f92c5082475c,9fff2d24-77d9-467f-869a-235ddcd37ad7,1f3bde56-b2e7-4596-ab3a-6aa4327d29c7,d5058121-c1b6-4360-824d-3c712371f042\u0026#39; --format yaml 74970cdc-9f94-4d51-8f20-96326212b468 class_id: 74970cdc-9f94-4d51-8f20-96326212b468 comment: UAI broker class default: false namespace: uas opt_ports: [] priority_class_name: uai-priority public_ip: true resource_config: uai_compute_network: false uai_creation_class: a623a04a-8ff0-425e-94cc-4409bdd49d9c uai_image: default: false image_id: c5dcb261-5271-49b3-9347-afe7f3e31941 imagename: dtr.dev.cray.com/cray/cray-uai-broker:latest volume_mounts: - mount_path: /etc/sssd volume_description: secret: default_mode: 384 secret_name: broker-sssd-conf volume_id: 4dc6691e-e7d9-4af3-acde-fc6d308dd7b4 volumename: broker-sssd-config - mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: 55a02475-5770-4a77-b621-f92c5082475c volumename: timezone - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: 9fff2d24-77d9-467f-869a-235ddcd37ad7 volumename: lustre - mount_path: /app/broker volume_description: config_map: default_mode: 493 name: broker-entrypoint volume_id: 1f3bde56-b2e7-4596-ab3a-6aa4327d29c7 volumename: broker-entrypoint - mount_path: /etc/switchboard volume_description: config_map: default_mode: 384 name: broker-sshd-conf volume_id: d5058121-c1b6-4360-824d-3c712371f042 volumename: broker-sshd-config With the new configuration installed, clean out the old UAIs and restart the broker:\nNOTE: clearing out existing UAIs will terminate any user activity on those UAIs, make sure that users are warned of the disruption. ncn-m001-pit# cray uas admin uais delete --class-id a623a04a-8ff0-425e-94cc-4409bdd49d9c results = [ \u0026#34;Successfully deleted uai-vers-e937b810\u0026#34;,] ncn-m001-pit# cray uas admin uais delete --class-id 74970cdc-9f94-4d51-8f20-96326212b468 results = [ \u0026#34;Successfully deleted uai-broker-a50407d5\u0026#34;,] ncn-m001-pit# cray uas admin uais create --class-id 74970cdc-9f94-4d51-8f20-96326212b468 --owner broker uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh broker@10.103.13.162\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026#34; uai_ip = \u0026#34;10.103.13.162\u0026#34; uai_msg = \u0026#34;PodInitializing\u0026#34; uai_name = \u0026#34;uai-broker-df7e6939\u0026#34; uai_status = \u0026#34;Waiting\u0026#34; username = \u0026#34;broker\u0026#34; [uai_portmap] Now when the user connects to the broker to log in\nvers\u0026gt; ssh vers@10.103.13.162 Here is a banner that will be displayed before login to SSH on Broker UAIs Password: The Provided End-User UAI Image The provided end-user UAI image is a basic UAI image that includes an up-to-date version of the Sles Linux Distribution and client support for both the Slurm and PBS Professional workload managers. It provides an entrypoint to using UAIs and doing workload management from UAIs. This UAI image is not suitable for use with the Cray PE because it cannot be assured of being up-to-date with what is running on Shasta compute nodes at a given site. To support building software to be run in compute nodes, it is necessary to create a custom end-user UAI image and use that.\nCustom End-User UAI Images A custom end-user UAI image can be any container image set up with the end-user UAI entrypoint script, but, for the purpose here, it will be a UAI image built from the squashfs image used on compute nodes on the host system. This section describes how to create this kind of custom end-user UAI image.\nBuilding a Custom End-User UAI Image The following steps are used to build a custom End-User UAI image called registry.local/cray/cray-uai-compute:latest. Alter this name as needed by changing the following in the procedure:\nncn-w001# UAI_IMAGE_NAME=registry.local/cray/cray-uai-compute:latest to use a different name. All steps in this procedure must be run from a true NCN (master or worker node) not from the LiveCD node. In particular, pushing the final image to registry.local will fail with an error reporting a bad x509 certificate if it is attempted on the LiveCD node.\nQuery BOS for a sessiontemplate ID Identify the Sessiontemplate name to use. A full list may be found with:\nncn-w001# cray bos sessiontemplate list --format yaml - boot_sets: compute: boot_ordinal: 2 etag: d54782b3853a2d8713a597d80286b93e kernel_parameters: console=ttyS0,115200 bad_page=panic crashkernel=340M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell turbo_boost_limit=999 spire_join_token=${SPIRE_JOIN_TOKEN} network: nmn node_roles_groups: - Compute path: s3://boot-images/0c0d4081-2e8b-433f-b6f7-e1ef0b907be3/manifest.json rootfs_provider: cpss3 rootfs_provider_passthrough: dvs:api-gw-service-nmn.local:300:nmn0 type: s3 cfs: configuration: wlm-config-0.1.0 enable_cfs: true name: wlm-sessiontemplate-0.1.0 Alternatively, collect the sessiontemplate name used when performing the \u0026ldquo;Install and Configure the Cray Operating System (COS)\u0026rdquo; procedure in the Installation and Configuration Guide. Near the end of that procedure the step to \u0026ldquo;Create a BOS Session to boot the compute nodes\u0026rdquo; should contain the name.\nncn-w001# SESSION_NAME=wlm-sessiontemplate-0.1.0 Download a Compute Node squashfs Using the Sessiontemplate name, download a compute node squashfs from a BOS sessiontemplate name:\nncn-w001# SESSION_ID=$(cray bos sessiontemplate describe $SESSION_NAME --format json | jq -r \u0026#39;.boot_sets.compute.path\u0026#39; | awk -F/ \u0026#39;{print $4}\u0026#39;) ncn-w001# cray artifacts get boot-images $SESSION_ID/rootfs rootfs.squashfs Mount the squashfs and Create a tarball Create a directory to mount the squashfs:\nncn-w001# mkdir mount ncn-w001# mount -o loop,rdonly rootfs.squashfs `pwd`/mount Create the tarball.\nIMPORTANT: 99-slingshot-network.conf is omitted from the tarball as that prevents the UAI from running sshd as the UAI user with the su command: ncn-w001# (cd `pwd`/mount; tar --xattrs --xattrs-include=\u0026#39;*\u0026#39; --exclude=\u0026#34;99-slingshot-network.conf\u0026#34; -cf \u0026#34;../$SESSION_ID.tar\u0026#34; .) \u0026gt; /dev/null This may take several minutes. Notice that this does not create a compressed tarball. Using an uncompressed format makes it possible to add files if needed once the tarball is made. It also makes the procedure run just a bit more quickly. When making the tarball completes, check that the tarball contains \u0026lsquo;./usr/bin/uai-ssh.sh:\nncn-w001# tar tf $SESSION_ID.tar | grep \u0026#39;[.]/usr/bin/uai-ssh[.]sh\u0026#39; ./usr/bin/uai-ssh.sh If the script is not present, the easiest place to get a copy of the script is from a UAI built from the end-user UAI image provided with UAS, and it can be appended to the tarball:\nmkdir -p ./usr/bin ncn-w001# cray uas create --publickey ~/.ssh/id_rsa.pub uai_connect_string = \u0026#34;ssh vers@10.26.23.123\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.26.23.123\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-32079250\u0026#34; uai_status = \u0026#34;Pending\u0026#34; username = \u0026#34;vers\u0026#34; [uai_portmap] ncn-w001# scp vers@10.26.23.123:/usr/bin/uai-ssh.sh ./usr/bin/uai-ssh.sh The authenticity of host \u0026#39;10.26.23.123 (10.26.23.123)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:voQUCKDG4C9FGkmUcHZVrYJBXVKVYqcJ4kmTpe4tvOA. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;10.26.23.123\u0026#39; (ECDSA) to the list of known hosts. uai-ssh.sh 100% 5035 3.0MB/s 00:00 ncn-w001# cray uas delete --uai-list uai-vers-32079250 results = [ \u0026#34;Successfully deleted uai-vers-32079250\u0026#34;,] ncn-w001# tar rf 0c0d4081-2e8b-433f-b6f7-e1ef0b907be3.tar ./usr/bin/uai-ssh.sh Create and Push the Container Image Create a container image using podman or docker and push it to the site container registry. Any container-specific modifications may also be done here with a Dockerfile. The ENTRYPOINT layer must be /usr/bin/uai-ssh.sh as that starts SSHD for the user in the UAI container started by UAS.\nncn-w001# UAI_IMAGE_NAME=registry.local/cray/cray-uai-compute:latest ncn-w001# podman import --change \u0026#34;ENTRYPOINT /usr/bin/uai-ssh.sh\u0026#34; $SESSION_ID.tar $UAI_IMAGE_NAME ncn-w001# podman push $UAI_IMAGE_NAME Register the New Container Image With UAS ncn-w001# cray uas admin config images create --imagename $UAI_IMAGE_NAME Cleanup the Mount Directory and tarball ncn-w001# umount mount; rmdir mount ncn-w001# rm $SESSION_ID.tar rootfs.squashfs # NOTE: the next step could be done as an `rm -rf` but, since the user # is `root` and the path is very similar to an important system # path a more cautious approach is taken. ncn-w001# rm -f ./usr/bin/uai-ssh.sh \u0026amp;\u0026amp; rmdir ./usr/bin ./usr Troubleshooting This section contains troubleshooting tips for common problems that can arise with UAS, and some general tips that should help in diagnosing problems that are less common.\nGetting Log Output from UAS At times there will be problems with UAS. Usually this takes the form of errors showing up on CLI commands that are not immediately interpretable as some sort of input error. It is sometimes useful to examine the UAS service logs to find out what is wrong.\nThe first thing to do is to find out the names of the Kubernetes pods running UAS:\nncn-m001-pit# kubectl get po -n services | grep uas | grep -v etcd cray-uas-mgr-6bbd584ccb-zg8vx 2/2 Running 0 7d7h cray-uas-mgr-6bbd584ccb-acg7y 2/2 Running 0 7d7h The logs are collected in the pods, and can be seen using the kubectl logs command on each of the pods. Since the pods produce a lot of debug logging in the form\n127.0.0.1 - - [02/Feb/2021 22:57:18] \u0026#34;GET /v1/mgr-info HTTP/1.1\u0026#34; 200 - it is a good idea to filter this out unless the problem lies in specifically in the area of GET operations or aliveness checks. Here is an example where the last 25 lines of useful log output are retrieved from the pod cray-uas-mgr-6bbd584ccb-zg8vx:\nkubectl logs -n services cray-uas-mgr-6bbd584ccb-zg8vx cray-uas-mgr | grep -v \u0026#39;\u0026#34;GET \u0026#39; | tail -25 2021-02-03 22:02:01,576 - uas_mgr - INFO - UAS request for: vers 2021-02-03 22:02:01,628 - uas_mgr - INFO - opt_ports: [] 2021-02-03 22:02:01,702 - uas_mgr - INFO - cfg_ports: [30123] 2021-02-03 22:02:01,702 - uas_mgr - INFO - UAI Name: uai-vers-32079250; Container ports: [{\u0026#39;container_port\u0026#39;: 30123, \u0026#39;host_ip\u0026#39;: None, \u0026#39;host_port\u0026#39;: None, \u0026#39;name\u0026#39;: \u0026#39;port30123\u0026#39;, \u0026#39;protocol\u0026#39;: \u0026#39;TCP\u0026#39;}]; Optional ports: [] 2021-02-03 22:02:02,211 - uas_mgr - INFO - opt_ports: [] 2021-02-03 22:02:02,566 - uas_mgr - INFO - cfg_ports: [30123] 2021-02-03 22:02:02,703 - uas_mgr - INFO - getting deployment uai-vers-32079250 in namespace user 2021-02-03 22:02:02,718 - uas_mgr - INFO - creating deployment uai-vers-32079250 in namespace user 2021-02-03 22:02:02,734 - uas_mgr - INFO - creating the UAI service uai-vers-32079250-ssh 2021-02-03 22:02:02,734 - uas_mgr - INFO - getting service uai-vers-32079250-ssh in namespace user 2021-02-03 22:02:02,746 - uas_mgr - INFO - creating service uai-vers-32079250-ssh in namespace user 2021-02-03 22:02:02,757 - uas_mgr - INFO - getting pod info uai-vers-32079250 2021-02-03 22:02:02,841 - uas_mgr - INFO - No start time provided from pod 2021-02-03 22:02:02,841 - uas_mgr - INFO - getting service info for uai-vers-32079250-ssh in namespace user 127.0.0.1 - - [03/Feb/2021 22:02:02] \u0026#34;POST /v1/uas HTTP/1.1\u0026#34; 200 - 2021-02-03 22:15:32,697 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-03 22:15:32,698 - uas_mgr - INFO - UAS request for: vers 2021-02-03 22:15:32,698 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-03 22:15:32,770 - uas_mgr - INFO - deleting service uai-vers-32079250-ssh in namespace user 2021-02-03 22:15:32,802 - uas_mgr - INFO - delete deployment uai-vers-32079250 in namespace user 127.0.0.1 - - [03/Feb/2021 22:15:32] \u0026#34;DELETE /v1/uas?uai_list=uai-vers-32079250 HTTP/1.1\u0026#34; 200 - If an error had occurred in UAS that error would likely show up here. Because there are two replicas of cray-uas-mgr running, the logging of interest may be in the other pod, so apply the same command to the other pod if the information is not here.\nGetting Log Output from UAIs Sometimes a UAI will come up and run but will not work correctly. It is possible to see errors reported by elements of the UAI entrypoint script using the kubectl logs command. First find the UAI of interest. This starts by identifying the UAI name using the CLI:\nncn-m001-pit# cray uas admin uais list [[results]] uai_age = \u0026#34;4h30m\u0026#34; uai_connect_string = \u0026#34;ssh broker@10.103.13.162\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026#34; uai_ip = \u0026#34;10.103.13.162\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-broker-2e6ce6b7\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;broker\u0026#34; [[results]] uai_age = \u0026#34;1h12m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.20.49.135\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.20.49.135\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-6da50e7a\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; Using this, find the UAI in question, remembering that end-user UAIs run in the user Kubernetes namespace and broker UAIs run in the uas Kubernetes namespace.\nncn-m001-pit# kubectl get po -n user | grep uai-vers-6da50e7a uai-vers-6da50e7a-54dbc99fdd-csxmk 1/1 Running 0 76m or\nncn-m001-pit# kubectl get po -n uas | grep uai-broker-2e6ce6b7 uai-broker-2e6ce6b7-68d78c6c95-s28dh 2/2 Running 0 4h34m Using the UAI\u0026rsquo;s pod name and the user namespace, get the logs:\nncn-m001-pit# kubectl logs -n user uai-vers-6da50e7a-54dbc99fdd-csxmk uai-vers-6da50e7a Setting up passwd and group entries for vers Setting profile for vers Adding vers to groups Disabling password based login passwd: password expiry information changed. Checking to see if /home/users/vers exists If this hangs, please ensure that /home/users/vers is properly mounted/working on the host of this pod No home directory exists, creating one Checking for munge.key Setting up munge.key Check for pbs.conf Generating SSH keys and sshd_config ssh-keygen: generating new host keys: RSA DSA ECDSA ED25519 ... or, for the broker using the broker UAI pod\u0026rsquo;s name and the uas namespace:\nncn-m001-pit# kubectl logs -n uas uai-broker-2e6ce6b7-68d78c6c95-s28dh uai-broker-2e6ce6b7 /bin/bash: warning: setlocale: LC_ALL: cannot change locale (C.UTF-8) Configure PAM to use sssd... Generating broker host keys... ssh-keygen: generating new host keys: RSA DSA ECDSA ED25519 Checking for UAI_CREATION_CLASS... Starting sshd... Starting sssd... (Wed Feb 3 18:34:41:792821 2021) [sssd] [sss_ini_get_config] (0x0020): Config merge error: Directory /etc/sssd/conf.d does not exist. The above is from a successful broker starting and running.\nStale Brokered UAIs When a broker UAI terminates and re-starts the SSH key used to forward SSH sessions to end-user UAIs changes (this is a known problem) and subsequent broker UAIs are unable to forward sessions to end-user UAIs. The symptom of this is that a user logging into a broker UAI will receive a password prompt from the end-user UAI and be unable to log in even if providing the correct password. To fix this, remove the stale end-user UAIs and allow the broker UAI to re-create them. The easy way to do this is to use the\ncray uas admin uais delete --class-id \u0026lt;creation-class-id\u0026gt; command specifying the uai-creation-class identifier from the broker\u0026rsquo;s UAI class. For example:\nncn-m001-pit# cray uas admin config classes list | grep -e class_id -e comment class_id = \u0026#34;74970cdc-9f94-4d51-8f20-96326212b468\u0026#34; comment = \u0026#34;UAI broker class\u0026#34; class_id = \u0026#34;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026#34; comment = \u0026#34;UAI User Class\u0026#34; class_id = \u0026#34;bb28a35a-6cbc-4c30-84b0-6050314af76b\u0026#34; comment = \u0026#34;Non-Brokered UAI User Class\u0026#34; ncn-m001-pit# cray uas admin config classes describe 74970cdc-9f94-4d51-8f20-96326212b468 | grep uai_creation_class uai_creation_class = \u0026#34;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026#34; ncn-m001-pit# cray uas admin uais delete --class-id a623a04a-8ff0-425e-94cc-4409bdd49d9c results = [ \u0026#34;Successfully deleted uai-vers-6da50e7a\u0026#34;,] After that, users should be able to log into the broker UAI and be directed to an end-user UAI as before.\nStuck UAIs Sometimes the UAI will show a uai_status field of Waiting and a uai_msg field of ContainerCreating. It is possible that this is just a matter of starting the UAI taking longer than normal, perhaps as it pulls in a new UAI image from a registry, but, if it persists for a long time, it is worth investigating. To do this, first find the UAI:\nncn-m001-pit# cray uas admin uais list --owner ctuser [[results]] uai_age = \u0026#34;1m\u0026#34; uai_connect_string = \u0026#34;ssh ctuser@10.103.13.159\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.103.13.159\u0026#34; uai_msg = \u0026#34;ContainerCreating\u0026#34; uai_name = \u0026#34;uai-ctuser-bcd1ff74\u0026#34; uai_status = \u0026#34;Waiting\u0026#34; username = \u0026#34;ctuser\u0026#34; Then look up its pod in Kubernetes:\nncn-m001-pit# kubectl get po -n user | grep uai-ctuser-bcd1ff74 uai-ctuser-bcd1ff74-7d94967bdc-4vm66 0/1 ContainerCreating 0 2m58s Then describe the pod in Kubernetes:\nncn-m001-pit# kubectl describe pod -n user uai-ctuser-bcd1ff74-7d94967bdc-4vm66 Name: uai-ctuser-bcd1ff74-7d94967bdc-4vm66 Namespace: user Priority: -100 Priority Class Name: uai-priority Node: ncn-w001/10.252.1.12 Start Time: Wed, 03 Feb 2021 18:33:00 -0600 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled \u0026lt;unknown\u0026gt; default-scheduler Successfully assigned user/uai-ctuser-bcd1ff74-7d94967bdc-4vm66 to ncn-w001 Warning FailedMount 2m53s (x8 over 3m57s) kubelet, ncn-w001 MountVolume.SetUp failed for volume \u0026#34;broker-sssd-config\u0026#34; : secret \u0026#34;broker-sssd-conf\u0026#34; not found Warning FailedMount 2m53s (x8 over 3m57s) kubelet, ncn-w001 MountVolume.SetUp failed for volume \u0026#34;broker-sshd-config\u0026#34; : configmap \u0026#34;broker-sshd-conf\u0026#34; not found Warning FailedMount 2m53s (x8 over 3m57s) kubelet, ncn-w001 MountVolume.SetUp failed for volume \u0026#34;broker-entrypoint\u0026#34; : configmap \u0026#34;broker-entrypoint\u0026#34; not found Warning FailedMount 114s kubelet, ncn-w001 Unable to attach or mount volumes: unmounted volumes=[broker-sssd-config broker-entrypoint broker-sshd-config], unattached volumes=[optcraype optlmod etcprofiled optr optforgelicense broker-sssd-config lustre timezone optintel optmodulefiles usrsharelmod default-token-58t5p optarmlicenceserver optcraycrayucx slurm-config opttoolworks optnvidiahpcsdk munge-key optamd opttotalview optgcc opttotalviewlicense broker-entrypoint broker-sshd-config etccrayped opttotalviewsupport optcraymodulefilescrayucx optforge usrlocalmodules varoptcraypepeimages]: timed out waiting for the condition This produces a lot of output, all of which can be useful for diagnosis, but a good place to start is in the Events section at the bottom. Notice the warnings here about volumes whose secrets and configmaps are not found. In this case, that means the UAI cannot start because it was started in legacy mode without a default UAI class, and some of the volumes configured in the UAS are in the uas namespace to support localization of broker UAIs and cannot be found in the user namespace. To solve this particular problem, the best move would be to configure a default UAI class with the correct volume list in it, delete the UAI and allow the user to try creating it again using the default class.\nOther problems can usually be quickly identified using this and other information found in the output from the kubectl describe pod command.\nDuplicate Mount Paths in a UAI If a user attempts to create a UAI in the legacy mode and cannot create the UAI at all, a good place to look is at volumes. Duplicate mount_path specifications in the list of volumes in a UAI will cause a failure that looks like this:\nncn-m001-pit# cray uas create --publickey ~/.ssh/id_rsa.pub Usage: cray uas create [OPTIONS] Try \u0026#39;cray uas create --help\u0026#39; for help. Error: Unprocessable Entity: Failed to create deployment uai-erl-543cdbbc: Unprocessable Entity At present there is not a lot of UAS log information available from this error (this is a known problem), but a likely cause is duplicate mount_path specifications in volumes. Looking through the configured volumes for duplicates can be helpful.\nncn-m001-pit# cray uas admin config volumes list | grep -e mount_path -e volumename -e volume_id mount_path = \u0026#34;/app/broker\u0026#34; volume_id = \u0026#34;1f3bde56-b2e7-4596-ab3a-6aa4327d29c7\u0026#34; volumename = \u0026#34;broker-entrypoint\u0026#34; mount_path = \u0026#34;/etc/sssd\u0026#34; volume_id = \u0026#34;4dc6691e-e7d9-4af3-acde-fc6d308dd7b4\u0026#34; volumename = \u0026#34;broker-sssd-config\u0026#34; mount_path = \u0026#34;/etc/localtime\u0026#34; volume_id = \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34; volumename = \u0026#34;timezone\u0026#34; mount_path = \u0026#34;/root/slurm_config/munge\u0026#34; volume_id = \u0026#34;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026#34; volumename = \u0026#34;munge-key\u0026#34; mount_path = \u0026#34;/opt/forge\u0026#34; volume_id = \u0026#34;7b924270-c9e9-4b0e-85f5-5bc62c02457e\u0026#34; volumename = \u0026#34;delete-me\u0026#34; mount_path = \u0026#34;/lus\u0026#34; volume_id = \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34; volumename = \u0026#34;lustre\u0026#34; mount_path = \u0026#34;/etc/switchboard\u0026#34; volume_id = \u0026#34;d5058121-c1b6-4360-824d-3c712371f042\u0026#34; volumename = \u0026#34;broker-sshd-config\u0026#34; mount_path = \u0026#34;/etc/slurm\u0026#34; volume_id = \u0026#34;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026#34; volumename = \u0026#34;slurm-config\u0026#34; mount_path = \u0026#34;/opt/forge_license\u0026#34; volume_id = \u0026#34;ecfae4b2-d530-4c06-b757-49b30061c90a\u0026#34; volumename = \u0026#34;optforgelicense\u0026#34; mount_path = \u0026#34;/opt/forge\u0026#34; volume_id = \u0026#34;fc95d0da-6296-4d0b-8f26-2d4338604991\u0026#34; volumename = \u0026#34;optforge\u0026#34; Looking through this list, the mount path for the volume named delete-me and the mount path for the volume named optforge are the same. The obvious candidate for deletion in this case is delete-me so it can be deleted\nncn-m001-pit# cray uas admin config volumes delete 7b924270-c9e9-4b0e-85f5-5bc62c02457e mount_path = \u0026#34;/opt/forge\u0026#34; volume_id = \u0026#34;7b924270-c9e9-4b0e-85f5-5bc62c02457e\u0026#34; volumename = \u0026#34;delete-me\u0026#34; [volume_description.host_path] path = \u0026#34;/tmp/foo\u0026#34; type = \u0026#34;DirectoryOrCreate\u0026#34; Missing or Incorrect UAI Images If a UAI shows a uai_status of Waiting and a uai_msg of ImagePullBackOff that indicates that the UAI or the UAI class is configured to use an image that is not in the image registry. Either obtaining and pushing the image to the image registry, or correcting the name / version of the image in the UAS configuration will usually resolve this.\nAdministrative Access to UAIs for Diagnosis Sometimes there is no better way to figure out a problem with a UAI than to get inside it and look around as an administrator. This is done using kubectl exec to start a shell inside the running container as \u0026ldquo;root\u0026rdquo; (in the container). With this an administrator can diagnose problems, make changes to the running UAI and find solutions. It is important to remember, though, that any change made inside a UAI is transitory. These changes only last as long as the UAI is running. To make a permanent change, either the UAI image has to be changed or external customizations must be applied.\nHere is an example session showing a ps command inside the container of a UAI by an administrator:\nncn-m001-pit# cray uas admin uais list [[results]] uai_age = \u0026#34;1d4h\u0026#34; uai_connect_string = \u0026#34;ssh broker@10.103.13.162\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026#34; uai_ip = \u0026#34;10.103.13.162\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-broker-2e6ce6b7\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;broker\u0026#34; [[results]] uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.29.162.104\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.29.162.104\u0026#34; uai_msg = \u0026#34;\u0026#34; uai_name = \u0026#34;uai-vers-4ebe1966\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; ncn-m001-pit# kubectl get po -n user | grep uai-vers-4ebe1966 uai-vers-4ebe1966-77b7c9c84f-xgqm4 1/1 Running 0 77s ncn-m001-pit# kubectl exec -it -n user uai-vers-4ebe1966-77b7c9c84f-xgqm4 -c uai-vers-4ebe1966 -- /bin/sh sh-4.4# ps -afe UID PID PPID C STIME TTY TIME CMD root 1 0 0 22:56 ? 00:00:00 /bin/bash /usr/bin/uai-ssh.sh munge 36 1 0 22:56 ? 00:00:00 /usr/sbin/munged root 54 1 0 22:56 ? 00:00:00 su vers -c /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 55 54 0 22:56 ? 00:00:00 /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D root 90 0 0 22:58 pts/0 00:00:00 /bin/sh root 97 90 0 22:58 pts/0 00:00:00 ps -afe sh-4.4# The procedure is to find the name of the UAI in question, use that with kubectl to find the pod containing that UAI, use the pod name and the user namespace to set up the kubectl exec specifying the UAI name as the container to exec into and specifying /bin/sh as the command to run. From there, the administrator can look around inside the UAI as needed.\nCommon Mistakes to Check for When Making a Custom End-User UAI Image If problems occur while making or working with a custom end-user UAI image some basic troubleshooting questions to ask are:\nDoes SESSION_NAME match an actual entry in \u0026ldquo;cray bos sessiontemplate list\u0026rdquo;? Is SESSION_ID set to an appropriate uuid format? Did the awk command not parse the uuid correctly? Did the file /etc/security/limits.d/99-slingshot-network.conf get removed from the tarball correctly? Does the ENTRYPOINT /usr/bin/uai-ssh.sh exist? Did the container image get pushed and registered with UAS? Did the creation process run from a real worker or master node as opposed to a LiveCD node? "
},
{
	"uri": "/docs-csm/en-09/",
	"title": "CRAY System Management - Guides and References",
	"tags": [],
	"description": "",
	"content": "CRAY System Management - Guides and References These pages are available offline on the LiveCD all CSM documentation can be found at /usr/share/doc/metal.\nCRAY System Management - Guides and References Offline Documentation Review and Contribution Releases and Vintages Versioning Discussions Original Authors / Reviewers This repository serves to provides coherent guides for installing or upgrading a CRAY system across all its various node-types and states.\nProduct Coverage:\nCray Pre-Install Toolkit (LiveCD/PIT) Non-Compute Nodes (NCN) Compute Nodes (CNs) User Access Nodes (UAN) High-Speed Network (HSN) hpe-portal: 📑slingshot documentation One may also find technical information, see the following for navigating and contributing to this guidebook:\nInfo / Inside-Panel Contribution and rules Table of Contents Lay of the land; where information is by chapter Offline Documentation The documentation on a customer\u0026rsquo;s LiveCD should match their reality, their install should follow the docs shipped on their liveCD.\nThis will report the version of your installed documentation:\nsles# rpm -q docs-csm To install the latest docs-csm RPM after installation:\nsles# zypper ar -cf --gpgcheck-allow-unsigned https://packages.local/repository/csm-sle-15sp2 csm-sle-15sp2 sles# zypper ref csm-sle-15sp2 sles# zypper in -y --from csm-sle-15sp2 docs-csm Review and Contribution Anyone with Git access to this repo may feel free to submit changes for review, tagging to the relevant JIRA(s) (if necessary).\nAll changes undergo a review process, this governance is up to the reviewers\u0026rsquo; own discretions. The review serves to keep core contributors on the \u0026ldquo;same page\u0026rdquo; while maintaining coherency throughout the doc.\nReleases and Vintages This guide follows a basic release model for enabling amendments and maintenance across releases.\nNote: Leading up to a release heads out the door the \u0026ldquo;stable\u0026rdquo; and \u0026ldquo;unstable\u0026rdquo; branches may be equal. However once a release has shipped, any amendments to that release must be made to the respective release branch.\nThe \u0026ldquo;stable\u0026rdquo; (release) version of this guide exists within branches prefixed with \u0026ldquo;release/\u0026rdquo; The \u0026ldquo;unstable\u0026rdquo; (latest) version of this guide exists within the master branch Versioning This guide is versioned and packaged for offline or in-field reference.\nX.Y.Z-HASH The HASH will always change, it changes for every contribution that is pushed to this repository.\nThe X.Y.Z does not always change, it must be incremented by the contributor or this repository\u0026rsquo;s owner(s). This pattern follows semver:\nX: Major Version - this should be incremented by the repo owner for dramatic, or substantial changes to the structure or format of the guide. Y: Minor Version - this should be incremented by the developer when making new pages or large amendments to the flow. Z: Bug Fix/patch - this should be incremented by the developer when making amendments confined to a page. Any contributor should feel welcome to ask for clarification on versioning within their change\u0026rsquo;s review.\nDiscussions See the Cray /HPE Slack #docs-csm (not public; external access may be available for various partners and customers).\nOriginal Authors / Reviewers This document can be discussed in #docs-csm.\nThese folks are main contributors or reviewers, none of which are the owners of this repository. Any email should include the list, otherwise ping the slack channel.\nPET: Brad Klein PET: Craig DeLatte METAL: Jacob Salmela PET: Jeanne Ohren METAL: Russell Bunch CMS: Mitch Harding "
},
{
	"uri": "/docs-csm/en-09/release_notes/",
	"title": "Cray System Management (CSM) - Release Notes",
	"tags": [],
	"description": "",
	"content": "Cray System Management (CSM) - Release Notes What’s new Bug Fixes Known Issues Cfs_session_stuck_in_pending: Under some circumstances Configuration Framework Service (CFS) sessions can get stuck in a pending state, never completing and potentially blocking other sessions. This addresses cleaning up those sessions. Conman_pod_kubernetes_copy_fails: The kubernetes copy file command fails when attempting to copy log files from the cray-conman pod. After a boot or reboot a few CFS Pods may continue running even after they\u0026rsquo;ve finished and never go away. For more information see Orphaned CFS Pods After Booting or Rebooting. "
},
{
	"uri": "/docs-csm/en-09/troubleshooting/known_issues/",
	"title": "known issues",
	"tags": [],
	"description": "",
	"content": "known issues Topics:\nCFS Sessions are Stuck in Pending State Copying file from the cray-conman pod fails Orphaned CFS Pods After Booting or Rebooting "
},
{
	"uri": "/docs-csm/en-09/upgrade/0.9/csm-0.9.5/operations/network/metallb_bgp/",
	"title": "metallb bgp",
	"tags": [],
	"description": "",
	"content": "metallb bgp Topics:\nCheck BGP Status and Reset Sessions "
},
{
	"uri": "/docs-csm/en-09/upgrade/0.9/csm-0.9.5/operations/network/",
	"title": "network",
	"tags": [],
	"description": "",
	"content": "network Topics:\nmetallb bgp "
},
{
	"uri": "/docs-csm/en-09/upgrade/0.9/csm-0.9.5/operations/node_management/",
	"title": "node management",
	"tags": [],
	"description": "",
	"content": "node management Topics:\nCheck and Set the metal.no-wipe Setting on NCNs Reboot NCNs "
},
{
	"uri": "/docs-csm/en-09/upgrade/0.9/csm-0.9.5/operations/",
	"title": "operations",
	"tags": [],
	"description": "",
	"content": "operations Topics:\nnetwork node management "
},
{
	"uri": "/docs-csm/en-09/operations/boot_orchestration/",
	"title": "boot orchestration",
	"tags": [],
	"description": "",
	"content": "boot orchestration Topics:\nStage Changes Without BOS Upload Node Boot Information to Boot Script Service (BSS) "
},
{
	"uri": "/docs-csm/en-09/operations/configuration_management/",
	"title": "configuration management",
	"tags": [],
	"description": "",
	"content": "configuration management Topics:\nConfiguration Layers "
},
{
	"uri": "/docs-csm/en-09/operations/csm_product_management/",
	"title": "CSM product management",
	"tags": [],
	"description": "",
	"content": "CSM product management Topics:\nSecurity Hardening Change Passwords and Credentials "
},
{
	"uri": "/docs-csm/en-09/operations/kubernetes/",
	"title": "kubernetes",
	"tags": [],
	"description": "",
	"content": "kubernetes Topics:\nKubernetes and Bare Metal EtcD Certificate Renewal "
},
{
	"uri": "/docs-csm/en-09/operations/network/metallb_bgp/",
	"title": "metallb bgp",
	"tags": [],
	"description": "",
	"content": "metallb bgp Topics:\nCheck BGP Status and Reset Sessions "
},
{
	"uri": "/docs-csm/en-09/operations/network/",
	"title": "network",
	"tags": [],
	"description": "",
	"content": "network Topics:\nmetallb bgp "
},
{
	"uri": "/docs-csm/en-09/operations/system_management_health/",
	"title": "system management health",
	"tags": [],
	"description": "",
	"content": "system management health Topics:\nTroubleshoot Prometheus Alerts "
},
{
	"uri": "/docs-csm/en-09/upgrade/",
	"title": "upgrade",
	"tags": [],
	"description": "",
	"content": "upgrade Topics:\n0.9 "
},
{
	"uri": "/docs-csm/en-09/operations/fas_loader/",
	"title": "FAS Loader",
	"tags": [],
	"description": "",
	"content": "FAS Loader Topics:\nTROUBLESHOOTING FAS LOADER FAILS "
},
{
	"uri": "/docs-csm/en-09/operations/hmcollector/",
	"title": "hmcollector",
	"tags": [],
	"description": "",
	"content": "hmcollector Topics:\nAdjust HM Collector resource limits and requests "
},
{
	"uri": "/docs-csm/en-09/operations/security_and_authentication/",
	"title": "security and authentication",
	"tags": [],
	"description": "",
	"content": "security and authentication Topics:\nAdd LDAP User Federation Change Air-Cooled Node BMC Credentials Change Credentials on ServerTech PDUs Change Cray EX Cabinet Global Default Password Change NCN Image Root Password and SSH Keys Change NCN Image Root Password and SSH Keys on PIT Node Change SMNP Credentials on Leaf Switches Change the Keycloak Admin Password Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials Restrict Network Access to the ncn-images S3 Bucket Update Default Air-Cooled BMC and Leaf Switch SNMP Credentials Update Default ServerTech PDU Credentials used by the Redfish Translation Service (RTS) Update NCN Passwords Updating the Liquid-Cooled EX Cabinet Default Credentials after a CEC Password Change "
},
{
	"uri": "/docs-csm/en-09/operations/node_management/",
	"title": "node management",
	"tags": [],
	"description": "",
	"content": "node management Topics:\nAdding a Liquid-cooled blade to a System Check and Set the metal.no-wipe Setting on NCNs Enable Kdump Reboot NCNs Removing a Liquid-cooled blade from a System "
},
{
	"uri": "/docs-csm/en-09/operations/",
	"title": "operations",
	"tags": [],
	"description": "",
	"content": "operations Topics:\nCSM product management FAS Loader boot orchestration configuration management hmcollector kubernetes network node management security and authentication system management health "
},
{
	"uri": "/docs-csm/en-09/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/docs-csm/en-09/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]