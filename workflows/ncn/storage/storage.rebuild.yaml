#
# MIT License
#
# (C) Copyright 2022 Hewlett Packard Enterprise Development LP
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included
# in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
# OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
# ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
# OTHER DEALINGS IN THE SOFTWARE.
#
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ncn-lifecycle-rebuild-
  labels:
    target-ncns: "{{$length := len .TargetNcns }}{{range $index,$value := .TargetNcns }}{{$myvar := add $index 1}}{{if lt $myvar $length}}{{$value}}.{{else}}{{$value}}{{end}}{{ end }}"
    type: rebuild
    node-type: storage
spec:
  podMetadata:
    annotations:
      sidecar.istio.io/inject: "false"
  volumes:
    - name: ssh
      hostPath:
        path: /root/.ssh
        type: Directory
    - name: host-usr-bin
      hostPath:
        path: /usr/bin
        type: Directory
    - name: podinfo
      downwardAPI:
        items:
          - path: "labels"
            fieldRef:
              fieldPath: metadata.labels
          - path: "annotations"
            fieldRef:
              fieldPath: metadata.annotations
  # schedule workflow jobs asap
  priorityCLassName: system-node-critical
  # Pod GC strategy must be one of the following:
  # * OnPodCompletion - delete pods immediately when pod is completed (including errors/failures)
  # * OnPodSuccess - delete pods immediately when pod is successful
  # * OnWorkflowCompletion - delete pods when workflow is completed
  # * OnWorkflowSuccess - delete pods when workflow is successful
  podGC:
    strategy: OnPodCompletion
  # allow workflow jobs running on master node
  #   we may have a situation that all worker nodes
  #   are marked as "being rebuilt" (cray.nls=ncn-w001)
  tolerations:
    - key: "node-role.kubernetes.io/master"
      operator: "Exists"
      effect: "NoSchedule"
  affinity:
    nodeAffinity:
      # avoid putting workflow jobs onto workers that will be rebuilt
      # this label is set onto each workers at beginning of workflow
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cray.nls
            operator: NotIn
            values:
            {{- range $index,$value := .TargetNcns }}
            - {{$value -}}
            {{- end }}
      # try to use master nodes as much as possible
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 50
          preference:
            matchExpressions:
            - key: node-role.kubernetes.io/master
              operator: Exists
  entrypoint: main
  templates:
    - name: main
      dag:
        tasks:
          - name: check-ceph-ro-key
            templateRef:
              name: ssh-template
              template: shell-script
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"
                - name: scriptContent
                  value: |
                    # REVIEW: Should I wrap this jq or not? Leaving as-is for now.
                    ceph auth get client.ro >/dev/null 2>/dev/null || ceph-authtool -C /etc/ceph/ceph.client.ro.keyring -n client.ro --cap mon 'allow r' --cap mds 'allow r' --cap osd 'allow r' --cap mgr 'allow r' --gen-key
                    ceph auth import -i /etc/ceph/ceph.client.ro.keyring
                    for node in $(ceph orch host ls --format=json|jq -r '.[].hostname'); do scp /etc/ceph/ceph.client.ro.keyring $node:/etc/ceph/ceph.client.ro.keyring; done
          - name: pre-upgrade-ceph-health-check
            templateRef:
              name: ssh-template
              template: shell-script
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"
                - name: scriptContent
                  value: |
                    /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -v true || /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -A true -v true
          {{- range $index,$value := .TargetNcns}}
          - name: upgrade-{{$value}}
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies:
              - check-ceph-ro-key
              - pre-upgrade-ceph-health-check
              # each upgrade depends on previous upgrade action and success
              {{ if ne $index 0 }}
              - check-ceph-health-{{ index $.TargetNcns (add $index -1) }}
              {{ end }}
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"
                - name: scriptContent
                  value: |
                    /usr/share/doc/csm/upgrade/scripts/upgrade/ncn-upgrade-ceph-nodes.sh {{$value}}
          - name: check-ceph-health-{{$value}}
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies:
              # check health once node upgrade is complete
              - upgrade-{{$value}}
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"
                - name: scriptContent
                  value: |
                    /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -v true
                    /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -n {{$value}} -a true -v true
          {{- end }}
          - name: rescan-ssh-keys
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies:
              # wait for health check of last storage node complete
              - check-ceph-health-{{ with $length := len $.TargetNcns }}{{ index $.TargetNcns (add $length -1) }}{{end}}
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"
                - name: scriptContent
                  value: |
                    grep -oP "(ncn-s\w+)" /etc/hosts | sort -u | xargs -t -i ssh {} 'truncate --size=0 ~/.ssh/known_hosts'
                    grep -oP "(ncn-s\w+)" /etc/hosts | sort -u | xargs -t -i ssh {} 'grep -oP "(ncn-s\w+|ncn-m\w+|ncn-w\w+)" /etc/hosts | sort -u | xargs -t -i ssh-keyscan -H \{\} >> /root/.ssh/known_hosts'
          - name: update-bss
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies:
              - rescan-ssh-keys
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"
                - name: scriptContent
                  value: |
                    . /usr/share/doc/csm/upgrade/scripts/ceph/lib/update_bss_metadata.sh
                    RESULT=$(update_bss_storage)
                    # check that result contains success!
                    if [[ $RESULT != *"Success!"* ]]
                    then
                      echo "BSS metadata was not successfully updated. Output:"
                      echo "$RESULT"
                      exit 7
                    fi
