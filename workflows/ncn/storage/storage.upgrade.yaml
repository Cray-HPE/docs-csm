#
# MIT License
#
# (C) Copyright 2022-2023 Hewlett Packard Enterprise Development LP
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included
# in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
# OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
# ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
# OTHER DEALINGS IN THE SOFTWARE.
#
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ncn-lifecycle-rebuild-
  labels:
    target-ncns: "{{$length := len .TargetNcns }}{{range $index,$value := .TargetNcns }}{{$myvar := add $index 1}}{{if lt $myvar $length}}{{$value}}.{{else}}{{$value}}{{end}}{{ end }}"
    type: rebuild
    node-type: storage
spec:
  podMetadata:
    annotations:
      sidecar.istio.io/inject: "false"    
  volumes:
    - name: ssh
      hostPath:
        path: /root/.ssh
        type: Directory
    - name: host-usr-bin
      hostPath:
        path: /usr/bin
        type: Directory
    - name: podinfo
      downwardAPI:
        items:
          - path: "labels"
            fieldRef:
              fieldPath: metadata.labels
          - path: "annotations"
            fieldRef:
              fieldPath: metadata.annotations
  # schedule workflow jobs asap
  priorityCLassName: system-node-critical
  # Pod GC strategy must be one of the following:
  # * OnPodCompletion - delete pods immediately when pod is completed (including errors/failures)
  # * OnPodSuccess - delete pods immediately when pod is successful
  # * OnWorkflowCompletion - delete pods when workflow is completed
  # * OnWorkflowSuccess - delete pods when workflow is successful
  podGC:
    strategy: OnPodCompletion
  # allow workflow jobs running on master node
  #   we may have a situation that all worker nodes
  #   are marked as "being rebuilt" (cray.nls=ncn-w001)
  tolerations:
    - key: "node-role.kubernetes.io/master"
      operator: "Exists"
      effect: "NoSchedule"
  affinity:
    nodeAffinity:
      # avoid putting workflow jobs onto workers that will be rebuilt
      # this label is set onto each workers at beginning of workflow
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cray.nls
            operator: NotIn
            values:
            {{- range $index,$value := .TargetNcns }}
            - {{$value -}}
            {{- end }}
      # try to use master nodes as much as possible
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 50
          preference:
            matchExpressions:
            - key: node-role.kubernetes.io/master
              operator: Exists
  entrypoint: main
  templates:
    - name: main
      dag:
        tasks:
          - name: check-ceph-ro-key
            templateRef:
              name: ssh-template
              template: shell-script
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    if ! $(ceph auth get client.ro >/dev/null 2>/dev/null); then
                      ceph-authtool -C /etc/ceph/ceph.client.ro.keyring -n client.ro --cap mon 'allow r' --cap mds 'allow r' --cap osd 'allow r' --cap mgr 'allow r' --gen-key
                    fi
                    if [ -f "/etc/ceph/ceph.client.ro.keyring" ]; then
                      ceph auth import -i /etc/ceph/ceph.client.ro.keyring
                    else
                      ceph auth get client.ro -o /etc/ceph/ceph.client.ro.keyring
                      ceph auth import -i /etc/ceph/ceph.client.ro.keyring
                    fi
                    for node in $(ceph orch host ls --format=json|jq -r '.[].hostname'); do scp /etc/ceph/ceph.client.ro.keyring $node:/etc/ceph/ceph.client.ro.keyring; done
          - name: pre-upgrade-ceph-health-check
            templateRef:
              name: ssh-template
              template: shell-script
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"
                - name: scriptContent
                  value: |
                    /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -v true || /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -A true -v true
          - name: change-ceph-config
            templateRef:
              name: ssh-template
              template: shell-script
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"
                - name: scriptContent
                  value: |
                    echo "This step will remove mgr/cephadm/container_image_base if is set in the ceph configuration."
                    exists=$(ceph config get mgr | grep "mgr/cephadm/container_image_base")
                    if [[ -n $exists ]]; then
                      ceph config rm mgr mgr/cephadm/container_image_base
                    fi
          {{- range $index,$value := .TargetNcns}}
          {{ if eq $index 0 }}
          - name: upload-ceph-images-to-nexus
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies: 
              - check-ceph-ro-key
              - pre-upgrade-ceph-health-check
              - change-ceph-config
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    ts=$(echo $RANDOM | md5sum | head -c 20; echo)
                    cat <<'EOF' > "/tmp/${ts}.sh"
                    m001_ip=$(host ncn-m001 | awk '{ print $NF }')
                    ssh-keygen -R ncn-m001 -f ~/.ssh/known_hosts > /dev/null 2>&1
                    ssh-keygen -R ${m001_ip} -f ~/.ssh/known_hosts > /dev/null 2>&1
                    ssh-keyscan -H "ncn-m001,${m001_ip}" >> ~/.ssh/known_hosts
                    nexus_username=$(ssh ncn-m001 kubectl get secret -n nexus nexus-admin-credential -o json | jq '.data.username' | tr -d '"' | base64 --decode)
                    nexus_password=$(ssh ncn-m001 kubectl get secret -n nexus nexus-admin-credential -o json | jq '.data.password' | tr -d '"' | base64 --decode)
                    ssh_options="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
                    function upload_image() {
                        # get local image and nexus image location
                        name=$1
                        prefix=$2
                        to_configure=$3
                        local_image=$(ceph --name client.ro orch ps --format json | jq --arg DAEMON $name '.[] | select(.daemon_type == $DAEMON) | .container_image_name' | tr -d '"' | sort -u | tail -1)
                        # if sha in image then remove and use version
                        if [[ $local_image == *"@sha"* ]]; then
                            without_sha=${local_image%"@sha"*}
                            version=$(ceph --name client.ro orch ps --format json | jq --arg DAEMON $name '.[] | select(.daemon_type == $DAEMON) | .version' | tr -d '"' | sort -u)
                            if [[ $version != "v"* ]]; then version="v""$version"; fi
                            local_image="$without_sha"":""$version"
                        fi
                        nexus_location="${prefix}""$(echo "$local_image" | rev | cut -d "/" -f1 | rev)"

                        # push images to nexus, point to nexus and run upgrade
                        echo "Pushing image: $local_image to $nexus_location"
                        podman pull $local_image
                        podman tag $local_image $nexus_location
                        podman push --creds $nexus_username:$nexus_password $nexus_location

                        # set global container and write image if mgr
                        if [[ $name == "mgr" ]]; then
                          for storage_node in "ncn-s001" "ncn-s002" "ncn-s003"; do
                            ssh $storage_node ${ssh_options} "ceph config set global container_image $nexus_location"
                            ssh ncn-m001 "echo $nexus_location > /tmp/ceph_global_container_image.txt"
                            if [[ $? == 0 ]]; then
                              break
                            fi
                          done
                        else
                          for storage_node in "ncn-s001" "ncn-s002" "ncn-s003"; do
                              ssh $storage_node ${ssh_options} "ceph config set mgr $to_configure $nexus_location"
                              if [[ $? == 0 ]]; then
                                break
                              fi
                          done
                        fi
                    }
                    prometheus_prefix="registry.local/artifactory.algol60.net/csm-docker/stable/quay.io/prometheus/"
                    upload_image "prometheus" $prometheus_prefix "mgr/cephadm/container_image_prometheus"
                    upload_image "node-exporter" $prometheus_prefix "mgr/cephadm/container_image_node_exporter"
                    upload_image "alertmanager" $prometheus_prefix "mgr/cephadm/container_image_alertmanager"
                    # mgr and grafana have this prfix
                    ceph_prefix="registry.local/artifactory.algol60.net/csm-docker/stable/quay.io/ceph/"
                    upload_image "grafana" $ceph_prefix "mgr/cephadm/container_image_grafana"
                    upload_image "mgr" $ceph_prefix "mgr/cephadm/container_image"
                    EOF
                    chmod +x /tmp/${ts}.sh
                    scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null /tmp/${ts}.sh {{$value}}:/tmp/${ts}.sh
                    ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{$value}} "/tmp/${ts}.sh"
          - name: pull-ceph-image-in-nexus
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies: 
              - upload-ceph-images-to-nexus
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"
                - name: scriptContent
                  value: |
                    nexus_image=$(cat /tmp/ceph_global_container_image.txt)
                    pdsh -w $(ceph orch host ls --format=json|jq -r '.[].hostname' | sort -u |  tr -t '\n' ',') "podman pull $nexus_image"
          - name: run-ceph-upgrade
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies:
              - pull-ceph-image-in-nexus
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    ssh_options="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
                    nexus_location=$(cat /tmp/ceph_global_container_image.txt)
                    ceph orch upgrade start --image $nexus_location
                    echo "Waiting for upgrade to complete..."
                    sleep 10
                    int=0
                    success=false
                    while [[ $int -lt 100 ]] && ! $success; do
                        if [[ -n $(ceph orch upgrade status --format json | jq '.message' | grep Error) ]]; then
                          echo "Error: there was an issue with the upgrade. Run 'ceph orch upgrade status' from ncn-s00[1/2/3]."
                          exit 1
                        fi
                        if [[ $(ceph orch upgrade status --format json | jq '.in_progress') != "true" ]]; then
                          echo "Upgrade complete"
                          success=true
                          break
                        else
                          int=$(( $int + 1 ))
                          sleep 10
                        fi
                    done
                    if ! $success; then
                      echo "Error completing 'ceph orch upgrade'. Check upgrade status by running 'ceph orch upgrade status' from ncn-s00[1/2/3]."
                      exit 1
                    fi
                    ceph config set global container_image $nexus_location
          {{ end }}
          - name: upgrade-{{$value}}
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies: 
              {{ if eq $index 0 }}
              - run-ceph-upgrade
              {{ end }}
              # each upgrade depends on previous upgrade action and success
              {{ if ne $index 0 }}
              - check-ceph-health-{{ index $.TargetNcns (add $index -1) }}
              {{ end }}
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"
                - name: scriptContent
                  value: |
                    /usr/share/doc/csm/upgrade/scripts/upgrade/ncn-upgrade-ceph-nodes.sh {{$value}}
          - name: check-ceph-health-{{$value}}
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies:
              # check health once node upgrade is complete
              - upgrade-{{$value}}
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -v true
                    /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -n {{$value}} -a true -v true
          {{- end }}
          - name: rescan-ssh-keys
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies:
              # wait for health check of last storage node complete
              - check-ceph-health-{{ with $length := len $.TargetNcns }}{{ index $.TargetNcns (add $length -1) }}{{end}}
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    grep -oP "(ncn-s\w+)" /etc/hosts | sort -u | xargs -t -i ssh {} 'truncate --size=0 ~/.ssh/known_hosts'
                    grep -oP "(ncn-s\w+)" /etc/hosts | sort -u | xargs -t -i ssh {} 'grep -oP "(ncn-s\w+|ncn-m\w+|ncn-w\w+)" /etc/hosts | sort -u | xargs -t -i ssh-keyscan -H \{\} >> /root/.ssh/known_hosts'
          - name: update-bss
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies:
              - rescan-ssh-keys
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    . /usr/share/doc/csm/upgrade/scripts/ceph/lib/update_bss_metadata.sh
                    RESULT=$(update_bss_storage)
                    # check that result contains success!
                    if [[ $RESULT != *"Success!"* ]]
                    then
                      echo "BSS metadata was not successfully updated. Output:"
                      echo "$RESULT"
                      exit 7
                    fi
          - name: ceph-health-check
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies:
              - update-bss
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -v true
          - name: check-ceph-config
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies:
              - update-bss
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    if [[ $(ceph config dump --format json-pretty|jq '.[]|select(.name == "container_image")|.value' | wc -l) -gt 1 ]]; then
                      echo "ERROR  the ceph config has more that one value for container_image set."
                      echo "Look at the following output from ceph config dump --format json-pretty|jq '.[]|select(.name == "container_image")'"
                      ceph config dump --format json-pretty|jq '.[]|select(.name == "container_image")'
                      echo "There should only be one item and its section should be 'global'"
                      echo
                      echo "Troubleshoot this failure by investigating following items."
                      echo "Run 'ceph -s' and 'ceph health detail'"
                      echo "All ceph mon,mgr,mds,crash,rgw,osd daemons should be running the same image. Check this by running 'podman ps' on storage nodes."
                      echo "Look at the ceph upgrade status by running 'ceph orch upgrade status' and check that it is not running and does not have an error."
                      exit 1
                    else
                      echo "Ceph config check succeeded."
                    fi
