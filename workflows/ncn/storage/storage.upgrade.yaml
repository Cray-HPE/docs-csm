#
# MIT License
#
# (C) Copyright 2022-2023 Hewlett Packard Enterprise Development LP
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included
# in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
# OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
# ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
# OTHER DEALINGS IN THE SOFTWARE.
#
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ncn-lifecycle-rebuild-
  labels:
    target-ncns: "{{$length := len .TargetNcns }}{{range $index,$value := .TargetNcns }}{{$myvar := add $index 1}}{{if lt $myvar $length}}{{$value}}.{{else}}{{$value}}{{end}}{{ end }}"
    type: rebuild
    node-type: storage
spec:
  podMetadata:
    annotations:
      sidecar.istio.io/inject: "false"    
  volumes:
    - name: ssh
      hostPath:
        path: /root/.ssh
        type: Directory
    - name: host-usr-bin
      hostPath:
        path: /usr/bin
        type: Directory
    - name: podinfo
      downwardAPI:
        items:
          - path: "labels"
            fieldRef:
              fieldPath: metadata.labels
          - path: "annotations"
            fieldRef:
              fieldPath: metadata.annotations
  # schedule workflow jobs asap
  priorityCLassName: system-node-critical
  # Pod GC strategy must be one of the following:
  # * OnPodCompletion - delete pods immediately when pod is completed (including errors/failures)
  # * OnPodSuccess - delete pods immediately when pod is successful
  # * OnWorkflowCompletion - delete pods when workflow is completed
  # * OnWorkflowSuccess - delete pods when workflow is successful
  podGC:
    strategy: OnPodCompletion
  # allow workflow jobs running on master node
  #   we may have a situation that all worker nodes
  #   are marked as "being rebuilt" (cray.nls=ncn-w001)
  tolerations:
    - key: "node-role.kubernetes.io/master"
      operator: "Exists"
      effect: "NoSchedule"
  affinity:
    nodeAffinity:
      # avoid putting workflow jobs onto workers that will be rebuilt
      # this label is set onto each workers at beginning of workflow
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cray.nls
            operator: NotIn
            values:
            {{- range $index,$value := .TargetNcns }}
            - {{$value -}}
            {{- end }}
      # try to use master nodes as much as possible
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 50
          preference:
            matchExpressions:
            - key: node-role.kubernetes.io/master
              operator: Exists
  entrypoint: main
  templates:
    - name: main
      dag:
        tasks:
          - name: check-ceph-ro-key
            templateRef:
              name: ssh-template
              template: shell-script
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    if ! $(ceph auth get client.ro >/dev/null 2>/dev/null); then
                      ceph-authtool -C /etc/ceph/ceph.client.ro.keyring -n client.ro --cap mon 'allow r' --cap mds 'allow r' --cap osd 'allow r' --cap mgr 'allow r' --gen-key
                    fi
                    if [ -f "/etc/ceph/ceph.client.ro.keyring" ]; then
                      ceph auth import -i /etc/ceph/ceph.client.ro.keyring
                    else
                      ceph auth get client.ro -o /etc/ceph/ceph.client.ro.keyring
                      ceph auth import -i /etc/ceph/ceph.client.ro.keyring
                    fi
                    for node in $(ceph orch host ls --format=json|jq -r '.[].hostname'); do scp /etc/ceph/ceph.client.ro.keyring $node:/etc/ceph/ceph.client.ro.keyring; done
          - name: pre-upgrade-ceph-health-check
            templateRef:
              name: ssh-template
              template: shell-script
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"
                - name: scriptContent
                  value: |
                    /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -v true || /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -A true -v true
          {{- range $index,$value := .TargetNcns}}
          - name: upload-ceph-images-to-nexus-{{$value}}
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies: 
              - check-ceph-ro-key
              - pre-upgrade-ceph-health-check
              # each upgrade depends on previous upgrade action and success
              {{ if ne $index 0 }}
              - check-ceph-health-{{ index $.TargetNcns (add $index -1) }}
              {{ end }}
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    ts=$(echo $RANDOM | md5sum | head -c 20; echo)
                    cat <<'EOF' > "/tmp/${ts}.sh"
                    m001_ip=$(host ncn-m001 | awk '{ print $NF }')
                    ssh-keygen -R ncn-m001 -f ~/.ssh/known_hosts > /dev/null 2>&1
                    ssh-keygen -R ${m001_ip} -f ~/.ssh/known_hosts > /dev/null 2>&1
                    ssh-keyscan -H "ncn-m001,${m001_ip}" >> ~/.ssh/known_hosts
                    nexus_username=$(ssh ncn-m001 kubectl get secret -n nexus nexus-admin-credential -o json | jq '.data.username' | tr -d '"' | base64 --decode)
                    nexus_password=$(ssh ncn-m001 kubectl get secret -n nexus nexus-admin-credential -o json | jq '.data.password' | tr -d '"' | base64 --decode)
                    ssh_options="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
                    function upload_image() {
                        # get local image and nexus image location
                        name=$1
                        prefix=$2
                        to_configure=$3
                        local_image=$(ceph --name client.ro orch ps --format json | jq --arg DAEMON $name '.[] | select(.daemon_type == $DAEMON) | .container_image_name' | tr -d '"' | sort -u | tail -1)
                        # if sha in image then remove and use version
                        if [[ $local_image == *"@sha"* ]]; then
                            without_sha=${local_image%"@sha"*}
                            version=$(ceph --name client.ro orch ps --format json | jq --arg DAEMON $name '.[] | select(.daemon_type == $DAEMON) | .version' | tr -d '"' | sort -u)
                            if [[ $version != "v"* ]]; then version="v""$version"; fi
                            local_image="$without_sha"":""$version"
                        fi
                        nexus_location="${prefix}""$(echo "$local_image" | rev | cut -d "/" -f1 | rev)"

                        # push images to nexus, point to nexus and run upgrade
                        echo "Pushing image: $local_image to $nexus_location"
                        podman pull $local_image
                        podman tag $local_image $nexus_location
                        podman push --creds $nexus_username:$nexus_password $nexus_location
                    }
                    prometheus_prefix="registry.local/artifactory.algol60.net/csm-docker/stable/quay.io/prometheus/"
                    upload_image "prometheus" $prometheus_prefix "mgr/cephadm/container_image_prometheus"
                    upload_image "node-exporter" $prometheus_prefix "mgr/cephadm/container_image_node_exporter"
                    upload_image "alertmanager" $prometheus_prefix "mgr/cephadm/container_image_alertmanager"
                    # mgr and grafana have this prfix
                    ceph_prefix="registry.local/artifactory.algol60.net/csm-docker/stable/quay.io/ceph/"
                    upload_image "grafana" $ceph_prefix "mgr/cephadm/container_image_grafana"
                    upload_image "mgr" $ceph_prefix "container_image"
                    EOF
                    chmod +x /tmp/${ts}.sh
                    scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null /tmp/${ts}.sh {{$value}}:/tmp/${ts}.sh
                    ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{$value}} "/tmp/${ts}.sh"
          - name: upgrade-{{$value}}
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies: 
              - upload-ceph-images-to-nexus-{{$value}}
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"
                - name: scriptContent
                  value: |
                    /usr/share/doc/csm/upgrade/scripts/upgrade/ncn-upgrade-ceph-nodes.sh {{$value}}
          - name: check-ceph-health-{{$value}}
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies:
              # check health once node upgrade is complete
              - upgrade-{{$value}}
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -v true
                    /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -n {{$value}} -a true -v true
          {{- end }}
          - name: rescan-ssh-keys
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies:
              # wait for health check of last storage node complete
              - check-ceph-health-{{ with $length := len $.TargetNcns }}{{ index $.TargetNcns (add $length -1) }}{{end}}
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    grep -oP "(ncn-s\w+)" /etc/hosts | sort -u | xargs -t -i ssh {} 'truncate --size=0 ~/.ssh/known_hosts'
                    grep -oP "(ncn-s\w+)" /etc/hosts | sort -u | xargs -t -i ssh {} 'grep -oP "(ncn-s\w+|ncn-m\w+|ncn-w\w+)" /etc/hosts | sort -u | xargs -t -i ssh-keyscan -H \{\} >> /root/.ssh/known_hosts'
          - name: update-bss
            templateRef:
              name: ssh-template
              template: shell-script
            dependencies:
              - rescan-ssh-keys
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    . /usr/share/doc/csm/upgrade/scripts/ceph/lib/update_bss_metadata.sh
                    RESULT=$(update_bss_storage)
                    # check that result contains success!
                    if [[ $RESULT != *"Success!"* ]]
                    then
                      echo "BSS metadata was not successfully updated. Output:"
                      echo "$RESULT"
                      exit 7
                    fi
