#
# MIT License
#
# (C) Copyright 2022 Hewlett Packard Enterprise Development LP
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included
# in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
# OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
# ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
# OTHER DEALINGS IN THE SOFTWARE.
#
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ncn-lifecycle-rebuild-
  labels:
    target-ncns: "{{$length := len .TargetNcns }}{{range $index,$value := .TargetNcns }}{{$myvar := add $index 1}}{{if lt $myvar $length}}{{$value}}.{{else}}{{$value}}{{end}}{{ end }}"
    type: rebuild
    node-type: worker
spec:
  podMetadata:
    annotations:
      sidecar.istio.io/inject: "false"    
  volumes:
    - name: ssh
      hostPath:
        path: /root/.ssh
        type: Directory
    - name: host-usr-bin
      hostPath:
        path: /usr/bin
        type: Directory
    - name: podinfo
      downwardAPI:
        items:
          - path: "labels"
            fieldRef:
              fieldPath: metadata.labels
          - path: "annotations"
            fieldRef:
              fieldPath: metadata.annotations
  # schedule workflow jobs asap
  priorityCLassName: system-node-critical
  # Pod GC strategy must be one of the following:
  # * OnPodCompletion - delete pods immediately when pod is completed (including errors/failures)
  # * OnPodSuccess - delete pods immediately when pod is successful
  # * OnWorkflowCompletion - delete pods when workflow is completed
  # * OnWorkflowSuccess - delete pods when workflow is successful
  podGC:
    strategy: OnPodCompletion
  # allow workflow jobs running on master node
  #   we may have a situation that all worker nodes
  #   are marked as "being rebuilt" (cray.nls=ncn-w001)
  tolerations:
    - key: "node-role.kubernetes.io/master"
      operator: "Exists"
      effect: "NoSchedule"
  affinity:
    nodeAffinity:
      # avoid putting workflow jobs onto workers that will be rebuilt
      # this label is set onto each worker at beginning of workflow
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cray.nls
            operator: NotIn
            values:
            {{- range $index,$value := .TargetNcns }}
            - {{$value -}}
            {{- end }}
      # try to use master nodes as much as possible
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 50
          preference:
            matchExpressions:
            - key: node-role.kubernetes.io/master
              operator: Exists
  entrypoint: main
  templates:
    - name: main
      dag:
        tasks:
          - name: install-csi
            templateRef:
              name: ssh-template
              template: shell-script
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    source /srv/cray/scripts/metal/metal-lib.sh
                    csi_url=$(paginate "https://packages.local/service/rest/v1/components?repository=csm-sle-15sp3" \
                      | jq -r  '.items[] | .assets[] | .downloadUrl' | grep "cray-site-init" | sort -V | tail -1)
                    pdsh -w $(grep -oP 'ncn-\m\d+' /etc/hosts | sort -u | tr -t '\n' ',') "zypper install -y $csi_url"
          - name: move-critical-singleton-pods
            templateRef:
              name: ssh-template
              template: shell-script
            arguments:
              parameters:
                - name: dryRun
                  value: "{{$.DryRun}}"  
                - name: scriptContent
                  value: |
                    set -x
                    targetNcns="{{.TargetNcns}}"
                    # remove '[' and ']'
                    targetNcns=${targetNcns:1:-1}
                    # convert to array
                    targetNcnsArray=($targetNcns)
                    # set last rebuilding worker to nodeMoveTo
                    nodeMoveTo=${targetNcnsArray[-1]}
                    # try to find a node that is not in targetNcns
                    nodes=$(kubectl get nodes | grep "ncn-w" | awk '{print $1}')
                    for node in $nodes;do
                      if echo $targetNcns | grep $node; then
                        echo "skip $node"
                      else
                        nodeMoveTo=${node}
                        break
                      fi
                    done

                    echo "nodeMoveTo=${nodeMoveTo}"                    
                    echo

                    pods=( "nexus" "ipxe" "kea" "cray-cfs-api-db" )
                    for pod in ${pods[@]};do                        
                        currentNode=""
                        podName=$(kubectl get pods --all-namespaces | awk '{print $2}' | grep $pod)
                        if [[ -z $podName ]];then
                            echo "Something is wrong. Could not find pod: $pod"
                            exit 1
                        fi
                        echo "pod=$pod podName=$podName"                        
                        while [[ $currentNode != $nodeMoveTo ]]; do                    
                            currentNode=$(kubectl get po -A -o wide | grep $podName | awk '{print $8}')
                            if [[ -z $currentNode ]]; then
                                echo "Unable to determine current node for pod $podName"
                                exit 1
                            fi

                            echo "pod=$pod podName=$podName currentNode=$currentNode"
                            if [[ "$nodeMoveTo" != "$currentNode" ]]; then
                                echo "Move Pod: $podName to Node: $nodeMoveTo"
                                /opt/cray/platform-utils/move_pod.sh $podName $nodeMoveTo
                            fi

                            podName=$(kubectl get pods --all-namespaces | awk '{print $2}' | grep $pod)
                            if [[ -z $podName ]];then
                                echo "Something is wrong. Could not find pod: $pod"
                                exit 1
                            fi
                            ns=$(kubectl get po -A | grep $podName | awk '{print $1}')
                            if [[ -z $ns ]];then
                                echo "Something is wrong. Could not find namespace for pod: $podName"
                                exit 1
                            fi
                            kubectl wait --for=condition=ready pod $podName -n $ns --timeout=5m

                            # Verify that pod is now on the expected node
                            currentNode=$(kubectl get po -A -o wide | grep $podName | awk '{print $8}')
                            echo "pod=$pod podName=$podName currentNode=$currentNode"
                        done
                    done
          {{- range $index,$value := .TargetNcns }}
          - name: add-labels-{{$value}}
            template: add-labels
            arguments: 
              parameters:
              - name: targetNcn
                value: {{$value}}
          # drain: sync
          #     Only one worker can be drained at a time
          - name: drain-{{$value}}
            template: drain
            dependencies:
              - add-labels-{{$value}}
              - move-critical-singleton-pods
              - install-csi
              # each drain depends on previous drain action
              # so we make sure only one node is drained at a time
              {{ if ne $index 0 }}
              - drain-{{ index $.TargetNcns (add $index -1) }}
              {{ end }}
            arguments:
              parameters:
              - name: targetNcn
                value: {{$value}}
              - name: dryRun
                value: "{{$.DryRun}}"
          # wipe and reboot: parallel
          #     once a worker node is drained from k8s
          #     we can safely wipe and reboot this node
          #     regardless of what state other nodes are in
          - name: wipe-and-reboot-{{$value}}
            dependencies: 
              - drain-{{$value}}
            template: wipe-and-reboot
            arguments:
              parameters:
              - name: targetNcn
                value: {{$value}}
              - name: dryRun
                value: "{{$.DryRun}}"
          # post rebuild: parallel
          #     Post rebuild validation can be run in parallel
          - name: post-rebuild-{{$value}}
            dependencies: 
              - wipe-and-reboot-{{$value}}
            template: post-rebuild
            arguments:
              parameters:
              - name: targetNcn
                value: {{$value}}
              - name: dryRun
                value: "{{$.DryRun}}"
          {{- end }}
    # reference to individual tasks
    - name: add-labels
      inputs:
        parameters:
          - name: targetNcn
      resource:
        action: patch
        mergeStrategy: json
        flags:
          - "node"
          - "{{ `{{inputs.parameters.targetNcn}}` }}"
        manifest: |
          - op: add
            path: /metadata/labels/cray.nls
            value: {{ `{{inputs.parameters.targetNcn}}` }}
    - name: drain
      inputs:
        # import ./common.envar.yaml
        {{- include "worker.common.parameters" . | indent 8 }}
      dag:
        # import ./worker.drain.yaml
        {{- include "worker.drain" . | indent 8 }}
    - name: wipe-and-reboot
      inputs:
        # import ./worker.common.parameters.yaml
        {{- include "worker.common.parameters" . | indent 8 }}
      dag:
        # import ./worker.wipe-and-reboot.yaml
        {{- include "worker.wipe-and-reboot" . | indent 8 }}
    - name: post-rebuild
      inputs:
        # import ./worker.common.parameters.yaml
        {{- include "worker.common.parameters" . | indent 8 }}
      dag:
        # import ./worker.post-rebuild.yaml
        {{- include "worker.post-rebuild" . | indent 8 }}
